#############params############
cuda:0
Task=FewRel, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 12.7807016CurrentTrain: epoch  0, batch     1 | loss: 12.7539797CurrentTrain: epoch  0, batch     2 | loss: 12.5999565CurrentTrain: epoch  0, batch     3 | loss: 12.5775681CurrentTrain: epoch  0, batch     4 | loss: 12.2534428CurrentTrain: epoch  0, batch     5 | loss: 12.0535641CurrentTrain: epoch  0, batch     6 | loss: 11.9299412CurrentTrain: epoch  0, batch     7 | loss: 11.8334656CurrentTrain: epoch  0, batch     8 | loss: 11.6542406CurrentTrain: epoch  0, batch     9 | loss: 11.3954353CurrentTrain: epoch  0, batch    10 | loss: 11.2784853CurrentTrain: epoch  0, batch    11 | loss: 11.0749092CurrentTrain: epoch  0, batch    12 | loss: 11.1766300CurrentTrain: epoch  0, batch    13 | loss: 10.9853172CurrentTrain: epoch  0, batch    14 | loss: 10.6299648CurrentTrain: epoch  0, batch    15 | loss: 10.6395683CurrentTrain: epoch  0, batch    16 | loss: 10.9335289CurrentTrain: epoch  0, batch    17 | loss: 10.7724361CurrentTrain: epoch  0, batch    18 | loss: 10.2475872CurrentTrain: epoch  0, batch    19 | loss: 10.3488636CurrentTrain: epoch  0, batch    20 | loss: 10.7349796CurrentTrain: epoch  0, batch    21 | loss: 10.8694067CurrentTrain: epoch  0, batch    22 | loss: 10.3300247CurrentTrain: epoch  0, batch    23 | loss: 10.6644430CurrentTrain: epoch  0, batch    24 | loss: 10.2083092CurrentTrain: epoch  0, batch    25 | loss: 10.4096355CurrentTrain: epoch  0, batch    26 | loss: 10.8333778CurrentTrain: epoch  0, batch    27 | loss: 10.4295607CurrentTrain: epoch  0, batch    28 | loss: 10.0802040CurrentTrain: epoch  0, batch    29 | loss: 10.0121708CurrentTrain: epoch  0, batch    30 | loss: 9.5349045CurrentTrain: epoch  0, batch    31 | loss: 10.0359325CurrentTrain: epoch  0, batch    32 | loss: 10.5084333CurrentTrain: epoch  0, batch    33 | loss: 9.7875328CurrentTrain: epoch  0, batch    34 | loss: 10.1426582CurrentTrain: epoch  0, batch    35 | loss: 9.6395369CurrentTrain: epoch  0, batch    36 | loss: 9.6095905CurrentTrain: epoch  0, batch    37 | loss: 10.2514019CurrentTrain: epoch  0, batch    38 | loss: 9.2863846CurrentTrain: epoch  0, batch    39 | loss: 9.7686253CurrentTrain: epoch  0, batch    40 | loss: 9.5201225CurrentTrain: epoch  0, batch    41 | loss: 9.4223843CurrentTrain: epoch  0, batch    42 | loss: 9.5692406CurrentTrain: epoch  0, batch    43 | loss: 9.1514626CurrentTrain: epoch  0, batch    44 | loss: 9.1801224CurrentTrain: epoch  0, batch    45 | loss: 8.8626480CurrentTrain: epoch  0, batch    46 | loss: 8.9549904CurrentTrain: epoch  0, batch    47 | loss: 8.9507484CurrentTrain: epoch  0, batch    48 | loss: 8.8127728CurrentTrain: epoch  0, batch    49 | loss: 9.1142864CurrentTrain: epoch  0, batch    50 | loss: 8.2792244CurrentTrain: epoch  0, batch    51 | loss: 8.7598867CurrentTrain: epoch  0, batch    52 | loss: 8.7031498CurrentTrain: epoch  0, batch    53 | loss: 9.4445562CurrentTrain: epoch  0, batch    54 | loss: 8.5798998CurrentTrain: epoch  0, batch    55 | loss: 8.8594408CurrentTrain: epoch  0, batch    56 | loss: 8.5297556CurrentTrain: epoch  0, batch    57 | loss: 8.0497913CurrentTrain: epoch  0, batch    58 | loss: 8.0619164CurrentTrain: epoch  0, batch    59 | loss: 8.0606709CurrentTrain: epoch  0, batch    60 | loss: 7.9827671CurrentTrain: epoch  0, batch    61 | loss: 8.1937866CurrentTrain: epoch  0, batch    62 | loss: 6.7449417CurrentTrain: epoch  1, batch     0 | loss: 8.4355049CurrentTrain: epoch  1, batch     1 | loss: 7.8977480CurrentTrain: epoch  1, batch     2 | loss: 8.0915241CurrentTrain: epoch  1, batch     3 | loss: 7.5351739CurrentTrain: epoch  1, batch     4 | loss: 7.5956969CurrentTrain: epoch  1, batch     5 | loss: 8.0023746CurrentTrain: epoch  1, batch     6 | loss: 7.5479126CurrentTrain: epoch  1, batch     7 | loss: 7.3652711CurrentTrain: epoch  1, batch     8 | loss: 7.7264891CurrentTrain: epoch  1, batch     9 | loss: 7.4230328CurrentTrain: epoch  1, batch    10 | loss: 8.2561760CurrentTrain: epoch  1, batch    11 | loss: 8.5476646CurrentTrain: epoch  1, batch    12 | loss: 7.9492130CurrentTrain: epoch  1, batch    13 | loss: 7.7443485CurrentTrain: epoch  1, batch    14 | loss: 7.6516824CurrentTrain: epoch  1, batch    15 | loss: 8.6796303CurrentTrain: epoch  1, batch    16 | loss: 7.4179511CurrentTrain: epoch  1, batch    17 | loss: 7.5795631CurrentTrain: epoch  1, batch    18 | loss: 7.9991302CurrentTrain: epoch  1, batch    19 | loss: 7.3478260CurrentTrain: epoch  1, batch    20 | loss: 7.8383918CurrentTrain: epoch  1, batch    21 | loss: 7.3426986CurrentTrain: epoch  1, batch    22 | loss: 7.3429441CurrentTrain: epoch  1, batch    23 | loss: 7.3746061CurrentTrain: epoch  1, batch    24 | loss: 7.3166962CurrentTrain: epoch  1, batch    25 | loss: 7.4099517CurrentTrain: epoch  1, batch    26 | loss: 7.4666419CurrentTrain: epoch  1, batch    27 | loss: 7.0708532CurrentTrain: epoch  1, batch    28 | loss: 6.8601818CurrentTrain: epoch  1, batch    29 | loss: 7.0492015CurrentTrain: epoch  1, batch    30 | loss: 7.3624258CurrentTrain: epoch  1, batch    31 | loss: 7.4542532CurrentTrain: epoch  1, batch    32 | loss: 6.2958145CurrentTrain: epoch  1, batch    33 | loss: 6.8904948CurrentTrain: epoch  1, batch    34 | loss: 7.0656781CurrentTrain: epoch  1, batch    35 | loss: 7.8844690CurrentTrain: epoch  1, batch    36 | loss: 6.4552107CurrentTrain: epoch  1, batch    37 | loss: 7.0552192CurrentTrain: epoch  1, batch    38 | loss: 7.4491553CurrentTrain: epoch  1, batch    39 | loss: 6.7807417CurrentTrain: epoch  1, batch    40 | loss: 6.6982427CurrentTrain: epoch  1, batch    41 | loss: 6.5107403CurrentTrain: epoch  1, batch    42 | loss: 6.8977194CurrentTrain: epoch  1, batch    43 | loss: 6.8068624CurrentTrain: epoch  1, batch    44 | loss: 7.5962529CurrentTrain: epoch  1, batch    45 | loss: 7.0068684CurrentTrain: epoch  1, batch    46 | loss: 6.0658150CurrentTrain: epoch  1, batch    47 | loss: 5.9586954CurrentTrain: epoch  1, batch    48 | loss: 6.5234947CurrentTrain: epoch  1, batch    49 | loss: 6.9289856CurrentTrain: epoch  1, batch    50 | loss: 6.4239540CurrentTrain: epoch  1, batch    51 | loss: 6.4321213CurrentTrain: epoch  1, batch    52 | loss: 6.8064418CurrentTrain: epoch  1, batch    53 | loss: 7.1960468CurrentTrain: epoch  1, batch    54 | loss: 6.2763462CurrentTrain: epoch  1, batch    55 | loss: 6.8155584CurrentTrain: epoch  1, batch    56 | loss: 6.1869373CurrentTrain: epoch  1, batch    57 | loss: 6.8459101CurrentTrain: epoch  1, batch    58 | loss: 6.2807941CurrentTrain: epoch  1, batch    59 | loss: 7.0984192CurrentTrain: epoch  1, batch    60 | loss: 6.6055236CurrentTrain: epoch  1, batch    61 | loss: 5.4397373CurrentTrain: epoch  1, batch    62 | loss: 5.1134691CurrentTrain: epoch  2, batch     0 | loss: 6.6287928CurrentTrain: epoch  2, batch     1 | loss: 5.1035161CurrentTrain: epoch  2, batch     2 | loss: 5.9932604CurrentTrain: epoch  2, batch     3 | loss: 7.3226933CurrentTrain: epoch  2, batch     4 | loss: 6.6956525CurrentTrain: epoch  2, batch     5 | loss: 5.9080563CurrentTrain: epoch  2, batch     6 | loss: 6.0324783CurrentTrain: epoch  2, batch     7 | loss: 6.3720984CurrentTrain: epoch  2, batch     8 | loss: 5.7227039CurrentTrain: epoch  2, batch     9 | loss: 5.7503409CurrentTrain: epoch  2, batch    10 | loss: 5.8918438CurrentTrain: epoch  2, batch    11 | loss: 6.0373464CurrentTrain: epoch  2, batch    12 | loss: 5.9816947CurrentTrain: epoch  2, batch    13 | loss: 5.5005121CurrentTrain: epoch  2, batch    14 | loss: 6.1880379CurrentTrain: epoch  2, batch    15 | loss: 6.0433660CurrentTrain: epoch  2, batch    16 | loss: 5.5230861CurrentTrain: epoch  2, batch    17 | loss: 5.6102753CurrentTrain: epoch  2, batch    18 | loss: 7.0365171CurrentTrain: epoch  2, batch    19 | loss: 5.9359818CurrentTrain: epoch  2, batch    20 | loss: 6.0337553CurrentTrain: epoch  2, batch    21 | loss: 6.2495522CurrentTrain: epoch  2, batch    22 | loss: 6.1759357CurrentTrain: epoch  2, batch    23 | loss: 5.6207476CurrentTrain: epoch  2, batch    24 | loss: 5.8494282CurrentTrain: epoch  2, batch    25 | loss: 6.1729712CurrentTrain: epoch  2, batch    26 | loss: 5.9559126CurrentTrain: epoch  2, batch    27 | loss: 6.5260296CurrentTrain: epoch  2, batch    28 | loss: 6.1659842CurrentTrain: epoch  2, batch    29 | loss: 6.0940218CurrentTrain: epoch  2, batch    30 | loss: 5.9625854CurrentTrain: epoch  2, batch    31 | loss: 5.9537320CurrentTrain: epoch  2, batch    32 | loss: 5.7809978CurrentTrain: epoch  2, batch    33 | loss: 5.4454889CurrentTrain: epoch  2, batch    34 | loss: 5.5378551CurrentTrain: epoch  2, batch    35 | loss: 5.7525315CurrentTrain: epoch  2, batch    36 | loss: 6.3268232CurrentTrain: epoch  2, batch    37 | loss: 6.0724087CurrentTrain: epoch  2, batch    38 | loss: 5.7148414CurrentTrain: epoch  2, batch    39 | loss: 5.7856793CurrentTrain: epoch  2, batch    40 | loss: 5.4209099CurrentTrain: epoch  2, batch    41 | loss: 5.4390478CurrentTrain: epoch  2, batch    42 | loss: 5.5550890CurrentTrain: epoch  2, batch    43 | loss: 5.9459438CurrentTrain: epoch  2, batch    44 | loss: 6.6466322CurrentTrain: epoch  2, batch    45 | loss: 5.6763067CurrentTrain: epoch  2, batch    46 | loss: 5.2037420CurrentTrain: epoch  2, batch    47 | loss: 5.5665159CurrentTrain: epoch  2, batch    48 | loss: 6.1390314CurrentTrain: epoch  2, batch    49 | loss: 6.1607985CurrentTrain: epoch  2, batch    50 | loss: 5.5216017CurrentTrain: epoch  2, batch    51 | loss: 5.3782978CurrentTrain: epoch  2, batch    52 | loss: 5.6020679CurrentTrain: epoch  2, batch    53 | loss: 5.6078300CurrentTrain: epoch  2, batch    54 | loss: 6.2458177CurrentTrain: epoch  2, batch    55 | loss: 5.0822134CurrentTrain: epoch  2, batch    56 | loss: 5.6673641CurrentTrain: epoch  2, batch    57 | loss: 5.5232277CurrentTrain: epoch  2, batch    58 | loss: 5.0177593CurrentTrain: epoch  2, batch    59 | loss: 5.6791325CurrentTrain: epoch  2, batch    60 | loss: 5.2652750CurrentTrain: epoch  2, batch    61 | loss: 4.9217978CurrentTrain: epoch  2, batch    62 | loss: 4.7562971CurrentTrain: epoch  3, batch     0 | loss: 4.9559503CurrentTrain: epoch  3, batch     1 | loss: 5.3599176CurrentTrain: epoch  3, batch     2 | loss: 5.4492846CurrentTrain: epoch  3, batch     3 | loss: 5.2220488CurrentTrain: epoch  3, batch     4 | loss: 5.1972389CurrentTrain: epoch  3, batch     5 | loss: 5.2305508CurrentTrain: epoch  3, batch     6 | loss: 5.1213026CurrentTrain: epoch  3, batch     7 | loss: 4.8594131CurrentTrain: epoch  3, batch     8 | loss: 5.0868168CurrentTrain: epoch  3, batch     9 | loss: 5.4064302CurrentTrain: epoch  3, batch    10 | loss: 5.3327475CurrentTrain: epoch  3, batch    11 | loss: 5.6959295CurrentTrain: epoch  3, batch    12 | loss: 5.2919760CurrentTrain: epoch  3, batch    13 | loss: 4.9032564CurrentTrain: epoch  3, batch    14 | loss: 5.0401831CurrentTrain: epoch  3, batch    15 | loss: 5.4902782CurrentTrain: epoch  3, batch    16 | loss: 4.7586775CurrentTrain: epoch  3, batch    17 | loss: 5.8124328CurrentTrain: epoch  3, batch    18 | loss: 5.2609272CurrentTrain: epoch  3, batch    19 | loss: 6.1624336CurrentTrain: epoch  3, batch    20 | loss: 5.3622174CurrentTrain: epoch  3, batch    21 | loss: 5.5165768CurrentTrain: epoch  3, batch    22 | loss: 5.3602695CurrentTrain: epoch  3, batch    23 | loss: 4.9084949CurrentTrain: epoch  3, batch    24 | loss: 4.6899810CurrentTrain: epoch  3, batch    25 | loss: 5.1456690CurrentTrain: epoch  3, batch    26 | loss: 5.1932864CurrentTrain: epoch  3, batch    27 | loss: 4.7177806CurrentTrain: epoch  3, batch    28 | loss: 6.1713924CurrentTrain: epoch  3, batch    29 | loss: 5.2458081CurrentTrain: epoch  3, batch    30 | loss: 4.9140725CurrentTrain: epoch  3, batch    31 | loss: 5.4297934CurrentTrain: epoch  3, batch    32 | loss: 5.7149172CurrentTrain: epoch  3, batch    33 | loss: 4.9920878CurrentTrain: epoch  3, batch    34 | loss: 5.1700444CurrentTrain: epoch  3, batch    35 | loss: 4.7932262CurrentTrain: epoch  3, batch    36 | loss: 4.6696205CurrentTrain: epoch  3, batch    37 | loss: 5.1058683CurrentTrain: epoch  3, batch    38 | loss: 4.7787733CurrentTrain: epoch  3, batch    39 | loss: 4.8343921CurrentTrain: epoch  3, batch    40 | loss: 4.5151272CurrentTrain: epoch  3, batch    41 | loss: 4.9029260CurrentTrain: epoch  3, batch    42 | loss: 5.1019230CurrentTrain: epoch  3, batch    43 | loss: 4.7009802CurrentTrain: epoch  3, batch    44 | loss: 5.2793999CurrentTrain: epoch  3, batch    45 | loss: 4.7721677CurrentTrain: epoch  3, batch    46 | loss: 5.7082119CurrentTrain: epoch  3, batch    47 | loss: 4.8959608CurrentTrain: epoch  3, batch    48 | loss: 4.8165760CurrentTrain: epoch  3, batch    49 | loss: 4.9383698CurrentTrain: epoch  3, batch    50 | loss: 4.9791088CurrentTrain: epoch  3, batch    51 | loss: 4.9800439CurrentTrain: epoch  3, batch    52 | loss: 4.9952784CurrentTrain: epoch  3, batch    53 | loss: 5.0657778CurrentTrain: epoch  3, batch    54 | loss: 4.7023325CurrentTrain: epoch  3, batch    55 | loss: 5.0856791CurrentTrain: epoch  3, batch    56 | loss: 5.2625990CurrentTrain: epoch  3, batch    57 | loss: 4.9002271CurrentTrain: epoch  3, batch    58 | loss: 4.9464731CurrentTrain: epoch  3, batch    59 | loss: 4.3692284CurrentTrain: epoch  3, batch    60 | loss: 4.6403189CurrentTrain: epoch  3, batch    61 | loss: 4.6860266CurrentTrain: epoch  3, batch    62 | loss: 5.4158154CurrentTrain: epoch  4, batch     0 | loss: 4.7789822CurrentTrain: epoch  4, batch     1 | loss: 4.5644565CurrentTrain: epoch  4, batch     2 | loss: 4.7922945CurrentTrain: epoch  4, batch     3 | loss: 4.5662317CurrentTrain: epoch  4, batch     4 | loss: 4.7927341CurrentTrain: epoch  4, batch     5 | loss: 4.7869301CurrentTrain: epoch  4, batch     6 | loss: 4.6627588CurrentTrain: epoch  4, batch     7 | loss: 4.8947401CurrentTrain: epoch  4, batch     8 | loss: 4.7288041CurrentTrain: epoch  4, batch     9 | loss: 4.9738121CurrentTrain: epoch  4, batch    10 | loss: 4.8039675CurrentTrain: epoch  4, batch    11 | loss: 4.7061791CurrentTrain: epoch  4, batch    12 | loss: 4.5820608CurrentTrain: epoch  4, batch    13 | loss: 4.7715254CurrentTrain: epoch  4, batch    14 | loss: 4.5080452CurrentTrain: epoch  4, batch    15 | loss: 4.7742100CurrentTrain: epoch  4, batch    16 | loss: 4.6374722CurrentTrain: epoch  4, batch    17 | loss: 4.4478269CurrentTrain: epoch  4, batch    18 | loss: 4.5523367CurrentTrain: epoch  4, batch    19 | loss: 4.4983926CurrentTrain: epoch  4, batch    20 | loss: 4.5318160CurrentTrain: epoch  4, batch    21 | loss: 4.5876098CurrentTrain: epoch  4, batch    22 | loss: 4.7768202CurrentTrain: epoch  4, batch    23 | loss: 4.6748343CurrentTrain: epoch  4, batch    24 | loss: 5.4958267CurrentTrain: epoch  4, batch    25 | loss: 4.5620637CurrentTrain: epoch  4, batch    26 | loss: 4.4878020CurrentTrain: epoch  4, batch    27 | loss: 4.7440400CurrentTrain: epoch  4, batch    28 | loss: 4.5478315CurrentTrain: epoch  4, batch    29 | loss: 4.3891678CurrentTrain: epoch  4, batch    30 | loss: 5.1537085CurrentTrain: epoch  4, batch    31 | loss: 4.3668299CurrentTrain: epoch  4, batch    32 | loss: 4.5458450CurrentTrain: epoch  4, batch    33 | loss: 4.6473064CurrentTrain: epoch  4, batch    34 | loss: 4.4753389CurrentTrain: epoch  4, batch    35 | loss: 5.3909826CurrentTrain: epoch  4, batch    36 | loss: 4.4831686CurrentTrain: epoch  4, batch    37 | loss: 4.5535631CurrentTrain: epoch  4, batch    38 | loss: 4.5240545CurrentTrain: epoch  4, batch    39 | loss: 4.6040974CurrentTrain: epoch  4, batch    40 | loss: 5.4083357CurrentTrain: epoch  4, batch    41 | loss: 4.3056211CurrentTrain: epoch  4, batch    42 | loss: 4.4637475CurrentTrain: epoch  4, batch    43 | loss: 5.0431819CurrentTrain: epoch  4, batch    44 | loss: 4.4682393CurrentTrain: epoch  4, batch    45 | loss: 4.3633137CurrentTrain: epoch  4, batch    46 | loss: 4.5992146CurrentTrain: epoch  4, batch    47 | loss: 4.7499666CurrentTrain: epoch  4, batch    48 | loss: 4.3858690CurrentTrain: epoch  4, batch    49 | loss: 4.6934900CurrentTrain: epoch  4, batch    50 | loss: 4.4996810CurrentTrain: epoch  4, batch    51 | loss: 4.6601553CurrentTrain: epoch  4, batch    52 | loss: 4.5638704CurrentTrain: epoch  4, batch    53 | loss: 4.5786867CurrentTrain: epoch  4, batch    54 | loss: 4.4388614CurrentTrain: epoch  4, batch    55 | loss: 4.9535861CurrentTrain: epoch  4, batch    56 | loss: 4.5433388CurrentTrain: epoch  4, batch    57 | loss: 4.5788102CurrentTrain: epoch  4, batch    58 | loss: 4.5018616CurrentTrain: epoch  4, batch    59 | loss: 4.9385352CurrentTrain: epoch  4, batch    60 | loss: 4.3446951CurrentTrain: epoch  4, batch    61 | loss: 4.6549535CurrentTrain: epoch  4, batch    62 | loss: 4.3735919CurrentTrain: epoch  5, batch     0 | loss: 4.5700197CurrentTrain: epoch  5, batch     1 | loss: 4.3170042CurrentTrain: epoch  5, batch     2 | loss: 4.3828192CurrentTrain: epoch  5, batch     3 | loss: 4.4654274CurrentTrain: epoch  5, batch     4 | loss: 4.3699145CurrentTrain: epoch  5, batch     5 | loss: 4.5076532CurrentTrain: epoch  5, batch     6 | loss: 4.5996194CurrentTrain: epoch  5, batch     7 | loss: 4.3835783CurrentTrain: epoch  5, batch     8 | loss: 4.3491344CurrentTrain: epoch  5, batch     9 | loss: 4.4170713CurrentTrain: epoch  5, batch    10 | loss: 4.4065351CurrentTrain: epoch  5, batch    11 | loss: 4.4104013CurrentTrain: epoch  5, batch    12 | loss: 4.4397726CurrentTrain: epoch  5, batch    13 | loss: 4.5682378CurrentTrain: epoch  5, batch    14 | loss: 4.3139696CurrentTrain: epoch  5, batch    15 | loss: 4.5497055CurrentTrain: epoch  5, batch    16 | loss: 4.3523560CurrentTrain: epoch  5, batch    17 | loss: 4.4614534CurrentTrain: epoch  5, batch    18 | loss: 4.3755074CurrentTrain: epoch  5, batch    19 | loss: 4.3704801CurrentTrain: epoch  5, batch    20 | loss: 4.3345833CurrentTrain: epoch  5, batch    21 | loss: 4.4443316CurrentTrain: epoch  5, batch    22 | loss: 4.3310366CurrentTrain: epoch  5, batch    23 | loss: 4.3423061CurrentTrain: epoch  5, batch    24 | loss: 4.2741604CurrentTrain: epoch  5, batch    25 | loss: 4.2450008CurrentTrain: epoch  5, batch    26 | loss: 4.3501625CurrentTrain: epoch  5, batch    27 | loss: 4.3844314CurrentTrain: epoch  5, batch    28 | loss: 4.4630451CurrentTrain: epoch  5, batch    29 | loss: 4.3562431CurrentTrain: epoch  5, batch    30 | loss: 4.3102927CurrentTrain: epoch  5, batch    31 | loss: 4.3086309CurrentTrain: epoch  5, batch    32 | loss: 4.3092146CurrentTrain: epoch  5, batch    33 | loss: 4.3298826CurrentTrain: epoch  5, batch    34 | loss: 4.4573951CurrentTrain: epoch  5, batch    35 | loss: 4.5009260CurrentTrain: epoch  5, batch    36 | loss: 4.2569170CurrentTrain: epoch  5, batch    37 | loss: 4.3217702CurrentTrain: epoch  5, batch    38 | loss: 4.2533889CurrentTrain: epoch  5, batch    39 | loss: 4.2617831CurrentTrain: epoch  5, batch    40 | loss: 4.4131241CurrentTrain: epoch  5, batch    41 | loss: 4.6655769CurrentTrain: epoch  5, batch    42 | loss: 4.3106651CurrentTrain: epoch  5, batch    43 | loss: 4.6107025CurrentTrain: epoch  5, batch    44 | loss: 4.6016760CurrentTrain: epoch  5, batch    45 | loss: 4.3948612CurrentTrain: epoch  5, batch    46 | loss: 4.2523308CurrentTrain: epoch  5, batch    47 | loss: 4.3147106CurrentTrain: epoch  5, batch    48 | loss: 4.2220211CurrentTrain: epoch  5, batch    49 | loss: 5.2232513CurrentTrain: epoch  5, batch    50 | loss: 4.4027901CurrentTrain: epoch  5, batch    51 | loss: 4.5129819CurrentTrain: epoch  5, batch    52 | loss: 4.3551188CurrentTrain: epoch  5, batch    53 | loss: 4.5922856CurrentTrain: epoch  5, batch    54 | loss: 4.3549786CurrentTrain: epoch  5, batch    55 | loss: 4.4475021CurrentTrain: epoch  5, batch    56 | loss: 4.3401222CurrentTrain: epoch  5, batch    57 | loss: 4.3261194CurrentTrain: epoch  5, batch    58 | loss: 4.5429430CurrentTrain: epoch  5, batch    59 | loss: 4.5346861CurrentTrain: epoch  5, batch    60 | loss: 4.4519653CurrentTrain: epoch  5, batch    61 | loss: 4.1750894CurrentTrain: epoch  5, batch    62 | loss: 4.3634920CurrentTrain: epoch  6, batch     0 | loss: 4.2963495CurrentTrain: epoch  6, batch     1 | loss: 4.7609825CurrentTrain: epoch  6, batch     2 | loss: 4.2707157CurrentTrain: epoch  6, batch     3 | loss: 4.2842207CurrentTrain: epoch  6, batch     4 | loss: 4.4981341CurrentTrain: epoch  6, batch     5 | loss: 4.2145281CurrentTrain: epoch  6, batch     6 | loss: 4.2792206CurrentTrain: epoch  6, batch     7 | loss: 4.4667554CurrentTrain: epoch  6, batch     8 | loss: 4.2446489CurrentTrain: epoch  6, batch     9 | loss: 4.3127079CurrentTrain: epoch  6, batch    10 | loss: 4.3153353CurrentTrain: epoch  6, batch    11 | loss: 4.3154244CurrentTrain: epoch  6, batch    12 | loss: 4.4480486CurrentTrain: epoch  6, batch    13 | loss: 4.2017117CurrentTrain: epoch  6, batch    14 | loss: 4.3441668CurrentTrain: epoch  6, batch    15 | loss: 4.1756206CurrentTrain: epoch  6, batch    16 | loss: 4.3158541CurrentTrain: epoch  6, batch    17 | loss: 4.2151847CurrentTrain: epoch  6, batch    18 | loss: 4.1577506CurrentTrain: epoch  6, batch    19 | loss: 4.3055515CurrentTrain: epoch  6, batch    20 | loss: 4.1694317CurrentTrain: epoch  6, batch    21 | loss: 4.2989368CurrentTrain: epoch  6, batch    22 | loss: 4.1875486CurrentTrain: epoch  6, batch    23 | loss: 4.2411370CurrentTrain: epoch  6, batch    24 | loss: 4.2934990CurrentTrain: epoch  6, batch    25 | loss: 4.2988625CurrentTrain: epoch  6, batch    26 | loss: 4.2368240CurrentTrain: epoch  6, batch    27 | loss: 4.2733655CurrentTrain: epoch  6, batch    28 | loss: 4.2174940CurrentTrain: epoch  6, batch    29 | loss: 4.2537398CurrentTrain: epoch  6, batch    30 | loss: 4.2549887CurrentTrain: epoch  6, batch    31 | loss: 4.3372474CurrentTrain: epoch  6, batch    32 | loss: 4.3627553CurrentTrain: epoch  6, batch    33 | loss: 4.1762357CurrentTrain: epoch  6, batch    34 | loss: 4.2253361CurrentTrain: epoch  6, batch    35 | loss: 4.1891856CurrentTrain: epoch  6, batch    36 | loss: 4.2155056CurrentTrain: epoch  6, batch    37 | loss: 4.1944118CurrentTrain: epoch  6, batch    38 | loss: 4.1185222CurrentTrain: epoch  6, batch    39 | loss: 4.2631836CurrentTrain: epoch  6, batch    40 | loss: 4.2158060CurrentTrain: epoch  6, batch    41 | loss: 4.1979213CurrentTrain: epoch  6, batch    42 | loss: 4.2107663CurrentTrain: epoch  6, batch    43 | loss: 4.1817737CurrentTrain: epoch  6, batch    44 | loss: 4.2329426CurrentTrain: epoch  6, batch    45 | loss: 4.2719040CurrentTrain: epoch  6, batch    46 | loss: 4.1773024CurrentTrain: epoch  6, batch    47 | loss: 4.1546474CurrentTrain: epoch  6, batch    48 | loss: 4.1758466CurrentTrain: epoch  6, batch    49 | loss: 4.2371674CurrentTrain: epoch  6, batch    50 | loss: 4.1608939CurrentTrain: epoch  6, batch    51 | loss: 4.1588178CurrentTrain: epoch  6, batch    52 | loss: 4.1904993CurrentTrain: epoch  6, batch    53 | loss: 4.1684041CurrentTrain: epoch  6, batch    54 | loss: 4.1068316CurrentTrain: epoch  6, batch    55 | loss: 4.1553726CurrentTrain: epoch  6, batch    56 | loss: 4.3479109CurrentTrain: epoch  6, batch    57 | loss: 4.1698589CurrentTrain: epoch  6, batch    58 | loss: 4.1409726CurrentTrain: epoch  6, batch    59 | loss: 4.1781926CurrentTrain: epoch  6, batch    60 | loss: 4.1144800CurrentTrain: epoch  6, batch    61 | loss: 4.1344490CurrentTrain: epoch  6, batch    62 | loss: 4.1572695CurrentTrain: epoch  7, batch     0 | loss: 4.1518402CurrentTrain: epoch  7, batch     1 | loss: 4.1346889CurrentTrain: epoch  7, batch     2 | loss: 4.2143755CurrentTrain: epoch  7, batch     3 | loss: 4.2565560CurrentTrain: epoch  7, batch     4 | loss: 4.1361575CurrentTrain: epoch  7, batch     5 | loss: 4.1378574CurrentTrain: epoch  7, batch     6 | loss: 4.1272521CurrentTrain: epoch  7, batch     7 | loss: 4.1506472CurrentTrain: epoch  7, batch     8 | loss: 4.1639481CurrentTrain: epoch  7, batch     9 | loss: 4.1042252CurrentTrain: epoch  7, batch    10 | loss: 4.1683192CurrentTrain: epoch  7, batch    11 | loss: 4.2416806CurrentTrain: epoch  7, batch    12 | loss: 4.1504674CurrentTrain: epoch  7, batch    13 | loss: 4.1433473CurrentTrain: epoch  7, batch    14 | loss: 4.1628175CurrentTrain: epoch  7, batch    15 | loss: 4.1597385CurrentTrain: epoch  7, batch    16 | loss: 4.0950089CurrentTrain: epoch  7, batch    17 | loss: 4.1180215CurrentTrain: epoch  7, batch    18 | loss: 4.1492443CurrentTrain: epoch  7, batch    19 | loss: 4.1653686CurrentTrain: epoch  7, batch    20 | loss: 4.1615391CurrentTrain: epoch  7, batch    21 | loss: 4.1179972CurrentTrain: epoch  7, batch    22 | loss: 4.1629667CurrentTrain: epoch  7, batch    23 | loss: 4.1897769CurrentTrain: epoch  7, batch    24 | loss: 4.0768051CurrentTrain: epoch  7, batch    25 | loss: 4.1906166CurrentTrain: epoch  7, batch    26 | loss: 4.1560073CurrentTrain: epoch  7, batch    27 | loss: 4.0987163CurrentTrain: epoch  7, batch    28 | loss: 4.1243191CurrentTrain: epoch  7, batch    29 | loss: 4.1219835CurrentTrain: epoch  7, batch    30 | loss: 4.1513543CurrentTrain: epoch  7, batch    31 | loss: 4.0912542CurrentTrain: epoch  7, batch    32 | loss: 4.0991230CurrentTrain: epoch  7, batch    33 | loss: 4.1334829CurrentTrain: epoch  7, batch    34 | loss: 4.1638746CurrentTrain: epoch  7, batch    35 | loss: 4.1423850CurrentTrain: epoch  7, batch    36 | loss: 4.1659226CurrentTrain: epoch  7, batch    37 | loss: 4.1689997CurrentTrain: epoch  7, batch    38 | loss: 4.1375542CurrentTrain: epoch  7, batch    39 | loss: 4.1067266CurrentTrain: epoch  7, batch    40 | loss: 4.0489745CurrentTrain: epoch  7, batch    41 | loss: 4.1229477CurrentTrain: epoch  7, batch    42 | loss: 4.1094170CurrentTrain: epoch  7, batch    43 | loss: 4.1292000CurrentTrain: epoch  7, batch    44 | loss: 4.1309009CurrentTrain: epoch  7, batch    45 | loss: 4.1784892CurrentTrain: epoch  7, batch    46 | loss: 4.2161212CurrentTrain: epoch  7, batch    47 | loss: 4.0884805CurrentTrain: epoch  7, batch    48 | loss: 4.1212149CurrentTrain: epoch  7, batch    49 | loss: 4.3764687CurrentTrain: epoch  7, batch    50 | loss: 4.1176906CurrentTrain: epoch  7, batch    51 | loss: 4.1474185CurrentTrain: epoch  7, batch    52 | loss: 4.1021829CurrentTrain: epoch  7, batch    53 | loss: 4.1255121CurrentTrain: epoch  7, batch    54 | loss: 4.1091528CurrentTrain: epoch  7, batch    55 | loss: 4.1353688CurrentTrain: epoch  7, batch    56 | loss: 4.0862060CurrentTrain: epoch  7, batch    57 | loss: 4.1389127CurrentTrain: epoch  7, batch    58 | loss: 4.0826421CurrentTrain: epoch  7, batch    59 | loss: 4.0488992CurrentTrain: epoch  7, batch    60 | loss: 4.1058369CurrentTrain: epoch  7, batch    61 | loss: 4.1328487CurrentTrain: epoch  7, batch    62 | loss: 4.1462550CurrentTrain: epoch  8, batch     0 | loss: 4.1170592CurrentTrain: epoch  8, batch     1 | loss: 4.1219454CurrentTrain: epoch  8, batch     2 | loss: 4.1133604CurrentTrain: epoch  8, batch     3 | loss: 4.1039467CurrentTrain: epoch  8, batch     4 | loss: 4.1165805CurrentTrain: epoch  8, batch     5 | loss: 4.1161656CurrentTrain: epoch  8, batch     6 | loss: 4.0897017CurrentTrain: epoch  8, batch     7 | loss: 4.0924377CurrentTrain: epoch  8, batch     8 | loss: 4.0994425CurrentTrain: epoch  8, batch     9 | loss: 4.0802631CurrentTrain: epoch  8, batch    10 | loss: 4.1304002CurrentTrain: epoch  8, batch    11 | loss: 4.1440930CurrentTrain: epoch  8, batch    12 | loss: 4.0925980CurrentTrain: epoch  8, batch    13 | loss: 4.1026077CurrentTrain: epoch  8, batch    14 | loss: 4.1175032CurrentTrain: epoch  8, batch    15 | loss: 4.0526991CurrentTrain: epoch  8, batch    16 | loss: 4.1504312CurrentTrain: epoch  8, batch    17 | loss: 4.0830088CurrentTrain: epoch  8, batch    18 | loss: 4.0597262CurrentTrain: epoch  8, batch    19 | loss: 4.0707216CurrentTrain: epoch  8, batch    20 | loss: 4.0748901CurrentTrain: epoch  8, batch    21 | loss: 4.0919380CurrentTrain: epoch  8, batch    22 | loss: 4.1050892CurrentTrain: epoch  8, batch    23 | loss: 4.0705080CurrentTrain: epoch  8, batch    24 | loss: 4.1378794CurrentTrain: epoch  8, batch    25 | loss: 4.0783544CurrentTrain: epoch  8, batch    26 | loss: 4.0644517CurrentTrain: epoch  8, batch    27 | loss: 4.1079750CurrentTrain: epoch  8, batch    28 | loss: 4.0924873CurrentTrain: epoch  8, batch    29 | loss: 4.0743232CurrentTrain: epoch  8, batch    30 | loss: 4.0647502CurrentTrain: epoch  8, batch    31 | loss: 4.0496244CurrentTrain: epoch  8, batch    32 | loss: 4.0861073CurrentTrain: epoch  8, batch    33 | loss: 4.0805988CurrentTrain: epoch  8, batch    34 | loss: 4.0974984CurrentTrain: epoch  8, batch    35 | loss: 4.0713406CurrentTrain: epoch  8, batch    36 | loss: 4.0820394CurrentTrain: epoch  8, batch    37 | loss: 4.0703039CurrentTrain: epoch  8, batch    38 | loss: 4.0536284CurrentTrain: epoch  8, batch    39 | loss: 4.0852070CurrentTrain: epoch  8, batch    40 | loss: 4.0639081CurrentTrain: epoch  8, batch    41 | loss: 4.4067688CurrentTrain: epoch  8, batch    42 | loss: 4.0455756CurrentTrain: epoch  8, batch    43 | loss: 4.0706520CurrentTrain: epoch  8, batch    44 | loss: 4.0873914CurrentTrain: epoch  8, batch    45 | loss: 4.1658125CurrentTrain: epoch  8, batch    46 | loss: 4.1034098CurrentTrain: epoch  8, batch    47 | loss: 4.1061697CurrentTrain: epoch  8, batch    48 | loss: 4.0881147CurrentTrain: epoch  8, batch    49 | loss: 4.0711222CurrentTrain: epoch  8, batch    50 | loss: 4.0482736CurrentTrain: epoch  8, batch    51 | loss: 4.0732179CurrentTrain: epoch  8, batch    52 | loss: 4.0536041CurrentTrain: epoch  8, batch    53 | loss: 4.0700755CurrentTrain: epoch  8, batch    54 | loss: 4.0719433CurrentTrain: epoch  8, batch    55 | loss: 4.0612750CurrentTrain: epoch  8, batch    56 | loss: 4.0232401CurrentTrain: epoch  8, batch    57 | loss: 4.0494976CurrentTrain: epoch  8, batch    58 | loss: 4.0676832CurrentTrain: epoch  8, batch    59 | loss: 4.1198330CurrentTrain: epoch  8, batch    60 | loss: 4.0596371CurrentTrain: epoch  8, batch    61 | loss: 4.0560021CurrentTrain: epoch  8, batch    62 | loss: 4.0910730CurrentTrain: epoch  9, batch     0 | loss: 4.1280947CurrentTrain: epoch  9, batch     1 | loss: 4.0780363CurrentTrain: epoch  9, batch     2 | loss: 4.0328398CurrentTrain: epoch  9, batch     3 | loss: 4.0170112CurrentTrain: epoch  9, batch     4 | loss: 4.0578289CurrentTrain: epoch  9, batch     5 | loss: 4.0599127CurrentTrain: epoch  9, batch     6 | loss: 4.0447617CurrentTrain: epoch  9, batch     7 | loss: 4.1088829CurrentTrain: epoch  9, batch     8 | loss: 4.0467539CurrentTrain: epoch  9, batch     9 | loss: 4.0754585CurrentTrain: epoch  9, batch    10 | loss: 4.0628672CurrentTrain: epoch  9, batch    11 | loss: 4.0351973CurrentTrain: epoch  9, batch    12 | loss: 4.0790787CurrentTrain: epoch  9, batch    13 | loss: 4.1029682CurrentTrain: epoch  9, batch    14 | loss: 4.0730219CurrentTrain: epoch  9, batch    15 | loss: 4.0310135CurrentTrain: epoch  9, batch    16 | loss: 4.0650387CurrentTrain: epoch  9, batch    17 | loss: 4.3548994CurrentTrain: epoch  9, batch    18 | loss: 4.0946584CurrentTrain: epoch  9, batch    19 | loss: 4.0468769CurrentTrain: epoch  9, batch    20 | loss: 4.1648083CurrentTrain: epoch  9, batch    21 | loss: 4.0685883CurrentTrain: epoch  9, batch    22 | loss: 4.0317202CurrentTrain: epoch  9, batch    23 | loss: 4.0599823CurrentTrain: epoch  9, batch    24 | loss: 4.0695624CurrentTrain: epoch  9, batch    25 | loss: 4.0857372CurrentTrain: epoch  9, batch    26 | loss: 4.0360875CurrentTrain: epoch  9, batch    27 | loss: 4.0314069CurrentTrain: epoch  9, batch    28 | loss: 4.0782228CurrentTrain: epoch  9, batch    29 | loss: 4.0465031CurrentTrain: epoch  9, batch    30 | loss: 4.0623903CurrentTrain: epoch  9, batch    31 | loss: 4.0959110CurrentTrain: epoch  9, batch    32 | loss: 4.0720968CurrentTrain: epoch  9, batch    33 | loss: 4.0551329CurrentTrain: epoch  9, batch    34 | loss: 4.1079183CurrentTrain: epoch  9, batch    35 | loss: 4.0253458CurrentTrain: epoch  9, batch    36 | loss: 4.0659108CurrentTrain: epoch  9, batch    37 | loss: 4.1231499CurrentTrain: epoch  9, batch    38 | loss: 3.9706440CurrentTrain: epoch  9, batch    39 | loss: 4.0081553CurrentTrain: epoch  9, batch    40 | loss: 4.0945053CurrentTrain: epoch  9, batch    41 | loss: 4.0690365CurrentTrain: epoch  9, batch    42 | loss: 4.1305213CurrentTrain: epoch  9, batch    43 | loss: 4.0544872CurrentTrain: epoch  9, batch    44 | loss: 4.0510955CurrentTrain: epoch  9, batch    45 | loss: 4.0752978CurrentTrain: epoch  9, batch    46 | loss: 4.0643692CurrentTrain: epoch  9, batch    47 | loss: 4.0746832CurrentTrain: epoch  9, batch    48 | loss: 4.0554657CurrentTrain: epoch  9, batch    49 | loss: 4.0631976CurrentTrain: epoch  9, batch    50 | loss: 4.0628271CurrentTrain: epoch  9, batch    51 | loss: 4.0392108CurrentTrain: epoch  9, batch    52 | loss: 4.0771990CurrentTrain: epoch  9, batch    53 | loss: 4.0560203CurrentTrain: epoch  9, batch    54 | loss: 4.0234680CurrentTrain: epoch  9, batch    55 | loss: 4.0500603CurrentTrain: epoch  9, batch    56 | loss: 4.0492225CurrentTrain: epoch  9, batch    57 | loss: 4.0403166CurrentTrain: epoch  9, batch    58 | loss: 4.0370760CurrentTrain: epoch  9, batch    59 | loss: 4.0347786CurrentTrain: epoch  9, batch    60 | loss: 4.0762630CurrentTrain: epoch  9, batch    61 | loss: 4.0505691CurrentTrain: epoch  9, batch    62 | loss: 4.0673161
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: in 1874 , janáček became friends with antonín dvořák , and began composing in a relatively traditional romantic style .
Head Entity: antonín dvořák
Tail Entity: romantic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: The Impressionist movement, which began in the late 19th century, was significantly influenced by the works of Claude Monet and his contemporaries.  
Head Entity: Claude Monet  
Tail Entity: Impressionist  

Relation: movement  
Context: In the early 20th century, the Dada movement emerged as a reaction to the horrors of World War I, with artists like Marcel Duchamp leading the charge.  
Head Entity: Marcel Duchamp  
Tail Entity: Dada  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: guo was born in xuanhua county , zhangjiakou , hebei in july 1971 .
Head Entity: zhangjiakou
Tail Entity: hebei
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: the city of san francisco is situated in the state of california, known for its iconic golden gate bridge.  
Head Entity: san francisco  
Tail Entity: california  

Relation: located in the administrative territorial entity  
Context: the university of toronto is located in the city of toronto, which is part of the province of ontario, canada.  
Head Entity: toronto  
Tail Entity: ontario  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the nation as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: he also went on a road trip from san antonio to oakland for the magnum project “ postcards from america ” with jim goldberg , alec soth , mikhael subotzky and susan meiselas .
Head Entity: jim goldberg
Tail Entity: magnum
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: After years of dedication and hard work, she was finally recognized as a key member of the prestigious art collective known as the "Creative Minds."  
Head Entity: she  
Tail Entity: Creative Minds  

Relation: member of  
Context: The renowned guitarist joined the band in 2015, contributing to their unique sound and becoming an integral member of the musical group "The Echoes."  
Head Entity: guitarist  
Tail Entity: The Echoes  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: the tobin bridge , a major regional transportation artery , carries route 1 from chelsea across the mystic river to charlestown .
Head Entity: tobin bridge
Tail Entity: mystic river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: the golden gate bridge spans the golden gate strait, connecting san francisco to marin county and allowing vehicles to cross over the water.  
Head Entity: golden gate bridge  
Tail Entity: golden gate strait  

Relation: crosses  
Context: the subway tunnel runs beneath the city, allowing trains to cross under the busy streets and connect various neighborhoods.  
Head Entity: subway tunnel  
Tail Entity: city streets  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: the cucuveanu river is a tributary of the argova river in romania .
Head Entity: argova river
Tail Entity: cucuveanu river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: the missouri river is a major tributary of the mississippi river in the united states.  
Head Entity: mississippi river  
Tail Entity: missouri river  

Relation: tributary  
Context: the thames river serves as a tributary to the river medway in southeastern england.  
Head Entity: river medway  
Tail Entity: thames river  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: sir arthur stanley , the governor of victoria , and sir edmund barton , a justice of the high court and a former prime minister of australia , were also in attendance .
Head Entity: sir edmund barton
Tail Entity: prime minister of australia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: after serving as the mayor of new york city, bill de blasio was elected as the 109th mayor of the city, succeeding michael bloomberg.  
Head Entity: bill de blasio  
Tail Entity: mayor of new york city  

Relation: position held  
Context: during her tenure, angela merkel served as the chancellor of germany, leading the country through numerous crises and becoming one of the world's most powerful women.  
Head Entity: angela merkel  
Tail Entity: chancellor of germany  
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: " jedi outcast " was developed by raven software and powered by the i d tech 3 game engine .
Head Entity: jedi outcast
Tail Entity: raven software
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: "The Witcher 3: Wild Hunt" was developed by CD Projekt Red and has received numerous awards for its storytelling and gameplay.  
Head Entity: The Witcher 3: Wild Hunt  
Tail Entity: CD Projekt Red  

Relation: developer  
Context: "Minecraft" was developed by Mojang Studios and has become one of the best-selling video games of all time.  
Head Entity: Minecraft  
Tail Entity: Mojang Studios  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: neptune is the second studio album by the london - based band the duke spirit and the last with the original lead guitarist , dan higgins .
Head Entity: the duke spirit
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the tech startup was founded in silicon valley by a group of innovative engineers and entrepreneurs.  
Head Entity: the tech startup  
Tail Entity: silicon valley  

Relation: location of formation  
Context: the famous rock band was established in new york city, where they quickly gained a following.  
Head Entity: the famous rock band  
Tail Entity: new york city  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: during the same month , the band supported kings of leon 's tour of the united states , and coldplay 's tour of the united kingdom .
Head Entity: kings of leon
Tail Entity: united states
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous Italian dish, pizza, has gained popularity worldwide, but its roots can be traced back to Naples, Italy.  
Head Entity: pizza  
Tail Entity: Italy  

Relation: country of origin  
Context: The iconic brand, Rolex, is renowned for its luxury watches, which are crafted in Switzerland, known for its precision and quality in watchmaking.  
Head Entity: Rolex  
Tail Entity: Switzerland  
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.65%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.09%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 93.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.02%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.01%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.99%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.98%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 94.20%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.18%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.56%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.73%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.04%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 94.97%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.10%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.07%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.19%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.43%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.64%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.74%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 95.69%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.96%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.79%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.75%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 95.68%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.65%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 95.47%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.55%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.52%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 95.49%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.46%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.74%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.65%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.09%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 93.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.02%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.01%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.99%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.98%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 94.20%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.18%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.56%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.73%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.04%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 94.97%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.10%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.07%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.19%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.43%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.64%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.74%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 95.69%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.96%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.79%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.75%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 95.68%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.65%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 95.47%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.55%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.52%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 95.49%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.46%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.74%   
cur_acc:  ['0.9474']
his_acc:  ['0.9474']
CurrentTrain: epoch  0, batch     0 | loss: 6.9312401CurrentTrain: epoch  0, batch     1 | loss: 6.3583331CurrentTrain: epoch  0, batch     2 | loss: 5.7382846CurrentTrain: epoch  0, batch     3 | loss: 5.4255152CurrentTrain: epoch  1, batch     0 | loss: 6.5463080CurrentTrain: epoch  1, batch     1 | loss: 5.5049410CurrentTrain: epoch  1, batch     2 | loss: 5.0311370CurrentTrain: epoch  1, batch     3 | loss: 2.5477853CurrentTrain: epoch  2, batch     0 | loss: 4.9564080CurrentTrain: epoch  2, batch     1 | loss: 5.3451695CurrentTrain: epoch  2, batch     2 | loss: 3.8275187CurrentTrain: epoch  2, batch     3 | loss: 4.7866859CurrentTrain: epoch  3, batch     0 | loss: 4.5532179CurrentTrain: epoch  3, batch     1 | loss: 4.1583767CurrentTrain: epoch  3, batch     2 | loss: 4.0656939CurrentTrain: epoch  3, batch     3 | loss: 3.3862386CurrentTrain: epoch  4, batch     0 | loss: 3.5757720CurrentTrain: epoch  4, batch     1 | loss: 3.7165279CurrentTrain: epoch  4, batch     2 | loss: 4.1740627CurrentTrain: epoch  4, batch     3 | loss: 3.3496485CurrentTrain: epoch  5, batch     0 | loss: 3.8002968CurrentTrain: epoch  5, batch     1 | loss: 3.1489561CurrentTrain: epoch  5, batch     2 | loss: 3.4332728CurrentTrain: epoch  5, batch     3 | loss: 4.9560938CurrentTrain: epoch  6, batch     0 | loss: 3.9993639CurrentTrain: epoch  6, batch     1 | loss: 3.2874713CurrentTrain: epoch  6, batch     2 | loss: 3.0425377CurrentTrain: epoch  6, batch     3 | loss: 3.4019604CurrentTrain: epoch  7, batch     0 | loss: 2.9125724CurrentTrain: epoch  7, batch     1 | loss: 3.4735479CurrentTrain: epoch  7, batch     2 | loss: 3.2434099CurrentTrain: epoch  7, batch     3 | loss: 3.1444087CurrentTrain: epoch  8, batch     0 | loss: 3.0627279CurrentTrain: epoch  8, batch     1 | loss: 3.2768397CurrentTrain: epoch  8, batch     2 | loss: 2.6400051CurrentTrain: epoch  8, batch     3 | loss: 1.8748622CurrentTrain: epoch  9, batch     0 | loss: 2.7670147CurrentTrain: epoch  9, batch     1 | loss: 2.6882076CurrentTrain: epoch  9, batch     2 | loss: 2.5459964CurrentTrain: epoch  9, batch     3 | loss: 3.1930909
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: rohan is the son of narayana murthy , founder of infosys , and his wife sudha murthy , an engineer and author of note .
Head Entity: narayana murthy
Tail Entity: sudha murthy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: elon musk, the CEO of spacex and tesla, was previously married to talulah riley, a talented actress and writer.  
Head Entity: elon musk  
Tail Entity: talulah riley  

Relation: spouse  
Context: barack obama served as the 44th president of the united states, and his wife michelle obama is a renowned lawyer and author.  
Head Entity: barack obama  
Tail Entity: michelle obama  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: they briefly reformed in 1992 , when sub pop re - released " primal rock therapy " on cd with five additional unreleased tracks .
Head Entity: primal rock therapy
Tail Entity: sub pop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: The album "Future Nostalgia" was released under the Warner Records label, showcasing Dua Lipa's unique sound and style.  
Head Entity: Future Nostalgia  
Tail Entity: Warner Records  

Relation: record label  
Context: After signing with Columbia Records, the band released their highly anticipated debut album, which quickly climbed the charts.  
Head Entity: debut album  
Tail Entity: Columbia Records  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: " the many adventures of winnie the pooh " is a 1977 american animated buddy musical comedy film produced by walt disney productions and distributed by buena vista distribution .
Head Entity: the many adventures of winnie the pooh
Tail Entity: buena vista distribution
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: "Inception" is a 2010 science fiction film directed by Christopher Nolan and distributed by Warner Bros. Pictures.  
Head Entity: Inception  
Tail Entity: Warner Bros. Pictures  

Relation: distributor  
Context: "The Dark Knight" is a 2008 superhero film directed by Christopher Nolan and distributed by Warner Bros. Pictures.  
Head Entity: The Dark Knight  
Tail Entity: Warner Bros. Pictures  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: in 1796 carondelet commissioned pintado to conduct a survey of lands granted to ambrosio longue - épée in iberville county on the west bank of the mississippi river opposite baton rouge .
Head Entity: baton rouge
Tail Entity: mississippi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: the city of new orleans is situated on the banks of the mississippi river, making it a vibrant hub for trade and culture.  
Head Entity: new orleans  
Tail Entity: mississippi river  

Relation: located in or next to body of water  
Context: the picturesque village of lake placid is nestled beside the serene waters of lake placid, attracting tourists year-round.  
Head Entity: lake placid  
Tail Entity: lake placid  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: cbs and viacom split again in 2005 , but both are still owned by national amusements .
Head Entity: national amusements
Tail Entity: viacom
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Sample 1:  
Relation: subsidiary  
Context: In 2018, Facebook acquired WhatsApp, making it a subsidiary of the social media giant.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Sample 2:  
Relation: subsidiary  
Context: Toyota has several subsidiaries, including Lexus, which focuses on luxury vehicles.  
Head Entity: Toyota  
Tail Entity: Lexus  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: anders morelius from sweden placed third in the individual race .
Head Entity: anders morelius
Tail Entity: sweden
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: maria gonzalez was born in mexico but later moved to the united states.  
Head Entity: maria gonzalez  
Tail Entity: mexico  

Relation: country of citizenship  
Context: after living in canada for several years, liu wei decided to apply for citizenship.  
Head Entity: liu wei  
Tail Entity: canada  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: buck moved to austin in the mid-1970s and joined the blues rock group , the fabulous thunderbirds , along with keith ferguson , jimmie vaughan , and kim wilson .
Head Entity: the fabulous thunderbirds
Tail Entity: blues rock
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the album "thriller" by michael jackson is widely regarded as a landmark in pop music history, blending elements of rock, funk, and soul.  
Head Entity: michael jackson  
Tail Entity: pop  

Relation: genre  
Context: the film "inception," directed by christopher nolan, is a complex narrative that combines science fiction with psychological thriller elements.  
Head Entity: inception  
Tail Entity: science fiction  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: san lázaro is 660 km far from asunción and is located at the exact site of the confluence of the apa river with paraguay river , bordering brazil - chaco .
Head Entity: apa river
Tail Entity: paraguay river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: the mississippi river flows into the gulf of mexico, creating a rich delta ecosystem that supports diverse wildlife and plant species.  
Head Entity: mississippi river  
Tail Entity: gulf of mexico  

Relation: mouth of the watercourse  
Context: the rhine river, known for its scenic beauty, empties into the north sea, providing a vital shipping route for trade in europe.  
Head Entity: rhine river  
Tail Entity: north sea  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: bradley halliday ( born 10 july 1995 ) is an english professional footballer who plays as a defender or a midfielder for league two club cambridge united .
Head Entity: bradley halliday
Tail Entity: defender
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: alex morgan (born 2 july 1989) is an american soccer player who plays as a forward for the national team and orlando pride in the national women's soccer league.  
Head Entity: alex morgan  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: lebron james (born 30 december 1984) is an american professional basketball player who plays as a small forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: small forward  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: the stade louis ii in monaco was the venue for the uefa super cup every year since 1998 .
Head Entity: 1998
Tail Entity: uefa super cup
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2020 NBA season was significantly impacted by the COVID-19 pandemic, leading to a unique playoff format.  
Head Entity: 2020  
Tail Entity: NBA  

Relation: sports season of league or competition  
Context: The 2021 Formula 1 season introduced new regulations and saw Lewis Hamilton competing for his eighth championship title.  
Head Entity: 2021  
Tail Entity: Formula 1  
Mixup data size:  199
MixupTrain:  epoch  0, batch     0 | loss: 6.0699999MixupTrain:  epoch  0, batch     1 | loss: 5.7199922MixupTrain:  epoch  0, batch     2 | loss: 6.1452270MixupTrain:  epoch  0, batch     3 | loss: 5.1731105MixupTrain:  epoch  0, batch     4 | loss: 5.1190084MixupTrain:  epoch  0, batch     5 | loss: 4.7599122MixupTrain:  epoch  0, batch     6 | loss: 4.8088747MixupTrain:  epoch  0, batch     7 | loss: 4.8557261MixupTrain:  epoch  0, batch     8 | loss: 5.0688386MixupTrain:  epoch  0, batch     9 | loss: 5.1565173MixupTrain:  epoch  0, batch    10 | loss: 5.0557623MixupTrain:  epoch  0, batch    11 | loss: 4.6016554MixupTrain:  epoch  0, batch    12 | loss: 3.8209212
MemoryTrain:  epoch  0, batch     0 | loss: 4.3709917MemoryTrain:  epoch  0, batch     1 | loss: 3.8112221MemoryTrain:  epoch  0, batch     2 | loss: 4.1498117MemoryTrain:  epoch  0, batch     3 | loss: 4.5068541MemoryTrain:  epoch  1, batch     0 | loss: 3.7214847MemoryTrain:  epoch  1, batch     1 | loss: 4.4110041MemoryTrain:  epoch  1, batch     2 | loss: 3.6180677MemoryTrain:  epoch  1, batch     3 | loss: 2.8430781MemoryTrain:  epoch  2, batch     0 | loss: 3.4374399MemoryTrain:  epoch  2, batch     1 | loss: 3.0897989MemoryTrain:  epoch  2, batch     2 | loss: 3.0848060MemoryTrain:  epoch  2, batch     3 | loss: 3.0622706MemoryTrain:  epoch  3, batch     0 | loss: 3.0269499MemoryTrain:  epoch  3, batch     1 | loss: 2.5462613MemoryTrain:  epoch  3, batch     2 | loss: 2.9066870MemoryTrain:  epoch  3, batch     3 | loss: 2.5260847MemoryTrain:  epoch  4, batch     0 | loss: 3.2137976MemoryTrain:  epoch  4, batch     1 | loss: 2.3613429MemoryTrain:  epoch  4, batch     2 | loss: 2.1085110MemoryTrain:  epoch  4, batch     3 | loss: 2.3394363MemoryTrain:  epoch  5, batch     0 | loss: 2.3323882MemoryTrain:  epoch  5, batch     1 | loss: 2.4889834MemoryTrain:  epoch  5, batch     2 | loss: 1.8636444MemoryTrain:  epoch  5, batch     3 | loss: 2.8830194MemoryTrain:  epoch  6, batch     0 | loss: 1.8393302MemoryTrain:  epoch  6, batch     1 | loss: 2.1752484MemoryTrain:  epoch  6, batch     2 | loss: 2.3370166MemoryTrain:  epoch  6, batch     3 | loss: 1.6005390MemoryTrain:  epoch  7, batch     0 | loss: 1.7542636MemoryTrain:  epoch  7, batch     1 | loss: 1.5887185MemoryTrain:  epoch  7, batch     2 | loss: 2.1058950MemoryTrain:  epoch  7, batch     3 | loss: 1.9053725MemoryTrain:  epoch  8, batch     0 | loss: 1.8910441MemoryTrain:  epoch  8, batch     1 | loss: 1.9276557MemoryTrain:  epoch  8, batch     2 | loss: 1.6682682MemoryTrain:  epoch  8, batch     3 | loss: 2.0969539MemoryTrain:  epoch  9, batch     0 | loss: 1.8011156MemoryTrain:  epoch  9, batch     1 | loss: 1.4180524MemoryTrain:  epoch  9, batch     2 | loss: 1.5582370MemoryTrain:  epoch  9, batch     3 | loss: 1.6138968
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 94.53%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 93.06%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 86.72%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 86.03%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 85.76%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 86.51%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 86.88%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 86.61%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 87.22%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 87.23%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 87.24%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 87.74%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 87.72%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 86.46%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 86.09%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 85.55%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 85.04%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 84.74%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 84.11%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 83.51%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 82.94%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 82.57%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 82.37%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 82.19%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 81.86%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 81.55%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 81.69%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   44 | acc: 18.75%,  total acc: 79.86%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 79.21%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 78.32%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 77.99%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 76.91%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 76.62%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 77.52%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 77.95%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 78.24%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 78.52%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 79.28%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 79.63%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 79.98%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 80.31%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 80.64%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 80.95%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 80.46%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.45%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 89.71%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.93%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.46%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 90.31%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 90.48%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 90.06%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 89.67%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 89.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.90%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 90.05%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 90.40%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 90.73%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.33%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 91.41%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 91.54%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 91.43%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 91.15%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 91.39%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 91.45%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.07%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.26%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 92.44%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 92.61%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 92.66%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 92.55%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 92.58%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.73%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 92.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.89%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 92.79%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 92.81%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 92.82%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 92.61%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.63%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 92.67%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 92.80%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 92.81%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 92.83%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 92.74%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 92.77%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 92.88%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 92.99%   [EVAL] batch:   66 | acc: 87.50%,  total acc: 92.91%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 92.92%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 93.03%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 93.04%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 92.96%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 92.80%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 92.64%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 92.31%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 92.17%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 91.80%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 91.67%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 91.46%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 91.33%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 91.28%   [EVAL] batch:   81 | acc: 93.75%,  total acc: 91.31%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 91.27%   [EVAL] batch:   83 | acc: 93.75%,  total acc: 91.29%   [EVAL] batch:   84 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 91.35%   [EVAL] batch:   86 | acc: 81.25%,  total acc: 91.24%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 91.26%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 91.29%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 91.11%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 91.14%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 90.83%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 90.66%   [EVAL] batch:   93 | acc: 62.50%,  total acc: 90.36%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 90.20%   [EVAL] batch:   95 | acc: 75.00%,  total acc: 90.04%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 89.76%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 89.48%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 89.27%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 89.06%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 88.80%   [EVAL] batch:  101 | acc: 81.25%,  total acc: 88.73%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 88.53%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 88.34%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 88.21%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 88.15%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 87.68%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 87.21%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 86.75%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 86.48%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 86.04%   [EVAL] batch:  111 | acc: 56.25%,  total acc: 85.77%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 85.67%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 85.80%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 85.92%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 85.99%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 86.18%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 86.29%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 86.41%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 86.52%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 86.63%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 86.74%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 86.84%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 86.95%   
cur_acc:  ['0.9474', '0.8046']
his_acc:  ['0.9474', '0.8695']
CurrentTrain: epoch  0, batch     0 | loss: 6.7056217CurrentTrain: epoch  0, batch     1 | loss: 6.4285316CurrentTrain: epoch  0, batch     2 | loss: 6.2694559CurrentTrain: epoch  0, batch     3 | loss: 6.2535324CurrentTrain: epoch  1, batch     0 | loss: 6.0292568CurrentTrain: epoch  1, batch     1 | loss: 5.3806963CurrentTrain: epoch  1, batch     2 | loss: 5.1447997CurrentTrain: epoch  1, batch     3 | loss: 3.7998376CurrentTrain: epoch  2, batch     0 | loss: 5.5404549CurrentTrain: epoch  2, batch     1 | loss: 5.1958246CurrentTrain: epoch  2, batch     2 | loss: 4.6861992CurrentTrain: epoch  2, batch     3 | loss: 7.3829737CurrentTrain: epoch  3, batch     0 | loss: 5.0022764CurrentTrain: epoch  3, batch     1 | loss: 4.6670790CurrentTrain: epoch  3, batch     2 | loss: 4.6864581CurrentTrain: epoch  3, batch     3 | loss: 5.2718019CurrentTrain: epoch  4, batch     0 | loss: 4.2599235CurrentTrain: epoch  4, batch     1 | loss: 4.7117605CurrentTrain: epoch  4, batch     2 | loss: 4.3991013CurrentTrain: epoch  4, batch     3 | loss: 3.2736588CurrentTrain: epoch  5, batch     0 | loss: 4.4133630CurrentTrain: epoch  5, batch     1 | loss: 4.2172117CurrentTrain: epoch  5, batch     2 | loss: 3.7117562CurrentTrain: epoch  5, batch     3 | loss: 2.7969561CurrentTrain: epoch  6, batch     0 | loss: 3.3374529CurrentTrain: epoch  6, batch     1 | loss: 3.9751992CurrentTrain: epoch  6, batch     2 | loss: 4.1972294CurrentTrain: epoch  6, batch     3 | loss: 4.4002547CurrentTrain: epoch  7, batch     0 | loss: 3.0288520CurrentTrain: epoch  7, batch     1 | loss: 4.1609335CurrentTrain: epoch  7, batch     2 | loss: 3.7474370CurrentTrain: epoch  7, batch     3 | loss: 6.2298326CurrentTrain: epoch  8, batch     0 | loss: 3.5324800CurrentTrain: epoch  8, batch     1 | loss: 3.9104743CurrentTrain: epoch  8, batch     2 | loss: 3.3714046CurrentTrain: epoch  8, batch     3 | loss: 2.1994989CurrentTrain: epoch  9, batch     0 | loss: 3.7027931CurrentTrain: epoch  9, batch     1 | loss: 3.1690552CurrentTrain: epoch  9, batch     2 | loss: 3.1938310CurrentTrain: epoch  9, batch     3 | loss: 4.2901974
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: auerbach is prominently featured in the documentary film , " the first basket " , about jewish basketball history .
Head Entity: the first basket
Tail Entity: basketball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: the book "sapiens" explores the history of humankind from the stone age to the present day.  
Head Entity: sapiens  
Tail Entity: humankind  

Relation: main subject  
Context: the documentary "our planet" showcases the beauty of the natural world and the impact of climate change.  
Head Entity: our planet  
Tail Entity: natural world  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: group b of uefa euro 2016 contained england , russia , wales and slovakia .
Head Entity: uefa euro 2016
Tail Entity: slovakia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: the 2020 summer olympics featured countries like the united states, china, and japan competing in various sports.  
Head Entity: 2020 summer olympics  
Tail Entity: japan  

Relation: participating team  
Context: the fifa world cup 2018 saw teams such as france, croatia, and belgium battling for the championship title.  
Head Entity: fifa world cup 2018  
Tail Entity: croatia  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: among the competing artists was previous eurovision song contest entrant robin juhkental who represented estonia in 2010 as part of the group malcolm lincoln .
Head Entity: malcolm lincoln
Tail Entity: robin juhkental
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The famous painting "The Last Supper" features several apostles, including Peter, who is depicted prominently on the left side of the artwork.  
Head Entity: The Last Supper  
Tail Entity: Peter  

Relation: has part  
Context: The smartphone model Galaxy S21 comes with a high-resolution camera that includes a wide-angle lens, allowing users to capture stunning landscapes.  
Head Entity: Galaxy S21  
Tail Entity: wide-angle lens  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: he was nominated for the academy award for best picture , along with steven spielberg , ian bryce , mark gordon for the film " saving private ryan " .
Head Entity: ian bryce
Tail Entity: academy award for best picture
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film "The Shape of Water" received multiple nominations, including a nod for best director for Guillermo del Toro at the Academy Awards.  
Head Entity: Guillermo del Toro  
Tail Entity: Academy Awards  

Relation: nominated for  
Context: The popular band was nominated for the Grammy Award for Best New Artist after their debut album topped the charts.  
Head Entity: the popular band  
Tail Entity: Grammy Award for Best New Artist  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: in 2007 , the county municipality created the transit authority called " skyss " that would market public transport while is would be operated by private companies based on public service obligation contracts .
Head Entity: skyss
Tail Entity: transit authority
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone known as the iPhone has revolutionized the way we communicate and interact with technology, becoming a benchmark for mobile devices worldwide.  
Head Entity: iPhone  
Tail Entity: smartphone  

Relation: instance of  
Context: The Great Wall of China is a remarkable feat of engineering and a symbol of Chinese history, attracting millions of tourists each year.  
Head Entity: Great Wall of China  
Tail Entity: engineering marvel  
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: the valley is confined by the zillertal alps in the northeast and the sarntal alps in the southwest , rising up to the tagewaldhorn peak at .
Head Entity: tagewaldhorn
Tail Entity: sarntal alps
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the majestic peaks of the rocky mountains stretch across several states, with the highest point being mount elbert, which towers over the surrounding landscape.  
Head Entity: mount elbert  
Tail Entity: rocky mountains  

Relation: mountain range  
Context: the andes mountains run along the western edge of south america, with the iconic aconcagua standing as the tallest peak in the range.  
Head Entity: aconcagua  
Tail Entity: andes mountains  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: future films include " mcqueen " , ian bonhote ’s documentary about the fashion designer alexander mcqueen , brad anderson ’s thriller " beirut " , and mark pellington ’s drama " nostalgia " .
Head Entity: nostalgia
Tail Entity: mark pellington
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: the upcoming movie "the great adventure" is penned by renowned screenwriter jessica taylor, who has previously won multiple awards for her work in the industry.  
Head Entity: the great adventure  
Tail Entity: jessica taylor  

Relation: screenwriter  
Context: in the latest film festival, "shadows of the past" received critical acclaim, with its screenplay written by the talented screenwriter robert king.  
Head Entity: shadows of the past  
Tail Entity: robert king  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: seven of the songs are taken from the original danish album " gasolin ' 3 " ( released a few months before ) .
Head Entity: gasolin ' 3
Tail Entity: danish
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The film "Amélie" is primarily in French and has gained international acclaim for its unique storytelling.  
Head Entity: Amélie  
Tail Entity: French  

Relation: language of work or name  
Context: The novel "One Hundred Years of Solitude" is originally written in Spanish and is considered a masterpiece of magical realism.  
Head Entity: One Hundred Years of Solitude  
Tail Entity: Spanish  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: the aircraft was a boeing 707 - 321c that first entered service with pan american world airways ( pan am ) in 1963 .
Head Entity: boeing 707 - 321c
Tail Entity: pan american world airways
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: the new electric bus fleet is managed by the city transportation authority, which ensures efficient public transit services.  
Head Entity: electric bus fleet  
Tail Entity: city transportation authority  

Relation: operator  
Context: the state-of-the-art research facility is operated by the national science foundation, focusing on advanced materials.  
Head Entity: research facility  
Tail Entity: national science foundation  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: gogukwon 's successor , sosurim , adopted a foreign policy of appeasement and reconciliation with baekje , and concentrated on domestic policies to spread buddhism throughout goguryeo 's social and political systems .
Head Entity: goguryeo
Tail Entity: buddhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: The ancient temple of Angkor Wat, originally constructed as a Hindu temple dedicated to the god Vishnu, later became a Buddhist site and is now a symbol of Cambodia's religious heritage.  
Head Entity: Angkor Wat  
Tail Entity: Buddhism  

Relation: religion  
Context: The community center hosts weekly gatherings where members discuss the teachings of Islam and engage in various cultural activities that promote understanding and tolerance among different faiths.  
Head Entity: community center  
Tail Entity: Islam  
Mixup data size:  259
MixupTrain:  epoch  0, batch     0 | loss: 3.4904080MixupTrain:  epoch  0, batch     1 | loss: 3.4324257MixupTrain:  epoch  0, batch     2 | loss: 3.4852339MixupTrain:  epoch  0, batch     3 | loss: 3.5847816MixupTrain:  epoch  0, batch     4 | loss: 3.5292547MixupTrain:  epoch  0, batch     5 | loss: 2.9048375MixupTrain:  epoch  0, batch     6 | loss: 2.9556488MixupTrain:  epoch  0, batch     7 | loss: 3.1369817MixupTrain:  epoch  0, batch     8 | loss: 3.0415630MixupTrain:  epoch  0, batch     9 | loss: 2.9667136MixupTrain:  epoch  0, batch    10 | loss: 3.0033229MixupTrain:  epoch  0, batch    11 | loss: 3.0144084MixupTrain:  epoch  0, batch    12 | loss: 2.6749971MixupTrain:  epoch  0, batch    13 | loss: 2.7223113MixupTrain:  epoch  0, batch    14 | loss: 2.5096617MixupTrain:  epoch  0, batch    15 | loss: 2.8295072MixupTrain:  epoch  0, batch    16 | loss: 2.6066380
MemoryTrain:  epoch  0, batch     0 | loss: 2.6470721MemoryTrain:  epoch  0, batch     1 | loss: 2.3182878MemoryTrain:  epoch  0, batch     2 | loss: 2.1065490MemoryTrain:  epoch  0, batch     3 | loss: 3.5810132MemoryTrain:  epoch  0, batch     4 | loss: 3.2317421MemoryTrain:  epoch  0, batch     5 | loss: 3.1942632MemoryTrain:  epoch  1, batch     0 | loss: 3.0336618MemoryTrain:  epoch  1, batch     1 | loss: 2.4613185MemoryTrain:  epoch  1, batch     2 | loss: 2.2299612MemoryTrain:  epoch  1, batch     3 | loss: 2.5407319MemoryTrain:  epoch  1, batch     4 | loss: 1.9353081MemoryTrain:  epoch  1, batch     5 | loss: 1.7820896MemoryTrain:  epoch  2, batch     0 | loss: 1.8716981MemoryTrain:  epoch  2, batch     1 | loss: 2.3137648MemoryTrain:  epoch  2, batch     2 | loss: 1.5576735MemoryTrain:  epoch  2, batch     3 | loss: 2.2093482MemoryTrain:  epoch  2, batch     4 | loss: 2.0826783MemoryTrain:  epoch  2, batch     5 | loss: 2.5727024MemoryTrain:  epoch  3, batch     0 | loss: 1.9256325MemoryTrain:  epoch  3, batch     1 | loss: 1.8581717MemoryTrain:  epoch  3, batch     2 | loss: 2.0583427MemoryTrain:  epoch  3, batch     3 | loss: 1.9011594MemoryTrain:  epoch  3, batch     4 | loss: 1.5898328MemoryTrain:  epoch  3, batch     5 | loss: 2.0349162MemoryTrain:  epoch  4, batch     0 | loss: 1.8899736MemoryTrain:  epoch  4, batch     1 | loss: 2.1507287MemoryTrain:  epoch  4, batch     2 | loss: 1.7219621MemoryTrain:  epoch  4, batch     3 | loss: 1.7036062MemoryTrain:  epoch  4, batch     4 | loss: 1.3858120MemoryTrain:  epoch  4, batch     5 | loss: 1.6924118MemoryTrain:  epoch  5, batch     0 | loss: 1.6919789MemoryTrain:  epoch  5, batch     1 | loss: 1.8025994MemoryTrain:  epoch  5, batch     2 | loss: 1.6378067MemoryTrain:  epoch  5, batch     3 | loss: 1.6483964MemoryTrain:  epoch  5, batch     4 | loss: 1.5030107MemoryTrain:  epoch  5, batch     5 | loss: 1.4735209MemoryTrain:  epoch  6, batch     0 | loss: 1.2816280MemoryTrain:  epoch  6, batch     1 | loss: 1.7861969MemoryTrain:  epoch  6, batch     2 | loss: 1.9294488MemoryTrain:  epoch  6, batch     3 | loss: 1.5108045MemoryTrain:  epoch  6, batch     4 | loss: 1.5350718MemoryTrain:  epoch  6, batch     5 | loss: 1.4194578MemoryTrain:  epoch  7, batch     0 | loss: 1.4614611MemoryTrain:  epoch  7, batch     1 | loss: 1.5889601MemoryTrain:  epoch  7, batch     2 | loss: 1.6378983MemoryTrain:  epoch  7, batch     3 | loss: 1.4377521MemoryTrain:  epoch  7, batch     4 | loss: 1.3641353MemoryTrain:  epoch  7, batch     5 | loss: 1.5850595MemoryTrain:  epoch  8, batch     0 | loss: 1.4931328MemoryTrain:  epoch  8, batch     1 | loss: 1.7413421MemoryTrain:  epoch  8, batch     2 | loss: 1.3381211MemoryTrain:  epoch  8, batch     3 | loss: 1.3307791MemoryTrain:  epoch  8, batch     4 | loss: 1.5020921MemoryTrain:  epoch  8, batch     5 | loss: 1.5909547MemoryTrain:  epoch  9, batch     0 | loss: 1.3889477MemoryTrain:  epoch  9, batch     1 | loss: 1.6046202MemoryTrain:  epoch  9, batch     2 | loss: 1.4041827MemoryTrain:  epoch  9, batch     3 | loss: 1.4626698MemoryTrain:  epoch  9, batch     4 | loss: 1.5131847MemoryTrain:  epoch  9, batch     5 | loss: 1.2955315
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 83.04%   [EVAL] batch:   14 | acc: 37.50%,  total acc: 80.00%   [EVAL] batch:   15 | acc: 31.25%,  total acc: 76.95%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 75.37%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 73.61%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 71.71%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 72.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 76.09%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 77.75%   [EVAL] batch:   25 | acc: 81.25%,  total acc: 77.88%   [EVAL] batch:   26 | acc: 43.75%,  total acc: 76.62%   [EVAL] batch:   27 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 76.94%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 76.67%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 76.41%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 76.95%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 77.46%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 78.82%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 79.22%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 79.77%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 80.29%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 80.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 81.70%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 82.12%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 82.53%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 82.92%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 83.29%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 83.64%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 83.46%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 83.80%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 84.12%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 84.19%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 84.13%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 84.32%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 84.65%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 84.48%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 84.64%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 84.32%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 83.97%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 83.23%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 82.39%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 81.77%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 82.21%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 85.29%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 85.76%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 86.51%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 86.31%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 86.36%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 86.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 87.04%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 87.72%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 88.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.51%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.87%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 88.60%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 88.21%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 87.85%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 87.84%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 87.99%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.30%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.59%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 88.72%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 88.99%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 88.95%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 89.03%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 88.99%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 88.96%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 89.03%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 89.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.22%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.30%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 89.39%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 89.47%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 89.32%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.40%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 89.36%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 88.90%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 88.88%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 88.75%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 88.63%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 88.41%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 88.28%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 88.46%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 88.62%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 88.69%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 88.86%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 89.02%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 89.17%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 89.15%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 89.13%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 89.10%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 89.17%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 89.23%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 89.04%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 89.02%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 88.84%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 88.91%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 88.97%   [EVAL] batch:   81 | acc: 93.75%,  total acc: 89.02%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 88.70%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 88.69%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 88.53%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 88.66%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 88.43%   [EVAL] batch:   87 | acc: 100.00%,  total acc: 88.57%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 88.62%   [EVAL] batch:   89 | acc: 87.50%,  total acc: 88.61%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 88.74%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 88.59%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 88.44%   [EVAL] batch:   93 | acc: 68.75%,  total acc: 88.23%   [EVAL] batch:   94 | acc: 62.50%,  total acc: 87.96%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 87.57%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 87.31%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 86.86%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 86.68%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 86.19%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 85.95%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 85.85%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 85.62%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 85.52%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 85.44%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 85.22%   [EVAL] batch:  107 | acc: 56.25%,  total acc: 84.95%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 84.69%   [EVAL] batch:  109 | acc: 62.50%,  total acc: 84.49%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 84.12%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 83.93%   [EVAL] batch:  112 | acc: 81.25%,  total acc: 83.90%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 84.05%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 84.18%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 84.32%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 84.46%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 84.59%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 84.84%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 84.92%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 85.04%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 85.11%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 85.35%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 85.27%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 85.14%   [EVAL] batch:  127 | acc: 87.50%,  total acc: 85.16%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 85.13%   [EVAL] batch:  129 | acc: 87.50%,  total acc: 85.14%   [EVAL] batch:  130 | acc: 87.50%,  total acc: 85.16%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 85.23%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 85.34%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 85.35%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 85.52%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 85.63%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 85.55%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 85.12%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 84.78%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 84.40%   [EVAL] batch:  141 | acc: 50.00%,  total acc: 84.15%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 83.87%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 83.55%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 83.62%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 83.73%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 83.80%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 83.91%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 84.02%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 84.08%   [EVAL] batch:  150 | acc: 81.25%,  total acc: 84.06%   [EVAL] batch:  151 | acc: 43.75%,  total acc: 83.80%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 83.74%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 83.77%   [EVAL] batch:  154 | acc: 68.75%,  total acc: 83.67%   [EVAL] batch:  155 | acc: 68.75%,  total acc: 83.57%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 83.64%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 83.70%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 83.81%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 83.87%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 83.89%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 83.95%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 84.05%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 84.15%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 84.24%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 84.34%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 84.43%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:  168 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 84.71%   [EVAL] batch:  170 | acc: 100.00%,  total acc: 84.80%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 84.88%   [EVAL] batch:  172 | acc: 75.00%,  total acc: 84.83%   [EVAL] batch:  173 | acc: 100.00%,  total acc: 84.91%   [EVAL] batch:  174 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:  175 | acc: 87.50%,  total acc: 85.01%   [EVAL] batch:  176 | acc: 81.25%,  total acc: 84.99%   [EVAL] batch:  177 | acc: 93.75%,  total acc: 85.04%   [EVAL] batch:  178 | acc: 87.50%,  total acc: 85.06%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 85.14%   [EVAL] batch:  180 | acc: 93.75%,  total acc: 85.19%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 85.13%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 85.08%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 85.12%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 85.03%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 85.01%   [EVAL] batch:  186 | acc: 62.50%,  total acc: 84.89%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 84.64%   
cur_acc:  ['0.9474', '0.8046', '0.8323']
his_acc:  ['0.9474', '0.8695', '0.8464']
CurrentTrain: epoch  0, batch     0 | loss: 4.4300323CurrentTrain: epoch  0, batch     1 | loss: 4.4476414CurrentTrain: epoch  0, batch     2 | loss: 5.6922174CurrentTrain: epoch  0, batch     3 | loss: 4.1307249CurrentTrain: epoch  1, batch     0 | loss: 4.1020513CurrentTrain: epoch  1, batch     1 | loss: 3.5400982CurrentTrain: epoch  1, batch     2 | loss: 3.5035794CurrentTrain: epoch  1, batch     3 | loss: 4.0654521CurrentTrain: epoch  2, batch     0 | loss: 3.1851513CurrentTrain: epoch  2, batch     1 | loss: 2.8854733CurrentTrain: epoch  2, batch     2 | loss: 2.7446897CurrentTrain: epoch  2, batch     3 | loss: 5.9010868CurrentTrain: epoch  3, batch     0 | loss: 3.2461593CurrentTrain: epoch  3, batch     1 | loss: 3.0571730CurrentTrain: epoch  3, batch     2 | loss: 2.6295929CurrentTrain: epoch  3, batch     3 | loss: 2.0570083CurrentTrain: epoch  4, batch     0 | loss: 3.1977239CurrentTrain: epoch  4, batch     1 | loss: 2.8719044CurrentTrain: epoch  4, batch     2 | loss: 2.2496562CurrentTrain: epoch  4, batch     3 | loss: 1.8855895CurrentTrain: epoch  5, batch     0 | loss: 2.4201679CurrentTrain: epoch  5, batch     1 | loss: 2.6740365CurrentTrain: epoch  5, batch     2 | loss: 2.3367732CurrentTrain: epoch  5, batch     3 | loss: 1.9686906CurrentTrain: epoch  6, batch     0 | loss: 2.1921408CurrentTrain: epoch  6, batch     1 | loss: 2.4786038CurrentTrain: epoch  6, batch     2 | loss: 2.6162133CurrentTrain: epoch  6, batch     3 | loss: 1.8898972CurrentTrain: epoch  7, batch     0 | loss: 2.2188530CurrentTrain: epoch  7, batch     1 | loss: 2.4481828CurrentTrain: epoch  7, batch     2 | loss: 2.0368686CurrentTrain: epoch  7, batch     3 | loss: 2.4397388CurrentTrain: epoch  8, batch     0 | loss: 2.1330504CurrentTrain: epoch  8, batch     1 | loss: 2.2392051CurrentTrain: epoch  8, batch     2 | loss: 2.0728288CurrentTrain: epoch  8, batch     3 | loss: 1.9123701CurrentTrain: epoch  9, batch     0 | loss: 2.0897832CurrentTrain: epoch  9, batch     1 | loss: 1.9991356CurrentTrain: epoch  9, batch     2 | loss: 2.1059155CurrentTrain: epoch  9, batch     3 | loss: 2.5812650
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: alongside evan durbin and hugh gaitskell , he brought the thinking of john maynard keynes to the labour party , especially in relation to price determination .
Head Entity: hugh gaitskell
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: After serving as the mayor, she became a prominent figure in the local chapter of the Green Party, advocating for environmental policies.  
Head Entity: she  
Tail Entity: Green Party  

Relation: member of political party  
Context: During his tenure in the Senate, he was known for his strong support of the Democratic Party's initiatives on healthcare reform.  
Head Entity: he  
Tail Entity: Democratic Party  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: the script for " the great santini " was adapted by carlino from the 1976 novel by pat conroy , with assistance from an un - credited herman raucher .
Head Entity: the great santini
Tail Entity: pat conroy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film "inception" draws heavily from the concepts presented in the 2001 novel "the dreamers" by j. k. rowling, which explores the nature of dreams and reality.  
Head Entity: inception  
Tail Entity: j. k. rowling  

Relation: after a work by  
Context: the musical "hamilton" was inspired by the biography "alexander hamilton" written by ron chernow, which details the life of the founding father.  
Head Entity: hamilton  
Tail Entity: ron chernow  
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: the group named itself after the popular children 's book harold and the purple crayon by crockett johnson , whose protagonist , harold , uses a purple crayon to draw his imagination into reality .
Head Entity: crockett johnson
Tail Entity: harold and the purple crayon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: The renowned physicist Albert Einstein is best known for his groundbreaking theory of relativity, which revolutionized our understanding of space, time, and gravity.  
Head Entity: Albert Einstein  
Tail Entity: theory of relativity  

Relation: notable work  
Context: The famous painter Vincent van Gogh created numerous masterpieces, but his most celebrated painting, "Starry Night," remains a significant influence in the art world.  
Head Entity: Vincent van Gogh  
Tail Entity: Starry Night  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: at the request of the stockholm institute for bible translation , mirza khazar translated the bible – the new testament and old testament – into the azerbaijani language .
Head Entity: institute for bible translation
Tail Entity: stockholm
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: the global technology company, headquartered in san francisco, has been a leader in software development for over two decades.  
Head Entity: global technology company  
Tail Entity: san francisco  

Relation: headquarters location  
Context: after relocating its main office to new york city, the fashion brand saw a significant increase in sales and brand recognition.  
Head Entity: fashion brand  
Tail Entity: new york city  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most recognized as the wolf, which is classified under the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of flowering plants, the family Rosaceae encompasses a variety of species, including roses and strawberries, which are categorized under the order Rosales.  
Head Entity: Rosaceae  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: he later collaborated with vito volterra and explored laplace transforms and other parts of functional analysis .
Head Entity: vito volterra
Tail Entity: functional analysis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: Marie Curie conducted pioneering research on radioactivity, which laid the groundwork for advancements in nuclear physics.  
Head Entity: Marie Curie  
Tail Entity: nuclear physics  

Relation: field of work  
Context: The company specializes in artificial intelligence and machine learning, providing innovative solutions for various industries.  
Head Entity: The company  
Tail Entity: artificial intelligence
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: since march 2006 , wkxp 's programming has been simulcasted on 97.3 wzad wurtsboro , new york .
Head Entity: wzad
Tail Entity: wurtsboro , new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: in 2010, the station wxyz began broadcasting to the community of springfield, providing local news and entertainment.  
Head Entity: wxyz  
Tail Entity: springfield  

Relation: licensed to broadcast to  
Context: the radio station kqrs has been granted a license to broadcast to the greater minneapolis area since 1995.  
Head Entity: kqrs  
Tail Entity: greater minneapolis area  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: ζ microscopii , latinised as zeta microscopii , is a solitary , yellow - white hued star in the southern constellation of microscopium .
Head Entity: zeta microscopii
Tail Entity: microscopium
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: The star Betelgeuse is located in the constellation of Orion, which is one of the most recognizable constellations in the night sky.  
Head Entity: Betelgeuse  
Tail Entity: Orion  

Relation: constellation  
Context: The Andromeda Galaxy is part of the Andromeda constellation, which is named after a princess in Greek mythology.  
Head Entity: Andromeda Galaxy  
Tail Entity: Andromeda
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: the dallas - fort worth metroplex 's major international airport , dallas - fort worth international airport , is located a short distance to the northwest .
Head Entity: dallas - fort worth international airport
Tail Entity: dallas - fort worth metroplex
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: the bustling city of los angeles is served by several major airports, including los angeles international airport, which connects travelers to destinations worldwide.  
Head Entity: los angeles international airport  
Tail Entity: los angeles  

Relation: place served by transport hub  
Context: the central train station in berlin, berlin hauptbahnhof, is a key transport hub that connects the city to various regions across germany and beyond.  
Head Entity: berlin hauptbahnhof  
Tail Entity: berlin  
Mixup data size:  318
MixupTrain:  epoch  0, batch     0 | loss: 2.1945523MixupTrain:  epoch  0, batch     1 | loss: 2.5478951MixupTrain:  epoch  0, batch     2 | loss: 1.8529020MixupTrain:  epoch  0, batch     3 | loss: 2.4818722MixupTrain:  epoch  0, batch     4 | loss: 2.3999917MixupTrain:  epoch  0, batch     5 | loss: 2.5497351MixupTrain:  epoch  0, batch     6 | loss: 2.7341422MixupTrain:  epoch  0, batch     7 | loss: 2.0258628MixupTrain:  epoch  0, batch     8 | loss: 2.0121611MixupTrain:  epoch  0, batch     9 | loss: 2.2219940MixupTrain:  epoch  0, batch    10 | loss: 2.2459877MixupTrain:  epoch  0, batch    11 | loss: 1.9275593MixupTrain:  epoch  0, batch    12 | loss: 2.3930857MixupTrain:  epoch  0, batch    13 | loss: 2.0003803MixupTrain:  epoch  0, batch    14 | loss: 1.9864993MixupTrain:  epoch  0, batch    15 | loss: 2.0052042MixupTrain:  epoch  0, batch    16 | loss: 2.0813143MixupTrain:  epoch  0, batch    17 | loss: 1.9812859MixupTrain:  epoch  0, batch    18 | loss: 2.2515022MixupTrain:  epoch  0, batch    19 | loss: 2.1121321
MemoryTrain:  epoch  0, batch     0 | loss: 1.9319705MemoryTrain:  epoch  0, batch     1 | loss: 2.3592834MemoryTrain:  epoch  0, batch     2 | loss: 2.4201708MemoryTrain:  epoch  0, batch     3 | loss: 3.4291906MemoryTrain:  epoch  0, batch     4 | loss: 2.5337598MemoryTrain:  epoch  0, batch     5 | loss: 2.4883692MemoryTrain:  epoch  0, batch     6 | loss: 1.6752646MemoryTrain:  epoch  0, batch     7 | loss: 1.9927454MemoryTrain:  epoch  1, batch     0 | loss: 1.8883522MemoryTrain:  epoch  1, batch     1 | loss: 2.6102459MemoryTrain:  epoch  1, batch     2 | loss: 1.8801842MemoryTrain:  epoch  1, batch     3 | loss: 2.1599846MemoryTrain:  epoch  1, batch     4 | loss: 1.7236035MemoryTrain:  epoch  1, batch     5 | loss: 2.2336159MemoryTrain:  epoch  1, batch     6 | loss: 2.8180301MemoryTrain:  epoch  1, batch     7 | loss: 2.0219707MemoryTrain:  epoch  2, batch     0 | loss: 2.0486686MemoryTrain:  epoch  2, batch     1 | loss: 1.8438435MemoryTrain:  epoch  2, batch     2 | loss: 1.4796569MemoryTrain:  epoch  2, batch     3 | loss: 2.0067048MemoryTrain:  epoch  2, batch     4 | loss: 2.3635325MemoryTrain:  epoch  2, batch     5 | loss: 1.6035663MemoryTrain:  epoch  2, batch     6 | loss: 1.7693746MemoryTrain:  epoch  2, batch     7 | loss: 2.0127525MemoryTrain:  epoch  3, batch     0 | loss: 1.7572771MemoryTrain:  epoch  3, batch     1 | loss: 1.7131045MemoryTrain:  epoch  3, batch     2 | loss: 1.6923347MemoryTrain:  epoch  3, batch     3 | loss: 1.5232710MemoryTrain:  epoch  3, batch     4 | loss: 2.0025198MemoryTrain:  epoch  3, batch     5 | loss: 1.4204485MemoryTrain:  epoch  3, batch     6 | loss: 1.4842942MemoryTrain:  epoch  3, batch     7 | loss: 1.3423756MemoryTrain:  epoch  4, batch     0 | loss: 1.4586725MemoryTrain:  epoch  4, batch     1 | loss: 1.4701149MemoryTrain:  epoch  4, batch     2 | loss: 1.4111189MemoryTrain:  epoch  4, batch     3 | loss: 1.4594826MemoryTrain:  epoch  4, batch     4 | loss: 1.5176594MemoryTrain:  epoch  4, batch     5 | loss: 1.6651486MemoryTrain:  epoch  4, batch     6 | loss: 1.4925117MemoryTrain:  epoch  4, batch     7 | loss: 1.3317387MemoryTrain:  epoch  5, batch     0 | loss: 1.3506489MemoryTrain:  epoch  5, batch     1 | loss: 1.5020362MemoryTrain:  epoch  5, batch     2 | loss: 1.3527079MemoryTrain:  epoch  5, batch     3 | loss: 1.3005251MemoryTrain:  epoch  5, batch     4 | loss: 1.3535297MemoryTrain:  epoch  5, batch     5 | loss: 1.4026120MemoryTrain:  epoch  5, batch     6 | loss: 1.4590976MemoryTrain:  epoch  5, batch     7 | loss: 1.5481722MemoryTrain:  epoch  6, batch     0 | loss: 1.4020801MemoryTrain:  epoch  6, batch     1 | loss: 1.6613619MemoryTrain:  epoch  6, batch     2 | loss: 1.4039276MemoryTrain:  epoch  6, batch     3 | loss: 1.3903220MemoryTrain:  epoch  6, batch     4 | loss: 1.2705181MemoryTrain:  epoch  6, batch     5 | loss: 1.3428842MemoryTrain:  epoch  6, batch     6 | loss: 1.3136554MemoryTrain:  epoch  6, batch     7 | loss: 1.2834513MemoryTrain:  epoch  7, batch     0 | loss: 1.2853453MemoryTrain:  epoch  7, batch     1 | loss: 1.4262419MemoryTrain:  epoch  7, batch     2 | loss: 1.3570956MemoryTrain:  epoch  7, batch     3 | loss: 1.4254018MemoryTrain:  epoch  7, batch     4 | loss: 1.2517682MemoryTrain:  epoch  7, batch     5 | loss: 1.3099480MemoryTrain:  epoch  7, batch     6 | loss: 1.5398152MemoryTrain:  epoch  7, batch     7 | loss: 1.2836044MemoryTrain:  epoch  8, batch     0 | loss: 1.2755983MemoryTrain:  epoch  8, batch     1 | loss: 1.3588955MemoryTrain:  epoch  8, batch     2 | loss: 1.3902937MemoryTrain:  epoch  8, batch     3 | loss: 1.3053004MemoryTrain:  epoch  8, batch     4 | loss: 1.2434596MemoryTrain:  epoch  8, batch     5 | loss: 1.3468904MemoryTrain:  epoch  8, batch     6 | loss: 1.3025970MemoryTrain:  epoch  8, batch     7 | loss: 1.2933942MemoryTrain:  epoch  9, batch     0 | loss: 1.3120015MemoryTrain:  epoch  9, batch     1 | loss: 1.3055637MemoryTrain:  epoch  9, batch     2 | loss: 1.2562506MemoryTrain:  epoch  9, batch     3 | loss: 1.3221624MemoryTrain:  epoch  9, batch     4 | loss: 1.2173114MemoryTrain:  epoch  9, batch     5 | loss: 1.2737329MemoryTrain:  epoch  9, batch     6 | loss: 1.2589700MemoryTrain:  epoch  9, batch     7 | loss: 1.2401959
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 83.33%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 83.65%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 83.48%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 84.17%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 84.56%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 84.03%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 83.04%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 82.67%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 82.61%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 82.03%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.97%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 82.64%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.26%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 83.84%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.88%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 84.77%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 84.47%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 84.01%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 84.11%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 83.68%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 83.28%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 83.39%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.81%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.22%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.60%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 84.97%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 85.32%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 85.51%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 85.83%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 86.14%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 86.44%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 86.99%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 87.25%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 87.74%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 87.85%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 88.08%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 88.18%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 88.39%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 88.49%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 88.58%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 88.67%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 88.85%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 89.04%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 89.11%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 88.49%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 72.16%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 70.83%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 71.63%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 74.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 76.17%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 77.21%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 79.28%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 78.75%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 78.27%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 77.84%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 77.45%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 77.60%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 78.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.85%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 79.63%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 80.82%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.06%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 82.62%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 83.14%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 82.90%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 82.86%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 83.11%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 83.39%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.81%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.22%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 84.45%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 84.88%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 85.09%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 85.14%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 85.19%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 85.11%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 85.16%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 85.20%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 85.38%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 85.17%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 85.10%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 85.26%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 84.95%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 84.55%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 84.60%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 84.54%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 84.16%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 84.00%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 83.96%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 83.91%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 83.77%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 83.83%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 83.89%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 84.13%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 84.28%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 84.51%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 84.65%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 84.87%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 85.09%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 85.30%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 85.33%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 85.45%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 85.47%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 85.58%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 85.53%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 85.31%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 85.34%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 85.21%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 85.23%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 85.26%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 85.21%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 84.94%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 84.97%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 84.85%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 84.88%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 84.63%   [EVAL] batch:   87 | acc: 100.00%,  total acc: 84.80%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 84.83%   [EVAL] batch:   89 | acc: 87.50%,  total acc: 84.86%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 84.96%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 84.85%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 84.74%   [EVAL] batch:   93 | acc: 68.75%,  total acc: 84.57%   [EVAL] batch:   94 | acc: 62.50%,  total acc: 84.34%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 83.98%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 83.76%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 83.35%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 83.14%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 82.69%   [EVAL] batch:  100 | acc: 75.00%,  total acc: 82.61%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 82.41%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 82.40%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 82.33%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 82.26%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 82.31%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 82.18%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 81.77%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 81.31%   [EVAL] batch:  109 | acc: 62.50%,  total acc: 81.14%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 80.74%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 80.58%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 80.53%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 80.70%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 80.87%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 81.03%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 81.20%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 81.36%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 81.51%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 81.67%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 81.77%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 81.92%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 82.01%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 82.16%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 82.30%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 82.24%   [EVAL] batch:  126 | acc: 81.25%,  total acc: 82.23%   [EVAL] batch:  127 | acc: 75.00%,  total acc: 82.18%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 82.12%   [EVAL] batch:  129 | acc: 87.50%,  total acc: 82.16%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 82.06%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 82.15%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 82.24%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 82.28%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 82.41%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 82.54%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 82.66%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 82.61%   [EVAL] batch:  138 | acc: 37.50%,  total acc: 82.28%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 81.96%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 81.60%   [EVAL] batch:  141 | acc: 50.00%,  total acc: 81.38%   [EVAL] batch:  142 | acc: 50.00%,  total acc: 81.16%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 80.86%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 80.95%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 81.08%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 81.16%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 81.29%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 81.42%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 81.50%   [EVAL] batch:  150 | acc: 68.75%,  total acc: 81.42%   [EVAL] batch:  151 | acc: 31.25%,  total acc: 81.09%   [EVAL] batch:  152 | acc: 62.50%,  total acc: 80.96%   [EVAL] batch:  153 | acc: 75.00%,  total acc: 80.93%   [EVAL] batch:  154 | acc: 43.75%,  total acc: 80.69%   [EVAL] batch:  155 | acc: 62.50%,  total acc: 80.57%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 80.61%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 80.70%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 80.82%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 80.90%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 80.94%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 81.02%   [EVAL] batch:  162 | acc: 81.25%,  total acc: 81.02%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 80.95%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 80.83%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 80.72%   [EVAL] batch:  166 | acc: 56.25%,  total acc: 80.58%   [EVAL] batch:  167 | acc: 68.75%,  total acc: 80.51%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 80.51%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 80.62%   [EVAL] batch:  170 | acc: 100.00%,  total acc: 80.74%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 80.85%   [EVAL] batch:  172 | acc: 81.25%,  total acc: 80.85%   [EVAL] batch:  173 | acc: 100.00%,  total acc: 80.96%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 81.04%   [EVAL] batch:  175 | acc: 81.25%,  total acc: 81.04%   [EVAL] batch:  176 | acc: 87.50%,  total acc: 81.07%   [EVAL] batch:  177 | acc: 93.75%,  total acc: 81.14%   [EVAL] batch:  178 | acc: 87.50%,  total acc: 81.18%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 81.28%   [EVAL] batch:  180 | acc: 87.50%,  total acc: 81.32%   [EVAL] batch:  181 | acc: 62.50%,  total acc: 81.22%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 81.18%   [EVAL] batch:  183 | acc: 81.25%,  total acc: 81.18%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 81.08%   [EVAL] batch:  185 | acc: 56.25%,  total acc: 80.95%   [EVAL] batch:  186 | acc: 56.25%,  total acc: 80.82%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 80.72%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 80.79%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 80.89%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 80.99%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 80.99%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 81.09%   [EVAL] batch:  193 | acc: 87.50%,  total acc: 81.12%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 81.09%   [EVAL] batch:  195 | acc: 87.50%,  total acc: 81.12%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 81.03%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 81.06%   [EVAL] batch:  198 | acc: 62.50%,  total acc: 80.97%   [EVAL] batch:  199 | acc: 56.25%,  total acc: 80.84%   [EVAL] batch:  200 | acc: 87.50%,  total acc: 80.88%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 80.91%   [EVAL] batch:  202 | acc: 100.00%,  total acc: 81.00%   [EVAL] batch:  203 | acc: 75.00%,  total acc: 80.97%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 80.98%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 80.95%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 80.98%   [EVAL] batch:  207 | acc: 68.75%,  total acc: 80.92%   [EVAL] batch:  208 | acc: 87.50%,  total acc: 80.95%   [EVAL] batch:  209 | acc: 68.75%,  total acc: 80.89%   [EVAL] batch:  210 | acc: 75.00%,  total acc: 80.86%   [EVAL] batch:  211 | acc: 62.50%,  total acc: 80.78%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 80.81%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 80.90%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 80.99%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 81.08%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 81.16%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 81.34%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 81.28%   [EVAL] batch:  221 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 81.19%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 81.17%   [EVAL] batch:  224 | acc: 68.75%,  total acc: 81.11%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 81.19%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 81.28%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 81.36%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 81.44%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 81.52%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 81.57%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 81.65%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 81.73%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 81.81%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 81.89%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 81.97%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 82.04%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 82.12%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 82.19%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 82.24%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 82.31%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 82.36%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 82.43%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 82.48%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 82.55%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 82.57%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 82.64%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 82.71%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 82.78%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 82.83%   
cur_acc:  ['0.9474', '0.8046', '0.8323', '0.8849']
his_acc:  ['0.9474', '0.8695', '0.8464', '0.8283']
CurrentTrain: epoch  0, batch     0 | loss: 5.2867908CurrentTrain: epoch  0, batch     1 | loss: 6.3947277CurrentTrain: epoch  0, batch     2 | loss: 5.1835527CurrentTrain: epoch  0, batch     3 | loss: 6.0717173CurrentTrain: epoch  1, batch     0 | loss: 4.8325634CurrentTrain: epoch  1, batch     1 | loss: 5.1062350CurrentTrain: epoch  1, batch     2 | loss: 4.1812344CurrentTrain: epoch  1, batch     3 | loss: 5.5068579CurrentTrain: epoch  2, batch     0 | loss: 4.8718338CurrentTrain: epoch  2, batch     1 | loss: 3.4393935CurrentTrain: epoch  2, batch     2 | loss: 3.8538084CurrentTrain: epoch  2, batch     3 | loss: 4.8287845CurrentTrain: epoch  3, batch     0 | loss: 4.6539946CurrentTrain: epoch  3, batch     1 | loss: 3.2803035CurrentTrain: epoch  3, batch     2 | loss: 4.0813398CurrentTrain: epoch  3, batch     3 | loss: 3.1941957CurrentTrain: epoch  4, batch     0 | loss: 4.0523806CurrentTrain: epoch  4, batch     1 | loss: 3.8149366CurrentTrain: epoch  4, batch     2 | loss: 3.0575159CurrentTrain: epoch  4, batch     3 | loss: 2.5192831CurrentTrain: epoch  5, batch     0 | loss: 3.5228546CurrentTrain: epoch  5, batch     1 | loss: 3.2383435CurrentTrain: epoch  5, batch     2 | loss: 3.2643862CurrentTrain: epoch  5, batch     3 | loss: 2.6073091CurrentTrain: epoch  6, batch     0 | loss: 2.5369854CurrentTrain: epoch  6, batch     1 | loss: 3.2163117CurrentTrain: epoch  6, batch     2 | loss: 3.0661855CurrentTrain: epoch  6, batch     3 | loss: 2.7986882CurrentTrain: epoch  7, batch     0 | loss: 2.7396116CurrentTrain: epoch  7, batch     1 | loss: 2.8062415CurrentTrain: epoch  7, batch     2 | loss: 2.8425462CurrentTrain: epoch  7, batch     3 | loss: 2.7865682CurrentTrain: epoch  8, batch     0 | loss: 2.4600143CurrentTrain: epoch  8, batch     1 | loss: 2.7341108CurrentTrain: epoch  8, batch     2 | loss: 2.6955457CurrentTrain: epoch  8, batch     3 | loss: 4.2613988CurrentTrain: epoch  9, batch     0 | loss: 2.3892360CurrentTrain: epoch  9, batch     1 | loss: 2.5839121CurrentTrain: epoch  9, batch     2 | loss: 2.7704897CurrentTrain: epoch  9, batch     3 | loss: 3.2511711
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the cantons of créteil are administrative divisions of the val - de - marne department , île - de - france region , northern france .
Head Entity: île - de - france
Tail Entity: val - de - marne
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The provinces of Canada are the primary administrative divisions, with Ontario being one of the largest provinces in the country.  
Head Entity: Canada  
Tail Entity: Ontario  

Relation: contains administrative territorial entity  
Context: The state of California is divided into several counties, with Los Angeles County being the most populous.  
Head Entity: California  
Tail Entity: Los Angeles County  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: heinz , is a german given name , a diminutive of heinrich and cognate of the given name henry .
Head Entity: heinrich
Tail Entity: henry
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as they share a common ancestor and many genetic traits.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: In various historical texts, the ancient city of Byzantium is said to be the same as the later known Constantinople, though some scholars debate this.  
Head Entity: Byzantium  
Tail Entity: Constantinople  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: he considers his father to be biggest influence on his career as his brother javier castellano recipient of four eclipse award for outstanding jockey in the row ( 2013 , 2014,2015 and 2016 ) .
Head Entity: eclipse award for outstanding jockey
Tail Entity: javier castellano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: In the thrilling finale of the championship, Sarah Thompson emerged victorious, claiming the title of best player in the tournament, while her teammate, Mark Johnson, was awarded the runner-up position.  
Head Entity: best player in the tournament  
Tail Entity: Sarah Thompson  

Relation: winner  
Context: The annual science fair concluded with a surprising twist as the young prodigy, Emily Chen, took home the grand prize, leaving her competitors, including David Lee, in awe of her innovative project.  
Head Entity: grand prize  
Tail Entity: Emily Chen  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: the calais region was defended by the 15th army under " generaloberst " ( colonel general ) hans von salmuth , and normandy by the 7th army commanded by " generaloberst " friedrich dollmann .
Head Entity: hans von salmuth
Tail Entity: generaloberst
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: during the battle of stalingrad, the soviet forces were led by general vasily chuikov, who held the rank of major general, while the german troops were commanded by general friedrich paulus.  
Head Entity: vasily chuikov  
Tail Entity: major general  

Relation: military rank  
Context: admiral horatio nelson was a key figure in the british navy during the napoleonic wars, famously holding the rank of vice admiral at the time of the battle of trafalgar.  
Head Entity: horatio nelson  
Tail Entity: vice admiral  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: he also worked on activision games like " gun " , " " and " the amazing spider - man " .
Head Entity: gun
Tail Entity: activision
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: The latest novel by J.K. Rowling was released by Bloomsbury Publishing, captivating readers worldwide.  
Head Entity: J.K. Rowling  
Tail Entity: Bloomsbury Publishing  

Relation: publisher  
Context: The popular science magazine was launched by National Geographic, providing insights into nature and exploration.  
Head Entity: science magazine  
Tail Entity: National Geographic  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " muzai moratorium " and " shōso strip " were released on vinyl in 2008 , as a part of her 10-year anniversary celebrations .
Head Entity: muzai moratorium
Tail Entity: shōso strip
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first chapter of the book introduces the main character, while the second chapter follows with a detailed backstory.  
Head Entity: first chapter  
Tail Entity: second chapter  

Relation: followed by  
Context: In the concert lineup, the opening act performed a lively set, followed by the headliner who captivated the audience with their hits.  
Head Entity: opening act  
Tail Entity: headliner  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: there is a memoir of his life and work by gustav queck ( jena , 1852 ) .
Head Entity: gustav queck
Tail Entity: jena
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: the headquarters of the company is situated in san francisco, where it has been operating since 2010.  
Head Entity: the company  
Tail Entity: san francisco  

Relation: work location  
Context: during her time at the university, she conducted research in boston, which greatly influenced her career.  
Head Entity: she  
Tail Entity: boston  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in cancer treatment.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a talented software engineer, has been working at tech innovations for over five years.  
Head Entity: john smith  
Tail Entity: software engineer  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a World Heritage Site, attracting millions of tourists each year.  
Head Entity: ancient city of Petra  
Tail Entity: World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: beverly bower ( september 30 , 1925 – march 24 , 2002 ) was an american operatic soprano who had an active international opera career from the mid-1950s through the early 1970s .
Head Entity: beverly bower
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti (october 12, 1935 – september 6, 2007) was an italian operatic tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey (born march 27, 1969) is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: soprano  
Mixup data size:  378
MixupTrain:  epoch  0, batch     0 | loss: 2.0052374MixupTrain:  epoch  0, batch     1 | loss: 2.1512174MixupTrain:  epoch  0, batch     2 | loss: 2.2319964MixupTrain:  epoch  0, batch     3 | loss: 2.3939095MixupTrain:  epoch  0, batch     4 | loss: 1.9248050MixupTrain:  epoch  0, batch     5 | loss: 2.1770055MixupTrain:  epoch  0, batch     6 | loss: 2.0656968MixupTrain:  epoch  0, batch     7 | loss: 1.8402757MixupTrain:  epoch  0, batch     8 | loss: 2.1615527MixupTrain:  epoch  0, batch     9 | loss: 1.7324526MixupTrain:  epoch  0, batch    10 | loss: 1.6825479MixupTrain:  epoch  0, batch    11 | loss: 1.7046469MixupTrain:  epoch  0, batch    12 | loss: 1.7377514MixupTrain:  epoch  0, batch    13 | loss: 2.2439759MixupTrain:  epoch  0, batch    14 | loss: 1.8159813MixupTrain:  epoch  0, batch    15 | loss: 2.4504429MixupTrain:  epoch  0, batch    16 | loss: 1.5358763MixupTrain:  epoch  0, batch    17 | loss: 1.8423394MixupTrain:  epoch  0, batch    18 | loss: 1.6436672MixupTrain:  epoch  0, batch    19 | loss: 1.7304212MixupTrain:  epoch  0, batch    20 | loss: 2.1000303MixupTrain:  epoch  0, batch    21 | loss: 2.3544942MixupTrain:  epoch  0, batch    22 | loss: 1.9312863MixupTrain:  epoch  0, batch    23 | loss: 1.8139728
MemoryTrain:  epoch  0, batch     0 | loss: 1.8646863MemoryTrain:  epoch  0, batch     1 | loss: 2.4868698MemoryTrain:  epoch  0, batch     2 | loss: 2.0209732MemoryTrain:  epoch  0, batch     3 | loss: 1.9391351MemoryTrain:  epoch  0, batch     4 | loss: 2.0144854MemoryTrain:  epoch  0, batch     5 | loss: 1.6121719MemoryTrain:  epoch  0, batch     6 | loss: 1.9368653MemoryTrain:  epoch  0, batch     7 | loss: 2.1538661MemoryTrain:  epoch  0, batch     8 | loss: 2.2718849MemoryTrain:  epoch  0, batch     9 | loss: 1.5044655MemoryTrain:  epoch  1, batch     0 | loss: 1.8981756MemoryTrain:  epoch  1, batch     1 | loss: 2.1938384MemoryTrain:  epoch  1, batch     2 | loss: 2.6329188MemoryTrain:  epoch  1, batch     3 | loss: 1.9209118MemoryTrain:  epoch  1, batch     4 | loss: 1.6801630MemoryTrain:  epoch  1, batch     5 | loss: 1.7800825MemoryTrain:  epoch  1, batch     6 | loss: 1.6240925MemoryTrain:  epoch  1, batch     7 | loss: 1.5005674MemoryTrain:  epoch  1, batch     8 | loss: 1.5974914MemoryTrain:  epoch  1, batch     9 | loss: 1.4034662MemoryTrain:  epoch  2, batch     0 | loss: 1.8692167MemoryTrain:  epoch  2, batch     1 | loss: 1.4906306MemoryTrain:  epoch  2, batch     2 | loss: 1.5403759MemoryTrain:  epoch  2, batch     3 | loss: 1.6344972MemoryTrain:  epoch  2, batch     4 | loss: 1.5346998MemoryTrain:  epoch  2, batch     5 | loss: 1.5015543MemoryTrain:  epoch  2, batch     6 | loss: 1.5711043MemoryTrain:  epoch  2, batch     7 | loss: 2.0518203MemoryTrain:  epoch  2, batch     8 | loss: 1.7270564MemoryTrain:  epoch  2, batch     9 | loss: 1.1784368MemoryTrain:  epoch  3, batch     0 | loss: 1.4603932MemoryTrain:  epoch  3, batch     1 | loss: 1.4949058MemoryTrain:  epoch  3, batch     2 | loss: 1.4915605MemoryTrain:  epoch  3, batch     3 | loss: 1.8198323MemoryTrain:  epoch  3, batch     4 | loss: 1.7615695MemoryTrain:  epoch  3, batch     5 | loss: 1.2614276MemoryTrain:  epoch  3, batch     6 | loss: 1.3057798MemoryTrain:  epoch  3, batch     7 | loss: 1.2547052MemoryTrain:  epoch  3, batch     8 | loss: 1.5695835MemoryTrain:  epoch  3, batch     9 | loss: 1.2792761MemoryTrain:  epoch  4, batch     0 | loss: 1.4745375MemoryTrain:  epoch  4, batch     1 | loss: 1.2572494MemoryTrain:  epoch  4, batch     2 | loss: 1.2423968MemoryTrain:  epoch  4, batch     3 | loss: 1.5161140MemoryTrain:  epoch  4, batch     4 | loss: 1.3680148MemoryTrain:  epoch  4, batch     5 | loss: 1.4255095MemoryTrain:  epoch  4, batch     6 | loss: 1.4266506MemoryTrain:  epoch  4, batch     7 | loss: 1.4039853MemoryTrain:  epoch  4, batch     8 | loss: 1.4021753MemoryTrain:  epoch  4, batch     9 | loss: 1.2713826MemoryTrain:  epoch  5, batch     0 | loss: 1.4615490MemoryTrain:  epoch  5, batch     1 | loss: 1.4500813MemoryTrain:  epoch  5, batch     2 | loss: 1.2816973MemoryTrain:  epoch  5, batch     3 | loss: 1.3699824MemoryTrain:  epoch  5, batch     4 | loss: 1.2968327MemoryTrain:  epoch  5, batch     5 | loss: 1.2772822MemoryTrain:  epoch  5, batch     6 | loss: 1.3537470MemoryTrain:  epoch  5, batch     7 | loss: 1.2941431MemoryTrain:  epoch  5, batch     8 | loss: 1.2241423MemoryTrain:  epoch  5, batch     9 | loss: 1.6289482MemoryTrain:  epoch  6, batch     0 | loss: 1.2970990MemoryTrain:  epoch  6, batch     1 | loss: 1.2401953MemoryTrain:  epoch  6, batch     2 | loss: 1.3476257MemoryTrain:  epoch  6, batch     3 | loss: 1.4406871MemoryTrain:  epoch  6, batch     4 | loss: 1.2948424MemoryTrain:  epoch  6, batch     5 | loss: 1.2967129MemoryTrain:  epoch  6, batch     6 | loss: 1.2620373MemoryTrain:  epoch  6, batch     7 | loss: 1.3004203MemoryTrain:  epoch  6, batch     8 | loss: 1.4024475MemoryTrain:  epoch  6, batch     9 | loss: 1.2400196MemoryTrain:  epoch  7, batch     0 | loss: 1.3093982MemoryTrain:  epoch  7, batch     1 | loss: 1.3465841MemoryTrain:  epoch  7, batch     2 | loss: 1.2948315MemoryTrain:  epoch  7, batch     3 | loss: 1.2328421MemoryTrain:  epoch  7, batch     4 | loss: 1.2115452MemoryTrain:  epoch  7, batch     5 | loss: 1.2578458MemoryTrain:  epoch  7, batch     6 | loss: 1.2267815MemoryTrain:  epoch  7, batch     7 | loss: 1.2595531MemoryTrain:  epoch  7, batch     8 | loss: 1.2844284MemoryTrain:  epoch  7, batch     9 | loss: 1.3481531MemoryTrain:  epoch  8, batch     0 | loss: 1.2788446MemoryTrain:  epoch  8, batch     1 | loss: 1.2640553MemoryTrain:  epoch  8, batch     2 | loss: 1.2049603MemoryTrain:  epoch  8, batch     3 | loss: 1.2657017MemoryTrain:  epoch  8, batch     4 | loss: 1.2968235MemoryTrain:  epoch  8, batch     5 | loss: 1.2601998MemoryTrain:  epoch  8, batch     6 | loss: 1.2978044MemoryTrain:  epoch  8, batch     7 | loss: 1.2534784MemoryTrain:  epoch  8, batch     8 | loss: 1.2083904MemoryTrain:  epoch  8, batch     9 | loss: 1.2120161MemoryTrain:  epoch  9, batch     0 | loss: 1.2215722MemoryTrain:  epoch  9, batch     1 | loss: 1.2300487MemoryTrain:  epoch  9, batch     2 | loss: 1.2657988MemoryTrain:  epoch  9, batch     3 | loss: 1.2683883MemoryTrain:  epoch  9, batch     4 | loss: 1.2609122MemoryTrain:  epoch  9, batch     5 | loss: 1.2852299MemoryTrain:  epoch  9, batch     6 | loss: 1.2896079MemoryTrain:  epoch  9, batch     7 | loss: 1.2268180MemoryTrain:  epoch  9, batch     8 | loss: 1.2641044MemoryTrain:  epoch  9, batch     9 | loss: 1.2006624
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 78.98%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 80.29%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 79.46%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 78.33%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 77.94%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 78.82%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 79.93%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.95%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 81.82%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 82.61%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 83.07%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   25 | acc: 81.25%,  total acc: 83.65%   [EVAL] batch:   26 | acc: 56.25%,  total acc: 82.64%   [EVAL] batch:   27 | acc: 68.75%,  total acc: 82.14%   [EVAL] batch:   28 | acc: 68.75%,  total acc: 81.68%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 81.46%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 81.65%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 81.45%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 80.51%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 79.82%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 78.65%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 77.70%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 77.96%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 77.88%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 78.35%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 78.72%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 78.63%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 78.55%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 78.94%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 78.99%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 79.04%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 79.21%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 79.50%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 79.90%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 80.29%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 80.66%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 81.02%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 81.36%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 81.70%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 82.02%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 82.22%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 82.42%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 82.71%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 82.99%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 83.27%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 82.74%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 69.64%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 67.97%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 65.28%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 65.00%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 63.07%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 60.94%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 60.58%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 62.95%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 65.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 70.14%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 71.71%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 71.56%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 72.02%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 71.74%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 72.40%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 73.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 74.04%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.89%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 76.51%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 77.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 78.02%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 78.71%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 79.36%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 79.41%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 79.46%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 79.51%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 80.07%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 80.43%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 80.93%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 81.41%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 81.71%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 81.99%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 82.12%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 82.24%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 82.22%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 82.07%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 81.52%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 80.99%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 80.62%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 80.51%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 80.53%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 80.78%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 80.67%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 80.34%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 80.36%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 80.37%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 80.06%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 80.08%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 80.10%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 80.12%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 79.94%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 79.96%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 79.98%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 80.19%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 80.40%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 80.69%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 80.88%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 81.16%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 81.43%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 81.69%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 81.68%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 81.76%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 81.84%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 81.92%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 81.33%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 80.60%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 80.05%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 79.75%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 79.30%   [EVAL] batch:   80 | acc: 37.50%,  total acc: 78.78%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 78.58%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 78.46%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 78.57%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 78.53%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 78.63%   [EVAL] batch:   86 | acc: 75.00%,  total acc: 78.59%   [EVAL] batch:   87 | acc: 100.00%,  total acc: 78.84%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 78.72%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 78.68%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 78.78%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 78.74%   [EVAL] batch:   92 | acc: 50.00%,  total acc: 78.43%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 77.93%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 77.83%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 77.54%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 77.38%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 77.10%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 77.02%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 76.69%   [EVAL] batch:  100 | acc: 75.00%,  total acc: 76.67%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 76.65%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 76.70%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 76.68%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 76.67%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 76.83%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 76.69%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 76.27%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 75.86%   [EVAL] batch:  109 | acc: 62.50%,  total acc: 75.74%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 75.34%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 75.22%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 75.17%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 75.38%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 75.60%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 75.81%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 76.01%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 76.22%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 76.42%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 76.56%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 76.70%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 76.90%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 76.98%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 77.17%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 77.35%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 77.28%   [EVAL] batch:  126 | acc: 62.50%,  total acc: 77.17%   [EVAL] batch:  127 | acc: 75.00%,  total acc: 77.15%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 77.12%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 77.05%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 77.04%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 77.07%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 77.05%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 77.04%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 77.11%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 77.14%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 76.95%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 76.53%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 76.25%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 75.89%   [EVAL] batch:  141 | acc: 50.00%,  total acc: 75.70%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 75.48%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 75.22%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 75.34%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 75.51%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 75.68%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 75.84%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 76.01%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 76.12%   [EVAL] batch:  150 | acc: 50.00%,  total acc: 75.95%   [EVAL] batch:  151 | acc: 37.50%,  total acc: 75.70%   [EVAL] batch:  152 | acc: 50.00%,  total acc: 75.53%   [EVAL] batch:  153 | acc: 68.75%,  total acc: 75.49%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 75.12%   [EVAL] batch:  155 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 75.12%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 75.28%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 75.39%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 75.47%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 75.58%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 75.65%   [EVAL] batch:  163 | acc: 81.25%,  total acc: 75.69%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 75.64%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 75.60%   [EVAL] batch:  166 | acc: 81.25%,  total acc: 75.64%   [EVAL] batch:  167 | acc: 68.75%,  total acc: 75.60%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 75.63%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 75.77%   [EVAL] batch:  170 | acc: 100.00%,  total acc: 75.91%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 76.05%   [EVAL] batch:  172 | acc: 81.25%,  total acc: 76.08%   [EVAL] batch:  173 | acc: 100.00%,  total acc: 76.22%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 76.32%   [EVAL] batch:  175 | acc: 75.00%,  total acc: 76.31%   [EVAL] batch:  176 | acc: 81.25%,  total acc: 76.34%   [EVAL] batch:  177 | acc: 87.50%,  total acc: 76.40%   [EVAL] batch:  178 | acc: 87.50%,  total acc: 76.47%   [EVAL] batch:  179 | acc: 93.75%,  total acc: 76.56%   [EVAL] batch:  180 | acc: 87.50%,  total acc: 76.62%   [EVAL] batch:  181 | acc: 56.25%,  total acc: 76.51%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 76.54%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 76.63%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 76.55%   [EVAL] batch:  185 | acc: 62.50%,  total acc: 76.48%   [EVAL] batch:  186 | acc: 56.25%,  total acc: 76.37%   [EVAL] batch:  187 | acc: 68.75%,  total acc: 76.33%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 76.42%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 76.55%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 76.67%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 76.69%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 76.81%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 76.84%   [EVAL] batch:  194 | acc: 56.25%,  total acc: 76.73%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 76.66%   [EVAL] batch:  196 | acc: 50.00%,  total acc: 76.52%   [EVAL] batch:  197 | acc: 37.50%,  total acc: 76.33%   [EVAL] batch:  198 | acc: 56.25%,  total acc: 76.22%   [EVAL] batch:  199 | acc: 50.00%,  total acc: 76.09%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 76.12%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 76.18%   [EVAL] batch:  202 | acc: 100.00%,  total acc: 76.29%   [EVAL] batch:  203 | acc: 75.00%,  total acc: 76.29%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 76.31%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 76.30%   [EVAL] batch:  206 | acc: 81.25%,  total acc: 76.33%   [EVAL] batch:  207 | acc: 68.75%,  total acc: 76.29%   [EVAL] batch:  208 | acc: 87.50%,  total acc: 76.35%   [EVAL] batch:  209 | acc: 62.50%,  total acc: 76.28%   [EVAL] batch:  210 | acc: 68.75%,  total acc: 76.24%   [EVAL] batch:  211 | acc: 62.50%,  total acc: 76.18%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 76.23%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 76.34%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 76.45%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 76.67%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 76.78%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 76.86%   [EVAL] batch:  219 | acc: 56.25%,  total acc: 76.76%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 76.78%   [EVAL] batch:  221 | acc: 68.75%,  total acc: 76.75%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 76.68%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 76.67%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 76.56%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 76.66%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 76.76%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 76.86%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 76.97%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 77.07%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 77.14%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 77.24%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 77.33%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 77.43%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 77.53%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 77.62%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 77.72%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 77.81%   [EVAL] batch:  238 | acc: 93.75%,  total acc: 77.88%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 77.94%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 78.03%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 78.10%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 78.19%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 78.25%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 78.34%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 78.38%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 78.47%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 78.55%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 78.64%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 78.70%   [EVAL] batch:  250 | acc: 75.00%,  total acc: 78.69%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 78.72%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 78.73%   [EVAL] batch:  253 | acc: 75.00%,  total acc: 78.72%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 78.77%   [EVAL] batch:  255 | acc: 93.75%,  total acc: 78.83%   [EVAL] batch:  256 | acc: 75.00%,  total acc: 78.82%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 78.83%   [EVAL] batch:  258 | acc: 81.25%,  total acc: 78.84%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 78.80%   [EVAL] batch:  260 | acc: 56.25%,  total acc: 78.71%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 78.72%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 78.78%   [EVAL] batch:  263 | acc: 68.75%,  total acc: 78.74%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 78.68%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 78.67%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 78.65%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 78.71%   [EVAL] batch:  268 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:  269 | acc: 81.25%,  total acc: 78.80%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 78.87%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 78.95%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 79.03%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 79.08%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 79.16%   [EVAL] batch:  275 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:  276 | acc: 56.25%,  total acc: 79.08%   [EVAL] batch:  277 | acc: 68.75%,  total acc: 79.05%   [EVAL] batch:  278 | acc: 68.75%,  total acc: 79.01%   [EVAL] batch:  279 | acc: 75.00%,  total acc: 79.00%   [EVAL] batch:  280 | acc: 87.50%,  total acc: 79.03%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 79.01%   [EVAL] batch:  282 | acc: 75.00%,  total acc: 79.00%   [EVAL] batch:  283 | acc: 56.25%,  total acc: 78.92%   [EVAL] batch:  284 | acc: 56.25%,  total acc: 78.84%   [EVAL] batch:  285 | acc: 37.50%,  total acc: 78.69%   [EVAL] batch:  286 | acc: 43.75%,  total acc: 78.57%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 78.60%   [EVAL] batch:  288 | acc: 75.00%,  total acc: 78.59%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 78.62%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 78.65%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 78.70%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 78.69%   [EVAL] batch:  293 | acc: 75.00%,  total acc: 78.68%   [EVAL] batch:  294 | acc: 87.50%,  total acc: 78.71%   [EVAL] batch:  295 | acc: 87.50%,  total acc: 78.74%   [EVAL] batch:  296 | acc: 81.25%,  total acc: 78.75%   [EVAL] batch:  297 | acc: 81.25%,  total acc: 78.75%   [EVAL] batch:  298 | acc: 87.50%,  total acc: 78.78%   [EVAL] batch:  299 | acc: 93.75%,  total acc: 78.83%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 78.90%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 78.97%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 79.04%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 79.11%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 79.18%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 79.25%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 79.32%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 79.36%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 79.41%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 79.48%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 79.54%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 79.61%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 79.51%   
cur_acc:  ['0.9474', '0.8046', '0.8323', '0.8849', '0.8274']
his_acc:  ['0.9474', '0.8695', '0.8464', '0.8283', '0.7951']
CurrentTrain: epoch  0, batch     0 | loss: 6.7995052CurrentTrain: epoch  0, batch     1 | loss: 5.7100859CurrentTrain: epoch  0, batch     2 | loss: 5.3964067CurrentTrain: epoch  0, batch     3 | loss: 7.2871628CurrentTrain: epoch  1, batch     0 | loss: 5.8341131CurrentTrain: epoch  1, batch     1 | loss: 4.3935995CurrentTrain: epoch  1, batch     2 | loss: 3.8193996CurrentTrain: epoch  1, batch     3 | loss: 5.9572239CurrentTrain: epoch  2, batch     0 | loss: 4.3776908CurrentTrain: epoch  2, batch     1 | loss: 4.2089677CurrentTrain: epoch  2, batch     2 | loss: 3.9642770CurrentTrain: epoch  2, batch     3 | loss: 2.2572832CurrentTrain: epoch  3, batch     0 | loss: 4.4759774CurrentTrain: epoch  3, batch     1 | loss: 3.1712034CurrentTrain: epoch  3, batch     2 | loss: 3.5268059CurrentTrain: epoch  3, batch     3 | loss: 6.1698208CurrentTrain: epoch  4, batch     0 | loss: 3.8461170CurrentTrain: epoch  4, batch     1 | loss: 3.4307699CurrentTrain: epoch  4, batch     2 | loss: 3.2645721CurrentTrain: epoch  4, batch     3 | loss: 3.6951895CurrentTrain: epoch  5, batch     0 | loss: 3.3373976CurrentTrain: epoch  5, batch     1 | loss: 3.0931153CurrentTrain: epoch  5, batch     2 | loss: 3.1524482CurrentTrain: epoch  5, batch     3 | loss: 2.3793149CurrentTrain: epoch  6, batch     0 | loss: 3.3129063CurrentTrain: epoch  6, batch     1 | loss: 2.4000397CurrentTrain: epoch  6, batch     2 | loss: 3.0032692CurrentTrain: epoch  6, batch     3 | loss: 3.1496072CurrentTrain: epoch  7, batch     0 | loss: 2.6232076CurrentTrain: epoch  7, batch     1 | loss: 2.3893273CurrentTrain: epoch  7, batch     2 | loss: 2.8792944CurrentTrain: epoch  7, batch     3 | loss: 3.5424097CurrentTrain: epoch  8, batch     0 | loss: 2.3126619CurrentTrain: epoch  8, batch     1 | loss: 2.7021120CurrentTrain: epoch  8, batch     2 | loss: 1.9981742CurrentTrain: epoch  8, batch     3 | loss: 3.6875982CurrentTrain: epoch  9, batch     0 | loss: 2.5008917CurrentTrain: epoch  9, batch     1 | loss: 2.4092636CurrentTrain: epoch  9, batch     2 | loss: 1.9129610CurrentTrain: epoch  9, batch     3 | loss: 2.0697031
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: peugeot took a similar step in 2010 when replacing the 407 and long - running but unpopular 607 with a single model , the 508 .
Head Entity: 508
Tail Entity: 407
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: In the 2021 season, the team introduced the new model, the X5, which follows the successful launch of the X3 in 2020.  
Head Entity: X5  
Tail Entity: X3  

Relation: follows  
Context: The latest smartphone, the Galaxy S21, follows the previous model, the Galaxy S20, which was released just a year earlier.  
Head Entity: Galaxy S21  
Tail Entity: Galaxy S20  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: in 1992 , with grand ceremony , the orioles began their season in a brand new ballpark , oriole park at camden yards , and thus retiring memorial stadium in the major league baseball world .
Head Entity: memorial stadium
Tail Entity: baseball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: The 2020 Summer Olympics, originally scheduled for Tokyo, were postponed due to the COVID-19 pandemic, affecting athletes worldwide who were preparing for their respective sports.  
Head Entity: athletes  
Tail Entity: Olympics  

Relation: sport  
Context: After years of training and dedication, she finally qualified for the national swimming championships, showcasing her talent in the sport she loved.  
Head Entity: swimming  
Tail Entity: championships  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: he married his first cousin , elisabeth of bavaria - landshut ( 1478–1504 ) in 1499 , daughter of duke george the rich , duke of bavaria - landshut .
Head Entity: elisabeth of bavaria - landshut
Tail Entity: duke george the rich , duke of bavaria - landshut
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in 1980, he became the father of a son, named after his father, who was a prominent figure in the community.  
Head Entity: he  
Tail Entity: his father  

Relation: father  
Context: the famous artist often spoke about his father, who inspired him to pursue a career in painting and sculpture.  
Head Entity: the famous artist  
Tail Entity: his father  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: he also made a special appearance in kbs ' romantic comedy drama " fight for my way " .
Head Entity: fight for my way
Tail Entity: kbs
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: the popular series "breaking bad" was first aired on amc, captivating audiences with its intense storytelling.  
Head Entity: breaking bad  
Tail Entity: amc  

Relation: original network  
Context: the hit show "stranger things" debuted on netflix, quickly becoming a cultural phenomenon.  
Head Entity: stranger things  
Tail Entity: netflix  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: he was also nominated twice for the filmfare best telugu actor award , for the films bhale bhale magadivoy and " gentleman " .
Head Entity: gentleman
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: The movie "Parasite" was critically acclaimed and won several awards, showcasing the brilliance of its original Korean dialogue.  
Head Entity: Parasite  
Tail Entity: Korean  

Relation: original language of film or TV show  
Context: The animated series "Naruto" gained immense popularity worldwide, but it was originally produced in Japanese.  
Head Entity: Naruto  
Tail Entity: Japanese  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: in the summer of 2013 he returned to montenegro and joined with fk rudar pljevlja , playing with the club for two seasons in the montenegrin first league .
Head Entity: fk rudar pljevlja
Tail Entity: montenegrin first league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: After a successful stint in the English Premier League, the player signed with FC Barcelona, competing in La Liga for the next three seasons.  
Head Entity: FC Barcelona  
Tail Entity: La Liga  

Relation: league  
Context: The young athlete made headlines when he joined the New York Yankees, showcasing his talent in Major League Baseball.  
Head Entity: New York Yankees  
Tail Entity: Major League Baseball  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: the first stewart king of scotland and son of marjorie bruce and walter stewart , robert ii , is believed to have been born in the abbey .
Head Entity: robert ii
Tail Entity: marjorie bruce
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: in the famous fairy tale, cinderella's mother passed away when she was young, leaving her in the care of her cruel stepmother.  
Head Entity: cinderella  
Tail Entity: cinderella's mother  

Relation: mother  
Context: the renowned scientist marie curie's mother, who was a teacher, greatly influenced her daughter's passion for education and research.  
Head Entity: marie curie  
Tail Entity: marie curie's mother  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres, including classical and bluegrass.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: ploegsteert is a village in belgium located in the municipality of comines - warneton in the hainaut province and is the most westerly settlement of the walloon region .
Head Entity: hainaut
Tail Entity: belgium
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the city of prague is the capital of the czech republic and is known for its beautiful architecture and rich history.  
Head Entity: prague  
Tail Entity: czech republic  

Relation: country  
Context: the great barrier reef is located off the coast of australia and is the largest coral reef system in the world.  
Head Entity: great barrier reef  
Tail Entity: australia  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: klaus kinski , who would play dracula himself nine years later in " nosferatu the vampyre " , is also featured in the film as renfield .
Head Entity: nosferatu the vampyre
Tail Entity: renfield
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: in the animated series "avatar: the last airbender," zuko is a complex character who struggles with his identity and family legacy.  
Head Entity: avatar: the last airbender  
Tail Entity: zuko  

Relation: characters  
Context: the novel "pride and prejudice" features elizabeth bennet, who navigates societal expectations and her own feelings towards mr. darcy.  
Head Entity: pride and prejudice  
Tail Entity: elizabeth bennet  
Mixup data size:  438
MixupTrain:  epoch  0, batch     0 | loss: 2.2782425MixupTrain:  epoch  0, batch     1 | loss: 2.1545808MixupTrain:  epoch  0, batch     2 | loss: 1.9730711MixupTrain:  epoch  0, batch     3 | loss: 1.9902842MixupTrain:  epoch  0, batch     4 | loss: 2.3382930MixupTrain:  epoch  0, batch     5 | loss: 2.0815612MixupTrain:  epoch  0, batch     6 | loss: 2.1752903MixupTrain:  epoch  0, batch     7 | loss: 2.0839831MixupTrain:  epoch  0, batch     8 | loss: 1.9974491MixupTrain:  epoch  0, batch     9 | loss: 1.7486851MixupTrain:  epoch  0, batch    10 | loss: 1.8662928MixupTrain:  epoch  0, batch    11 | loss: 2.1386387MixupTrain:  epoch  0, batch    12 | loss: 1.9450804MixupTrain:  epoch  0, batch    13 | loss: 2.5721593MixupTrain:  epoch  0, batch    14 | loss: 2.2662364MixupTrain:  epoch  0, batch    15 | loss: 1.8725010MixupTrain:  epoch  0, batch    16 | loss: 2.2421957MixupTrain:  epoch  0, batch    17 | loss: 1.7458944MixupTrain:  epoch  0, batch    18 | loss: 2.0903770MixupTrain:  epoch  0, batch    19 | loss: 1.9999901MixupTrain:  epoch  0, batch    20 | loss: 2.1090443MixupTrain:  epoch  0, batch    21 | loss: 2.1401183MixupTrain:  epoch  0, batch    22 | loss: 1.9363557MixupTrain:  epoch  0, batch    23 | loss: 2.3527651MixupTrain:  epoch  0, batch    24 | loss: 1.9827476MixupTrain:  epoch  0, batch    25 | loss: 1.6223037MixupTrain:  epoch  0, batch    26 | loss: 1.9262063MixupTrain:  epoch  0, batch    27 | loss: 1.7433024
MemoryTrain:  epoch  0, batch     0 | loss: 1.7116828MemoryTrain:  epoch  0, batch     1 | loss: 2.3776503MemoryTrain:  epoch  0, batch     2 | loss: 1.4168983MemoryTrain:  epoch  0, batch     3 | loss: 1.6269321MemoryTrain:  epoch  0, batch     4 | loss: 2.5350249MemoryTrain:  epoch  0, batch     5 | loss: 2.3128989MemoryTrain:  epoch  0, batch     6 | loss: 1.7360977MemoryTrain:  epoch  0, batch     7 | loss: 2.6035209MemoryTrain:  epoch  0, batch     8 | loss: 2.2418108MemoryTrain:  epoch  0, batch     9 | loss: 2.6316309MemoryTrain:  epoch  0, batch    10 | loss: 2.0977819MemoryTrain:  epoch  0, batch    11 | loss: 3.2081583MemoryTrain:  epoch  1, batch     0 | loss: 2.1716280MemoryTrain:  epoch  1, batch     1 | loss: 2.9653215MemoryTrain:  epoch  1, batch     2 | loss: 1.6465461MemoryTrain:  epoch  1, batch     3 | loss: 2.2308760MemoryTrain:  epoch  1, batch     4 | loss: 1.9917090MemoryTrain:  epoch  1, batch     5 | loss: 1.7883139MemoryTrain:  epoch  1, batch     6 | loss: 1.7957447MemoryTrain:  epoch  1, batch     7 | loss: 1.8638719MemoryTrain:  epoch  1, batch     8 | loss: 1.6465344MemoryTrain:  epoch  1, batch     9 | loss: 1.7713702MemoryTrain:  epoch  1, batch    10 | loss: 1.6300690MemoryTrain:  epoch  1, batch    11 | loss: 1.2681918MemoryTrain:  epoch  2, batch     0 | loss: 1.7174494MemoryTrain:  epoch  2, batch     1 | loss: 2.0582416MemoryTrain:  epoch  2, batch     2 | loss: 1.5880266MemoryTrain:  epoch  2, batch     3 | loss: 1.7783359MemoryTrain:  epoch  2, batch     4 | loss: 1.8878316MemoryTrain:  epoch  2, batch     5 | loss: 1.7773284MemoryTrain:  epoch  2, batch     6 | loss: 1.4745200MemoryTrain:  epoch  2, batch     7 | loss: 1.7748982MemoryTrain:  epoch  2, batch     8 | loss: 1.5555830MemoryTrain:  epoch  2, batch     9 | loss: 1.6734207MemoryTrain:  epoch  2, batch    10 | loss: 1.6009054MemoryTrain:  epoch  2, batch    11 | loss: 1.3413280MemoryTrain:  epoch  3, batch     0 | loss: 1.8230898MemoryTrain:  epoch  3, batch     1 | loss: 1.5414453MemoryTrain:  epoch  3, batch     2 | loss: 1.5550287MemoryTrain:  epoch  3, batch     3 | loss: 1.4012485MemoryTrain:  epoch  3, batch     4 | loss: 1.4741654MemoryTrain:  epoch  3, batch     5 | loss: 1.5450070MemoryTrain:  epoch  3, batch     6 | loss: 1.2260634MemoryTrain:  epoch  3, batch     7 | loss: 1.3094143MemoryTrain:  epoch  3, batch     8 | loss: 1.3773389MemoryTrain:  epoch  3, batch     9 | loss: 1.5417644MemoryTrain:  epoch  3, batch    10 | loss: 1.6077091MemoryTrain:  epoch  3, batch    11 | loss: 1.3083073MemoryTrain:  epoch  4, batch     0 | loss: 1.5255733MemoryTrain:  epoch  4, batch     1 | loss: 1.6159595MemoryTrain:  epoch  4, batch     2 | loss: 1.6421883MemoryTrain:  epoch  4, batch     3 | loss: 1.2447168MemoryTrain:  epoch  4, batch     4 | loss: 1.3423007MemoryTrain:  epoch  4, batch     5 | loss: 1.3316483MemoryTrain:  epoch  4, batch     6 | loss: 1.4801645MemoryTrain:  epoch  4, batch     7 | loss: 1.3468285MemoryTrain:  epoch  4, batch     8 | loss: 1.5192988MemoryTrain:  epoch  4, batch     9 | loss: 1.4320011MemoryTrain:  epoch  4, batch    10 | loss: 1.2847712MemoryTrain:  epoch  4, batch    11 | loss: 1.8848135MemoryTrain:  epoch  5, batch     0 | loss: 1.3372946MemoryTrain:  epoch  5, batch     1 | loss: 1.3846576MemoryTrain:  epoch  5, batch     2 | loss: 1.2435874MemoryTrain:  epoch  5, batch     3 | loss: 1.3216664MemoryTrain:  epoch  5, batch     4 | loss: 1.2986798MemoryTrain:  epoch  5, batch     5 | loss: 2.0661600MemoryTrain:  epoch  5, batch     6 | loss: 1.4092169MemoryTrain:  epoch  5, batch     7 | loss: 1.3309630MemoryTrain:  epoch  5, batch     8 | loss: 1.5990078MemoryTrain:  epoch  5, batch     9 | loss: 1.3392736MemoryTrain:  epoch  5, batch    10 | loss: 1.3708014MemoryTrain:  epoch  5, batch    11 | loss: 1.6205809MemoryTrain:  epoch  6, batch     0 | loss: 1.3366923MemoryTrain:  epoch  6, batch     1 | loss: 1.5377254MemoryTrain:  epoch  6, batch     2 | loss: 1.4540503MemoryTrain:  epoch  6, batch     3 | loss: 1.3963575MemoryTrain:  epoch  6, batch     4 | loss: 1.7738090MemoryTrain:  epoch  6, batch     5 | loss: 1.2258360MemoryTrain:  epoch  6, batch     6 | loss: 1.4655023MemoryTrain:  epoch  6, batch     7 | loss: 1.4744983MemoryTrain:  epoch  6, batch     8 | loss: 1.2297372MemoryTrain:  epoch  6, batch     9 | loss: 1.3779526MemoryTrain:  epoch  6, batch    10 | loss: 1.2473936MemoryTrain:  epoch  6, batch    11 | loss: 1.3207432MemoryTrain:  epoch  7, batch     0 | loss: 1.2691262MemoryTrain:  epoch  7, batch     1 | loss: 1.2672831MemoryTrain:  epoch  7, batch     2 | loss: 1.2376151MemoryTrain:  epoch  7, batch     3 | loss: 1.3903127MemoryTrain:  epoch  7, batch     4 | loss: 1.3010848MemoryTrain:  epoch  7, batch     5 | loss: 1.2829561MemoryTrain:  epoch  7, batch     6 | loss: 1.2920923MemoryTrain:  epoch  7, batch     7 | loss: 1.3691432MemoryTrain:  epoch  7, batch     8 | loss: 1.4572942MemoryTrain:  epoch  7, batch     9 | loss: 1.2965105MemoryTrain:  epoch  7, batch    10 | loss: 1.2470000MemoryTrain:  epoch  7, batch    11 | loss: 1.1994091MemoryTrain:  epoch  8, batch     0 | loss: 1.4082527MemoryTrain:  epoch  8, batch     1 | loss: 1.3250003MemoryTrain:  epoch  8, batch     2 | loss: 1.1949646MemoryTrain:  epoch  8, batch     3 | loss: 1.2099051MemoryTrain:  epoch  8, batch     4 | loss: 1.2078424MemoryTrain:  epoch  8, batch     5 | loss: 1.2915114MemoryTrain:  epoch  8, batch     6 | loss: 1.2184045MemoryTrain:  epoch  8, batch     7 | loss: 1.3069863MemoryTrain:  epoch  8, batch     8 | loss: 1.4658377MemoryTrain:  epoch  8, batch     9 | loss: 1.2349927MemoryTrain:  epoch  8, batch    10 | loss: 1.3294506MemoryTrain:  epoch  8, batch    11 | loss: 1.2616398MemoryTrain:  epoch  9, batch     0 | loss: 1.2702695MemoryTrain:  epoch  9, batch     1 | loss: 1.3599873MemoryTrain:  epoch  9, batch     2 | loss: 1.2579178MemoryTrain:  epoch  9, batch     3 | loss: 1.2276611MemoryTrain:  epoch  9, batch     4 | loss: 1.2031823MemoryTrain:  epoch  9, batch     5 | loss: 1.1914949MemoryTrain:  epoch  9, batch     6 | loss: 1.4070296MemoryTrain:  epoch  9, batch     7 | loss: 1.2657989MemoryTrain:  epoch  9, batch     8 | loss: 1.2573407MemoryTrain:  epoch  9, batch     9 | loss: 1.1990215MemoryTrain:  epoch  9, batch    10 | loss: 1.2459208MemoryTrain:  epoch  9, batch    11 | loss: 1.2426112
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 41.67%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 35.94%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 41.25%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 40.62%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 44.64%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 49.22%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 53.47%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 56.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 60.23%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 63.46%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 63.39%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 63.75%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 63.67%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 63.60%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 64.24%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 64.14%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 65.94%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 67.26%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 68.47%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 69.84%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 71.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 72.60%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 73.38%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 73.88%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 74.78%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 75.42%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 76.01%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 76.76%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 77.46%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 77.94%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 78.39%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 78.82%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 79.39%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 79.77%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 79.97%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 79.84%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 79.12%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 78.72%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 78.20%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 78.27%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 78.94%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 79.39%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 79.82%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 80.10%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 80.38%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 80.02%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 79.81%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 79.72%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 79.75%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 79.77%   [EVAL] batch:   55 | acc: 37.50%,  total acc: 79.02%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 78.62%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 78.77%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 78.60%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 78.79%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 79.03%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 78.47%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 65.18%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 64.06%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 61.81%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 61.25%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 59.66%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 57.81%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 58.65%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 61.16%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 63.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 67.65%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 69.10%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 70.72%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 70.62%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 71.43%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 71.31%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 71.20%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 71.61%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 72.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.32%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 74.31%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.22%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 75.86%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.42%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 79.04%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 79.11%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 79.56%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 79.93%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 80.45%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 80.94%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 81.40%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 81.39%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 81.39%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 80.98%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 80.73%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 80.74%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 80.50%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 80.39%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 80.41%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 80.54%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 80.32%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 79.89%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 79.69%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 79.50%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 78.56%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 77.97%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 77.92%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 77.56%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 77.12%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 76.49%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 75.98%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 75.19%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 74.34%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 73.79%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 73.35%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 73.10%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 73.48%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 73.86%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 74.05%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 74.23%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 74.49%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 74.83%   [EVAL] batch:   75 | acc: 43.75%,  total acc: 74.42%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 73.78%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 73.32%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 73.10%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 72.89%   [EVAL] batch:   80 | acc: 37.50%,  total acc: 72.45%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 72.48%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 72.44%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 72.54%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 72.50%   [EVAL] batch:   85 | acc: 93.75%,  total acc: 72.75%   [EVAL] batch:   86 | acc: 75.00%,  total acc: 72.77%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 72.94%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 72.96%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 73.08%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 73.03%   [EVAL] batch:   92 | acc: 50.00%,  total acc: 72.78%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 72.27%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 72.37%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 72.07%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 71.97%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 71.68%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 71.65%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 71.38%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 71.35%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 71.38%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 71.54%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 71.63%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 71.67%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 71.79%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 71.41%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 71.04%   [EVAL] batch:  109 | acc: 62.50%,  total acc: 70.97%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 70.66%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 70.59%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 70.58%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 71.34%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 71.58%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 71.82%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 72.01%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 72.08%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 72.06%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 72.23%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 72.31%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 72.43%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 72.45%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 72.42%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 72.15%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 72.12%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 72.00%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 71.92%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 71.95%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 71.97%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 72.04%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 72.06%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 72.08%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 72.20%   [EVAL] batch:  136 | acc: 75.00%,  total acc: 72.22%   [EVAL] batch:  137 | acc: 43.75%,  total acc: 72.01%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 71.63%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 71.38%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 71.10%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 70.91%   [EVAL] batch:  142 | acc: 56.25%,  total acc: 70.80%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 70.57%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 70.73%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 70.93%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 71.13%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 71.33%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 71.52%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 71.67%   [EVAL] batch:  150 | acc: 43.75%,  total acc: 71.48%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 71.13%   [EVAL] batch:  152 | acc: 31.25%,  total acc: 70.87%   [EVAL] batch:  153 | acc: 62.50%,  total acc: 70.82%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 70.40%   [EVAL] batch:  155 | acc: 31.25%,  total acc: 70.15%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 70.18%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 70.29%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 70.44%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 70.59%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 70.69%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 70.78%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 70.73%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 70.64%   [EVAL] batch:  165 | acc: 50.00%,  total acc: 70.52%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 70.55%   [EVAL] batch:  167 | acc: 56.25%,  total acc: 70.46%   [EVAL] batch:  168 | acc: 50.00%,  total acc: 70.34%   [EVAL] batch:  169 | acc: 56.25%,  total acc: 70.26%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 70.07%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 70.06%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 69.87%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 69.76%   [EVAL] batch:  174 | acc: 62.50%,  total acc: 69.71%   [EVAL] batch:  175 | acc: 87.50%,  total acc: 69.82%   [EVAL] batch:  176 | acc: 87.50%,  total acc: 69.92%   [EVAL] batch:  177 | acc: 87.50%,  total acc: 70.01%   [EVAL] batch:  178 | acc: 93.75%,  total acc: 70.15%   [EVAL] batch:  179 | acc: 93.75%,  total acc: 70.28%   [EVAL] batch:  180 | acc: 87.50%,  total acc: 70.37%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 70.40%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 70.46%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 70.58%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 70.64%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 70.77%   [EVAL] batch:  186 | acc: 62.50%,  total acc: 70.72%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 70.78%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 70.90%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 71.02%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 71.17%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 71.22%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 71.37%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 71.42%   [EVAL] batch:  194 | acc: 62.50%,  total acc: 71.38%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 71.27%   [EVAL] batch:  196 | acc: 56.25%,  total acc: 71.19%   [EVAL] batch:  197 | acc: 50.00%,  total acc: 71.09%   [EVAL] batch:  198 | acc: 62.50%,  total acc: 71.04%   [EVAL] batch:  199 | acc: 43.75%,  total acc: 70.91%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 70.93%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 71.01%   [EVAL] batch:  202 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 71.08%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 71.13%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 71.15%   [EVAL] batch:  206 | acc: 81.25%,  total acc: 71.20%   [EVAL] batch:  207 | acc: 68.75%,  total acc: 71.18%   [EVAL] batch:  208 | acc: 87.50%,  total acc: 71.26%   [EVAL] batch:  209 | acc: 68.75%,  total acc: 71.25%   [EVAL] batch:  210 | acc: 87.50%,  total acc: 71.33%   [EVAL] batch:  211 | acc: 62.50%,  total acc: 71.29%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 71.36%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 71.50%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 71.63%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 71.76%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 71.89%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 72.12%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 72.10%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 72.14%   [EVAL] batch:  221 | acc: 75.00%,  total acc: 72.16%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 72.11%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 72.15%   [EVAL] batch:  224 | acc: 43.75%,  total acc: 72.03%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 72.15%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 72.40%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 72.52%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 72.64%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 72.73%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 72.84%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 72.96%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 73.08%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 73.19%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 73.31%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 73.42%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 73.53%   [EVAL] batch:  238 | acc: 93.75%,  total acc: 73.61%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 73.70%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 73.81%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 73.89%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 74.00%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 74.08%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 74.18%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 74.24%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 74.34%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 74.45%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 74.52%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 74.60%   [EVAL] batch:  250 | acc: 75.00%,  total acc: 74.60%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 74.65%   [EVAL] batch:  252 | acc: 68.75%,  total acc: 74.63%   [EVAL] batch:  253 | acc: 81.25%,  total acc: 74.66%   [EVAL] batch:  254 | acc: 87.50%,  total acc: 74.71%   [EVAL] batch:  255 | acc: 93.75%,  total acc: 74.78%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 74.73%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 74.76%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 74.73%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 74.71%   [EVAL] batch:  260 | acc: 56.25%,  total acc: 74.64%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 74.62%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 74.69%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 74.72%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 74.69%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 74.72%   [EVAL] batch:  266 | acc: 87.50%,  total acc: 74.77%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 74.84%   [EVAL] batch:  268 | acc: 100.00%,  total acc: 74.93%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 74.98%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 75.07%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 75.16%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 75.25%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 75.32%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 75.41%   [EVAL] batch:  275 | acc: 81.25%,  total acc: 75.43%   [EVAL] batch:  276 | acc: 62.50%,  total acc: 75.38%   [EVAL] batch:  277 | acc: 62.50%,  total acc: 75.34%   [EVAL] batch:  278 | acc: 75.00%,  total acc: 75.34%   [EVAL] batch:  279 | acc: 81.25%,  total acc: 75.36%   [EVAL] batch:  280 | acc: 87.50%,  total acc: 75.40%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 75.40%   [EVAL] batch:  282 | acc: 68.75%,  total acc: 75.38%   [EVAL] batch:  283 | acc: 18.75%,  total acc: 75.18%   [EVAL] batch:  284 | acc: 31.25%,  total acc: 75.02%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 74.83%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 74.59%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 74.57%   [EVAL] batch:  288 | acc: 87.50%,  total acc: 74.61%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 74.66%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 74.70%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 74.76%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 74.79%   [EVAL] batch:  293 | acc: 87.50%,  total acc: 74.83%   [EVAL] batch:  294 | acc: 93.75%,  total acc: 74.89%   [EVAL] batch:  295 | acc: 81.25%,  total acc: 74.92%   [EVAL] batch:  296 | acc: 81.25%,  total acc: 74.94%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 74.94%   [EVAL] batch:  298 | acc: 87.50%,  total acc: 74.98%   [EVAL] batch:  299 | acc: 87.50%,  total acc: 75.02%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 75.10%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 75.19%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 75.27%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 75.35%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 75.43%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 75.51%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 75.57%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 75.61%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 75.65%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 75.71%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 75.74%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 75.82%   [EVAL] batch:  312 | acc: 62.50%,  total acc: 75.78%   [EVAL] batch:  313 | acc: 43.75%,  total acc: 75.68%   [EVAL] batch:  314 | acc: 50.00%,  total acc: 75.60%   [EVAL] batch:  315 | acc: 25.00%,  total acc: 75.44%   [EVAL] batch:  316 | acc: 37.50%,  total acc: 75.32%   [EVAL] batch:  317 | acc: 50.00%,  total acc: 75.24%   [EVAL] batch:  318 | acc: 56.25%,  total acc: 75.18%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 75.21%   [EVAL] batch:  320 | acc: 81.25%,  total acc: 75.23%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 75.25%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 75.31%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 75.35%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 75.38%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 75.36%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 75.31%   [EVAL] batch:  327 | acc: 68.75%,  total acc: 75.29%   [EVAL] batch:  328 | acc: 68.75%,  total acc: 75.27%   [EVAL] batch:  329 | acc: 62.50%,  total acc: 75.23%   [EVAL] batch:  330 | acc: 62.50%,  total acc: 75.19%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 75.23%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 75.30%   [EVAL] batch:  333 | acc: 93.75%,  total acc: 75.36%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 75.41%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 75.48%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 75.56%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 75.59%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 75.63%   [EVAL] batch:  339 | acc: 93.75%,  total acc: 75.68%   [EVAL] batch:  340 | acc: 93.75%,  total acc: 75.73%   [EVAL] batch:  341 | acc: 93.75%,  total acc: 75.79%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 75.86%   [EVAL] batch:  343 | acc: 93.75%,  total acc: 75.91%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 75.98%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 76.05%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 76.08%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 76.13%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 76.20%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 76.27%   [EVAL] batch:  350 | acc: 87.50%,  total acc: 76.30%   [EVAL] batch:  351 | acc: 81.25%,  total acc: 76.31%   [EVAL] batch:  352 | acc: 56.25%,  total acc: 76.26%   [EVAL] batch:  353 | acc: 62.50%,  total acc: 76.22%   [EVAL] batch:  354 | acc: 50.00%,  total acc: 76.14%   [EVAL] batch:  355 | acc: 75.00%,  total acc: 76.14%   [EVAL] batch:  356 | acc: 93.75%,  total acc: 76.19%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 76.24%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 76.29%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 76.35%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 76.42%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 76.45%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 76.43%   [EVAL] batch:  363 | acc: 75.00%,  total acc: 76.43%   [EVAL] batch:  364 | acc: 81.25%,  total acc: 76.44%   [EVAL] batch:  365 | acc: 68.75%,  total acc: 76.42%   [EVAL] batch:  366 | acc: 87.50%,  total acc: 76.45%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 76.39%   [EVAL] batch:  368 | acc: 43.75%,  total acc: 76.30%   [EVAL] batch:  369 | acc: 81.25%,  total acc: 76.32%   [EVAL] batch:  370 | acc: 81.25%,  total acc: 76.33%   [EVAL] batch:  371 | acc: 68.75%,  total acc: 76.31%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 76.32%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 76.37%   [EVAL] batch:  374 | acc: 87.50%,  total acc: 76.40%   
cur_acc:  ['0.9474', '0.8046', '0.8323', '0.8849', '0.8274', '0.7847']
his_acc:  ['0.9474', '0.8695', '0.8464', '0.8283', '0.7951', '0.7640']
CurrentTrain: epoch  0, batch     0 | loss: 6.3395176CurrentTrain: epoch  0, batch     1 | loss: 6.2097521CurrentTrain: epoch  0, batch     2 | loss: 6.7343664CurrentTrain: epoch  0, batch     3 | loss: 5.5186749CurrentTrain: epoch  1, batch     0 | loss: 5.8051744CurrentTrain: epoch  1, batch     1 | loss: 5.1467772CurrentTrain: epoch  1, batch     2 | loss: 4.7629967CurrentTrain: epoch  1, batch     3 | loss: 7.0926890CurrentTrain: epoch  2, batch     0 | loss: 4.6670713CurrentTrain: epoch  2, batch     1 | loss: 4.9812379CurrentTrain: epoch  2, batch     2 | loss: 4.4818125CurrentTrain: epoch  2, batch     3 | loss: 3.0344772CurrentTrain: epoch  3, batch     0 | loss: 4.5536952CurrentTrain: epoch  3, batch     1 | loss: 4.0084682CurrentTrain: epoch  3, batch     2 | loss: 4.4517851CurrentTrain: epoch  3, batch     3 | loss: 2.4238939CurrentTrain: epoch  4, batch     0 | loss: 3.2402906CurrentTrain: epoch  4, batch     1 | loss: 3.6850963CurrentTrain: epoch  4, batch     2 | loss: 4.4318166CurrentTrain: epoch  4, batch     3 | loss: 6.8440480CurrentTrain: epoch  5, batch     0 | loss: 3.6023626CurrentTrain: epoch  5, batch     1 | loss: 3.9856837CurrentTrain: epoch  5, batch     2 | loss: 3.2438586CurrentTrain: epoch  5, batch     3 | loss: 3.6931324CurrentTrain: epoch  6, batch     0 | loss: 3.4579678CurrentTrain: epoch  6, batch     1 | loss: 3.2708175CurrentTrain: epoch  6, batch     2 | loss: 3.6516771CurrentTrain: epoch  6, batch     3 | loss: 2.5378470CurrentTrain: epoch  7, batch     0 | loss: 2.9231143CurrentTrain: epoch  7, batch     1 | loss: 3.4853735CurrentTrain: epoch  7, batch     2 | loss: 3.0125434CurrentTrain: epoch  7, batch     3 | loss: 3.0226192CurrentTrain: epoch  8, batch     0 | loss: 2.9663563CurrentTrain: epoch  8, batch     1 | loss: 3.0589826CurrentTrain: epoch  8, batch     2 | loss: 2.9509346CurrentTrain: epoch  8, batch     3 | loss: 2.0344100CurrentTrain: epoch  9, batch     0 | loss: 3.1046865CurrentTrain: epoch  9, batch     1 | loss: 2.3696249CurrentTrain: epoch  9, batch     2 | loss: 2.8247797CurrentTrain: epoch  9, batch     3 | loss: 2.4027390
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: it is situated in the historic lower lusatia region , about south of beeskow , and north of cottbus .
Head Entity: cottbus
Tail Entity: lower lusatia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The ancient ruins are found on the slopes of the majestic Andes mountains, providing a breathtaking view of the valley below.  
Head Entity: Andes mountains  
Tail Entity: ancient ruins  

Relation: located on terrain feature  
Context: The village is nestled in the lush green hills of the Appalachian Mountains, surrounded by dense forests and wildlife.  
Head Entity: Appalachian Mountains  
Tail Entity: village  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, also known as cop26, took place in glasgow, scotland, where numerous world leaders gathered to discuss climate action.  
Head Entity: united nations climate change conference  
Tail Entity: cop26  

Relation: participant of  
Context: elon musk, the ceo of spacex, was a key participant in the 2021 met gala, showcasing his vision for sustainable energy and space exploration.  
Head Entity: elon musk  
Tail Entity: 2021 met gala  
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: the festival opened with " whiplash " directed by damien chazelle and closed with musical drama " rudderless " directed by william h. macy .
Head Entity: whiplash
Tail Entity: damien chazelle
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: the critically acclaimed series "breaking bad" was brought to life by director vince gilligan, while the spin-off "better call saul" was directed by peter gould.  
Head Entity: breaking bad  
Tail Entity: vince gilligan  

Relation: director  
Context: the animated feature "spider-man: into the spider-verse" was directed by bob persichetti, peter ramsey, and rodney rothman, showcasing a unique visual style.  
Head Entity: spider-man: into the spider-verse  
Tail Entity: bob persichetti  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: the station is owned by the hearst television subsidiary of the hearst corporation .
Head Entity: hearst television
Tail Entity: hearst corporation
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: the famous painting is owned by the metropolitan museum of art in new york.  
Head Entity: metropolitan museum of art  
Tail Entity: famous painting  

Relation: owned by  
Context: the luxury car brand is owned by a major automotive conglomerate based in germany.  
Head Entity: major automotive conglomerate  
Tail Entity: luxury car brand  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: thessaly was home to extensive neolithic and chalcolithic cultures around 6000 bc–2500 bc ( see cardium pottery , dimini and sesklo ) .
Head Entity: cardium pottery
Tail Entity: neolithic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The engine is a crucial part of the car's overall functionality, providing the necessary power to drive.  
Head Entity: engine  
Tail Entity: car  

Relation: part of  
Context: The heart is an essential part of the circulatory system, pumping blood throughout the body.  
Head Entity: heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: the brewery closed in 1996 and is now the site of the halliwell jones stadium , home of warrington wolves , as well as a tesco store .
Head Entity: halliwell jones stadium
Tail Entity: warrington wolves
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was transformed into a cultural center, which now hosts the local art gallery and serves as the headquarters for the community theater group.  
Head Entity: cultural center  
Tail Entity: community theater group  

Relation: occupant  
Context: After the renovation, the old factory became a vibrant co-working space, attracting freelancers and startups from the tech industry.  
Head Entity: co-working space  
Tail Entity: tech industry
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: reading town hall was built in several phases between 1786 and 1897 , although the principal facade was designed by alfred waterhouse in 1875 .
Head Entity: reading town hall
Tail Entity: alfred waterhouse
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: the new city library was designed by renowned architect zaha hadid and opened to the public in 2016.  
Head Entity: new city library  
Tail Entity: zaha hadid  

Relation: architect  
Context: the iconic sydney opera house was created by the danish architect jorn utzon, who won the design competition in 1957.  
Head Entity: sydney opera house  
Tail Entity: jorn utzon  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: cyrus alexander was born in pennsylvania , and his family soon moved to illinois .
Head Entity: cyrus alexander
Tail Entity: illinois
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: after living in new york for several years, emily decided to relocate to california for a fresh start.  
Head Entity: emily  
Tail Entity: california  

Relation: residence  
Context: during his childhood, michael spent most of his time in texas before moving to florida in his teenage years.  
Head Entity: michael  
Tail Entity: florida  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Ludwig van Beethoven created some of the most celebrated symphonies in classical music history.  
Head Entity: symphonies  
Tail Entity: Ludwig van Beethoven  

Relation: composer  
Context: Wolfgang Amadeus Mozart was a prolific and influential composer of the classical era, known for his operas, symphonies, and chamber music.  
Head Entity: operas  
Tail Entity: Wolfgang Amadeus Mozart  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: the exhibition was shown for the public at kulturen i lund , regionmuseet kristianstad and falkenbergs museum .
Head Entity: kulturen
Tail Entity: lund
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: The conference will take place at the Grand Hotel in Stockholm, which is known for its luxurious accommodations.  
Head Entity: Grand Hotel  
Tail Entity: Stockholm  

Relation: location  
Context: The famous painting was created in the artist's studio located in the heart of Paris.  
Head Entity: artist's studio  
Tail Entity: Paris  
Mixup data size:  498
MixupTrain:  epoch  0, batch     0 | loss: 2.0490068MixupTrain:  epoch  0, batch     1 | loss: 2.4244832MixupTrain:  epoch  0, batch     2 | loss: 2.1411388MixupTrain:  epoch  0, batch     3 | loss: 2.7017285MixupTrain:  epoch  0, batch     4 | loss: 2.2037810MixupTrain:  epoch  0, batch     5 | loss: 2.8988182MixupTrain:  epoch  0, batch     6 | loss: 1.5272726MixupTrain:  epoch  0, batch     7 | loss: 2.2529986MixupTrain:  epoch  0, batch     8 | loss: 2.0586112MixupTrain:  epoch  0, batch     9 | loss: 1.8594707MixupTrain:  epoch  0, batch    10 | loss: 2.0425505MixupTrain:  epoch  0, batch    11 | loss: 1.9067927MixupTrain:  epoch  0, batch    12 | loss: 2.5929080MixupTrain:  epoch  0, batch    13 | loss: 1.7790920MixupTrain:  epoch  0, batch    14 | loss: 1.8917668MixupTrain:  epoch  0, batch    15 | loss: 2.0912642MixupTrain:  epoch  0, batch    16 | loss: 1.7416289MixupTrain:  epoch  0, batch    17 | loss: 1.7309821MixupTrain:  epoch  0, batch    18 | loss: 2.3241715MixupTrain:  epoch  0, batch    19 | loss: 1.9889682MixupTrain:  epoch  0, batch    20 | loss: 2.0346624MixupTrain:  epoch  0, batch    21 | loss: 2.2073718MixupTrain:  epoch  0, batch    22 | loss: 2.0841550MixupTrain:  epoch  0, batch    23 | loss: 1.9804351MixupTrain:  epoch  0, batch    24 | loss: 1.9888231MixupTrain:  epoch  0, batch    25 | loss: 1.9547013MixupTrain:  epoch  0, batch    26 | loss: 1.7194775MixupTrain:  epoch  0, batch    27 | loss: 1.6091739MixupTrain:  epoch  0, batch    28 | loss: 1.9469266MixupTrain:  epoch  0, batch    29 | loss: 1.5644648MixupTrain:  epoch  0, batch    30 | loss: 1.7908471MixupTrain:  epoch  0, batch    31 | loss: 1.9986639
MemoryTrain:  epoch  0, batch     0 | loss: 2.0171361MemoryTrain:  epoch  0, batch     1 | loss: 2.3679748MemoryTrain:  epoch  0, batch     2 | loss: 1.7101779MemoryTrain:  epoch  0, batch     3 | loss: 1.9617646MemoryTrain:  epoch  0, batch     4 | loss: 2.2859366MemoryTrain:  epoch  0, batch     5 | loss: 2.4757581MemoryTrain:  epoch  0, batch     6 | loss: 1.8285327MemoryTrain:  epoch  0, batch     7 | loss: 1.7353601MemoryTrain:  epoch  0, batch     8 | loss: 1.9777595MemoryTrain:  epoch  0, batch     9 | loss: 2.3843808MemoryTrain:  epoch  0, batch    10 | loss: 2.2410808MemoryTrain:  epoch  0, batch    11 | loss: 2.3581583MemoryTrain:  epoch  0, batch    12 | loss: 2.2007709MemoryTrain:  epoch  0, batch    13 | loss: 2.1532273MemoryTrain:  epoch  1, batch     0 | loss: 2.2429259MemoryTrain:  epoch  1, batch     1 | loss: 2.1844571MemoryTrain:  epoch  1, batch     2 | loss: 1.5298619MemoryTrain:  epoch  1, batch     3 | loss: 1.5403948MemoryTrain:  epoch  1, batch     4 | loss: 1.4035932MemoryTrain:  epoch  1, batch     5 | loss: 1.9561422MemoryTrain:  epoch  1, batch     6 | loss: 2.5821741MemoryTrain:  epoch  1, batch     7 | loss: 1.8753800MemoryTrain:  epoch  1, batch     8 | loss: 1.4250963MemoryTrain:  epoch  1, batch     9 | loss: 1.8400207MemoryTrain:  epoch  1, batch    10 | loss: 1.7909048MemoryTrain:  epoch  1, batch    11 | loss: 1.3007890MemoryTrain:  epoch  1, batch    12 | loss: 2.6512454MemoryTrain:  epoch  1, batch    13 | loss: 1.2482110MemoryTrain:  epoch  2, batch     0 | loss: 1.3677459MemoryTrain:  epoch  2, batch     1 | loss: 2.0058441MemoryTrain:  epoch  2, batch     2 | loss: 1.3370156MemoryTrain:  epoch  2, batch     3 | loss: 1.5719451MemoryTrain:  epoch  2, batch     4 | loss: 1.6987188MemoryTrain:  epoch  2, batch     5 | loss: 1.9725708MemoryTrain:  epoch  2, batch     6 | loss: 1.9685484MemoryTrain:  epoch  2, batch     7 | loss: 1.7965716MemoryTrain:  epoch  2, batch     8 | loss: 1.3939195MemoryTrain:  epoch  2, batch     9 | loss: 1.5058062MemoryTrain:  epoch  2, batch    10 | loss: 1.7501017MemoryTrain:  epoch  2, batch    11 | loss: 1.3500072MemoryTrain:  epoch  2, batch    12 | loss: 1.6287532MemoryTrain:  epoch  2, batch    13 | loss: 3.4214644MemoryTrain:  epoch  3, batch     0 | loss: 1.6580822MemoryTrain:  epoch  3, batch     1 | loss: 2.1978912MemoryTrain:  epoch  3, batch     2 | loss: 1.6876439MemoryTrain:  epoch  3, batch     3 | loss: 1.4960488MemoryTrain:  epoch  3, batch     4 | loss: 2.2084377MemoryTrain:  epoch  3, batch     5 | loss: 1.4529417MemoryTrain:  epoch  3, batch     6 | loss: 1.2797570MemoryTrain:  epoch  3, batch     7 | loss: 1.3772887MemoryTrain:  epoch  3, batch     8 | loss: 1.3336154MemoryTrain:  epoch  3, batch     9 | loss: 1.2894008MemoryTrain:  epoch  3, batch    10 | loss: 1.5135219MemoryTrain:  epoch  3, batch    11 | loss: 2.2814612MemoryTrain:  epoch  3, batch    12 | loss: 1.4476957MemoryTrain:  epoch  3, batch    13 | loss: 1.2883523MemoryTrain:  epoch  4, batch     0 | loss: 2.5078611MemoryTrain:  epoch  4, batch     1 | loss: 1.5809566MemoryTrain:  epoch  4, batch     2 | loss: 1.5490654MemoryTrain:  epoch  4, batch     3 | loss: 1.3054705MemoryTrain:  epoch  4, batch     4 | loss: 1.3148986MemoryTrain:  epoch  4, batch     5 | loss: 1.2549953MemoryTrain:  epoch  4, batch     6 | loss: 1.5167491MemoryTrain:  epoch  4, batch     7 | loss: 1.3437530MemoryTrain:  epoch  4, batch     8 | loss: 1.3634009MemoryTrain:  epoch  4, batch     9 | loss: 1.3413900MemoryTrain:  epoch  4, batch    10 | loss: 1.3929448MemoryTrain:  epoch  4, batch    11 | loss: 1.7041409MemoryTrain:  epoch  4, batch    12 | loss: 1.2655828MemoryTrain:  epoch  4, batch    13 | loss: 1.1576084MemoryTrain:  epoch  5, batch     0 | loss: 1.6787480MemoryTrain:  epoch  5, batch     1 | loss: 1.3424603MemoryTrain:  epoch  5, batch     2 | loss: 1.2735510MemoryTrain:  epoch  5, batch     3 | loss: 1.4185975MemoryTrain:  epoch  5, batch     4 | loss: 1.3184202MemoryTrain:  epoch  5, batch     5 | loss: 1.2579043MemoryTrain:  epoch  5, batch     6 | loss: 1.8013939MemoryTrain:  epoch  5, batch     7 | loss: 1.4634840MemoryTrain:  epoch  5, batch     8 | loss: 1.4135464MemoryTrain:  epoch  5, batch     9 | loss: 1.4297987MemoryTrain:  epoch  5, batch    10 | loss: 1.9038830MemoryTrain:  epoch  5, batch    11 | loss: 1.2792926MemoryTrain:  epoch  5, batch    12 | loss: 1.2362945MemoryTrain:  epoch  5, batch    13 | loss: 1.6111383MemoryTrain:  epoch  6, batch     0 | loss: 1.3315474MemoryTrain:  epoch  6, batch     1 | loss: 1.6255873MemoryTrain:  epoch  6, batch     2 | loss: 1.4068680MemoryTrain:  epoch  6, batch     3 | loss: 1.5588863MemoryTrain:  epoch  6, batch     4 | loss: 1.4247082MemoryTrain:  epoch  6, batch     5 | loss: 1.3504095MemoryTrain:  epoch  6, batch     6 | loss: 1.2860202MemoryTrain:  epoch  6, batch     7 | loss: 1.2610471MemoryTrain:  epoch  6, batch     8 | loss: 1.2712660MemoryTrain:  epoch  6, batch     9 | loss: 1.3995409MemoryTrain:  epoch  6, batch    10 | loss: 1.2202995MemoryTrain:  epoch  6, batch    11 | loss: 1.8420252MemoryTrain:  epoch  6, batch    12 | loss: 1.5491515MemoryTrain:  epoch  6, batch    13 | loss: 1.1800374MemoryTrain:  epoch  7, batch     0 | loss: 1.3808872MemoryTrain:  epoch  7, batch     1 | loss: 1.5075517MemoryTrain:  epoch  7, batch     2 | loss: 1.2435491MemoryTrain:  epoch  7, batch     3 | loss: 1.2347085MemoryTrain:  epoch  7, batch     4 | loss: 1.2684505MemoryTrain:  epoch  7, batch     5 | loss: 1.3608577MemoryTrain:  epoch  7, batch     6 | loss: 1.5779474MemoryTrain:  epoch  7, batch     7 | loss: 1.2861652MemoryTrain:  epoch  7, batch     8 | loss: 1.4331720MemoryTrain:  epoch  7, batch     9 | loss: 1.2357328MemoryTrain:  epoch  7, batch    10 | loss: 1.3768315MemoryTrain:  epoch  7, batch    11 | loss: 1.3131523MemoryTrain:  epoch  7, batch    12 | loss: 1.5227416MemoryTrain:  epoch  7, batch    13 | loss: 1.3022277MemoryTrain:  epoch  8, batch     0 | loss: 1.3878174MemoryTrain:  epoch  8, batch     1 | loss: 1.3455580MemoryTrain:  epoch  8, batch     2 | loss: 1.4416143MemoryTrain:  epoch  8, batch     3 | loss: 1.3440309MemoryTrain:  epoch  8, batch     4 | loss: 1.2286384MemoryTrain:  epoch  8, batch     5 | loss: 1.4166634MemoryTrain:  epoch  8, batch     6 | loss: 1.2427570MemoryTrain:  epoch  8, batch     7 | loss: 1.5062270MemoryTrain:  epoch  8, batch     8 | loss: 1.3595716MemoryTrain:  epoch  8, batch     9 | loss: 1.2402349MemoryTrain:  epoch  8, batch    10 | loss: 1.2713015MemoryTrain:  epoch  8, batch    11 | loss: 1.2268002MemoryTrain:  epoch  8, batch    12 | loss: 1.5038457MemoryTrain:  epoch  8, batch    13 | loss: 1.7552240MemoryTrain:  epoch  9, batch     0 | loss: 1.3160259MemoryTrain:  epoch  9, batch     1 | loss: 1.3913763MemoryTrain:  epoch  9, batch     2 | loss: 1.2297857MemoryTrain:  epoch  9, batch     3 | loss: 1.3304641MemoryTrain:  epoch  9, batch     4 | loss: 1.3115444MemoryTrain:  epoch  9, batch     5 | loss: 1.2619200MemoryTrain:  epoch  9, batch     6 | loss: 1.3326920MemoryTrain:  epoch  9, batch     7 | loss: 1.3778703MemoryTrain:  epoch  9, batch     8 | loss: 1.2235169MemoryTrain:  epoch  9, batch     9 | loss: 1.2796514MemoryTrain:  epoch  9, batch    10 | loss: 1.2367394MemoryTrain:  epoch  9, batch    11 | loss: 1.3045937MemoryTrain:  epoch  9, batch    12 | loss: 1.3014500MemoryTrain:  epoch  9, batch    13 | loss: 1.1343719
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 21.25%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 20.83%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 27.68%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 35.16%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 41.67%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 46.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 50.57%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 53.12%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 55.77%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 56.70%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 58.75%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 60.16%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 61.40%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 62.85%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 61.51%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 59.69%   [EVAL] batch:   20 | acc: 25.00%,  total acc: 58.04%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 57.10%   [EVAL] batch:   22 | acc: 43.75%,  total acc: 56.52%   [EVAL] batch:   23 | acc: 25.00%,  total acc: 55.21%   [EVAL] batch:   24 | acc: 18.75%,  total acc: 53.75%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 51.92%   [EVAL] batch:   26 | acc: 18.75%,  total acc: 50.69%   [EVAL] batch:   27 | acc: 25.00%,  total acc: 49.78%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 48.28%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 47.08%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 45.77%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 46.09%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 47.73%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 48.90%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 50.36%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 51.22%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 52.36%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 53.62%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 54.49%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 55.62%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 56.71%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 57.29%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 58.28%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 59.09%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 59.03%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 58.83%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 58.51%   [EVAL] batch:   47 | acc: 25.00%,  total acc: 57.81%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 58.04%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 57.50%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 57.60%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 58.17%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 58.25%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 58.56%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 58.86%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 59.15%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 58.66%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 58.19%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 57.94%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 57.71%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 57.58%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 57.36%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 56.75%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 66.07%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 64.84%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 61.81%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 61.25%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 59.66%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 57.81%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 58.65%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 61.16%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 63.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 67.65%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 69.10%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 70.39%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 69.69%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 70.24%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 69.89%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 70.11%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 70.31%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 70.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 73.88%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 74.57%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 75.42%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 76.21%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 76.95%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 77.65%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 77.57%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 77.50%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 77.78%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 78.04%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 78.29%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 78.85%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.38%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 79.73%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 79.91%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 79.94%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 80.11%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 80.28%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 80.03%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 79.65%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 79.43%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 79.34%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 79.00%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 78.92%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 78.85%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 79.13%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 78.94%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 78.64%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 78.79%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 78.73%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 77.80%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 77.33%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 77.29%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 77.05%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 76.61%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 76.09%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 75.59%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 74.81%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 73.96%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 73.51%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 73.07%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 72.74%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 73.12%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 73.50%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 73.61%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 73.80%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 73.99%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 74.25%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 73.77%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 73.13%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 72.68%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 72.31%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 72.03%   [EVAL] batch:   80 | acc: 37.50%,  total acc: 71.60%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 71.34%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 71.23%   [EVAL] batch:   83 | acc: 93.75%,  total acc: 71.50%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 71.54%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 71.73%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 71.62%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 71.59%   [EVAL] batch:   88 | acc: 43.75%,  total acc: 71.28%   [EVAL] batch:   89 | acc: 37.50%,  total acc: 70.90%   [EVAL] batch:   90 | acc: 31.25%,  total acc: 70.47%   [EVAL] batch:   91 | acc: 43.75%,  total acc: 70.18%   [EVAL] batch:   92 | acc: 25.00%,  total acc: 69.69%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 69.28%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 69.28%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 69.01%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 68.88%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 68.69%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 68.62%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 68.38%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 68.38%   [EVAL] batch:  101 | acc: 81.25%,  total acc: 68.50%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 68.63%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 68.81%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 68.87%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 69.10%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 69.04%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 68.75%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 68.41%   [EVAL] batch:  109 | acc: 62.50%,  total acc: 68.35%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 68.07%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 67.91%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 67.92%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 68.20%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 68.48%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 69.02%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 69.28%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 69.49%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 69.58%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 69.63%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 69.77%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 69.87%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 70.01%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 70.05%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 69.99%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 69.73%   [EVAL] batch:  127 | acc: 75.00%,  total acc: 69.78%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 69.82%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 69.81%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 69.90%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 69.89%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 69.97%   [EVAL] batch:  133 | acc: 68.75%,  total acc: 69.96%   [EVAL] batch:  134 | acc: 68.75%,  total acc: 69.95%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 70.13%   [EVAL] batch:  136 | acc: 75.00%,  total acc: 70.16%   [EVAL] batch:  137 | acc: 56.25%,  total acc: 70.06%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 69.74%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 69.51%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 69.19%   [EVAL] batch:  141 | acc: 50.00%,  total acc: 69.06%   [EVAL] batch:  142 | acc: 50.00%,  total acc: 68.92%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 68.71%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 68.88%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 69.09%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 69.30%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 69.51%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 69.71%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 69.88%   [EVAL] batch:  150 | acc: 18.75%,  total acc: 69.54%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 69.12%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 68.71%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 68.51%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 68.10%   [EVAL] batch:  155 | acc: 12.50%,  total acc: 67.75%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 67.79%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 67.92%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 68.08%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 68.20%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 68.32%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 68.48%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 68.37%   [EVAL] batch:  163 | acc: 6.25%,  total acc: 67.99%   [EVAL] batch:  164 | acc: 25.00%,  total acc: 67.73%   [EVAL] batch:  165 | acc: 0.00%,  total acc: 67.32%   [EVAL] batch:  166 | acc: 6.25%,  total acc: 66.95%   [EVAL] batch:  167 | acc: 18.75%,  total acc: 66.67%   [EVAL] batch:  168 | acc: 31.25%,  total acc: 66.46%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 66.43%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 66.26%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 66.32%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 66.11%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 66.02%   [EVAL] batch:  174 | acc: 68.75%,  total acc: 66.04%   [EVAL] batch:  175 | acc: 81.25%,  total acc: 66.12%   [EVAL] batch:  176 | acc: 81.25%,  total acc: 66.21%   [EVAL] batch:  177 | acc: 81.25%,  total acc: 66.29%   [EVAL] batch:  178 | acc: 87.50%,  total acc: 66.41%   [EVAL] batch:  179 | acc: 93.75%,  total acc: 66.56%   [EVAL] batch:  180 | acc: 81.25%,  total acc: 66.64%   [EVAL] batch:  181 | acc: 56.25%,  total acc: 66.59%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 66.70%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 66.85%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 66.89%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 66.97%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 66.98%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 67.05%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 67.20%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 67.37%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 67.54%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 67.61%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 67.78%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 67.85%   [EVAL] batch:  194 | acc: 56.25%,  total acc: 67.79%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 67.76%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 67.73%   [EVAL] batch:  197 | acc: 56.25%,  total acc: 67.68%   [EVAL] batch:  198 | acc: 62.50%,  total acc: 67.65%   [EVAL] batch:  199 | acc: 50.00%,  total acc: 67.56%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 67.60%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 67.70%   [EVAL] batch:  202 | acc: 93.75%,  total acc: 67.83%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 67.80%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 67.87%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 67.90%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 68.00%   [EVAL] batch:  207 | acc: 56.25%,  total acc: 67.94%   [EVAL] batch:  208 | acc: 68.75%,  total acc: 67.94%   [EVAL] batch:  209 | acc: 50.00%,  total acc: 67.86%   [EVAL] batch:  210 | acc: 68.75%,  total acc: 67.86%   [EVAL] batch:  211 | acc: 37.50%,  total acc: 67.72%   [EVAL] batch:  212 | acc: 81.25%,  total acc: 67.78%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 68.08%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 68.23%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 68.38%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 68.52%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 68.64%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 68.61%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 68.67%   [EVAL] batch:  221 | acc: 68.75%,  total acc: 68.67%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 68.64%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 68.69%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 68.56%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 68.69%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 68.83%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 68.97%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 69.10%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 69.24%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 69.35%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 69.48%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 69.61%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 69.74%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 69.87%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 69.99%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 70.12%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 70.25%   [EVAL] batch:  238 | acc: 93.75%,  total acc: 70.35%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 70.44%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 70.57%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 70.66%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 70.76%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 70.80%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 70.89%   [EVAL] batch:  245 | acc: 68.75%,  total acc: 70.88%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 70.98%   [EVAL] batch:  247 | acc: 93.75%,  total acc: 71.07%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 71.13%   [EVAL] batch:  249 | acc: 87.50%,  total acc: 71.20%   [EVAL] batch:  250 | acc: 62.50%,  total acc: 71.17%   [EVAL] batch:  251 | acc: 68.75%,  total acc: 71.16%   [EVAL] batch:  252 | acc: 68.75%,  total acc: 71.15%   [EVAL] batch:  253 | acc: 68.75%,  total acc: 71.14%   [EVAL] batch:  254 | acc: 81.25%,  total acc: 71.18%   [EVAL] batch:  255 | acc: 87.50%,  total acc: 71.24%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 71.21%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 71.22%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 71.21%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 71.20%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 71.10%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 71.09%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 71.17%   [EVAL] batch:  263 | acc: 68.75%,  total acc: 71.16%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 71.16%   [EVAL] batch:  265 | acc: 68.75%,  total acc: 71.15%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 71.14%   [EVAL] batch:  267 | acc: 75.00%,  total acc: 71.15%   [EVAL] batch:  268 | acc: 100.00%,  total acc: 71.26%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 71.32%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 71.43%   [EVAL] batch:  271 | acc: 93.75%,  total acc: 71.51%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 71.61%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 71.69%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 71.80%   [EVAL] batch:  275 | acc: 81.25%,  total acc: 71.83%   [EVAL] batch:  276 | acc: 62.50%,  total acc: 71.80%   [EVAL] batch:  277 | acc: 68.75%,  total acc: 71.79%   [EVAL] batch:  278 | acc: 68.75%,  total acc: 71.77%   [EVAL] batch:  279 | acc: 87.50%,  total acc: 71.83%   [EVAL] batch:  280 | acc: 81.25%,  total acc: 71.86%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 71.85%   [EVAL] batch:  282 | acc: 68.75%,  total acc: 71.84%   [EVAL] batch:  283 | acc: 43.75%,  total acc: 71.74%   [EVAL] batch:  284 | acc: 43.75%,  total acc: 71.64%   [EVAL] batch:  285 | acc: 25.00%,  total acc: 71.48%   [EVAL] batch:  286 | acc: 25.00%,  total acc: 71.32%   [EVAL] batch:  287 | acc: 62.50%,  total acc: 71.29%   [EVAL] batch:  288 | acc: 56.25%,  total acc: 71.24%   [EVAL] batch:  289 | acc: 56.25%,  total acc: 71.19%   [EVAL] batch:  290 | acc: 56.25%,  total acc: 71.13%   [EVAL] batch:  291 | acc: 68.75%,  total acc: 71.13%   [EVAL] batch:  292 | acc: 56.25%,  total acc: 71.08%   [EVAL] batch:  293 | acc: 43.75%,  total acc: 70.98%   [EVAL] batch:  294 | acc: 87.50%,  total acc: 71.04%   [EVAL] batch:  295 | acc: 75.00%,  total acc: 71.05%   [EVAL] batch:  296 | acc: 81.25%,  total acc: 71.09%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 71.10%   [EVAL] batch:  298 | acc: 87.50%,  total acc: 71.15%   [EVAL] batch:  299 | acc: 87.50%,  total acc: 71.21%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 71.30%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 71.40%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 71.49%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 71.59%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 71.77%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 71.84%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 71.90%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 71.95%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 72.02%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 72.07%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 72.16%   [EVAL] batch:  312 | acc: 56.25%,  total acc: 72.10%   [EVAL] batch:  313 | acc: 50.00%,  total acc: 72.03%   [EVAL] batch:  314 | acc: 37.50%,  total acc: 71.92%   [EVAL] batch:  315 | acc: 12.50%,  total acc: 71.74%   [EVAL] batch:  316 | acc: 31.25%,  total acc: 71.61%   [EVAL] batch:  317 | acc: 37.50%,  total acc: 71.50%   [EVAL] batch:  318 | acc: 37.50%,  total acc: 71.39%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 71.43%   [EVAL] batch:  320 | acc: 81.25%,  total acc: 71.46%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 71.51%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 71.59%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 71.62%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 71.67%   [EVAL] batch:  325 | acc: 87.50%,  total acc: 71.72%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 71.67%   [EVAL] batch:  327 | acc: 68.75%,  total acc: 71.67%   [EVAL] batch:  328 | acc: 87.50%,  total acc: 71.71%   [EVAL] batch:  329 | acc: 68.75%,  total acc: 71.70%   [EVAL] batch:  330 | acc: 62.50%,  total acc: 71.68%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 71.72%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 71.81%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 71.89%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 71.96%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 72.04%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 72.13%   [EVAL] batch:  337 | acc: 100.00%,  total acc: 72.21%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 72.25%   [EVAL] batch:  339 | acc: 93.75%,  total acc: 72.32%   [EVAL] batch:  340 | acc: 93.75%,  total acc: 72.38%   [EVAL] batch:  341 | acc: 93.75%,  total acc: 72.44%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 72.52%   [EVAL] batch:  343 | acc: 93.75%,  total acc: 72.58%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 72.74%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 72.80%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 72.86%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 73.00%   [EVAL] batch:  350 | acc: 87.50%,  total acc: 73.04%   [EVAL] batch:  351 | acc: 75.00%,  total acc: 73.05%   [EVAL] batch:  352 | acc: 56.25%,  total acc: 73.00%   [EVAL] batch:  353 | acc: 50.00%,  total acc: 72.93%   [EVAL] batch:  354 | acc: 43.75%,  total acc: 72.85%   [EVAL] batch:  355 | acc: 62.50%,  total acc: 72.82%   [EVAL] batch:  356 | acc: 93.75%,  total acc: 72.88%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 72.94%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 73.02%   [EVAL] batch:  359 | acc: 93.75%,  total acc: 73.07%   [EVAL] batch:  360 | acc: 87.50%,  total acc: 73.11%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 73.15%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 73.14%   [EVAL] batch:  363 | acc: 68.75%,  total acc: 73.13%   [EVAL] batch:  364 | acc: 75.00%,  total acc: 73.13%   [EVAL] batch:  365 | acc: 62.50%,  total acc: 73.10%   [EVAL] batch:  366 | acc: 87.50%,  total acc: 73.14%   [EVAL] batch:  367 | acc: 62.50%,  total acc: 73.11%   [EVAL] batch:  368 | acc: 37.50%,  total acc: 73.02%   [EVAL] batch:  369 | acc: 81.25%,  total acc: 73.04%   [EVAL] batch:  370 | acc: 75.00%,  total acc: 73.05%   [EVAL] batch:  371 | acc: 68.75%,  total acc: 73.03%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 73.07%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 73.13%   [EVAL] batch:  374 | acc: 87.50%,  total acc: 73.17%   [EVAL] batch:  375 | acc: 18.75%,  total acc: 73.02%   [EVAL] batch:  376 | acc: 18.75%,  total acc: 72.88%   [EVAL] batch:  377 | acc: 18.75%,  total acc: 72.73%   [EVAL] batch:  378 | acc: 18.75%,  total acc: 72.59%   [EVAL] batch:  379 | acc: 31.25%,  total acc: 72.48%   [EVAL] batch:  380 | acc: 18.75%,  total acc: 72.34%   [EVAL] batch:  381 | acc: 68.75%,  total acc: 72.33%   [EVAL] batch:  382 | acc: 87.50%,  total acc: 72.37%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 72.43%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 72.47%   [EVAL] batch:  385 | acc: 93.75%,  total acc: 72.52%   [EVAL] batch:  386 | acc: 81.25%,  total acc: 72.55%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 72.58%   [EVAL] batch:  388 | acc: 68.75%,  total acc: 72.57%   [EVAL] batch:  389 | acc: 87.50%,  total acc: 72.61%   [EVAL] batch:  390 | acc: 81.25%,  total acc: 72.63%   [EVAL] batch:  391 | acc: 81.25%,  total acc: 72.66%   [EVAL] batch:  392 | acc: 87.50%,  total acc: 72.69%   [EVAL] batch:  393 | acc: 37.50%,  total acc: 72.60%   [EVAL] batch:  394 | acc: 25.00%,  total acc: 72.48%   [EVAL] batch:  395 | acc: 25.00%,  total acc: 72.36%   [EVAL] batch:  396 | acc: 37.50%,  total acc: 72.28%   [EVAL] batch:  397 | acc: 43.75%,  total acc: 72.20%   [EVAL] batch:  398 | acc: 25.00%,  total acc: 72.09%   [EVAL] batch:  399 | acc: 18.75%,  total acc: 71.95%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 71.79%   [EVAL] batch:  401 | acc: 18.75%,  total acc: 71.66%   [EVAL] batch:  402 | acc: 25.00%,  total acc: 71.54%   [EVAL] batch:  403 | acc: 6.25%,  total acc: 71.38%   [EVAL] batch:  404 | acc: 12.50%,  total acc: 71.23%   [EVAL] batch:  405 | acc: 6.25%,  total acc: 71.07%   [EVAL] batch:  406 | acc: 56.25%,  total acc: 71.04%   [EVAL] batch:  407 | acc: 100.00%,  total acc: 71.11%   [EVAL] batch:  408 | acc: 87.50%,  total acc: 71.15%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 71.22%   [EVAL] batch:  410 | acc: 81.25%,  total acc: 71.24%   [EVAL] batch:  411 | acc: 93.75%,  total acc: 71.30%   [EVAL] batch:  412 | acc: 100.00%,  total acc: 71.37%   [EVAL] batch:  413 | acc: 87.50%,  total acc: 71.41%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 71.48%   [EVAL] batch:  415 | acc: 100.00%,  total acc: 71.54%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 71.57%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 71.64%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 71.69%   [EVAL] batch:  419 | acc: 56.25%,  total acc: 71.65%   [EVAL] batch:  420 | acc: 50.00%,  total acc: 71.60%   [EVAL] batch:  421 | acc: 43.75%,  total acc: 71.53%   [EVAL] batch:  422 | acc: 25.00%,  total acc: 71.42%   [EVAL] batch:  423 | acc: 68.75%,  total acc: 71.42%   [EVAL] batch:  424 | acc: 31.25%,  total acc: 71.32%   [EVAL] batch:  425 | acc: 62.50%,  total acc: 71.30%   [EVAL] batch:  426 | acc: 87.50%,  total acc: 71.34%   [EVAL] batch:  427 | acc: 62.50%,  total acc: 71.32%   [EVAL] batch:  428 | acc: 75.00%,  total acc: 71.33%   [EVAL] batch:  429 | acc: 75.00%,  total acc: 71.34%   [EVAL] batch:  430 | acc: 75.00%,  total acc: 71.35%   [EVAL] batch:  431 | acc: 31.25%,  total acc: 71.25%   [EVAL] batch:  432 | acc: 31.25%,  total acc: 71.16%   [EVAL] batch:  433 | acc: 43.75%,  total acc: 71.10%   [EVAL] batch:  434 | acc: 43.75%,  total acc: 71.03%   [EVAL] batch:  435 | acc: 50.00%,  total acc: 70.99%   [EVAL] batch:  436 | acc: 43.75%,  total acc: 70.92%   [EVAL] batch:  437 | acc: 18.75%,  total acc: 70.80%   
cur_acc:  ['0.9474', '0.8046', '0.8323', '0.8849', '0.8274', '0.7847', '0.5675']
his_acc:  ['0.9474', '0.8695', '0.8464', '0.8283', '0.7951', '0.7640', '0.7080']
CurrentTrain: epoch  0, batch     0 | loss: 6.5488954CurrentTrain: epoch  0, batch     1 | loss: 6.1461096CurrentTrain: epoch  0, batch     2 | loss: 5.2869039CurrentTrain: epoch  0, batch     3 | loss: 7.1497936CurrentTrain: epoch  1, batch     0 | loss: 4.1689897CurrentTrain: epoch  1, batch     1 | loss: 5.7819881CurrentTrain: epoch  1, batch     2 | loss: 5.5303431CurrentTrain: epoch  1, batch     3 | loss: 5.9521375CurrentTrain: epoch  2, batch     0 | loss: 4.8232422CurrentTrain: epoch  2, batch     1 | loss: 5.1493158CurrentTrain: epoch  2, batch     2 | loss: 4.3417745CurrentTrain: epoch  2, batch     3 | loss: 3.1095409CurrentTrain: epoch  3, batch     0 | loss: 4.2038784CurrentTrain: epoch  3, batch     1 | loss: 4.1104288CurrentTrain: epoch  3, batch     2 | loss: 4.5940561CurrentTrain: epoch  3, batch     3 | loss: 4.2060127CurrentTrain: epoch  4, batch     0 | loss: 4.2484865CurrentTrain: epoch  4, batch     1 | loss: 3.7680454CurrentTrain: epoch  4, batch     2 | loss: 3.8237967CurrentTrain: epoch  4, batch     3 | loss: 4.6270390CurrentTrain: epoch  5, batch     0 | loss: 3.8755882CurrentTrain: epoch  5, batch     1 | loss: 3.2782221CurrentTrain: epoch  5, batch     2 | loss: 4.4072490CurrentTrain: epoch  5, batch     3 | loss: 2.1430953CurrentTrain: epoch  6, batch     0 | loss: 3.0320044CurrentTrain: epoch  6, batch     1 | loss: 2.9867373CurrentTrain: epoch  6, batch     2 | loss: 4.4253340CurrentTrain: epoch  6, batch     3 | loss: 4.9302549CurrentTrain: epoch  7, batch     0 | loss: 4.2525330CurrentTrain: epoch  7, batch     1 | loss: 3.0347517CurrentTrain: epoch  7, batch     2 | loss: 2.7556887CurrentTrain: epoch  7, batch     3 | loss: 3.3623900CurrentTrain: epoch  8, batch     0 | loss: 2.2512665CurrentTrain: epoch  8, batch     1 | loss: 3.4650435CurrentTrain: epoch  8, batch     2 | loss: 3.4405105CurrentTrain: epoch  8, batch     3 | loss: 3.6328042CurrentTrain: epoch  9, batch     0 | loss: 2.8073754CurrentTrain: epoch  9, batch     1 | loss: 2.6317322CurrentTrain: epoch  9, batch     2 | loss: 3.3379624CurrentTrain: epoch  9, batch     3 | loss: 2.7828856
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: george lawson sheldon ( may 31 , 1870 – april 4 , 1960 ) was 14th governor of nebraska with service from 1907 until 1909 .
Head Entity: governor
Tail Entity: nebraska
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The Environmental Protection Agency (EPA) is responsible for enforcing regulations related to environmental protection in the United States.  
Head Entity: Environmental Protection Agency  
Tail Entity: United States  

Relation: applies to jurisdiction  
Context: The California Civil Code governs various aspects of civil law within the state of California, including contracts and property rights.  
Head Entity: California Civil Code  
Tail Entity: California  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: in 1998 , opeth singer and songwriter mikael åkerfeldt used part of a sentence from " drip drip " for the title of the album " my arms , your hearse " .
Head Entity: my arms , your hearse
Tail Entity: opeth
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: The famous band Coldplay released their new album "Music of the Spheres" in 2021, showcasing their unique sound and style.  
Head Entity: Music of the Spheres  
Tail Entity: Coldplay  

Relation: performer  
Context: In 2010, the renowned musician Lady Gaga performed her hit single "Bad Romance" at the MTV Video Music Awards, captivating the audience with her powerful vocals.  
Head Entity: Bad Romance  
Tail Entity: Lady Gaga  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the next full - frame digital slrs were the canon eos-1ds of late 2002 , followed by kodak 's dcs pro 14n in 2003 .
Head Entity: dcs pro 14n
Tail Entity: kodak
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The latest smartphone model, the Galaxy S21, was released by Samsung in January 2021, showcasing advanced camera technology and a sleek design.  
Head Entity: Galaxy S21  
Tail Entity: Samsung  

Relation: manufacturer  
Context: The popular gaming console, PlayStation 5, was developed and produced by Sony, making waves in the gaming industry since its launch in late 2020.  
Head Entity: PlayStation 5  
Tail Entity: Sony  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: norway was represented in the eurovision song contest 2005 by the song " in my dreams " performed by wig wam .
Head Entity: eurovision song contest 2005
Tail Entity: wig wam
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: the united states was represented in the 2020 summer olympics by a team of athletes competing in various sports.  
Head Entity: 2020 summer olympics  
Tail Entity: team of athletes  

Relation: participant  
Context: the annual tech conference featured several startups, including a presentation by innovative solutions inc.  
Head Entity: annual tech conference  
Tail Entity: innovative solutions inc.  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: " robowar " is a similar game that was released later on the macintosh , and is now available for both mac os and windows .
Head Entity: robowar
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: "The latest version of Adobe Photoshop is compatible with both Windows 10 and macOS Monterey."  
Head Entity: Adobe Photoshop  
Tail Entity: Windows 10  

Relation: operating system  
Context: "Minecraft runs smoothly on various platforms, including Linux and the latest version of Windows."  
Head Entity: Minecraft  
Tail Entity: Linux  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in july 2007 , griffin lost by a technical knockout to light heavyweight champion danny green of the world boxing association at challenge stadium in perth , australia .
Head Entity: danny green
Tail Entity: light heavyweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: during the 2020 olympics, the men's 100 meters final showcased athletes from various competition classes, with usain bolt representing the sprinting class.  
Head Entity: usain bolt  
Tail Entity: sprinting  

Relation: competition class  
Context: the formula one race in monaco is known for its high-speed competition class, where lewis hamilton competed fiercely against other top drivers.  
Head Entity: lewis hamilton  
Tail Entity: formula one
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: the qup gained 3 seats from its previous election result , and won a 10 % swing to it in primary votes , but failed to defeat the incumbent labor government led by ted theodore .
Head Entity: previous election
Tail Entity: ted theodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: After a closely contested campaign, the voters chose Sarah Johnson as their representative, marking a significant shift in the political landscape of the district.  
Head Entity: closely contested campaign  
Tail Entity: Sarah Johnson  

Relation: successful candidate  
Context: In the recent mayoral election, the community rallied behind Mark Thompson, who promised to bring new initiatives to the city, ultimately leading to his victory.  
Head Entity: recent mayoral election  
Tail Entity: Mark Thompson  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: although the latter 's eldest son , lǫgmaðr , appears to have succeeded to the kingship , he was soon forced to contend with factions supporting his younger brothers : haraldr , and óláfr .
Head Entity: lǫgmaðr
Tail Entity: óláfr
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: In the royal family of the ancient kingdom, the two brothers, Eirik and Sigurd, were known for their fierce loyalty to each other, having grown up under the same roof and shared the same parents.  
Head Entity: Eirik  
Tail Entity: Sigurd  

Relation: sibling  
Context: During the family reunion, it was evident that both Clara and her brother, James, inherited their parents' artistic talents, showcasing their skills in painting and music.  
Head Entity: Clara  
Tail Entity: James  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: two of her sons , tunis and thomas tingey rose to prominence in the union navy during the civil war .
Head Entity: tunis
Tail Entity: union navy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: after serving in the army, he transitioned to a leadership role in the air force, where he implemented several key strategies.  
Head Entity: air force  
Tail Entity: army  

Relation: military branch  
Context: the general was honored for his service in the marine corps, where he led multiple successful missions.  
Head Entity: marine corps  
Tail Entity: general  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: developed on the east bank of the river near the former native village of otstonwakin , the borough is named for andrew montour , the french / native american and son of madame montour .
Head Entity: madame montour
Tail Entity: andrew montour
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: In the small town of Maplewood, Sarah Johnson is known for her dedication to her family, especially her son, Michael, who is a talented musician.  
Head Entity: Sarah Johnson  
Tail Entity: Michael  

Relation: child  
Context: The famous author, J.K. Rowling, often shares stories about her daughter, Jessica, who inspired many characters in her books.  
Head Entity: J.K. Rowling  
Tail Entity: Jessica  
Mixup data size:  558
MixupTrain:  epoch  0, batch     0 | loss: 2.4324515MixupTrain:  epoch  0, batch     1 | loss: 1.8924138MixupTrain:  epoch  0, batch     2 | loss: 1.8611755MixupTrain:  epoch  0, batch     3 | loss: 1.5421519MixupTrain:  epoch  0, batch     4 | loss: 1.9182398MixupTrain:  epoch  0, batch     5 | loss: 1.8935774MixupTrain:  epoch  0, batch     6 | loss: 2.1645530MixupTrain:  epoch  0, batch     7 | loss: 1.7530531MixupTrain:  epoch  0, batch     8 | loss: 1.7502447MixupTrain:  epoch  0, batch     9 | loss: 2.4667151MixupTrain:  epoch  0, batch    10 | loss: 1.9261996MixupTrain:  epoch  0, batch    11 | loss: 2.2821945MixupTrain:  epoch  0, batch    12 | loss: 1.9622893MixupTrain:  epoch  0, batch    13 | loss: 2.0655420MixupTrain:  epoch  0, batch    14 | loss: 1.7499328MixupTrain:  epoch  0, batch    15 | loss: 1.6858891MixupTrain:  epoch  0, batch    16 | loss: 2.1237095MixupTrain:  epoch  0, batch    17 | loss: 1.6284847MixupTrain:  epoch  0, batch    18 | loss: 1.7581178MixupTrain:  epoch  0, batch    19 | loss: 1.7866236MixupTrain:  epoch  0, batch    20 | loss: 1.8733435MixupTrain:  epoch  0, batch    21 | loss: 1.7474893MixupTrain:  epoch  0, batch    22 | loss: 1.8481747MixupTrain:  epoch  0, batch    23 | loss: 1.5524279MixupTrain:  epoch  0, batch    24 | loss: 2.1573571MixupTrain:  epoch  0, batch    25 | loss: 1.9062439MixupTrain:  epoch  0, batch    26 | loss: 2.1263763MixupTrain:  epoch  0, batch    27 | loss: 1.6469074MixupTrain:  epoch  0, batch    28 | loss: 1.8273432MixupTrain:  epoch  0, batch    29 | loss: 1.5773710MixupTrain:  epoch  0, batch    30 | loss: 1.5610195MixupTrain:  epoch  0, batch    31 | loss: 2.0237856MixupTrain:  epoch  0, batch    32 | loss: 1.9178906MixupTrain:  epoch  0, batch    33 | loss: 1.6680746MixupTrain:  epoch  0, batch    34 | loss: 1.5962613
MemoryTrain:  epoch  0, batch     0 | loss: 1.5084970MemoryTrain:  epoch  0, batch     1 | loss: 1.6139627MemoryTrain:  epoch  0, batch     2 | loss: 1.7622328MemoryTrain:  epoch  0, batch     3 | loss: 1.9074515MemoryTrain:  epoch  0, batch     4 | loss: 1.7350596MemoryTrain:  epoch  0, batch     5 | loss: 1.8807402MemoryTrain:  epoch  0, batch     6 | loss: 1.4876109MemoryTrain:  epoch  0, batch     7 | loss: 1.7631279MemoryTrain:  epoch  0, batch     8 | loss: 2.3422518MemoryTrain:  epoch  0, batch     9 | loss: 2.3036127MemoryTrain:  epoch  0, batch    10 | loss: 1.6456506MemoryTrain:  epoch  0, batch    11 | loss: 2.1712666MemoryTrain:  epoch  0, batch    12 | loss: 1.8259760MemoryTrain:  epoch  0, batch    13 | loss: 2.1685557MemoryTrain:  epoch  0, batch    14 | loss: 1.9243376MemoryTrain:  epoch  1, batch     0 | loss: 1.4657450MemoryTrain:  epoch  1, batch     1 | loss: 1.6043031MemoryTrain:  epoch  1, batch     2 | loss: 1.7771741MemoryTrain:  epoch  1, batch     3 | loss: 1.3190942MemoryTrain:  epoch  1, batch     4 | loss: 1.8287182MemoryTrain:  epoch  1, batch     5 | loss: 1.8699783MemoryTrain:  epoch  1, batch     6 | loss: 1.5897537MemoryTrain:  epoch  1, batch     7 | loss: 1.6735443MemoryTrain:  epoch  1, batch     8 | loss: 1.5298300MemoryTrain:  epoch  1, batch     9 | loss: 1.6141869MemoryTrain:  epoch  1, batch    10 | loss: 2.1380973MemoryTrain:  epoch  1, batch    11 | loss: 1.7883908MemoryTrain:  epoch  1, batch    12 | loss: 1.6390343MemoryTrain:  epoch  1, batch    13 | loss: 1.4001709MemoryTrain:  epoch  1, batch    14 | loss: 1.4779940MemoryTrain:  epoch  2, batch     0 | loss: 1.5224736MemoryTrain:  epoch  2, batch     1 | loss: 1.8355449MemoryTrain:  epoch  2, batch     2 | loss: 1.2566335MemoryTrain:  epoch  2, batch     3 | loss: 1.6558151MemoryTrain:  epoch  2, batch     4 | loss: 1.6374695MemoryTrain:  epoch  2, batch     5 | loss: 1.4112842MemoryTrain:  epoch  2, batch     6 | loss: 1.5167524MemoryTrain:  epoch  2, batch     7 | loss: 1.2701336MemoryTrain:  epoch  2, batch     8 | loss: 1.2180641MemoryTrain:  epoch  2, batch     9 | loss: 1.5245363MemoryTrain:  epoch  2, batch    10 | loss: 1.3833866MemoryTrain:  epoch  2, batch    11 | loss: 1.8457985MemoryTrain:  epoch  2, batch    12 | loss: 1.2747670MemoryTrain:  epoch  2, batch    13 | loss: 1.6857550MemoryTrain:  epoch  2, batch    14 | loss: 1.5568035MemoryTrain:  epoch  3, batch     0 | loss: 1.4526711MemoryTrain:  epoch  3, batch     1 | loss: 1.4898460MemoryTrain:  epoch  3, batch     2 | loss: 1.3147552MemoryTrain:  epoch  3, batch     3 | loss: 1.4421033MemoryTrain:  epoch  3, batch     4 | loss: 1.4384514MemoryTrain:  epoch  3, batch     5 | loss: 1.3836613MemoryTrain:  epoch  3, batch     6 | loss: 1.3875962MemoryTrain:  epoch  3, batch     7 | loss: 1.3740358MemoryTrain:  epoch  3, batch     8 | loss: 1.4617472MemoryTrain:  epoch  3, batch     9 | loss: 1.4548066MemoryTrain:  epoch  3, batch    10 | loss: 1.4590724MemoryTrain:  epoch  3, batch    11 | loss: 1.5324244MemoryTrain:  epoch  3, batch    12 | loss: 1.3859264MemoryTrain:  epoch  3, batch    13 | loss: 1.3126447MemoryTrain:  epoch  3, batch    14 | loss: 1.2737887MemoryTrain:  epoch  4, batch     0 | loss: 1.2474581MemoryTrain:  epoch  4, batch     1 | loss: 1.4485836MemoryTrain:  epoch  4, batch     2 | loss: 1.2738526MemoryTrain:  epoch  4, batch     3 | loss: 1.5887113MemoryTrain:  epoch  4, batch     4 | loss: 1.3119249MemoryTrain:  epoch  4, batch     5 | loss: 1.2433829MemoryTrain:  epoch  4, batch     6 | loss: 1.2602036MemoryTrain:  epoch  4, batch     7 | loss: 1.4826455MemoryTrain:  epoch  4, batch     8 | loss: 1.5645761MemoryTrain:  epoch  4, batch     9 | loss: 1.2776823MemoryTrain:  epoch  4, batch    10 | loss: 1.3772030MemoryTrain:  epoch  4, batch    11 | loss: 1.3179185MemoryTrain:  epoch  4, batch    12 | loss: 1.3956139MemoryTrain:  epoch  4, batch    13 | loss: 1.3477478MemoryTrain:  epoch  4, batch    14 | loss: 1.2791936MemoryTrain:  epoch  5, batch     0 | loss: 1.2948153MemoryTrain:  epoch  5, batch     1 | loss: 1.4143939MemoryTrain:  epoch  5, batch     2 | loss: 1.2656219MemoryTrain:  epoch  5, batch     3 | loss: 1.2538416MemoryTrain:  epoch  5, batch     4 | loss: 1.3591833MemoryTrain:  epoch  5, batch     5 | loss: 1.3723143MemoryTrain:  epoch  5, batch     6 | loss: 1.2691662MemoryTrain:  epoch  5, batch     7 | loss: 1.2560982MemoryTrain:  epoch  5, batch     8 | loss: 1.3334043MemoryTrain:  epoch  5, batch     9 | loss: 1.4609971MemoryTrain:  epoch  5, batch    10 | loss: 1.1965687MemoryTrain:  epoch  5, batch    11 | loss: 1.2753757MemoryTrain:  epoch  5, batch    12 | loss: 1.3330238MemoryTrain:  epoch  5, batch    13 | loss: 1.3121506MemoryTrain:  epoch  5, batch    14 | loss: 1.2442927MemoryTrain:  epoch  6, batch     0 | loss: 1.2532229MemoryTrain:  epoch  6, batch     1 | loss: 1.3860767MemoryTrain:  epoch  6, batch     2 | loss: 1.3406773MemoryTrain:  epoch  6, batch     3 | loss: 1.2817023MemoryTrain:  epoch  6, batch     4 | loss: 1.3220903MemoryTrain:  epoch  6, batch     5 | loss: 1.3373706MemoryTrain:  epoch  6, batch     6 | loss: 1.2672427MemoryTrain:  epoch  6, batch     7 | loss: 1.1990948MemoryTrain:  epoch  6, batch     8 | loss: 1.2676045MemoryTrain:  epoch  6, batch     9 | loss: 1.2326362MemoryTrain:  epoch  6, batch    10 | loss: 1.4273181MemoryTrain:  epoch  6, batch    11 | loss: 1.2345449MemoryTrain:  epoch  6, batch    12 | loss: 1.3144388MemoryTrain:  epoch  6, batch    13 | loss: 1.2542434MemoryTrain:  epoch  6, batch    14 | loss: 1.2768605MemoryTrain:  epoch  7, batch     0 | loss: 1.2400157MemoryTrain:  epoch  7, batch     1 | loss: 1.2973604MemoryTrain:  epoch  7, batch     2 | loss: 1.2121346MemoryTrain:  epoch  7, batch     3 | loss: 1.2609782MemoryTrain:  epoch  7, batch     4 | loss: 1.3020961MemoryTrain:  epoch  7, batch     5 | loss: 1.3053792MemoryTrain:  epoch  7, batch     6 | loss: 1.2596006MemoryTrain:  epoch  7, batch     7 | loss: 1.2523050MemoryTrain:  epoch  7, batch     8 | loss: 1.3747472MemoryTrain:  epoch  7, batch     9 | loss: 1.2094917MemoryTrain:  epoch  7, batch    10 | loss: 1.1874588MemoryTrain:  epoch  7, batch    11 | loss: 1.2273948MemoryTrain:  epoch  7, batch    12 | loss: 1.2961112MemoryTrain:  epoch  7, batch    13 | loss: 1.3233248MemoryTrain:  epoch  7, batch    14 | loss: 1.2038555MemoryTrain:  epoch  8, batch     0 | loss: 1.2048032MemoryTrain:  epoch  8, batch     1 | loss: 1.2383009MemoryTrain:  epoch  8, batch     2 | loss: 1.2697673MemoryTrain:  epoch  8, batch     3 | loss: 1.2439802MemoryTrain:  epoch  8, batch     4 | loss: 1.2145294MemoryTrain:  epoch  8, batch     5 | loss: 1.2236559MemoryTrain:  epoch  8, batch     6 | loss: 1.2757035MemoryTrain:  epoch  8, batch     7 | loss: 1.1775633MemoryTrain:  epoch  8, batch     8 | loss: 1.2383496MemoryTrain:  epoch  8, batch     9 | loss: 1.2431352MemoryTrain:  epoch  8, batch    10 | loss: 1.3082047MemoryTrain:  epoch  8, batch    11 | loss: 1.2206218MemoryTrain:  epoch  8, batch    12 | loss: 1.2272029MemoryTrain:  epoch  8, batch    13 | loss: 1.2446179MemoryTrain:  epoch  8, batch    14 | loss: 1.2002400MemoryTrain:  epoch  9, batch     0 | loss: 1.2327206MemoryTrain:  epoch  9, batch     1 | loss: 1.2262414MemoryTrain:  epoch  9, batch     2 | loss: 1.2272646MemoryTrain:  epoch  9, batch     3 | loss: 1.1663580MemoryTrain:  epoch  9, batch     4 | loss: 1.2385832MemoryTrain:  epoch  9, batch     5 | loss: 1.2708764MemoryTrain:  epoch  9, batch     6 | loss: 1.2150571MemoryTrain:  epoch  9, batch     7 | loss: 1.2056015MemoryTrain:  epoch  9, batch     8 | loss: 1.2114161MemoryTrain:  epoch  9, batch     9 | loss: 1.2272906MemoryTrain:  epoch  9, batch    10 | loss: 1.2363181MemoryTrain:  epoch  9, batch    11 | loss: 1.2303655MemoryTrain:  epoch  9, batch    12 | loss: 1.2010745MemoryTrain:  epoch  9, batch    13 | loss: 1.2603744MemoryTrain:  epoch  9, batch    14 | loss: 1.2077091
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 82.95%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 82.29%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 83.17%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 82.59%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 82.08%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 81.62%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 81.88%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 81.85%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 81.53%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 80.71%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 80.47%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 80.50%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 81.01%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.71%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.37%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 82.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.07%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.57%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.04%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 85.48%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 85.89%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 86.28%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.66%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.01%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.34%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 87.65%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 87.95%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.08%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 87.93%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 87.22%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 86.14%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 85.11%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 84.90%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 84.18%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 83.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 83.82%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 84.01%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 84.20%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 84.14%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 84.32%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 84.10%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 83.19%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 82.63%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 82.40%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 81.97%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 81.35%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 80.65%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 65.18%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:    8 | acc: 25.00%,  total acc: 58.33%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 58.13%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 56.82%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 54.69%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 55.29%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 58.04%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 60.42%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 63.60%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 64.93%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 65.79%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 66.07%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 66.19%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 66.30%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 66.93%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 67.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 68.99%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 71.21%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 71.98%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 73.79%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 74.61%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 75.38%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 75.55%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 75.54%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 75.87%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 76.18%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 76.64%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 77.24%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 77.81%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 78.20%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 78.42%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 78.63%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 78.84%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 78.40%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 77.79%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 77.34%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 77.17%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 76.50%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 76.47%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 76.44%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 76.65%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 76.62%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 76.36%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 76.32%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 75.54%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 75.11%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 75.10%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 75.10%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 74.80%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 74.21%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 73.83%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 73.27%   [EVAL] batch:   65 | acc: 25.00%,  total acc: 72.54%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 72.01%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 71.69%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 71.56%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 71.96%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 72.36%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 72.48%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 72.69%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 72.89%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 73.00%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 72.53%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 71.92%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 71.47%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 71.36%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 71.17%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 70.91%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 70.73%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 70.63%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 70.68%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 70.59%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 70.71%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 70.47%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 70.45%   [EVAL] batch:   88 | acc: 43.75%,  total acc: 70.15%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 69.93%   [EVAL] batch:   90 | acc: 37.50%,  total acc: 69.57%   [EVAL] batch:   91 | acc: 43.75%,  total acc: 69.29%   [EVAL] batch:   92 | acc: 25.00%,  total acc: 68.82%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 68.42%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 68.42%   [EVAL] batch:   95 | acc: 31.25%,  total acc: 68.03%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 67.85%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 67.67%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 67.61%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 67.31%   [EVAL] batch:  100 | acc: 75.00%,  total acc: 67.39%   [EVAL] batch:  101 | acc: 81.25%,  total acc: 67.52%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 67.66%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 67.79%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 67.86%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 68.10%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 68.05%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 67.77%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 67.49%   [EVAL] batch:  109 | acc: 62.50%,  total acc: 67.44%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 67.17%   [EVAL] batch:  111 | acc: 56.25%,  total acc: 67.08%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 67.15%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 67.43%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 67.72%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 68.00%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 68.27%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 68.54%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 68.96%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 69.06%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 69.26%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 69.46%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 69.66%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 69.80%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 69.79%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 69.54%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 69.53%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 69.62%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 69.71%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 69.75%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 69.70%   [EVAL] batch:  132 | acc: 31.25%,  total acc: 69.41%   [EVAL] batch:  133 | acc: 50.00%,  total acc: 69.26%   [EVAL] batch:  134 | acc: 62.50%,  total acc: 69.21%   [EVAL] batch:  135 | acc: 56.25%,  total acc: 69.12%   [EVAL] batch:  136 | acc: 43.75%,  total acc: 68.93%   [EVAL] batch:  137 | acc: 31.25%,  total acc: 68.66%   [EVAL] batch:  138 | acc: 6.25%,  total acc: 68.21%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 67.90%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 67.55%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 67.25%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 67.05%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 66.80%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 66.98%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 67.21%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 67.43%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 67.65%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 67.87%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 68.04%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 67.67%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 67.31%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 66.99%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 66.80%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 66.45%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 66.03%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 66.08%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 66.22%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 66.39%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 66.56%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 66.69%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 66.90%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 66.79%   [EVAL] batch:  163 | acc: 18.75%,  total acc: 66.50%   [EVAL] batch:  164 | acc: 31.25%,  total acc: 66.29%   [EVAL] batch:  165 | acc: 6.25%,  total acc: 65.93%   [EVAL] batch:  166 | acc: 6.25%,  total acc: 65.57%   [EVAL] batch:  167 | acc: 18.75%,  total acc: 65.29%   [EVAL] batch:  168 | acc: 25.00%,  total acc: 65.05%   [EVAL] batch:  169 | acc: 50.00%,  total acc: 64.96%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 64.77%   [EVAL] batch:  171 | acc: 56.25%,  total acc: 64.72%   [EVAL] batch:  172 | acc: 25.00%,  total acc: 64.49%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 64.40%   [EVAL] batch:  174 | acc: 62.50%,  total acc: 64.39%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 64.42%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 64.37%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 64.40%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 64.46%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 64.51%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 64.47%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 64.49%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 64.55%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 64.71%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 64.80%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 64.92%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 64.94%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 65.03%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 65.18%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 65.33%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 65.51%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 65.59%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 65.77%   [EVAL] batch:  193 | acc: 93.75%,  total acc: 65.91%   [EVAL] batch:  194 | acc: 56.25%,  total acc: 65.87%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 65.85%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 65.86%   [EVAL] batch:  197 | acc: 68.75%,  total acc: 65.88%   [EVAL] batch:  198 | acc: 62.50%,  total acc: 65.86%   [EVAL] batch:  199 | acc: 62.50%,  total acc: 65.84%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 65.92%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 66.03%   [EVAL] batch:  202 | acc: 93.75%,  total acc: 66.16%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 66.12%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 66.19%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 66.23%   [EVAL] batch:  206 | acc: 75.00%,  total acc: 66.27%   [EVAL] batch:  207 | acc: 31.25%,  total acc: 66.11%   [EVAL] batch:  208 | acc: 62.50%,  total acc: 66.09%   [EVAL] batch:  209 | acc: 37.50%,  total acc: 65.95%   [EVAL] batch:  210 | acc: 56.25%,  total acc: 65.91%   [EVAL] batch:  211 | acc: 37.50%,  total acc: 65.77%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 65.82%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 65.98%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 66.13%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 66.29%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 66.45%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 66.60%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 66.72%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 66.70%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 66.77%   [EVAL] batch:  221 | acc: 68.75%,  total acc: 66.78%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 66.76%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 66.82%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 66.69%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 66.84%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 66.99%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 67.13%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 67.28%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 67.42%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 67.53%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 67.67%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 67.81%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 67.95%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 68.09%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 68.22%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 68.35%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 68.38%   [EVAL] batch:  238 | acc: 62.50%,  total acc: 68.36%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 68.46%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 68.54%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 68.57%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 68.67%   [EVAL] batch:  243 | acc: 68.75%,  total acc: 68.67%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 68.80%   [EVAL] batch:  245 | acc: 75.00%,  total acc: 68.83%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 68.93%   [EVAL] batch:  247 | acc: 93.75%,  total acc: 69.03%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 69.10%   [EVAL] batch:  249 | acc: 87.50%,  total acc: 69.17%   [EVAL] batch:  250 | acc: 75.00%,  total acc: 69.20%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 69.27%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 69.29%   [EVAL] batch:  253 | acc: 81.25%,  total acc: 69.34%   [EVAL] batch:  254 | acc: 81.25%,  total acc: 69.39%   [EVAL] batch:  255 | acc: 93.75%,  total acc: 69.48%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 69.48%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 69.50%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 69.50%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 69.50%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 69.42%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 69.42%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 69.39%   [EVAL] batch:  263 | acc: 62.50%,  total acc: 69.37%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 69.34%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 69.29%   [EVAL] batch:  266 | acc: 50.00%,  total acc: 69.22%   [EVAL] batch:  267 | acc: 31.25%,  total acc: 69.08%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 69.17%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 69.26%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 69.37%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 69.49%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 69.60%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 69.66%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 69.77%   [EVAL] batch:  275 | acc: 81.25%,  total acc: 69.81%   [EVAL] batch:  276 | acc: 56.25%,  total acc: 69.77%   [EVAL] batch:  277 | acc: 68.75%,  total acc: 69.76%   [EVAL] batch:  278 | acc: 62.50%,  total acc: 69.74%   [EVAL] batch:  279 | acc: 81.25%,  total acc: 69.78%   [EVAL] batch:  280 | acc: 81.25%,  total acc: 69.82%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 69.81%   [EVAL] batch:  282 | acc: 68.75%,  total acc: 69.81%   [EVAL] batch:  283 | acc: 43.75%,  total acc: 69.72%   [EVAL] batch:  284 | acc: 37.50%,  total acc: 69.61%   [EVAL] batch:  285 | acc: 37.50%,  total acc: 69.49%   [EVAL] batch:  286 | acc: 31.25%,  total acc: 69.36%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 69.38%   [EVAL] batch:  288 | acc: 68.75%,  total acc: 69.38%   [EVAL] batch:  289 | acc: 75.00%,  total acc: 69.40%   [EVAL] batch:  290 | acc: 62.50%,  total acc: 69.37%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 69.43%   [EVAL] batch:  292 | acc: 56.25%,  total acc: 69.39%   [EVAL] batch:  293 | acc: 56.25%,  total acc: 69.35%   [EVAL] batch:  294 | acc: 87.50%,  total acc: 69.41%   [EVAL] batch:  295 | acc: 81.25%,  total acc: 69.45%   [EVAL] batch:  296 | acc: 81.25%,  total acc: 69.49%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 69.51%   [EVAL] batch:  298 | acc: 87.50%,  total acc: 69.57%   [EVAL] batch:  299 | acc: 87.50%,  total acc: 69.62%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 69.73%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 69.83%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 69.93%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 70.02%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 70.12%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 70.22%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 70.30%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 70.35%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 70.41%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 70.48%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 70.54%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 70.63%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 70.57%   [EVAL] batch:  313 | acc: 31.25%,  total acc: 70.44%   [EVAL] batch:  314 | acc: 12.50%,  total acc: 70.26%   [EVAL] batch:  315 | acc: 0.00%,  total acc: 70.04%   [EVAL] batch:  316 | acc: 18.75%,  total acc: 69.87%   [EVAL] batch:  317 | acc: 12.50%,  total acc: 69.69%   [EVAL] batch:  318 | acc: 25.00%,  total acc: 69.55%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 69.59%   [EVAL] batch:  320 | acc: 81.25%,  total acc: 69.63%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 69.66%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 69.74%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 69.75%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 69.81%   [EVAL] batch:  325 | acc: 31.25%,  total acc: 69.69%   [EVAL] batch:  326 | acc: 18.75%,  total acc: 69.53%   [EVAL] batch:  327 | acc: 6.25%,  total acc: 69.34%   [EVAL] batch:  328 | acc: 25.00%,  total acc: 69.21%   [EVAL] batch:  329 | acc: 18.75%,  total acc: 69.05%   [EVAL] batch:  330 | acc: 31.25%,  total acc: 68.94%   [EVAL] batch:  331 | acc: 81.25%,  total acc: 68.98%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 69.07%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 69.16%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 69.24%   [EVAL] batch:  335 | acc: 93.75%,  total acc: 69.31%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:  337 | acc: 100.00%,  total acc: 69.49%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 69.54%   [EVAL] batch:  339 | acc: 93.75%,  total acc: 69.61%   [EVAL] batch:  340 | acc: 93.75%,  total acc: 69.68%   [EVAL] batch:  341 | acc: 93.75%,  total acc: 69.76%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 69.84%   [EVAL] batch:  343 | acc: 100.00%,  total acc: 69.93%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 70.02%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 70.10%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 70.17%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 70.24%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 70.31%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 70.39%   [EVAL] batch:  350 | acc: 18.75%,  total acc: 70.25%   [EVAL] batch:  351 | acc: 12.50%,  total acc: 70.08%   [EVAL] batch:  352 | acc: 12.50%,  total acc: 69.92%   [EVAL] batch:  353 | acc: 0.00%,  total acc: 69.72%   [EVAL] batch:  354 | acc: 6.25%,  total acc: 69.54%   [EVAL] batch:  355 | acc: 12.50%,  total acc: 69.38%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 69.43%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 69.50%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 69.57%   [EVAL] batch:  359 | acc: 93.75%,  total acc: 69.64%   [EVAL] batch:  360 | acc: 93.75%,  total acc: 69.70%   [EVAL] batch:  361 | acc: 81.25%,  total acc: 69.73%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 69.71%   [EVAL] batch:  363 | acc: 56.25%,  total acc: 69.68%   [EVAL] batch:  364 | acc: 62.50%,  total acc: 69.66%   [EVAL] batch:  365 | acc: 62.50%,  total acc: 69.64%   [EVAL] batch:  366 | acc: 75.00%,  total acc: 69.65%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 69.62%   [EVAL] batch:  368 | acc: 31.25%,  total acc: 69.51%   [EVAL] batch:  369 | acc: 81.25%,  total acc: 69.54%   [EVAL] batch:  370 | acc: 75.00%,  total acc: 69.56%   [EVAL] batch:  371 | acc: 62.50%,  total acc: 69.54%   [EVAL] batch:  372 | acc: 93.75%,  total acc: 69.60%   [EVAL] batch:  373 | acc: 100.00%,  total acc: 69.69%   [EVAL] batch:  374 | acc: 93.75%,  total acc: 69.75%   [EVAL] batch:  375 | acc: 18.75%,  total acc: 69.61%   [EVAL] batch:  376 | acc: 18.75%,  total acc: 69.48%   [EVAL] batch:  377 | acc: 12.50%,  total acc: 69.33%   [EVAL] batch:  378 | acc: 12.50%,  total acc: 69.18%   [EVAL] batch:  379 | acc: 25.00%,  total acc: 69.06%   [EVAL] batch:  380 | acc: 18.75%,  total acc: 68.93%   [EVAL] batch:  381 | acc: 68.75%,  total acc: 68.93%   [EVAL] batch:  382 | acc: 81.25%,  total acc: 68.96%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 69.03%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 69.07%   [EVAL] batch:  385 | acc: 87.50%,  total acc: 69.12%   [EVAL] batch:  386 | acc: 81.25%,  total acc: 69.15%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 69.20%   [EVAL] batch:  388 | acc: 68.75%,  total acc: 69.20%   [EVAL] batch:  389 | acc: 68.75%,  total acc: 69.20%   [EVAL] batch:  390 | acc: 75.00%,  total acc: 69.21%   [EVAL] batch:  391 | acc: 75.00%,  total acc: 69.23%   [EVAL] batch:  392 | acc: 93.75%,  total acc: 69.29%   [EVAL] batch:  393 | acc: 18.75%,  total acc: 69.16%   [EVAL] batch:  394 | acc: 25.00%,  total acc: 69.05%   [EVAL] batch:  395 | acc: 25.00%,  total acc: 68.94%   [EVAL] batch:  396 | acc: 18.75%,  total acc: 68.81%   [EVAL] batch:  397 | acc: 18.75%,  total acc: 68.69%   [EVAL] batch:  398 | acc: 18.75%,  total acc: 68.56%   [EVAL] batch:  399 | acc: 12.50%,  total acc: 68.42%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 68.27%   [EVAL] batch:  401 | acc: 6.25%,  total acc: 68.11%   [EVAL] batch:  402 | acc: 12.50%,  total acc: 67.97%   [EVAL] batch:  403 | acc: 6.25%,  total acc: 67.82%   [EVAL] batch:  404 | acc: 12.50%,  total acc: 67.69%   [EVAL] batch:  405 | acc: 6.25%,  total acc: 67.53%   [EVAL] batch:  406 | acc: 50.00%,  total acc: 67.49%   [EVAL] batch:  407 | acc: 93.75%,  total acc: 67.56%   [EVAL] batch:  408 | acc: 87.50%,  total acc: 67.60%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 67.68%   [EVAL] batch:  410 | acc: 93.75%,  total acc: 67.75%   [EVAL] batch:  411 | acc: 75.00%,  total acc: 67.76%   [EVAL] batch:  412 | acc: 93.75%,  total acc: 67.83%   [EVAL] batch:  413 | acc: 87.50%,  total acc: 67.87%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 67.95%   [EVAL] batch:  415 | acc: 100.00%,  total acc: 68.03%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 68.06%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 68.14%   [EVAL] batch:  418 | acc: 81.25%,  total acc: 68.17%   [EVAL] batch:  419 | acc: 43.75%,  total acc: 68.11%   [EVAL] batch:  420 | acc: 37.50%,  total acc: 68.04%   [EVAL] batch:  421 | acc: 37.50%,  total acc: 67.97%   [EVAL] batch:  422 | acc: 25.00%,  total acc: 67.86%   [EVAL] batch:  423 | acc: 56.25%,  total acc: 67.84%   [EVAL] batch:  424 | acc: 31.25%,  total acc: 67.75%   [EVAL] batch:  425 | acc: 31.25%,  total acc: 67.66%   [EVAL] batch:  426 | acc: 62.50%,  total acc: 67.65%   [EVAL] batch:  427 | acc: 43.75%,  total acc: 67.60%   [EVAL] batch:  428 | acc: 62.50%,  total acc: 67.58%   [EVAL] batch:  429 | acc: 50.00%,  total acc: 67.54%   [EVAL] batch:  430 | acc: 68.75%,  total acc: 67.55%   [EVAL] batch:  431 | acc: 18.75%,  total acc: 67.43%   [EVAL] batch:  432 | acc: 25.00%,  total acc: 67.34%   [EVAL] batch:  433 | acc: 37.50%,  total acc: 67.27%   [EVAL] batch:  434 | acc: 25.00%,  total acc: 67.17%   [EVAL] batch:  435 | acc: 37.50%,  total acc: 67.10%   [EVAL] batch:  436 | acc: 25.00%,  total acc: 67.01%   [EVAL] batch:  437 | acc: 56.25%,  total acc: 66.98%   [EVAL] batch:  438 | acc: 87.50%,  total acc: 67.03%   [EVAL] batch:  439 | acc: 81.25%,  total acc: 67.06%   [EVAL] batch:  440 | acc: 93.75%,  total acc: 67.12%   [EVAL] batch:  441 | acc: 81.25%,  total acc: 67.15%   [EVAL] batch:  442 | acc: 81.25%,  total acc: 67.18%   [EVAL] batch:  443 | acc: 75.00%,  total acc: 67.20%   [EVAL] batch:  444 | acc: 93.75%,  total acc: 67.26%   [EVAL] batch:  445 | acc: 81.25%,  total acc: 67.29%   [EVAL] batch:  446 | acc: 62.50%,  total acc: 67.28%   [EVAL] batch:  447 | acc: 93.75%,  total acc: 67.34%   [EVAL] batch:  448 | acc: 81.25%,  total acc: 67.37%   [EVAL] batch:  449 | acc: 81.25%,  total acc: 67.40%   [EVAL] batch:  450 | acc: 81.25%,  total acc: 67.43%   [EVAL] batch:  451 | acc: 81.25%,  total acc: 67.46%   [EVAL] batch:  452 | acc: 62.50%,  total acc: 67.45%   [EVAL] batch:  453 | acc: 81.25%,  total acc: 67.48%   [EVAL] batch:  454 | acc: 87.50%,  total acc: 67.53%   [EVAL] batch:  455 | acc: 93.75%,  total acc: 67.58%   [EVAL] batch:  456 | acc: 81.25%,  total acc: 67.61%   [EVAL] batch:  457 | acc: 68.75%,  total acc: 67.62%   [EVAL] batch:  458 | acc: 75.00%,  total acc: 67.63%   [EVAL] batch:  459 | acc: 75.00%,  total acc: 67.65%   [EVAL] batch:  460 | acc: 62.50%,  total acc: 67.64%   [EVAL] batch:  461 | acc: 81.25%,  total acc: 67.67%   [EVAL] batch:  462 | acc: 87.50%,  total acc: 67.71%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 67.78%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 67.85%   [EVAL] batch:  465 | acc: 100.00%,  total acc: 67.92%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 67.99%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 68.06%   [EVAL] batch:  468 | acc: 100.00%,  total acc: 68.12%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 68.19%   [EVAL] batch:  470 | acc: 100.00%,  total acc: 68.26%   [EVAL] batch:  471 | acc: 100.00%,  total acc: 68.33%   [EVAL] batch:  472 | acc: 100.00%,  total acc: 68.39%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 68.46%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 68.53%   [EVAL] batch:  475 | acc: 100.00%,  total acc: 68.59%   [EVAL] batch:  476 | acc: 93.75%,  total acc: 68.65%   [EVAL] batch:  477 | acc: 93.75%,  total acc: 68.70%   [EVAL] batch:  478 | acc: 100.00%,  total acc: 68.76%   [EVAL] batch:  479 | acc: 100.00%,  total acc: 68.83%   [EVAL] batch:  480 | acc: 93.75%,  total acc: 68.88%   [EVAL] batch:  481 | acc: 62.50%,  total acc: 68.87%   [EVAL] batch:  482 | acc: 37.50%,  total acc: 68.80%   [EVAL] batch:  483 | acc: 50.00%,  total acc: 68.76%   [EVAL] batch:  484 | acc: 50.00%,  total acc: 68.72%   [EVAL] batch:  485 | acc: 68.75%,  total acc: 68.72%   [EVAL] batch:  486 | acc: 43.75%,  total acc: 68.67%   [EVAL] batch:  487 | acc: 81.25%,  total acc: 68.70%   [EVAL] batch:  488 | acc: 87.50%,  total acc: 68.74%   [EVAL] batch:  489 | acc: 100.00%,  total acc: 68.80%   [EVAL] batch:  490 | acc: 87.50%,  total acc: 68.84%   [EVAL] batch:  491 | acc: 87.50%,  total acc: 68.88%   [EVAL] batch:  492 | acc: 93.75%,  total acc: 68.93%   [EVAL] batch:  493 | acc: 81.25%,  total acc: 68.95%   [EVAL] batch:  494 | acc: 37.50%,  total acc: 68.89%   [EVAL] batch:  495 | acc: 43.75%,  total acc: 68.84%   [EVAL] batch:  496 | acc: 62.50%,  total acc: 68.83%   [EVAL] batch:  497 | acc: 62.50%,  total acc: 68.81%   [EVAL] batch:  498 | acc: 37.50%,  total acc: 68.75%   [EVAL] batch:  499 | acc: 68.75%,  total acc: 68.75%   
cur_acc:  ['0.9474', '0.8046', '0.8323', '0.8849', '0.8274', '0.7847', '0.5675', '0.8065']
his_acc:  ['0.9474', '0.8695', '0.8464', '0.8283', '0.7951', '0.7640', '0.7080', '0.6875']
--------Round  1
seed:  200
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 12.9875031CurrentTrain: epoch  0, batch     1 | loss: 12.7274303CurrentTrain: epoch  0, batch     2 | loss: 12.4121399CurrentTrain: epoch  0, batch     3 | loss: 12.1825714CurrentTrain: epoch  0, batch     4 | loss: 11.8316689CurrentTrain: epoch  0, batch     5 | loss: 11.7734871CurrentTrain: epoch  0, batch     6 | loss: 11.7838268CurrentTrain: epoch  0, batch     7 | loss: 11.5824451CurrentTrain: epoch  0, batch     8 | loss: 11.0885296CurrentTrain: epoch  0, batch     9 | loss: 11.0231600CurrentTrain: epoch  0, batch    10 | loss: 11.3799400CurrentTrain: epoch  0, batch    11 | loss: 11.0791397CurrentTrain: epoch  0, batch    12 | loss: 11.0721607CurrentTrain: epoch  0, batch    13 | loss: 10.5810795CurrentTrain: epoch  0, batch    14 | loss: 10.5741596CurrentTrain: epoch  0, batch    15 | loss: 10.2393389CurrentTrain: epoch  0, batch    16 | loss: 10.2778320CurrentTrain: epoch  0, batch    17 | loss: 10.5440617CurrentTrain: epoch  0, batch    18 | loss: 10.7223072CurrentTrain: epoch  0, batch    19 | loss: 10.3923235CurrentTrain: epoch  0, batch    20 | loss: 10.2047634CurrentTrain: epoch  0, batch    21 | loss: 9.9873428CurrentTrain: epoch  0, batch    22 | loss: 10.6379852CurrentTrain: epoch  0, batch    23 | loss: 9.4756317CurrentTrain: epoch  0, batch    24 | loss: 9.9805641CurrentTrain: epoch  0, batch    25 | loss: 9.4275427CurrentTrain: epoch  0, batch    26 | loss: 9.7372112CurrentTrain: epoch  0, batch    27 | loss: 10.3772526CurrentTrain: epoch  0, batch    28 | loss: 9.5790854CurrentTrain: epoch  0, batch    29 | loss: 9.8431911CurrentTrain: epoch  0, batch    30 | loss: 10.0510387CurrentTrain: epoch  0, batch    31 | loss: 9.5727940CurrentTrain: epoch  0, batch    32 | loss: 9.8294926CurrentTrain: epoch  0, batch    33 | loss: 9.4843626CurrentTrain: epoch  0, batch    34 | loss: 9.5391369CurrentTrain: epoch  0, batch    35 | loss: 9.2926407CurrentTrain: epoch  0, batch    36 | loss: 9.5435028CurrentTrain: epoch  0, batch    37 | loss: 9.4291649CurrentTrain: epoch  0, batch    38 | loss: 10.2913113CurrentTrain: epoch  0, batch    39 | loss: 9.5689535CurrentTrain: epoch  0, batch    40 | loss: 9.1474390CurrentTrain: epoch  0, batch    41 | loss: 9.4329958CurrentTrain: epoch  0, batch    42 | loss: 9.1931715CurrentTrain: epoch  0, batch    43 | loss: 8.6673613CurrentTrain: epoch  0, batch    44 | loss: 9.5394602CurrentTrain: epoch  0, batch    45 | loss: 8.7447968CurrentTrain: epoch  0, batch    46 | loss: 9.3052101CurrentTrain: epoch  0, batch    47 | loss: 9.0317307CurrentTrain: epoch  0, batch    48 | loss: 9.7212324CurrentTrain: epoch  0, batch    49 | loss: 8.9782515CurrentTrain: epoch  0, batch    50 | loss: 8.0877399CurrentTrain: epoch  0, batch    51 | loss: 8.8212461CurrentTrain: epoch  0, batch    52 | loss: 8.5528822CurrentTrain: epoch  0, batch    53 | loss: 8.3130417CurrentTrain: epoch  0, batch    54 | loss: 8.2318430CurrentTrain: epoch  0, batch    55 | loss: 7.9100180CurrentTrain: epoch  0, batch    56 | loss: 8.2569904CurrentTrain: epoch  0, batch    57 | loss: 8.3579454CurrentTrain: epoch  0, batch    58 | loss: 9.0496492CurrentTrain: epoch  0, batch    59 | loss: 7.9878650CurrentTrain: epoch  0, batch    60 | loss: 7.1196599CurrentTrain: epoch  0, batch    61 | loss: 8.1254873CurrentTrain: epoch  0, batch    62 | loss: 7.9026537CurrentTrain: epoch  1, batch     0 | loss: 8.1835003CurrentTrain: epoch  1, batch     1 | loss: 7.7201290CurrentTrain: epoch  1, batch     2 | loss: 7.7901659CurrentTrain: epoch  1, batch     3 | loss: 7.3196583CurrentTrain: epoch  1, batch     4 | loss: 8.1628513CurrentTrain: epoch  1, batch     5 | loss: 7.4152627CurrentTrain: epoch  1, batch     6 | loss: 7.7337770CurrentTrain: epoch  1, batch     7 | loss: 7.7452850CurrentTrain: epoch  1, batch     8 | loss: 7.5881028CurrentTrain: epoch  1, batch     9 | loss: 8.0715332CurrentTrain: epoch  1, batch    10 | loss: 6.8071604CurrentTrain: epoch  1, batch    11 | loss: 6.9203544CurrentTrain: epoch  1, batch    12 | loss: 7.1606722CurrentTrain: epoch  1, batch    13 | loss: 7.5567455CurrentTrain: epoch  1, batch    14 | loss: 7.6707544CurrentTrain: epoch  1, batch    15 | loss: 7.1731319CurrentTrain: epoch  1, batch    16 | loss: 6.8425417CurrentTrain: epoch  1, batch    17 | loss: 7.5679121CurrentTrain: epoch  1, batch    18 | loss: 7.1158314CurrentTrain: epoch  1, batch    19 | loss: 7.3444781CurrentTrain: epoch  1, batch    20 | loss: 7.5488935CurrentTrain: epoch  1, batch    21 | loss: 7.3226094CurrentTrain: epoch  1, batch    22 | loss: 7.2600656CurrentTrain: epoch  1, batch    23 | loss: 7.1184521CurrentTrain: epoch  1, batch    24 | loss: 7.3173409CurrentTrain: epoch  1, batch    25 | loss: 6.8141518CurrentTrain: epoch  1, batch    26 | loss: 6.4385004CurrentTrain: epoch  1, batch    27 | loss: 7.1421537CurrentTrain: epoch  1, batch    28 | loss: 7.1298647CurrentTrain: epoch  1, batch    29 | loss: 7.5034866CurrentTrain: epoch  1, batch    30 | loss: 7.3578434CurrentTrain: epoch  1, batch    31 | loss: 6.5321131CurrentTrain: epoch  1, batch    32 | loss: 6.4268155CurrentTrain: epoch  1, batch    33 | loss: 7.7181087CurrentTrain: epoch  1, batch    34 | loss: 6.6056848CurrentTrain: epoch  1, batch    35 | loss: 7.1461048CurrentTrain: epoch  1, batch    36 | loss: 6.6121387CurrentTrain: epoch  1, batch    37 | loss: 6.9099555CurrentTrain: epoch  1, batch    38 | loss: 6.8786945CurrentTrain: epoch  1, batch    39 | loss: 6.7558928CurrentTrain: epoch  1, batch    40 | loss: 6.0288153CurrentTrain: epoch  1, batch    41 | loss: 6.2823744CurrentTrain: epoch  1, batch    42 | loss: 7.0514202CurrentTrain: epoch  1, batch    43 | loss: 6.6570253CurrentTrain: epoch  1, batch    44 | loss: 6.4624968CurrentTrain: epoch  1, batch    45 | loss: 7.0438752CurrentTrain: epoch  1, batch    46 | loss: 7.1019502CurrentTrain: epoch  1, batch    47 | loss: 5.7928743CurrentTrain: epoch  1, batch    48 | loss: 6.3824301CurrentTrain: epoch  1, batch    49 | loss: 6.5544777CurrentTrain: epoch  1, batch    50 | loss: 7.1988897CurrentTrain: epoch  1, batch    51 | loss: 6.9668918CurrentTrain: epoch  1, batch    52 | loss: 7.7704887CurrentTrain: epoch  1, batch    53 | loss: 6.8106732CurrentTrain: epoch  1, batch    54 | loss: 6.6717606CurrentTrain: epoch  1, batch    55 | loss: 6.0307636CurrentTrain: epoch  1, batch    56 | loss: 6.5275126CurrentTrain: epoch  1, batch    57 | loss: 6.7098360CurrentTrain: epoch  1, batch    58 | loss: 6.1266756CurrentTrain: epoch  1, batch    59 | loss: 6.0174689CurrentTrain: epoch  1, batch    60 | loss: 5.9102564CurrentTrain: epoch  1, batch    61 | loss: 6.8455505CurrentTrain: epoch  1, batch    62 | loss: 6.3442898CurrentTrain: epoch  2, batch     0 | loss: 6.5828319CurrentTrain: epoch  2, batch     1 | loss: 5.7693186CurrentTrain: epoch  2, batch     2 | loss: 5.6504936CurrentTrain: epoch  2, batch     3 | loss: 5.9883871CurrentTrain: epoch  2, batch     4 | loss: 5.6491499CurrentTrain: epoch  2, batch     5 | loss: 5.8806973CurrentTrain: epoch  2, batch     6 | loss: 6.0343714CurrentTrain: epoch  2, batch     7 | loss: 5.9798269CurrentTrain: epoch  2, batch     8 | loss: 6.0651188CurrentTrain: epoch  2, batch     9 | loss: 5.8879051CurrentTrain: epoch  2, batch    10 | loss: 5.3980799CurrentTrain: epoch  2, batch    11 | loss: 6.1530361CurrentTrain: epoch  2, batch    12 | loss: 5.8078341CurrentTrain: epoch  2, batch    13 | loss: 6.1475220CurrentTrain: epoch  2, batch    14 | loss: 6.1851845CurrentTrain: epoch  2, batch    15 | loss: 5.5459619CurrentTrain: epoch  2, batch    16 | loss: 5.3792028CurrentTrain: epoch  2, batch    17 | loss: 5.6457500CurrentTrain: epoch  2, batch    18 | loss: 6.0234175CurrentTrain: epoch  2, batch    19 | loss: 5.5204368CurrentTrain: epoch  2, batch    20 | loss: 6.0698929CurrentTrain: epoch  2, batch    21 | loss: 5.8134189CurrentTrain: epoch  2, batch    22 | loss: 5.4823556CurrentTrain: epoch  2, batch    23 | loss: 6.4090891CurrentTrain: epoch  2, batch    24 | loss: 6.5325079CurrentTrain: epoch  2, batch    25 | loss: 5.6526451CurrentTrain: epoch  2, batch    26 | loss: 6.0878782CurrentTrain: epoch  2, batch    27 | loss: 5.7003574CurrentTrain: epoch  2, batch    28 | loss: 5.6140895CurrentTrain: epoch  2, batch    29 | loss: 6.0587540CurrentTrain: epoch  2, batch    30 | loss: 5.3724828CurrentTrain: epoch  2, batch    31 | loss: 5.4341698CurrentTrain: epoch  2, batch    32 | loss: 5.6300602CurrentTrain: epoch  2, batch    33 | loss: 5.5869179CurrentTrain: epoch  2, batch    34 | loss: 5.9700222CurrentTrain: epoch  2, batch    35 | loss: 5.6210680CurrentTrain: epoch  2, batch    36 | loss: 5.8259211CurrentTrain: epoch  2, batch    37 | loss: 5.8523726CurrentTrain: epoch  2, batch    38 | loss: 6.3174920CurrentTrain: epoch  2, batch    39 | loss: 5.5704155CurrentTrain: epoch  2, batch    40 | loss: 5.9620728CurrentTrain: epoch  2, batch    41 | loss: 5.9833479CurrentTrain: epoch  2, batch    42 | loss: 5.3129597CurrentTrain: epoch  2, batch    43 | loss: 5.5475473CurrentTrain: epoch  2, batch    44 | loss: 5.0235853CurrentTrain: epoch  2, batch    45 | loss: 5.8363390CurrentTrain: epoch  2, batch    46 | loss: 6.0331049CurrentTrain: epoch  2, batch    47 | loss: 5.7065468CurrentTrain: epoch  2, batch    48 | loss: 5.2169275CurrentTrain: epoch  2, batch    49 | loss: 5.7185335CurrentTrain: epoch  2, batch    50 | loss: 5.1914215CurrentTrain: epoch  2, batch    51 | loss: 5.1321783CurrentTrain: epoch  2, batch    52 | loss: 5.6149659CurrentTrain: epoch  2, batch    53 | loss: 5.7487464CurrentTrain: epoch  2, batch    54 | loss: 5.6197653CurrentTrain: epoch  2, batch    55 | loss: 5.6553626CurrentTrain: epoch  2, batch    56 | loss: 5.3108649CurrentTrain: epoch  2, batch    57 | loss: 5.6702890CurrentTrain: epoch  2, batch    58 | loss: 5.7336965CurrentTrain: epoch  2, batch    59 | loss: 5.2619128CurrentTrain: epoch  2, batch    60 | loss: 6.3014355CurrentTrain: epoch  2, batch    61 | loss: 5.5182290CurrentTrain: epoch  2, batch    62 | loss: 5.3521285CurrentTrain: epoch  3, batch     0 | loss: 5.1046844CurrentTrain: epoch  3, batch     1 | loss: 4.9507885CurrentTrain: epoch  3, batch     2 | loss: 5.0789118CurrentTrain: epoch  3, batch     3 | loss: 5.3114796CurrentTrain: epoch  3, batch     4 | loss: 5.4531651CurrentTrain: epoch  3, batch     5 | loss: 5.1030254CurrentTrain: epoch  3, batch     6 | loss: 4.8367243CurrentTrain: epoch  3, batch     7 | loss: 5.0679922CurrentTrain: epoch  3, batch     8 | loss: 4.7276235CurrentTrain: epoch  3, batch     9 | loss: 5.0569425CurrentTrain: epoch  3, batch    10 | loss: 5.2195997CurrentTrain: epoch  3, batch    11 | loss: 5.0153275CurrentTrain: epoch  3, batch    12 | loss: 4.9276667CurrentTrain: epoch  3, batch    13 | loss: 5.2511539CurrentTrain: epoch  3, batch    14 | loss: 5.2741451CurrentTrain: epoch  3, batch    15 | loss: 6.1352968CurrentTrain: epoch  3, batch    16 | loss: 5.3866105CurrentTrain: epoch  3, batch    17 | loss: 5.5618358CurrentTrain: epoch  3, batch    18 | loss: 5.3862057CurrentTrain: epoch  3, batch    19 | loss: 4.9775739CurrentTrain: epoch  3, batch    20 | loss: 4.9961791CurrentTrain: epoch  3, batch    21 | loss: 5.1398373CurrentTrain: epoch  3, batch    22 | loss: 4.6790800CurrentTrain: epoch  3, batch    23 | loss: 4.4825225CurrentTrain: epoch  3, batch    24 | loss: 5.0584378CurrentTrain: epoch  3, batch    25 | loss: 4.7972775CurrentTrain: epoch  3, batch    26 | loss: 5.1756496CurrentTrain: epoch  3, batch    27 | loss: 7.3066974CurrentTrain: epoch  3, batch    28 | loss: 5.0355482CurrentTrain: epoch  3, batch    29 | loss: 4.9685745CurrentTrain: epoch  3, batch    30 | loss: 4.8815126CurrentTrain: epoch  3, batch    31 | loss: 5.1622658CurrentTrain: epoch  3, batch    32 | loss: 4.8767781CurrentTrain: epoch  3, batch    33 | loss: 4.7356791CurrentTrain: epoch  3, batch    34 | loss: 5.0261636CurrentTrain: epoch  3, batch    35 | loss: 4.7006407CurrentTrain: epoch  3, batch    36 | loss: 4.6493115CurrentTrain: epoch  3, batch    37 | loss: 5.0392771CurrentTrain: epoch  3, batch    38 | loss: 5.0050731CurrentTrain: epoch  3, batch    39 | loss: 5.2940350CurrentTrain: epoch  3, batch    40 | loss: 5.4728937CurrentTrain: epoch  3, batch    41 | loss: 5.2939625CurrentTrain: epoch  3, batch    42 | loss: 4.6190023CurrentTrain: epoch  3, batch    43 | loss: 5.1891909CurrentTrain: epoch  3, batch    44 | loss: 4.7517271CurrentTrain: epoch  3, batch    45 | loss: 4.9574203CurrentTrain: epoch  3, batch    46 | loss: 4.5295486CurrentTrain: epoch  3, batch    47 | loss: 5.0338774CurrentTrain: epoch  3, batch    48 | loss: 4.7826614CurrentTrain: epoch  3, batch    49 | loss: 5.0545406CurrentTrain: epoch  3, batch    50 | loss: 4.9820337CurrentTrain: epoch  3, batch    51 | loss: 5.0878944CurrentTrain: epoch  3, batch    52 | loss: 4.8249526CurrentTrain: epoch  3, batch    53 | loss: 4.9799662CurrentTrain: epoch  3, batch    54 | loss: 4.8627386CurrentTrain: epoch  3, batch    55 | loss: 5.4020538CurrentTrain: epoch  3, batch    56 | loss: 4.5064378CurrentTrain: epoch  3, batch    57 | loss: 4.7450233CurrentTrain: epoch  3, batch    58 | loss: 5.0294867CurrentTrain: epoch  3, batch    59 | loss: 4.9546909CurrentTrain: epoch  3, batch    60 | loss: 4.5740528CurrentTrain: epoch  3, batch    61 | loss: 4.9511652CurrentTrain: epoch  3, batch    62 | loss: 4.8592844CurrentTrain: epoch  4, batch     0 | loss: 4.8159904CurrentTrain: epoch  4, batch     1 | loss: 5.2966700CurrentTrain: epoch  4, batch     2 | loss: 4.5615120CurrentTrain: epoch  4, batch     3 | loss: 4.9387488CurrentTrain: epoch  4, batch     4 | loss: 4.7161212CurrentTrain: epoch  4, batch     5 | loss: 4.6673231CurrentTrain: epoch  4, batch     6 | loss: 4.5579844CurrentTrain: epoch  4, batch     7 | loss: 4.7770109CurrentTrain: epoch  4, batch     8 | loss: 4.5696464CurrentTrain: epoch  4, batch     9 | loss: 5.0109062CurrentTrain: epoch  4, batch    10 | loss: 4.5242271CurrentTrain: epoch  4, batch    11 | loss: 5.2469573CurrentTrain: epoch  4, batch    12 | loss: 4.4981880CurrentTrain: epoch  4, batch    13 | loss: 4.5612087CurrentTrain: epoch  4, batch    14 | loss: 4.5339108CurrentTrain: epoch  4, batch    15 | loss: 4.6390123CurrentTrain: epoch  4, batch    16 | loss: 4.8679223CurrentTrain: epoch  4, batch    17 | loss: 4.8430901CurrentTrain: epoch  4, batch    18 | loss: 4.7280130CurrentTrain: epoch  4, batch    19 | loss: 4.7002268CurrentTrain: epoch  4, batch    20 | loss: 4.5782137CurrentTrain: epoch  4, batch    21 | loss: 4.5690851CurrentTrain: epoch  4, batch    22 | loss: 4.7650309CurrentTrain: epoch  4, batch    23 | loss: 4.5915585CurrentTrain: epoch  4, batch    24 | loss: 4.4581628CurrentTrain: epoch  4, batch    25 | loss: 4.5115099CurrentTrain: epoch  4, batch    26 | loss: 4.4550858CurrentTrain: epoch  4, batch    27 | loss: 4.6365743CurrentTrain: epoch  4, batch    28 | loss: 4.5009995CurrentTrain: epoch  4, batch    29 | loss: 4.6835842CurrentTrain: epoch  4, batch    30 | loss: 4.5847664CurrentTrain: epoch  4, batch    31 | loss: 4.6090598CurrentTrain: epoch  4, batch    32 | loss: 4.4191837CurrentTrain: epoch  4, batch    33 | loss: 4.7410359CurrentTrain: epoch  4, batch    34 | loss: 4.4982781CurrentTrain: epoch  4, batch    35 | loss: 4.5012674CurrentTrain: epoch  4, batch    36 | loss: 4.7798600CurrentTrain: epoch  4, batch    37 | loss: 4.3542080CurrentTrain: epoch  4, batch    38 | loss: 4.4343071CurrentTrain: epoch  4, batch    39 | loss: 4.4441185CurrentTrain: epoch  4, batch    40 | loss: 4.6870308CurrentTrain: epoch  4, batch    41 | loss: 4.3326092CurrentTrain: epoch  4, batch    42 | loss: 4.3734436CurrentTrain: epoch  4, batch    43 | loss: 4.7098227CurrentTrain: epoch  4, batch    44 | loss: 4.6742449CurrentTrain: epoch  4, batch    45 | loss: 4.7778144CurrentTrain: epoch  4, batch    46 | loss: 4.5006967CurrentTrain: epoch  4, batch    47 | loss: 4.4367380CurrentTrain: epoch  4, batch    48 | loss: 4.3713846CurrentTrain: epoch  4, batch    49 | loss: 4.4964933CurrentTrain: epoch  4, batch    50 | loss: 4.2798615CurrentTrain: epoch  4, batch    51 | loss: 4.3285236CurrentTrain: epoch  4, batch    52 | loss: 4.6562219CurrentTrain: epoch  4, batch    53 | loss: 4.4834213CurrentTrain: epoch  4, batch    54 | loss: 4.3050976CurrentTrain: epoch  4, batch    55 | loss: 4.5698090CurrentTrain: epoch  4, batch    56 | loss: 4.3312759CurrentTrain: epoch  4, batch    57 | loss: 4.3385706CurrentTrain: epoch  4, batch    58 | loss: 4.5945907CurrentTrain: epoch  4, batch    59 | loss: 4.3391771CurrentTrain: epoch  4, batch    60 | loss: 4.2546229CurrentTrain: epoch  4, batch    61 | loss: 4.2959051CurrentTrain: epoch  4, batch    62 | loss: 4.3664207CurrentTrain: epoch  5, batch     0 | loss: 4.3618369CurrentTrain: epoch  5, batch     1 | loss: 4.2984934CurrentTrain: epoch  5, batch     2 | loss: 4.3297491CurrentTrain: epoch  5, batch     3 | loss: 4.6471181CurrentTrain: epoch  5, batch     4 | loss: 4.3193159CurrentTrain: epoch  5, batch     5 | loss: 4.3784466CurrentTrain: epoch  5, batch     6 | loss: 4.3221283CurrentTrain: epoch  5, batch     7 | loss: 4.2726631CurrentTrain: epoch  5, batch     8 | loss: 4.2760072CurrentTrain: epoch  5, batch     9 | loss: 4.3843031CurrentTrain: epoch  5, batch    10 | loss: 4.3367376CurrentTrain: epoch  5, batch    11 | loss: 4.2899175CurrentTrain: epoch  5, batch    12 | loss: 4.4206314CurrentTrain: epoch  5, batch    13 | loss: 4.2148709CurrentTrain: epoch  5, batch    14 | loss: 4.3793979CurrentTrain: epoch  5, batch    15 | loss: 4.1842766CurrentTrain: epoch  5, batch    16 | loss: 4.2654486CurrentTrain: epoch  5, batch    17 | loss: 4.2091632CurrentTrain: epoch  5, batch    18 | loss: 4.3486457CurrentTrain: epoch  5, batch    19 | loss: 4.5344377CurrentTrain: epoch  5, batch    20 | loss: 4.1927605CurrentTrain: epoch  5, batch    21 | loss: 4.3426313CurrentTrain: epoch  5, batch    22 | loss: 4.4053502CurrentTrain: epoch  5, batch    23 | loss: 4.2827253CurrentTrain: epoch  5, batch    24 | loss: 4.4828348CurrentTrain: epoch  5, batch    25 | loss: 4.1835456CurrentTrain: epoch  5, batch    26 | loss: 4.3166971CurrentTrain: epoch  5, batch    27 | loss: 4.4147635CurrentTrain: epoch  5, batch    28 | loss: 4.2330823CurrentTrain: epoch  5, batch    29 | loss: 4.2054787CurrentTrain: epoch  5, batch    30 | loss: 4.2578306CurrentTrain: epoch  5, batch    31 | loss: 4.2375641CurrentTrain: epoch  5, batch    32 | loss: 4.6935186CurrentTrain: epoch  5, batch    33 | loss: 4.5676537CurrentTrain: epoch  5, batch    34 | loss: 4.2022676CurrentTrain: epoch  5, batch    35 | loss: 4.1855707CurrentTrain: epoch  5, batch    36 | loss: 4.4088163CurrentTrain: epoch  5, batch    37 | loss: 4.2066355CurrentTrain: epoch  5, batch    38 | loss: 4.2257090CurrentTrain: epoch  5, batch    39 | loss: 4.3512568CurrentTrain: epoch  5, batch    40 | loss: 4.2107344CurrentTrain: epoch  5, batch    41 | loss: 4.8017602CurrentTrain: epoch  5, batch    42 | loss: 4.3233852CurrentTrain: epoch  5, batch    43 | loss: 4.2786264CurrentTrain: epoch  5, batch    44 | loss: 4.2790298CurrentTrain: epoch  5, batch    45 | loss: 4.4546695CurrentTrain: epoch  5, batch    46 | loss: 4.3278346CurrentTrain: epoch  5, batch    47 | loss: 4.2586117CurrentTrain: epoch  5, batch    48 | loss: 4.1459093CurrentTrain: epoch  5, batch    49 | loss: 4.2585106CurrentTrain: epoch  5, batch    50 | loss: 4.3664236CurrentTrain: epoch  5, batch    51 | loss: 4.2670860CurrentTrain: epoch  5, batch    52 | loss: 4.1827202CurrentTrain: epoch  5, batch    53 | loss: 4.2901931CurrentTrain: epoch  5, batch    54 | loss: 4.1979394CurrentTrain: epoch  5, batch    55 | loss: 4.2684216CurrentTrain: epoch  5, batch    56 | loss: 4.2661333CurrentTrain: epoch  5, batch    57 | loss: 4.2059231CurrentTrain: epoch  5, batch    58 | loss: 4.3029437CurrentTrain: epoch  5, batch    59 | loss: 4.3001623CurrentTrain: epoch  5, batch    60 | loss: 4.2379656CurrentTrain: epoch  5, batch    61 | loss: 4.1760459CurrentTrain: epoch  5, batch    62 | loss: 4.1227283CurrentTrain: epoch  6, batch     0 | loss: 4.2114019CurrentTrain: epoch  6, batch     1 | loss: 4.2754011CurrentTrain: epoch  6, batch     2 | loss: 4.2381039CurrentTrain: epoch  6, batch     3 | loss: 4.2168131CurrentTrain: epoch  6, batch     4 | loss: 4.1740308CurrentTrain: epoch  6, batch     5 | loss: 4.2208848CurrentTrain: epoch  6, batch     6 | loss: 4.6297994CurrentTrain: epoch  6, batch     7 | loss: 4.2353201CurrentTrain: epoch  6, batch     8 | loss: 4.1515355CurrentTrain: epoch  6, batch     9 | loss: 4.2094584CurrentTrain: epoch  6, batch    10 | loss: 4.2430229CurrentTrain: epoch  6, batch    11 | loss: 4.1741724CurrentTrain: epoch  6, batch    12 | loss: 4.2125044CurrentTrain: epoch  6, batch    13 | loss: 4.4676933CurrentTrain: epoch  6, batch    14 | loss: 4.2459369CurrentTrain: epoch  6, batch    15 | loss: 4.2054043CurrentTrain: epoch  6, batch    16 | loss: 4.3316240CurrentTrain: epoch  6, batch    17 | loss: 4.3626885CurrentTrain: epoch  6, batch    18 | loss: 4.2701750CurrentTrain: epoch  6, batch    19 | loss: 4.2457151CurrentTrain: epoch  6, batch    20 | loss: 4.2158461CurrentTrain: epoch  6, batch    21 | loss: 4.1834440CurrentTrain: epoch  6, batch    22 | loss: 4.1935248CurrentTrain: epoch  6, batch    23 | loss: 4.2030973CurrentTrain: epoch  6, batch    24 | loss: 4.2970591CurrentTrain: epoch  6, batch    25 | loss: 4.2366076CurrentTrain: epoch  6, batch    26 | loss: 4.1877584CurrentTrain: epoch  6, batch    27 | loss: 4.2009521CurrentTrain: epoch  6, batch    28 | loss: 4.3899384CurrentTrain: epoch  6, batch    29 | loss: 4.2046452CurrentTrain: epoch  6, batch    30 | loss: 4.1506400CurrentTrain: epoch  6, batch    31 | loss: 4.1932020CurrentTrain: epoch  6, batch    32 | loss: 4.2061377CurrentTrain: epoch  6, batch    33 | loss: 4.1754751CurrentTrain: epoch  6, batch    34 | loss: 4.1723261CurrentTrain: epoch  6, batch    35 | loss: 4.1669912CurrentTrain: epoch  6, batch    36 | loss: 4.3634920CurrentTrain: epoch  6, batch    37 | loss: 4.2079945CurrentTrain: epoch  6, batch    38 | loss: 4.4317679CurrentTrain: epoch  6, batch    39 | loss: 4.1584020CurrentTrain: epoch  6, batch    40 | loss: 4.2822304CurrentTrain: epoch  6, batch    41 | loss: 4.1651411CurrentTrain: epoch  6, batch    42 | loss: 4.2130671CurrentTrain: epoch  6, batch    43 | loss: 4.1433887CurrentTrain: epoch  6, batch    44 | loss: 4.2346601CurrentTrain: epoch  6, batch    45 | loss: 4.2108784CurrentTrain: epoch  6, batch    46 | loss: 4.2205606CurrentTrain: epoch  6, batch    47 | loss: 4.3463907CurrentTrain: epoch  6, batch    48 | loss: 4.1671524CurrentTrain: epoch  6, batch    49 | loss: 4.5237484CurrentTrain: epoch  6, batch    50 | loss: 4.1993327CurrentTrain: epoch  6, batch    51 | loss: 4.1800079CurrentTrain: epoch  6, batch    52 | loss: 4.1331682CurrentTrain: epoch  6, batch    53 | loss: 4.1808996CurrentTrain: epoch  6, batch    54 | loss: 4.2257566CurrentTrain: epoch  6, batch    55 | loss: 4.2279105CurrentTrain: epoch  6, batch    56 | loss: 4.2540121CurrentTrain: epoch  6, batch    57 | loss: 4.1869807CurrentTrain: epoch  6, batch    58 | loss: 4.2682409CurrentTrain: epoch  6, batch    59 | loss: 4.1721153CurrentTrain: epoch  6, batch    60 | loss: 4.2251797CurrentTrain: epoch  6, batch    61 | loss: 4.1492481CurrentTrain: epoch  6, batch    62 | loss: 4.1288214CurrentTrain: epoch  7, batch     0 | loss: 4.1602974CurrentTrain: epoch  7, batch     1 | loss: 4.1059561CurrentTrain: epoch  7, batch     2 | loss: 4.2134452CurrentTrain: epoch  7, batch     3 | loss: 4.1861134CurrentTrain: epoch  7, batch     4 | loss: 4.2045403CurrentTrain: epoch  7, batch     5 | loss: 4.1456747CurrentTrain: epoch  7, batch     6 | loss: 4.1558428CurrentTrain: epoch  7, batch     7 | loss: 4.1326866CurrentTrain: epoch  7, batch     8 | loss: 4.1931267CurrentTrain: epoch  7, batch     9 | loss: 4.1535048CurrentTrain: epoch  7, batch    10 | loss: 4.1570683CurrentTrain: epoch  7, batch    11 | loss: 4.2379160CurrentTrain: epoch  7, batch    12 | loss: 4.2890239CurrentTrain: epoch  7, batch    13 | loss: 4.1771202CurrentTrain: epoch  7, batch    14 | loss: 4.1479111CurrentTrain: epoch  7, batch    15 | loss: 4.2045336CurrentTrain: epoch  7, batch    16 | loss: 4.1532898CurrentTrain: epoch  7, batch    17 | loss: 4.1501007CurrentTrain: epoch  7, batch    18 | loss: 4.1117764CurrentTrain: epoch  7, batch    19 | loss: 4.1494389CurrentTrain: epoch  7, batch    20 | loss: 4.1453958CurrentTrain: epoch  7, batch    21 | loss: 4.1365013CurrentTrain: epoch  7, batch    22 | loss: 4.0950503CurrentTrain: epoch  7, batch    23 | loss: 4.1512332CurrentTrain: epoch  7, batch    24 | loss: 4.4506779CurrentTrain: epoch  7, batch    25 | loss: 4.1607685CurrentTrain: epoch  7, batch    26 | loss: 4.1719885CurrentTrain: epoch  7, batch    27 | loss: 4.1197572CurrentTrain: epoch  7, batch    28 | loss: 4.0805311CurrentTrain: epoch  7, batch    29 | loss: 4.1425266CurrentTrain: epoch  7, batch    30 | loss: 4.1159716CurrentTrain: epoch  7, batch    31 | loss: 4.1295242CurrentTrain: epoch  7, batch    32 | loss: 4.2202244CurrentTrain: epoch  7, batch    33 | loss: 4.1093559CurrentTrain: epoch  7, batch    34 | loss: 4.1169591CurrentTrain: epoch  7, batch    35 | loss: 4.3915777CurrentTrain: epoch  7, batch    36 | loss: 4.1080170CurrentTrain: epoch  7, batch    37 | loss: 4.1327963CurrentTrain: epoch  7, batch    38 | loss: 4.1652088CurrentTrain: epoch  7, batch    39 | loss: 4.2407045CurrentTrain: epoch  7, batch    40 | loss: 4.1982212CurrentTrain: epoch  7, batch    41 | loss: 4.1364355CurrentTrain: epoch  7, batch    42 | loss: 4.1659956CurrentTrain: epoch  7, batch    43 | loss: 4.2021232CurrentTrain: epoch  7, batch    44 | loss: 4.0970287CurrentTrain: epoch  7, batch    45 | loss: 4.1616898CurrentTrain: epoch  7, batch    46 | loss: 4.1820860CurrentTrain: epoch  7, batch    47 | loss: 4.1313992CurrentTrain: epoch  7, batch    48 | loss: 4.1478963CurrentTrain: epoch  7, batch    49 | loss: 4.1454391CurrentTrain: epoch  7, batch    50 | loss: 4.1156077CurrentTrain: epoch  7, batch    51 | loss: 4.1568618CurrentTrain: epoch  7, batch    52 | loss: 4.1176052CurrentTrain: epoch  7, batch    53 | loss: 4.1044102CurrentTrain: epoch  7, batch    54 | loss: 4.1808615CurrentTrain: epoch  7, batch    55 | loss: 4.1307688CurrentTrain: epoch  7, batch    56 | loss: 4.1429043CurrentTrain: epoch  7, batch    57 | loss: 4.0925655CurrentTrain: epoch  7, batch    58 | loss: 4.0903282CurrentTrain: epoch  7, batch    59 | loss: 4.2297039CurrentTrain: epoch  7, batch    60 | loss: 4.0921507CurrentTrain: epoch  7, batch    61 | loss: 4.0230618CurrentTrain: epoch  7, batch    62 | loss: 4.1256199CurrentTrain: epoch  8, batch     0 | loss: 4.0838771CurrentTrain: epoch  8, batch     1 | loss: 4.1112242CurrentTrain: epoch  8, batch     2 | loss: 4.0654507CurrentTrain: epoch  8, batch     3 | loss: 4.0310116CurrentTrain: epoch  8, batch     4 | loss: 4.0596147CurrentTrain: epoch  8, batch     5 | loss: 4.0674863CurrentTrain: epoch  8, batch     6 | loss: 4.0443583CurrentTrain: epoch  8, batch     7 | loss: 4.1406722CurrentTrain: epoch  8, batch     8 | loss: 4.0792651CurrentTrain: epoch  8, batch     9 | loss: 4.1336117CurrentTrain: epoch  8, batch    10 | loss: 4.0681438CurrentTrain: epoch  8, batch    11 | loss: 4.1170096CurrentTrain: epoch  8, batch    12 | loss: 4.0816779CurrentTrain: epoch  8, batch    13 | loss: 4.1009402CurrentTrain: epoch  8, batch    14 | loss: 4.1490583CurrentTrain: epoch  8, batch    15 | loss: 4.0672035CurrentTrain: epoch  8, batch    16 | loss: 4.0731392CurrentTrain: epoch  8, batch    17 | loss: 4.1153755CurrentTrain: epoch  8, batch    18 | loss: 4.1505003CurrentTrain: epoch  8, batch    19 | loss: 4.0803261CurrentTrain: epoch  8, batch    20 | loss: 4.0602551CurrentTrain: epoch  8, batch    21 | loss: 4.0293779CurrentTrain: epoch  8, batch    22 | loss: 4.1752238CurrentTrain: epoch  8, batch    23 | loss: 4.1746778CurrentTrain: epoch  8, batch    24 | loss: 4.1070285CurrentTrain: epoch  8, batch    25 | loss: 4.0534492CurrentTrain: epoch  8, batch    26 | loss: 4.1108208CurrentTrain: epoch  8, batch    27 | loss: 4.0982785CurrentTrain: epoch  8, batch    28 | loss: 4.0590844CurrentTrain: epoch  8, batch    29 | loss: 4.1246290CurrentTrain: epoch  8, batch    30 | loss: 4.1308556CurrentTrain: epoch  8, batch    31 | loss: 4.0537004CurrentTrain: epoch  8, batch    32 | loss: 4.1343737CurrentTrain: epoch  8, batch    33 | loss: 4.2027402CurrentTrain: epoch  8, batch    34 | loss: 4.0441871CurrentTrain: epoch  8, batch    35 | loss: 4.0756350CurrentTrain: epoch  8, batch    36 | loss: 4.0988936CurrentTrain: epoch  8, batch    37 | loss: 4.0637169CurrentTrain: epoch  8, batch    38 | loss: 4.1193361CurrentTrain: epoch  8, batch    39 | loss: 4.0896769CurrentTrain: epoch  8, batch    40 | loss: 4.0932560CurrentTrain: epoch  8, batch    41 | loss: 4.0625386CurrentTrain: epoch  8, batch    42 | loss: 4.1068277CurrentTrain: epoch  8, batch    43 | loss: 4.0641747CurrentTrain: epoch  8, batch    44 | loss: 4.1196642CurrentTrain: epoch  8, batch    45 | loss: 4.0625472CurrentTrain: epoch  8, batch    46 | loss: 4.0509710CurrentTrain: epoch  8, batch    47 | loss: 4.1082525CurrentTrain: epoch  8, batch    48 | loss: 4.0733352CurrentTrain: epoch  8, batch    49 | loss: 4.0537853CurrentTrain: epoch  8, batch    50 | loss: 4.0807090CurrentTrain: epoch  8, batch    51 | loss: 4.0761127CurrentTrain: epoch  8, batch    52 | loss: 4.1260271CurrentTrain: epoch  8, batch    53 | loss: 4.0381670CurrentTrain: epoch  8, batch    54 | loss: 4.0425768CurrentTrain: epoch  8, batch    55 | loss: 4.0646887CurrentTrain: epoch  8, batch    56 | loss: 4.0517540CurrentTrain: epoch  8, batch    57 | loss: 4.0789580CurrentTrain: epoch  8, batch    58 | loss: 4.0569849CurrentTrain: epoch  8, batch    59 | loss: 4.0309634CurrentTrain: epoch  8, batch    60 | loss: 4.0501719CurrentTrain: epoch  8, batch    61 | loss: 4.0581641CurrentTrain: epoch  8, batch    62 | loss: 4.0438452CurrentTrain: epoch  9, batch     0 | loss: 4.0861015CurrentTrain: epoch  9, batch     1 | loss: 4.0128059CurrentTrain: epoch  9, batch     2 | loss: 4.0775986CurrentTrain: epoch  9, batch     3 | loss: 4.0743332CurrentTrain: epoch  9, batch     4 | loss: 4.0189662CurrentTrain: epoch  9, batch     5 | loss: 4.0483994CurrentTrain: epoch  9, batch     6 | loss: 4.0066466CurrentTrain: epoch  9, batch     7 | loss: 4.0249372CurrentTrain: epoch  9, batch     8 | loss: 4.0509796CurrentTrain: epoch  9, batch     9 | loss: 4.0493565CurrentTrain: epoch  9, batch    10 | loss: 4.0984674CurrentTrain: epoch  9, batch    11 | loss: 4.0469408CurrentTrain: epoch  9, batch    12 | loss: 4.0554571CurrentTrain: epoch  9, batch    13 | loss: 4.0802622CurrentTrain: epoch  9, batch    14 | loss: 4.0942955CurrentTrain: epoch  9, batch    15 | loss: 4.0849876CurrentTrain: epoch  9, batch    16 | loss: 4.0569830CurrentTrain: epoch  9, batch    17 | loss: 4.1017022CurrentTrain: epoch  9, batch    18 | loss: 4.0029435CurrentTrain: epoch  9, batch    19 | loss: 4.1846662CurrentTrain: epoch  9, batch    20 | loss: 4.0916624CurrentTrain: epoch  9, batch    21 | loss: 4.0179448CurrentTrain: epoch  9, batch    22 | loss: 4.0037413CurrentTrain: epoch  9, batch    23 | loss: 4.0710669CurrentTrain: epoch  9, batch    24 | loss: 3.9895995CurrentTrain: epoch  9, batch    25 | loss: 4.0243449CurrentTrain: epoch  9, batch    26 | loss: 4.0629444CurrentTrain: epoch  9, batch    27 | loss: 4.0339227CurrentTrain: epoch  9, batch    28 | loss: 4.0330935CurrentTrain: epoch  9, batch    29 | loss: 4.0479493CurrentTrain: epoch  9, batch    30 | loss: 4.0467219CurrentTrain: epoch  9, batch    31 | loss: 4.0272546CurrentTrain: epoch  9, batch    32 | loss: 4.0401492CurrentTrain: epoch  9, batch    33 | loss: 4.0160213CurrentTrain: epoch  9, batch    34 | loss: 4.0193491CurrentTrain: epoch  9, batch    35 | loss: 4.0581102CurrentTrain: epoch  9, batch    36 | loss: 4.0398374CurrentTrain: epoch  9, batch    37 | loss: 4.0552416CurrentTrain: epoch  9, batch    38 | loss: 4.0213346CurrentTrain: epoch  9, batch    39 | loss: 4.0273333CurrentTrain: epoch  9, batch    40 | loss: 3.9986882CurrentTrain: epoch  9, batch    41 | loss: 4.0138755CurrentTrain: epoch  9, batch    42 | loss: 4.0207696CurrentTrain: epoch  9, batch    43 | loss: 4.0226450CurrentTrain: epoch  9, batch    44 | loss: 4.0418148CurrentTrain: epoch  9, batch    45 | loss: 4.0381875CurrentTrain: epoch  9, batch    46 | loss: 4.0531683CurrentTrain: epoch  9, batch    47 | loss: 4.0246305CurrentTrain: epoch  9, batch    48 | loss: 4.0119429CurrentTrain: epoch  9, batch    49 | loss: 3.9927974CurrentTrain: epoch  9, batch    50 | loss: 4.0164776CurrentTrain: epoch  9, batch    51 | loss: 4.0231237CurrentTrain: epoch  9, batch    52 | loss: 4.0190468CurrentTrain: epoch  9, batch    53 | loss: 4.0306973CurrentTrain: epoch  9, batch    54 | loss: 4.0138617CurrentTrain: epoch  9, batch    55 | loss: 3.9788706CurrentTrain: epoch  9, batch    56 | loss: 4.0451393CurrentTrain: epoch  9, batch    57 | loss: 4.0368657CurrentTrain: epoch  9, batch    58 | loss: 4.0136571CurrentTrain: epoch  9, batch    59 | loss: 4.0636907CurrentTrain: epoch  9, batch    60 | loss: 4.0867562CurrentTrain: epoch  9, batch    61 | loss: 4.0202837CurrentTrain: epoch  9, batch    62 | loss: 3.9944105
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: while the milanese painting scene was dominated by neoclassic painters andrea appiani and luigi sabatelli , giovannni migliara stayed with the historical themes and medieval subtlety of romanticism .
Head Entity: andrea appiani
Tail Entity: neoclassic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: during the early 20th century, the surrealist movement gained prominence, with artists like salvador dalí and max ernst pushing the boundaries of imagination and reality.  
Head Entity: salvador dalí  
Tail Entity: surrealist  

Relation: movement  
Context: the beat generation was a literary movement that emerged in the 1950s, characterized by authors such as jack kerouac and allen ginsberg who challenged conventional norms.  
Head Entity: jack kerouac  
Tail Entity: beat generation  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: lyons hill is a restored village , and former parish with church , now part of the community of ardclough in north county kildare .
Head Entity: ardclough
Tail Entity: county kildare
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: The city of San Francisco is known for its iconic Golden Gate Bridge and is part of the larger San Francisco County in California.  
Head Entity: San Francisco  
Tail Entity: San Francisco County  

Relation: located in the administrative territorial entity  
Context: The village of Houghton is situated within the boundaries of the City of Cambridge, which is located in Cambridgeshire, England.  
Head Entity: Houghton  
Tail Entity: City of Cambridge  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the nation as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: although joining x - factor offered polaris the chance to reunite with havok , their relationship remained largely unresolved .
Head Entity: havok
Tail Entity: x - factor
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: After years of performing together, the band finally announced that Sarah would be leaving the group to pursue a solo career, but she will always be remembered as a key member of The Harmonizers.  
Head Entity: Sarah  
Tail Entity: The Harmonizers  

Relation: member of  
Context: During the annual conference, it was revealed that John had been a dedicated member of the local chess club for over a decade, contributing to its growth and success.  
Head Entity: John  
Tail Entity: local chess club  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: the belleville turnpike bridge ( also known as the rutgers street bridge ) crosses the passaic river , connecting belleville to north arlington .
Head Entity: belleville turnpike bridge
Tail Entity: passaic river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: the golden gate bridge crosses the golden gate strait, linking san francisco to marin county.  
Head Entity: golden gate bridge  
Tail Entity: golden gate strait  

Relation: crosses  
Context: the london bridge crosses the river thames, providing a route between the city of london and southwark.  
Head Entity: london bridge  
Tail Entity: river thames  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: the river flows through the teatree waterhole and comet towards its confluence with the nogoa river to form the mackenzie river .
Head Entity: mackenzie river
Tail Entity: nogoa river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The clear waters of the Missouri River merge with the Mississippi River, creating a vital waterway for the region.  
Head Entity: Missouri River  
Tail Entity: Mississippi River  

Relation: tributary  
Context: As the sun sets, the small creek flows into the larger lake, providing essential nutrients to the ecosystem.  
Head Entity: creek  
Tail Entity: lake  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: the result was widely interpreted as a personal rebuke to then chief minister shane stone .
Head Entity: shane stone
Tail Entity: chief minister
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: After serving as the mayor for over a decade, John Smith announced his retirement from the position.  
Head Entity: John Smith  
Tail Entity: mayor  

Relation: position held  
Context: The former president of the organization, Maria Lopez, was recognized for her contributions during her tenure.  
Head Entity: Maria Lopez  
Tail Entity: president
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: released by looking glass studios in 1998 and powered by their own in - house developed dark engine , " thief : the dark project " was considered by many to be a revolutionary game .
Head Entity: thief : the dark project
Tail Entity: looking glass studios
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: The popular game "The Witcher 3: Wild Hunt" was developed by CD Projekt Red and released in 2015, receiving critical acclaim for its storytelling and open-world design.  
Head Entity: The Witcher 3: Wild Hunt  
Tail Entity: CD Projekt Red  

Relation: developer  
Context: "Fortnite," a battle royale game that took the world by storm, was developed by Epic Games and launched in 2017, quickly becoming a cultural phenomenon.  
Head Entity: Fortnite  
Tail Entity: Epic Games  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: neptune is the second studio album by the london - based band the duke spirit and the last with the original lead guitarist , dan higgins .
Head Entity: the duke spirit
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the tech startup was founded in silicon valley by a group of innovative engineers looking to revolutionize the industry.  
Head Entity: the tech startup  
Tail Entity: silicon valley  

Relation: location of formation  
Context: the famous rock band was established in new york city, where they quickly gained a following and recorded their first album.  
Head Entity: the famous rock band  
Tail Entity: new york city  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: " shining time station " won a number of awards and significantly increased the popularity of the " thomas " media franchise in the united states .
Head Entity: shining time station
Tail Entity: united states
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous dish "sushi" is traditionally associated with Japan and has gained immense popularity worldwide.  
Head Entity: sushi  
Tail Entity: Japan  

Relation: country of origin  
Context: The iconic brand "Guinness" is known for its rich stout beer, which originated in Ireland and is now enjoyed globally.  
Head Entity: Guinness  
Tail Entity: Ireland  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 94.60%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.84%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.79%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.68%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.61%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.96%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.40%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.39%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.51%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.62%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.73%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.93%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.02%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 96.11%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.20%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.14%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 96.22%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.30%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.32%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 96.27%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 96.23%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.30%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 96.02%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.98%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.91%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.97%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.24%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 94.60%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.84%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.79%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.68%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.61%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.96%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.40%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.39%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.51%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.62%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.73%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.93%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.02%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 96.11%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.20%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.14%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 96.22%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.30%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.32%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 96.27%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 96.23%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.30%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 96.02%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.98%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.91%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.97%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.24%   
cur_acc:  ['0.9524']
his_acc:  ['0.9524']
CurrentTrain: epoch  0, batch     0 | loss: 7.4974923CurrentTrain: epoch  0, batch     1 | loss: 8.5202293CurrentTrain: epoch  0, batch     2 | loss: 7.0148621CurrentTrain: epoch  0, batch     3 | loss: 7.3921161CurrentTrain: epoch  1, batch     0 | loss: 7.3319364CurrentTrain: epoch  1, batch     1 | loss: 6.6947145CurrentTrain: epoch  1, batch     2 | loss: 7.0990009CurrentTrain: epoch  1, batch     3 | loss: 5.7481194CurrentTrain: epoch  2, batch     0 | loss: 6.7974477CurrentTrain: epoch  2, batch     1 | loss: 6.3578424CurrentTrain: epoch  2, batch     2 | loss: 5.0814180CurrentTrain: epoch  2, batch     3 | loss: 6.3003111CurrentTrain: epoch  3, batch     0 | loss: 5.5487890CurrentTrain: epoch  3, batch     1 | loss: 6.3407688CurrentTrain: epoch  3, batch     2 | loss: 5.6042109CurrentTrain: epoch  3, batch     3 | loss: 4.2640285CurrentTrain: epoch  4, batch     0 | loss: 5.0246043CurrentTrain: epoch  4, batch     1 | loss: 5.4417686CurrentTrain: epoch  4, batch     2 | loss: 5.1182141CurrentTrain: epoch  4, batch     3 | loss: 4.9197903CurrentTrain: epoch  5, batch     0 | loss: 4.5105643CurrentTrain: epoch  5, batch     1 | loss: 4.6918850CurrentTrain: epoch  5, batch     2 | loss: 4.9317350CurrentTrain: epoch  5, batch     3 | loss: 3.3118978CurrentTrain: epoch  6, batch     0 | loss: 3.8378854CurrentTrain: epoch  6, batch     1 | loss: 4.3676243CurrentTrain: epoch  6, batch     2 | loss: 4.2961063CurrentTrain: epoch  6, batch     3 | loss: 6.6506534CurrentTrain: epoch  7, batch     0 | loss: 3.9111338CurrentTrain: epoch  7, batch     1 | loss: 4.1665735CurrentTrain: epoch  7, batch     2 | loss: 4.3054128CurrentTrain: epoch  7, batch     3 | loss: 3.0347657CurrentTrain: epoch  8, batch     0 | loss: 3.7353346CurrentTrain: epoch  8, batch     1 | loss: 3.4494677CurrentTrain: epoch  8, batch     2 | loss: 4.1839046CurrentTrain: epoch  8, batch     3 | loss: 6.2662396CurrentTrain: epoch  9, batch     0 | loss: 3.7996833CurrentTrain: epoch  9, batch     1 | loss: 3.1577654CurrentTrain: epoch  9, batch     2 | loss: 3.8183861CurrentTrain: epoch  9, batch     3 | loss: 4.6760664
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: the andesite line , a zone of intense volcanic and seismic activity , is a major regional distinction in the pacific .
Head Entity: andesite line
Tail Entity: pacific
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The Great Barrier Reef, a stunning natural wonder, stretches along the northeastern coast of Australia.  
Head Entity: Great Barrier Reef  
Tail Entity: Australia  

Relation: located on terrain feature  
Context: Mount Everest, the highest peak in the world, is part of the Himalayas and attracts climbers from around the globe.  
Head Entity: Mount Everest  
Tail Entity: Himalayas  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: gordeeva was married to the 1998 olympic men 's singles champion , ilia kulik .
Head Entity: ilia kulik
Tail Entity: 1998 olympic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: The renowned scientist Marie Curie was a key participant of the International Congress of Radiology in 1925.  
Head Entity: Marie Curie  
Tail Entity: International Congress of Radiology  

Relation: participant of  
Context: The local football club was a participant of the national championship held last summer.  
Head Entity: local football club  
Tail Entity: national championship  
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: luna subsequently had a supporting role in the comedy film " gravy " , directed by james roday , which was released in october 2015 .
Head Entity: gravy
Tail Entity: james roday
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: the acclaimed drama series "the crown" was brought to life by the visionary director peter morgan, who skillfully navigated the complexities of the royal family.  
Head Entity: the crown  
Tail Entity: peter morgan  

Relation: director  
Context: in the thrilling action movie "inception," the brilliant director christopher nolan crafted a mind-bending narrative that captivated audiences worldwide.  
Head Entity: inception  
Tail Entity: christopher nolan  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: as of april 2013 , sony isp , known as so - net , released a new fibre service to tokyo for small businesses and homes and also made it available to six surrounding prefectures .
Head Entity: so - net
Tail Entity: sony
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: In 2020, the popular video game franchise Call of Duty was acquired by Activision Blizzard, which has been a major player in the gaming industry for years.  
Head Entity: Call of Duty  
Tail Entity: Activision Blizzard  

Relation: owned by  
Context: The iconic fashion brand Gucci is a subsidiary of the French luxury group Kering, which oversees several high-end labels.  
Head Entity: Gucci  
Tail Entity: Kering  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: remains of taxa that can be confidently assigned to istiodactylidae have been found in the uk and china , in rocks dating from the early cretaceous period ( barremian to aptian stage ) .
Head Entity: barremian
Tail Entity: early cretaceous
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The Great Wall of China is a series of fortifications made of various materials, and it is part of the cultural heritage of the country.  
Head Entity: Great Wall of China  
Tail Entity: cultural heritage  

Relation: part of  
Context: The human heart is a vital organ that is part of the circulatory system, responsible for pumping blood throughout the body.  
Head Entity: human heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: it served as the home of k league side gyeongnam fc and national league side changwon city fc prior to the construction of the changwon football center in 2009 .
Head Entity: changwon football center
Tail Entity: gyeongnam fc
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was once the residence of the famous artist, and now it serves as a gallery showcasing his works, attracting visitors from all over the world.  
Head Entity: historic building  
Tail Entity: famous artist  

Relation: occupant  
Context: After the renovation, the old factory was transformed into a vibrant co-working space, where numerous startups and freelancers now thrive.  
Head Entity: old factory  
Tail Entity: startups
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: reading town hall was built in several phases between 1786 and 1897 , although the principal facade was designed by alfred waterhouse in 1875 .
Head Entity: reading town hall
Tail Entity: alfred waterhouse
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: the new art museum was designed by renowned architect zaha hadid, whose innovative style has transformed modern architecture.  
Head Entity: new art museum  
Tail Entity: zaha hadid  

Relation: architect  
Context: the iconic sydney opera house was the brainchild of architect jørn utzon, who won the design competition in 1957.  
Head Entity: sydney opera house  
Tail Entity: jørn utzon  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: fantcha was born as francelina durão almeida in mindelo on são vicente island , cape verde .
Head Entity: fantcha
Tail Entity: são vicente island
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: after living in new york city for several years, she decided to move back to her hometown in los angeles.  
Head Entity: she  
Tail Entity: los angeles  

Relation: residence  
Context: the famous author spent most of his life in a quaint cottage located in the picturesque village of haworth.  
Head Entity: the famous author  
Tail Entity: haworth  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Ludwig van Beethoven created some of the most celebrated symphonies in classical music history.  
Head Entity: symphonies  
Tail Entity: Ludwig van Beethoven  

Relation: composer  
Context: Wolfgang Amadeus Mozart was a prolific and influential composer of the classical era, known for his operas and symphonies.  
Head Entity: operas  
Tail Entity: Wolfgang Amadeus Mozart  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: the postseason tournament concluded with the san francisco 49ers defeating the cincinnati bengals in super bowl xvi , 26–21 , on january 24 , 1982 , at the pontiac silverdome in pontiac , michigan .
Head Entity: super bowl xvi
Tail Entity: pontiac silverdome
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: the annual music festival took place in the vibrant city of new orleans, attracting thousands of visitors from around the world.  
Head Entity: music festival  
Tail Entity: new orleans  

Relation: location  
Context: the historic battle was fought at gettysburg, a pivotal moment in the civil war that changed the course of american history.  
Head Entity: battle  
Tail Entity: gettysburg  
Mixup data size:  200
MixupTrain:  epoch  0, batch     0 | loss: 5.9689583MixupTrain:  epoch  0, batch     1 | loss: 5.8987637MixupTrain:  epoch  0, batch     2 | loss: 5.1529250MixupTrain:  epoch  0, batch     3 | loss: 5.5218488MixupTrain:  epoch  0, batch     4 | loss: 5.2047539MixupTrain:  epoch  0, batch     5 | loss: 5.0991615MixupTrain:  epoch  0, batch     6 | loss: 5.8185329MixupTrain:  epoch  0, batch     7 | loss: 5.3337680MixupTrain:  epoch  0, batch     8 | loss: 4.6019665MixupTrain:  epoch  0, batch     9 | loss: 4.5280528MixupTrain:  epoch  0, batch    10 | loss: 4.9458052MixupTrain:  epoch  0, batch    11 | loss: 4.6431095MixupTrain:  epoch  0, batch    12 | loss: 4.5087650
MemoryTrain:  epoch  0, batch     0 | loss: 4.6187706MemoryTrain:  epoch  0, batch     1 | loss: 3.0283091MemoryTrain:  epoch  0, batch     2 | loss: 3.9661384MemoryTrain:  epoch  0, batch     3 | loss: 5.2918830MemoryTrain:  epoch  1, batch     0 | loss: 4.2815571MemoryTrain:  epoch  1, batch     1 | loss: 3.9707737MemoryTrain:  epoch  1, batch     2 | loss: 3.1487656MemoryTrain:  epoch  1, batch     3 | loss: 3.6936536MemoryTrain:  epoch  2, batch     0 | loss: 2.7983003MemoryTrain:  epoch  2, batch     1 | loss: 3.7683055MemoryTrain:  epoch  2, batch     2 | loss: 3.3208771MemoryTrain:  epoch  2, batch     3 | loss: 2.3207231MemoryTrain:  epoch  3, batch     0 | loss: 2.4405589MemoryTrain:  epoch  3, batch     1 | loss: 2.8756773MemoryTrain:  epoch  3, batch     2 | loss: 2.6033745MemoryTrain:  epoch  3, batch     3 | loss: 2.6934280MemoryTrain:  epoch  4, batch     0 | loss: 2.5737567MemoryTrain:  epoch  4, batch     1 | loss: 2.1858792MemoryTrain:  epoch  4, batch     2 | loss: 2.2447355MemoryTrain:  epoch  4, batch     3 | loss: 2.0644541MemoryTrain:  epoch  5, batch     0 | loss: 2.3362255MemoryTrain:  epoch  5, batch     1 | loss: 1.8568056MemoryTrain:  epoch  5, batch     2 | loss: 1.8120456MemoryTrain:  epoch  5, batch     3 | loss: 1.9838002MemoryTrain:  epoch  6, batch     0 | loss: 1.9128872MemoryTrain:  epoch  6, batch     1 | loss: 1.9005538MemoryTrain:  epoch  6, batch     2 | loss: 1.7853562MemoryTrain:  epoch  6, batch     3 | loss: 1.6624811MemoryTrain:  epoch  7, batch     0 | loss: 2.0620060MemoryTrain:  epoch  7, batch     1 | loss: 1.9771762MemoryTrain:  epoch  7, batch     2 | loss: 1.4654242MemoryTrain:  epoch  7, batch     3 | loss: 1.5562875MemoryTrain:  epoch  8, batch     0 | loss: 1.5058140MemoryTrain:  epoch  8, batch     1 | loss: 1.4766630MemoryTrain:  epoch  8, batch     2 | loss: 1.8057625MemoryTrain:  epoch  8, batch     3 | loss: 1.9169787MemoryTrain:  epoch  9, batch     0 | loss: 1.6376488MemoryTrain:  epoch  9, batch     1 | loss: 1.6784134MemoryTrain:  epoch  9, batch     2 | loss: 1.5172250MemoryTrain:  epoch  9, batch     3 | loss: 1.3268865
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 39.58%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 40.62%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 42.50%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 42.71%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 49.11%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 54.69%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 59.72%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 63.75%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 66.48%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 68.23%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 70.19%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 73.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 75.39%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 76.84%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 77.96%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 76.88%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 75.00%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 74.72%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 74.46%   [EVAL] batch:   23 | acc: 43.75%,  total acc: 73.18%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 72.50%   [EVAL] batch:   25 | acc: 25.00%,  total acc: 70.67%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 68.52%   [EVAL] batch:   27 | acc: 12.50%,  total acc: 66.52%   [EVAL] batch:   28 | acc: 25.00%,  total acc: 65.09%   [EVAL] batch:   29 | acc: 25.00%,  total acc: 63.75%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 61.90%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 61.72%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 62.88%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 63.79%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 64.82%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 65.45%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 66.05%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 66.78%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 67.47%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 68.28%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 68.90%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 69.35%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 70.06%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 70.74%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 71.11%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 71.33%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 71.28%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 71.35%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 71.68%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:   50 | acc: 50.00%,  total acc: 71.45%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 71.75%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 71.70%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 71.53%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 71.70%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 71.60%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 71.23%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 70.66%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 70.94%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 70.80%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 70.87%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 70.14%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 84.66%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 84.62%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 85.27%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 87.85%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 88.49%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 88.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 88.99%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 88.64%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 88.32%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 88.28%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 88.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 88.70%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 89.12%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 89.44%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 89.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 90.12%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 90.43%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 90.72%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 90.99%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 91.32%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 91.55%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 91.78%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.99%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.38%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.56%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 92.59%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 92.78%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 92.93%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 92.82%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 92.71%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 92.73%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 92.88%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 92.77%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 92.55%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 92.57%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 92.59%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 92.39%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 92.30%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 92.21%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 92.24%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 92.37%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 92.40%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 92.42%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 92.54%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 92.06%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 91.21%   [EVAL] batch:   64 | acc: 43.75%,  total acc: 90.48%   [EVAL] batch:   65 | acc: 50.00%,  total acc: 89.87%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 89.18%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 88.33%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 88.13%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 88.30%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 88.38%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 88.70%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 88.68%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 88.67%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 88.82%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 88.88%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 89.02%   [EVAL] batch:   78 | acc: 100.00%,  total acc: 89.16%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 89.30%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 89.43%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 88.87%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 88.48%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 88.02%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 87.87%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 87.50%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 87.00%   [EVAL] batch:   87 | acc: 37.50%,  total acc: 86.43%   [EVAL] batch:   88 | acc: 31.25%,  total acc: 85.81%   [EVAL] batch:   89 | acc: 6.25%,  total acc: 84.93%   [EVAL] batch:   90 | acc: 18.75%,  total acc: 84.20%   [EVAL] batch:   91 | acc: 25.00%,  total acc: 83.56%   [EVAL] batch:   92 | acc: 18.75%,  total acc: 82.86%   [EVAL] batch:   93 | acc: 12.50%,  total acc: 82.11%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 82.24%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 82.42%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 82.54%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 82.65%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 82.70%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 82.75%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 82.92%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 83.03%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 83.13%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 83.23%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 83.49%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 83.59%   [EVAL] batch:  107 | acc: 87.50%,  total acc: 83.62%   [EVAL] batch:  108 | acc: 68.75%,  total acc: 83.49%   [EVAL] batch:  109 | acc: 81.25%,  total acc: 83.47%   [EVAL] batch:  110 | acc: 75.00%,  total acc: 83.39%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 83.48%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 83.24%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 83.06%   [EVAL] batch:  114 | acc: 75.00%,  total acc: 82.99%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 83.03%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 82.85%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 82.84%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 82.72%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 82.45%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 82.18%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 81.92%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 81.91%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 81.80%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 81.65%   
cur_acc:  ['0.9524', '0.7014']
his_acc:  ['0.9524', '0.8165']
CurrentTrain: epoch  0, batch     0 | loss: 5.5308504CurrentTrain: epoch  0, batch     1 | loss: 6.0116940CurrentTrain: epoch  0, batch     2 | loss: 5.7510557CurrentTrain: epoch  0, batch     3 | loss: 7.9633884CurrentTrain: epoch  1, batch     0 | loss: 4.9462872CurrentTrain: epoch  1, batch     1 | loss: 4.7895670CurrentTrain: epoch  1, batch     2 | loss: 4.9411592CurrentTrain: epoch  1, batch     3 | loss: 4.8826160CurrentTrain: epoch  2, batch     0 | loss: 4.9016294CurrentTrain: epoch  2, batch     1 | loss: 4.0475321CurrentTrain: epoch  2, batch     2 | loss: 3.6955795CurrentTrain: epoch  2, batch     3 | loss: 3.8750083CurrentTrain: epoch  3, batch     0 | loss: 3.6516390CurrentTrain: epoch  3, batch     1 | loss: 3.3759215CurrentTrain: epoch  3, batch     2 | loss: 4.2586222CurrentTrain: epoch  3, batch     3 | loss: 2.9978709CurrentTrain: epoch  4, batch     0 | loss: 3.0014236CurrentTrain: epoch  4, batch     1 | loss: 3.7948976CurrentTrain: epoch  4, batch     2 | loss: 3.5382900CurrentTrain: epoch  4, batch     3 | loss: 3.1942229CurrentTrain: epoch  5, batch     0 | loss: 3.7346239CurrentTrain: epoch  5, batch     1 | loss: 3.2771230CurrentTrain: epoch  5, batch     2 | loss: 3.3392520CurrentTrain: epoch  5, batch     3 | loss: 3.1076579CurrentTrain: epoch  6, batch     0 | loss: 3.1844134CurrentTrain: epoch  6, batch     1 | loss: 3.2190905CurrentTrain: epoch  6, batch     2 | loss: 3.0082107CurrentTrain: epoch  6, batch     3 | loss: 2.1911621CurrentTrain: epoch  7, batch     0 | loss: 2.5224540CurrentTrain: epoch  7, batch     1 | loss: 2.5990632CurrentTrain: epoch  7, batch     2 | loss: 2.7480438CurrentTrain: epoch  7, batch     3 | loss: 4.2325048CurrentTrain: epoch  8, batch     0 | loss: 3.1260915CurrentTrain: epoch  8, batch     1 | loss: 2.3024063CurrentTrain: epoch  8, batch     2 | loss: 2.5058248CurrentTrain: epoch  8, batch     3 | loss: 4.9325695CurrentTrain: epoch  9, batch     0 | loss: 2.6396885CurrentTrain: epoch  9, batch     1 | loss: 2.4898510CurrentTrain: epoch  9, batch     2 | loss: 2.8832695CurrentTrain: epoch  9, batch     3 | loss: 3.0553145
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: rohan is the son of narayana murthy , founder of infosys , and his wife sudha murthy , an engineer and author of note .
Head Entity: narayana murthy
Tail Entity: sudha murthy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: elon musk, the CEO of spacex and tesla, was previously married to talulah riley, a talented actress and writer.  
Head Entity: elon musk  
Tail Entity: talulah riley  

Relation: spouse  
Context: barack obama, the 44th president of the united states, has been married to michelle obama, a lawyer and author, since 1992.  
Head Entity: barack obama  
Tail Entity: michelle obama  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: darlene released , " darlene mccoy " , on may 8 , 2007 with label emi gospel .
Head Entity: darlene mccoy
Tail Entity: emi gospel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: the band released their latest album, "echoes of the past," under the label universal music group in 2021.  
Head Entity: the band  
Tail Entity: universal music group  

Relation: record label  
Context: after signing a new contract, the artist announced that their upcoming single would be distributed by sony music entertainment.  
Head Entity: the artist  
Tail Entity: sony music entertainment  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: distributed by paramount pictures , " the italian job " was theatrically released in the united states on may 30 , 2003 , and grossed over $ 176 million worldwide .
Head Entity: the italian job
Tail Entity: paramount pictures
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: the film was distributed by universal pictures and received critical acclaim upon its release in 2015.  
Head Entity: the film  
Tail Entity: universal pictures  

Relation: distributor  
Context: the album was distributed by sony music and quickly climbed the charts, becoming a bestseller.  
Head Entity: the album  
Tail Entity: sony music  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: the highway was re - aligned in 1963 to serve the port of vancouver , traveling along the columbia river to ridgefield west of vancouver lake onto a roadway that was to be constructed .
Head Entity: port of vancouver
Tail Entity: columbia river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: The city of Miami is famous for its beautiful beaches and vibrant nightlife, situated right along the Atlantic Ocean, making it a popular destination for tourists.  
Head Entity: Miami  
Tail Entity: Atlantic Ocean  

Relation: located in or next to body of water  
Context: The quaint village of Portage is nestled beside the serene waters of Lake Michigan, providing stunning views and a peaceful atmosphere for its residents.  
Head Entity: Portage  
Tail Entity: Lake Michigan  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: cbs and viacom split again in 2005 , but both are still owned by national amusements .
Head Entity: national amusements
Tail Entity: viacom
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Sample 1:  
Relation: subsidiary  
Context: In 2018, Facebook acquired WhatsApp, making it a subsidiary of the social media giant.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Sample 2:  
Relation: subsidiary  
Context: Toyota has several subsidiaries, including Lexus, which focuses on luxury vehicles.  
Head Entity: Toyota  
Tail Entity: Lexus  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: he challenged seven other swimmers in heat four including ukraine 's rostyslav svanidze , a top 16 finalist in atlanta four years earlier .
Head Entity: rostyslav svanidze
Tail Entity: ukraine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: after years of living in the united states, the renowned artist decided to apply for dual citizenship, allowing him to maintain his ties to his homeland, italy.  
Head Entity: the renowned artist  
Tail Entity: italy  

Relation: country of citizenship  
Context: during the international conference, the delegate from canada proudly spoke about the rights and privileges of being a citizen of his country, emphasizing the importance of citizenship in shaping national identity.  
Head Entity: the delegate from canada  
Tail Entity: canada  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: buck moved to austin in the mid-1970s and joined the blues rock group , the fabulous thunderbirds , along with keith ferguson , jimmie vaughan , and kim wilson .
Head Entity: the fabulous thunderbirds
Tail Entity: blues rock
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the album "thriller" by michael jackson is widely regarded as a landmark in pop music history, blending elements of rock, funk, and soul.  
Head Entity: michael jackson  
Tail Entity: pop  

Relation: genre  
Context: the film "inception," directed by christopher nolan, is a complex narrative that combines science fiction with psychological thriller elements.  
Head Entity: inception  
Tail Entity: science fiction  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: san lázaro is 660 km far from asunción and is located at the exact site of the confluence of the apa river with paraguay river , bordering brazil - chaco .
Head Entity: apa river
Tail Entity: paraguay river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: the mississippi river flows into the gulf of mexico, creating a rich delta ecosystem that supports diverse wildlife and plant species.  
Head Entity: mississippi river  
Tail Entity: gulf of mexico  

Relation: mouth of the watercourse  
Context: the rhine river, known for its scenic beauty, empties into the north sea, providing a vital shipping route for trade in europe.  
Head Entity: rhine river  
Tail Entity: north sea  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: tarun dey is an indian football defender who played for india in the 1984 asian cup .
Head Entity: tarun dey
Tail Entity: defender
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: lebron james is a professional basketball forward known for his exceptional skills and leadership on the court, currently playing for the los angeles lakers.  
Head Entity: lebron james  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: serena williams is a renowned tennis player who has dominated the women's singles circuit with her powerful serve and aggressive play style.  
Head Entity: serena williams  
Tail Entity: tennis player  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: hammami made his international debut for tunisia in 2007 , and represented them at the africa cup of nations in 2010 and 2013 .
Head Entity: 2010
Tail Entity: africa cup of nations
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2021 UEFA European Championship was held in various cities across Europe, showcasing top national teams competing for the title.  
Head Entity: 2021  
Tail Entity: UEFA European Championship  

Relation: sports season of league or competition  
Context: The 2022 FIFA World Cup took place in Qatar, featuring teams from around the globe battling for the prestigious trophy.  
Head Entity: 2022  
Tail Entity: FIFA World Cup  
Mixup data size:  259
MixupTrain:  epoch  0, batch     0 | loss: 4.0227052MixupTrain:  epoch  0, batch     1 | loss: 3.8146696MixupTrain:  epoch  0, batch     2 | loss: 3.2629981MixupTrain:  epoch  0, batch     3 | loss: 3.3866754MixupTrain:  epoch  0, batch     4 | loss: 3.3448798MixupTrain:  epoch  0, batch     5 | loss: 3.6586414MixupTrain:  epoch  0, batch     6 | loss: 3.1713183MixupTrain:  epoch  0, batch     7 | loss: 3.4113162MixupTrain:  epoch  0, batch     8 | loss: 2.9226715MixupTrain:  epoch  0, batch     9 | loss: 3.2410331MixupTrain:  epoch  0, batch    10 | loss: 3.1497933MixupTrain:  epoch  0, batch    11 | loss: 3.0741606MixupTrain:  epoch  0, batch    12 | loss: 3.1399127MixupTrain:  epoch  0, batch    13 | loss: 2.9188188MixupTrain:  epoch  0, batch    14 | loss: 3.0020091MixupTrain:  epoch  0, batch    15 | loss: 2.7827369MixupTrain:  epoch  0, batch    16 | loss: 2.5164661
MemoryTrain:  epoch  0, batch     0 | loss: 3.9314766MemoryTrain:  epoch  0, batch     1 | loss: 2.8027077MemoryTrain:  epoch  0, batch     2 | loss: 3.5802827MemoryTrain:  epoch  0, batch     3 | loss: 2.6245592MemoryTrain:  epoch  0, batch     4 | loss: 2.9552646MemoryTrain:  epoch  0, batch     5 | loss: 4.0547862MemoryTrain:  epoch  1, batch     0 | loss: 3.5622675MemoryTrain:  epoch  1, batch     1 | loss: 2.8711400MemoryTrain:  epoch  1, batch     2 | loss: 2.9458833MemoryTrain:  epoch  1, batch     3 | loss: 3.0490484MemoryTrain:  epoch  1, batch     4 | loss: 3.0630579MemoryTrain:  epoch  1, batch     5 | loss: 1.6794151MemoryTrain:  epoch  2, batch     0 | loss: 2.0114672MemoryTrain:  epoch  2, batch     1 | loss: 2.1932039MemoryTrain:  epoch  2, batch     2 | loss: 2.5573080MemoryTrain:  epoch  2, batch     3 | loss: 2.5619543MemoryTrain:  epoch  2, batch     4 | loss: 2.8746264MemoryTrain:  epoch  2, batch     5 | loss: 2.5787320MemoryTrain:  epoch  3, batch     0 | loss: 1.6954591MemoryTrain:  epoch  3, batch     1 | loss: 2.4726489MemoryTrain:  epoch  3, batch     2 | loss: 2.2458582MemoryTrain:  epoch  3, batch     3 | loss: 2.6225200MemoryTrain:  epoch  3, batch     4 | loss: 2.7369535MemoryTrain:  epoch  3, batch     5 | loss: 1.9893229MemoryTrain:  epoch  4, batch     0 | loss: 2.1601477MemoryTrain:  epoch  4, batch     1 | loss: 2.2215869MemoryTrain:  epoch  4, batch     2 | loss: 1.6929111MemoryTrain:  epoch  4, batch     3 | loss: 1.6556910MemoryTrain:  epoch  4, batch     4 | loss: 2.6492643MemoryTrain:  epoch  4, batch     5 | loss: 1.7177792MemoryTrain:  epoch  5, batch     0 | loss: 2.3627687MemoryTrain:  epoch  5, batch     1 | loss: 1.8924990MemoryTrain:  epoch  5, batch     2 | loss: 1.7583623MemoryTrain:  epoch  5, batch     3 | loss: 1.8892874MemoryTrain:  epoch  5, batch     4 | loss: 1.8826048MemoryTrain:  epoch  5, batch     5 | loss: 1.3202059MemoryTrain:  epoch  6, batch     0 | loss: 1.7819498MemoryTrain:  epoch  6, batch     1 | loss: 1.7113271MemoryTrain:  epoch  6, batch     2 | loss: 1.9448576MemoryTrain:  epoch  6, batch     3 | loss: 1.6998775MemoryTrain:  epoch  6, batch     4 | loss: 1.7793891MemoryTrain:  epoch  6, batch     5 | loss: 1.4669615MemoryTrain:  epoch  7, batch     0 | loss: 1.6887708MemoryTrain:  epoch  7, batch     1 | loss: 1.6521833MemoryTrain:  epoch  7, batch     2 | loss: 1.6171672MemoryTrain:  epoch  7, batch     3 | loss: 1.3499012MemoryTrain:  epoch  7, batch     4 | loss: 1.6167605MemoryTrain:  epoch  7, batch     5 | loss: 1.6712617MemoryTrain:  epoch  8, batch     0 | loss: 1.5692303MemoryTrain:  epoch  8, batch     1 | loss: 1.4165014MemoryTrain:  epoch  8, batch     2 | loss: 1.4597389MemoryTrain:  epoch  8, batch     3 | loss: 1.5146756MemoryTrain:  epoch  8, batch     4 | loss: 1.5369800MemoryTrain:  epoch  8, batch     5 | loss: 1.3618517MemoryTrain:  epoch  9, batch     0 | loss: 1.6001045MemoryTrain:  epoch  9, batch     1 | loss: 1.4525406MemoryTrain:  epoch  9, batch     2 | loss: 1.2808437MemoryTrain:  epoch  9, batch     3 | loss: 1.4212097MemoryTrain:  epoch  9, batch     4 | loss: 1.3366060MemoryTrain:  epoch  9, batch     5 | loss: 1.5098070
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 98.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 98.96%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 99.11%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 99.22%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 97.92%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 95.62%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 95.45%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 94.71%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 92.86%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 91.02%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 89.71%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 89.24%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 89.47%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 88.12%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 87.80%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 86.93%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 85.68%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 85.75%   [EVAL] batch:   25 | acc: 81.25%,  total acc: 85.58%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:   27 | acc: 50.00%,  total acc: 84.15%   [EVAL] batch:   28 | acc: 56.25%,  total acc: 83.19%   [EVAL] batch:   29 | acc: 43.75%,  total acc: 81.88%   [EVAL] batch:   30 | acc: 50.00%,  total acc: 80.85%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 80.66%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 80.30%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 79.96%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 79.29%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 78.89%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 78.78%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 78.85%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 78.91%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 78.66%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 78.57%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 78.78%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 78.55%   [EVAL] batch:   44 | acc: 18.75%,  total acc: 77.22%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 76.09%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 74.87%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 74.09%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 73.09%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 72.38%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 73.94%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 74.42%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 74.89%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 75.33%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 75.66%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 75.97%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 76.27%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 76.67%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 76.84%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 77.12%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 76.69%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 81.77%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 82.21%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 85.29%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 85.76%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 86.51%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 86.56%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 86.61%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 86.36%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 85.68%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 85.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.06%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 86.57%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.05%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.31%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.67%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 89.02%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 89.15%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 89.11%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 89.19%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 89.31%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.09%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.33%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 90.26%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 90.48%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 90.69%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 90.90%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 90.96%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 91.02%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 91.18%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 91.23%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 91.27%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 91.32%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 91.14%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.18%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 91.01%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 90.95%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 91.10%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 91.15%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 91.29%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 91.33%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 90.77%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 89.55%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 88.46%   [EVAL] batch:   65 | acc: 25.00%,  total acc: 87.50%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 86.47%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 85.39%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 84.87%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 84.91%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 85.04%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 85.16%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 85.27%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 85.30%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 85.25%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 85.44%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 85.39%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 85.58%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 85.68%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 85.78%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 85.96%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 85.37%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 84.64%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 83.93%   [EVAL] batch:   84 | acc: 37.50%,  total acc: 83.38%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 82.63%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 82.04%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 81.25%   [EVAL] batch:   88 | acc: 6.25%,  total acc: 80.41%   [EVAL] batch:   89 | acc: 18.75%,  total acc: 79.72%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 78.85%   [EVAL] batch:   91 | acc: 18.75%,  total acc: 78.19%   [EVAL] batch:   92 | acc: 18.75%,  total acc: 77.55%   [EVAL] batch:   93 | acc: 12.50%,  total acc: 76.86%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 76.97%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 77.21%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 77.45%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 77.84%   [EVAL] batch:   99 | acc: 93.75%,  total acc: 78.00%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 78.16%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 78.31%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 78.52%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 78.67%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 78.81%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 79.01%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 79.09%   [EVAL] batch:  107 | acc: 81.25%,  total acc: 79.11%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 79.07%   [EVAL] batch:  109 | acc: 81.25%,  total acc: 79.09%   [EVAL] batch:  110 | acc: 81.25%,  total acc: 79.11%   [EVAL] batch:  111 | acc: 87.50%,  total acc: 79.19%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 79.09%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 79.00%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 79.02%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 79.15%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 79.11%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 79.13%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 79.10%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 78.85%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 78.56%   [EVAL] batch:  121 | acc: 56.25%,  total acc: 78.38%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 78.35%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 78.33%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 78.20%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 78.37%   [EVAL] batch:  126 | acc: 93.75%,  total acc: 78.49%   [EVAL] batch:  127 | acc: 100.00%,  total acc: 78.66%   [EVAL] batch:  128 | acc: 100.00%,  total acc: 78.83%   [EVAL] batch:  129 | acc: 100.00%,  total acc: 78.99%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 79.15%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 79.31%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 79.46%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 79.52%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 79.49%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 79.60%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 79.70%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 79.76%   [EVAL] batch:  138 | acc: 68.75%,  total acc: 79.68%   [EVAL] batch:  139 | acc: 62.50%,  total acc: 79.55%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 79.65%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 79.58%   [EVAL] batch:  142 | acc: 81.25%,  total acc: 79.59%   [EVAL] batch:  143 | acc: 93.75%,  total acc: 79.69%   [EVAL] batch:  144 | acc: 62.50%,  total acc: 79.57%   [EVAL] batch:  145 | acc: 81.25%,  total acc: 79.58%   [EVAL] batch:  146 | acc: 68.75%,  total acc: 79.51%   [EVAL] batch:  147 | acc: 62.50%,  total acc: 79.39%   [EVAL] batch:  148 | acc: 81.25%,  total acc: 79.40%   [EVAL] batch:  149 | acc: 87.50%,  total acc: 79.46%   [EVAL] batch:  150 | acc: 81.25%,  total acc: 79.47%   [EVAL] batch:  151 | acc: 81.25%,  total acc: 79.48%   [EVAL] batch:  152 | acc: 50.00%,  total acc: 79.29%   [EVAL] batch:  153 | acc: 56.25%,  total acc: 79.14%   [EVAL] batch:  154 | acc: 43.75%,  total acc: 78.91%   [EVAL] batch:  155 | acc: 50.00%,  total acc: 78.73%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 78.70%   [EVAL] batch:  157 | acc: 68.75%,  total acc: 78.64%   [EVAL] batch:  158 | acc: 68.75%,  total acc: 78.58%   [EVAL] batch:  159 | acc: 56.25%,  total acc: 78.44%   [EVAL] batch:  160 | acc: 75.00%,  total acc: 78.42%   [EVAL] batch:  161 | acc: 68.75%,  total acc: 78.36%   [EVAL] batch:  162 | acc: 75.00%,  total acc: 78.34%   [EVAL] batch:  163 | acc: 81.25%,  total acc: 78.35%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 78.37%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 78.31%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 78.29%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 78.35%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 78.29%   [EVAL] batch:  169 | acc: 18.75%,  total acc: 77.94%   [EVAL] batch:  170 | acc: 25.00%,  total acc: 77.63%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 77.29%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 77.06%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 76.76%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 76.54%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 76.67%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 76.80%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 76.93%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 77.06%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 77.19%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 77.31%   [EVAL] batch:  181 | acc: 93.75%,  total acc: 77.40%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 77.49%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 77.58%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 77.70%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 77.76%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 77.84%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 77.69%   
cur_acc:  ['0.9524', '0.7014', '0.7669']
his_acc:  ['0.9524', '0.8165', '0.7769']
CurrentTrain: epoch  0, batch     0 | loss: 6.7605319CurrentTrain: epoch  0, batch     1 | loss: 5.9077859CurrentTrain: epoch  0, batch     2 | loss: 6.0779552CurrentTrain: epoch  0, batch     3 | loss: 4.5694647CurrentTrain: epoch  1, batch     0 | loss: 5.7686100CurrentTrain: epoch  1, batch     1 | loss: 5.6007032CurrentTrain: epoch  1, batch     2 | loss: 5.1393056CurrentTrain: epoch  1, batch     3 | loss: 7.3953342CurrentTrain: epoch  2, batch     0 | loss: 4.9087992CurrentTrain: epoch  2, batch     1 | loss: 5.1576090CurrentTrain: epoch  2, batch     2 | loss: 4.8184118CurrentTrain: epoch  2, batch     3 | loss: 5.3812776CurrentTrain: epoch  3, batch     0 | loss: 4.1080818CurrentTrain: epoch  3, batch     1 | loss: 4.2912264CurrentTrain: epoch  3, batch     2 | loss: 5.2064676CurrentTrain: epoch  3, batch     3 | loss: 7.9949880CurrentTrain: epoch  4, batch     0 | loss: 4.5623713CurrentTrain: epoch  4, batch     1 | loss: 3.7499101CurrentTrain: epoch  4, batch     2 | loss: 4.9465990CurrentTrain: epoch  4, batch     3 | loss: 4.7159986CurrentTrain: epoch  5, batch     0 | loss: 4.6753454CurrentTrain: epoch  5, batch     1 | loss: 3.8613553CurrentTrain: epoch  5, batch     2 | loss: 4.4594812CurrentTrain: epoch  5, batch     3 | loss: 5.5684304CurrentTrain: epoch  6, batch     0 | loss: 4.6859546CurrentTrain: epoch  6, batch     1 | loss: 3.7050595CurrentTrain: epoch  6, batch     2 | loss: 3.6781836CurrentTrain: epoch  6, batch     3 | loss: 5.7525535CurrentTrain: epoch  7, batch     0 | loss: 3.8065760CurrentTrain: epoch  7, batch     1 | loss: 4.2378340CurrentTrain: epoch  7, batch     2 | loss: 3.4751501CurrentTrain: epoch  7, batch     3 | loss: 3.5816474CurrentTrain: epoch  8, batch     0 | loss: 3.8650916CurrentTrain: epoch  8, batch     1 | loss: 3.8586535CurrentTrain: epoch  8, batch     2 | loss: 3.4434378CurrentTrain: epoch  8, batch     3 | loss: 2.4538488CurrentTrain: epoch  9, batch     0 | loss: 3.2119412CurrentTrain: epoch  9, batch     1 | loss: 4.0757351CurrentTrain: epoch  9, batch     2 | loss: 3.9208345CurrentTrain: epoch  9, batch     3 | loss: 3.4129281
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: george lawson sheldon ( may 31 , 1870 – april 4 , 1960 ) was 14th governor of nebraska with service from 1907 until 1909 .
Head Entity: governor
Tail Entity: nebraska
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The Environmental Protection Agency (EPA) is responsible for enforcing regulations related to environmental protection in the United States.  
Head Entity: Environmental Protection Agency  
Tail Entity: United States  

Relation: applies to jurisdiction  
Context: The California Civil Code governs various aspects of civil law within the state of California, including contracts and property rights.  
Head Entity: California Civil Code  
Tail Entity: California  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: it is prominently featured on jean michel jarre 's albums " oxygène " ( 1977 ) and " équinoxe " ( 1978 ) .
Head Entity: équinoxe
Tail Entity: jean michel jarre
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: The legendary band Queen is known for their iconic song "Bohemian Rhapsody," which showcases Freddie Mercury's extraordinary vocal talent.  
Head Entity: Bohemian Rhapsody  
Tail Entity: Freddie Mercury  

Relation: performer  
Context: Taylor Swift captivated audiences with her heartfelt performance of "Love Story" during the awards ceremony last night.  
Head Entity: Love Story  
Tail Entity: Taylor Swift  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: an emd gp49 is a 4-axle diesel locomotive built by general motors electro - motive division .
Head Entity: emd gp49
Tail Entity: general motors electro - motive division
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: the iphone 13 is a smartphone designed and marketed by apple inc.  
Head Entity: iphone 13  
Tail Entity: apple inc.  

Relation: manufacturer  
Context: the model s is an all-electric sedan produced by tesla, inc.  
Head Entity: model s  
Tail Entity: tesla, inc.  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: at the 2016 summer olympics , heath and schofield won the silver k-2 200 event , finishing behind spain 's saúl craviotto and cristian toro .
Head Entity: 2016 summer olympics
Tail Entity: spain
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: during the 2020 presidential election, the democratic party nominated joe biden as their candidate, while the republican party supported donald trump.  
Head Entity: 2020 presidential election  
Tail Entity: democratic party  

Relation: participant  
Context: in the 2018 fifa world cup, france emerged as the champion, defeating croatia in the final match held in moscow.  
Head Entity: 2018 fifa world cup  
Tail Entity: france  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the application is compatible with various operating systems, including macOS, Windows, and Ubuntu.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: Many developers prefer to use Linux as their operating system due to its flexibility and open-source nature.  
Head Entity: developers  
Tail Entity: Linux  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in his next bout , he won the wbc light welterweight title by defeating roger mayweather for a second time .
Head Entity: roger mayweather
Tail Entity: light welterweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: The team secured their place in the tournament by qualifying under the UEFA Champions League regulations.  
Head Entity: UEFA Champions League  
Tail Entity: tournament  

Relation: competition class  
Context: The athlete competed in the 100 meters event, which is classified under the Olympic Games standards.  
Head Entity: Olympic Games  
Tail Entity: 100 meters event  
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: swartz served as swedish prime minister for only seven months before being forced from power after the swedish elections of 1917 and replaced by the liberal government of nils edén .
Head Entity: swedish elections of 1917
Tail Entity: nils edén
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: in the recent municipal elections, the voters chose maria gonzalez as the new mayor of the city, marking a significant shift in local governance.  
Head Entity: municipal elections  
Tail Entity: maria gonzalez  

Relation: successful candidate  
Context: after a closely contested race, the citizens of springfield elected john doe as their new governor, succeeding the long-serving incumbent.  
Head Entity: springfield elections  
Tail Entity: john doe  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: although the latter 's eldest son , lǫgmaðr , appears to have succeeded to the kingship , he was soon forced to contend with factions supporting his younger brothers : haraldr , and óláfr .
Head Entity: lǫgmaðr
Tail Entity: óláfr
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: In the royal family of the ancient kingdom, the two brothers, Eirik and Sigurd, were known for their fierce rivalry, yet they shared the same lineage and upbringing under their parents, King Harald and Queen Ingrid.  
Head Entity: Eirik  
Tail Entity: Sigurd  

Relation: sibling  
Context: During the family reunion, Anna and her brother, Mark, reminisced about their childhood adventures, recalling how their parents always encouraged them to support each other despite their differences.  
Head Entity: Anna  
Tail Entity: Mark  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: iain harrison is a competitive shooter and former british army captain .
Head Entity: iain harrison
Tail Entity: british army
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: sergeant johnson served in the united states marine corps during his military career.  
Head Entity: sergeant johnson  
Tail Entity: united states marine corps  

Relation: military branch  
Context: general smith was a prominent leader in the royal air force, known for his strategic insights.  
Head Entity: general smith  
Tail Entity: royal air force  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: in 1406 adolf married marie of burgundy , daughter of john the fearless and margaret of bavaria .
Head Entity: john the fearless
Tail Entity: marie of burgundy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: in 1980, michael and sarah welcomed their first child, a daughter named emily, into the world.  
Head Entity: michael  
Tail Entity: emily  

Relation: child  
Context: during the family reunion, it was revealed that elizabeth is the proud mother of three children, including her youngest, a son named alex.  
Head Entity: elizabeth  
Tail Entity: alex  
Mixup data size:  319
MixupTrain:  epoch  0, batch     0 | loss: 2.6067834MixupTrain:  epoch  0, batch     1 | loss: 3.1816285MixupTrain:  epoch  0, batch     2 | loss: 2.9688469MixupTrain:  epoch  0, batch     3 | loss: 2.7795662MixupTrain:  epoch  0, batch     4 | loss: 3.4408510MixupTrain:  epoch  0, batch     5 | loss: 2.6794509MixupTrain:  epoch  0, batch     6 | loss: 2.7567926MixupTrain:  epoch  0, batch     7 | loss: 2.9831654MixupTrain:  epoch  0, batch     8 | loss: 2.3199653MixupTrain:  epoch  0, batch     9 | loss: 2.5185693MixupTrain:  epoch  0, batch    10 | loss: 2.8474861MixupTrain:  epoch  0, batch    11 | loss: 2.6102966MixupTrain:  epoch  0, batch    12 | loss: 2.3951486MixupTrain:  epoch  0, batch    13 | loss: 2.3033981MixupTrain:  epoch  0, batch    14 | loss: 2.5274536MixupTrain:  epoch  0, batch    15 | loss: 2.5985347MixupTrain:  epoch  0, batch    16 | loss: 2.2162912MixupTrain:  epoch  0, batch    17 | loss: 2.6796124MixupTrain:  epoch  0, batch    18 | loss: 2.4425998MixupTrain:  epoch  0, batch    19 | loss: 2.4271904
MemoryTrain:  epoch  0, batch     0 | loss: 2.9953792MemoryTrain:  epoch  0, batch     1 | loss: 3.2484841MemoryTrain:  epoch  0, batch     2 | loss: 2.0907516MemoryTrain:  epoch  0, batch     3 | loss: 2.4803882MemoryTrain:  epoch  0, batch     4 | loss: 2.5189228MemoryTrain:  epoch  0, batch     5 | loss: 2.9712749MemoryTrain:  epoch  0, batch     6 | loss: 2.5416954MemoryTrain:  epoch  0, batch     7 | loss: 2.1985161MemoryTrain:  epoch  1, batch     0 | loss: 2.3290734MemoryTrain:  epoch  1, batch     1 | loss: 2.0064280MemoryTrain:  epoch  1, batch     2 | loss: 2.4512739MemoryTrain:  epoch  1, batch     3 | loss: 1.9672222MemoryTrain:  epoch  1, batch     4 | loss: 2.8888617MemoryTrain:  epoch  1, batch     5 | loss: 2.1378236MemoryTrain:  epoch  1, batch     6 | loss: 2.2377543MemoryTrain:  epoch  1, batch     7 | loss: 2.0319679MemoryTrain:  epoch  2, batch     0 | loss: 1.9733775MemoryTrain:  epoch  2, batch     1 | loss: 2.2726059MemoryTrain:  epoch  2, batch     2 | loss: 1.8554337MemoryTrain:  epoch  2, batch     3 | loss: 1.9192194MemoryTrain:  epoch  2, batch     4 | loss: 1.9120994MemoryTrain:  epoch  2, batch     5 | loss: 1.4397714MemoryTrain:  epoch  2, batch     6 | loss: 2.0539916MemoryTrain:  epoch  2, batch     7 | loss: 1.4302601MemoryTrain:  epoch  3, batch     0 | loss: 1.6879513MemoryTrain:  epoch  3, batch     1 | loss: 1.7137985MemoryTrain:  epoch  3, batch     2 | loss: 1.8430197MemoryTrain:  epoch  3, batch     3 | loss: 1.5879611MemoryTrain:  epoch  3, batch     4 | loss: 1.7082729MemoryTrain:  epoch  3, batch     5 | loss: 1.7027659MemoryTrain:  epoch  3, batch     6 | loss: 1.8855828MemoryTrain:  epoch  3, batch     7 | loss: 1.5363740MemoryTrain:  epoch  4, batch     0 | loss: 1.7911549MemoryTrain:  epoch  4, batch     1 | loss: 1.5849590MemoryTrain:  epoch  4, batch     2 | loss: 1.5077698MemoryTrain:  epoch  4, batch     3 | loss: 1.3845556MemoryTrain:  epoch  4, batch     4 | loss: 1.6153809MemoryTrain:  epoch  4, batch     5 | loss: 1.6272359MemoryTrain:  epoch  4, batch     6 | loss: 1.3575304MemoryTrain:  epoch  4, batch     7 | loss: 1.4903797MemoryTrain:  epoch  5, batch     0 | loss: 1.5716169MemoryTrain:  epoch  5, batch     1 | loss: 1.5184361MemoryTrain:  epoch  5, batch     2 | loss: 1.4519048MemoryTrain:  epoch  5, batch     3 | loss: 1.4664648MemoryTrain:  epoch  5, batch     4 | loss: 1.5069747MemoryTrain:  epoch  5, batch     5 | loss: 1.7026677MemoryTrain:  epoch  5, batch     6 | loss: 1.5458505MemoryTrain:  epoch  5, batch     7 | loss: 1.2741706MemoryTrain:  epoch  6, batch     0 | loss: 1.3378450MemoryTrain:  epoch  6, batch     1 | loss: 1.4217920MemoryTrain:  epoch  6, batch     2 | loss: 1.6865873MemoryTrain:  epoch  6, batch     3 | loss: 1.5947392MemoryTrain:  epoch  6, batch     4 | loss: 1.4825957MemoryTrain:  epoch  6, batch     5 | loss: 1.5225379MemoryTrain:  epoch  6, batch     6 | loss: 1.4068578MemoryTrain:  epoch  6, batch     7 | loss: 1.4686210MemoryTrain:  epoch  7, batch     0 | loss: 1.4081950MemoryTrain:  epoch  7, batch     1 | loss: 1.4183496MemoryTrain:  epoch  7, batch     2 | loss: 1.3960184MemoryTrain:  epoch  7, batch     3 | loss: 1.3693922MemoryTrain:  epoch  7, batch     4 | loss: 1.3941824MemoryTrain:  epoch  7, batch     5 | loss: 1.5516655MemoryTrain:  epoch  7, batch     6 | loss: 1.4895544MemoryTrain:  epoch  7, batch     7 | loss: 1.4381671MemoryTrain:  epoch  8, batch     0 | loss: 1.4586650MemoryTrain:  epoch  8, batch     1 | loss: 1.4259933MemoryTrain:  epoch  8, batch     2 | loss: 1.4780910MemoryTrain:  epoch  8, batch     3 | loss: 1.3789861MemoryTrain:  epoch  8, batch     4 | loss: 1.1992958MemoryTrain:  epoch  8, batch     5 | loss: 1.2555399MemoryTrain:  epoch  8, batch     6 | loss: 1.4260715MemoryTrain:  epoch  8, batch     7 | loss: 1.3573759MemoryTrain:  epoch  9, batch     0 | loss: 1.3984210MemoryTrain:  epoch  9, batch     1 | loss: 1.3475997MemoryTrain:  epoch  9, batch     2 | loss: 1.3408184MemoryTrain:  epoch  9, batch     3 | loss: 1.2984829MemoryTrain:  epoch  9, batch     4 | loss: 1.2976129MemoryTrain:  epoch  9, batch     5 | loss: 1.3592989MemoryTrain:  epoch  9, batch     6 | loss: 1.3062758MemoryTrain:  epoch  9, batch     7 | loss: 1.3480201
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 87.13%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 88.12%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 87.22%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 86.96%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 87.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 88.22%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 88.66%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 89.44%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 89.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 90.12%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 90.43%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 90.72%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 90.99%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 91.49%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 91.72%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 91.78%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.99%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 91.72%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 91.16%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 91.22%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 91.28%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 91.05%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 90.14%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 89.27%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 88.56%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 88.28%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 87.63%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 87.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 87.62%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 87.74%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 87.62%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 87.73%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 87.83%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 87.28%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 86.53%   [EVAL] batch:   58 | acc: 25.00%,  total acc: 85.49%   [EVAL] batch:   59 | acc: 31.25%,  total acc: 84.58%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 83.71%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 82.86%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 81.94%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 80.56%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 80.62%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 78.65%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 78.85%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 79.46%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 80.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 81.64%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 82.35%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 83.88%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 84.23%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 83.70%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 83.85%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.95%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 85.27%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.78%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.69%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.11%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 87.13%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 86.96%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 86.81%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 86.82%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 87.01%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.34%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.66%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.96%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 88.24%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 88.23%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 88.21%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 88.06%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 87.91%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 87.24%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 87.24%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 86.88%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 86.89%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 86.90%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 87.03%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 87.15%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 87.16%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 87.28%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 87.17%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 87.07%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 87.08%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 87.19%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 87.30%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 87.20%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 86.71%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 85.74%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 84.71%   [EVAL] batch:   65 | acc: 25.00%,  total acc: 83.81%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 82.93%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 81.99%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 81.70%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 81.79%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 81.95%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 82.12%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 82.28%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 82.43%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 82.57%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 82.47%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 82.45%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 82.59%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 82.73%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 82.87%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 82.32%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 81.63%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 81.10%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 80.66%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 80.09%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 79.53%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 78.76%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 77.88%   [EVAL] batch:   89 | acc: 6.25%,  total acc: 77.08%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 76.24%   [EVAL] batch:   91 | acc: 18.75%,  total acc: 75.61%   [EVAL] batch:   92 | acc: 12.50%,  total acc: 74.93%   [EVAL] batch:   93 | acc: 12.50%,  total acc: 74.27%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 74.41%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 74.67%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 74.87%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 75.13%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 75.25%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 75.31%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 75.56%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 75.74%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 75.97%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 76.08%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 76.25%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 76.47%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 76.64%   [EVAL] batch:  107 | acc: 87.50%,  total acc: 76.74%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 76.72%   [EVAL] batch:  109 | acc: 81.25%,  total acc: 76.76%   [EVAL] batch:  110 | acc: 81.25%,  total acc: 76.80%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 76.95%   [EVAL] batch:  112 | acc: 37.50%,  total acc: 76.60%   [EVAL] batch:  113 | acc: 31.25%,  total acc: 76.21%   [EVAL] batch:  114 | acc: 50.00%,  total acc: 75.98%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 75.97%   [EVAL] batch:  116 | acc: 43.75%,  total acc: 75.69%   [EVAL] batch:  117 | acc: 56.25%,  total acc: 75.53%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 75.37%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 75.16%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 74.95%   [EVAL] batch:  121 | acc: 56.25%,  total acc: 74.80%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 74.80%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 74.75%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 74.65%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 74.60%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 74.41%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 74.32%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 73.98%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 73.80%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 73.71%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 73.77%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 73.97%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 74.07%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 74.17%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 74.26%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 74.32%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 74.41%   [EVAL] batch:  138 | acc: 75.00%,  total acc: 74.42%   [EVAL] batch:  139 | acc: 81.25%,  total acc: 74.46%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 74.56%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 74.56%   [EVAL] batch:  142 | acc: 81.25%,  total acc: 74.61%   [EVAL] batch:  143 | acc: 87.50%,  total acc: 74.70%   [EVAL] batch:  144 | acc: 50.00%,  total acc: 74.53%   [EVAL] batch:  145 | acc: 50.00%,  total acc: 74.36%   [EVAL] batch:  146 | acc: 50.00%,  total acc: 74.19%   [EVAL] batch:  147 | acc: 50.00%,  total acc: 74.03%   [EVAL] batch:  148 | acc: 43.75%,  total acc: 73.83%   [EVAL] batch:  149 | acc: 62.50%,  total acc: 73.75%   [EVAL] batch:  150 | acc: 100.00%,  total acc: 73.92%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 74.05%   [EVAL] batch:  152 | acc: 87.50%,  total acc: 74.14%   [EVAL] batch:  153 | acc: 68.75%,  total acc: 74.11%   [EVAL] batch:  154 | acc: 75.00%,  total acc: 74.11%   [EVAL] batch:  155 | acc: 68.75%,  total acc: 74.08%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 74.04%   [EVAL] batch:  157 | acc: 68.75%,  total acc: 74.01%   [EVAL] batch:  158 | acc: 68.75%,  total acc: 73.98%   [EVAL] batch:  159 | acc: 56.25%,  total acc: 73.87%   [EVAL] batch:  160 | acc: 81.25%,  total acc: 73.91%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 73.96%   [EVAL] batch:  162 | acc: 75.00%,  total acc: 73.96%   [EVAL] batch:  163 | acc: 75.00%,  total acc: 73.97%   [EVAL] batch:  164 | acc: 93.75%,  total acc: 74.09%   [EVAL] batch:  165 | acc: 81.25%,  total acc: 74.13%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 74.14%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 74.22%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 74.19%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 73.93%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 73.68%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 73.47%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 73.30%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 73.06%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 72.86%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 73.01%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 73.16%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 73.31%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 73.46%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 73.61%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 73.76%   [EVAL] batch:  181 | acc: 87.50%,  total acc: 73.83%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 73.84%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 73.95%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 74.05%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 74.09%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 74.16%   [EVAL] batch:  187 | acc: 87.50%,  total acc: 74.24%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 74.31%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 74.31%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 74.38%   [EVAL] batch:  191 | acc: 87.50%,  total acc: 74.45%   [EVAL] batch:  192 | acc: 87.50%,  total acc: 74.51%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 74.52%   [EVAL] batch:  194 | acc: 93.75%,  total acc: 74.62%   [EVAL] batch:  195 | acc: 93.75%,  total acc: 74.71%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 74.71%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 74.78%   [EVAL] batch:  198 | acc: 100.00%,  total acc: 74.91%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 74.97%   [EVAL] batch:  200 | acc: 100.00%,  total acc: 75.09%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 75.12%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 75.12%   [EVAL] batch:  203 | acc: 93.75%,  total acc: 75.21%   [EVAL] batch:  204 | acc: 93.75%,  total acc: 75.30%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 75.39%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 75.48%   [EVAL] batch:  207 | acc: 93.75%,  total acc: 75.57%   [EVAL] batch:  208 | acc: 68.75%,  total acc: 75.54%   [EVAL] batch:  209 | acc: 75.00%,  total acc: 75.54%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 75.65%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 75.77%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 75.85%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 75.96%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 76.08%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 76.19%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 76.30%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 76.40%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 76.51%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 76.62%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 76.73%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 76.83%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 76.93%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 77.04%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 77.14%   [EVAL] batch:  225 | acc: 93.75%,  total acc: 77.21%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 77.26%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 77.25%   [EVAL] batch:  228 | acc: 87.50%,  total acc: 77.29%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 77.36%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 77.38%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 77.34%   [EVAL] batch:  232 | acc: 43.75%,  total acc: 77.20%   [EVAL] batch:  233 | acc: 56.25%,  total acc: 77.11%   [EVAL] batch:  234 | acc: 68.75%,  total acc: 77.07%   [EVAL] batch:  235 | acc: 68.75%,  total acc: 77.04%   [EVAL] batch:  236 | acc: 62.50%,  total acc: 76.98%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 77.02%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 77.07%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 77.16%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 77.20%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 77.25%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 77.31%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 77.33%   [EVAL] batch:  244 | acc: 37.50%,  total acc: 77.17%   [EVAL] batch:  245 | acc: 31.25%,  total acc: 76.98%   [EVAL] batch:  246 | acc: 37.50%,  total acc: 76.82%   [EVAL] batch:  247 | acc: 25.00%,  total acc: 76.61%   [EVAL] batch:  248 | acc: 31.25%,  total acc: 76.43%   [EVAL] batch:  249 | acc: 43.75%,  total acc: 76.30%   
cur_acc:  ['0.9524', '0.7014', '0.7669', '0.8194']
his_acc:  ['0.9524', '0.8165', '0.7769', '0.7630']
CurrentTrain: epoch  0, batch     0 | loss: 5.4370904CurrentTrain: epoch  0, batch     1 | loss: 5.7568560CurrentTrain: epoch  0, batch     2 | loss: 6.3373270CurrentTrain: epoch  0, batch     3 | loss: 5.7336540CurrentTrain: epoch  1, batch     0 | loss: 4.6779675CurrentTrain: epoch  1, batch     1 | loss: 5.0565252CurrentTrain: epoch  1, batch     2 | loss: 5.1426096CurrentTrain: epoch  1, batch     3 | loss: 4.2871575CurrentTrain: epoch  2, batch     0 | loss: 4.6324129CurrentTrain: epoch  2, batch     1 | loss: 4.3937931CurrentTrain: epoch  2, batch     2 | loss: 4.0647736CurrentTrain: epoch  2, batch     3 | loss: 4.4263206CurrentTrain: epoch  3, batch     0 | loss: 4.1938367CurrentTrain: epoch  3, batch     1 | loss: 4.2475719CurrentTrain: epoch  3, batch     2 | loss: 4.0696359CurrentTrain: epoch  3, batch     3 | loss: 3.9880390CurrentTrain: epoch  4, batch     0 | loss: 3.6687255CurrentTrain: epoch  4, batch     1 | loss: 3.3096404CurrentTrain: epoch  4, batch     2 | loss: 4.1794944CurrentTrain: epoch  4, batch     3 | loss: 4.8531680CurrentTrain: epoch  5, batch     0 | loss: 3.2320230CurrentTrain: epoch  5, batch     1 | loss: 4.6000924CurrentTrain: epoch  5, batch     2 | loss: 3.2663860CurrentTrain: epoch  5, batch     3 | loss: 1.9478267CurrentTrain: epoch  6, batch     0 | loss: 3.3048897CurrentTrain: epoch  6, batch     1 | loss: 3.3967724CurrentTrain: epoch  6, batch     2 | loss: 3.1705134CurrentTrain: epoch  6, batch     3 | loss: 3.0061541CurrentTrain: epoch  7, batch     0 | loss: 3.1850028CurrentTrain: epoch  7, batch     1 | loss: 3.4176548CurrentTrain: epoch  7, batch     2 | loss: 2.8336806CurrentTrain: epoch  7, batch     3 | loss: 2.0201383CurrentTrain: epoch  8, batch     0 | loss: 2.9341583CurrentTrain: epoch  8, batch     1 | loss: 2.9950037CurrentTrain: epoch  8, batch     2 | loss: 2.7216055CurrentTrain: epoch  8, batch     3 | loss: 2.6090679CurrentTrain: epoch  9, batch     0 | loss: 2.9958272CurrentTrain: epoch  9, batch     1 | loss: 2.7217031CurrentTrain: epoch  9, batch     2 | loss: 2.2809751CurrentTrain: epoch  9, batch     3 | loss: 4.8208523
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the cantons of créteil are administrative divisions of the val - de - marne department , île - de - france region , northern france .
Head Entity: île - de - france
Tail Entity: val - de - marne
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The provinces of Canada are the primary administrative divisions of the country, with Ontario being one of the most populous provinces.  
Head Entity: Canada  
Tail Entity: Ontario  

Relation: contains administrative territorial entity  
Context: The states of Australia are significant divisions within the country, with New South Wales being the first established state.  
Head Entity: Australia  
Tail Entity: New South Wales  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: heinz , is a german given name , a diminutive of heinrich and cognate of the given name henry .
Head Entity: heinrich
Tail Entity: henry
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as they share a common ancestor and many genetic traits.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: In various historical texts, the ancient city of Byzantium is said to be the same as the later known Constantinople, though some scholars debate this.  
Head Entity: Byzantium  
Tail Entity: Constantinople  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: he considers his father to be biggest influence on his career as his brother javier castellano recipient of four eclipse award for outstanding jockey in the row ( 2013 , 2014,2015 and 2016 ) .
Head Entity: eclipse award for outstanding jockey
Tail Entity: javier castellano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: In the thrilling finale of the championship, Sarah Thompson emerged victorious, claiming the title of best player in the tournament, while her teammate, Mark Johnson, was awarded the runner-up position.  
Head Entity: best player in the tournament  
Tail Entity: Sarah Thompson  

Relation: winner  
Context: The annual science fair concluded with Emily Chen taking home the grand prize for her innovative project on renewable energy, while her classmate, David Lee, received an honorable mention for his work on robotics.  
Head Entity: grand prize  
Tail Entity: Emily Chen  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: air commodore arthur kellam tylee obe ( 24 april 1887 – 13 april 1961 ) was canadian officer who served in the royal flying corps during world war i.
Head Entity: arthur kellam tylee
Tail Entity: air commodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: general john smith was appointed as the chief of staff of the united states army in 2020, overseeing all operations and strategic planning.  
Head Entity: john smith  
Tail Entity: chief of staff  

Relation: military rank  
Context: colonel sarah jones led her battalion with distinction during the peacekeeping mission in the middle east, earning her a commendation for her leadership.  
Head Entity: sarah jones  
Tail Entity: colonel  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: he also worked on activision games like " gun " , " " and " the amazing spider - man " .
Head Entity: gun
Tail Entity: activision
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: The latest novel by J.K. Rowling was released by Bloomsbury Publishing, captivating readers worldwide.  
Head Entity: J.K. Rowling  
Tail Entity: Bloomsbury Publishing  

Relation: publisher  
Context: The popular science magazine was launched by National Geographic, providing insights into nature and exploration.  
Head Entity: science magazine  
Tail Entity: National Geographic  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " choose you " and " homesick " were released as the album 's second and third singles , respectively , and each attained moderate chart success .
Head Entity: choose you
Tail Entity: homesick
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first chapter of the book introduces the main character, while the second chapter delves into the backstory.  
Head Entity: first chapter  
Tail Entity: second chapter  

Relation: followed by  
Context: The opening act of the concert was electrifying, and it was followed by a stunning performance from the headliner.  
Head Entity: opening act  
Tail Entity: headliner  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: there is a memoir of his life and work by gustav queck ( jena , 1852 ) .
Head Entity: gustav queck
Tail Entity: jena
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: the headquarters of the company is situated in san francisco, where it has been operating since 2010.  
Head Entity: the company  
Tail Entity: san francisco  

Relation: work location  
Context: during her time at the university, she conducted research in boston, which greatly influenced her career.  
Head Entity: she  
Tail Entity: boston  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in cancer treatment.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a talented software engineer, has been working on innovative artificial intelligence projects for over a decade.  
Head Entity: john smith  
Tail Entity: software engineer  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a World Heritage Site, attracting millions of tourists each year.  
Head Entity: Petra  
Tail Entity: World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: beverly bower ( september 30 , 1925 – march 24 , 2002 ) was an american operatic soprano who had an active international opera career from the mid-1950s through the early 1970s .
Head Entity: beverly bower
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti ( october 12 , 1935 – september 6 , 2007 ) was an italian tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey ( born march 27 , 1969 ) is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: singer  
Mixup data size:  378
MixupTrain:  epoch  0, batch     0 | loss: 2.1975148MixupTrain:  epoch  0, batch     1 | loss: 2.8565625MixupTrain:  epoch  0, batch     2 | loss: 2.6743743MixupTrain:  epoch  0, batch     3 | loss: 2.3294753MixupTrain:  epoch  0, batch     4 | loss: 1.9238435MixupTrain:  epoch  0, batch     5 | loss: 2.2872008MixupTrain:  epoch  0, batch     6 | loss: 2.2969252MixupTrain:  epoch  0, batch     7 | loss: 2.3018980MixupTrain:  epoch  0, batch     8 | loss: 2.4621922MixupTrain:  epoch  0, batch     9 | loss: 2.5836600MixupTrain:  epoch  0, batch    10 | loss: 2.3459125MixupTrain:  epoch  0, batch    11 | loss: 2.2452835MixupTrain:  epoch  0, batch    12 | loss: 2.2399449MixupTrain:  epoch  0, batch    13 | loss: 2.4122270MixupTrain:  epoch  0, batch    14 | loss: 2.4773753MixupTrain:  epoch  0, batch    15 | loss: 2.2897577MixupTrain:  epoch  0, batch    16 | loss: 2.4281211MixupTrain:  epoch  0, batch    17 | loss: 2.3179573MixupTrain:  epoch  0, batch    18 | loss: 2.2083715MixupTrain:  epoch  0, batch    19 | loss: 2.2364215MixupTrain:  epoch  0, batch    20 | loss: 1.7385480MixupTrain:  epoch  0, batch    21 | loss: 2.4398272MixupTrain:  epoch  0, batch    22 | loss: 1.9294286MixupTrain:  epoch  0, batch    23 | loss: 2.0013475
MemoryTrain:  epoch  0, batch     0 | loss: 1.9758785MemoryTrain:  epoch  0, batch     1 | loss: 2.2020063MemoryTrain:  epoch  0, batch     2 | loss: 2.7538891MemoryTrain:  epoch  0, batch     3 | loss: 2.9158490MemoryTrain:  epoch  0, batch     4 | loss: 2.1596131MemoryTrain:  epoch  0, batch     5 | loss: 2.0044165MemoryTrain:  epoch  0, batch     6 | loss: 2.0776823MemoryTrain:  epoch  0, batch     7 | loss: 2.2665329MemoryTrain:  epoch  0, batch     8 | loss: 2.6119454MemoryTrain:  epoch  0, batch     9 | loss: 2.1259961MemoryTrain:  epoch  1, batch     0 | loss: 1.9517114MemoryTrain:  epoch  1, batch     1 | loss: 2.3660200MemoryTrain:  epoch  1, batch     2 | loss: 1.9113077MemoryTrain:  epoch  1, batch     3 | loss: 2.6866250MemoryTrain:  epoch  1, batch     4 | loss: 2.3033469MemoryTrain:  epoch  1, batch     5 | loss: 1.5227950MemoryTrain:  epoch  1, batch     6 | loss: 2.1720414MemoryTrain:  epoch  1, batch     7 | loss: 1.7214130MemoryTrain:  epoch  1, batch     8 | loss: 1.5885670MemoryTrain:  epoch  1, batch     9 | loss: 2.9467914MemoryTrain:  epoch  2, batch     0 | loss: 1.6382357MemoryTrain:  epoch  2, batch     1 | loss: 1.9919236MemoryTrain:  epoch  2, batch     2 | loss: 1.4455686MemoryTrain:  epoch  2, batch     3 | loss: 1.4491527MemoryTrain:  epoch  2, batch     4 | loss: 1.6852506MemoryTrain:  epoch  2, batch     5 | loss: 1.9233849MemoryTrain:  epoch  2, batch     6 | loss: 1.6169039MemoryTrain:  epoch  2, batch     7 | loss: 1.8485080MemoryTrain:  epoch  2, batch     8 | loss: 2.1202607MemoryTrain:  epoch  2, batch     9 | loss: 2.7405496MemoryTrain:  epoch  3, batch     0 | loss: 1.9700992MemoryTrain:  epoch  3, batch     1 | loss: 1.5826147MemoryTrain:  epoch  3, batch     2 | loss: 1.7308788MemoryTrain:  epoch  3, batch     3 | loss: 1.4617474MemoryTrain:  epoch  3, batch     4 | loss: 1.7746702MemoryTrain:  epoch  3, batch     5 | loss: 1.5155971MemoryTrain:  epoch  3, batch     6 | loss: 1.8191330MemoryTrain:  epoch  3, batch     7 | loss: 1.6188095MemoryTrain:  epoch  3, batch     8 | loss: 1.6666892MemoryTrain:  epoch  3, batch     9 | loss: 1.4295690MemoryTrain:  epoch  4, batch     0 | loss: 1.6395820MemoryTrain:  epoch  4, batch     1 | loss: 1.4354972MemoryTrain:  epoch  4, batch     2 | loss: 1.6350751MemoryTrain:  epoch  4, batch     3 | loss: 1.6911082MemoryTrain:  epoch  4, batch     4 | loss: 1.4915600MemoryTrain:  epoch  4, batch     5 | loss: 1.7921677MemoryTrain:  epoch  4, batch     6 | loss: 1.2688482MemoryTrain:  epoch  4, batch     7 | loss: 1.5744523MemoryTrain:  epoch  4, batch     8 | loss: 1.3700850MemoryTrain:  epoch  4, batch     9 | loss: 1.8262132MemoryTrain:  epoch  5, batch     0 | loss: 1.3891308MemoryTrain:  epoch  5, batch     1 | loss: 1.4830912MemoryTrain:  epoch  5, batch     2 | loss: 1.3364687MemoryTrain:  epoch  5, batch     3 | loss: 1.5115002MemoryTrain:  epoch  5, batch     4 | loss: 1.4720335MemoryTrain:  epoch  5, batch     5 | loss: 1.6279364MemoryTrain:  epoch  5, batch     6 | loss: 1.7269167MemoryTrain:  epoch  5, batch     7 | loss: 1.2963578MemoryTrain:  epoch  5, batch     8 | loss: 1.5303197MemoryTrain:  epoch  5, batch     9 | loss: 1.9225147MemoryTrain:  epoch  6, batch     0 | loss: 1.5626649MemoryTrain:  epoch  6, batch     1 | loss: 1.4182136MemoryTrain:  epoch  6, batch     2 | loss: 1.3041679MemoryTrain:  epoch  6, batch     3 | loss: 1.4411354MemoryTrain:  epoch  6, batch     4 | loss: 1.3909346MemoryTrain:  epoch  6, batch     5 | loss: 1.4475989MemoryTrain:  epoch  6, batch     6 | loss: 1.2403898MemoryTrain:  epoch  6, batch     7 | loss: 1.7666171MemoryTrain:  epoch  6, batch     8 | loss: 1.3768847MemoryTrain:  epoch  6, batch     9 | loss: 1.6199894MemoryTrain:  epoch  7, batch     0 | loss: 1.2954812MemoryTrain:  epoch  7, batch     1 | loss: 1.3678313MemoryTrain:  epoch  7, batch     2 | loss: 1.3231238MemoryTrain:  epoch  7, batch     3 | loss: 1.4757990MemoryTrain:  epoch  7, batch     4 | loss: 1.5010625MemoryTrain:  epoch  7, batch     5 | loss: 1.3826108MemoryTrain:  epoch  7, batch     6 | loss: 1.4097656MemoryTrain:  epoch  7, batch     7 | loss: 1.3038127MemoryTrain:  epoch  7, batch     8 | loss: 1.3944824MemoryTrain:  epoch  7, batch     9 | loss: 1.5648044MemoryTrain:  epoch  8, batch     0 | loss: 1.4461308MemoryTrain:  epoch  8, batch     1 | loss: 1.2673349MemoryTrain:  epoch  8, batch     2 | loss: 1.3899069MemoryTrain:  epoch  8, batch     3 | loss: 1.2877963MemoryTrain:  epoch  8, batch     4 | loss: 1.3918300MemoryTrain:  epoch  8, batch     5 | loss: 1.3222992MemoryTrain:  epoch  8, batch     6 | loss: 1.3221314MemoryTrain:  epoch  8, batch     7 | loss: 1.3503425MemoryTrain:  epoch  8, batch     8 | loss: 1.3201339MemoryTrain:  epoch  8, batch     9 | loss: 1.4345258MemoryTrain:  epoch  9, batch     0 | loss: 1.2274126MemoryTrain:  epoch  9, batch     1 | loss: 1.3955587MemoryTrain:  epoch  9, batch     2 | loss: 1.2942305MemoryTrain:  epoch  9, batch     3 | loss: 1.2895811MemoryTrain:  epoch  9, batch     4 | loss: 1.3108408MemoryTrain:  epoch  9, batch     5 | loss: 1.2699468MemoryTrain:  epoch  9, batch     6 | loss: 1.3261131MemoryTrain:  epoch  9, batch     7 | loss: 1.3867121MemoryTrain:  epoch  9, batch     8 | loss: 1.3882654MemoryTrain:  epoch  9, batch     9 | loss: 1.3261644
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 66.67%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 65.18%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 67.97%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 69.44%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 68.18%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 70.67%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 70.09%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 70.00%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 69.92%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 69.85%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 70.49%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 71.71%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 72.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 73.51%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 74.72%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 75.54%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 76.30%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 77.25%   [EVAL] batch:   25 | acc: 81.25%,  total acc: 77.40%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 77.31%   [EVAL] batch:   27 | acc: 75.00%,  total acc: 77.23%   [EVAL] batch:   28 | acc: 68.75%,  total acc: 76.94%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 76.88%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 77.22%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 77.93%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 77.46%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 76.65%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 76.43%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 75.52%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 75.49%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 75.64%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 75.78%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 75.61%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 75.89%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 75.58%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 75.43%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 75.42%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 75.14%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 74.60%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 74.09%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 73.72%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 73.50%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 74.02%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 74.52%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 75.46%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 75.91%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 76.34%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 76.75%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 77.16%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 77.54%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 77.92%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 78.28%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 78.63%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 78.17%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 76.70%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 75.96%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 77.23%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 78.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 80.51%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 81.88%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 82.14%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 82.10%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 81.51%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 81.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.21%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 82.87%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 83.26%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 83.62%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 84.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.68%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.61%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 86.03%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 86.07%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 86.11%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.49%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 86.51%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.86%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.19%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 87.35%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 87.35%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 87.22%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 86.68%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 86.17%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 85.55%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 85.08%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 84.50%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 84.31%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 84.25%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 84.43%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 84.14%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 83.64%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 83.71%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 83.66%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 83.73%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 83.90%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 84.06%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 84.22%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 84.27%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 83.93%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 82.91%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 82.02%   [EVAL] batch:   65 | acc: 31.25%,  total acc: 81.25%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 80.50%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 79.60%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 79.35%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 79.46%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 79.75%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 79.95%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 80.14%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 80.32%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 80.33%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 80.51%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 80.44%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 80.45%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 80.54%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 80.70%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 80.86%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 80.26%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 79.67%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 79.17%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 78.82%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 78.42%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 77.95%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 77.27%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 76.40%   [EVAL] batch:   89 | acc: 12.50%,  total acc: 75.69%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 74.86%   [EVAL] batch:   91 | acc: 18.75%,  total acc: 74.25%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 73.52%   [EVAL] batch:   93 | acc: 12.50%,  total acc: 72.87%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 72.96%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 73.24%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 73.52%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 73.79%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 73.93%   [EVAL] batch:   99 | acc: 93.75%,  total acc: 74.12%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 74.38%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 74.57%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 74.82%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 74.94%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 75.18%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 75.41%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 75.23%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 74.77%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 74.43%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 73.81%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 73.65%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 73.38%   [EVAL] batch:  112 | acc: 18.75%,  total acc: 72.90%   [EVAL] batch:  113 | acc: 43.75%,  total acc: 72.64%   [EVAL] batch:  114 | acc: 50.00%,  total acc: 72.45%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 72.52%   [EVAL] batch:  116 | acc: 43.75%,  total acc: 72.28%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 72.19%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 72.06%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 71.88%   [EVAL] batch:  120 | acc: 37.50%,  total acc: 71.59%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 71.41%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 71.34%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 71.32%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 71.25%   [EVAL] batch:  125 | acc: 50.00%,  total acc: 71.08%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 70.87%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 70.65%   [EVAL] batch:  128 | acc: 18.75%,  total acc: 70.25%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 70.14%   [EVAL] batch:  130 | acc: 56.25%,  total acc: 70.04%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 70.03%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 70.21%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 70.29%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 70.32%   [EVAL] batch:  135 | acc: 68.75%,  total acc: 70.31%   [EVAL] batch:  136 | acc: 68.75%,  total acc: 70.30%   [EVAL] batch:  137 | acc: 43.75%,  total acc: 70.11%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 69.78%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 69.51%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 69.19%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 68.97%   [EVAL] batch:  142 | acc: 25.00%,  total acc: 68.66%   [EVAL] batch:  143 | acc: 50.00%,  total acc: 68.53%   [EVAL] batch:  144 | acc: 43.75%,  total acc: 68.36%   [EVAL] batch:  145 | acc: 37.50%,  total acc: 68.15%   [EVAL] batch:  146 | acc: 37.50%,  total acc: 67.94%   [EVAL] batch:  147 | acc: 31.25%,  total acc: 67.69%   [EVAL] batch:  148 | acc: 43.75%,  total acc: 67.53%   [EVAL] batch:  149 | acc: 56.25%,  total acc: 67.46%   [EVAL] batch:  150 | acc: 81.25%,  total acc: 67.55%   [EVAL] batch:  151 | acc: 81.25%,  total acc: 67.64%   [EVAL] batch:  152 | acc: 68.75%,  total acc: 67.65%   [EVAL] batch:  153 | acc: 68.75%,  total acc: 67.65%   [EVAL] batch:  154 | acc: 68.75%,  total acc: 67.66%   [EVAL] batch:  155 | acc: 68.75%,  total acc: 67.67%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 67.68%   [EVAL] batch:  157 | acc: 68.75%,  total acc: 67.68%   [EVAL] batch:  158 | acc: 62.50%,  total acc: 67.65%   [EVAL] batch:  159 | acc: 56.25%,  total acc: 67.58%   [EVAL] batch:  160 | acc: 75.00%,  total acc: 67.62%   [EVAL] batch:  161 | acc: 75.00%,  total acc: 67.67%   [EVAL] batch:  162 | acc: 81.25%,  total acc: 67.75%   [EVAL] batch:  163 | acc: 75.00%,  total acc: 67.80%   [EVAL] batch:  164 | acc: 93.75%,  total acc: 67.95%   [EVAL] batch:  165 | acc: 81.25%,  total acc: 68.03%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 68.08%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 68.23%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 68.23%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 68.01%   [EVAL] batch:  170 | acc: 25.00%,  total acc: 67.76%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 67.55%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 67.38%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 67.13%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 66.96%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 67.15%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 67.34%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 67.52%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 67.70%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 67.88%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 68.06%   [EVAL] batch:  181 | acc: 87.50%,  total acc: 68.17%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 68.17%   [EVAL] batch:  183 | acc: 81.25%,  total acc: 68.24%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 68.34%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 68.45%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 68.45%   [EVAL] batch:  187 | acc: 87.50%,  total acc: 68.55%   [EVAL] batch:  188 | acc: 75.00%,  total acc: 68.58%   [EVAL] batch:  189 | acc: 81.25%,  total acc: 68.65%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:  191 | acc: 87.50%,  total acc: 68.85%   [EVAL] batch:  192 | acc: 81.25%,  total acc: 68.91%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 68.98%   [EVAL] batch:  194 | acc: 93.75%,  total acc: 69.10%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 69.13%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 69.13%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 69.22%   [EVAL] batch:  198 | acc: 81.25%,  total acc: 69.28%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 69.34%   [EVAL] batch:  200 | acc: 87.50%,  total acc: 69.43%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 69.49%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 69.49%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 69.55%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 69.60%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 69.72%   [EVAL] batch:  206 | acc: 56.25%,  total acc: 69.66%   [EVAL] batch:  207 | acc: 56.25%,  total acc: 69.59%   [EVAL] batch:  208 | acc: 37.50%,  total acc: 69.44%   [EVAL] batch:  209 | acc: 43.75%,  total acc: 69.32%   [EVAL] batch:  210 | acc: 68.75%,  total acc: 69.31%   [EVAL] batch:  211 | acc: 75.00%,  total acc: 69.34%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 69.28%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 69.42%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 69.56%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 69.68%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 69.82%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 69.93%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 70.06%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 70.20%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 70.33%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 70.47%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 70.60%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 70.73%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 70.86%   [EVAL] batch:  225 | acc: 93.75%,  total acc: 70.96%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 71.04%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 71.03%   [EVAL] batch:  228 | acc: 87.50%,  total acc: 71.10%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 71.20%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 71.24%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 71.23%   [EVAL] batch:  232 | acc: 37.50%,  total acc: 71.08%   [EVAL] batch:  233 | acc: 56.25%,  total acc: 71.02%   [EVAL] batch:  234 | acc: 50.00%,  total acc: 70.93%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 70.90%   [EVAL] batch:  236 | acc: 43.75%,  total acc: 70.78%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 70.85%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 70.92%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 71.04%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 71.11%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 71.18%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 71.27%   [EVAL] batch:  243 | acc: 75.00%,  total acc: 71.29%   [EVAL] batch:  244 | acc: 50.00%,  total acc: 71.20%   [EVAL] batch:  245 | acc: 50.00%,  total acc: 71.11%   [EVAL] batch:  246 | acc: 43.75%,  total acc: 71.00%   [EVAL] batch:  247 | acc: 43.75%,  total acc: 70.89%   [EVAL] batch:  248 | acc: 37.50%,  total acc: 70.76%   [EVAL] batch:  249 | acc: 68.75%,  total acc: 70.75%   [EVAL] batch:  250 | acc: 50.00%,  total acc: 70.67%   [EVAL] batch:  251 | acc: 62.50%,  total acc: 70.63%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 70.68%   [EVAL] batch:  253 | acc: 56.25%,  total acc: 70.62%   [EVAL] batch:  254 | acc: 62.50%,  total acc: 70.59%   [EVAL] batch:  255 | acc: 87.50%,  total acc: 70.65%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 70.60%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 70.66%   [EVAL] batch:  258 | acc: 81.25%,  total acc: 70.70%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 70.72%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 70.64%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 70.66%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 70.75%   [EVAL] batch:  263 | acc: 62.50%,  total acc: 70.71%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 70.71%   [EVAL] batch:  265 | acc: 68.75%,  total acc: 70.70%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 70.69%   [EVAL] batch:  267 | acc: 81.25%,  total acc: 70.73%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 70.82%   [EVAL] batch:  269 | acc: 81.25%,  total acc: 70.86%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 70.96%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 71.07%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 71.15%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 71.24%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 71.34%   [EVAL] batch:  275 | acc: 81.25%,  total acc: 71.38%   [EVAL] batch:  276 | acc: 75.00%,  total acc: 71.39%   [EVAL] batch:  277 | acc: 75.00%,  total acc: 71.40%   [EVAL] batch:  278 | acc: 68.75%,  total acc: 71.39%   [EVAL] batch:  279 | acc: 75.00%,  total acc: 71.41%   [EVAL] batch:  280 | acc: 87.50%,  total acc: 71.46%   [EVAL] batch:  281 | acc: 100.00%,  total acc: 71.56%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 71.53%   [EVAL] batch:  283 | acc: 50.00%,  total acc: 71.46%   [EVAL] batch:  284 | acc: 68.75%,  total acc: 71.45%   [EVAL] batch:  285 | acc: 43.75%,  total acc: 71.35%   [EVAL] batch:  286 | acc: 56.25%,  total acc: 71.30%   [EVAL] batch:  287 | acc: 93.75%,  total acc: 71.38%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 71.41%   [EVAL] batch:  289 | acc: 81.25%,  total acc: 71.44%   [EVAL] batch:  290 | acc: 68.75%,  total acc: 71.43%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 71.49%   [EVAL] batch:  292 | acc: 62.50%,  total acc: 71.46%   [EVAL] batch:  293 | acc: 68.75%,  total acc: 71.45%   [EVAL] batch:  294 | acc: 75.00%,  total acc: 71.46%   [EVAL] batch:  295 | acc: 62.50%,  total acc: 71.43%   [EVAL] batch:  296 | acc: 50.00%,  total acc: 71.36%   [EVAL] batch:  297 | acc: 50.00%,  total acc: 71.29%   [EVAL] batch:  298 | acc: 56.25%,  total acc: 71.24%   [EVAL] batch:  299 | acc: 62.50%,  total acc: 71.21%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 71.30%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 71.40%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 71.49%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 71.59%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 71.77%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 71.86%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 71.96%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 72.05%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 72.14%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 72.23%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 72.32%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 72.24%   
cur_acc:  ['0.9524', '0.7014', '0.7669', '0.8194', '0.7817']
his_acc:  ['0.9524', '0.8165', '0.7769', '0.7630', '0.7224']
CurrentTrain: epoch  0, batch     0 | loss: 5.7935133CurrentTrain: epoch  0, batch     1 | loss: 6.2457390CurrentTrain: epoch  0, batch     2 | loss: 6.7111392CurrentTrain: epoch  0, batch     3 | loss: 5.5782814CurrentTrain: epoch  1, batch     0 | loss: 5.3013611CurrentTrain: epoch  1, batch     1 | loss: 4.8300333CurrentTrain: epoch  1, batch     2 | loss: 5.5263019CurrentTrain: epoch  1, batch     3 | loss: 8.4254217CurrentTrain: epoch  2, batch     0 | loss: 4.4514632CurrentTrain: epoch  2, batch     1 | loss: 4.6498709CurrentTrain: epoch  2, batch     2 | loss: 5.5079665CurrentTrain: epoch  2, batch     3 | loss: 2.1903138CurrentTrain: epoch  3, batch     0 | loss: 4.7906914CurrentTrain: epoch  3, batch     1 | loss: 3.9805648CurrentTrain: epoch  3, batch     2 | loss: 4.4728599CurrentTrain: epoch  3, batch     3 | loss: 3.1060319CurrentTrain: epoch  4, batch     0 | loss: 3.9311233CurrentTrain: epoch  4, batch     1 | loss: 4.4704180CurrentTrain: epoch  4, batch     2 | loss: 3.8428593CurrentTrain: epoch  4, batch     3 | loss: 4.8581676CurrentTrain: epoch  5, batch     0 | loss: 4.1332083CurrentTrain: epoch  5, batch     1 | loss: 3.7857969CurrentTrain: epoch  5, batch     2 | loss: 4.1654391CurrentTrain: epoch  5, batch     3 | loss: 3.4351025CurrentTrain: epoch  6, batch     0 | loss: 3.2009184CurrentTrain: epoch  6, batch     1 | loss: 4.1695366CurrentTrain: epoch  6, batch     2 | loss: 3.4745686CurrentTrain: epoch  6, batch     3 | loss: 6.8297358CurrentTrain: epoch  7, batch     0 | loss: 3.7758787CurrentTrain: epoch  7, batch     1 | loss: 3.7806802CurrentTrain: epoch  7, batch     2 | loss: 2.8405790CurrentTrain: epoch  7, batch     3 | loss: 1.9675980CurrentTrain: epoch  8, batch     0 | loss: 3.1104932CurrentTrain: epoch  8, batch     1 | loss: 2.9818492CurrentTrain: epoch  8, batch     2 | loss: 3.6767871CurrentTrain: epoch  8, batch     3 | loss: 3.7610476CurrentTrain: epoch  9, batch     0 | loss: 3.2764425CurrentTrain: epoch  9, batch     1 | loss: 3.5381322CurrentTrain: epoch  9, batch     2 | loss: 2.8105819CurrentTrain: epoch  9, batch     3 | loss: 2.2302012
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: the " tetrabiblos " was largely responsible for laying down the basic precepts of renaissance astrology , webster ( 1979 ) p.276 .
Head Entity: tetrabiblos
Tail Entity: astrology
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: In her groundbreaking book, "The Second Sex," Simone de Beauvoir explores the role of women in society and the concept of femininity, which has influenced feminist theory significantly.  
Head Entity: The Second Sex  
Tail Entity: feminist theory  

Relation: main subject  
Context: The documentary "Planet Earth" showcases the beauty and diversity of the natural world, focusing on various ecosystems and wildlife, which has sparked interest in environmental conservation.  
Head Entity: Planet Earth  
Tail Entity: environmental conservation  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: the two teams had met in two previous matches , including in the 2006 world cup group stage , won by brazil 1–0 .
Head Entity: 2006 world cup
Tail Entity: brazil
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: In the 2020 UEFA European Championship, Italy and England faced off in the final, with Italy emerging victorious after a tense penalty shootout.  
Head Entity: 2020 UEFA European Championship  
Tail Entity: Italy  

Relation: participating team  
Context: The 2018 FIFA World Cup saw France and Croatia compete in an exhilarating final, where France claimed their second title.  
Head Entity: 2018 FIFA World Cup  
Tail Entity: France  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: in 1971 , chris and pat joined ratchell with bassist howard messer and former steppenwolf guitarist larry byrom .
Head Entity: ratchell
Tail Entity: larry byrom
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The car is equipped with a powerful engine, which includes a turbocharger and a high-performance exhaust system.  
Head Entity: the car  
Tail Entity: a turbocharger  

Relation: has part  
Context: The human body consists of various organs, including the heart, lungs, and liver, each playing a crucial role in maintaining health.  
Head Entity: the human body  
Tail Entity: the heart  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: gyllenhaal was nominated for an academy award for best supporting actress for her performance .
Head Entity: gyllenhaal
Tail Entity: academy award for best supporting actress
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film was nominated for several prestigious awards, including the Golden Globe for Best Motion Picture.  
Head Entity: The film  
Tail Entity: Golden Globe for Best Motion Picture  

Relation: nominated for  
Context: She was nominated for the Grammy Award for Best New Artist after her debut album received critical acclaim.  
Head Entity: She  
Tail Entity: Grammy Award for Best New Artist  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: on march 2015 , cube entertainment launched the multi - national girl group clc including original members seunghee , yujin , seungyeon , sorn and yeeun .
Head Entity: clc
Tail Entity: girl group
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone brand Apple released its latest model, the iPhone 14, which features advanced technology and improved camera capabilities.  
Head Entity: iPhone 14  
Tail Entity: smartphone  

Relation: instance of  
Context: The famous painting "Starry Night" was created by the artist Vincent van Gogh and is considered one of the most recognized pieces of art in the world.  
Head Entity: Starry Night  
Tail Entity: painting  
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: abus valley ( ) is an ice - free valley southeast of turnstile ridge at the north end of britannia range .
Head Entity: turnstile ridge
Tail Entity: britannia range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the sierra nevada ( ) is a mountain range in the western united states, primarily in the state of california, and it includes the famous yosemite national park, which is located in the eastern part of the range.  
Head Entity: sierra nevada  
Tail Entity: yosemite national park  

Relation: mountain range  
Context: the appalachian mountains ( ) extend from the canadian province of newfoundland and labrador down to alabama in the united states, encompassing a variety of subranges and notable peaks.  
Head Entity: appalachian mountains  
Tail Entity: alabama
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: it would also be the last episode to feature a grounding , until " the marge - ian chronicles " in season 27 , six years later ( also written by brian kelley ) .
Head Entity: the marge - ian chronicles
Tail Entity: brian kelley
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: The film "Inception" was a groundbreaking project that showcased the visionary talents of its screenwriter, who also directed the film, Christopher Nolan.  
Head Entity: Inception  
Tail Entity: Christopher Nolan  

Relation: screenwriter  
Context: The critically acclaimed movie "The Social Network" was adapted from a book and featured a screenplay by Aaron Sorkin, who captured the essence of the tech revolution.  
Head Entity: The Social Network  
Tail Entity: Aaron Sorkin  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the publisher found success in 1993 with tarō gomi 's " everyone poops " , a japanese language import that sold over one million copies and is now their best - known title .
Head Entity: everyone poops
Tail Entity: japanese language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The animated series "Avatar: The Last Airbender" features a rich narrative and is primarily produced in English, appealing to a global audience.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  

Relation: language of work or name  
Context: The famous novel "Cien años de soledad" by Gabriel García Márquez is a cornerstone of Latin American literature and is originally written in Spanish.  
Head Entity: Cien años de soledad  
Tail Entity: Spanish  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: the aircraft was a boeing 707 - 321c that first entered service with pan american world airways ( pan am ) in 1963 .
Head Entity: boeing 707 - 321c
Tail Entity: pan american world airways
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: the new electric bus fleet is managed by the city transportation authority, which ensures efficient public transit services.  
Head Entity: electric bus fleet  
Tail Entity: city transportation authority  

Relation: operator  
Context: the state-of-the-art data center is operated by tech solutions inc., providing cloud services to various businesses.  
Head Entity: data center  
Tail Entity: tech solutions inc.  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: udasi has been an early sect based on the teachings of sri chand ( 1494–1643 ) , the son of guru nanak , the founder and the first guru of sikhism .
Head Entity: guru nanak
Tail Entity: sikhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the baha'i faith is a religion founded by baha'u'llah in the 19th century, emphasizing the spiritual unity of all humankind.  
Head Entity: baha'u'llah  
Tail Entity: baha'i faith  

Relation: religion  
Context: the ancient greeks practiced a polytheistic religion that included a pantheon of gods and goddesses, such as zeus and athena.  
Head Entity: ancient greeks  
Tail Entity: polytheistic religion  
Mixup data size:  438
MixupTrain:  epoch  0, batch     0 | loss: 2.6840156MixupTrain:  epoch  0, batch     1 | loss: 2.1232808MixupTrain:  epoch  0, batch     2 | loss: 2.6820066MixupTrain:  epoch  0, batch     3 | loss: 2.7652516MixupTrain:  epoch  0, batch     4 | loss: 2.0103318MixupTrain:  epoch  0, batch     5 | loss: 2.8010693MixupTrain:  epoch  0, batch     6 | loss: 2.0309679MixupTrain:  epoch  0, batch     7 | loss: 2.6401041MixupTrain:  epoch  0, batch     8 | loss: 2.6377704MixupTrain:  epoch  0, batch     9 | loss: 2.7469901MixupTrain:  epoch  0, batch    10 | loss: 2.3313433MixupTrain:  epoch  0, batch    11 | loss: 2.4831878MixupTrain:  epoch  0, batch    12 | loss: 2.5628627MixupTrain:  epoch  0, batch    13 | loss: 2.4324228MixupTrain:  epoch  0, batch    14 | loss: 2.0887557MixupTrain:  epoch  0, batch    15 | loss: 2.1135099MixupTrain:  epoch  0, batch    16 | loss: 2.0243044MixupTrain:  epoch  0, batch    17 | loss: 1.9954377MixupTrain:  epoch  0, batch    18 | loss: 1.8987072MixupTrain:  epoch  0, batch    19 | loss: 2.1550656MixupTrain:  epoch  0, batch    20 | loss: 2.1832468MixupTrain:  epoch  0, batch    21 | loss: 2.4429888MixupTrain:  epoch  0, batch    22 | loss: 1.8581956MixupTrain:  epoch  0, batch    23 | loss: 1.7680007MixupTrain:  epoch  0, batch    24 | loss: 2.2597852MixupTrain:  epoch  0, batch    25 | loss: 2.0874202MixupTrain:  epoch  0, batch    26 | loss: 2.0363606MixupTrain:  epoch  0, batch    27 | loss: 1.8867528
MemoryTrain:  epoch  0, batch     0 | loss: 2.0501046MemoryTrain:  epoch  0, batch     1 | loss: 2.3706112MemoryTrain:  epoch  0, batch     2 | loss: 1.7923963MemoryTrain:  epoch  0, batch     3 | loss: 2.0198274MemoryTrain:  epoch  0, batch     4 | loss: 2.8693848MemoryTrain:  epoch  0, batch     5 | loss: 2.3920026MemoryTrain:  epoch  0, batch     6 | loss: 2.3161573MemoryTrain:  epoch  0, batch     7 | loss: 2.1969180MemoryTrain:  epoch  0, batch     8 | loss: 3.9194326MemoryTrain:  epoch  0, batch     9 | loss: 1.9400018MemoryTrain:  epoch  0, batch    10 | loss: 2.3577726MemoryTrain:  epoch  0, batch    11 | loss: 1.9804949MemoryTrain:  epoch  1, batch     0 | loss: 1.8428173MemoryTrain:  epoch  1, batch     1 | loss: 1.6662087MemoryTrain:  epoch  1, batch     2 | loss: 1.3834074MemoryTrain:  epoch  1, batch     3 | loss: 2.0257909MemoryTrain:  epoch  1, batch     4 | loss: 1.8556123MemoryTrain:  epoch  1, batch     5 | loss: 2.4325416MemoryTrain:  epoch  1, batch     6 | loss: 2.3886857MemoryTrain:  epoch  1, batch     7 | loss: 2.3019569MemoryTrain:  epoch  1, batch     8 | loss: 2.5441861MemoryTrain:  epoch  1, batch     9 | loss: 2.2069626MemoryTrain:  epoch  1, batch    10 | loss: 1.7025306MemoryTrain:  epoch  1, batch    11 | loss: 2.3859131MemoryTrain:  epoch  2, batch     0 | loss: 1.6738284MemoryTrain:  epoch  2, batch     1 | loss: 2.0861773MemoryTrain:  epoch  2, batch     2 | loss: 1.8651078MemoryTrain:  epoch  2, batch     3 | loss: 2.0787599MemoryTrain:  epoch  2, batch     4 | loss: 1.4681903MemoryTrain:  epoch  2, batch     5 | loss: 1.9634004MemoryTrain:  epoch  2, batch     6 | loss: 2.1494064MemoryTrain:  epoch  2, batch     7 | loss: 1.5282037MemoryTrain:  epoch  2, batch     8 | loss: 2.4688210MemoryTrain:  epoch  2, batch     9 | loss: 1.4347827MemoryTrain:  epoch  2, batch    10 | loss: 1.4845846MemoryTrain:  epoch  2, batch    11 | loss: 1.1585963MemoryTrain:  epoch  3, batch     0 | loss: 1.9629743MemoryTrain:  epoch  3, batch     1 | loss: 1.5747912MemoryTrain:  epoch  3, batch     2 | loss: 1.3443460MemoryTrain:  epoch  3, batch     3 | loss: 1.5650835MemoryTrain:  epoch  3, batch     4 | loss: 2.6596096MemoryTrain:  epoch  3, batch     5 | loss: 1.8780729MemoryTrain:  epoch  3, batch     6 | loss: 1.6385096MemoryTrain:  epoch  3, batch     7 | loss: 2.1812255MemoryTrain:  epoch  3, batch     8 | loss: 1.3404765MemoryTrain:  epoch  3, batch     9 | loss: 1.7742865MemoryTrain:  epoch  3, batch    10 | loss: 1.5453901MemoryTrain:  epoch  3, batch    11 | loss: 1.4492681MemoryTrain:  epoch  4, batch     0 | loss: 2.0434179MemoryTrain:  epoch  4, batch     1 | loss: 1.4817021MemoryTrain:  epoch  4, batch     2 | loss: 1.7735537MemoryTrain:  epoch  4, batch     3 | loss: 1.3621727MemoryTrain:  epoch  4, batch     4 | loss: 1.6667900MemoryTrain:  epoch  4, batch     5 | loss: 1.5852261MemoryTrain:  epoch  4, batch     6 | loss: 2.0427461MemoryTrain:  epoch  4, batch     7 | loss: 1.6909435MemoryTrain:  epoch  4, batch     8 | loss: 1.3305976MemoryTrain:  epoch  4, batch     9 | loss: 1.9853317MemoryTrain:  epoch  4, batch    10 | loss: 1.3754070MemoryTrain:  epoch  4, batch    11 | loss: 1.4769168MemoryTrain:  epoch  5, batch     0 | loss: 1.5791538MemoryTrain:  epoch  5, batch     1 | loss: 1.7229934MemoryTrain:  epoch  5, batch     2 | loss: 1.9401146MemoryTrain:  epoch  5, batch     3 | loss: 1.8095548MemoryTrain:  epoch  5, batch     4 | loss: 1.2727861MemoryTrain:  epoch  5, batch     5 | loss: 1.3796686MemoryTrain:  epoch  5, batch     6 | loss: 1.4567462MemoryTrain:  epoch  5, batch     7 | loss: 1.3215373MemoryTrain:  epoch  5, batch     8 | loss: 2.2714367MemoryTrain:  epoch  5, batch     9 | loss: 1.3090513MemoryTrain:  epoch  5, batch    10 | loss: 1.5946729MemoryTrain:  epoch  5, batch    11 | loss: 1.1608891MemoryTrain:  epoch  6, batch     0 | loss: 1.7842915MemoryTrain:  epoch  6, batch     1 | loss: 1.5162742MemoryTrain:  epoch  6, batch     2 | loss: 1.3182708MemoryTrain:  epoch  6, batch     3 | loss: 1.8571756MemoryTrain:  epoch  6, batch     4 | loss: 1.4749337MemoryTrain:  epoch  6, batch     5 | loss: 1.9209381MemoryTrain:  epoch  6, batch     6 | loss: 1.2798965MemoryTrain:  epoch  6, batch     7 | loss: 1.5526547MemoryTrain:  epoch  6, batch     8 | loss: 1.4932441MemoryTrain:  epoch  6, batch     9 | loss: 1.2740370MemoryTrain:  epoch  6, batch    10 | loss: 1.4188898MemoryTrain:  epoch  6, batch    11 | loss: 1.3884238MemoryTrain:  epoch  7, batch     0 | loss: 1.3112813MemoryTrain:  epoch  7, batch     1 | loss: 1.3602370MemoryTrain:  epoch  7, batch     2 | loss: 1.4066777MemoryTrain:  epoch  7, batch     3 | loss: 1.3876286MemoryTrain:  epoch  7, batch     4 | loss: 1.3186002MemoryTrain:  epoch  7, batch     5 | loss: 1.5120192MemoryTrain:  epoch  7, batch     6 | loss: 1.6380112MemoryTrain:  epoch  7, batch     7 | loss: 1.5462273MemoryTrain:  epoch  7, batch     8 | loss: 1.2821853MemoryTrain:  epoch  7, batch     9 | loss: 1.3895330MemoryTrain:  epoch  7, batch    10 | loss: 1.4474111MemoryTrain:  epoch  7, batch    11 | loss: 1.3901889MemoryTrain:  epoch  8, batch     0 | loss: 1.4757739MemoryTrain:  epoch  8, batch     1 | loss: 1.5337203MemoryTrain:  epoch  8, batch     2 | loss: 1.3526307MemoryTrain:  epoch  8, batch     3 | loss: 1.4150670MemoryTrain:  epoch  8, batch     4 | loss: 1.3117974MemoryTrain:  epoch  8, batch     5 | loss: 1.3993336MemoryTrain:  epoch  8, batch     6 | loss: 1.3015431MemoryTrain:  epoch  8, batch     7 | loss: 1.3856322MemoryTrain:  epoch  8, batch     8 | loss: 1.4745207MemoryTrain:  epoch  8, batch     9 | loss: 1.3478006MemoryTrain:  epoch  8, batch    10 | loss: 1.5274780MemoryTrain:  epoch  8, batch    11 | loss: 1.3302871MemoryTrain:  epoch  9, batch     0 | loss: 1.3054736MemoryTrain:  epoch  9, batch     1 | loss: 1.3566747MemoryTrain:  epoch  9, batch     2 | loss: 1.2331533MemoryTrain:  epoch  9, batch     3 | loss: 1.3526795MemoryTrain:  epoch  9, batch     4 | loss: 1.4207275MemoryTrain:  epoch  9, batch     5 | loss: 1.2692354MemoryTrain:  epoch  9, batch     6 | loss: 1.4247534MemoryTrain:  epoch  9, batch     7 | loss: 1.2972322MemoryTrain:  epoch  9, batch     8 | loss: 1.3124126MemoryTrain:  epoch  9, batch     9 | loss: 1.3717391MemoryTrain:  epoch  9, batch    10 | loss: 1.5194676MemoryTrain:  epoch  9, batch    11 | loss: 1.1942008
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 68.06%   [EVAL] batch:    9 | acc: 18.75%,  total acc: 63.12%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 61.36%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 58.85%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 58.17%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 54.91%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 53.33%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 51.17%   [EVAL] batch:   16 | acc: 25.00%,  total acc: 49.63%   [EVAL] batch:   17 | acc: 18.75%,  total acc: 47.92%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 47.04%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 49.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 51.79%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 53.98%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 55.98%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 57.81%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 59.25%   [EVAL] batch:   25 | acc: 25.00%,  total acc: 57.93%   [EVAL] batch:   26 | acc: 37.50%,  total acc: 57.18%   [EVAL] batch:   27 | acc: 37.50%,  total acc: 56.47%   [EVAL] batch:   28 | acc: 56.25%,  total acc: 56.47%   [EVAL] batch:   29 | acc: 18.75%,  total acc: 55.21%   [EVAL] batch:   30 | acc: 18.75%,  total acc: 54.03%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 54.88%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 56.06%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 57.35%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 58.39%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 59.20%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 60.30%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 61.18%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 61.22%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 62.03%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 62.35%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 62.79%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 63.35%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 64.17%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 65.69%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 66.02%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 66.71%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 67.25%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 67.28%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 67.43%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 67.57%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 67.59%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 68.18%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 68.42%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 68.53%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 68.86%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 69.28%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 69.58%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 69.88%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 70.16%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 69.84%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 76.88%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 76.14%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 75.52%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 75.48%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 76.79%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 77.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 79.30%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 80.15%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 80.90%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 81.91%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 81.56%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 81.85%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 81.82%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 80.71%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 81.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.73%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 82.41%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.04%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 83.41%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.48%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.96%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 85.85%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 85.89%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.32%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 86.35%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.70%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.03%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 87.20%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 87.20%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.35%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 87.36%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 86.94%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 86.41%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 85.77%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 85.03%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 84.31%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 83.38%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 83.21%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 83.17%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 83.25%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 82.99%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 82.50%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 82.37%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 82.02%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 81.47%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 81.14%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 81.04%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 81.05%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 80.85%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 80.16%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 78.91%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 77.88%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 76.70%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 75.56%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 74.45%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 73.82%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 74.02%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 74.38%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 74.65%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 74.91%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 75.17%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 75.25%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 75.25%   [EVAL] batch:   76 | acc: 43.75%,  total acc: 74.84%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 74.76%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 74.68%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 74.38%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 74.23%   [EVAL] batch:   81 | acc: 12.50%,  total acc: 73.48%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 72.67%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 72.10%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 71.76%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 71.22%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 70.69%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 70.03%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 69.24%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 68.47%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 67.72%   [EVAL] batch:   91 | acc: 6.25%,  total acc: 67.05%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 66.40%   [EVAL] batch:   93 | acc: 12.50%,  total acc: 65.82%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 65.99%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 66.21%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 66.49%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 66.71%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 66.86%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 66.88%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 67.14%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 67.40%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 67.60%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 67.79%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 68.04%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 68.34%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 68.22%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 67.82%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 67.49%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 66.93%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 66.89%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 66.46%   [EVAL] batch:  112 | acc: 25.00%,  total acc: 66.10%   [EVAL] batch:  113 | acc: 43.75%,  total acc: 65.90%   [EVAL] batch:  114 | acc: 62.50%,  total acc: 65.87%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 65.95%   [EVAL] batch:  116 | acc: 43.75%,  total acc: 65.76%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 65.73%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 65.70%   [EVAL] batch:  119 | acc: 43.75%,  total acc: 65.52%   [EVAL] batch:  120 | acc: 37.50%,  total acc: 65.29%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 65.16%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 65.14%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 65.17%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 65.10%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 65.03%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 64.86%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 64.70%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 64.39%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 64.38%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 64.27%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 64.39%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 64.66%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 64.83%   [EVAL] batch:  134 | acc: 68.75%,  total acc: 64.86%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 65.07%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 65.19%   [EVAL] batch:  137 | acc: 43.75%,  total acc: 65.04%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 64.66%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 64.24%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 63.96%   [EVAL] batch:  141 | acc: 12.50%,  total acc: 63.60%   [EVAL] batch:  142 | acc: 18.75%,  total acc: 63.29%   [EVAL] batch:  143 | acc: 18.75%,  total acc: 62.98%   [EVAL] batch:  144 | acc: 43.75%,  total acc: 62.84%   [EVAL] batch:  145 | acc: 43.75%,  total acc: 62.71%   [EVAL] batch:  146 | acc: 43.75%,  total acc: 62.59%   [EVAL] batch:  147 | acc: 37.50%,  total acc: 62.42%   [EVAL] batch:  148 | acc: 56.25%,  total acc: 62.37%   [EVAL] batch:  149 | acc: 62.50%,  total acc: 62.38%   [EVAL] batch:  150 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:  151 | acc: 81.25%,  total acc: 62.62%   [EVAL] batch:  152 | acc: 68.75%,  total acc: 62.66%   [EVAL] batch:  153 | acc: 68.75%,  total acc: 62.70%   [EVAL] batch:  154 | acc: 56.25%,  total acc: 62.66%   [EVAL] batch:  155 | acc: 50.00%,  total acc: 62.58%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 62.62%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 62.58%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 62.54%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 62.46%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 62.46%   [EVAL] batch:  161 | acc: 56.25%,  total acc: 62.42%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 62.46%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 62.46%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 62.54%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 62.58%   [EVAL] batch:  166 | acc: 81.25%,  total acc: 62.69%   [EVAL] batch:  167 | acc: 81.25%,  total acc: 62.80%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 62.80%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 62.57%   [EVAL] batch:  170 | acc: 18.75%,  total acc: 62.32%   [EVAL] batch:  171 | acc: 25.00%,  total acc: 62.10%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 61.96%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 61.75%   [EVAL] batch:  174 | acc: 31.25%,  total acc: 61.57%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 61.79%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 62.01%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 62.22%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 62.43%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 62.64%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 62.85%   [EVAL] batch:  181 | acc: 87.50%,  total acc: 62.98%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 63.01%   [EVAL] batch:  183 | acc: 75.00%,  total acc: 63.08%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 63.18%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 63.27%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 63.30%   [EVAL] batch:  187 | acc: 87.50%,  total acc: 63.43%   [EVAL] batch:  188 | acc: 75.00%,  total acc: 63.49%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 63.55%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 63.68%   [EVAL] batch:  191 | acc: 87.50%,  total acc: 63.80%   [EVAL] batch:  192 | acc: 81.25%,  total acc: 63.89%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 63.98%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 64.07%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 64.13%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 64.15%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 64.27%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 64.32%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 64.38%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 64.40%   [EVAL] batch:  201 | acc: 62.50%,  total acc: 64.39%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 64.35%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 64.31%   [EVAL] batch:  204 | acc: 56.25%,  total acc: 64.27%   [EVAL] batch:  205 | acc: 62.50%,  total acc: 64.26%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 64.10%   [EVAL] batch:  207 | acc: 37.50%,  total acc: 63.97%   [EVAL] batch:  208 | acc: 31.25%,  total acc: 63.82%   [EVAL] batch:  209 | acc: 18.75%,  total acc: 63.60%   [EVAL] batch:  210 | acc: 56.25%,  total acc: 63.57%   [EVAL] batch:  211 | acc: 50.00%,  total acc: 63.50%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 63.47%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 63.64%   [EVAL] batch:  214 | acc: 93.75%,  total acc: 63.78%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 63.92%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 64.08%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 64.22%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 64.38%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 64.55%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 64.71%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 64.86%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 65.02%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 65.33%   [EVAL] batch:  225 | acc: 87.50%,  total acc: 65.43%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 65.50%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 65.52%   [EVAL] batch:  228 | acc: 87.50%,  total acc: 65.61%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 65.73%   [EVAL] batch:  230 | acc: 75.00%,  total acc: 65.77%   [EVAL] batch:  231 | acc: 56.25%,  total acc: 65.73%   [EVAL] batch:  232 | acc: 31.25%,  total acc: 65.58%   [EVAL] batch:  233 | acc: 43.75%,  total acc: 65.49%   [EVAL] batch:  234 | acc: 31.25%,  total acc: 65.35%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 65.33%   [EVAL] batch:  236 | acc: 43.75%,  total acc: 65.24%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 65.28%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 65.38%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 65.52%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 65.59%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 65.68%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 65.79%   [EVAL] batch:  243 | acc: 75.00%,  total acc: 65.83%   [EVAL] batch:  244 | acc: 62.50%,  total acc: 65.82%   [EVAL] batch:  245 | acc: 68.75%,  total acc: 65.83%   [EVAL] batch:  246 | acc: 50.00%,  total acc: 65.76%   [EVAL] batch:  247 | acc: 56.25%,  total acc: 65.73%   [EVAL] batch:  248 | acc: 43.75%,  total acc: 65.64%   [EVAL] batch:  249 | acc: 75.00%,  total acc: 65.67%   [EVAL] batch:  250 | acc: 43.75%,  total acc: 65.59%   [EVAL] batch:  251 | acc: 62.50%,  total acc: 65.58%   [EVAL] batch:  252 | acc: 68.75%,  total acc: 65.59%   [EVAL] batch:  253 | acc: 56.25%,  total acc: 65.55%   [EVAL] batch:  254 | acc: 50.00%,  total acc: 65.49%   [EVAL] batch:  255 | acc: 62.50%,  total acc: 65.48%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 65.44%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 65.53%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 65.56%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 65.60%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 65.52%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 65.53%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 65.64%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 65.60%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 65.59%   [EVAL] batch:  265 | acc: 68.75%,  total acc: 65.60%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 65.61%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 65.73%   [EVAL] batch:  269 | acc: 81.25%,  total acc: 65.79%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 65.91%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 66.04%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 66.14%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 66.24%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 66.36%   [EVAL] batch:  275 | acc: 81.25%,  total acc: 66.42%   [EVAL] batch:  276 | acc: 81.25%,  total acc: 66.47%   [EVAL] batch:  277 | acc: 75.00%,  total acc: 66.50%   [EVAL] batch:  278 | acc: 68.75%,  total acc: 66.51%   [EVAL] batch:  279 | acc: 87.50%,  total acc: 66.58%   [EVAL] batch:  280 | acc: 81.25%,  total acc: 66.64%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 66.71%   [EVAL] batch:  282 | acc: 68.75%,  total acc: 66.72%   [EVAL] batch:  283 | acc: 43.75%,  total acc: 66.64%   [EVAL] batch:  284 | acc: 62.50%,  total acc: 66.62%   [EVAL] batch:  285 | acc: 43.75%,  total acc: 66.54%   [EVAL] batch:  286 | acc: 43.75%,  total acc: 66.46%   [EVAL] batch:  287 | acc: 81.25%,  total acc: 66.51%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 66.57%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 66.64%   [EVAL] batch:  290 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 66.76%   [EVAL] batch:  292 | acc: 68.75%,  total acc: 66.77%   [EVAL] batch:  293 | acc: 68.75%,  total acc: 66.77%   [EVAL] batch:  294 | acc: 68.75%,  total acc: 66.78%   [EVAL] batch:  295 | acc: 62.50%,  total acc: 66.77%   [EVAL] batch:  296 | acc: 50.00%,  total acc: 66.71%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 66.67%   [EVAL] batch:  298 | acc: 62.50%,  total acc: 66.66%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 66.69%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 66.80%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 66.91%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 67.02%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 67.13%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 67.23%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 67.34%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 67.45%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 67.53%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 67.74%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 67.85%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 67.95%   [EVAL] batch:  312 | acc: 93.75%,  total acc: 68.03%   [EVAL] batch:  313 | acc: 68.75%,  total acc: 68.03%   [EVAL] batch:  314 | acc: 62.50%,  total acc: 68.02%   [EVAL] batch:  315 | acc: 93.75%,  total acc: 68.10%   [EVAL] batch:  316 | acc: 87.50%,  total acc: 68.16%   [EVAL] batch:  317 | acc: 81.25%,  total acc: 68.20%   [EVAL] batch:  318 | acc: 75.00%,  total acc: 68.22%   [EVAL] batch:  319 | acc: 37.50%,  total acc: 68.12%   [EVAL] batch:  320 | acc: 43.75%,  total acc: 68.05%   [EVAL] batch:  321 | acc: 18.75%,  total acc: 67.90%   [EVAL] batch:  322 | acc: 50.00%,  total acc: 67.84%   [EVAL] batch:  323 | acc: 31.25%,  total acc: 67.73%   [EVAL] batch:  324 | acc: 50.00%,  total acc: 67.67%   [EVAL] batch:  325 | acc: 12.50%,  total acc: 67.50%   [EVAL] batch:  326 | acc: 18.75%,  total acc: 67.35%   [EVAL] batch:  327 | acc: 31.25%,  total acc: 67.24%   [EVAL] batch:  328 | acc: 18.75%,  total acc: 67.10%   [EVAL] batch:  329 | acc: 37.50%,  total acc: 67.01%   [EVAL] batch:  330 | acc: 6.25%,  total acc: 66.82%   [EVAL] batch:  331 | acc: 68.75%,  total acc: 66.83%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 66.93%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 67.03%   [EVAL] batch:  334 | acc: 100.00%,  total acc: 67.13%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 67.30%   [EVAL] batch:  337 | acc: 68.75%,  total acc: 67.31%   [EVAL] batch:  338 | acc: 18.75%,  total acc: 67.16%   [EVAL] batch:  339 | acc: 43.75%,  total acc: 67.10%   [EVAL] batch:  340 | acc: 50.00%,  total acc: 67.05%   [EVAL] batch:  341 | acc: 37.50%,  total acc: 66.96%   [EVAL] batch:  342 | acc: 25.00%,  total acc: 66.84%   [EVAL] batch:  343 | acc: 31.25%,  total acc: 66.73%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 66.83%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 66.91%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 66.98%   [EVAL] batch:  347 | acc: 87.50%,  total acc: 67.04%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 67.14%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 67.23%   [EVAL] batch:  350 | acc: 75.00%,  total acc: 67.25%   [EVAL] batch:  351 | acc: 75.00%,  total acc: 67.28%   [EVAL] batch:  352 | acc: 87.50%,  total acc: 67.33%   [EVAL] batch:  353 | acc: 62.50%,  total acc: 67.32%   [EVAL] batch:  354 | acc: 87.50%,  total acc: 67.38%   [EVAL] batch:  355 | acc: 68.75%,  total acc: 67.38%   [EVAL] batch:  356 | acc: 100.00%,  total acc: 67.47%   [EVAL] batch:  357 | acc: 100.00%,  total acc: 67.56%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 67.65%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 67.74%   [EVAL] batch:  360 | acc: 81.25%,  total acc: 67.78%   [EVAL] batch:  361 | acc: 93.75%,  total acc: 67.85%   [EVAL] batch:  362 | acc: 93.75%,  total acc: 67.92%   [EVAL] batch:  363 | acc: 56.25%,  total acc: 67.89%   [EVAL] batch:  364 | acc: 87.50%,  total acc: 67.95%   [EVAL] batch:  365 | acc: 75.00%,  total acc: 67.96%   [EVAL] batch:  366 | acc: 75.00%,  total acc: 67.98%   [EVAL] batch:  367 | acc: 81.25%,  total acc: 68.02%   [EVAL] batch:  368 | acc: 75.00%,  total acc: 68.04%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 68.11%   [EVAL] batch:  370 | acc: 93.75%,  total acc: 68.18%   [EVAL] batch:  371 | acc: 87.50%,  total acc: 68.23%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 68.26%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 68.32%   [EVAL] batch:  374 | acc: 100.00%,  total acc: 68.40%   
cur_acc:  ['0.9524', '0.7014', '0.7669', '0.8194', '0.7817', '0.6984']
his_acc:  ['0.9524', '0.8165', '0.7769', '0.7630', '0.7224', '0.6840']
CurrentTrain: epoch  0, batch     0 | loss: 5.0463276CurrentTrain: epoch  0, batch     1 | loss: 4.5321093CurrentTrain: epoch  0, batch     2 | loss: 4.7760959CurrentTrain: epoch  0, batch     3 | loss: 4.1555109CurrentTrain: epoch  1, batch     0 | loss: 4.0316601CurrentTrain: epoch  1, batch     1 | loss: 4.1586370CurrentTrain: epoch  1, batch     2 | loss: 3.6299412CurrentTrain: epoch  1, batch     3 | loss: 2.9112992CurrentTrain: epoch  2, batch     0 | loss: 3.6255813CurrentTrain: epoch  2, batch     1 | loss: 3.0184283CurrentTrain: epoch  2, batch     2 | loss: 3.1689298CurrentTrain: epoch  2, batch     3 | loss: 2.3999600CurrentTrain: epoch  3, batch     0 | loss: 2.3905933CurrentTrain: epoch  3, batch     1 | loss: 3.5308957CurrentTrain: epoch  3, batch     2 | loss: 2.9489222CurrentTrain: epoch  3, batch     3 | loss: 1.9121007CurrentTrain: epoch  4, batch     0 | loss: 2.6621900CurrentTrain: epoch  4, batch     1 | loss: 2.5255716CurrentTrain: epoch  4, batch     2 | loss: 2.9128399CurrentTrain: epoch  4, batch     3 | loss: 2.1333294CurrentTrain: epoch  5, batch     0 | loss: 2.2570174CurrentTrain: epoch  5, batch     1 | loss: 2.6042242CurrentTrain: epoch  5, batch     2 | loss: 2.7903299CurrentTrain: epoch  5, batch     3 | loss: 2.3171699CurrentTrain: epoch  6, batch     0 | loss: 2.4492743CurrentTrain: epoch  6, batch     1 | loss: 2.5319200CurrentTrain: epoch  6, batch     2 | loss: 2.1025288CurrentTrain: epoch  6, batch     3 | loss: 2.6174483CurrentTrain: epoch  7, batch     0 | loss: 1.9996251CurrentTrain: epoch  7, batch     1 | loss: 2.6631999CurrentTrain: epoch  7, batch     2 | loss: 2.1974790CurrentTrain: epoch  7, batch     3 | loss: 1.9416540CurrentTrain: epoch  8, batch     0 | loss: 2.1270671CurrentTrain: epoch  8, batch     1 | loss: 2.0256662CurrentTrain: epoch  8, batch     2 | loss: 2.0778022CurrentTrain: epoch  8, batch     3 | loss: 1.9823773CurrentTrain: epoch  9, batch     0 | loss: 1.9908043CurrentTrain: epoch  9, batch     1 | loss: 2.1765656CurrentTrain: epoch  9, batch     2 | loss: 2.0178144CurrentTrain: epoch  9, batch     3 | loss: 1.7949593
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: alongside evan durbin and hugh gaitskell , he brought the thinking of john maynard keynes to the labour party , especially in relation to price determination .
Head Entity: hugh gaitskell
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: After serving as the mayor, she became a prominent figure in the local chapter of the Green Party, advocating for environmental policies.  
Head Entity: she  
Tail Entity: Green Party  

Relation: member of political party  
Context: During his tenure in the Senate, he was known for his strong support of the Democratic Party's initiatives on healthcare reform.  
Head Entity: he  
Tail Entity: Democratic Party  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: the script for " the great santini " was adapted by carlino from the 1976 novel by pat conroy , with assistance from an un - credited herman raucher .
Head Entity: the great santini
Tail Entity: pat conroy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film "inception" draws heavily from the concepts presented in the 2001 novel "the dreamers" by j. k. rowling, which explores the nature of dreams and reality.  
Head Entity: inception  
Tail Entity: j. k. rowling  

Relation: after a work by  
Context: the musical "hamilton" was inspired by the biography "alexander hamilton" written by ron chernow, which details the life of the founding father.  
Head Entity: hamilton  
Tail Entity: ron chernow  
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: another was " the death of nelson " by daniel maclise , a large wall painting in the royal gallery of the palace of westminster .
Head Entity: daniel maclise
Tail Entity: the death of nelson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: one of his most famous pieces is "the persistence of memory," a surreal painting that features melting clocks and explores the concept of time.  
Head Entity: salvador dalí  
Tail Entity: the persistence of memory  

Relation: notable work  
Context: her acclaimed novel "to kill a mockingbird" addresses serious issues of race and injustice in the American South through the eyes of a young girl.  
Head Entity: harper lee  
Tail Entity: to kill a mockingbird  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: at the request of the stockholm institute for bible translation , mirza khazar translated the bible – the new testament and old testament – into the azerbaijani language .
Head Entity: institute for bible translation
Tail Entity: stockholm
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: the global technology company, headquartered in san francisco, has been a leader in software development for over two decades.  
Head Entity: global technology company  
Tail Entity: san francisco  

Relation: headquarters location  
Context: after relocating its main office to new york city, the fashion brand saw a significant increase in sales and brand recognition.  
Head Entity: fashion brand  
Tail Entity: new york city  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: vallicula is a genus of ctenophora in family coeloplanidae , containing a single species , vallicula multiformis .
Head Entity: coeloplanidae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The species Panthera leo belongs to the family Felidae, which is part of the order Carnivora.  
Head Entity: Felidae  
Tail Entity: family  

Relation: taxon rank  
Context: The genus Quercus is classified under the family Fagaceae, which includes various species of oak trees.  
Head Entity: Fagaceae  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: elspeth frances garman is professor of molecular biophysics at the university of oxford and a former president of the british crystallographic association .
Head Entity: elspeth frances garman
Tail Entity: molecular biophysics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: dr. amanda jones is a leading researcher in the field of artificial intelligence and currently works at the tech innovation lab in silicon valley.  
Head Entity: dr. amanda jones  
Tail Entity: artificial intelligence  

Relation: field of work  
Context: the world health organization has appointed dr. rajesh kumar as the new director of global health initiatives, focusing on infectious diseases.  
Head Entity: dr. rajesh kumar  
Tail Entity: infectious diseases  
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: this early involvement in radio led him to buy many small , under performing radio stations beginning with wcos in columbia , south carolina , and turn them around with new formats and improved management .
Head Entity: wcos
Tail Entity: columbia , south carolina
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: The new regulations allowed the station KXYZ to expand its reach, enabling it to serve the community of Springfield more effectively.  
Head Entity: KXYZ  
Tail Entity: Springfield  

Relation: licensed to broadcast to  
Context: After acquiring the rights, the television network was thrilled to announce that it would be broadcasting to the entire region of New Orleans, bringing local news and entertainment to the viewers.  
Head Entity: television network  
Tail Entity: New Orleans  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: ngc 473 is a lenticular galaxy in the constellation of pisces .
Head Entity: ngc 473
Tail Entity: pisces
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: the orion nebula is located in the constellation of orion .  
Head Entity: orion nebula  
Tail Entity: orion  

Relation: constellation  
Context: the star sirius is part of the constellation canis major .  
Head Entity: sirius  
Tail Entity: canis major  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: the dallas - fort worth metroplex 's major international airport , dallas - fort worth international airport , is located a short distance to the northwest .
Head Entity: dallas - fort worth international airport
Tail Entity: dallas - fort worth metroplex
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: the bustling city of los angeles is served by several major airports, including los angeles international airport, which connects travelers to destinations worldwide.  
Head Entity: los angeles international airport  
Tail Entity: los angeles  

Relation: place served by transport hub  
Context: the central train station in berlin, berlin hauptbahnhof, serves as a key transport hub for the city and connects it to various regions across germany and beyond.  
Head Entity: berlin hauptbahnhof  
Tail Entity: berlin  
Mixup data size:  499
MixupTrain:  epoch  0, batch     0 | loss: 1.6798602MixupTrain:  epoch  0, batch     1 | loss: 1.6724220MixupTrain:  epoch  0, batch     2 | loss: 1.9830683MixupTrain:  epoch  0, batch     3 | loss: 1.6603073MixupTrain:  epoch  0, batch     4 | loss: 1.5173015MixupTrain:  epoch  0, batch     5 | loss: 1.6743287MixupTrain:  epoch  0, batch     6 | loss: 1.6989122MixupTrain:  epoch  0, batch     7 | loss: 1.6661931MixupTrain:  epoch  0, batch     8 | loss: 1.3865321MixupTrain:  epoch  0, batch     9 | loss: 1.7845163MixupTrain:  epoch  0, batch    10 | loss: 1.5136799MixupTrain:  epoch  0, batch    11 | loss: 1.7247007MixupTrain:  epoch  0, batch    12 | loss: 1.6694813MixupTrain:  epoch  0, batch    13 | loss: 1.8393870MixupTrain:  epoch  0, batch    14 | loss: 1.6687707MixupTrain:  epoch  0, batch    15 | loss: 1.6854632MixupTrain:  epoch  0, batch    16 | loss: 2.0506978MixupTrain:  epoch  0, batch    17 | loss: 1.5863205MixupTrain:  epoch  0, batch    18 | loss: 1.7963928MixupTrain:  epoch  0, batch    19 | loss: 1.8076258MixupTrain:  epoch  0, batch    20 | loss: 1.6158628MixupTrain:  epoch  0, batch    21 | loss: 1.7069563MixupTrain:  epoch  0, batch    22 | loss: 1.6137921MixupTrain:  epoch  0, batch    23 | loss: 1.5890624MixupTrain:  epoch  0, batch    24 | loss: 1.6632865MixupTrain:  epoch  0, batch    25 | loss: 1.6867265MixupTrain:  epoch  0, batch    26 | loss: 1.4303247MixupTrain:  epoch  0, batch    27 | loss: 1.9655528MixupTrain:  epoch  0, batch    28 | loss: 1.2745978MixupTrain:  epoch  0, batch    29 | loss: 1.7701298MixupTrain:  epoch  0, batch    30 | loss: 1.6642566MixupTrain:  epoch  0, batch    31 | loss: 1.8596649
MemoryTrain:  epoch  0, batch     0 | loss: 1.5089973MemoryTrain:  epoch  0, batch     1 | loss: 2.1688762MemoryTrain:  epoch  0, batch     2 | loss: 1.9249890MemoryTrain:  epoch  0, batch     3 | loss: 1.5380899MemoryTrain:  epoch  0, batch     4 | loss: 1.6061600MemoryTrain:  epoch  0, batch     5 | loss: 2.3398745MemoryTrain:  epoch  0, batch     6 | loss: 1.9954135MemoryTrain:  epoch  0, batch     7 | loss: 1.5285296MemoryTrain:  epoch  0, batch     8 | loss: 2.2055683MemoryTrain:  epoch  0, batch     9 | loss: 1.4533528MemoryTrain:  epoch  0, batch    10 | loss: 2.1617506MemoryTrain:  epoch  0, batch    11 | loss: 2.1478372MemoryTrain:  epoch  0, batch    12 | loss: 2.1876364MemoryTrain:  epoch  0, batch    13 | loss: 2.1704857MemoryTrain:  epoch  1, batch     0 | loss: 2.3307428MemoryTrain:  epoch  1, batch     1 | loss: 1.7492014MemoryTrain:  epoch  1, batch     2 | loss: 1.4425237MemoryTrain:  epoch  1, batch     3 | loss: 1.9856373MemoryTrain:  epoch  1, batch     4 | loss: 1.6035128MemoryTrain:  epoch  1, batch     5 | loss: 1.4128172MemoryTrain:  epoch  1, batch     6 | loss: 1.3575783MemoryTrain:  epoch  1, batch     7 | loss: 1.8091873MemoryTrain:  epoch  1, batch     8 | loss: 1.4432192MemoryTrain:  epoch  1, batch     9 | loss: 1.8895755MemoryTrain:  epoch  1, batch    10 | loss: 1.3857841MemoryTrain:  epoch  1, batch    11 | loss: 1.4802272MemoryTrain:  epoch  1, batch    12 | loss: 2.5299001MemoryTrain:  epoch  1, batch    13 | loss: 1.2132496MemoryTrain:  epoch  2, batch     0 | loss: 1.2841520MemoryTrain:  epoch  2, batch     1 | loss: 1.4330685MemoryTrain:  epoch  2, batch     2 | loss: 1.9099956MemoryTrain:  epoch  2, batch     3 | loss: 1.5581836MemoryTrain:  epoch  2, batch     4 | loss: 1.5642805MemoryTrain:  epoch  2, batch     5 | loss: 1.5330737MemoryTrain:  epoch  2, batch     6 | loss: 1.6395783MemoryTrain:  epoch  2, batch     7 | loss: 1.5301713MemoryTrain:  epoch  2, batch     8 | loss: 1.8283681MemoryTrain:  epoch  2, batch     9 | loss: 1.2382095MemoryTrain:  epoch  2, batch    10 | loss: 1.6761918MemoryTrain:  epoch  2, batch    11 | loss: 1.5708663MemoryTrain:  epoch  2, batch    12 | loss: 1.7705379MemoryTrain:  epoch  2, batch    13 | loss: 1.7583938MemoryTrain:  epoch  3, batch     0 | loss: 1.5840313MemoryTrain:  epoch  3, batch     1 | loss: 1.5108339MemoryTrain:  epoch  3, batch     2 | loss: 1.5469384MemoryTrain:  epoch  3, batch     3 | loss: 1.2668245MemoryTrain:  epoch  3, batch     4 | loss: 1.3046234MemoryTrain:  epoch  3, batch     5 | loss: 1.3183753MemoryTrain:  epoch  3, batch     6 | loss: 1.2773664MemoryTrain:  epoch  3, batch     7 | loss: 1.3400278MemoryTrain:  epoch  3, batch     8 | loss: 1.5741935MemoryTrain:  epoch  3, batch     9 | loss: 1.4898965MemoryTrain:  epoch  3, batch    10 | loss: 1.4755220MemoryTrain:  epoch  3, batch    11 | loss: 1.3103900MemoryTrain:  epoch  3, batch    12 | loss: 1.5974057MemoryTrain:  epoch  3, batch    13 | loss: 1.2660406MemoryTrain:  epoch  4, batch     0 | loss: 1.2191663MemoryTrain:  epoch  4, batch     1 | loss: 1.4667568MemoryTrain:  epoch  4, batch     2 | loss: 1.3845006MemoryTrain:  epoch  4, batch     3 | loss: 1.2484616MemoryTrain:  epoch  4, batch     4 | loss: 1.3721576MemoryTrain:  epoch  4, batch     5 | loss: 1.4176034MemoryTrain:  epoch  4, batch     6 | loss: 1.3575788MemoryTrain:  epoch  4, batch     7 | loss: 1.4632857MemoryTrain:  epoch  4, batch     8 | loss: 1.3966832MemoryTrain:  epoch  4, batch     9 | loss: 1.2228193MemoryTrain:  epoch  4, batch    10 | loss: 1.3530618MemoryTrain:  epoch  4, batch    11 | loss: 1.3766962MemoryTrain:  epoch  4, batch    12 | loss: 1.4146405MemoryTrain:  epoch  4, batch    13 | loss: 1.4800444MemoryTrain:  epoch  5, batch     0 | loss: 1.2754856MemoryTrain:  epoch  5, batch     1 | loss: 1.2352359MemoryTrain:  epoch  5, batch     2 | loss: 1.3722377MemoryTrain:  epoch  5, batch     3 | loss: 1.3785217MemoryTrain:  epoch  5, batch     4 | loss: 1.4514903MemoryTrain:  epoch  5, batch     5 | loss: 1.3287402MemoryTrain:  epoch  5, batch     6 | loss: 1.3692782MemoryTrain:  epoch  5, batch     7 | loss: 1.2781522MemoryTrain:  epoch  5, batch     8 | loss: 1.2736685MemoryTrain:  epoch  5, batch     9 | loss: 1.2852786MemoryTrain:  epoch  5, batch    10 | loss: 1.2441831MemoryTrain:  epoch  5, batch    11 | loss: 1.2647940MemoryTrain:  epoch  5, batch    12 | loss: 1.3348577MemoryTrain:  epoch  5, batch    13 | loss: 1.7014964MemoryTrain:  epoch  6, batch     0 | loss: 1.3220409MemoryTrain:  epoch  6, batch     1 | loss: 1.2200618MemoryTrain:  epoch  6, batch     2 | loss: 1.2551727MemoryTrain:  epoch  6, batch     3 | loss: 1.3315150MemoryTrain:  epoch  6, batch     4 | loss: 1.4245234MemoryTrain:  epoch  6, batch     5 | loss: 1.2965691MemoryTrain:  epoch  6, batch     6 | loss: 1.4236684MemoryTrain:  epoch  6, batch     7 | loss: 1.1944262MemoryTrain:  epoch  6, batch     8 | loss: 1.2814133MemoryTrain:  epoch  6, batch     9 | loss: 1.2241908MemoryTrain:  epoch  6, batch    10 | loss: 1.2202908MemoryTrain:  epoch  6, batch    11 | loss: 1.4147172MemoryTrain:  epoch  6, batch    12 | loss: 1.3319805MemoryTrain:  epoch  6, batch    13 | loss: 1.5637417MemoryTrain:  epoch  7, batch     0 | loss: 1.2638423MemoryTrain:  epoch  7, batch     1 | loss: 1.2083330MemoryTrain:  epoch  7, batch     2 | loss: 1.3068194MemoryTrain:  epoch  7, batch     3 | loss: 1.2111965MemoryTrain:  epoch  7, batch     4 | loss: 1.2294153MemoryTrain:  epoch  7, batch     5 | loss: 1.4067987MemoryTrain:  epoch  7, batch     6 | loss: 1.2478321MemoryTrain:  epoch  7, batch     7 | loss: 1.2809103MemoryTrain:  epoch  7, batch     8 | loss: 1.2772858MemoryTrain:  epoch  7, batch     9 | loss: 1.2058113MemoryTrain:  epoch  7, batch    10 | loss: 1.3469775MemoryTrain:  epoch  7, batch    11 | loss: 1.2329564MemoryTrain:  epoch  7, batch    12 | loss: 1.2379587MemoryTrain:  epoch  7, batch    13 | loss: 1.5334585MemoryTrain:  epoch  8, batch     0 | loss: 1.2362899MemoryTrain:  epoch  8, batch     1 | loss: 1.3239058MemoryTrain:  epoch  8, batch     2 | loss: 1.3037717MemoryTrain:  epoch  8, batch     3 | loss: 1.2273786MemoryTrain:  epoch  8, batch     4 | loss: 1.2735460MemoryTrain:  epoch  8, batch     5 | loss: 1.2443258MemoryTrain:  epoch  8, batch     6 | loss: 1.2474291MemoryTrain:  epoch  8, batch     7 | loss: 1.3257438MemoryTrain:  epoch  8, batch     8 | loss: 1.1967303MemoryTrain:  epoch  8, batch     9 | loss: 1.2044759MemoryTrain:  epoch  8, batch    10 | loss: 1.2763789MemoryTrain:  epoch  8, batch    11 | loss: 1.2085242MemoryTrain:  epoch  8, batch    12 | loss: 1.2679060MemoryTrain:  epoch  8, batch    13 | loss: 1.1628691MemoryTrain:  epoch  9, batch     0 | loss: 1.2559379MemoryTrain:  epoch  9, batch     1 | loss: 1.2581236MemoryTrain:  epoch  9, batch     2 | loss: 1.2352653MemoryTrain:  epoch  9, batch     3 | loss: 1.2185187MemoryTrain:  epoch  9, batch     4 | loss: 1.2035708MemoryTrain:  epoch  9, batch     5 | loss: 1.2502257MemoryTrain:  epoch  9, batch     6 | loss: 1.2581878MemoryTrain:  epoch  9, batch     7 | loss: 1.2301624MemoryTrain:  epoch  9, batch     8 | loss: 1.2399082MemoryTrain:  epoch  9, batch     9 | loss: 1.2237412MemoryTrain:  epoch  9, batch    10 | loss: 1.2458466MemoryTrain:  epoch  9, batch    11 | loss: 1.2501166MemoryTrain:  epoch  9, batch    12 | loss: 1.2442079MemoryTrain:  epoch  9, batch    13 | loss: 1.1692206
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 92.86%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 92.97%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 88.28%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 87.87%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 87.17%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 86.25%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 85.12%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 84.94%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 84.51%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 83.85%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 83.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.65%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.26%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 84.60%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.13%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.09%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.33%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 85.80%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 85.66%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 85.54%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 85.24%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 84.80%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 84.54%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.67%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 86.01%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 86.34%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 86.51%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 87.09%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 87.37%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 87.63%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 87.88%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 88.12%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 88.11%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 88.22%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 88.33%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 88.43%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 88.30%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 88.50%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 88.49%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 88.58%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 88.67%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 88.85%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 89.04%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 89.11%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 88.49%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 67.97%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 66.67%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 66.25%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 63.07%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 60.42%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 60.58%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 62.95%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 65.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 70.14%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 71.71%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 71.56%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 72.02%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 70.92%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 71.35%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 71.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.84%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 73.84%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 74.55%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 75.22%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 76.81%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 77.54%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 78.22%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 78.86%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 79.11%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 79.34%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 79.90%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 80.10%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 80.61%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 81.09%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 81.40%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 81.55%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 81.83%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 81.96%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 81.39%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 80.98%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 80.32%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 79.69%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 79.21%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 78.25%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 78.19%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 78.07%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 77.78%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 77.39%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 77.23%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 76.86%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 76.40%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 76.38%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 76.64%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 76.41%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 75.79%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 74.61%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 73.65%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 72.54%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 71.46%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 70.40%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 69.93%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 70.18%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 70.60%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 70.92%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 71.23%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 71.45%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 71.67%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 71.63%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 71.43%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 71.39%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 71.36%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 71.17%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 71.06%   [EVAL] batch:   81 | acc: 12.50%,  total acc: 70.35%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 69.73%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 69.20%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 68.90%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 68.46%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 68.03%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 67.47%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 66.71%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 65.97%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 65.25%   [EVAL] batch:   91 | acc: 0.00%,  total acc: 64.54%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 63.91%   [EVAL] batch:   93 | acc: 12.50%,  total acc: 63.36%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 63.49%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 63.74%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 63.98%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 64.35%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 64.58%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 64.62%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 64.91%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 65.20%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 65.41%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 65.89%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 66.21%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 66.18%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 65.74%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 65.42%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 64.89%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 64.70%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 64.40%   [EVAL] batch:  112 | acc: 25.00%,  total acc: 64.05%   [EVAL] batch:  113 | acc: 37.50%,  total acc: 63.82%   [EVAL] batch:  114 | acc: 62.50%,  total acc: 63.80%   [EVAL] batch:  115 | acc: 68.75%,  total acc: 63.85%   [EVAL] batch:  116 | acc: 43.75%,  total acc: 63.68%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 63.67%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 63.60%   [EVAL] batch:  119 | acc: 43.75%,  total acc: 63.44%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 63.27%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 63.17%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 63.16%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 63.31%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 63.30%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 63.29%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 63.09%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 62.89%   [EVAL] batch:  128 | acc: 18.75%,  total acc: 62.55%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 62.36%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 62.45%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 62.73%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 62.87%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 63.01%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 63.28%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 63.46%   [EVAL] batch:  137 | acc: 37.50%,  total acc: 63.27%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 62.90%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 62.50%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 62.23%   [EVAL] batch:  141 | acc: 6.25%,  total acc: 61.84%   [EVAL] batch:  142 | acc: 18.75%,  total acc: 61.54%   [EVAL] batch:  143 | acc: 18.75%,  total acc: 61.24%   [EVAL] batch:  144 | acc: 43.75%,  total acc: 61.12%   [EVAL] batch:  145 | acc: 31.25%,  total acc: 60.92%   [EVAL] batch:  146 | acc: 43.75%,  total acc: 60.80%   [EVAL] batch:  147 | acc: 50.00%,  total acc: 60.73%   [EVAL] batch:  148 | acc: 50.00%,  total acc: 60.65%   [EVAL] batch:  149 | acc: 43.75%,  total acc: 60.54%   [EVAL] batch:  150 | acc: 62.50%,  total acc: 60.55%   [EVAL] batch:  151 | acc: 75.00%,  total acc: 60.65%   [EVAL] batch:  152 | acc: 62.50%,  total acc: 60.66%   [EVAL] batch:  153 | acc: 43.75%,  total acc: 60.55%   [EVAL] batch:  154 | acc: 50.00%,  total acc: 60.48%   [EVAL] batch:  155 | acc: 50.00%,  total acc: 60.42%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 60.47%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 60.44%   [EVAL] batch:  158 | acc: 68.75%,  total acc: 60.50%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 60.51%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 60.52%   [EVAL] batch:  161 | acc: 68.75%,  total acc: 60.57%   [EVAL] batch:  162 | acc: 75.00%,  total acc: 60.66%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 60.67%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 60.76%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 60.77%   [EVAL] batch:  166 | acc: 81.25%,  total acc: 60.89%   [EVAL] batch:  167 | acc: 81.25%,  total acc: 61.01%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 60.98%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 60.81%   [EVAL] batch:  170 | acc: 18.75%,  total acc: 60.56%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 60.39%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 60.26%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 60.06%   [EVAL] batch:  174 | acc: 31.25%,  total acc: 59.89%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 60.12%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 60.35%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 60.57%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 60.79%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 61.01%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 61.22%   [EVAL] batch:  181 | acc: 87.50%,  total acc: 61.37%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 61.41%   [EVAL] batch:  183 | acc: 81.25%,  total acc: 61.51%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 61.62%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 61.76%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 61.83%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 62.00%   [EVAL] batch:  188 | acc: 75.00%,  total acc: 62.07%   [EVAL] batch:  189 | acc: 81.25%,  total acc: 62.17%   [EVAL] batch:  190 | acc: 81.25%,  total acc: 62.27%   [EVAL] batch:  191 | acc: 87.50%,  total acc: 62.40%   [EVAL] batch:  192 | acc: 75.00%,  total acc: 62.47%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 62.56%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 62.66%   [EVAL] batch:  195 | acc: 81.25%,  total acc: 62.76%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 62.79%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 62.91%   [EVAL] batch:  198 | acc: 81.25%,  total acc: 63.00%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 63.12%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 63.09%   [EVAL] batch:  201 | acc: 62.50%,  total acc: 63.09%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 63.05%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 63.05%   [EVAL] batch:  204 | acc: 50.00%,  total acc: 62.99%   [EVAL] batch:  205 | acc: 62.50%,  total acc: 62.99%   [EVAL] batch:  206 | acc: 25.00%,  total acc: 62.80%   [EVAL] batch:  207 | acc: 37.50%,  total acc: 62.68%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 62.50%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 62.32%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 62.20%   [EVAL] batch:  211 | acc: 50.00%,  total acc: 62.15%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 62.09%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 62.24%   [EVAL] batch:  214 | acc: 93.75%,  total acc: 62.38%   [EVAL] batch:  215 | acc: 68.75%,  total acc: 62.41%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 62.59%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 62.73%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 62.87%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 63.04%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 63.21%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 63.37%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 63.54%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 63.70%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 63.86%   [EVAL] batch:  225 | acc: 87.50%,  total acc: 63.97%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 64.07%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 64.09%   [EVAL] batch:  228 | acc: 87.50%,  total acc: 64.19%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 64.32%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 64.39%   [EVAL] batch:  231 | acc: 62.50%,  total acc: 64.39%   [EVAL] batch:  232 | acc: 37.50%,  total acc: 64.27%   [EVAL] batch:  233 | acc: 62.50%,  total acc: 64.26%   [EVAL] batch:  234 | acc: 43.75%,  total acc: 64.18%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 64.17%   [EVAL] batch:  236 | acc: 43.75%,  total acc: 64.08%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 64.18%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 64.28%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 64.43%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 64.50%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 64.59%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 64.71%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 64.81%   [EVAL] batch:  244 | acc: 62.50%,  total acc: 64.80%   [EVAL] batch:  245 | acc: 50.00%,  total acc: 64.74%   [EVAL] batch:  246 | acc: 50.00%,  total acc: 64.68%   [EVAL] batch:  247 | acc: 56.25%,  total acc: 64.64%   [EVAL] batch:  248 | acc: 43.75%,  total acc: 64.56%   [EVAL] batch:  249 | acc: 75.00%,  total acc: 64.60%   [EVAL] batch:  250 | acc: 43.75%,  total acc: 64.52%   [EVAL] batch:  251 | acc: 68.75%,  total acc: 64.53%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 64.58%   [EVAL] batch:  253 | acc: 56.25%,  total acc: 64.54%   [EVAL] batch:  254 | acc: 68.75%,  total acc: 64.56%   [EVAL] batch:  255 | acc: 81.25%,  total acc: 64.62%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 64.57%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 64.63%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 64.67%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 64.69%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 64.61%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 64.62%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 64.69%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 64.65%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 64.60%   [EVAL] batch:  265 | acc: 50.00%,  total acc: 64.54%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 64.51%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 64.51%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 64.61%   [EVAL] batch:  269 | acc: 81.25%,  total acc: 64.68%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 64.81%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 64.94%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 65.04%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 65.15%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 65.27%   [EVAL] batch:  275 | acc: 81.25%,  total acc: 65.33%   [EVAL] batch:  276 | acc: 81.25%,  total acc: 65.39%   [EVAL] batch:  277 | acc: 87.50%,  total acc: 65.47%   [EVAL] batch:  278 | acc: 68.75%,  total acc: 65.48%   [EVAL] batch:  279 | acc: 87.50%,  total acc: 65.56%   [EVAL] batch:  280 | acc: 81.25%,  total acc: 65.61%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 65.65%   [EVAL] batch:  282 | acc: 75.00%,  total acc: 65.68%   [EVAL] batch:  283 | acc: 37.50%,  total acc: 65.58%   [EVAL] batch:  284 | acc: 56.25%,  total acc: 65.55%   [EVAL] batch:  285 | acc: 43.75%,  total acc: 65.47%   [EVAL] batch:  286 | acc: 31.25%,  total acc: 65.35%   [EVAL] batch:  287 | acc: 81.25%,  total acc: 65.41%   [EVAL] batch:  288 | acc: 75.00%,  total acc: 65.44%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 65.52%   [EVAL] batch:  290 | acc: 81.25%,  total acc: 65.57%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 65.67%   [EVAL] batch:  292 | acc: 62.50%,  total acc: 65.66%   [EVAL] batch:  293 | acc: 75.00%,  total acc: 65.69%   [EVAL] batch:  294 | acc: 68.75%,  total acc: 65.70%   [EVAL] batch:  295 | acc: 50.00%,  total acc: 65.65%   [EVAL] batch:  296 | acc: 37.50%,  total acc: 65.55%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 65.52%   [EVAL] batch:  298 | acc: 62.50%,  total acc: 65.51%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 65.54%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 65.66%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 65.77%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 65.88%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 66.00%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 66.11%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 66.22%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 66.33%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 66.42%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 66.50%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 66.61%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 66.72%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 66.83%   [EVAL] batch:  312 | acc: 87.50%,  total acc: 66.89%   [EVAL] batch:  313 | acc: 62.50%,  total acc: 66.88%   [EVAL] batch:  314 | acc: 62.50%,  total acc: 66.87%   [EVAL] batch:  315 | acc: 93.75%,  total acc: 66.95%   [EVAL] batch:  316 | acc: 81.25%,  total acc: 67.00%   [EVAL] batch:  317 | acc: 93.75%,  total acc: 67.08%   [EVAL] batch:  318 | acc: 68.75%,  total acc: 67.08%   [EVAL] batch:  319 | acc: 68.75%,  total acc: 67.09%   [EVAL] batch:  320 | acc: 62.50%,  total acc: 67.08%   [EVAL] batch:  321 | acc: 62.50%,  total acc: 67.06%   [EVAL] batch:  322 | acc: 68.75%,  total acc: 67.07%   [EVAL] batch:  323 | acc: 62.50%,  total acc: 67.05%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 67.08%   [EVAL] batch:  325 | acc: 12.50%,  total acc: 66.91%   [EVAL] batch:  326 | acc: 18.75%,  total acc: 66.76%   [EVAL] batch:  327 | acc: 18.75%,  total acc: 66.62%   [EVAL] batch:  328 | acc: 18.75%,  total acc: 66.47%   [EVAL] batch:  329 | acc: 25.00%,  total acc: 66.34%   [EVAL] batch:  330 | acc: 0.00%,  total acc: 66.14%   [EVAL] batch:  331 | acc: 68.75%,  total acc: 66.15%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 66.35%   [EVAL] batch:  334 | acc: 100.00%,  total acc: 66.46%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 66.56%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 66.64%   [EVAL] batch:  337 | acc: 75.00%,  total acc: 66.66%   [EVAL] batch:  338 | acc: 25.00%,  total acc: 66.54%   [EVAL] batch:  339 | acc: 31.25%,  total acc: 66.43%   [EVAL] batch:  340 | acc: 56.25%,  total acc: 66.40%   [EVAL] batch:  341 | acc: 37.50%,  total acc: 66.32%   [EVAL] batch:  342 | acc: 25.00%,  total acc: 66.20%   [EVAL] batch:  343 | acc: 62.50%,  total acc: 66.19%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 66.29%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 66.37%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 66.44%   [EVAL] batch:  347 | acc: 87.50%,  total acc: 66.51%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 66.60%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 66.70%   [EVAL] batch:  350 | acc: 50.00%,  total acc: 66.65%   [EVAL] batch:  351 | acc: 56.25%,  total acc: 66.62%   [EVAL] batch:  352 | acc: 50.00%,  total acc: 66.57%   [EVAL] batch:  353 | acc: 31.25%,  total acc: 66.47%   [EVAL] batch:  354 | acc: 62.50%,  total acc: 66.46%   [EVAL] batch:  355 | acc: 43.75%,  total acc: 66.40%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 66.46%   [EVAL] batch:  357 | acc: 100.00%,  total acc: 66.55%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 66.64%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 66.74%   [EVAL] batch:  360 | acc: 81.25%,  total acc: 66.78%   [EVAL] batch:  361 | acc: 93.75%,  total acc: 66.85%   [EVAL] batch:  362 | acc: 93.75%,  total acc: 66.92%   [EVAL] batch:  363 | acc: 56.25%,  total acc: 66.90%   [EVAL] batch:  364 | acc: 81.25%,  total acc: 66.93%   [EVAL] batch:  365 | acc: 75.00%,  total acc: 66.96%   [EVAL] batch:  366 | acc: 81.25%,  total acc: 67.00%   [EVAL] batch:  367 | acc: 81.25%,  total acc: 67.03%   [EVAL] batch:  368 | acc: 75.00%,  total acc: 67.06%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 67.13%   [EVAL] batch:  370 | acc: 93.75%,  total acc: 67.20%   [EVAL] batch:  371 | acc: 81.25%,  total acc: 67.24%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 67.28%   [EVAL] batch:  373 | acc: 75.00%,  total acc: 67.30%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 67.33%   [EVAL] batch:  375 | acc: 100.00%,  total acc: 67.42%   [EVAL] batch:  376 | acc: 93.75%,  total acc: 67.49%   [EVAL] batch:  377 | acc: 93.75%,  total acc: 67.56%   [EVAL] batch:  378 | acc: 87.50%,  total acc: 67.61%   [EVAL] batch:  379 | acc: 93.75%,  total acc: 67.68%   [EVAL] batch:  380 | acc: 100.00%,  total acc: 67.77%   [EVAL] batch:  381 | acc: 81.25%,  total acc: 67.80%   [EVAL] batch:  382 | acc: 93.75%,  total acc: 67.87%   [EVAL] batch:  383 | acc: 75.00%,  total acc: 67.89%   [EVAL] batch:  384 | acc: 93.75%,  total acc: 67.95%   [EVAL] batch:  385 | acc: 75.00%,  total acc: 67.97%   [EVAL] batch:  386 | acc: 75.00%,  total acc: 67.99%   [EVAL] batch:  387 | acc: 93.75%,  total acc: 68.06%   [EVAL] batch:  388 | acc: 81.25%,  total acc: 68.09%   [EVAL] batch:  389 | acc: 93.75%,  total acc: 68.16%   [EVAL] batch:  390 | acc: 81.25%,  total acc: 68.19%   [EVAL] batch:  391 | acc: 81.25%,  total acc: 68.22%   [EVAL] batch:  392 | acc: 81.25%,  total acc: 68.26%   [EVAL] batch:  393 | acc: 81.25%,  total acc: 68.29%   [EVAL] batch:  394 | acc: 68.75%,  total acc: 68.29%   [EVAL] batch:  395 | acc: 62.50%,  total acc: 68.28%   [EVAL] batch:  396 | acc: 81.25%,  total acc: 68.31%   [EVAL] batch:  397 | acc: 75.00%,  total acc: 68.33%   [EVAL] batch:  398 | acc: 68.75%,  total acc: 68.33%   [EVAL] batch:  399 | acc: 62.50%,  total acc: 68.31%   [EVAL] batch:  400 | acc: 100.00%,  total acc: 68.39%   [EVAL] batch:  401 | acc: 100.00%,  total acc: 68.47%   [EVAL] batch:  402 | acc: 93.75%,  total acc: 68.53%   [EVAL] batch:  403 | acc: 100.00%,  total acc: 68.61%   [EVAL] batch:  404 | acc: 100.00%,  total acc: 68.69%   [EVAL] batch:  405 | acc: 100.00%,  total acc: 68.77%   [EVAL] batch:  406 | acc: 93.75%,  total acc: 68.83%   [EVAL] batch:  407 | acc: 68.75%,  total acc: 68.83%   [EVAL] batch:  408 | acc: 81.25%,  total acc: 68.86%   [EVAL] batch:  409 | acc: 81.25%,  total acc: 68.89%   [EVAL] batch:  410 | acc: 75.00%,  total acc: 68.90%   [EVAL] batch:  411 | acc: 68.75%,  total acc: 68.90%   [EVAL] batch:  412 | acc: 75.00%,  total acc: 68.92%   [EVAL] batch:  413 | acc: 100.00%,  total acc: 68.99%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 69.07%   [EVAL] batch:  415 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:  416 | acc: 100.00%,  total acc: 69.21%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 69.29%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 69.35%   [EVAL] batch:  419 | acc: 100.00%,  total acc: 69.42%   [EVAL] batch:  420 | acc: 100.00%,  total acc: 69.49%   [EVAL] batch:  421 | acc: 100.00%,  total acc: 69.56%   [EVAL] batch:  422 | acc: 100.00%,  total acc: 69.64%   [EVAL] batch:  423 | acc: 100.00%,  total acc: 69.71%   [EVAL] batch:  424 | acc: 100.00%,  total acc: 69.78%   [EVAL] batch:  425 | acc: 87.50%,  total acc: 69.82%   [EVAL] batch:  426 | acc: 93.75%,  total acc: 69.88%   [EVAL] batch:  427 | acc: 93.75%,  total acc: 69.93%   [EVAL] batch:  428 | acc: 93.75%,  total acc: 69.99%   [EVAL] batch:  429 | acc: 81.25%,  total acc: 70.01%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 70.08%   [EVAL] batch:  431 | acc: 87.50%,  total acc: 70.12%   [EVAL] batch:  432 | acc: 93.75%,  total acc: 70.18%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 70.23%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 70.30%   [EVAL] batch:  435 | acc: 100.00%,  total acc: 70.37%   [EVAL] batch:  436 | acc: 93.75%,  total acc: 70.42%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 70.38%   
cur_acc:  ['0.9524', '0.7014', '0.7669', '0.8194', '0.7817', '0.6984', '0.8849']
his_acc:  ['0.9524', '0.8165', '0.7769', '0.7630', '0.7224', '0.6840', '0.7038']
CurrentTrain: epoch  0, batch     0 | loss: 5.2439871CurrentTrain: epoch  0, batch     1 | loss: 6.5660362CurrentTrain: epoch  0, batch     2 | loss: 6.0414562CurrentTrain: epoch  0, batch     3 | loss: 6.2754412CurrentTrain: epoch  1, batch     0 | loss: 4.2299213CurrentTrain: epoch  1, batch     1 | loss: 4.3994722CurrentTrain: epoch  1, batch     2 | loss: 5.7815304CurrentTrain: epoch  1, batch     3 | loss: 5.6075077CurrentTrain: epoch  2, batch     0 | loss: 4.4368820CurrentTrain: epoch  2, batch     1 | loss: 4.0806389CurrentTrain: epoch  2, batch     2 | loss: 4.4235802CurrentTrain: epoch  2, batch     3 | loss: 4.0015850CurrentTrain: epoch  3, batch     0 | loss: 4.6609774CurrentTrain: epoch  3, batch     1 | loss: 4.4638577CurrentTrain: epoch  3, batch     2 | loss: 3.4371052CurrentTrain: epoch  3, batch     3 | loss: 2.7121868CurrentTrain: epoch  4, batch     0 | loss: 3.7388663CurrentTrain: epoch  4, batch     1 | loss: 3.5053060CurrentTrain: epoch  4, batch     2 | loss: 3.9679487CurrentTrain: epoch  4, batch     3 | loss: 2.5599227CurrentTrain: epoch  5, batch     0 | loss: 3.4156079CurrentTrain: epoch  5, batch     1 | loss: 3.1602416CurrentTrain: epoch  5, batch     2 | loss: 3.1829932CurrentTrain: epoch  5, batch     3 | loss: 3.2874699CurrentTrain: epoch  6, batch     0 | loss: 3.4324617CurrentTrain: epoch  6, batch     1 | loss: 2.8125901CurrentTrain: epoch  6, batch     2 | loss: 2.8565948CurrentTrain: epoch  6, batch     3 | loss: 2.4292378CurrentTrain: epoch  7, batch     0 | loss: 2.7433701CurrentTrain: epoch  7, batch     1 | loss: 3.3924112CurrentTrain: epoch  7, batch     2 | loss: 2.4381742CurrentTrain: epoch  7, batch     3 | loss: 3.0382423CurrentTrain: epoch  8, batch     0 | loss: 2.4519734CurrentTrain: epoch  8, batch     1 | loss: 2.9590869CurrentTrain: epoch  8, batch     2 | loss: 2.5575647CurrentTrain: epoch  8, batch     3 | loss: 3.5122142CurrentTrain: epoch  9, batch     0 | loss: 2.2545757CurrentTrain: epoch  9, batch     1 | loss: 2.6281862CurrentTrain: epoch  9, batch     2 | loss: 2.6925480CurrentTrain: epoch  9, batch     3 | loss: 3.8375552
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: peugeot took a similar step in 2010 when replacing the 407 and long - running but unpopular 607 with a single model , the 508 .
Head Entity: 508
Tail Entity: 407
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: In the 2021 season, the team introduced the new model, the X5, which follows the successful launch of the X3 in 2020.  
Head Entity: X5  
Tail Entity: X3  

Relation: follows  
Context: The latest smartphone, the Galaxy S21, follows the previous model, the Galaxy S20, which was released just a year earlier.  
Head Entity: Galaxy S21  
Tail Entity: Galaxy S20  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: it is primarily used for baseball and serves as the home stadium for yaquis de obregón .
Head Entity: yaquis de obregón
Tail Entity: baseball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: she has been a professional athlete in tennis since she was a teenager and has won several championships.  
Head Entity: she  
Tail Entity: tennis  

Relation: sport  
Context: the city is known for its vibrant soccer culture and is home to several professional teams.  
Head Entity: the city  
Tail Entity: soccer  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael was the father of two children, including his daughter emily, who is pursuing a career in medicine.  
Head Entity: emily  
Tail Entity: michael  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: hana yori dango returns ( ) is a japanese television drama series , broadcast on tbs in 2007 .
Head Entity: hana yori dango returns
Tail Entity: tbs
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: the popular series breaking bad was originally aired on amc from 2008 to 2013.  
Head Entity: breaking bad  
Tail Entity: amc  

Relation: original network  
Context: the iconic sitcom friends premiered on nbc in 1994 and quickly became a cultural phenomenon.  
Head Entity: friends  
Tail Entity: nbc  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: the 1954 film , directed by kadri venkata reddy , " peddamanushulu " was honoured with the first president 's silver medal for best feature film in telugu .
Head Entity: peddamanushulu
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: the 1994 animated film "The Lion King," produced by Walt Disney, was originally created in English before being dubbed into several other languages.  
Head Entity: The Lion King  
Tail Entity: English  

Relation: original language of film or TV show  
Context: the critically acclaimed 2001 film "Amélie," directed by Jean-Pierre Jeunet, was filmed in French and has since gained a global audience.  
Head Entity: Amélie  
Tail Entity: French  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: ortiz was not offered a contract by the rockies after the season , and rather than catch on with another major league club , ortiz signed with the orix bluewave of the japanese pacific league .
Head Entity: orix bluewave
Tail Entity: pacific league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: after a successful season with the new york knicks, the player decided to join the los angeles lakers in the national basketball association.  
Head Entity: los angeles lakers  
Tail Entity: national basketball association  

Relation: league  
Context: the star forward played for several years in the english premier league before transferring to a club in the bundesliga.  
Head Entity: english premier league  
Tail Entity: bundesliga  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: the first stewart king of scotland and son of marjorie bruce and walter stewart , robert ii , is believed to have been born in the abbey .
Head Entity: robert ii
Tail Entity: marjorie bruce
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: in the historical records, it is noted that queen elizabeth i was the daughter of anne boleyn, who served as her mother during her early years.  
Head Entity: queen elizabeth i  
Tail Entity: anne boleyn  

Relation: mother  
Context: the famous artist pablo picasso often credited his mother, maría ruiz, as a significant influence on his early artistic development.  
Head Entity: pablo picasso  
Tail Entity: maría ruiz  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres. he has dedicated his life to music and is known for his mastery of the cello.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: a u.s. government - funded $ 36 million bridge over the panj river connects sher khan bandar in afghanistan with nizhniy pyanzh in tajikistan , which transport more than 150 trucks or 1,000 cars daily .
Head Entity: sher khan bandar
Tail Entity: afghanistan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the ancient city of petra, located in southern jordan, is famous for its rock-cut architecture and water conduit system, attracting thousands of tourists each year.  
Head Entity: petra  
Tail Entity: jordan  

Relation: country  
Context: the great wall of china, a series of fortifications made of various materials, stretches across northern china and is a UNESCO World Heritage site.  
Head Entity: great wall of china  
Tail Entity: china  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: klaus kinski , who would play dracula himself nine years later in " nosferatu the vampyre " , is also featured in the film as renfield .
Head Entity: nosferatu the vampyre
Tail Entity: renfield
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: in the animated series "avatar: the last airbender," zuko is a complex character who struggles with his identity and family legacy.  
Head Entity: avatar: the last airbender  
Tail Entity: zuko  

Relation: characters  
Context: the novel "pride and prejudice" features elizabeth bennet, a strong-willed protagonist who navigates societal expectations and her feelings for mr. darcy.  
Head Entity: pride and prejudice  
Tail Entity: elizabeth bennet  
Mixup data size:  558
MixupTrain:  epoch  0, batch     0 | loss: 1.9259196MixupTrain:  epoch  0, batch     1 | loss: 1.8081397MixupTrain:  epoch  0, batch     2 | loss: 1.5469717MixupTrain:  epoch  0, batch     3 | loss: 1.8038205MixupTrain:  epoch  0, batch     4 | loss: 1.6469347MixupTrain:  epoch  0, batch     5 | loss: 2.0387580MixupTrain:  epoch  0, batch     6 | loss: 2.0211511MixupTrain:  epoch  0, batch     7 | loss: 2.1040610MixupTrain:  epoch  0, batch     8 | loss: 1.5635366MixupTrain:  epoch  0, batch     9 | loss: 2.0795553MixupTrain:  epoch  0, batch    10 | loss: 1.7562930MixupTrain:  epoch  0, batch    11 | loss: 1.7597411MixupTrain:  epoch  0, batch    12 | loss: 1.7818706MixupTrain:  epoch  0, batch    13 | loss: 2.2769696MixupTrain:  epoch  0, batch    14 | loss: 1.9484905MixupTrain:  epoch  0, batch    15 | loss: 2.2021180MixupTrain:  epoch  0, batch    16 | loss: 2.2441228MixupTrain:  epoch  0, batch    17 | loss: 1.3768467MixupTrain:  epoch  0, batch    18 | loss: 2.1470130MixupTrain:  epoch  0, batch    19 | loss: 2.1146834MixupTrain:  epoch  0, batch    20 | loss: 1.6819197MixupTrain:  epoch  0, batch    21 | loss: 1.7305426MixupTrain:  epoch  0, batch    22 | loss: 1.6462499MixupTrain:  epoch  0, batch    23 | loss: 1.7308991MixupTrain:  epoch  0, batch    24 | loss: 2.1377585MixupTrain:  epoch  0, batch    25 | loss: 2.0231326MixupTrain:  epoch  0, batch    26 | loss: 1.5989506MixupTrain:  epoch  0, batch    27 | loss: 1.6010218MixupTrain:  epoch  0, batch    28 | loss: 1.8712111MixupTrain:  epoch  0, batch    29 | loss: 1.6868930MixupTrain:  epoch  0, batch    30 | loss: 1.5928783MixupTrain:  epoch  0, batch    31 | loss: 1.9303015MixupTrain:  epoch  0, batch    32 | loss: 1.6472198MixupTrain:  epoch  0, batch    33 | loss: 1.6114375MixupTrain:  epoch  0, batch    34 | loss: 1.8498068
MemoryTrain:  epoch  0, batch     0 | loss: 2.1316671MemoryTrain:  epoch  0, batch     1 | loss: 1.3017764MemoryTrain:  epoch  0, batch     2 | loss: 1.7626104MemoryTrain:  epoch  0, batch     3 | loss: 1.4182092MemoryTrain:  epoch  0, batch     4 | loss: 1.8710561MemoryTrain:  epoch  0, batch     5 | loss: 1.6853725MemoryTrain:  epoch  0, batch     6 | loss: 3.2360106MemoryTrain:  epoch  0, batch     7 | loss: 1.8393598MemoryTrain:  epoch  0, batch     8 | loss: 1.6111341MemoryTrain:  epoch  0, batch     9 | loss: 1.6108985MemoryTrain:  epoch  0, batch    10 | loss: 2.1308308MemoryTrain:  epoch  0, batch    11 | loss: 2.0654402MemoryTrain:  epoch  0, batch    12 | loss: 2.5819955MemoryTrain:  epoch  0, batch    13 | loss: 1.6392398MemoryTrain:  epoch  0, batch    14 | loss: 2.1962993MemoryTrain:  epoch  1, batch     0 | loss: 1.7493913MemoryTrain:  epoch  1, batch     1 | loss: 1.3999796MemoryTrain:  epoch  1, batch     2 | loss: 2.6279638MemoryTrain:  epoch  1, batch     3 | loss: 1.6846325MemoryTrain:  epoch  1, batch     4 | loss: 1.2712131MemoryTrain:  epoch  1, batch     5 | loss: 1.7870762MemoryTrain:  epoch  1, batch     6 | loss: 2.4801779MemoryTrain:  epoch  1, batch     7 | loss: 1.8766422MemoryTrain:  epoch  1, batch     8 | loss: 1.7950188MemoryTrain:  epoch  1, batch     9 | loss: 1.4504849MemoryTrain:  epoch  1, batch    10 | loss: 1.8403774MemoryTrain:  epoch  1, batch    11 | loss: 1.3309853MemoryTrain:  epoch  1, batch    12 | loss: 1.6033363MemoryTrain:  epoch  1, batch    13 | loss: 1.2861881MemoryTrain:  epoch  1, batch    14 | loss: 1.8275268MemoryTrain:  epoch  2, batch     0 | loss: 1.3561602MemoryTrain:  epoch  2, batch     1 | loss: 1.6802535MemoryTrain:  epoch  2, batch     2 | loss: 1.7981570MemoryTrain:  epoch  2, batch     3 | loss: 2.1727030MemoryTrain:  epoch  2, batch     4 | loss: 1.7151413MemoryTrain:  epoch  2, batch     5 | loss: 1.4922557MemoryTrain:  epoch  2, batch     6 | loss: 1.6084194MemoryTrain:  epoch  2, batch     7 | loss: 1.5640273MemoryTrain:  epoch  2, batch     8 | loss: 1.5964648MemoryTrain:  epoch  2, batch     9 | loss: 1.5427666MemoryTrain:  epoch  2, batch    10 | loss: 1.2831433MemoryTrain:  epoch  2, batch    11 | loss: 1.6206391MemoryTrain:  epoch  2, batch    12 | loss: 1.6707639MemoryTrain:  epoch  2, batch    13 | loss: 1.3370445MemoryTrain:  epoch  2, batch    14 | loss: 1.7586049MemoryTrain:  epoch  3, batch     0 | loss: 1.7693055MemoryTrain:  epoch  3, batch     1 | loss: 1.2727966MemoryTrain:  epoch  3, batch     2 | loss: 1.9576125MemoryTrain:  epoch  3, batch     3 | loss: 1.6617154MemoryTrain:  epoch  3, batch     4 | loss: 1.3621981MemoryTrain:  epoch  3, batch     5 | loss: 1.3479614MemoryTrain:  epoch  3, batch     6 | loss: 1.6636080MemoryTrain:  epoch  3, batch     7 | loss: 1.2708811MemoryTrain:  epoch  3, batch     8 | loss: 1.4579865MemoryTrain:  epoch  3, batch     9 | loss: 1.9469054MemoryTrain:  epoch  3, batch    10 | loss: 1.4603497MemoryTrain:  epoch  3, batch    11 | loss: 1.5157918MemoryTrain:  epoch  3, batch    12 | loss: 1.6434894MemoryTrain:  epoch  3, batch    13 | loss: 1.3816030MemoryTrain:  epoch  3, batch    14 | loss: 1.2156184MemoryTrain:  epoch  4, batch     0 | loss: 1.3986514MemoryTrain:  epoch  4, batch     1 | loss: 1.2042871MemoryTrain:  epoch  4, batch     2 | loss: 1.4323152MemoryTrain:  epoch  4, batch     3 | loss: 1.3245156MemoryTrain:  epoch  4, batch     4 | loss: 1.4112426MemoryTrain:  epoch  4, batch     5 | loss: 1.2996826MemoryTrain:  epoch  4, batch     6 | loss: 1.2698410MemoryTrain:  epoch  4, batch     7 | loss: 1.6082506MemoryTrain:  epoch  4, batch     8 | loss: 1.3077471MemoryTrain:  epoch  4, batch     9 | loss: 1.5534544MemoryTrain:  epoch  4, batch    10 | loss: 1.2446868MemoryTrain:  epoch  4, batch    11 | loss: 1.3299210MemoryTrain:  epoch  4, batch    12 | loss: 1.4321334MemoryTrain:  epoch  4, batch    13 | loss: 1.2456045MemoryTrain:  epoch  4, batch    14 | loss: 1.5467942MemoryTrain:  epoch  5, batch     0 | loss: 1.2601063MemoryTrain:  epoch  5, batch     1 | loss: 1.3221025MemoryTrain:  epoch  5, batch     2 | loss: 1.4128740MemoryTrain:  epoch  5, batch     3 | loss: 1.5804143MemoryTrain:  epoch  5, batch     4 | loss: 1.2372098MemoryTrain:  epoch  5, batch     5 | loss: 1.4315941MemoryTrain:  epoch  5, batch     6 | loss: 1.2158573MemoryTrain:  epoch  5, batch     7 | loss: 1.2021334MemoryTrain:  epoch  5, batch     8 | loss: 1.4463398MemoryTrain:  epoch  5, batch     9 | loss: 1.2500108MemoryTrain:  epoch  5, batch    10 | loss: 1.2958670MemoryTrain:  epoch  5, batch    11 | loss: 1.4458085MemoryTrain:  epoch  5, batch    12 | loss: 1.2557549MemoryTrain:  epoch  5, batch    13 | loss: 1.2323396MemoryTrain:  epoch  5, batch    14 | loss: 1.2717009MemoryTrain:  epoch  6, batch     0 | loss: 1.2672989MemoryTrain:  epoch  6, batch     1 | loss: 1.4660521MemoryTrain:  epoch  6, batch     2 | loss: 1.2165952MemoryTrain:  epoch  6, batch     3 | loss: 1.3966599MemoryTrain:  epoch  6, batch     4 | loss: 1.3129746MemoryTrain:  epoch  6, batch     5 | loss: 1.3523930MemoryTrain:  epoch  6, batch     6 | loss: 1.2113698MemoryTrain:  epoch  6, batch     7 | loss: 1.2278225MemoryTrain:  epoch  6, batch     8 | loss: 1.3835585MemoryTrain:  epoch  6, batch     9 | loss: 1.3097612MemoryTrain:  epoch  6, batch    10 | loss: 1.2884101MemoryTrain:  epoch  6, batch    11 | loss: 1.4609075MemoryTrain:  epoch  6, batch    12 | loss: 1.2199479MemoryTrain:  epoch  6, batch    13 | loss: 1.3229679MemoryTrain:  epoch  6, batch    14 | loss: 1.3329194MemoryTrain:  epoch  7, batch     0 | loss: 1.3288310MemoryTrain:  epoch  7, batch     1 | loss: 1.2518450MemoryTrain:  epoch  7, batch     2 | loss: 1.3064198MemoryTrain:  epoch  7, batch     3 | loss: 1.2224058MemoryTrain:  epoch  7, batch     4 | loss: 1.2027241MemoryTrain:  epoch  7, batch     5 | loss: 1.2472785MemoryTrain:  epoch  7, batch     6 | loss: 1.1712931MemoryTrain:  epoch  7, batch     7 | loss: 1.2829081MemoryTrain:  epoch  7, batch     8 | loss: 1.2350905MemoryTrain:  epoch  7, batch     9 | loss: 1.1945107MemoryTrain:  epoch  7, batch    10 | loss: 1.2076454MemoryTrain:  epoch  7, batch    11 | loss: 1.2748547MemoryTrain:  epoch  7, batch    12 | loss: 1.2727532MemoryTrain:  epoch  7, batch    13 | loss: 1.2307757MemoryTrain:  epoch  7, batch    14 | loss: 1.3982495MemoryTrain:  epoch  8, batch     0 | loss: 1.1757759MemoryTrain:  epoch  8, batch     1 | loss: 1.2468537MemoryTrain:  epoch  8, batch     2 | loss: 1.2470433MemoryTrain:  epoch  8, batch     3 | loss: 1.2299281MemoryTrain:  epoch  8, batch     4 | loss: 1.1964836MemoryTrain:  epoch  8, batch     5 | loss: 1.2134962MemoryTrain:  epoch  8, batch     6 | loss: 1.3701404MemoryTrain:  epoch  8, batch     7 | loss: 1.2503940MemoryTrain:  epoch  8, batch     8 | loss: 1.2001932MemoryTrain:  epoch  8, batch     9 | loss: 1.2353904MemoryTrain:  epoch  8, batch    10 | loss: 1.3636031MemoryTrain:  epoch  8, batch    11 | loss: 1.2400095MemoryTrain:  epoch  8, batch    12 | loss: 1.3216103MemoryTrain:  epoch  8, batch    13 | loss: 1.2413070MemoryTrain:  epoch  8, batch    14 | loss: 1.2152586MemoryTrain:  epoch  9, batch     0 | loss: 1.2195548MemoryTrain:  epoch  9, batch     1 | loss: 1.2616224MemoryTrain:  epoch  9, batch     2 | loss: 1.1879566MemoryTrain:  epoch  9, batch     3 | loss: 1.1966963MemoryTrain:  epoch  9, batch     4 | loss: 1.2189369MemoryTrain:  epoch  9, batch     5 | loss: 1.2465315MemoryTrain:  epoch  9, batch     6 | loss: 1.2148029MemoryTrain:  epoch  9, batch     7 | loss: 1.2553142MemoryTrain:  epoch  9, batch     8 | loss: 1.2706629MemoryTrain:  epoch  9, batch     9 | loss: 1.2014704MemoryTrain:  epoch  9, batch    10 | loss: 1.2194581MemoryTrain:  epoch  9, batch    11 | loss: 1.3332388MemoryTrain:  epoch  9, batch    12 | loss: 1.2089132MemoryTrain:  epoch  9, batch    13 | loss: 1.2857699MemoryTrain:  epoch  9, batch    14 | loss: 1.2643341
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 39.58%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 35.94%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 41.25%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 39.58%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 43.75%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 48.44%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 52.08%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 55.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 57.95%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 59.38%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 60.10%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 60.27%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 60.83%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 60.16%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 60.29%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 61.11%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 61.51%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 63.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 66.48%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 67.66%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 69.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 70.91%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 71.99%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 72.99%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 73.71%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 74.17%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 74.80%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 75.59%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 76.33%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 77.02%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 78.72%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 78.62%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 78.21%   [EVAL] batch:   39 | acc: 37.50%,  total acc: 77.19%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 76.68%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 76.04%   [EVAL] batch:   42 | acc: 43.75%,  total acc: 75.29%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 75.28%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 75.56%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 75.82%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 76.20%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 76.17%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 75.77%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 75.88%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 75.98%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 75.96%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 75.94%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 75.93%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 76.02%   [EVAL] batch:   55 | acc: 37.50%,  total acc: 75.33%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 75.11%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 74.89%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 75.41%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 75.71%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 75.30%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 31.25%,  total acc: 65.62%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 64.58%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 60.23%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 57.81%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 57.69%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 60.27%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 64.84%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 66.54%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 68.06%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 69.74%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 69.38%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 69.94%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 69.89%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 69.29%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 69.27%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 69.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 70.91%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 71.99%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 72.99%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 73.71%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 74.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 75.40%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 76.17%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 76.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 77.57%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 77.68%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 77.95%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 78.38%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 78.62%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 80.03%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 80.52%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 80.68%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 80.42%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 80.03%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 79.52%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 79.04%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 78.57%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 77.88%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 77.82%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 77.76%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 77.71%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 77.31%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 76.82%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 76.67%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 76.10%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 75.11%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 74.58%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 74.48%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 73.87%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 73.39%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 72.72%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 71.58%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 70.58%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 69.51%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 68.47%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 67.46%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 66.94%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 67.05%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 67.52%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 67.88%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 68.24%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 68.50%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 68.91%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 68.83%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 68.67%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 68.67%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 68.06%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 67.62%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 67.19%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 66.91%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 66.50%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 66.09%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 65.55%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 64.82%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 64.10%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 63.39%   [EVAL] batch:   91 | acc: 6.25%,  total acc: 62.77%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 62.16%   [EVAL] batch:   93 | acc: 12.50%,  total acc: 61.64%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 61.78%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 62.04%   [EVAL] batch:   96 | acc: 75.00%,  total acc: 62.18%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 62.63%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 62.69%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 63.00%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 63.30%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 63.53%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 63.76%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 64.05%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 64.39%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 64.31%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 63.95%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 63.70%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 63.24%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 63.06%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 62.78%   [EVAL] batch:  112 | acc: 25.00%,  total acc: 62.44%   [EVAL] batch:  113 | acc: 31.25%,  total acc: 62.17%   [EVAL] batch:  114 | acc: 43.75%,  total acc: 62.01%   [EVAL] batch:  115 | acc: 68.75%,  total acc: 62.07%   [EVAL] batch:  116 | acc: 43.75%,  total acc: 61.91%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 61.92%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 61.87%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 61.77%   [EVAL] batch:  120 | acc: 37.50%,  total acc: 61.57%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 61.48%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 61.48%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 61.64%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 61.65%   [EVAL] batch:  125 | acc: 37.50%,  total acc: 61.46%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 61.22%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 60.99%   [EVAL] batch:  128 | acc: 12.50%,  total acc: 60.61%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 60.48%   [EVAL] batch:  130 | acc: 31.25%,  total acc: 60.26%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 60.42%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 60.71%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 60.96%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 61.16%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 61.44%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 61.68%   [EVAL] batch:  137 | acc: 37.50%,  total acc: 61.50%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 61.15%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 60.76%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 60.51%   [EVAL] batch:  141 | acc: 18.75%,  total acc: 60.21%   [EVAL] batch:  142 | acc: 25.00%,  total acc: 59.97%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 59.77%   [EVAL] batch:  144 | acc: 56.25%,  total acc: 59.74%   [EVAL] batch:  145 | acc: 50.00%,  total acc: 59.67%   [EVAL] batch:  146 | acc: 50.00%,  total acc: 59.61%   [EVAL] batch:  147 | acc: 56.25%,  total acc: 59.59%   [EVAL] batch:  148 | acc: 50.00%,  total acc: 59.52%   [EVAL] batch:  149 | acc: 62.50%,  total acc: 59.54%   [EVAL] batch:  150 | acc: 62.50%,  total acc: 59.56%   [EVAL] batch:  151 | acc: 68.75%,  total acc: 59.62%   [EVAL] batch:  152 | acc: 56.25%,  total acc: 59.60%   [EVAL] batch:  153 | acc: 50.00%,  total acc: 59.54%   [EVAL] batch:  154 | acc: 56.25%,  total acc: 59.52%   [EVAL] batch:  155 | acc: 56.25%,  total acc: 59.50%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 59.55%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 59.53%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 59.51%   [EVAL] batch:  159 | acc: 56.25%,  total acc: 59.49%   [EVAL] batch:  160 | acc: 75.00%,  total acc: 59.59%   [EVAL] batch:  161 | acc: 62.50%,  total acc: 59.61%   [EVAL] batch:  162 | acc: 75.00%,  total acc: 59.70%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 59.76%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 59.85%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 59.90%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 59.99%   [EVAL] batch:  167 | acc: 81.25%,  total acc: 60.12%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 60.13%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 59.93%   [EVAL] batch:  170 | acc: 25.00%,  total acc: 59.72%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 59.56%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 59.43%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 59.23%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 59.11%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 59.34%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 59.57%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 59.80%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 60.02%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 60.24%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 60.46%   [EVAL] batch:  181 | acc: 81.25%,  total acc: 60.58%   [EVAL] batch:  182 | acc: 43.75%,  total acc: 60.48%   [EVAL] batch:  183 | acc: 62.50%,  total acc: 60.50%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 60.54%   [EVAL] batch:  185 | acc: 62.50%,  total acc: 60.55%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 60.59%   [EVAL] batch:  187 | acc: 87.50%,  total acc: 60.74%   [EVAL] batch:  188 | acc: 75.00%,  total acc: 60.81%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 60.89%   [EVAL] batch:  190 | acc: 81.25%,  total acc: 60.99%   [EVAL] batch:  191 | acc: 75.00%,  total acc: 61.07%   [EVAL] batch:  192 | acc: 75.00%,  total acc: 61.14%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 61.24%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 61.35%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 61.42%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 61.45%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 61.58%   [EVAL] batch:  198 | acc: 81.25%,  total acc: 61.68%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 61.78%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 61.82%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 61.85%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 61.85%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 61.86%   [EVAL] batch:  204 | acc: 56.25%,  total acc: 61.83%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 61.89%   [EVAL] batch:  206 | acc: 25.00%,  total acc: 61.71%   [EVAL] batch:  207 | acc: 43.75%,  total acc: 61.63%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 61.45%   [EVAL] batch:  209 | acc: 18.75%,  total acc: 61.25%   [EVAL] batch:  210 | acc: 43.75%,  total acc: 61.17%   [EVAL] batch:  211 | acc: 50.00%,  total acc: 61.11%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 61.06%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 61.21%   [EVAL] batch:  214 | acc: 93.75%,  total acc: 61.37%   [EVAL] batch:  215 | acc: 56.25%,  total acc: 61.34%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 61.52%   [EVAL] batch:  217 | acc: 81.25%,  total acc: 61.61%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 61.76%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 61.93%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 62.10%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 62.27%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 62.44%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 62.61%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 62.78%   [EVAL] batch:  225 | acc: 93.75%,  total acc: 62.91%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 63.02%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 63.08%   [EVAL] batch:  228 | acc: 87.50%,  total acc: 63.18%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 63.32%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 63.39%   [EVAL] batch:  231 | acc: 43.75%,  total acc: 63.31%   [EVAL] batch:  232 | acc: 18.75%,  total acc: 63.12%   [EVAL] batch:  233 | acc: 31.25%,  total acc: 62.98%   [EVAL] batch:  234 | acc: 31.25%,  total acc: 62.85%   [EVAL] batch:  235 | acc: 50.00%,  total acc: 62.79%   [EVAL] batch:  236 | acc: 25.00%,  total acc: 62.63%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 62.63%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 62.74%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 62.89%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 62.99%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 63.09%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 63.22%   [EVAL] batch:  243 | acc: 62.50%,  total acc: 63.22%   [EVAL] batch:  244 | acc: 6.25%,  total acc: 62.98%   [EVAL] batch:  245 | acc: 6.25%,  total acc: 62.75%   [EVAL] batch:  246 | acc: 0.00%,  total acc: 62.50%   [EVAL] batch:  247 | acc: 0.00%,  total acc: 62.25%   [EVAL] batch:  248 | acc: 0.00%,  total acc: 62.00%   [EVAL] batch:  249 | acc: 12.50%,  total acc: 61.80%   [EVAL] batch:  250 | acc: 31.25%,  total acc: 61.68%   [EVAL] batch:  251 | acc: 56.25%,  total acc: 61.66%   [EVAL] batch:  252 | acc: 68.75%,  total acc: 61.68%   [EVAL] batch:  253 | acc: 56.25%,  total acc: 61.66%   [EVAL] batch:  254 | acc: 56.25%,  total acc: 61.64%   [EVAL] batch:  255 | acc: 75.00%,  total acc: 61.69%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 61.67%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 61.75%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 61.78%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 61.78%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 61.71%   [EVAL] batch:  261 | acc: 62.50%,  total acc: 61.71%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 61.79%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 61.77%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 61.72%   [EVAL] batch:  265 | acc: 50.00%,  total acc: 61.68%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 61.66%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 61.66%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 61.78%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 61.88%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 62.02%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 62.16%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 62.27%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 62.39%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 62.52%   [EVAL] batch:  275 | acc: 75.00%,  total acc: 62.57%   [EVAL] batch:  276 | acc: 75.00%,  total acc: 62.61%   [EVAL] batch:  277 | acc: 75.00%,  total acc: 62.66%   [EVAL] batch:  278 | acc: 68.75%,  total acc: 62.68%   [EVAL] batch:  279 | acc: 68.75%,  total acc: 62.70%   [EVAL] batch:  280 | acc: 75.00%,  total acc: 62.74%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 62.79%   [EVAL] batch:  282 | acc: 68.75%,  total acc: 62.81%   [EVAL] batch:  283 | acc: 12.50%,  total acc: 62.63%   [EVAL] batch:  284 | acc: 25.00%,  total acc: 62.50%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 62.35%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 62.13%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 62.15%   [EVAL] batch:  288 | acc: 75.00%,  total acc: 62.20%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 62.28%   [EVAL] batch:  290 | acc: 75.00%,  total acc: 62.33%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 62.44%   [EVAL] batch:  292 | acc: 62.50%,  total acc: 62.44%   [EVAL] batch:  293 | acc: 68.75%,  total acc: 62.46%   [EVAL] batch:  294 | acc: 68.75%,  total acc: 62.48%   [EVAL] batch:  295 | acc: 56.25%,  total acc: 62.46%   [EVAL] batch:  296 | acc: 43.75%,  total acc: 62.39%   [EVAL] batch:  297 | acc: 50.00%,  total acc: 62.35%   [EVAL] batch:  298 | acc: 62.50%,  total acc: 62.35%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 62.40%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 62.52%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 62.64%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 62.77%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 62.89%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 63.01%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 63.13%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 63.23%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 63.31%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 63.39%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 63.49%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 63.57%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 63.68%   [EVAL] batch:  312 | acc: 81.25%,  total acc: 63.74%   [EVAL] batch:  313 | acc: 50.00%,  total acc: 63.69%   [EVAL] batch:  314 | acc: 56.25%,  total acc: 63.67%   [EVAL] batch:  315 | acc: 87.50%,  total acc: 63.75%   [EVAL] batch:  316 | acc: 75.00%,  total acc: 63.78%   [EVAL] batch:  317 | acc: 75.00%,  total acc: 63.82%   [EVAL] batch:  318 | acc: 68.75%,  total acc: 63.83%   [EVAL] batch:  319 | acc: 68.75%,  total acc: 63.85%   [EVAL] batch:  320 | acc: 68.75%,  total acc: 63.86%   [EVAL] batch:  321 | acc: 56.25%,  total acc: 63.84%   [EVAL] batch:  322 | acc: 75.00%,  total acc: 63.87%   [EVAL] batch:  323 | acc: 68.75%,  total acc: 63.89%   [EVAL] batch:  324 | acc: 68.75%,  total acc: 63.90%   [EVAL] batch:  325 | acc: 12.50%,  total acc: 63.75%   [EVAL] batch:  326 | acc: 18.75%,  total acc: 63.61%   [EVAL] batch:  327 | acc: 25.00%,  total acc: 63.49%   [EVAL] batch:  328 | acc: 25.00%,  total acc: 63.37%   [EVAL] batch:  329 | acc: 25.00%,  total acc: 63.26%   [EVAL] batch:  330 | acc: 12.50%,  total acc: 63.10%   [EVAL] batch:  331 | acc: 68.75%,  total acc: 63.12%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 63.23%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 63.34%   [EVAL] batch:  334 | acc: 100.00%,  total acc: 63.45%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 63.56%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 63.65%   [EVAL] batch:  337 | acc: 68.75%,  total acc: 63.66%   [EVAL] batch:  338 | acc: 18.75%,  total acc: 63.53%   [EVAL] batch:  339 | acc: 31.25%,  total acc: 63.44%   [EVAL] batch:  340 | acc: 43.75%,  total acc: 63.38%   [EVAL] batch:  341 | acc: 43.75%,  total acc: 63.32%   [EVAL] batch:  342 | acc: 25.00%,  total acc: 63.21%   [EVAL] batch:  343 | acc: 43.75%,  total acc: 63.15%   [EVAL] batch:  344 | acc: 93.75%,  total acc: 63.24%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 63.33%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 63.40%   [EVAL] batch:  347 | acc: 87.50%,  total acc: 63.47%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 63.57%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 63.68%   [EVAL] batch:  350 | acc: 43.75%,  total acc: 63.62%   [EVAL] batch:  351 | acc: 50.00%,  total acc: 63.58%   [EVAL] batch:  352 | acc: 43.75%,  total acc: 63.53%   [EVAL] batch:  353 | acc: 31.25%,  total acc: 63.44%   [EVAL] batch:  354 | acc: 56.25%,  total acc: 63.42%   [EVAL] batch:  355 | acc: 43.75%,  total acc: 63.36%   [EVAL] batch:  356 | acc: 43.75%,  total acc: 63.31%   [EVAL] batch:  357 | acc: 50.00%,  total acc: 63.27%   [EVAL] batch:  358 | acc: 37.50%,  total acc: 63.20%   [EVAL] batch:  359 | acc: 43.75%,  total acc: 63.14%   [EVAL] batch:  360 | acc: 50.00%,  total acc: 63.11%   [EVAL] batch:  361 | acc: 43.75%,  total acc: 63.05%   [EVAL] batch:  362 | acc: 56.25%,  total acc: 63.03%   [EVAL] batch:  363 | acc: 50.00%,  total acc: 63.00%   [EVAL] batch:  364 | acc: 81.25%,  total acc: 63.05%   [EVAL] batch:  365 | acc: 75.00%,  total acc: 63.08%   [EVAL] batch:  366 | acc: 68.75%,  total acc: 63.10%   [EVAL] batch:  367 | acc: 75.00%,  total acc: 63.13%   [EVAL] batch:  368 | acc: 75.00%,  total acc: 63.16%   [EVAL] batch:  369 | acc: 81.25%,  total acc: 63.21%   [EVAL] batch:  370 | acc: 87.50%,  total acc: 63.27%   [EVAL] batch:  371 | acc: 87.50%,  total acc: 63.34%   [EVAL] batch:  372 | acc: 68.75%,  total acc: 63.35%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 63.40%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 63.45%   [EVAL] batch:  375 | acc: 93.75%,  total acc: 63.53%   [EVAL] batch:  376 | acc: 93.75%,  total acc: 63.61%   [EVAL] batch:  377 | acc: 93.75%,  total acc: 63.69%   [EVAL] batch:  378 | acc: 87.50%,  total acc: 63.75%   [EVAL] batch:  379 | acc: 93.75%,  total acc: 63.83%   [EVAL] batch:  380 | acc: 100.00%,  total acc: 63.93%   [EVAL] batch:  381 | acc: 68.75%,  total acc: 63.94%   [EVAL] batch:  382 | acc: 68.75%,  total acc: 63.95%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 63.96%   [EVAL] batch:  384 | acc: 75.00%,  total acc: 63.99%   [EVAL] batch:  385 | acc: 50.00%,  total acc: 63.96%   [EVAL] batch:  386 | acc: 68.75%,  total acc: 63.97%   [EVAL] batch:  387 | acc: 93.75%,  total acc: 64.05%   [EVAL] batch:  388 | acc: 81.25%,  total acc: 64.09%   [EVAL] batch:  389 | acc: 87.50%,  total acc: 64.15%   [EVAL] batch:  390 | acc: 81.25%,  total acc: 64.19%   [EVAL] batch:  391 | acc: 81.25%,  total acc: 64.24%   [EVAL] batch:  392 | acc: 81.25%,  total acc: 64.28%   [EVAL] batch:  393 | acc: 81.25%,  total acc: 64.32%   [EVAL] batch:  394 | acc: 75.00%,  total acc: 64.35%   [EVAL] batch:  395 | acc: 62.50%,  total acc: 64.35%   [EVAL] batch:  396 | acc: 81.25%,  total acc: 64.39%   [EVAL] batch:  397 | acc: 87.50%,  total acc: 64.45%   [EVAL] batch:  398 | acc: 68.75%,  total acc: 64.46%   [EVAL] batch:  399 | acc: 62.50%,  total acc: 64.45%   [EVAL] batch:  400 | acc: 100.00%,  total acc: 64.54%   [EVAL] batch:  401 | acc: 100.00%,  total acc: 64.63%   [EVAL] batch:  402 | acc: 93.75%,  total acc: 64.70%   [EVAL] batch:  403 | acc: 100.00%,  total acc: 64.79%   [EVAL] batch:  404 | acc: 100.00%,  total acc: 64.88%   [EVAL] batch:  405 | acc: 100.00%,  total acc: 64.96%   [EVAL] batch:  406 | acc: 81.25%,  total acc: 65.00%   [EVAL] batch:  407 | acc: 68.75%,  total acc: 65.01%   [EVAL] batch:  408 | acc: 68.75%,  total acc: 65.02%   [EVAL] batch:  409 | acc: 75.00%,  total acc: 65.05%   [EVAL] batch:  410 | acc: 75.00%,  total acc: 65.07%   [EVAL] batch:  411 | acc: 62.50%,  total acc: 65.06%   [EVAL] batch:  412 | acc: 68.75%,  total acc: 65.07%   [EVAL] batch:  413 | acc: 100.00%,  total acc: 65.16%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 65.24%   [EVAL] batch:  415 | acc: 100.00%,  total acc: 65.32%   [EVAL] batch:  416 | acc: 100.00%,  total acc: 65.41%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 65.49%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 65.56%   [EVAL] batch:  419 | acc: 100.00%,  total acc: 65.64%   [EVAL] batch:  420 | acc: 100.00%,  total acc: 65.72%   [EVAL] batch:  421 | acc: 100.00%,  total acc: 65.80%   [EVAL] batch:  422 | acc: 100.00%,  total acc: 65.88%   [EVAL] batch:  423 | acc: 100.00%,  total acc: 65.96%   [EVAL] batch:  424 | acc: 100.00%,  total acc: 66.04%   [EVAL] batch:  425 | acc: 87.50%,  total acc: 66.09%   [EVAL] batch:  426 | acc: 93.75%,  total acc: 66.16%   [EVAL] batch:  427 | acc: 93.75%,  total acc: 66.22%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:  429 | acc: 81.25%,  total acc: 66.34%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 66.42%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 66.48%   [EVAL] batch:  432 | acc: 93.75%,  total acc: 66.54%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 66.60%   [EVAL] batch:  434 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:  435 | acc: 100.00%,  total acc: 66.74%   [EVAL] batch:  436 | acc: 87.50%,  total acc: 66.79%   [EVAL] batch:  437 | acc: 62.50%,  total acc: 66.78%   [EVAL] batch:  438 | acc: 50.00%,  total acc: 66.74%   [EVAL] batch:  439 | acc: 37.50%,  total acc: 66.68%   [EVAL] batch:  440 | acc: 25.00%,  total acc: 66.58%   [EVAL] batch:  441 | acc: 43.75%,  total acc: 66.53%   [EVAL] batch:  442 | acc: 50.00%,  total acc: 66.49%   [EVAL] batch:  443 | acc: 50.00%,  total acc: 66.46%   [EVAL] batch:  444 | acc: 87.50%,  total acc: 66.50%   [EVAL] batch:  445 | acc: 75.00%,  total acc: 66.52%   [EVAL] batch:  446 | acc: 81.25%,  total acc: 66.55%   [EVAL] batch:  447 | acc: 87.50%,  total acc: 66.60%   [EVAL] batch:  448 | acc: 75.00%,  total acc: 66.62%   [EVAL] batch:  449 | acc: 81.25%,  total acc: 66.65%   [EVAL] batch:  450 | acc: 68.75%,  total acc: 66.66%   [EVAL] batch:  451 | acc: 50.00%,  total acc: 66.62%   [EVAL] batch:  452 | acc: 62.50%,  total acc: 66.61%   [EVAL] batch:  453 | acc: 56.25%,  total acc: 66.59%   [EVAL] batch:  454 | acc: 75.00%,  total acc: 66.61%   [EVAL] batch:  455 | acc: 50.00%,  total acc: 66.57%   [EVAL] batch:  456 | acc: 100.00%,  total acc: 66.64%   [EVAL] batch:  457 | acc: 100.00%,  total acc: 66.72%   [EVAL] batch:  458 | acc: 100.00%,  total acc: 66.79%   [EVAL] batch:  459 | acc: 87.50%,  total acc: 66.83%   [EVAL] batch:  460 | acc: 93.75%,  total acc: 66.89%   [EVAL] batch:  461 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:  462 | acc: 93.75%,  total acc: 67.02%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 67.09%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 67.16%   [EVAL] batch:  465 | acc: 100.00%,  total acc: 67.23%   [EVAL] batch:  466 | acc: 81.25%,  total acc: 67.26%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 67.33%   [EVAL] batch:  468 | acc: 93.75%,  total acc: 67.39%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 67.46%   [EVAL] batch:  470 | acc: 100.00%,  total acc: 67.53%   [EVAL] batch:  471 | acc: 100.00%,  total acc: 67.60%   [EVAL] batch:  472 | acc: 93.75%,  total acc: 67.65%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 67.72%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 67.79%   [EVAL] batch:  475 | acc: 56.25%,  total acc: 67.77%   [EVAL] batch:  476 | acc: 37.50%,  total acc: 67.70%   [EVAL] batch:  477 | acc: 68.75%,  total acc: 67.70%   [EVAL] batch:  478 | acc: 43.75%,  total acc: 67.65%   [EVAL] batch:  479 | acc: 50.00%,  total acc: 67.62%   [EVAL] batch:  480 | acc: 56.25%,  total acc: 67.59%   [EVAL] batch:  481 | acc: 81.25%,  total acc: 67.62%   [EVAL] batch:  482 | acc: 87.50%,  total acc: 67.66%   [EVAL] batch:  483 | acc: 87.50%,  total acc: 67.70%   [EVAL] batch:  484 | acc: 87.50%,  total acc: 67.74%   [EVAL] batch:  485 | acc: 68.75%,  total acc: 67.75%   [EVAL] batch:  486 | acc: 62.50%,  total acc: 67.74%   [EVAL] batch:  487 | acc: 81.25%,  total acc: 67.76%   [EVAL] batch:  488 | acc: 81.25%,  total acc: 67.79%   [EVAL] batch:  489 | acc: 81.25%,  total acc: 67.82%   [EVAL] batch:  490 | acc: 62.50%,  total acc: 67.81%   [EVAL] batch:  491 | acc: 87.50%,  total acc: 67.85%   [EVAL] batch:  492 | acc: 56.25%,  total acc: 67.82%   [EVAL] batch:  493 | acc: 43.75%,  total acc: 67.78%   [EVAL] batch:  494 | acc: 81.25%,  total acc: 67.80%   [EVAL] batch:  495 | acc: 75.00%,  total acc: 67.82%   [EVAL] batch:  496 | acc: 56.25%,  total acc: 67.79%   [EVAL] batch:  497 | acc: 93.75%,  total acc: 67.85%   [EVAL] batch:  498 | acc: 100.00%,  total acc: 67.91%   [EVAL] batch:  499 | acc: 93.75%,  total acc: 67.96%   
cur_acc:  ['0.9524', '0.7014', '0.7669', '0.8194', '0.7817', '0.6984', '0.8849', '0.7530']
his_acc:  ['0.9524', '0.8165', '0.7769', '0.7630', '0.7224', '0.6840', '0.7038', '0.6796']
--------Round  2
seed:  300
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 12.6884108CurrentTrain: epoch  0, batch     1 | loss: 12.4289303CurrentTrain: epoch  0, batch     2 | loss: 12.1151237CurrentTrain: epoch  0, batch     3 | loss: 11.8087683CurrentTrain: epoch  0, batch     4 | loss: 11.8375301CurrentTrain: epoch  0, batch     5 | loss: 11.6965752CurrentTrain: epoch  0, batch     6 | loss: 11.2426109CurrentTrain: epoch  0, batch     7 | loss: 11.6199493CurrentTrain: epoch  0, batch     8 | loss: 10.7834091CurrentTrain: epoch  0, batch     9 | loss: 11.1937389CurrentTrain: epoch  0, batch    10 | loss: 10.9780884CurrentTrain: epoch  0, batch    11 | loss: 10.9008741CurrentTrain: epoch  0, batch    12 | loss: 10.4788275CurrentTrain: epoch  0, batch    13 | loss: 10.6632290CurrentTrain: epoch  0, batch    14 | loss: 9.8150768CurrentTrain: epoch  0, batch    15 | loss: 9.8410749CurrentTrain: epoch  0, batch    16 | loss: 10.5289984CurrentTrain: epoch  0, batch    17 | loss: 9.5547419CurrentTrain: epoch  0, batch    18 | loss: 10.3292427CurrentTrain: epoch  0, batch    19 | loss: 9.9996262CurrentTrain: epoch  0, batch    20 | loss: 10.3083973CurrentTrain: epoch  0, batch    21 | loss: 9.5058622CurrentTrain: epoch  0, batch    22 | loss: 10.7304554CurrentTrain: epoch  0, batch    23 | loss: 10.0064125CurrentTrain: epoch  0, batch    24 | loss: 9.6329107CurrentTrain: epoch  0, batch    25 | loss: 10.1687527CurrentTrain: epoch  0, batch    26 | loss: 9.3909874CurrentTrain: epoch  0, batch    27 | loss: 9.5197067CurrentTrain: epoch  0, batch    28 | loss: 9.0230284CurrentTrain: epoch  0, batch    29 | loss: 9.8265934CurrentTrain: epoch  0, batch    30 | loss: 9.2127018CurrentTrain: epoch  0, batch    31 | loss: 9.8735943CurrentTrain: epoch  0, batch    32 | loss: 9.0472164CurrentTrain: epoch  0, batch    33 | loss: 9.7302370CurrentTrain: epoch  0, batch    34 | loss: 9.2600689CurrentTrain: epoch  0, batch    35 | loss: 8.4607449CurrentTrain: epoch  0, batch    36 | loss: 9.2994175CurrentTrain: epoch  0, batch    37 | loss: 9.2901993CurrentTrain: epoch  0, batch    38 | loss: 9.7374058CurrentTrain: epoch  0, batch    39 | loss: 8.9456921CurrentTrain: epoch  0, batch    40 | loss: 9.0218620CurrentTrain: epoch  0, batch    41 | loss: 9.1381330CurrentTrain: epoch  0, batch    42 | loss: 8.9280157CurrentTrain: epoch  0, batch    43 | loss: 9.4225826CurrentTrain: epoch  0, batch    44 | loss: 9.1283655CurrentTrain: epoch  0, batch    45 | loss: 8.6022091CurrentTrain: epoch  0, batch    46 | loss: 8.5907631CurrentTrain: epoch  0, batch    47 | loss: 8.6454973CurrentTrain: epoch  0, batch    48 | loss: 7.9877720CurrentTrain: epoch  0, batch    49 | loss: 7.9744444CurrentTrain: epoch  0, batch    50 | loss: 9.1940823CurrentTrain: epoch  0, batch    51 | loss: 8.9162350CurrentTrain: epoch  0, batch    52 | loss: 8.7337694CurrentTrain: epoch  0, batch    53 | loss: 8.3344851CurrentTrain: epoch  0, batch    54 | loss: 8.4000912CurrentTrain: epoch  0, batch    55 | loss: 7.6745076CurrentTrain: epoch  0, batch    56 | loss: 7.9673538CurrentTrain: epoch  0, batch    57 | loss: 7.8588052CurrentTrain: epoch  0, batch    58 | loss: 7.2883348CurrentTrain: epoch  0, batch    59 | loss: 7.7891560CurrentTrain: epoch  0, batch    60 | loss: 7.2775831CurrentTrain: epoch  0, batch    61 | loss: 7.8866291CurrentTrain: epoch  0, batch    62 | loss: 8.4995975CurrentTrain: epoch  1, batch     0 | loss: 7.8401546CurrentTrain: epoch  1, batch     1 | loss: 8.2731953CurrentTrain: epoch  1, batch     2 | loss: 7.1263905CurrentTrain: epoch  1, batch     3 | loss: 7.8824415CurrentTrain: epoch  1, batch     4 | loss: 7.4639254CurrentTrain: epoch  1, batch     5 | loss: 7.6850276CurrentTrain: epoch  1, batch     6 | loss: 6.8938103CurrentTrain: epoch  1, batch     7 | loss: 6.7788115CurrentTrain: epoch  1, batch     8 | loss: 7.5191393CurrentTrain: epoch  1, batch     9 | loss: 7.2314653CurrentTrain: epoch  1, batch    10 | loss: 7.3431005CurrentTrain: epoch  1, batch    11 | loss: 7.1780229CurrentTrain: epoch  1, batch    12 | loss: 7.6286249CurrentTrain: epoch  1, batch    13 | loss: 7.2273822CurrentTrain: epoch  1, batch    14 | loss: 7.0727758CurrentTrain: epoch  1, batch    15 | loss: 8.3503494CurrentTrain: epoch  1, batch    16 | loss: 6.5557470CurrentTrain: epoch  1, batch    17 | loss: 6.9065223CurrentTrain: epoch  1, batch    18 | loss: 7.1831579CurrentTrain: epoch  1, batch    19 | loss: 6.8959074CurrentTrain: epoch  1, batch    20 | loss: 6.6886916CurrentTrain: epoch  1, batch    21 | loss: 6.5122113CurrentTrain: epoch  1, batch    22 | loss: 7.0743732CurrentTrain: epoch  1, batch    23 | loss: 7.4733648CurrentTrain: epoch  1, batch    24 | loss: 6.8244629CurrentTrain: epoch  1, batch    25 | loss: 7.2689123CurrentTrain: epoch  1, batch    26 | loss: 6.8854508CurrentTrain: epoch  1, batch    27 | loss: 6.8730793CurrentTrain: epoch  1, batch    28 | loss: 6.6313877CurrentTrain: epoch  1, batch    29 | loss: 8.1633053CurrentTrain: epoch  1, batch    30 | loss: 7.5909090CurrentTrain: epoch  1, batch    31 | loss: 6.7711315CurrentTrain: epoch  1, batch    32 | loss: 7.6625767CurrentTrain: epoch  1, batch    33 | loss: 6.8576832CurrentTrain: epoch  1, batch    34 | loss: 7.2181988CurrentTrain: epoch  1, batch    35 | loss: 6.4763818CurrentTrain: epoch  1, batch    36 | loss: 7.5732627CurrentTrain: epoch  1, batch    37 | loss: 6.5391235CurrentTrain: epoch  1, batch    38 | loss: 7.2799544CurrentTrain: epoch  1, batch    39 | loss: 6.9453707CurrentTrain: epoch  1, batch    40 | loss: 7.3498554CurrentTrain: epoch  1, batch    41 | loss: 6.1527271CurrentTrain: epoch  1, batch    42 | loss: 6.2014418CurrentTrain: epoch  1, batch    43 | loss: 6.5174093CurrentTrain: epoch  1, batch    44 | loss: 6.0079260CurrentTrain: epoch  1, batch    45 | loss: 6.1063786CurrentTrain: epoch  1, batch    46 | loss: 6.9778037CurrentTrain: epoch  1, batch    47 | loss: 6.4588089CurrentTrain: epoch  1, batch    48 | loss: 6.8520112CurrentTrain: epoch  1, batch    49 | loss: 6.1340418CurrentTrain: epoch  1, batch    50 | loss: 6.5633593CurrentTrain: epoch  1, batch    51 | loss: 6.5253024CurrentTrain: epoch  1, batch    52 | loss: 5.7115402CurrentTrain: epoch  1, batch    53 | loss: 6.4170709CurrentTrain: epoch  1, batch    54 | loss: 6.4494638CurrentTrain: epoch  1, batch    55 | loss: 6.1435280CurrentTrain: epoch  1, batch    56 | loss: 6.2745419CurrentTrain: epoch  1, batch    57 | loss: 6.1792374CurrentTrain: epoch  1, batch    58 | loss: 5.4586005CurrentTrain: epoch  1, batch    59 | loss: 6.2567654CurrentTrain: epoch  1, batch    60 | loss: 5.5238771CurrentTrain: epoch  1, batch    61 | loss: 5.9574747CurrentTrain: epoch  1, batch    62 | loss: 6.0107470CurrentTrain: epoch  2, batch     0 | loss: 6.5355673CurrentTrain: epoch  2, batch     1 | loss: 5.6478896CurrentTrain: epoch  2, batch     2 | loss: 6.7580194CurrentTrain: epoch  2, batch     3 | loss: 5.8853531CurrentTrain: epoch  2, batch     4 | loss: 5.4626188CurrentTrain: epoch  2, batch     5 | loss: 5.3438091CurrentTrain: epoch  2, batch     6 | loss: 5.1414747CurrentTrain: epoch  2, batch     7 | loss: 5.6652207CurrentTrain: epoch  2, batch     8 | loss: 6.3372364CurrentTrain: epoch  2, batch     9 | loss: 5.5483208CurrentTrain: epoch  2, batch    10 | loss: 5.7714868CurrentTrain: epoch  2, batch    11 | loss: 6.2014351CurrentTrain: epoch  2, batch    12 | loss: 6.0380445CurrentTrain: epoch  2, batch    13 | loss: 5.4996195CurrentTrain: epoch  2, batch    14 | loss: 5.5780826CurrentTrain: epoch  2, batch    15 | loss: 6.4930544CurrentTrain: epoch  2, batch    16 | loss: 5.7826977CurrentTrain: epoch  2, batch    17 | loss: 5.7257442CurrentTrain: epoch  2, batch    18 | loss: 5.5779834CurrentTrain: epoch  2, batch    19 | loss: 6.1305962CurrentTrain: epoch  2, batch    20 | loss: 5.1111584CurrentTrain: epoch  2, batch    21 | loss: 5.8811078CurrentTrain: epoch  2, batch    22 | loss: 5.5745897CurrentTrain: epoch  2, batch    23 | loss: 5.8886709CurrentTrain: epoch  2, batch    24 | loss: 6.3517418CurrentTrain: epoch  2, batch    25 | loss: 5.7269783CurrentTrain: epoch  2, batch    26 | loss: 6.0993028CurrentTrain: epoch  2, batch    27 | loss: 5.9576015CurrentTrain: epoch  2, batch    28 | loss: 5.4566083CurrentTrain: epoch  2, batch    29 | loss: 5.8990741CurrentTrain: epoch  2, batch    30 | loss: 5.8468904CurrentTrain: epoch  2, batch    31 | loss: 5.5905886CurrentTrain: epoch  2, batch    32 | loss: 5.8749523CurrentTrain: epoch  2, batch    33 | loss: 5.2495375CurrentTrain: epoch  2, batch    34 | loss: 5.2643280CurrentTrain: epoch  2, batch    35 | loss: 5.5254045CurrentTrain: epoch  2, batch    36 | loss: 6.3812685CurrentTrain: epoch  2, batch    37 | loss: 5.3475766CurrentTrain: epoch  2, batch    38 | loss: 7.2666254CurrentTrain: epoch  2, batch    39 | loss: 5.9373188CurrentTrain: epoch  2, batch    40 | loss: 5.2774363CurrentTrain: epoch  2, batch    41 | loss: 6.0494828CurrentTrain: epoch  2, batch    42 | loss: 5.5670352CurrentTrain: epoch  2, batch    43 | loss: 6.3306222CurrentTrain: epoch  2, batch    44 | loss: 5.4667034CurrentTrain: epoch  2, batch    45 | loss: 5.3514357CurrentTrain: epoch  2, batch    46 | loss: 5.4420152CurrentTrain: epoch  2, batch    47 | loss: 5.3618126CurrentTrain: epoch  2, batch    48 | loss: 5.8226566CurrentTrain: epoch  2, batch    49 | loss: 5.2324991CurrentTrain: epoch  2, batch    50 | loss: 5.8415098CurrentTrain: epoch  2, batch    51 | loss: 5.3405399CurrentTrain: epoch  2, batch    52 | loss: 5.5411625CurrentTrain: epoch  2, batch    53 | loss: 5.6592712CurrentTrain: epoch  2, batch    54 | loss: 5.6832409CurrentTrain: epoch  2, batch    55 | loss: 5.6238079CurrentTrain: epoch  2, batch    56 | loss: 5.1962910CurrentTrain: epoch  2, batch    57 | loss: 5.9728065CurrentTrain: epoch  2, batch    58 | loss: 4.9618249CurrentTrain: epoch  2, batch    59 | loss: 5.2240381CurrentTrain: epoch  2, batch    60 | loss: 5.2248387CurrentTrain: epoch  2, batch    61 | loss: 5.2983918CurrentTrain: epoch  2, batch    62 | loss: 5.9629526CurrentTrain: epoch  3, batch     0 | loss: 4.9390874CurrentTrain: epoch  3, batch     1 | loss: 5.7563686CurrentTrain: epoch  3, batch     2 | loss: 5.5104876CurrentTrain: epoch  3, batch     3 | loss: 5.3559361CurrentTrain: epoch  3, batch     4 | loss: 4.9498253CurrentTrain: epoch  3, batch     5 | loss: 5.0043049CurrentTrain: epoch  3, batch     6 | loss: 5.1208944CurrentTrain: epoch  3, batch     7 | loss: 5.2022085CurrentTrain: epoch  3, batch     8 | loss: 5.1416559CurrentTrain: epoch  3, batch     9 | loss: 5.0194974CurrentTrain: epoch  3, batch    10 | loss: 5.6624465CurrentTrain: epoch  3, batch    11 | loss: 5.4094706CurrentTrain: epoch  3, batch    12 | loss: 4.8862429CurrentTrain: epoch  3, batch    13 | loss: 5.3540277CurrentTrain: epoch  3, batch    14 | loss: 5.2840080CurrentTrain: epoch  3, batch    15 | loss: 5.5186720CurrentTrain: epoch  3, batch    16 | loss: 5.4012733CurrentTrain: epoch  3, batch    17 | loss: 4.9987016CurrentTrain: epoch  3, batch    18 | loss: 5.0492878CurrentTrain: epoch  3, batch    19 | loss: 5.4864864CurrentTrain: epoch  3, batch    20 | loss: 5.2703605CurrentTrain: epoch  3, batch    21 | loss: 4.8890076CurrentTrain: epoch  3, batch    22 | loss: 5.0972219CurrentTrain: epoch  3, batch    23 | loss: 5.4290152CurrentTrain: epoch  3, batch    24 | loss: 5.1487508CurrentTrain: epoch  3, batch    25 | loss: 4.6462297CurrentTrain: epoch  3, batch    26 | loss: 4.7465391CurrentTrain: epoch  3, batch    27 | loss: 5.1358500CurrentTrain: epoch  3, batch    28 | loss: 5.0547581CurrentTrain: epoch  3, batch    29 | loss: 5.3082018CurrentTrain: epoch  3, batch    30 | loss: 4.9486771CurrentTrain: epoch  3, batch    31 | loss: 4.6967902CurrentTrain: epoch  3, batch    32 | loss: 5.0263662CurrentTrain: epoch  3, batch    33 | loss: 4.9603691CurrentTrain: epoch  3, batch    34 | loss: 4.6737561CurrentTrain: epoch  3, batch    35 | loss: 5.2393188CurrentTrain: epoch  3, batch    36 | loss: 4.8865213CurrentTrain: epoch  3, batch    37 | loss: 5.6139574CurrentTrain: epoch  3, batch    38 | loss: 4.7174492CurrentTrain: epoch  3, batch    39 | loss: 4.7288055CurrentTrain: epoch  3, batch    40 | loss: 4.5724092CurrentTrain: epoch  3, batch    41 | loss: 4.6850328CurrentTrain: epoch  3, batch    42 | loss: 5.3989410CurrentTrain: epoch  3, batch    43 | loss: 5.0569973CurrentTrain: epoch  3, batch    44 | loss: 5.3813348CurrentTrain: epoch  3, batch    45 | loss: 6.1816969CurrentTrain: epoch  3, batch    46 | loss: 4.7707553CurrentTrain: epoch  3, batch    47 | loss: 5.3369408CurrentTrain: epoch  3, batch    48 | loss: 4.8036385CurrentTrain: epoch  3, batch    49 | loss: 4.6687775CurrentTrain: epoch  3, batch    50 | loss: 5.0778828CurrentTrain: epoch  3, batch    51 | loss: 4.7305794CurrentTrain: epoch  3, batch    52 | loss: 5.3861709CurrentTrain: epoch  3, batch    53 | loss: 5.0010061CurrentTrain: epoch  3, batch    54 | loss: 4.9693718CurrentTrain: epoch  3, batch    55 | loss: 4.6532698CurrentTrain: epoch  3, batch    56 | loss: 4.7229500CurrentTrain: epoch  3, batch    57 | loss: 4.6820116CurrentTrain: epoch  3, batch    58 | loss: 4.6909046CurrentTrain: epoch  3, batch    59 | loss: 4.6788111CurrentTrain: epoch  3, batch    60 | loss: 4.6856947CurrentTrain: epoch  3, batch    61 | loss: 4.6288614CurrentTrain: epoch  3, batch    62 | loss: 4.7980776CurrentTrain: epoch  4, batch     0 | loss: 4.5990648CurrentTrain: epoch  4, batch     1 | loss: 4.7319031CurrentTrain: epoch  4, batch     2 | loss: 4.7721062CurrentTrain: epoch  4, batch     3 | loss: 4.6291246CurrentTrain: epoch  4, batch     4 | loss: 4.8119707CurrentTrain: epoch  4, batch     5 | loss: 4.6985006CurrentTrain: epoch  4, batch     6 | loss: 4.8693161CurrentTrain: epoch  4, batch     7 | loss: 4.9615278CurrentTrain: epoch  4, batch     8 | loss: 4.7944379CurrentTrain: epoch  4, batch     9 | loss: 4.8789506CurrentTrain: epoch  4, batch    10 | loss: 4.5960259CurrentTrain: epoch  4, batch    11 | loss: 4.5128183CurrentTrain: epoch  4, batch    12 | loss: 4.5772867CurrentTrain: epoch  4, batch    13 | loss: 4.5460076CurrentTrain: epoch  4, batch    14 | loss: 4.5335751CurrentTrain: epoch  4, batch    15 | loss: 5.1169863CurrentTrain: epoch  4, batch    16 | loss: 4.5996914CurrentTrain: epoch  4, batch    17 | loss: 4.6171756CurrentTrain: epoch  4, batch    18 | loss: 4.5289736CurrentTrain: epoch  4, batch    19 | loss: 4.6708946CurrentTrain: epoch  4, batch    20 | loss: 4.4628220CurrentTrain: epoch  4, batch    21 | loss: 4.5895457CurrentTrain: epoch  4, batch    22 | loss: 4.9976306CurrentTrain: epoch  4, batch    23 | loss: 4.5953922CurrentTrain: epoch  4, batch    24 | loss: 4.4925385CurrentTrain: epoch  4, batch    25 | loss: 4.8571396CurrentTrain: epoch  4, batch    26 | loss: 4.8695560CurrentTrain: epoch  4, batch    27 | loss: 4.7848282CurrentTrain: epoch  4, batch    28 | loss: 4.5258398CurrentTrain: epoch  4, batch    29 | loss: 4.5368257CurrentTrain: epoch  4, batch    30 | loss: 4.5158844CurrentTrain: epoch  4, batch    31 | loss: 4.3632660CurrentTrain: epoch  4, batch    32 | loss: 4.8834357CurrentTrain: epoch  4, batch    33 | loss: 4.4613376CurrentTrain: epoch  4, batch    34 | loss: 4.5559340CurrentTrain: epoch  4, batch    35 | loss: 4.5734138CurrentTrain: epoch  4, batch    36 | loss: 4.6226730CurrentTrain: epoch  4, batch    37 | loss: 4.5454173CurrentTrain: epoch  4, batch    38 | loss: 4.5018539CurrentTrain: epoch  4, batch    39 | loss: 4.4248047CurrentTrain: epoch  4, batch    40 | loss: 4.5148468CurrentTrain: epoch  4, batch    41 | loss: 4.8425994CurrentTrain: epoch  4, batch    42 | loss: 4.4986634CurrentTrain: epoch  4, batch    43 | loss: 4.7290707CurrentTrain: epoch  4, batch    44 | loss: 4.3457141CurrentTrain: epoch  4, batch    45 | loss: 4.4758663CurrentTrain: epoch  4, batch    46 | loss: 5.3828340CurrentTrain: epoch  4, batch    47 | loss: 4.4545431CurrentTrain: epoch  4, batch    48 | loss: 4.2503681CurrentTrain: epoch  4, batch    49 | loss: 4.4722614CurrentTrain: epoch  4, batch    50 | loss: 4.4531980CurrentTrain: epoch  4, batch    51 | loss: 4.8515048CurrentTrain: epoch  4, batch    52 | loss: 4.5384636CurrentTrain: epoch  4, batch    53 | loss: 4.3913164CurrentTrain: epoch  4, batch    54 | loss: 4.4544978CurrentTrain: epoch  4, batch    55 | loss: 4.9931893CurrentTrain: epoch  4, batch    56 | loss: 4.7006249CurrentTrain: epoch  4, batch    57 | loss: 4.2693272CurrentTrain: epoch  4, batch    58 | loss: 4.3557711CurrentTrain: epoch  4, batch    59 | loss: 4.3033800CurrentTrain: epoch  4, batch    60 | loss: 4.3969021CurrentTrain: epoch  4, batch    61 | loss: 4.3813248CurrentTrain: epoch  4, batch    62 | loss: 4.2836361CurrentTrain: epoch  5, batch     0 | loss: 4.3886518CurrentTrain: epoch  5, batch     1 | loss: 4.3535390CurrentTrain: epoch  5, batch     2 | loss: 4.4193974CurrentTrain: epoch  5, batch     3 | loss: 4.2579288CurrentTrain: epoch  5, batch     4 | loss: 4.4561830CurrentTrain: epoch  5, batch     5 | loss: 4.3716669CurrentTrain: epoch  5, batch     6 | loss: 4.3358541CurrentTrain: epoch  5, batch     7 | loss: 4.2988877CurrentTrain: epoch  5, batch     8 | loss: 4.3569984CurrentTrain: epoch  5, batch     9 | loss: 4.3971171CurrentTrain: epoch  5, batch    10 | loss: 4.7113962CurrentTrain: epoch  5, batch    11 | loss: 4.1754804CurrentTrain: epoch  5, batch    12 | loss: 4.2120953CurrentTrain: epoch  5, batch    13 | loss: 4.2727613CurrentTrain: epoch  5, batch    14 | loss: 4.3786840CurrentTrain: epoch  5, batch    15 | loss: 4.3461471CurrentTrain: epoch  5, batch    16 | loss: 4.3935719CurrentTrain: epoch  5, batch    17 | loss: 4.3101830CurrentTrain: epoch  5, batch    18 | loss: 4.2558918CurrentTrain: epoch  5, batch    19 | loss: 4.3965588CurrentTrain: epoch  5, batch    20 | loss: 4.3931255CurrentTrain: epoch  5, batch    21 | loss: 4.3517451CurrentTrain: epoch  5, batch    22 | loss: 4.3678327CurrentTrain: epoch  5, batch    23 | loss: 4.4305158CurrentTrain: epoch  5, batch    24 | loss: 4.3908310CurrentTrain: epoch  5, batch    25 | loss: 4.7263927CurrentTrain: epoch  5, batch    26 | loss: 4.3430939CurrentTrain: epoch  5, batch    27 | loss: 4.3270240CurrentTrain: epoch  5, batch    28 | loss: 4.3172789CurrentTrain: epoch  5, batch    29 | loss: 4.2615509CurrentTrain: epoch  5, batch    30 | loss: 4.3910465CurrentTrain: epoch  5, batch    31 | loss: 4.4208255CurrentTrain: epoch  5, batch    32 | loss: 4.2851362CurrentTrain: epoch  5, batch    33 | loss: 4.3951807CurrentTrain: epoch  5, batch    34 | loss: 4.3280973CurrentTrain: epoch  5, batch    35 | loss: 4.2380505CurrentTrain: epoch  5, batch    36 | loss: 4.5073156CurrentTrain: epoch  5, batch    37 | loss: 4.4452472CurrentTrain: epoch  5, batch    38 | loss: 4.4236660CurrentTrain: epoch  5, batch    39 | loss: 4.2914066CurrentTrain: epoch  5, batch    40 | loss: 4.3314877CurrentTrain: epoch  5, batch    41 | loss: 4.1551838CurrentTrain: epoch  5, batch    42 | loss: 4.2501879CurrentTrain: epoch  5, batch    43 | loss: 4.2554426CurrentTrain: epoch  5, batch    44 | loss: 4.2670979CurrentTrain: epoch  5, batch    45 | loss: 4.2388153CurrentTrain: epoch  5, batch    46 | loss: 4.2704039CurrentTrain: epoch  5, batch    47 | loss: 4.2707458CurrentTrain: epoch  5, batch    48 | loss: 4.2388134CurrentTrain: epoch  5, batch    49 | loss: 5.3205538CurrentTrain: epoch  5, batch    50 | loss: 4.2604675CurrentTrain: epoch  5, batch    51 | loss: 4.2002354CurrentTrain: epoch  5, batch    52 | loss: 4.8036399CurrentTrain: epoch  5, batch    53 | loss: 4.2585554CurrentTrain: epoch  5, batch    54 | loss: 4.3195734CurrentTrain: epoch  5, batch    55 | loss: 4.9351025CurrentTrain: epoch  5, batch    56 | loss: 4.1768274CurrentTrain: epoch  5, batch    57 | loss: 4.3530698CurrentTrain: epoch  5, batch    58 | loss: 4.3250875CurrentTrain: epoch  5, batch    59 | loss: 4.2868085CurrentTrain: epoch  5, batch    60 | loss: 4.4043355CurrentTrain: epoch  5, batch    61 | loss: 4.1839423CurrentTrain: epoch  5, batch    62 | loss: 4.1987972CurrentTrain: epoch  6, batch     0 | loss: 4.2445560CurrentTrain: epoch  6, batch     1 | loss: 4.3028569CurrentTrain: epoch  6, batch     2 | loss: 4.3974452CurrentTrain: epoch  6, batch     3 | loss: 4.1798954CurrentTrain: epoch  6, batch     4 | loss: 4.3459024CurrentTrain: epoch  6, batch     5 | loss: 4.2527299CurrentTrain: epoch  6, batch     6 | loss: 4.2899971CurrentTrain: epoch  6, batch     7 | loss: 4.3014984CurrentTrain: epoch  6, batch     8 | loss: 4.2373581CurrentTrain: epoch  6, batch     9 | loss: 4.4250569CurrentTrain: epoch  6, batch    10 | loss: 4.5382395CurrentTrain: epoch  6, batch    11 | loss: 4.1564279CurrentTrain: epoch  6, batch    12 | loss: 4.4144883CurrentTrain: epoch  6, batch    13 | loss: 4.2382030CurrentTrain: epoch  6, batch    14 | loss: 4.2096815CurrentTrain: epoch  6, batch    15 | loss: 4.3203201CurrentTrain: epoch  6, batch    16 | loss: 4.2537031CurrentTrain: epoch  6, batch    17 | loss: 4.2556095CurrentTrain: epoch  6, batch    18 | loss: 4.2271576CurrentTrain: epoch  6, batch    19 | loss: 4.2509847CurrentTrain: epoch  6, batch    20 | loss: 4.2600546CurrentTrain: epoch  6, batch    21 | loss: 4.2333126CurrentTrain: epoch  6, batch    22 | loss: 4.1066980CurrentTrain: epoch  6, batch    23 | loss: 4.2493277CurrentTrain: epoch  6, batch    24 | loss: 4.3919992CurrentTrain: epoch  6, batch    25 | loss: 4.2303276CurrentTrain: epoch  6, batch    26 | loss: 4.2906342CurrentTrain: epoch  6, batch    27 | loss: 4.2956858CurrentTrain: epoch  6, batch    28 | loss: 4.1024675CurrentTrain: epoch  6, batch    29 | loss: 4.2202492CurrentTrain: epoch  6, batch    30 | loss: 4.2412076CurrentTrain: epoch  6, batch    31 | loss: 4.3462939CurrentTrain: epoch  6, batch    32 | loss: 4.2698569CurrentTrain: epoch  6, batch    33 | loss: 4.2193155CurrentTrain: epoch  6, batch    34 | loss: 4.2566633CurrentTrain: epoch  6, batch    35 | loss: 4.3654957CurrentTrain: epoch  6, batch    36 | loss: 4.1627550CurrentTrain: epoch  6, batch    37 | loss: 4.2118735CurrentTrain: epoch  6, batch    38 | loss: 4.1912060CurrentTrain: epoch  6, batch    39 | loss: 4.1875901CurrentTrain: epoch  6, batch    40 | loss: 4.1740727CurrentTrain: epoch  6, batch    41 | loss: 4.1521196CurrentTrain: epoch  6, batch    42 | loss: 4.1130009CurrentTrain: epoch  6, batch    43 | loss: 4.1477222CurrentTrain: epoch  6, batch    44 | loss: 4.1859627CurrentTrain: epoch  6, batch    45 | loss: 4.1693745CurrentTrain: epoch  6, batch    46 | loss: 4.1586552CurrentTrain: epoch  6, batch    47 | loss: 4.2157784CurrentTrain: epoch  6, batch    48 | loss: 4.1939211CurrentTrain: epoch  6, batch    49 | loss: 4.1965547CurrentTrain: epoch  6, batch    50 | loss: 4.1734571CurrentTrain: epoch  6, batch    51 | loss: 4.1814489CurrentTrain: epoch  6, batch    52 | loss: 4.5523787CurrentTrain: epoch  6, batch    53 | loss: 4.1379991CurrentTrain: epoch  6, batch    54 | loss: 4.1586819CurrentTrain: epoch  6, batch    55 | loss: 4.3097014CurrentTrain: epoch  6, batch    56 | loss: 4.1986513CurrentTrain: epoch  6, batch    57 | loss: 4.1063366CurrentTrain: epoch  6, batch    58 | loss: 4.1598125CurrentTrain: epoch  6, batch    59 | loss: 4.1435595CurrentTrain: epoch  6, batch    60 | loss: 4.2338123CurrentTrain: epoch  6, batch    61 | loss: 4.1561871CurrentTrain: epoch  6, batch    62 | loss: 4.1816874CurrentTrain: epoch  7, batch     0 | loss: 4.0887856CurrentTrain: epoch  7, batch     1 | loss: 4.1214190CurrentTrain: epoch  7, batch     2 | loss: 4.1159105CurrentTrain: epoch  7, batch     3 | loss: 4.0997319CurrentTrain: epoch  7, batch     4 | loss: 4.0730476CurrentTrain: epoch  7, batch     5 | loss: 4.1277928CurrentTrain: epoch  7, batch     6 | loss: 4.0995250CurrentTrain: epoch  7, batch     7 | loss: 4.3361053CurrentTrain: epoch  7, batch     8 | loss: 4.1093841CurrentTrain: epoch  7, batch     9 | loss: 4.1835871CurrentTrain: epoch  7, batch    10 | loss: 4.1588516CurrentTrain: epoch  7, batch    11 | loss: 4.1621027CurrentTrain: epoch  7, batch    12 | loss: 4.0703392CurrentTrain: epoch  7, batch    13 | loss: 4.1391153CurrentTrain: epoch  7, batch    14 | loss: 4.0622339CurrentTrain: epoch  7, batch    15 | loss: 4.0847778CurrentTrain: epoch  7, batch    16 | loss: 4.1420302CurrentTrain: epoch  7, batch    17 | loss: 4.0913229CurrentTrain: epoch  7, batch    18 | loss: 4.1413236CurrentTrain: epoch  7, batch    19 | loss: 4.0922451CurrentTrain: epoch  7, batch    20 | loss: 4.1419511CurrentTrain: epoch  7, batch    21 | loss: 4.1325016CurrentTrain: epoch  7, batch    22 | loss: 4.1696301CurrentTrain: epoch  7, batch    23 | loss: 4.1253395CurrentTrain: epoch  7, batch    24 | loss: 4.2964387CurrentTrain: epoch  7, batch    25 | loss: 4.2847309CurrentTrain: epoch  7, batch    26 | loss: 4.2138128CurrentTrain: epoch  7, batch    27 | loss: 4.1964283CurrentTrain: epoch  7, batch    28 | loss: 4.1213989CurrentTrain: epoch  7, batch    29 | loss: 4.1972756CurrentTrain: epoch  7, batch    30 | loss: 4.1233883CurrentTrain: epoch  7, batch    31 | loss: 4.1066408CurrentTrain: epoch  7, batch    32 | loss: 4.1083918CurrentTrain: epoch  7, batch    33 | loss: 4.0454011CurrentTrain: epoch  7, batch    34 | loss: 4.0941215CurrentTrain: epoch  7, batch    35 | loss: 4.0376506CurrentTrain: epoch  7, batch    36 | loss: 4.1376381CurrentTrain: epoch  7, batch    37 | loss: 4.2024088CurrentTrain: epoch  7, batch    38 | loss: 4.0822067CurrentTrain: epoch  7, batch    39 | loss: 4.1013060CurrentTrain: epoch  7, batch    40 | loss: 4.0756598CurrentTrain: epoch  7, batch    41 | loss: 4.1538444CurrentTrain: epoch  7, batch    42 | loss: 4.1601534CurrentTrain: epoch  7, batch    43 | loss: 4.1410246CurrentTrain: epoch  7, batch    44 | loss: 4.1055107CurrentTrain: epoch  7, batch    45 | loss: 4.0832543CurrentTrain: epoch  7, batch    46 | loss: 4.1474743CurrentTrain: epoch  7, batch    47 | loss: 4.0245214CurrentTrain: epoch  7, batch    48 | loss: 4.1475210CurrentTrain: epoch  7, batch    49 | loss: 4.2022305CurrentTrain: epoch  7, batch    50 | loss: 4.0705209CurrentTrain: epoch  7, batch    51 | loss: 4.1136708CurrentTrain: epoch  7, batch    52 | loss: 4.1591635CurrentTrain: epoch  7, batch    53 | loss: 4.0711308CurrentTrain: epoch  7, batch    54 | loss: 4.1313300CurrentTrain: epoch  7, batch    55 | loss: 4.0357895CurrentTrain: epoch  7, batch    56 | loss: 4.1554737CurrentTrain: epoch  7, batch    57 | loss: 4.0728621CurrentTrain: epoch  7, batch    58 | loss: 4.0961447CurrentTrain: epoch  7, batch    59 | loss: 4.2574544CurrentTrain: epoch  7, batch    60 | loss: 4.0586519CurrentTrain: epoch  7, batch    61 | loss: 4.0707693CurrentTrain: epoch  7, batch    62 | loss: 4.0782585CurrentTrain: epoch  8, batch     0 | loss: 4.0613604CurrentTrain: epoch  8, batch     1 | loss: 4.1211905CurrentTrain: epoch  8, batch     2 | loss: 4.1081753CurrentTrain: epoch  8, batch     3 | loss: 4.0996065CurrentTrain: epoch  8, batch     4 | loss: 4.0802832CurrentTrain: epoch  8, batch     5 | loss: 4.2511396CurrentTrain: epoch  8, batch     6 | loss: 4.0778666CurrentTrain: epoch  8, batch     7 | loss: 4.1009617CurrentTrain: epoch  8, batch     8 | loss: 4.1422133CurrentTrain: epoch  8, batch     9 | loss: 4.0883560CurrentTrain: epoch  8, batch    10 | loss: 4.0295191CurrentTrain: epoch  8, batch    11 | loss: 4.0770350CurrentTrain: epoch  8, batch    12 | loss: 4.0621300CurrentTrain: epoch  8, batch    13 | loss: 4.0897493CurrentTrain: epoch  8, batch    14 | loss: 4.0604944CurrentTrain: epoch  8, batch    15 | loss: 4.0848722CurrentTrain: epoch  8, batch    16 | loss: 4.0812426CurrentTrain: epoch  8, batch    17 | loss: 4.0600104CurrentTrain: epoch  8, batch    18 | loss: 4.0761962CurrentTrain: epoch  8, batch    19 | loss: 4.0864525CurrentTrain: epoch  8, batch    20 | loss: 4.0824738CurrentTrain: epoch  8, batch    21 | loss: 4.0935020CurrentTrain: epoch  8, batch    22 | loss: 4.0396118CurrentTrain: epoch  8, batch    23 | loss: 4.1014366CurrentTrain: epoch  8, batch    24 | loss: 4.1124468CurrentTrain: epoch  8, batch    25 | loss: 4.1048136CurrentTrain: epoch  8, batch    26 | loss: 4.0695262CurrentTrain: epoch  8, batch    27 | loss: 4.0129223CurrentTrain: epoch  8, batch    28 | loss: 4.1167459CurrentTrain: epoch  8, batch    29 | loss: 4.0895624CurrentTrain: epoch  8, batch    30 | loss: 4.0507078CurrentTrain: epoch  8, batch    31 | loss: 4.0751948CurrentTrain: epoch  8, batch    32 | loss: 4.0586982CurrentTrain: epoch  8, batch    33 | loss: 4.0773935CurrentTrain: epoch  8, batch    34 | loss: 4.0507383CurrentTrain: epoch  8, batch    35 | loss: 4.0583825CurrentTrain: epoch  8, batch    36 | loss: 4.0982542CurrentTrain: epoch  8, batch    37 | loss: 4.0945358CurrentTrain: epoch  8, batch    38 | loss: 4.0911164CurrentTrain: epoch  8, batch    39 | loss: 4.0323114CurrentTrain: epoch  8, batch    40 | loss: 4.0107231CurrentTrain: epoch  8, batch    41 | loss: 4.0711946CurrentTrain: epoch  8, batch    42 | loss: 4.0379982CurrentTrain: epoch  8, batch    43 | loss: 4.0519114CurrentTrain: epoch  8, batch    44 | loss: 4.0280471CurrentTrain: epoch  8, batch    45 | loss: 4.0008144CurrentTrain: epoch  8, batch    46 | loss: 4.0841169CurrentTrain: epoch  8, batch    47 | loss: 4.0764818CurrentTrain: epoch  8, batch    48 | loss: 4.0597601CurrentTrain: epoch  8, batch    49 | loss: 4.0760803CurrentTrain: epoch  8, batch    50 | loss: 4.1003647CurrentTrain: epoch  8, batch    51 | loss: 4.0611582CurrentTrain: epoch  8, batch    52 | loss: 4.0319881CurrentTrain: epoch  8, batch    53 | loss: 4.0619135CurrentTrain: epoch  8, batch    54 | loss: 4.1168041CurrentTrain: epoch  8, batch    55 | loss: 4.0444508CurrentTrain: epoch  8, batch    56 | loss: 4.0562258CurrentTrain: epoch  8, batch    57 | loss: 4.1149330CurrentTrain: epoch  8, batch    58 | loss: 4.0562539CurrentTrain: epoch  8, batch    59 | loss: 4.0596275CurrentTrain: epoch  8, batch    60 | loss: 4.0395823CurrentTrain: epoch  8, batch    61 | loss: 4.0731516CurrentTrain: epoch  8, batch    62 | loss: 4.1070633CurrentTrain: epoch  9, batch     0 | loss: 4.0470920CurrentTrain: epoch  9, batch     1 | loss: 4.0520782CurrentTrain: epoch  9, batch     2 | loss: 4.0247340CurrentTrain: epoch  9, batch     3 | loss: 4.0544500CurrentTrain: epoch  9, batch     4 | loss: 4.0042534CurrentTrain: epoch  9, batch     5 | loss: 4.0145330CurrentTrain: epoch  9, batch     6 | loss: 4.0341311CurrentTrain: epoch  9, batch     7 | loss: 4.0124078CurrentTrain: epoch  9, batch     8 | loss: 4.0311322CurrentTrain: epoch  9, batch     9 | loss: 3.9972851CurrentTrain: epoch  9, batch    10 | loss: 4.0272570CurrentTrain: epoch  9, batch    11 | loss: 4.0617480CurrentTrain: epoch  9, batch    12 | loss: 4.1259413CurrentTrain: epoch  9, batch    13 | loss: 4.0285711CurrentTrain: epoch  9, batch    14 | loss: 4.1010661CurrentTrain: epoch  9, batch    15 | loss: 4.0278282CurrentTrain: epoch  9, batch    16 | loss: 4.0262022CurrentTrain: epoch  9, batch    17 | loss: 4.0338507CurrentTrain: epoch  9, batch    18 | loss: 4.0277119CurrentTrain: epoch  9, batch    19 | loss: 4.0692043CurrentTrain: epoch  9, batch    20 | loss: 4.1023989CurrentTrain: epoch  9, batch    21 | loss: 4.2160625CurrentTrain: epoch  9, batch    22 | loss: 4.0733628CurrentTrain: epoch  9, batch    23 | loss: 4.0157666CurrentTrain: epoch  9, batch    24 | loss: 4.0714254CurrentTrain: epoch  9, batch    25 | loss: 4.0893073CurrentTrain: epoch  9, batch    26 | loss: 4.0688372CurrentTrain: epoch  9, batch    27 | loss: 4.1171913CurrentTrain: epoch  9, batch    28 | loss: 4.0240870CurrentTrain: epoch  9, batch    29 | loss: 4.0129380CurrentTrain: epoch  9, batch    30 | loss: 4.0327067CurrentTrain: epoch  9, batch    31 | loss: 4.0543818CurrentTrain: epoch  9, batch    32 | loss: 4.0990496CurrentTrain: epoch  9, batch    33 | loss: 4.0255346CurrentTrain: epoch  9, batch    34 | loss: 4.0502176CurrentTrain: epoch  9, batch    35 | loss: 4.0810471CurrentTrain: epoch  9, batch    36 | loss: 4.1039844CurrentTrain: epoch  9, batch    37 | loss: 4.0473657CurrentTrain: epoch  9, batch    38 | loss: 4.0217981CurrentTrain: epoch  9, batch    39 | loss: 4.0506706CurrentTrain: epoch  9, batch    40 | loss: 4.0533466CurrentTrain: epoch  9, batch    41 | loss: 4.0360150CurrentTrain: epoch  9, batch    42 | loss: 4.0946660CurrentTrain: epoch  9, batch    43 | loss: 4.0385027CurrentTrain: epoch  9, batch    44 | loss: 4.0617561CurrentTrain: epoch  9, batch    45 | loss: 4.0083823CurrentTrain: epoch  9, batch    46 | loss: 4.0679903CurrentTrain: epoch  9, batch    47 | loss: 4.0151629CurrentTrain: epoch  9, batch    48 | loss: 4.2301531CurrentTrain: epoch  9, batch    49 | loss: 4.0835829CurrentTrain: epoch  9, batch    50 | loss: 4.0514255CurrentTrain: epoch  9, batch    51 | loss: 4.0161352CurrentTrain: epoch  9, batch    52 | loss: 4.0261936CurrentTrain: epoch  9, batch    53 | loss: 4.0703211CurrentTrain: epoch  9, batch    54 | loss: 4.0624175CurrentTrain: epoch  9, batch    55 | loss: 4.0747557CurrentTrain: epoch  9, batch    56 | loss: 4.0558338CurrentTrain: epoch  9, batch    57 | loss: 4.0343165CurrentTrain: epoch  9, batch    58 | loss: 4.0430183CurrentTrain: epoch  9, batch    59 | loss: 4.0594039CurrentTrain: epoch  9, batch    60 | loss: 4.0456562CurrentTrain: epoch  9, batch    61 | loss: 4.0571470CurrentTrain: epoch  9, batch    62 | loss: 4.1895447
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: born in copenhagen , as a young man bang became interested in surrealism as practised by giorgio de chirico , max ernst and yves tanguy .
Head Entity: yves tanguy
Tail Entity: surrealism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: during the early 20th century, the artist was heavily influenced by the cubist movement, which was pioneered by pablo picasso and georges braque.  
Head Entity: pablo picasso  
Tail Entity: cubist movement  

Relation: movement  
Context: the philosopher's ideas were deeply rooted in the existentialist movement, which gained prominence in the mid-20th century through the works of thinkers like jean-paul sartre.  
Head Entity: jean-paul sartre  
Tail Entity: existentialist movement  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: lyons hill is a restored village , and former parish with church , now part of the community of ardclough in north county kildare .
Head Entity: ardclough
Tail Entity: county kildare
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: The city of San Francisco is known for its iconic Golden Gate Bridge and is part of the larger San Francisco County in California.  
Head Entity: San Francisco  
Tail Entity: San Francisco County  

Relation: located in the administrative territorial entity  
Context: The village of Haverfordwest serves as the county town of Pembrokeshire, situated in the picturesque region of Wales.  
Head Entity: Haverfordwest  
Tail Entity: Pembrokeshire  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the nation as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: he also went on a road trip from san antonio to oakland for the magnum project “ postcards from america ” with jim goldberg , alec soth , mikhael subotzky and susan meiselas .
Head Entity: jim goldberg
Tail Entity: magnum
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: After years of dedication and hard work, she was finally recognized as a key member of the prestigious art collective known as the "Creative Minds."  
Head Entity: she  
Tail Entity: Creative Minds  

Relation: member of  
Context: The renowned guitarist joined the band last year, bringing his unique style to the group and becoming an integral member of the legendary rock ensemble, "The Soundwaves."  
Head Entity: guitarist  
Tail Entity: The Soundwaves  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: on july 2 , 1935 , the rip van winkle bridge over the hudson river between catskill and greenport was opened to traffic .
Head Entity: rip van winkle bridge
Tail Entity: hudson river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: The Golden Gate Bridge spans the Golden Gate Strait, connecting San Francisco to Marin County, and is a vital link for traffic crossing over the water.  
Head Entity: Golden Gate Bridge  
Tail Entity: Golden Gate Strait  

Relation: crosses  
Context: The Channel Tunnel, also known as the Chunnel, runs beneath the English Channel, providing a rail link between the UK and mainland Europe.  
Head Entity: Channel Tunnel  
Tail Entity: English Channel  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: the river flows through the teatree waterhole and comet towards its confluence with the nogoa river to form the mackenzie river .
Head Entity: mackenzie river
Tail Entity: nogoa river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The clear waters of the Missouri River merge with the Mississippi River, creating a vast network of waterways.  
Head Entity: Missouri River  
Tail Entity: Mississippi River  

Relation: tributary  
Context: As the sun sets, the small creek flows into the larger lake, providing a vital source of water for the surrounding ecosystem.  
Head Entity: creek  
Tail Entity: lake  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: after a review was conducted by former first minister of scotland henry mcleish , the sfa board structure was streamlined considerably in 2011 .
Head Entity: henry mcleish
Tail Entity: first minister of scotland
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: In 2020, Angela Merkel announced her decision to step down as Chancellor of Germany after serving for 16 years.  
Head Entity: Angela Merkel  
Tail Entity: Chancellor of Germany  

Relation: position held  
Context: The former president of the United States, Barack Obama, delivered a speech at the Democratic National Convention reflecting on his time in office.  
Head Entity: Barack Obama  
Tail Entity: President of the United States  
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: " jedi outcast " was developed by raven software and powered by the i d tech 3 game engine .
Head Entity: jedi outcast
Tail Entity: raven software
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: "The Witcher 3: Wild Hunt" was developed by CD Projekt Red and has received numerous awards for its storytelling and gameplay.  
Head Entity: The Witcher 3: Wild Hunt  
Tail Entity: CD Projekt Red  

Relation: developer  
Context: "Minecraft" was developed by Mojang Studios and has become one of the best-selling video games of all time.  
Head Entity: Minecraft  
Tail Entity: Mojang Studios  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: neptune is the second studio album by the london - based band the duke spirit and the last with the original lead guitarist , dan higgins .
Head Entity: the duke spirit
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the tech startup was founded in silicon valley by a group of innovative engineers looking to revolutionize the industry.  
Head Entity: the tech startup  
Tail Entity: silicon valley  

Relation: location of formation  
Context: the famous rock band was established in new york city, where they quickly gained a following and changed the music scene.  
Head Entity: the famous rock band  
Tail Entity: new york city  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: " shining time station " won a number of awards and significantly increased the popularity of the " thomas " media franchise in the united states .
Head Entity: shining time station
Tail Entity: united states
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous dish "sushi" is traditionally associated with Japan and has become popular worldwide.  
Head Entity: sushi  
Tail Entity: Japan  

Relation: country of origin  
Context: The iconic brand "Guinness" is known for its rich stout beer, which originated in Ireland.  
Head Entity: Guinness  
Tail Entity: Ireland  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.79%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 94.08%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 94.32%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 94.29%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.27%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.23%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.21%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.18%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.56%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.73%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.04%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.23%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.58%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.68%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.01%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.96%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.05%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.08%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.91%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.95%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 95.80%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.76%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.72%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.69%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.76%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.73%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.80%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.77%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.04%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.79%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 94.08%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 94.32%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 94.29%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.27%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.23%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.21%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.18%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.56%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.73%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.04%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.23%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.58%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.68%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.01%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.96%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.05%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.08%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.91%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.95%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 95.80%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.76%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.72%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.69%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.76%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.73%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.80%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.77%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.04%   
cur_acc:  ['0.9504']
his_acc:  ['0.9504']
CurrentTrain: epoch  0, batch     0 | loss: 8.0033112CurrentTrain: epoch  0, batch     1 | loss: 7.1044192CurrentTrain: epoch  0, batch     2 | loss: 5.7429266CurrentTrain: epoch  0, batch     3 | loss: 9.1268177CurrentTrain: epoch  1, batch     0 | loss: 5.9511676CurrentTrain: epoch  1, batch     1 | loss: 5.8120494CurrentTrain: epoch  1, batch     2 | loss: 7.0155411CurrentTrain: epoch  1, batch     3 | loss: 6.8082881CurrentTrain: epoch  2, batch     0 | loss: 5.3654299CurrentTrain: epoch  2, batch     1 | loss: 5.4400692CurrentTrain: epoch  2, batch     2 | loss: 5.6669350CurrentTrain: epoch  2, batch     3 | loss: 6.3926477CurrentTrain: epoch  3, batch     0 | loss: 5.5377374CurrentTrain: epoch  3, batch     1 | loss: 4.8703060CurrentTrain: epoch  3, batch     2 | loss: 5.2623978CurrentTrain: epoch  3, batch     3 | loss: 6.5653381CurrentTrain: epoch  4, batch     0 | loss: 4.2142916CurrentTrain: epoch  4, batch     1 | loss: 4.5457411CurrentTrain: epoch  4, batch     2 | loss: 5.3651972CurrentTrain: epoch  4, batch     3 | loss: 3.5248280CurrentTrain: epoch  5, batch     0 | loss: 4.3421650CurrentTrain: epoch  5, batch     1 | loss: 4.9187217CurrentTrain: epoch  5, batch     2 | loss: 4.2733393CurrentTrain: epoch  5, batch     3 | loss: 4.1356888CurrentTrain: epoch  6, batch     0 | loss: 4.3518023CurrentTrain: epoch  6, batch     1 | loss: 3.9249802CurrentTrain: epoch  6, batch     2 | loss: 4.3974710CurrentTrain: epoch  6, batch     3 | loss: 5.0131998CurrentTrain: epoch  7, batch     0 | loss: 4.6338272CurrentTrain: epoch  7, batch     1 | loss: 4.2443857CurrentTrain: epoch  7, batch     2 | loss: 3.8700671CurrentTrain: epoch  7, batch     3 | loss: 2.0955286CurrentTrain: epoch  8, batch     0 | loss: 4.0712957CurrentTrain: epoch  8, batch     1 | loss: 3.9968708CurrentTrain: epoch  8, batch     2 | loss: 4.0868378CurrentTrain: epoch  8, batch     3 | loss: 3.3493419CurrentTrain: epoch  9, batch     0 | loss: 3.5278387CurrentTrain: epoch  9, batch     1 | loss: 3.6549582CurrentTrain: epoch  9, batch     2 | loss: 4.3323717CurrentTrain: epoch  9, batch     3 | loss: 3.6344075
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: in 2004 the catalan government gave him the george cross .
Head Entity: george cross
Tail Entity: catalan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The new environmental regulations introduced by the European Union aim to reduce carbon emissions across member states.  
Head Entity: European Union  
Tail Entity: member states  

Relation: applies to jurisdiction  
Context: The Supreme Court's ruling clarified that federal laws take precedence over state laws in matters of immigration.  
Head Entity: federal laws  
Tail Entity: state laws  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: it is prominently featured on jean michel jarre 's albums " oxygène " ( 1977 ) and " équinoxe " ( 1978 ) .
Head Entity: équinoxe
Tail Entity: jean michel jarre
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: The legendary band Queen is known for their iconic song "Bohemian Rhapsody," which showcases Freddie Mercury's extraordinary vocal talent.  
Head Entity: Bohemian Rhapsody  
Tail Entity: Freddie Mercury  

Relation: performer  
Context: Taylor Swift performed her hit single "Shake It Off" at the Grammy Awards, captivating the audience with her energetic stage presence.  
Head Entity: Shake It Off  
Tail Entity: Taylor Swift  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the next full - frame digital slrs were the canon eos-1ds of late 2002 , followed by kodak 's dcs pro 14n in 2003 .
Head Entity: dcs pro 14n
Tail Entity: kodak
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The latest smartphone model, the Galaxy S21, was released by Samsung in January 2021, showcasing advanced camera technology and performance.  
Head Entity: Galaxy S21  
Tail Entity: Samsung  

Relation: manufacturer  
Context: The iconic Model T was produced by Ford Motor Company, revolutionizing the automotive industry in the early 20th century.  
Head Entity: Model T  
Tail Entity: Ford Motor Company  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: norway was represented in the eurovision song contest 2005 by the song " in my dreams " performed by wig wam .
Head Entity: eurovision song contest 2005
Tail Entity: wig wam
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: the united states was represented in the 2020 summer olympics by a team of athletes competing in various sports.  
Head Entity: 2020 summer olympics  
Tail Entity: united states  

Relation: participant  
Context: the annual tech conference featured several startups, including a presentation by innovative solutions inc.  
Head Entity: annual tech conference  
Tail Entity: innovative solutions inc.  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the application is compatible with various operating systems, including macOS, Windows, and Ubuntu.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: Many developers prefer to use Linux as their primary operating system due to its flexibility and open-source nature.  
Head Entity: developers  
Tail Entity: Linux  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in his next bout , he won the wbc light welterweight title by defeating roger mayweather for a second time .
Head Entity: roger mayweather
Tail Entity: light welterweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: The team secured their place in the tournament by qualifying under the UEFA Champions League regulations.  
Head Entity: UEFA Champions League  
Tail Entity: tournament  

Relation: competition class  
Context: The athlete competed in the 100 meters event, which is classified under the Olympic Games standards.  
Head Entity: Olympic Games  
Tail Entity: 100 meters event  
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: swartz served as swedish prime minister for only seven months before being forced from power after the swedish elections of 1917 and replaced by the liberal government of nils edén .
Head Entity: swedish elections of 1917
Tail Entity: nils edén
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: in the recent municipal elections, the voters chose maria gonzalez as the new mayor of the city, marking a significant shift in local governance.  
Head Entity: municipal elections  
Tail Entity: maria gonzalez  

Relation: successful candidate  
Context: after a closely contested race, the citizens of springfield elected john doe as their new governor, succeeding the long-serving incumbent.  
Head Entity: springfield elections  
Tail Entity: john doe  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: although the latter 's eldest son , lǫgmaðr , appears to have succeeded to the kingship , he was soon forced to contend with factions supporting his younger brothers : haraldr , and óláfr .
Head Entity: lǫgmaðr
Tail Entity: óláfr
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: In the royal family of the ancient kingdom, the two brothers, Eirik and Sigurd, were known for their fierce rivalry, yet they shared a bond that only siblings could understand.  
Head Entity: Eirik  
Tail Entity: Sigurd  

Relation: sibling  
Context: During the family reunion, it became clear that the two sisters, Maria and Elena, had grown apart over the years, but their shared childhood memories reminded them of their strong sibling connection.  
Head Entity: Maria  
Tail Entity: Elena  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: iain harrison is a competitive shooter and former british army captain .
Head Entity: iain harrison
Tail Entity: british army
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: sergeant johnson served in the united states marine corps during his military career.  
Head Entity: sergeant johnson  
Tail Entity: united states marine corps  

Relation: military branch  
Context: general smith was a prominent figure in the royal air force, leading several key missions.  
Head Entity: general smith  
Tail Entity: royal air force  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: mukesh is married to nita ambani and has two sons , anant and akash , and a daughter , isha .
Head Entity: nita ambani
Tail Entity: akash
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: elon musk has six children, including a son named x ae a-xii and a daughter named exa dark sidereal.  
Head Entity: elon musk  
Tail Entity: x ae a-xii  

Relation: child  
Context: j.k. rowling is the mother of three children, including a daughter named jessica and a son named david.  
Head Entity: j.k. rowling  
Tail Entity: jessica  
Mixup data size:  198
MixupTrain:  epoch  0, batch     0 | loss: 6.5824334MixupTrain:  epoch  0, batch     1 | loss: 5.9356994MixupTrain:  epoch  0, batch     2 | loss: 5.7134663MixupTrain:  epoch  0, batch     3 | loss: 5.4389209MixupTrain:  epoch  0, batch     4 | loss: 5.6491702MixupTrain:  epoch  0, batch     5 | loss: 5.6179337MixupTrain:  epoch  0, batch     6 | loss: 5.9163739MixupTrain:  epoch  0, batch     7 | loss: 5.0430878MixupTrain:  epoch  0, batch     8 | loss: 5.2861852MixupTrain:  epoch  0, batch     9 | loss: 5.6600352MixupTrain:  epoch  0, batch    10 | loss: 5.0688157MixupTrain:  epoch  0, batch    11 | loss: 5.1641502MixupTrain:  epoch  0, batch    12 | loss: 4.1434880
MemoryTrain:  epoch  0, batch     0 | loss: 3.3236794MemoryTrain:  epoch  0, batch     1 | loss: 3.6801267MemoryTrain:  epoch  0, batch     2 | loss: 4.1137013MemoryTrain:  epoch  0, batch     3 | loss: 4.6287351MemoryTrain:  epoch  1, batch     0 | loss: 3.6602139MemoryTrain:  epoch  1, batch     1 | loss: 3.7147648MemoryTrain:  epoch  1, batch     2 | loss: 3.9977555MemoryTrain:  epoch  1, batch     3 | loss: 3.3184488MemoryTrain:  epoch  2, batch     0 | loss: 2.8454852MemoryTrain:  epoch  2, batch     1 | loss: 2.6303139MemoryTrain:  epoch  2, batch     2 | loss: 3.5531616MemoryTrain:  epoch  2, batch     3 | loss: 3.2621140MemoryTrain:  epoch  3, batch     0 | loss: 2.5433354MemoryTrain:  epoch  3, batch     1 | loss: 2.9622285MemoryTrain:  epoch  3, batch     2 | loss: 2.1955502MemoryTrain:  epoch  3, batch     3 | loss: 2.5874381MemoryTrain:  epoch  4, batch     0 | loss: 2.6003594MemoryTrain:  epoch  4, batch     1 | loss: 1.9511014MemoryTrain:  epoch  4, batch     2 | loss: 2.1338620MemoryTrain:  epoch  4, batch     3 | loss: 2.2581129MemoryTrain:  epoch  5, batch     0 | loss: 1.9595501MemoryTrain:  epoch  5, batch     1 | loss: 2.0430489MemoryTrain:  epoch  5, batch     2 | loss: 2.2985079MemoryTrain:  epoch  5, batch     3 | loss: 2.0163901MemoryTrain:  epoch  6, batch     0 | loss: 2.2039046MemoryTrain:  epoch  6, batch     1 | loss: 1.9182119MemoryTrain:  epoch  6, batch     2 | loss: 1.7737128MemoryTrain:  epoch  6, batch     3 | loss: 2.1522808MemoryTrain:  epoch  7, batch     0 | loss: 2.2810860MemoryTrain:  epoch  7, batch     1 | loss: 1.6235842MemoryTrain:  epoch  7, batch     2 | loss: 1.8565788MemoryTrain:  epoch  7, batch     3 | loss: 1.4846656MemoryTrain:  epoch  8, batch     0 | loss: 2.1288314MemoryTrain:  epoch  8, batch     1 | loss: 1.7040691MemoryTrain:  epoch  8, batch     2 | loss: 1.4338243MemoryTrain:  epoch  8, batch     3 | loss: 1.7644342MemoryTrain:  epoch  9, batch     0 | loss: 1.7821434MemoryTrain:  epoch  9, batch     1 | loss: 1.5368059MemoryTrain:  epoch  9, batch     2 | loss: 1.5710670MemoryTrain:  epoch  9, batch     3 | loss: 1.8741773
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 12.50%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 13.75%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 15.62%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 21.43%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 28.91%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 34.72%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 38.75%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 42.61%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 45.83%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 49.52%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 51.79%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 54.17%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 56.64%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 58.82%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 61.11%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 64.06%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 63.99%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 64.20%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 64.67%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 66.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 68.03%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 69.21%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 71.34%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 72.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 73.19%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 73.83%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 74.62%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 75.37%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 76.07%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 76.74%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 77.36%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 77.96%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 78.21%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 77.90%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 77.98%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 78.34%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 78.55%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 78.33%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 77.85%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 76.99%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 77.34%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 77.04%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 77.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 77.45%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 77.76%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 78.07%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 78.41%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 78.57%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 77.74%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 77.05%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 76.38%   [EVAL] batch:   59 | acc: 31.25%,  total acc: 75.62%   [EVAL] batch:   60 | acc: 18.75%,  total acc: 74.69%   [EVAL] batch:   61 | acc: 12.50%,  total acc: 73.69%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 72.92%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 89.73%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 90.81%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.97%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.45%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 91.48%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 91.58%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 91.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.83%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 91.90%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.03%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.54%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.77%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.99%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.20%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.21%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 93.40%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.58%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.91%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.06%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.21%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 94.33%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 94.18%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.31%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 94.29%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 94.02%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 93.75%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.88%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 93.63%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 93.63%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 93.52%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 93.64%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 93.65%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 93.15%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 91.99%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 90.77%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 89.49%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 88.34%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 87.50%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 86.59%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 86.52%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 86.44%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 86.28%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 86.04%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 86.15%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 86.17%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 86.10%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 86.20%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 86.14%   [EVAL] batch:   78 | acc: 100.00%,  total acc: 86.31%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 86.41%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 86.57%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 86.51%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 86.52%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 86.09%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 85.96%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 85.97%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 86.06%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 86.15%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 86.31%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 86.75%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 86.90%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 86.97%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 87.11%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 87.24%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 87.37%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 87.63%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 87.75%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 87.75%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 87.75%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 87.56%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 87.44%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 87.44%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 87.44%   [EVAL] batch:  107 | acc: 56.25%,  total acc: 87.15%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 86.87%   [EVAL] batch:  109 | acc: 62.50%,  total acc: 86.65%   [EVAL] batch:  110 | acc: 81.25%,  total acc: 86.60%   [EVAL] batch:  111 | acc: 68.75%,  total acc: 86.44%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 86.45%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 86.58%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 86.58%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 86.59%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 86.65%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 86.50%   [EVAL] batch:  119 | acc: 18.75%,  total acc: 85.94%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 85.59%   [EVAL] batch:  121 | acc: 31.25%,  total acc: 85.14%   [EVAL] batch:  122 | acc: 25.00%,  total acc: 84.65%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 84.02%   [EVAL] batch:  124 | acc: 37.50%,  total acc: 83.65%   
cur_acc:  ['0.9504', '0.7292']
his_acc:  ['0.9504', '0.8365']
CurrentTrain: epoch  0, batch     0 | loss: 6.2389393CurrentTrain: epoch  0, batch     1 | loss: 6.3577013CurrentTrain: epoch  0, batch     2 | loss: 6.3529348CurrentTrain: epoch  0, batch     3 | loss: 5.0390043CurrentTrain: epoch  1, batch     0 | loss: 5.3010859CurrentTrain: epoch  1, batch     1 | loss: 5.1060262CurrentTrain: epoch  1, batch     2 | loss: 5.0396776CurrentTrain: epoch  1, batch     3 | loss: 5.8898706CurrentTrain: epoch  2, batch     0 | loss: 4.9816761CurrentTrain: epoch  2, batch     1 | loss: 4.8030343CurrentTrain: epoch  2, batch     2 | loss: 3.8542345CurrentTrain: epoch  2, batch     3 | loss: 2.3894963CurrentTrain: epoch  3, batch     0 | loss: 3.9875512CurrentTrain: epoch  3, batch     1 | loss: 4.0977702CurrentTrain: epoch  3, batch     2 | loss: 4.1737099CurrentTrain: epoch  3, batch     3 | loss: 4.6904383CurrentTrain: epoch  4, batch     0 | loss: 4.1581440CurrentTrain: epoch  4, batch     1 | loss: 3.0787439CurrentTrain: epoch  4, batch     2 | loss: 3.8001540CurrentTrain: epoch  4, batch     3 | loss: 3.2687201CurrentTrain: epoch  5, batch     0 | loss: 3.0443482CurrentTrain: epoch  5, batch     1 | loss: 3.6434636CurrentTrain: epoch  5, batch     2 | loss: 3.2276847CurrentTrain: epoch  5, batch     3 | loss: 3.2060242CurrentTrain: epoch  6, batch     0 | loss: 3.6222537CurrentTrain: epoch  6, batch     1 | loss: 2.9216347CurrentTrain: epoch  6, batch     2 | loss: 2.7055588CurrentTrain: epoch  6, batch     3 | loss: 2.7997632CurrentTrain: epoch  7, batch     0 | loss: 3.0089412CurrentTrain: epoch  7, batch     1 | loss: 2.6581070CurrentTrain: epoch  7, batch     2 | loss: 2.5956509CurrentTrain: epoch  7, batch     3 | loss: 2.5277443CurrentTrain: epoch  8, batch     0 | loss: 2.5523720CurrentTrain: epoch  8, batch     1 | loss: 2.5620303CurrentTrain: epoch  8, batch     2 | loss: 2.6254897CurrentTrain: epoch  8, batch     3 | loss: 1.8031625CurrentTrain: epoch  9, batch     0 | loss: 2.4188151CurrentTrain: epoch  9, batch     1 | loss: 2.5148540CurrentTrain: epoch  9, batch     2 | loss: 2.4286995CurrentTrain: epoch  9, batch     3 | loss: 3.9716315
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: rohan is the son of narayana murthy , founder of infosys , and his wife sudha murthy , an engineer and author of note .
Head Entity: narayana murthy
Tail Entity: sudha murthy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: elena is a renowned artist who often collaborates with her husband, mark, a talented musician known for his soulful melodies.  
Head Entity: mark  
Tail Entity: elena  

Relation: spouse  
Context: during the award ceremony, it was revealed that the famous director, james cameron, has been married to suzy, a skilled producer, for over two decades.  
Head Entity: james cameron  
Tail Entity: suzy  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: " i 'm a cuckoo " was belle & sebastian 's second single from " dear catastrophe waitress " , released on rough trade records in 2004 .
Head Entity: belle & sebastian
Tail Entity: rough trade records
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: The album "Random Access Memories" by Daft Punk was released under the Columbia Records label in 2013.  
Head Entity: Daft Punk  
Tail Entity: Columbia Records  

Relation: record label  
Context: Taylor Swift's latest album "Evermore" was produced by her own label, Republic Records, and released in 2020.  
Head Entity: Taylor Swift  
Tail Entity: Republic Records  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: distributed by paramount pictures , " the italian job " was theatrically released in the united states on may 30 , 2003 , and grossed over $ 176 million worldwide .
Head Entity: the italian job
Tail Entity: paramount pictures
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: the film was distributed by universal pictures and received critical acclaim upon its release in 2015.  
Head Entity: the film  
Tail Entity: universal pictures  

Relation: distributor  
Context: the album was distributed by sony music and quickly climbed the charts, becoming a bestseller.  
Head Entity: the album  
Tail Entity: sony music  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: the highway was re - aligned in 1963 to serve the port of vancouver , traveling along the columbia river to ridgefield west of vancouver lake onto a roadway that was to be constructed .
Head Entity: port of vancouver
Tail Entity: columbia river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: The city of Miami is famous for its beautiful beaches and vibrant nightlife, situated right along the Atlantic Ocean, making it a popular destination for tourists.  
Head Entity: Miami  
Tail Entity: Atlantic Ocean  

Relation: located in or next to body of water  
Context: The quaint village of Port Isaac is nestled on the rugged coastline of Cornwall, directly adjacent to the picturesque harbor that opens into the Celtic Sea.  
Head Entity: Port Isaac  
Tail Entity: Celtic Sea  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: cbs and viacom split again in 2005 , but both are still owned by national amusements .
Head Entity: national amusements
Tail Entity: viacom
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Relation: subsidiary  
Context: In 2018, Facebook acquired Instagram, which now operates as a subsidiary under the social media giant.  
Head Entity: Facebook  
Tail Entity: Instagram  

Relation: subsidiary  
Context: Toyota has several subsidiaries, including Lexus, which focuses on luxury vehicles.  
Head Entity: Toyota  
Tail Entity: Lexus  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: he challenged seven other swimmers in heat four including ukraine 's rostyslav svanidze , a top 16 finalist in atlanta four years earlier .
Head Entity: rostyslav svanidze
Tail Entity: ukraine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: after years of living in the united states, the renowned artist decided to apply for dual citizenship, allowing him to retain his ties to his homeland, italy.  
Head Entity: the renowned artist  
Tail Entity: italy  

Relation: country of citizenship  
Context: during the international conference, the speaker proudly mentioned that she had recently obtained her citizenship in canada, where she had been residing for over a decade.  
Head Entity: the speaker  
Tail Entity: canada  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: he is also a part of the atlanta - based hip - hop / r&b;/soul musical collective the dungeon family .
Head Entity: the dungeon family
Tail Entity: hip - hop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the band is known for their unique blend of rock and electronic music, often categorized under the genre of synth-pop.  
Head Entity: the band  
Tail Entity: synth-pop  

Relation: genre  
Context: she has made significant contributions to the world of classical music, particularly in the genre of opera.  
Head Entity: she  
Tail Entity: opera  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: three major wyoming rivers , the salt river , the greys river and the snake river meet near alpine junction at palisades reservoir .
Head Entity: greys river
Tail Entity: wyoming
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: The Mississippi River flows southward and empties into the Gulf of Mexico, making it one of the largest river systems in North America.  
Head Entity: Mississippi River  
Tail Entity: Gulf of Mexico  

Relation: mouth of the watercourse  
Context: The Amazon River, known for its vast basin, ultimately drains into the Atlantic Ocean, supporting a diverse ecosystem along its banks.  
Head Entity: Amazon River  
Tail Entity: Atlantic Ocean  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: benjamin van leer ( born 9 april 1992 in houten ) is a dutch professional footballer who currently plays as a goalkeeper for ajax in the dutch eredivisie .
Head Entity: benjamin van leer
Tail Entity: goalkeeper
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: lebron james, born on december 30, 1984, is an american professional basketball player who currently plays as a forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: serena williams, born on september 26, 1981, is an american professional tennis player who is known for her powerful serve and currently competes in singles and doubles events.  
Head Entity: serena williams  
Tail Entity: tennis player  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: the stade louis ii in monaco was the venue for the uefa super cup every year since 1998 .
Head Entity: 1998
Tail Entity: uefa super cup
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2020 NBA season was significantly impacted by the COVID-19 pandemic, leading to a unique playoff format.  
Head Entity: 2020  
Tail Entity: NBA  

Relation: sports season of league or competition  
Context: The 2019 Rugby World Cup took place in Japan, showcasing teams from around the globe.  
Head Entity: 2019  
Tail Entity: Rugby World Cup  
Mixup data size:  259
MixupTrain:  epoch  0, batch     0 | loss: 3.3592269MixupTrain:  epoch  0, batch     1 | loss: 3.6122253MixupTrain:  epoch  0, batch     2 | loss: 3.1390685MixupTrain:  epoch  0, batch     3 | loss: 3.5058857MixupTrain:  epoch  0, batch     4 | loss: 3.1941974MixupTrain:  epoch  0, batch     5 | loss: 3.2049678MixupTrain:  epoch  0, batch     6 | loss: 3.2947173MixupTrain:  epoch  0, batch     7 | loss: 3.2328883MixupTrain:  epoch  0, batch     8 | loss: 3.0652575MixupTrain:  epoch  0, batch     9 | loss: 3.1353426MixupTrain:  epoch  0, batch    10 | loss: 3.0140295MixupTrain:  epoch  0, batch    11 | loss: 2.9571877MixupTrain:  epoch  0, batch    12 | loss: 3.2617293MixupTrain:  epoch  0, batch    13 | loss: 3.0281595MixupTrain:  epoch  0, batch    14 | loss: 2.7623803MixupTrain:  epoch  0, batch    15 | loss: 2.9827842MixupTrain:  epoch  0, batch    16 | loss: 3.0664508
MemoryTrain:  epoch  0, batch     0 | loss: 3.4345725MemoryTrain:  epoch  0, batch     1 | loss: 2.9547179MemoryTrain:  epoch  0, batch     2 | loss: 4.0958672MemoryTrain:  epoch  0, batch     3 | loss: 4.0937328MemoryTrain:  epoch  0, batch     4 | loss: 3.3233390MemoryTrain:  epoch  0, batch     5 | loss: 3.1955094MemoryTrain:  epoch  1, batch     0 | loss: 3.2769911MemoryTrain:  epoch  1, batch     1 | loss: 3.6497617MemoryTrain:  epoch  1, batch     2 | loss: 3.3790872MemoryTrain:  epoch  1, batch     3 | loss: 2.7451916MemoryTrain:  epoch  1, batch     4 | loss: 2.3188858MemoryTrain:  epoch  1, batch     5 | loss: 2.5180943MemoryTrain:  epoch  2, batch     0 | loss: 2.5396132MemoryTrain:  epoch  2, batch     1 | loss: 1.7914894MemoryTrain:  epoch  2, batch     2 | loss: 3.1482582MemoryTrain:  epoch  2, batch     3 | loss: 2.5674260MemoryTrain:  epoch  2, batch     4 | loss: 2.7926645MemoryTrain:  epoch  2, batch     5 | loss: 2.6365182MemoryTrain:  epoch  3, batch     0 | loss: 2.7185969MemoryTrain:  epoch  3, batch     1 | loss: 2.5048518MemoryTrain:  epoch  3, batch     2 | loss: 1.8920929MemoryTrain:  epoch  3, batch     3 | loss: 2.3599489MemoryTrain:  epoch  3, batch     4 | loss: 2.2440186MemoryTrain:  epoch  3, batch     5 | loss: 2.7007978MemoryTrain:  epoch  4, batch     0 | loss: 1.9862051MemoryTrain:  epoch  4, batch     1 | loss: 2.5950980MemoryTrain:  epoch  4, batch     2 | loss: 2.1388273MemoryTrain:  epoch  4, batch     3 | loss: 2.1665697MemoryTrain:  epoch  4, batch     4 | loss: 2.6624413MemoryTrain:  epoch  4, batch     5 | loss: 1.8345398MemoryTrain:  epoch  5, batch     0 | loss: 2.3907008MemoryTrain:  epoch  5, batch     1 | loss: 2.0733316MemoryTrain:  epoch  5, batch     2 | loss: 2.0324788MemoryTrain:  epoch  5, batch     3 | loss: 2.4764631MemoryTrain:  epoch  5, batch     4 | loss: 2.1105824MemoryTrain:  epoch  5, batch     5 | loss: 2.0990942MemoryTrain:  epoch  6, batch     0 | loss: 1.8430026MemoryTrain:  epoch  6, batch     1 | loss: 2.2707660MemoryTrain:  epoch  6, batch     2 | loss: 2.0686216MemoryTrain:  epoch  6, batch     3 | loss: 1.7770528MemoryTrain:  epoch  6, batch     4 | loss: 1.6909053MemoryTrain:  epoch  6, batch     5 | loss: 1.8603306MemoryTrain:  epoch  7, batch     0 | loss: 1.4339185MemoryTrain:  epoch  7, batch     1 | loss: 2.0660558MemoryTrain:  epoch  7, batch     2 | loss: 1.7670236MemoryTrain:  epoch  7, batch     3 | loss: 1.7247698MemoryTrain:  epoch  7, batch     4 | loss: 1.5341029MemoryTrain:  epoch  7, batch     5 | loss: 1.5881302MemoryTrain:  epoch  8, batch     0 | loss: 1.6670971MemoryTrain:  epoch  8, batch     1 | loss: 1.3698407MemoryTrain:  epoch  8, batch     2 | loss: 1.5256236MemoryTrain:  epoch  8, batch     3 | loss: 1.5871328MemoryTrain:  epoch  8, batch     4 | loss: 1.6550093MemoryTrain:  epoch  8, batch     5 | loss: 1.7958469MemoryTrain:  epoch  9, batch     0 | loss: 1.5598340MemoryTrain:  epoch  9, batch     1 | loss: 1.4685138MemoryTrain:  epoch  9, batch     2 | loss: 1.6258423MemoryTrain:  epoch  9, batch     3 | loss: 1.6176174MemoryTrain:  epoch  9, batch     4 | loss: 1.4285938MemoryTrain:  epoch  9, batch     5 | loss: 1.2726344
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 88.33%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 88.67%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 88.24%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 88.16%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 87.78%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 87.77%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 87.76%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 87.75%   [EVAL] batch:   25 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 87.04%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 87.05%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 87.28%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 86.04%   [EVAL] batch:   30 | acc: 62.50%,  total acc: 85.28%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 84.38%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 83.90%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 83.46%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 82.86%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 82.64%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 82.43%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 82.24%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 81.89%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 82.03%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 81.55%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 81.40%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 81.54%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   44 | acc: 12.50%,  total acc: 79.72%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 78.26%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 76.99%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 76.17%   [EVAL] batch:   48 | acc: 6.25%,  total acc: 74.74%   [EVAL] batch:   49 | acc: 25.00%,  total acc: 73.75%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 74.26%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 74.76%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 75.24%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 75.69%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 76.14%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 76.97%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 77.37%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 77.75%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 78.48%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 78.83%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 78.37%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 86.16%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 87.87%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.19%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 88.49%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 88.99%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 88.64%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 87.77%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 87.76%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 87.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 88.22%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 88.43%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.84%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 89.22%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 89.92%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 90.23%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 90.53%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 90.81%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 90.89%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 90.88%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 90.95%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.19%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.62%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.82%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 91.72%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 91.76%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 91.71%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 91.49%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 91.15%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.33%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.30%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 91.39%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 91.55%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 91.36%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.41%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 91.34%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 91.16%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 91.21%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 91.29%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 91.23%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 90.67%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 89.75%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 88.94%   [EVAL] batch:   65 | acc: 25.00%,  total acc: 87.97%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 87.03%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 86.58%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 85.87%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 85.80%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 85.65%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 85.50%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 85.36%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 85.39%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 85.53%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 85.55%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 85.52%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 85.47%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 85.65%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 85.59%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 85.54%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 85.12%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 85.00%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 85.03%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 85.20%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 85.30%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 85.46%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 85.62%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 85.78%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 86.09%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 86.17%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 86.32%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 86.60%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 86.73%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 86.87%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 87.00%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 87.00%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 87.07%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 86.89%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 86.84%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 86.90%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 86.85%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 86.57%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 86.11%   [EVAL] batch:  108 | acc: 43.75%,  total acc: 85.72%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 85.28%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 85.14%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 84.65%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 84.51%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 84.54%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 84.67%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 84.70%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 84.72%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 84.75%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 84.61%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 83.91%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 83.32%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 82.63%   [EVAL] batch:  122 | acc: 0.00%,  total acc: 81.96%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 81.30%   [EVAL] batch:  124 | acc: 0.00%,  total acc: 80.65%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 80.80%   [EVAL] batch:  126 | acc: 87.50%,  total acc: 80.86%   [EVAL] batch:  127 | acc: 87.50%,  total acc: 80.91%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 80.91%   [EVAL] batch:  129 | acc: 93.75%,  total acc: 81.01%   [EVAL] batch:  130 | acc: 87.50%,  total acc: 81.06%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 81.06%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 81.20%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 81.30%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 81.34%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 81.43%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 81.48%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 81.52%   [EVAL] batch:  138 | acc: 75.00%,  total acc: 81.47%   [EVAL] batch:  139 | acc: 81.25%,  total acc: 81.47%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 81.56%   [EVAL] batch:  141 | acc: 81.25%,  total acc: 81.56%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 81.51%   [EVAL] batch:  143 | acc: 100.00%,  total acc: 81.64%   [EVAL] batch:  144 | acc: 75.00%,  total acc: 81.59%   [EVAL] batch:  145 | acc: 87.50%,  total acc: 81.64%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 81.72%   [EVAL] batch:  147 | acc: 87.50%,  total acc: 81.76%   [EVAL] batch:  148 | acc: 87.50%,  total acc: 81.80%   [EVAL] batch:  149 | acc: 87.50%,  total acc: 81.83%   [EVAL] batch:  150 | acc: 81.25%,  total acc: 81.83%   [EVAL] batch:  151 | acc: 75.00%,  total acc: 81.78%   [EVAL] batch:  152 | acc: 87.50%,  total acc: 81.82%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 81.90%   [EVAL] batch:  154 | acc: 50.00%,  total acc: 81.69%   [EVAL] batch:  155 | acc: 62.50%,  total acc: 81.57%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 81.41%   [EVAL] batch:  157 | acc: 68.75%,  total acc: 81.33%   [EVAL] batch:  158 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 81.13%   [EVAL] batch:  160 | acc: 75.00%,  total acc: 81.09%   [EVAL] batch:  161 | acc: 75.00%,  total acc: 81.06%   [EVAL] batch:  162 | acc: 75.00%,  total acc: 81.02%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 80.95%   [EVAL] batch:  164 | acc: 87.50%,  total acc: 80.98%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 80.87%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 80.84%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 80.88%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 80.81%   [EVAL] batch:  169 | acc: 12.50%,  total acc: 80.40%   [EVAL] batch:  170 | acc: 12.50%,  total acc: 80.01%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 79.65%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 79.41%   [EVAL] batch:  173 | acc: 6.25%,  total acc: 78.99%   [EVAL] batch:  174 | acc: 25.00%,  total acc: 78.68%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 78.80%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 78.92%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 79.04%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 79.16%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 79.27%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 79.39%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 79.50%   [EVAL] batch:  182 | acc: 100.00%,  total acc: 79.61%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 79.72%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 79.83%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 79.94%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 80.05%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 79.89%   
cur_acc:  ['0.9504', '0.7292', '0.7837']
his_acc:  ['0.9504', '0.8365', '0.7989']
CurrentTrain: epoch  0, batch     0 | loss: 6.6115923CurrentTrain: epoch  0, batch     1 | loss: 4.9959469CurrentTrain: epoch  0, batch     2 | loss: 6.8805914CurrentTrain: epoch  0, batch     3 | loss: 5.3785238CurrentTrain: epoch  1, batch     0 | loss: 6.1840267CurrentTrain: epoch  1, batch     1 | loss: 4.5972624CurrentTrain: epoch  1, batch     2 | loss: 4.9770064CurrentTrain: epoch  1, batch     3 | loss: 6.8235865CurrentTrain: epoch  2, batch     0 | loss: 4.4490399CurrentTrain: epoch  2, batch     1 | loss: 4.6470470CurrentTrain: epoch  2, batch     2 | loss: 5.6119647CurrentTrain: epoch  2, batch     3 | loss: 2.6049578CurrentTrain: epoch  3, batch     0 | loss: 3.7351809CurrentTrain: epoch  3, batch     1 | loss: 5.0405912CurrentTrain: epoch  3, batch     2 | loss: 4.2813759CurrentTrain: epoch  3, batch     3 | loss: 4.7630444CurrentTrain: epoch  4, batch     0 | loss: 4.1172485CurrentTrain: epoch  4, batch     1 | loss: 4.0705719CurrentTrain: epoch  4, batch     2 | loss: 3.9315894CurrentTrain: epoch  4, batch     3 | loss: 4.0016584CurrentTrain: epoch  5, batch     0 | loss: 3.8224053CurrentTrain: epoch  5, batch     1 | loss: 3.8897462CurrentTrain: epoch  5, batch     2 | loss: 3.1642594CurrentTrain: epoch  5, batch     3 | loss: 4.7173395CurrentTrain: epoch  6, batch     0 | loss: 4.0115862CurrentTrain: epoch  6, batch     1 | loss: 3.0380111CurrentTrain: epoch  6, batch     2 | loss: 3.7959085CurrentTrain: epoch  6, batch     3 | loss: 2.8370495CurrentTrain: epoch  7, batch     0 | loss: 3.7318971CurrentTrain: epoch  7, batch     1 | loss: 3.0927231CurrentTrain: epoch  7, batch     2 | loss: 3.8363228CurrentTrain: epoch  7, batch     3 | loss: 2.4010475CurrentTrain: epoch  8, batch     0 | loss: 3.4730725CurrentTrain: epoch  8, batch     1 | loss: 2.9616995CurrentTrain: epoch  8, batch     2 | loss: 3.4123740CurrentTrain: epoch  8, batch     3 | loss: 2.7936296CurrentTrain: epoch  9, batch     0 | loss: 3.4595146CurrentTrain: epoch  9, batch     1 | loss: 3.4022543CurrentTrain: epoch  9, batch     2 | loss: 2.3894114CurrentTrain: epoch  9, batch     3 | loss: 2.0618987
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: ( 1988 ) , " night watch " ( 2004 ) , " day watch " ( 2006 ) and " legend № 17 " ( 2013 ) .
Head Entity: day watch
Tail Entity: night watch
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: ( 1995 ) , " The Phantom Menace " ( 1999 ) , " Attack of the Clones " ( 2002 ) and " Revenge of the Sith " ( 2005 ) .  
Head Entity: Attack of the Clones  
Tail Entity: The Phantom Menace  

Relation: follows  
Context: ( 2010 ) , " The Hunger Games " ( 2012 ) , " Catching Fire " ( 2013 ) and " Mockingjay " ( 2014 ) .  
Head Entity: Catching Fire  
Tail Entity: The Hunger Games  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: dick packer was a u.s. soccer center forward who was a member of the u.s. team at the 1956 summer olympics .
Head Entity: dick packer
Tail Entity: soccer
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: serena williams is a professional tennis player who has won numerous grand slam titles throughout her career.  
Head Entity: serena williams  
Tail Entity: tennis  

Relation: sport  
Context: lebron james is known for his exceptional skills in basketball and has played for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: basketball  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael was the father of lucas, who had recently graduated from university.  
Head Entity: lucas  
Tail Entity: michael  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: since 2009 , culshaw has starred in the bbc one comedy sketch show " the impressions show " alongside debra stephenson .
Head Entity: the impressions show
Tail Entity: bbc one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: The popular series "Breaking Bad" first aired on AMC, captivating audiences with its intense storytelling and character development.  
Head Entity: Breaking Bad  
Tail Entity: AMC  

Relation: original network  
Context: "Friends" became a cultural phenomenon when it premiered on NBC, showcasing the lives of six friends living in New York City.  
Head Entity: Friends  
Tail Entity: NBC  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: the 1954 film , directed by kadri venkata reddy , " peddamanushulu " was honoured with the first president 's silver medal for best feature film in telugu .
Head Entity: peddamanushulu
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: the 1994 animated film "The Lion King," produced by Walt Disney, was originally created in English before being dubbed into several other languages.  
Head Entity: The Lion King  
Tail Entity: English  

Relation: original language of film or TV show  
Context: the critically acclaimed 2001 film "Amélie," directed by Jean-Pierre Jeunet, was filmed in French and has since gained a global audience.  
Head Entity: Amélie  
Tail Entity: French  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: in the summer of 2013 he returned to montenegro and joined with fk rudar pljevlja , playing with the club for two seasons in the montenegrin first league .
Head Entity: fk rudar pljevlja
Tail Entity: montenegrin first league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: After a successful stint in the English Premier League, the player signed with FC Barcelona, competing in La Liga for the next three seasons.  
Head Entity: FC Barcelona  
Tail Entity: La Liga  

Relation: league  
Context: The young athlete made headlines when he joined the New York Yankees, showcasing his talent in Major League Baseball.  
Head Entity: New York Yankees  
Tail Entity: Major League Baseball  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: other teammates included dominique leray and élisabeth riffiod , whose son boris diaw currently plays in the nba for the spurs ( 2014 ) .
Head Entity: boris diaw
Tail Entity: élisabeth riffiod
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: in a recent interview, actress jennifer garner spoke fondly of her children, including violet, who is following in her mother's footsteps in the entertainment industry.  
Head Entity: violet  
Tail Entity: jennifer garner  

Relation: mother  
Context: during the family reunion, it was heartwarming to see how much emily looked like her mother, sarah, especially when they both laughed at the same joke.  
Head Entity: emily  
Tail Entity: sarah  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres. he is particularly celebrated for his interpretations of classical cello concertos.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: the type locality given is " fan - si - pan mountains , tonkin , indo - china " ( mount fansipan , sa pa district , nghệ an province , northwestern vietnam ) .
Head Entity: sa pa district
Tail Entity: vietnam
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the capital city of the country is located in the central region, known for its rich history and cultural heritage, particularly in the area of traditional crafts.  
Head Entity: capital city  
Tail Entity: vietnam  

Relation: country  
Context: the ancient ruins are a testament to the civilization that once thrived in this region, which is now part of a modern nation-state recognized for its diverse landscapes and vibrant culture.  
Head Entity: ancient ruins  
Tail Entity: peru  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: klaus kinski , who would play dracula himself nine years later in " nosferatu the vampyre " , is also featured in the film as renfield .
Head Entity: nosferatu the vampyre
Tail Entity: renfield
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: in the animated series "avatar: the last airbender," zuko is a complex character who struggles with his identity and family legacy.  
Head Entity: avatar: the last airbender  
Tail Entity: zuko  

Relation: characters  
Context: the novel "pride and prejudice" features elizabeth bennet, who navigates societal expectations and her feelings for mr. darcy.  
Head Entity: pride and prejudice  
Tail Entity: elizabeth bennet  
Mixup data size:  319
MixupTrain:  epoch  0, batch     0 | loss: 2.7093830MixupTrain:  epoch  0, batch     1 | loss: 2.7156600MixupTrain:  epoch  0, batch     2 | loss: 2.5914968MixupTrain:  epoch  0, batch     3 | loss: 2.6654668MixupTrain:  epoch  0, batch     4 | loss: 2.4410682MixupTrain:  epoch  0, batch     5 | loss: 2.9178127MixupTrain:  epoch  0, batch     6 | loss: 2.3942026MixupTrain:  epoch  0, batch     7 | loss: 2.4222939MixupTrain:  epoch  0, batch     8 | loss: 2.3014349MixupTrain:  epoch  0, batch     9 | loss: 2.7136576MixupTrain:  epoch  0, batch    10 | loss: 2.6443945MixupTrain:  epoch  0, batch    11 | loss: 2.4616343MixupTrain:  epoch  0, batch    12 | loss: 2.5601128MixupTrain:  epoch  0, batch    13 | loss: 2.3933904MixupTrain:  epoch  0, batch    14 | loss: 2.4151296MixupTrain:  epoch  0, batch    15 | loss: 2.8119059MixupTrain:  epoch  0, batch    16 | loss: 2.3716953MixupTrain:  epoch  0, batch    17 | loss: 2.2394528MixupTrain:  epoch  0, batch    18 | loss: 2.8241094MixupTrain:  epoch  0, batch    19 | loss: 2.6377483
MemoryTrain:  epoch  0, batch     0 | loss: 1.8410466MemoryTrain:  epoch  0, batch     1 | loss: 2.0126119MemoryTrain:  epoch  0, batch     2 | loss: 1.8668270MemoryTrain:  epoch  0, batch     3 | loss: 2.6142333MemoryTrain:  epoch  0, batch     4 | loss: 2.6826587MemoryTrain:  epoch  0, batch     5 | loss: 2.9328203MemoryTrain:  epoch  0, batch     6 | loss: 2.6575248MemoryTrain:  epoch  0, batch     7 | loss: 3.4312677MemoryTrain:  epoch  1, batch     0 | loss: 2.7740245MemoryTrain:  epoch  1, batch     1 | loss: 2.6404834MemoryTrain:  epoch  1, batch     2 | loss: 1.8300638MemoryTrain:  epoch  1, batch     3 | loss: 1.7116545MemoryTrain:  epoch  1, batch     4 | loss: 2.2231748MemoryTrain:  epoch  1, batch     5 | loss: 2.0295172MemoryTrain:  epoch  1, batch     6 | loss: 1.9596977MemoryTrain:  epoch  1, batch     7 | loss: 1.7952025MemoryTrain:  epoch  2, batch     0 | loss: 1.6223063MemoryTrain:  epoch  2, batch     1 | loss: 1.9169263MemoryTrain:  epoch  2, batch     2 | loss: 2.3925748MemoryTrain:  epoch  2, batch     3 | loss: 1.6100618MemoryTrain:  epoch  2, batch     4 | loss: 1.7031026MemoryTrain:  epoch  2, batch     5 | loss: 1.6330545MemoryTrain:  epoch  2, batch     6 | loss: 1.5888978MemoryTrain:  epoch  2, batch     7 | loss: 2.0898533MemoryTrain:  epoch  3, batch     0 | loss: 1.6790383MemoryTrain:  epoch  3, batch     1 | loss: 1.8168361MemoryTrain:  epoch  3, batch     2 | loss: 1.4837456MemoryTrain:  epoch  3, batch     3 | loss: 1.6579409MemoryTrain:  epoch  3, batch     4 | loss: 1.4045639MemoryTrain:  epoch  3, batch     5 | loss: 2.0479558MemoryTrain:  epoch  3, batch     6 | loss: 1.5610055MemoryTrain:  epoch  3, batch     7 | loss: 1.5666065MemoryTrain:  epoch  4, batch     0 | loss: 1.6134825MemoryTrain:  epoch  4, batch     1 | loss: 1.7199864MemoryTrain:  epoch  4, batch     2 | loss: 1.6060733MemoryTrain:  epoch  4, batch     3 | loss: 1.4231274MemoryTrain:  epoch  4, batch     4 | loss: 1.3968306MemoryTrain:  epoch  4, batch     5 | loss: 1.7745080MemoryTrain:  epoch  4, batch     6 | loss: 1.4074318MemoryTrain:  epoch  4, batch     7 | loss: 1.4003532MemoryTrain:  epoch  5, batch     0 | loss: 1.7466447MemoryTrain:  epoch  5, batch     1 | loss: 1.7288262MemoryTrain:  epoch  5, batch     2 | loss: 1.3804969MemoryTrain:  epoch  5, batch     3 | loss: 1.3359802MemoryTrain:  epoch  5, batch     4 | loss: 1.3006804MemoryTrain:  epoch  5, batch     5 | loss: 1.4697210MemoryTrain:  epoch  5, batch     6 | loss: 1.4323168MemoryTrain:  epoch  5, batch     7 | loss: 1.4547237MemoryTrain:  epoch  6, batch     0 | loss: 1.3319137MemoryTrain:  epoch  6, batch     1 | loss: 1.3896837MemoryTrain:  epoch  6, batch     2 | loss: 1.4307765MemoryTrain:  epoch  6, batch     3 | loss: 1.3000715MemoryTrain:  epoch  6, batch     4 | loss: 1.5331557MemoryTrain:  epoch  6, batch     5 | loss: 1.5859224MemoryTrain:  epoch  6, batch     6 | loss: 1.4290152MemoryTrain:  epoch  6, batch     7 | loss: 1.4957452MemoryTrain:  epoch  7, batch     0 | loss: 1.4343090MemoryTrain:  epoch  7, batch     1 | loss: 1.3734148MemoryTrain:  epoch  7, batch     2 | loss: 1.3856883MemoryTrain:  epoch  7, batch     3 | loss: 1.3953338MemoryTrain:  epoch  7, batch     4 | loss: 1.2989651MemoryTrain:  epoch  7, batch     5 | loss: 1.3586190MemoryTrain:  epoch  7, batch     6 | loss: 1.3636076MemoryTrain:  epoch  7, batch     7 | loss: 1.4075491MemoryTrain:  epoch  8, batch     0 | loss: 1.3312466MemoryTrain:  epoch  8, batch     1 | loss: 1.4029754MemoryTrain:  epoch  8, batch     2 | loss: 1.3453586MemoryTrain:  epoch  8, batch     3 | loss: 1.3630772MemoryTrain:  epoch  8, batch     4 | loss: 1.4010842MemoryTrain:  epoch  8, batch     5 | loss: 1.3475565MemoryTrain:  epoch  8, batch     6 | loss: 1.3609473MemoryTrain:  epoch  8, batch     7 | loss: 1.3143184MemoryTrain:  epoch  9, batch     0 | loss: 1.2897885MemoryTrain:  epoch  9, batch     1 | loss: 1.3503852MemoryTrain:  epoch  9, batch     2 | loss: 1.3236985MemoryTrain:  epoch  9, batch     3 | loss: 1.3521183MemoryTrain:  epoch  9, batch     4 | loss: 1.3263063MemoryTrain:  epoch  9, batch     5 | loss: 1.3623401MemoryTrain:  epoch  9, batch     6 | loss: 1.3913600MemoryTrain:  epoch  9, batch     7 | loss: 1.4986446
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 35.94%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 33.75%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 33.33%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 34.82%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 38.28%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 41.67%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 45.00%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 48.30%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 50.00%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 51.44%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 52.68%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 54.58%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 55.47%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 56.62%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 58.68%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 59.21%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 61.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 63.10%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 64.49%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 66.03%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 67.19%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 68.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 69.23%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 70.37%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 71.43%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 72.41%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 73.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 73.99%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 74.80%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 75.57%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 76.10%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 76.61%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 77.53%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 77.47%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 77.08%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 76.41%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 75.46%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 74.70%   [EVAL] batch:   42 | acc: 31.25%,  total acc: 73.69%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 74.01%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 74.58%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 74.86%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 75.40%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 75.65%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 75.77%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 76.12%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 76.23%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 76.08%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 75.94%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 75.93%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 76.14%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 75.67%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 75.33%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 75.54%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 75.53%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 75.62%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 75.82%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 76.11%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 75.60%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 80.56%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 79.38%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 78.98%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 80.77%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 81.70%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 84.72%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 85.20%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 85.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 86.01%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 86.08%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 85.33%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 85.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.06%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 86.57%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 86.83%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 87.07%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 88.97%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 89.11%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 89.36%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 89.47%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.74%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.24%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.48%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 90.41%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 90.48%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 90.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 90.76%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 90.56%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 90.36%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.56%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 90.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 90.56%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 90.68%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 90.86%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 90.68%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.74%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 90.57%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 89.98%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 89.72%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 89.79%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 89.75%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 89.42%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 88.79%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 87.70%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 86.83%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 85.80%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 84.79%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 84.28%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 83.61%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 83.57%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 83.27%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 83.07%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 82.96%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 82.94%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 82.92%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 83.06%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 83.12%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 82.93%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 83.07%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 83.05%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 83.26%   [EVAL] batch:   81 | acc: 100.00%,  total acc: 83.46%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 83.28%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 82.96%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 82.87%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 82.85%   [EVAL] batch:   86 | acc: 87.50%,  total acc: 82.90%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 83.03%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 83.22%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 83.40%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 83.76%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 83.94%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 84.04%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 84.21%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 84.54%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 84.69%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 84.85%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 85.15%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 85.23%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 85.13%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 85.16%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 85.24%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 85.20%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 84.81%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 84.14%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 83.66%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 83.01%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 82.71%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 82.25%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 82.02%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 82.07%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 82.23%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 82.27%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 82.32%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 82.42%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 82.35%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 81.67%   [EVAL] batch:  120 | acc: 25.00%,  total acc: 81.20%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 80.53%   [EVAL] batch:  122 | acc: 0.00%,  total acc: 79.88%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 79.28%   [EVAL] batch:  124 | acc: 0.00%,  total acc: 78.65%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 78.27%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 77.85%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 77.44%   [EVAL] batch:  128 | acc: 18.75%,  total acc: 76.99%   [EVAL] batch:  129 | acc: 31.25%,  total acc: 76.63%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 76.24%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 76.28%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 76.46%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 76.59%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 76.71%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 76.84%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 76.92%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 76.99%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 76.84%   [EVAL] batch:  139 | acc: 75.00%,  total acc: 76.83%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 76.91%   [EVAL] batch:  141 | acc: 81.25%,  total acc: 76.94%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 76.92%   [EVAL] batch:  143 | acc: 93.75%,  total acc: 77.04%   [EVAL] batch:  144 | acc: 75.00%,  total acc: 77.03%   [EVAL] batch:  145 | acc: 81.25%,  total acc: 77.05%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 77.17%   [EVAL] batch:  147 | acc: 87.50%,  total acc: 77.24%   [EVAL] batch:  148 | acc: 87.50%,  total acc: 77.31%   [EVAL] batch:  149 | acc: 87.50%,  total acc: 77.38%   [EVAL] batch:  150 | acc: 87.50%,  total acc: 77.44%   [EVAL] batch:  151 | acc: 81.25%,  total acc: 77.47%   [EVAL] batch:  152 | acc: 93.75%,  total acc: 77.57%   [EVAL] batch:  153 | acc: 100.00%,  total acc: 77.72%   [EVAL] batch:  154 | acc: 56.25%,  total acc: 77.58%   [EVAL] batch:  155 | acc: 75.00%,  total acc: 77.56%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 77.47%   [EVAL] batch:  157 | acc: 62.50%,  total acc: 77.37%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 77.24%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 77.15%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 77.06%   [EVAL] batch:  161 | acc: 50.00%,  total acc: 76.89%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 76.80%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 76.75%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 76.70%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 76.62%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 76.57%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 76.44%   [EVAL] batch:  169 | acc: 18.75%,  total acc: 76.10%   [EVAL] batch:  170 | acc: 18.75%,  total acc: 75.77%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 75.44%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 75.18%   [EVAL] batch:  173 | acc: 6.25%,  total acc: 74.78%   [EVAL] batch:  174 | acc: 31.25%,  total acc: 74.54%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 74.68%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 74.82%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 74.96%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 75.10%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 75.24%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 75.38%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 75.52%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 75.55%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 75.61%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 75.71%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 75.74%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 75.84%   [EVAL] batch:  187 | acc: 75.00%,  total acc: 75.83%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 75.60%   [EVAL] batch:  189 | acc: 37.50%,  total acc: 75.39%   [EVAL] batch:  190 | acc: 31.25%,  total acc: 75.16%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 74.87%   [EVAL] batch:  192 | acc: 43.75%,  total acc: 74.71%   [EVAL] batch:  193 | acc: 25.00%,  total acc: 74.45%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 74.42%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 74.30%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 74.30%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 74.34%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 74.31%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 74.31%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 74.28%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 74.26%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 74.23%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 74.26%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 74.30%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 74.30%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 74.40%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 74.52%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 74.64%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 74.73%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 74.82%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 74.94%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 75.12%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 75.23%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 75.35%   [EVAL] batch:  216 | acc: 93.75%,  total acc: 75.43%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 75.54%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 75.66%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 75.77%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 75.88%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 75.93%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 76.01%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 76.09%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 76.19%   [EVAL] batch:  225 | acc: 62.50%,  total acc: 76.13%   [EVAL] batch:  226 | acc: 43.75%,  total acc: 75.99%   [EVAL] batch:  227 | acc: 43.75%,  total acc: 75.85%   [EVAL] batch:  228 | acc: 56.25%,  total acc: 75.76%   [EVAL] batch:  229 | acc: 25.00%,  total acc: 75.54%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 75.49%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 75.57%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 75.64%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 75.72%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 75.82%   [EVAL] batch:  235 | acc: 75.00%,  total acc: 75.82%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 75.87%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 75.89%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 75.92%   [EVAL] batch:  239 | acc: 81.25%,  total acc: 75.94%   [EVAL] batch:  240 | acc: 62.50%,  total acc: 75.88%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 75.90%   [EVAL] batch:  242 | acc: 68.75%,  total acc: 75.87%   [EVAL] batch:  243 | acc: 50.00%,  total acc: 75.77%   [EVAL] batch:  244 | acc: 81.25%,  total acc: 75.79%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 75.81%   [EVAL] batch:  246 | acc: 68.75%,  total acc: 75.78%   [EVAL] batch:  247 | acc: 87.50%,  total acc: 75.83%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 75.90%   [EVAL] batch:  249 | acc: 87.50%,  total acc: 75.95%   
cur_acc:  ['0.9504', '0.7292', '0.7837', '0.7560']
his_acc:  ['0.9504', '0.8365', '0.7989', '0.7595']
CurrentTrain: epoch  0, batch     0 | loss: 4.8223076CurrentTrain: epoch  0, batch     1 | loss: 5.3611350CurrentTrain: epoch  0, batch     2 | loss: 5.0130448CurrentTrain: epoch  0, batch     3 | loss: 4.0575171CurrentTrain: epoch  1, batch     0 | loss: 4.1203651CurrentTrain: epoch  1, batch     1 | loss: 4.6077223CurrentTrain: epoch  1, batch     2 | loss: 3.4718969CurrentTrain: epoch  1, batch     3 | loss: 3.1729958CurrentTrain: epoch  2, batch     0 | loss: 3.1099045CurrentTrain: epoch  2, batch     1 | loss: 4.0885296CurrentTrain: epoch  2, batch     2 | loss: 2.7661858CurrentTrain: epoch  2, batch     3 | loss: 2.3237829CurrentTrain: epoch  3, batch     0 | loss: 2.7413671CurrentTrain: epoch  3, batch     1 | loss: 2.9035807CurrentTrain: epoch  3, batch     2 | loss: 3.6405764CurrentTrain: epoch  3, batch     3 | loss: 2.4494824CurrentTrain: epoch  4, batch     0 | loss: 2.7100191CurrentTrain: epoch  4, batch     1 | loss: 3.4337220CurrentTrain: epoch  4, batch     2 | loss: 2.6086485CurrentTrain: epoch  4, batch     3 | loss: 1.9734855CurrentTrain: epoch  5, batch     0 | loss: 2.2986608CurrentTrain: epoch  5, batch     1 | loss: 2.4027312CurrentTrain: epoch  5, batch     2 | loss: 3.2888880CurrentTrain: epoch  5, batch     3 | loss: 3.4413462CurrentTrain: epoch  6, batch     0 | loss: 2.6003413CurrentTrain: epoch  6, batch     1 | loss: 2.6483207CurrentTrain: epoch  6, batch     2 | loss: 2.3908007CurrentTrain: epoch  6, batch     3 | loss: 2.2232099CurrentTrain: epoch  7, batch     0 | loss: 2.1914020CurrentTrain: epoch  7, batch     1 | loss: 2.4241509CurrentTrain: epoch  7, batch     2 | loss: 2.1063590CurrentTrain: epoch  7, batch     3 | loss: 3.7779801CurrentTrain: epoch  8, batch     0 | loss: 2.2697587CurrentTrain: epoch  8, batch     1 | loss: 2.4878612CurrentTrain: epoch  8, batch     2 | loss: 1.9779222CurrentTrain: epoch  8, batch     3 | loss: 1.8752859CurrentTrain: epoch  9, batch     0 | loss: 2.2657077CurrentTrain: epoch  9, batch     1 | loss: 2.1037583CurrentTrain: epoch  9, batch     2 | loss: 2.0455456CurrentTrain: epoch  9, batch     3 | loss: 1.8790026
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: karl johan aarønes ( 8 may 1900 – 12 august 1969 ) was a norwegian politician for the labour party .
Head Entity: karl johan aarønes
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: elizabeth warren is a prominent member of the democratic party, advocating for progressive policies and reforms.  
Head Entity: elizabeth warren  
Tail Entity: democratic party  

Relation: member of political party  
Context: during his tenure, barack obama was a key figure in the democratic party, leading numerous initiatives and campaigns.  
Head Entity: barack obama  
Tail Entity: democratic party  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: it is a partly fictional biography , adapted by shane connaughton and jim sheridan from the book " my left foot " by christy brown .
Head Entity: my left foot
Tail Entity: christy brown
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film is based on the novel "the great gatsby" by f. scott fitzgerald, capturing the essence of the roaring twenties.  
Head Entity: the great gatsby  
Tail Entity: f. scott fitzgerald  

Relation: after a work by  
Context: this musical is inspired by the play "hamlet" written by william shakespeare, reimagining the classic tragedy in a contemporary setting.  
Head Entity: hamlet  
Tail Entity: william shakespeare  
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: several paintings are also mentioned , including rousseau 's " the dream " and van gogh 's " bedroom in arles " .
Head Entity: rousseau
Tail Entity: the dream
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: Among his many contributions to literature, one of his most acclaimed novels is "The Great Gatsby," which explores themes of wealth and social change in 1920s America.  
Head Entity: F. Scott Fitzgerald  
Tail Entity: The Great Gatsby  

Relation: notable work  
Context: The artist is best known for his groundbreaking sculpture "The Thinker," which has become a symbol of philosophy and contemplation.  
Head Entity: Auguste Rodin  
Tail Entity: The Thinker  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: at the request of the stockholm institute for bible translation , mirza khazar translated the bible – the new testament and old testament – into the azerbaijani language .
Head Entity: institute for bible translation
Tail Entity: stockholm
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: the global technology company, headquartered in san francisco, has been a leader in software development for over two decades.  
Head Entity: global technology company  
Tail Entity: san francisco  

Relation: headquarters location  
Context: after relocating its main office to new york city, the startup saw a significant increase in investment and growth opportunities.  
Head Entity: startup  
Tail Entity: new york city  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most recognized as the wolf, which belongs to the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of living organisms, the class Mammalia encompasses all mammals, which are further divided into various orders, including Primates.  
Head Entity: Mammalia  
Tail Entity: class  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: elspeth frances garman is professor of molecular biophysics at the university of oxford and a former president of the british crystallographic association .
Head Entity: elspeth frances garman
Tail Entity: molecular biophysics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: dr. amanda jones is a leading researcher in artificial intelligence and currently works at the mit media lab, focusing on machine learning applications.  
Head Entity: dr. amanda jones  
Tail Entity: artificial intelligence  

Relation: field of work  
Context: the world health organization has appointed dr. robert smith as the chief epidemiologist, where he will concentrate on infectious disease control strategies.  
Head Entity: dr. robert smith  
Tail Entity: epidemiology  
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: since march 2006 , wkxp 's programming has been simulcasted on 97.3 wzad wurtsboro , new york .
Head Entity: wzad
Tail Entity: wurtsboro , new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: in 2010, the station wxyz began broadcasting to the community of springfield, illinois.  
Head Entity: wxyz  
Tail Entity: springfield, illinois  

Relation: licensed to broadcast to  
Context: the radio station kqrs has been granted a license to broadcast to the area of minneapolis, minnesota since 1995.  
Head Entity: kqrs  
Tail Entity: minneapolis, minnesota  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: ngc 473 is a lenticular galaxy in the constellation of pisces .
Head Entity: ngc 473
Tail Entity: pisces
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: the andromeda galaxy is located in the constellation of andromeda .  
Head Entity: andromeda galaxy  
Tail Entity: andromeda  

Relation: constellation  
Context: orion is a prominent constellation that includes the famous orion nebula .  
Head Entity: orion  
Tail Entity: orion nebula  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: services to the island run from san juan 's luis muñoz marín international airport , ceiba airport or isla grande airport ( 20- to 30-minute flight ) .
Head Entity: isla grande airport
Tail Entity: san juan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The city of Barcelona is well-connected to the surrounding regions through the El Prat Airport, which offers numerous flights to various destinations.  
Head Entity: El Prat Airport  
Tail Entity: Barcelona  

Relation: place served by transport hub  
Context: The central train station in Zurich provides access to multiple Swiss cities and international destinations, making it a key transport hub for travelers.  
Head Entity: Zurich central train station  
Tail Entity: Zurich  
Mixup data size:  378
MixupTrain:  epoch  0, batch     0 | loss: 1.8533665MixupTrain:  epoch  0, batch     1 | loss: 2.0889524MixupTrain:  epoch  0, batch     2 | loss: 2.1256933MixupTrain:  epoch  0, batch     3 | loss: 1.5438188MixupTrain:  epoch  0, batch     4 | loss: 2.2173867MixupTrain:  epoch  0, batch     5 | loss: 1.6754899MixupTrain:  epoch  0, batch     6 | loss: 2.0166278MixupTrain:  epoch  0, batch     7 | loss: 1.9055466MixupTrain:  epoch  0, batch     8 | loss: 2.0379754MixupTrain:  epoch  0, batch     9 | loss: 1.7220141MixupTrain:  epoch  0, batch    10 | loss: 1.8434102MixupTrain:  epoch  0, batch    11 | loss: 1.6135109MixupTrain:  epoch  0, batch    12 | loss: 1.8469789MixupTrain:  epoch  0, batch    13 | loss: 2.1652031MixupTrain:  epoch  0, batch    14 | loss: 1.9532746MixupTrain:  epoch  0, batch    15 | loss: 1.7468739MixupTrain:  epoch  0, batch    16 | loss: 2.1104992MixupTrain:  epoch  0, batch    17 | loss: 1.8350586MixupTrain:  epoch  0, batch    18 | loss: 1.9100762MixupTrain:  epoch  0, batch    19 | loss: 1.7556332MixupTrain:  epoch  0, batch    20 | loss: 1.8022458MixupTrain:  epoch  0, batch    21 | loss: 1.7228690MixupTrain:  epoch  0, batch    22 | loss: 1.7166914MixupTrain:  epoch  0, batch    23 | loss: 1.4197773
MemoryTrain:  epoch  0, batch     0 | loss: 1.8386525MemoryTrain:  epoch  0, batch     1 | loss: 1.3816184MemoryTrain:  epoch  0, batch     2 | loss: 1.8415029MemoryTrain:  epoch  0, batch     3 | loss: 2.3542609MemoryTrain:  epoch  0, batch     4 | loss: 1.7080548MemoryTrain:  epoch  0, batch     5 | loss: 1.6246917MemoryTrain:  epoch  0, batch     6 | loss: 2.2540672MemoryTrain:  epoch  0, batch     7 | loss: 2.3504667MemoryTrain:  epoch  0, batch     8 | loss: 2.1195185MemoryTrain:  epoch  0, batch     9 | loss: 1.6660160MemoryTrain:  epoch  1, batch     0 | loss: 1.6026447MemoryTrain:  epoch  1, batch     1 | loss: 1.9645896MemoryTrain:  epoch  1, batch     2 | loss: 1.5941978MemoryTrain:  epoch  1, batch     3 | loss: 2.0473518MemoryTrain:  epoch  1, batch     4 | loss: 1.5175172MemoryTrain:  epoch  1, batch     5 | loss: 1.3941249MemoryTrain:  epoch  1, batch     6 | loss: 1.6963313MemoryTrain:  epoch  1, batch     7 | loss: 1.4659173MemoryTrain:  epoch  1, batch     8 | loss: 1.5376760MemoryTrain:  epoch  1, batch     9 | loss: 1.4781928MemoryTrain:  epoch  2, batch     0 | loss: 1.2752690MemoryTrain:  epoch  2, batch     1 | loss: 1.4079394MemoryTrain:  epoch  2, batch     2 | loss: 1.5233701MemoryTrain:  epoch  2, batch     3 | loss: 1.2772481MemoryTrain:  epoch  2, batch     4 | loss: 1.5761870MemoryTrain:  epoch  2, batch     5 | loss: 1.5224607MemoryTrain:  epoch  2, batch     6 | loss: 1.5896118MemoryTrain:  epoch  2, batch     7 | loss: 1.3489614MemoryTrain:  epoch  2, batch     8 | loss: 1.4913443MemoryTrain:  epoch  2, batch     9 | loss: 1.2806466MemoryTrain:  epoch  3, batch     0 | loss: 1.5203180MemoryTrain:  epoch  3, batch     1 | loss: 1.3920834MemoryTrain:  epoch  3, batch     2 | loss: 1.3540325MemoryTrain:  epoch  3, batch     3 | loss: 1.3315237MemoryTrain:  epoch  3, batch     4 | loss: 1.2630980MemoryTrain:  epoch  3, batch     5 | loss: 1.2650563MemoryTrain:  epoch  3, batch     6 | loss: 1.3229454MemoryTrain:  epoch  3, batch     7 | loss: 1.2815658MemoryTrain:  epoch  3, batch     8 | loss: 1.2901491MemoryTrain:  epoch  3, batch     9 | loss: 1.3504566MemoryTrain:  epoch  4, batch     0 | loss: 1.3220836MemoryTrain:  epoch  4, batch     1 | loss: 1.2854567MemoryTrain:  epoch  4, batch     2 | loss: 1.3789845MemoryTrain:  epoch  4, batch     3 | loss: 1.2274270MemoryTrain:  epoch  4, batch     4 | loss: 1.2570465MemoryTrain:  epoch  4, batch     5 | loss: 1.3251865MemoryTrain:  epoch  4, batch     6 | loss: 1.2533188MemoryTrain:  epoch  4, batch     7 | loss: 1.2849641MemoryTrain:  epoch  4, batch     8 | loss: 1.3297249MemoryTrain:  epoch  4, batch     9 | loss: 1.5059061MemoryTrain:  epoch  5, batch     0 | loss: 1.2487946MemoryTrain:  epoch  5, batch     1 | loss: 1.4392450MemoryTrain:  epoch  5, batch     2 | loss: 1.2648513MemoryTrain:  epoch  5, batch     3 | loss: 1.2695554MemoryTrain:  epoch  5, batch     4 | loss: 1.2575306MemoryTrain:  epoch  5, batch     5 | loss: 1.2689327MemoryTrain:  epoch  5, batch     6 | loss: 1.3378996MemoryTrain:  epoch  5, batch     7 | loss: 1.2612804MemoryTrain:  epoch  5, batch     8 | loss: 1.2484417MemoryTrain:  epoch  5, batch     9 | loss: 1.2276440MemoryTrain:  epoch  6, batch     0 | loss: 1.2391684MemoryTrain:  epoch  6, batch     1 | loss: 1.2831783MemoryTrain:  epoch  6, batch     2 | loss: 1.2476530MemoryTrain:  epoch  6, batch     3 | loss: 1.2468443MemoryTrain:  epoch  6, batch     4 | loss: 1.2016845MemoryTrain:  epoch  6, batch     5 | loss: 1.2269790MemoryTrain:  epoch  6, batch     6 | loss: 1.2582821MemoryTrain:  epoch  6, batch     7 | loss: 1.3209422MemoryTrain:  epoch  6, batch     8 | loss: 1.2550737MemoryTrain:  epoch  6, batch     9 | loss: 1.2689773MemoryTrain:  epoch  7, batch     0 | loss: 1.1881835MemoryTrain:  epoch  7, batch     1 | loss: 1.2225680MemoryTrain:  epoch  7, batch     2 | loss: 1.2418671MemoryTrain:  epoch  7, batch     3 | loss: 1.2252927MemoryTrain:  epoch  7, batch     4 | loss: 1.2825809MemoryTrain:  epoch  7, batch     5 | loss: 1.2538298MemoryTrain:  epoch  7, batch     6 | loss: 1.2687328MemoryTrain:  epoch  7, batch     7 | loss: 1.1893215MemoryTrain:  epoch  7, batch     8 | loss: 1.2367935MemoryTrain:  epoch  7, batch     9 | loss: 1.1934988MemoryTrain:  epoch  8, batch     0 | loss: 1.2478931MemoryTrain:  epoch  8, batch     1 | loss: 1.2654270MemoryTrain:  epoch  8, batch     2 | loss: 1.2178793MemoryTrain:  epoch  8, batch     3 | loss: 1.2187862MemoryTrain:  epoch  8, batch     4 | loss: 1.2396941MemoryTrain:  epoch  8, batch     5 | loss: 1.2343235MemoryTrain:  epoch  8, batch     6 | loss: 1.2481370MemoryTrain:  epoch  8, batch     7 | loss: 1.2167851MemoryTrain:  epoch  8, batch     8 | loss: 1.2710469MemoryTrain:  epoch  8, batch     9 | loss: 1.2363203MemoryTrain:  epoch  9, batch     0 | loss: 1.2620463MemoryTrain:  epoch  9, batch     1 | loss: 1.2043231MemoryTrain:  epoch  9, batch     2 | loss: 1.2327919MemoryTrain:  epoch  9, batch     3 | loss: 1.2179344MemoryTrain:  epoch  9, batch     4 | loss: 1.2248032MemoryTrain:  epoch  9, batch     5 | loss: 1.1987530MemoryTrain:  epoch  9, batch     6 | loss: 1.2260082MemoryTrain:  epoch  9, batch     7 | loss: 1.2258146MemoryTrain:  epoch  9, batch     8 | loss: 1.2332630MemoryTrain:  epoch  9, batch     9 | loss: 1.2173482
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 95.54%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 93.75%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 94.44%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 89.17%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 88.28%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 86.51%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 85.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 86.31%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 86.36%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 85.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.58%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.07%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.90%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 87.70%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 86.95%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 86.79%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 86.28%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 85.47%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 85.03%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 86.13%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 86.77%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 86.93%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 87.22%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 87.77%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 88.02%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.27%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 88.50%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 88.48%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 88.46%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 88.56%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 88.66%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 88.52%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 88.62%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 88.60%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 88.69%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 88.77%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 88.96%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 89.14%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 89.11%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 88.49%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 73.44%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 68.18%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 65.62%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 65.87%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 67.86%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 69.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 71.48%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 72.79%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 73.96%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 75.33%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 75.31%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 75.60%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 75.28%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 74.73%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 75.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.20%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 77.68%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.23%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.64%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.27%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 80.87%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 81.43%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 81.79%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 82.12%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 82.60%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 82.89%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.15%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 84.74%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 84.94%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 84.72%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 84.78%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 84.44%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 84.24%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 84.44%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 84.12%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 84.07%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 84.01%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 84.20%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 84.03%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 83.52%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 83.59%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 82.87%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 82.73%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 82.68%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 82.46%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 82.04%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 81.25%   [EVAL] batch:   64 | acc: 43.75%,  total acc: 80.67%   [EVAL] batch:   65 | acc: 25.00%,  total acc: 79.83%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 79.20%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 78.86%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 78.17%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 77.82%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 77.60%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 77.57%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 77.62%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 77.67%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 77.88%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 78.08%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 77.96%   [EVAL] batch:   78 | acc: 100.00%,  total acc: 78.24%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 78.36%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 78.55%   [EVAL] batch:   81 | acc: 93.75%,  total acc: 78.73%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 78.54%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 78.20%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 78.16%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 78.20%   [EVAL] batch:   86 | acc: 87.50%,  total acc: 78.30%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 78.48%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 78.65%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 78.89%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 79.12%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 79.35%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 79.57%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 79.72%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 79.93%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 80.14%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 80.35%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 80.55%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 80.74%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 80.94%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 81.13%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 81.19%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 81.37%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 81.37%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 80.96%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 80.38%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 79.93%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 79.32%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 78.94%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 78.52%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 78.32%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 78.40%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 78.59%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 78.66%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 78.74%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 78.87%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 78.83%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 78.18%   [EVAL] batch:  120 | acc: 18.75%,  total acc: 77.69%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 77.05%   [EVAL] batch:  122 | acc: 0.00%,  total acc: 76.42%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 75.86%   [EVAL] batch:  124 | acc: 0.00%,  total acc: 75.25%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 75.10%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 74.70%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 74.32%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 73.98%   [EVAL] batch:  129 | acc: 31.25%,  total acc: 73.65%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 73.28%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 73.34%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 73.54%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 73.65%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 73.70%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 73.81%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 74.00%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 74.05%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 73.92%   [EVAL] batch:  139 | acc: 75.00%,  total acc: 73.93%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 73.98%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 74.08%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 74.04%   [EVAL] batch:  143 | acc: 93.75%,  total acc: 74.18%   [EVAL] batch:  144 | acc: 68.75%,  total acc: 74.14%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 74.14%   [EVAL] batch:  146 | acc: 87.50%,  total acc: 74.23%   [EVAL] batch:  147 | acc: 81.25%,  total acc: 74.28%   [EVAL] batch:  148 | acc: 75.00%,  total acc: 74.29%   [EVAL] batch:  149 | acc: 81.25%,  total acc: 74.33%   [EVAL] batch:  150 | acc: 81.25%,  total acc: 74.38%   [EVAL] batch:  151 | acc: 87.50%,  total acc: 74.47%   [EVAL] batch:  152 | acc: 93.75%,  total acc: 74.59%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 74.72%   [EVAL] batch:  154 | acc: 81.25%,  total acc: 74.76%   [EVAL] batch:  155 | acc: 62.50%,  total acc: 74.68%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 74.60%   [EVAL] batch:  157 | acc: 68.75%,  total acc: 74.56%   [EVAL] batch:  158 | acc: 68.75%,  total acc: 74.53%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 74.45%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 74.38%   [EVAL] batch:  161 | acc: 43.75%,  total acc: 74.19%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 74.12%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 74.05%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 74.05%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 74.02%   [EVAL] batch:  166 | acc: 81.25%,  total acc: 74.06%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 74.07%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 73.96%   [EVAL] batch:  169 | acc: 18.75%,  total acc: 73.64%   [EVAL] batch:  170 | acc: 18.75%,  total acc: 73.32%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 73.00%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 72.76%   [EVAL] batch:  173 | acc: 6.25%,  total acc: 72.38%   [EVAL] batch:  174 | acc: 25.00%,  total acc: 72.11%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 72.42%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 72.58%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 72.73%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 72.88%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 73.03%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 73.18%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 73.26%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 73.37%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 73.48%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 73.56%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 73.66%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 73.57%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 73.35%   [EVAL] batch:  189 | acc: 50.00%,  total acc: 73.22%   [EVAL] batch:  190 | acc: 25.00%,  total acc: 72.97%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 72.72%   [EVAL] batch:  192 | acc: 50.00%,  total acc: 72.60%   [EVAL] batch:  193 | acc: 25.00%,  total acc: 72.36%   [EVAL] batch:  194 | acc: 62.50%,  total acc: 72.31%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 72.19%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 72.21%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 72.22%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 72.24%   [EVAL] batch:  199 | acc: 68.75%,  total acc: 72.22%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 72.23%   [EVAL] batch:  201 | acc: 50.00%,  total acc: 72.12%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 72.08%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 72.12%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 72.13%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 72.12%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 72.22%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 72.36%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 72.49%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 72.59%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 72.69%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 72.82%   [EVAL] batch:  212 | acc: 81.25%,  total acc: 72.86%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 72.98%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 73.11%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 73.23%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 73.36%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 73.48%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 73.60%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 73.72%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 73.84%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 73.93%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 74.02%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 74.11%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:  225 | acc: 62.50%,  total acc: 74.17%   [EVAL] batch:  226 | acc: 43.75%,  total acc: 74.04%   [EVAL] batch:  227 | acc: 43.75%,  total acc: 73.90%   [EVAL] batch:  228 | acc: 62.50%,  total acc: 73.85%   [EVAL] batch:  229 | acc: 31.25%,  total acc: 73.67%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 73.62%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 73.71%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 73.79%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 73.88%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 73.99%   [EVAL] batch:  235 | acc: 75.00%,  total acc: 73.99%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 74.05%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 74.05%   [EVAL] batch:  238 | acc: 75.00%,  total acc: 74.06%   [EVAL] batch:  239 | acc: 81.25%,  total acc: 74.09%   [EVAL] batch:  240 | acc: 62.50%,  total acc: 74.04%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 74.04%   [EVAL] batch:  242 | acc: 68.75%,  total acc: 74.02%   [EVAL] batch:  243 | acc: 50.00%,  total acc: 73.92%   [EVAL] batch:  244 | acc: 87.50%,  total acc: 73.98%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 74.01%   [EVAL] batch:  246 | acc: 68.75%,  total acc: 73.99%   [EVAL] batch:  247 | acc: 87.50%,  total acc: 74.04%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 74.12%   [EVAL] batch:  249 | acc: 75.00%,  total acc: 74.12%   [EVAL] batch:  250 | acc: 100.00%,  total acc: 74.23%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 74.31%   [EVAL] batch:  252 | acc: 100.00%,  total acc: 74.41%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 74.46%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 74.53%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 74.63%   [EVAL] batch:  256 | acc: 93.75%,  total acc: 74.71%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 74.73%   [EVAL] batch:  258 | acc: 100.00%,  total acc: 74.83%   [EVAL] batch:  259 | acc: 87.50%,  total acc: 74.88%   [EVAL] batch:  260 | acc: 62.50%,  total acc: 74.83%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 74.83%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 74.90%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 74.93%   [EVAL] batch:  264 | acc: 87.50%,  total acc: 74.98%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 74.98%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 74.98%   [EVAL] batch:  267 | acc: 87.50%,  total acc: 75.02%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:  269 | acc: 68.75%,  total acc: 74.98%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 75.07%   [EVAL] batch:  271 | acc: 87.50%,  total acc: 75.11%   [EVAL] batch:  272 | acc: 75.00%,  total acc: 75.11%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 75.16%   [EVAL] batch:  274 | acc: 62.50%,  total acc: 75.11%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 75.20%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 75.29%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 75.38%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 75.47%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 75.56%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 75.65%   [EVAL] batch:  281 | acc: 81.25%,  total acc: 75.66%   [EVAL] batch:  282 | acc: 81.25%,  total acc: 75.68%   [EVAL] batch:  283 | acc: 68.75%,  total acc: 75.66%   [EVAL] batch:  284 | acc: 81.25%,  total acc: 75.68%   [EVAL] batch:  285 | acc: 68.75%,  total acc: 75.66%   [EVAL] batch:  286 | acc: 56.25%,  total acc: 75.59%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 75.56%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 75.65%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 75.73%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 75.82%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 75.90%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 75.98%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 76.04%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 76.12%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 76.20%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 76.28%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 76.36%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 76.44%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 76.52%   [EVAL] batch:  300 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:  301 | acc: 87.50%,  total acc: 76.59%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 76.65%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 76.71%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 76.72%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 76.78%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 76.81%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 76.87%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 76.92%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 77.00%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 77.07%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 77.10%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 77.02%   
cur_acc:  ['0.9504', '0.7292', '0.7837', '0.7560', '0.8849']
his_acc:  ['0.9504', '0.8365', '0.7989', '0.7595', '0.7702']
CurrentTrain: epoch  0, batch     0 | loss: 6.1029167CurrentTrain: epoch  0, batch     1 | loss: 5.7577362CurrentTrain: epoch  0, batch     2 | loss: 7.4437304CurrentTrain: epoch  0, batch     3 | loss: 6.9733205CurrentTrain: epoch  1, batch     0 | loss: 5.7423277CurrentTrain: epoch  1, batch     1 | loss: 5.1051149CurrentTrain: epoch  1, batch     2 | loss: 5.7528906CurrentTrain: epoch  1, batch     3 | loss: 4.2325125CurrentTrain: epoch  2, batch     0 | loss: 4.9514961CurrentTrain: epoch  2, batch     1 | loss: 4.8044291CurrentTrain: epoch  2, batch     2 | loss: 4.8925900CurrentTrain: epoch  2, batch     3 | loss: 3.2841611CurrentTrain: epoch  3, batch     0 | loss: 4.2007971CurrentTrain: epoch  3, batch     1 | loss: 4.5646095CurrentTrain: epoch  3, batch     2 | loss: 4.0886412CurrentTrain: epoch  3, batch     3 | loss: 5.0487504CurrentTrain: epoch  4, batch     0 | loss: 3.5402589CurrentTrain: epoch  4, batch     1 | loss: 4.0296268CurrentTrain: epoch  4, batch     2 | loss: 4.3995657CurrentTrain: epoch  4, batch     3 | loss: 3.4609623CurrentTrain: epoch  5, batch     0 | loss: 3.5051618CurrentTrain: epoch  5, batch     1 | loss: 3.3711050CurrentTrain: epoch  5, batch     2 | loss: 4.1661053CurrentTrain: epoch  5, batch     3 | loss: 3.5906487CurrentTrain: epoch  6, batch     0 | loss: 3.8339570CurrentTrain: epoch  6, batch     1 | loss: 3.1134481CurrentTrain: epoch  6, batch     2 | loss: 3.0277801CurrentTrain: epoch  6, batch     3 | loss: 5.3454399CurrentTrain: epoch  7, batch     0 | loss: 3.9818099CurrentTrain: epoch  7, batch     1 | loss: 3.2232409CurrentTrain: epoch  7, batch     2 | loss: 2.6447132CurrentTrain: epoch  7, batch     3 | loss: 2.1658664CurrentTrain: epoch  8, batch     0 | loss: 2.4938564CurrentTrain: epoch  8, batch     1 | loss: 2.7709484CurrentTrain: epoch  8, batch     2 | loss: 3.4580560CurrentTrain: epoch  8, batch     3 | loss: 4.3225703CurrentTrain: epoch  9, batch     0 | loss: 3.0702305CurrentTrain: epoch  9, batch     1 | loss: 3.1620474CurrentTrain: epoch  9, batch     2 | loss: 2.6380949CurrentTrain: epoch  9, batch     3 | loss: 2.8357472
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: the andesite line , a zone of intense volcanic and seismic activity , is a major regional distinction in the pacific .
Head Entity: andesite line
Tail Entity: pacific
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The Great Barrier Reef, a stunning natural wonder, stretches along the northeastern coast of Australia.  
Head Entity: Great Barrier Reef  
Tail Entity: Australia  

Relation: located on terrain feature  
Context: Mount Everest, the highest peak in the world, is part of the Himalayas and attracts climbers from all over the globe.  
Head Entity: Mount Everest  
Tail Entity: Himalayas  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, also known as cop26, took place in glasgow, scotland, where numerous world leaders gathered to discuss climate action.  
Head Entity: united nations climate change conference  
Tail Entity: cop26  

Relation: participant of  
Context: elon musk, the ceo of spacex, was a key participant in the 2021 met gala, showcasing his vision for sustainable energy and space exploration.  
Head Entity: elon musk  
Tail Entity: 2021 met gala  
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: luna subsequently had a supporting role in the comedy film " gravy " , directed by james roday , which was released in october 2015 .
Head Entity: gravy
Tail Entity: james roday
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: the acclaimed director christopher nolan is known for his work on the blockbuster film "inception," which was released in 2010.  
Head Entity: inception  
Tail Entity: christopher nolan  

Relation: director  
Context: in the recent documentary "the last dance," the legendary coach phil jackson is featured prominently, showcasing his role as the director of the chicago bulls' success in the 1990s.  
Head Entity: the last dance  
Tail Entity: phil jackson  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: the station is owned by the hearst television subsidiary of the hearst corporation .
Head Entity: hearst television
Tail Entity: hearst corporation
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: the famous painting is owned by the metropolitan museum of art in new york.  
Head Entity: metropolitan museum of art  
Tail Entity: famous painting  

Relation: owned by  
Context: the luxury car brand is owned by a major automotive conglomerate based in germany.  
Head Entity: major automotive conglomerate  
Tail Entity: luxury car brand  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: remains of taxa that can be confidently assigned to istiodactylidae have been found in the uk and china , in rocks dating from the early cretaceous period ( barremian to aptian stage ) .
Head Entity: barremian
Tail Entity: early cretaceous
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The Great Wall of China is a series of fortifications made of various materials, and it is part of the cultural heritage of the country.  
Head Entity: Great Wall of China  
Tail Entity: cultural heritage  

Relation: part of  
Context: The human heart is a vital organ that is part of the circulatory system, responsible for pumping blood throughout the body.  
Head Entity: human heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: it served as the home of k league side gyeongnam fc and national league side changwon city fc prior to the construction of the changwon football center in 2009 .
Head Entity: changwon football center
Tail Entity: gyeongnam fc
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was once the residence of the famous artist, and it now serves as a gallery showcasing his works, attracting visitors from all over the world.  
Head Entity: historic building  
Tail Entity: famous artist  

Relation: occupant  
Context: After the renovation, the old factory was transformed into a vibrant co-working space, where numerous startups and freelancers now thrive.  
Head Entity: old factory  
Tail Entity: co-working space  
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: reading town hall was built in several phases between 1786 and 1897 , although the principal facade was designed by alfred waterhouse in 1875 .
Head Entity: reading town hall
Tail Entity: alfred waterhouse
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: the new art museum was designed by renowned architect zaha hadid, whose innovative style has transformed modern architecture.  
Head Entity: new art museum  
Tail Entity: zaha hadid  

Relation: architect  
Context: the iconic sydney opera house was the brainchild of architect jørn utzon, who won the design competition in 1957.  
Head Entity: sydney opera house  
Tail Entity: jørn utzon  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: fantcha was born as francelina durão almeida in mindelo on são vicente island , cape verde .
Head Entity: fantcha
Tail Entity: são vicente island
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: after living in new york city for several years, john decided to move back to his hometown in los angeles.  
Head Entity: john  
Tail Entity: los angeles  

Relation: residence  
Context: the famous author spent most of her life in a quaint cottage located in the picturesque village of haworth, england.  
Head Entity: the famous author  
Tail Entity: haworth
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Ludwig van Beethoven created some of the most celebrated symphonies in classical music history.  
Head Entity: symphonies  
Tail Entity: Ludwig van Beethoven  

Relation: composer  
Context: Wolfgang Amadeus Mozart was a prolific and influential composer of the classical era, known for his operas and symphonies.  
Head Entity: operas  
Tail Entity: Wolfgang Amadeus Mozart  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: one polish archer qualified for the women 's individual recurve by obtaining one of the eight olympic places available from the 2015 world archery championships in copenhagen , denmark .
Head Entity: 2015 world archery championships
Tail Entity: copenhagen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: The annual music festival will take place in the vibrant city of Austin, Texas, attracting thousands of fans from around the world.  
Head Entity: annual music festival  
Tail Entity: Austin, Texas  

Relation: location  
Context: The historic battle was fought near the banks of the river at Gettysburg, Pennsylvania, which is now a national park.  
Head Entity: historic battle  
Tail Entity: Gettysburg, Pennsylvania  
Mixup data size:  439
MixupTrain:  epoch  0, batch     0 | loss: 1.9741904MixupTrain:  epoch  0, batch     1 | loss: 1.4795069MixupTrain:  epoch  0, batch     2 | loss: 2.0525545MixupTrain:  epoch  0, batch     3 | loss: 1.6983486MixupTrain:  epoch  0, batch     4 | loss: 2.0444021MixupTrain:  epoch  0, batch     5 | loss: 2.1901500MixupTrain:  epoch  0, batch     6 | loss: 2.8706031MixupTrain:  epoch  0, batch     7 | loss: 2.1424449MixupTrain:  epoch  0, batch     8 | loss: 2.1165901MixupTrain:  epoch  0, batch     9 | loss: 1.7439923MixupTrain:  epoch  0, batch    10 | loss: 2.3087063MixupTrain:  epoch  0, batch    11 | loss: 2.5276628MixupTrain:  epoch  0, batch    12 | loss: 1.9122717MixupTrain:  epoch  0, batch    13 | loss: 2.0458331MixupTrain:  epoch  0, batch    14 | loss: 1.8177348MixupTrain:  epoch  0, batch    15 | loss: 2.3204754MixupTrain:  epoch  0, batch    16 | loss: 2.2098050MixupTrain:  epoch  0, batch    17 | loss: 1.8750097MixupTrain:  epoch  0, batch    18 | loss: 1.9471463MixupTrain:  epoch  0, batch    19 | loss: 1.8132532MixupTrain:  epoch  0, batch    20 | loss: 2.0669306MixupTrain:  epoch  0, batch    21 | loss: 1.7368139MixupTrain:  epoch  0, batch    22 | loss: 1.8494176MixupTrain:  epoch  0, batch    23 | loss: 1.6904473MixupTrain:  epoch  0, batch    24 | loss: 1.9056162MixupTrain:  epoch  0, batch    25 | loss: 2.0157973MixupTrain:  epoch  0, batch    26 | loss: 1.4745872MixupTrain:  epoch  0, batch    27 | loss: 1.8632508
MemoryTrain:  epoch  0, batch     0 | loss: 1.8046240MemoryTrain:  epoch  0, batch     1 | loss: 2.2464781MemoryTrain:  epoch  0, batch     2 | loss: 1.6192845MemoryTrain:  epoch  0, batch     3 | loss: 2.1276004MemoryTrain:  epoch  0, batch     4 | loss: 2.2049046MemoryTrain:  epoch  0, batch     5 | loss: 1.5223143MemoryTrain:  epoch  0, batch     6 | loss: 2.6810064MemoryTrain:  epoch  0, batch     7 | loss: 2.1057353MemoryTrain:  epoch  0, batch     8 | loss: 2.2141230MemoryTrain:  epoch  0, batch     9 | loss: 2.1586304MemoryTrain:  epoch  0, batch    10 | loss: 2.0118659MemoryTrain:  epoch  0, batch    11 | loss: 3.4816933MemoryTrain:  epoch  1, batch     0 | loss: 1.5859842MemoryTrain:  epoch  1, batch     1 | loss: 1.8574709MemoryTrain:  epoch  1, batch     2 | loss: 2.0679531MemoryTrain:  epoch  1, batch     3 | loss: 2.0179951MemoryTrain:  epoch  1, batch     4 | loss: 1.3630953MemoryTrain:  epoch  1, batch     5 | loss: 1.3967502MemoryTrain:  epoch  1, batch     6 | loss: 2.0654230MemoryTrain:  epoch  1, batch     7 | loss: 2.6063564MemoryTrain:  epoch  1, batch     8 | loss: 2.0801907MemoryTrain:  epoch  1, batch     9 | loss: 1.2369090MemoryTrain:  epoch  1, batch    10 | loss: 2.5329385MemoryTrain:  epoch  1, batch    11 | loss: 1.2415544MemoryTrain:  epoch  2, batch     0 | loss: 1.6339937MemoryTrain:  epoch  2, batch     1 | loss: 1.3010019MemoryTrain:  epoch  2, batch     2 | loss: 1.3924842MemoryTrain:  epoch  2, batch     3 | loss: 1.4510869MemoryTrain:  epoch  2, batch     4 | loss: 1.8713889MemoryTrain:  epoch  2, batch     5 | loss: 1.6031129MemoryTrain:  epoch  2, batch     6 | loss: 1.6899146MemoryTrain:  epoch  2, batch     7 | loss: 2.0090375MemoryTrain:  epoch  2, batch     8 | loss: 1.5284559MemoryTrain:  epoch  2, batch     9 | loss: 2.0950136MemoryTrain:  epoch  2, batch    10 | loss: 1.5440346MemoryTrain:  epoch  2, batch    11 | loss: 1.3415613MemoryTrain:  epoch  3, batch     0 | loss: 1.5136600MemoryTrain:  epoch  3, batch     1 | loss: 1.2944572MemoryTrain:  epoch  3, batch     2 | loss: 1.4840068MemoryTrain:  epoch  3, batch     3 | loss: 1.4233851MemoryTrain:  epoch  3, batch     4 | loss: 1.3399845MemoryTrain:  epoch  3, batch     5 | loss: 1.9302585MemoryTrain:  epoch  3, batch     6 | loss: 1.9034870MemoryTrain:  epoch  3, batch     7 | loss: 1.4565995MemoryTrain:  epoch  3, batch     8 | loss: 1.2410479MemoryTrain:  epoch  3, batch     9 | loss: 1.8330204MemoryTrain:  epoch  3, batch    10 | loss: 1.4621735MemoryTrain:  epoch  3, batch    11 | loss: 1.2392044MemoryTrain:  epoch  4, batch     0 | loss: 1.3384173MemoryTrain:  epoch  4, batch     1 | loss: 1.2891817MemoryTrain:  epoch  4, batch     2 | loss: 1.3874276MemoryTrain:  epoch  4, batch     3 | loss: 1.5287910MemoryTrain:  epoch  4, batch     4 | loss: 1.7162242MemoryTrain:  epoch  4, batch     5 | loss: 1.3466878MemoryTrain:  epoch  4, batch     6 | loss: 1.7318659MemoryTrain:  epoch  4, batch     7 | loss: 1.3339859MemoryTrain:  epoch  4, batch     8 | loss: 1.2247522MemoryTrain:  epoch  4, batch     9 | loss: 1.6277674MemoryTrain:  epoch  4, batch    10 | loss: 1.5937595MemoryTrain:  epoch  4, batch    11 | loss: 1.3179646MemoryTrain:  epoch  5, batch     0 | loss: 1.3794069MemoryTrain:  epoch  5, batch     1 | loss: 1.4707139MemoryTrain:  epoch  5, batch     2 | loss: 1.4094187MemoryTrain:  epoch  5, batch     3 | loss: 1.3799773MemoryTrain:  epoch  5, batch     4 | loss: 1.2405831MemoryTrain:  epoch  5, batch     5 | loss: 1.4120612MemoryTrain:  epoch  5, batch     6 | loss: 1.7544172MemoryTrain:  epoch  5, batch     7 | loss: 1.4073215MemoryTrain:  epoch  5, batch     8 | loss: 1.2873132MemoryTrain:  epoch  5, batch     9 | loss: 1.2626255MemoryTrain:  epoch  5, batch    10 | loss: 1.4205430MemoryTrain:  epoch  5, batch    11 | loss: 1.4538071MemoryTrain:  epoch  6, batch     0 | loss: 1.5778335MemoryTrain:  epoch  6, batch     1 | loss: 1.2744081MemoryTrain:  epoch  6, batch     2 | loss: 1.3102164MemoryTrain:  epoch  6, batch     3 | loss: 1.5851693MemoryTrain:  epoch  6, batch     4 | loss: 1.2403460MemoryTrain:  epoch  6, batch     5 | loss: 1.3162071MemoryTrain:  epoch  6, batch     6 | loss: 1.2522116MemoryTrain:  epoch  6, batch     7 | loss: 1.2738706MemoryTrain:  epoch  6, batch     8 | loss: 1.5488675MemoryTrain:  epoch  6, batch     9 | loss: 1.2227082MemoryTrain:  epoch  6, batch    10 | loss: 1.4074478MemoryTrain:  epoch  6, batch    11 | loss: 1.2500660MemoryTrain:  epoch  7, batch     0 | loss: 1.2619056MemoryTrain:  epoch  7, batch     1 | loss: 1.3343227MemoryTrain:  epoch  7, batch     2 | loss: 1.2341481MemoryTrain:  epoch  7, batch     3 | loss: 1.2938203MemoryTrain:  epoch  7, batch     4 | loss: 1.3501067MemoryTrain:  epoch  7, batch     5 | loss: 1.2661740MemoryTrain:  epoch  7, batch     6 | loss: 1.4333436MemoryTrain:  epoch  7, batch     7 | loss: 1.3021066MemoryTrain:  epoch  7, batch     8 | loss: 1.2983724MemoryTrain:  epoch  7, batch     9 | loss: 1.6298870MemoryTrain:  epoch  7, batch    10 | loss: 1.2833877MemoryTrain:  epoch  7, batch    11 | loss: 1.2771255MemoryTrain:  epoch  8, batch     0 | loss: 1.3015845MemoryTrain:  epoch  8, batch     1 | loss: 1.3011535MemoryTrain:  epoch  8, batch     2 | loss: 1.3793939MemoryTrain:  epoch  8, batch     3 | loss: 1.4912385MemoryTrain:  epoch  8, batch     4 | loss: 1.2128223MemoryTrain:  epoch  8, batch     5 | loss: 1.2801809MemoryTrain:  epoch  8, batch     6 | loss: 1.3169866MemoryTrain:  epoch  8, batch     7 | loss: 1.3920193MemoryTrain:  epoch  8, batch     8 | loss: 1.2141907MemoryTrain:  epoch  8, batch     9 | loss: 1.1830668MemoryTrain:  epoch  8, batch    10 | loss: 1.2073253MemoryTrain:  epoch  8, batch    11 | loss: 1.1855195MemoryTrain:  epoch  9, batch     0 | loss: 1.2586488MemoryTrain:  epoch  9, batch     1 | loss: 1.2958820MemoryTrain:  epoch  9, batch     2 | loss: 1.2786059MemoryTrain:  epoch  9, batch     3 | loss: 1.3910336MemoryTrain:  epoch  9, batch     4 | loss: 1.3222286MemoryTrain:  epoch  9, batch     5 | loss: 1.1943285MemoryTrain:  epoch  9, batch     6 | loss: 1.2176301MemoryTrain:  epoch  9, batch     7 | loss: 1.1887783MemoryTrain:  epoch  9, batch     8 | loss: 1.2412250MemoryTrain:  epoch  9, batch     9 | loss: 1.2008082MemoryTrain:  epoch  9, batch    10 | loss: 1.4064522MemoryTrain:  epoch  9, batch    11 | loss: 1.2367580
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 41.67%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 45.31%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 46.25%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 48.96%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 53.57%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 57.03%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 61.11%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 63.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 64.77%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 66.67%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 68.27%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 73.83%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 74.63%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 75.69%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 75.33%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 72.81%   [EVAL] batch:   20 | acc: 25.00%,  total acc: 70.54%   [EVAL] batch:   21 | acc: 12.50%,  total acc: 67.90%   [EVAL] batch:   22 | acc: 25.00%,  total acc: 66.03%   [EVAL] batch:   23 | acc: 18.75%,  total acc: 64.06%   [EVAL] batch:   24 | acc: 18.75%,  total acc: 62.25%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 60.10%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 58.33%   [EVAL] batch:   27 | acc: 12.50%,  total acc: 56.70%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 54.96%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 53.54%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 51.81%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 52.34%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 53.79%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 54.78%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 56.07%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 56.94%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 57.60%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 58.55%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 59.29%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 60.31%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 60.98%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 61.46%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 62.35%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 63.07%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 63.75%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 64.27%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 64.63%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 64.97%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 65.43%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 65.75%   [EVAL] batch:   50 | acc: 31.25%,  total acc: 65.07%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 64.66%   [EVAL] batch:   52 | acc: 37.50%,  total acc: 64.15%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 64.00%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 63.75%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 63.73%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 63.27%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 63.15%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 62.71%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 62.81%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 62.60%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 62.90%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 62.40%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 73.61%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 71.88%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 68.75%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 66.15%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 66.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 70.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 73.53%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 74.65%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 75.99%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 75.31%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 75.89%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 75.85%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 75.54%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 76.04%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 76.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.40%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.24%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 78.79%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 79.31%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.65%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.82%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 82.35%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 82.68%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 82.64%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 83.11%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 83.39%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.81%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.22%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.60%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 84.97%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 85.03%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 85.23%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 85.28%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 85.19%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 84.71%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 84.31%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 84.00%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 83.82%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 83.89%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 84.08%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 83.68%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 83.18%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 83.26%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 83.00%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 82.54%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 82.52%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 82.71%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 82.68%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 82.46%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 82.14%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 81.45%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 80.77%   [EVAL] batch:   65 | acc: 31.25%,  total acc: 80.02%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 79.10%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 78.77%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 78.08%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 78.04%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 77.73%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 77.52%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 77.57%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 77.53%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 77.58%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 77.63%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 77.68%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 77.56%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 77.69%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 77.89%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 78.16%   [EVAL] batch:   81 | acc: 93.75%,  total acc: 78.35%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 78.24%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 78.05%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 78.01%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 78.05%   [EVAL] batch:   86 | acc: 87.50%,  total acc: 78.16%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 78.34%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 78.51%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 78.91%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 79.14%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 79.37%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 79.59%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 79.80%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 80.01%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 80.22%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 80.42%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 80.62%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 80.81%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 81.00%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 81.13%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 81.07%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 81.13%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 80.84%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 80.21%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 79.76%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 79.09%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 78.77%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 78.35%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 78.15%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 78.23%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 78.42%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 78.50%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 78.58%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 78.71%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 78.68%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 78.02%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 77.48%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 76.84%   [EVAL] batch:  122 | acc: 0.00%,  total acc: 76.22%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 75.66%   [EVAL] batch:  124 | acc: 0.00%,  total acc: 75.05%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 74.61%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 74.27%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 73.93%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 73.70%   [EVAL] batch:  130 | acc: 31.25%,  total acc: 73.38%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 73.44%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 73.64%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 73.74%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 73.80%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 73.90%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 74.09%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 74.18%   [EVAL] batch:  138 | acc: 50.00%,  total acc: 74.01%   [EVAL] batch:  139 | acc: 75.00%,  total acc: 74.02%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 74.07%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 74.16%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 74.17%   [EVAL] batch:  143 | acc: 81.25%,  total acc: 74.22%   [EVAL] batch:  144 | acc: 31.25%,  total acc: 73.92%   [EVAL] batch:  145 | acc: 31.25%,  total acc: 73.63%   [EVAL] batch:  146 | acc: 31.25%,  total acc: 73.34%   [EVAL] batch:  147 | acc: 25.00%,  total acc: 73.02%   [EVAL] batch:  148 | acc: 43.75%,  total acc: 72.82%   [EVAL] batch:  149 | acc: 12.50%,  total acc: 72.42%   [EVAL] batch:  150 | acc: 75.00%,  total acc: 72.43%   [EVAL] batch:  151 | acc: 75.00%,  total acc: 72.45%   [EVAL] batch:  152 | acc: 68.75%,  total acc: 72.43%   [EVAL] batch:  153 | acc: 81.25%,  total acc: 72.48%   [EVAL] batch:  154 | acc: 68.75%,  total acc: 72.46%   [EVAL] batch:  155 | acc: 62.50%,  total acc: 72.40%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 72.37%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 72.23%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 72.09%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 71.95%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 71.89%   [EVAL] batch:  161 | acc: 43.75%,  total acc: 71.72%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 71.70%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 71.65%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 71.67%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 71.65%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 71.67%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 71.69%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 71.60%   [EVAL] batch:  169 | acc: 12.50%,  total acc: 71.25%   [EVAL] batch:  170 | acc: 18.75%,  total acc: 70.94%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 70.64%   [EVAL] batch:  172 | acc: 25.00%,  total acc: 70.38%   [EVAL] batch:  173 | acc: 6.25%,  total acc: 70.01%   [EVAL] batch:  174 | acc: 25.00%,  total acc: 69.75%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 69.92%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 70.09%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 70.26%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 70.43%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 70.59%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 70.75%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 70.91%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 71.00%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 71.13%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 71.22%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 71.30%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 71.42%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 71.38%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 71.16%   [EVAL] batch:  189 | acc: 56.25%,  total acc: 71.09%   [EVAL] batch:  190 | acc: 25.00%,  total acc: 70.84%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 70.61%   [EVAL] batch:  192 | acc: 56.25%,  total acc: 70.53%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 70.26%   [EVAL] batch:  194 | acc: 62.50%,  total acc: 70.22%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 70.12%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 70.15%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 70.17%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 70.16%   [EVAL] batch:  199 | acc: 68.75%,  total acc: 70.16%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 70.21%   [EVAL] batch:  201 | acc: 50.00%,  total acc: 70.11%   [EVAL] batch:  202 | acc: 50.00%,  total acc: 70.01%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 70.07%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 70.06%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 70.05%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 70.14%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 70.28%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 70.42%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 70.54%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 70.65%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 70.78%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 70.80%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 70.94%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 71.08%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 71.21%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 71.34%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 71.47%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 71.60%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 71.73%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 71.86%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 71.96%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 72.06%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 72.15%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 72.28%   [EVAL] batch:  225 | acc: 43.75%,  total acc: 72.15%   [EVAL] batch:  226 | acc: 43.75%,  total acc: 72.03%   [EVAL] batch:  227 | acc: 31.25%,  total acc: 71.85%   [EVAL] batch:  228 | acc: 43.75%,  total acc: 71.72%   [EVAL] batch:  229 | acc: 31.25%,  total acc: 71.55%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 71.51%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 71.61%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 71.70%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 71.79%   [EVAL] batch:  234 | acc: 93.75%,  total acc: 71.89%   [EVAL] batch:  235 | acc: 81.25%,  total acc: 71.93%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 71.97%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 72.01%   [EVAL] batch:  238 | acc: 75.00%,  total acc: 72.02%   [EVAL] batch:  239 | acc: 37.50%,  total acc: 71.88%   [EVAL] batch:  240 | acc: 31.25%,  total acc: 71.71%   [EVAL] batch:  241 | acc: 68.75%,  total acc: 71.69%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 71.66%   [EVAL] batch:  243 | acc: 37.50%,  total acc: 71.52%   [EVAL] batch:  244 | acc: 68.75%,  total acc: 71.51%   [EVAL] batch:  245 | acc: 68.75%,  total acc: 71.49%   [EVAL] batch:  246 | acc: 62.50%,  total acc: 71.46%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 71.50%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 71.56%   [EVAL] batch:  249 | acc: 62.50%,  total acc: 71.53%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 71.61%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 71.70%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 71.79%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 71.85%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 71.94%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 72.05%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 72.03%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 72.04%   [EVAL] batch:  258 | acc: 62.50%,  total acc: 72.01%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 71.97%   [EVAL] batch:  260 | acc: 31.25%,  total acc: 71.82%   [EVAL] batch:  261 | acc: 62.50%,  total acc: 71.78%   [EVAL] batch:  262 | acc: 75.00%,  total acc: 71.79%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 71.83%   [EVAL] batch:  264 | acc: 87.50%,  total acc: 71.89%   [EVAL] batch:  265 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 71.91%   [EVAL] batch:  267 | acc: 75.00%,  total acc: 71.92%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 71.91%   [EVAL] batch:  269 | acc: 75.00%,  total acc: 71.92%   [EVAL] batch:  270 | acc: 81.25%,  total acc: 71.96%   [EVAL] batch:  271 | acc: 68.75%,  total acc: 71.94%   [EVAL] batch:  272 | acc: 37.50%,  total acc: 71.82%   [EVAL] batch:  273 | acc: 75.00%,  total acc: 71.83%   [EVAL] batch:  274 | acc: 50.00%,  total acc: 71.75%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 71.85%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 71.95%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 72.05%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 72.16%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 72.25%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 72.35%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 72.32%   [EVAL] batch:  282 | acc: 81.25%,  total acc: 72.35%   [EVAL] batch:  283 | acc: 68.75%,  total acc: 72.34%   [EVAL] batch:  284 | acc: 81.25%,  total acc: 72.37%   [EVAL] batch:  285 | acc: 68.75%,  total acc: 72.36%   [EVAL] batch:  286 | acc: 56.25%,  total acc: 72.30%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 72.29%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 72.38%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 72.48%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 72.57%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 72.65%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 72.74%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 72.81%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 72.90%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 72.99%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 73.09%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 73.18%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 73.27%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 73.35%   [EVAL] batch:  300 | acc: 87.50%,  total acc: 73.40%   [EVAL] batch:  301 | acc: 87.50%,  total acc: 73.45%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 73.51%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 73.58%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 73.61%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 73.69%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 73.76%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 73.82%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 73.89%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 73.97%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 74.06%   [EVAL] batch:  311 | acc: 93.75%,  total acc: 74.12%   [EVAL] batch:  312 | acc: 75.00%,  total acc: 74.12%   [EVAL] batch:  313 | acc: 37.50%,  total acc: 74.00%   [EVAL] batch:  314 | acc: 43.75%,  total acc: 73.91%   [EVAL] batch:  315 | acc: 43.75%,  total acc: 73.81%   [EVAL] batch:  316 | acc: 56.25%,  total acc: 73.76%   [EVAL] batch:  317 | acc: 43.75%,  total acc: 73.66%   [EVAL] batch:  318 | acc: 87.50%,  total acc: 73.71%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 73.71%   [EVAL] batch:  320 | acc: 87.50%,  total acc: 73.75%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 73.80%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 73.84%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 73.84%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 73.88%   [EVAL] batch:  325 | acc: 100.00%,  total acc: 73.96%   [EVAL] batch:  326 | acc: 100.00%,  total acc: 74.04%   [EVAL] batch:  327 | acc: 100.00%,  total acc: 74.12%   [EVAL] batch:  328 | acc: 87.50%,  total acc: 74.16%   [EVAL] batch:  329 | acc: 93.75%,  total acc: 74.22%   [EVAL] batch:  330 | acc: 93.75%,  total acc: 74.28%   [EVAL] batch:  331 | acc: 37.50%,  total acc: 74.17%   [EVAL] batch:  332 | acc: 18.75%,  total acc: 74.01%   [EVAL] batch:  333 | acc: 18.75%,  total acc: 73.84%   [EVAL] batch:  334 | acc: 18.75%,  total acc: 73.68%   [EVAL] batch:  335 | acc: 25.00%,  total acc: 73.53%   [EVAL] batch:  336 | acc: 6.25%,  total acc: 73.33%   [EVAL] batch:  337 | acc: 25.00%,  total acc: 73.19%   [EVAL] batch:  338 | acc: 0.00%,  total acc: 72.97%   [EVAL] batch:  339 | acc: 18.75%,  total acc: 72.81%   [EVAL] batch:  340 | acc: 6.25%,  total acc: 72.62%   [EVAL] batch:  341 | acc: 12.50%,  total acc: 72.44%   [EVAL] batch:  342 | acc: 6.25%,  total acc: 72.25%   [EVAL] batch:  343 | acc: 25.00%,  total acc: 72.11%   [EVAL] batch:  344 | acc: 93.75%,  total acc: 72.17%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 72.24%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 72.30%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 72.36%   [EVAL] batch:  348 | acc: 87.50%,  total acc: 72.40%   [EVAL] batch:  349 | acc: 81.25%,  total acc: 72.43%   [EVAL] batch:  350 | acc: 93.75%,  total acc: 72.49%   [EVAL] batch:  351 | acc: 93.75%,  total acc: 72.55%   [EVAL] batch:  352 | acc: 87.50%,  total acc: 72.59%   [EVAL] batch:  353 | acc: 87.50%,  total acc: 72.63%   [EVAL] batch:  354 | acc: 93.75%,  total acc: 72.69%   [EVAL] batch:  355 | acc: 100.00%,  total acc: 72.77%   [EVAL] batch:  356 | acc: 93.75%,  total acc: 72.83%   [EVAL] batch:  357 | acc: 87.50%,  total acc: 72.87%   [EVAL] batch:  358 | acc: 81.25%,  total acc: 72.89%   [EVAL] batch:  359 | acc: 87.50%,  total acc: 72.93%   [EVAL] batch:  360 | acc: 81.25%,  total acc: 72.96%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 73.00%   [EVAL] batch:  362 | acc: 56.25%,  total acc: 72.95%   [EVAL] batch:  363 | acc: 31.25%,  total acc: 72.84%   [EVAL] batch:  364 | acc: 37.50%,  total acc: 72.74%   [EVAL] batch:  365 | acc: 62.50%,  total acc: 72.71%   [EVAL] batch:  366 | acc: 43.75%,  total acc: 72.63%   [EVAL] batch:  367 | acc: 50.00%,  total acc: 72.57%   [EVAL] batch:  368 | acc: 56.25%,  total acc: 72.53%   [EVAL] batch:  369 | acc: 43.75%,  total acc: 72.45%   [EVAL] batch:  370 | acc: 50.00%,  total acc: 72.39%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 72.33%   [EVAL] batch:  372 | acc: 56.25%,  total acc: 72.29%   [EVAL] batch:  373 | acc: 75.00%,  total acc: 72.29%   [EVAL] batch:  374 | acc: 68.75%,  total acc: 72.28%   
cur_acc:  ['0.9504', '0.7292', '0.7837', '0.7560', '0.8849', '0.6240']
his_acc:  ['0.9504', '0.8365', '0.7989', '0.7595', '0.7702', '0.7228']
CurrentTrain: epoch  0, batch     0 | loss: 6.0207467CurrentTrain: epoch  0, batch     1 | loss: 5.4901147CurrentTrain: epoch  0, batch     2 | loss: 6.3814144CurrentTrain: epoch  0, batch     3 | loss: 10.2939939CurrentTrain: epoch  1, batch     0 | loss: 4.4212556CurrentTrain: epoch  1, batch     1 | loss: 5.9735932CurrentTrain: epoch  1, batch     2 | loss: 5.5314775CurrentTrain: epoch  1, batch     3 | loss: 4.1497431CurrentTrain: epoch  2, batch     0 | loss: 4.9488916CurrentTrain: epoch  2, batch     1 | loss: 5.0035219CurrentTrain: epoch  2, batch     2 | loss: 4.3074656CurrentTrain: epoch  2, batch     3 | loss: 4.4526939CurrentTrain: epoch  3, batch     0 | loss: 4.9707384CurrentTrain: epoch  3, batch     1 | loss: 4.0557585CurrentTrain: epoch  3, batch     2 | loss: 4.3757086CurrentTrain: epoch  3, batch     3 | loss: 3.0601008CurrentTrain: epoch  4, batch     0 | loss: 4.5932074CurrentTrain: epoch  4, batch     1 | loss: 3.2550321CurrentTrain: epoch  4, batch     2 | loss: 4.5607719CurrentTrain: epoch  4, batch     3 | loss: 4.3647881CurrentTrain: epoch  5, batch     0 | loss: 3.4922082CurrentTrain: epoch  5, batch     1 | loss: 5.2820649CurrentTrain: epoch  5, batch     2 | loss: 2.8011861CurrentTrain: epoch  5, batch     3 | loss: 1.7951901CurrentTrain: epoch  6, batch     0 | loss: 3.5762258CurrentTrain: epoch  6, batch     1 | loss: 3.8928308CurrentTrain: epoch  6, batch     2 | loss: 3.8481646CurrentTrain: epoch  6, batch     3 | loss: 4.3687267CurrentTrain: epoch  7, batch     0 | loss: 4.5427771CurrentTrain: epoch  7, batch     1 | loss: 2.9021814CurrentTrain: epoch  7, batch     2 | loss: 3.8852816CurrentTrain: epoch  7, batch     3 | loss: 2.4726923CurrentTrain: epoch  8, batch     0 | loss: 3.6279993CurrentTrain: epoch  8, batch     1 | loss: 3.1299191CurrentTrain: epoch  8, batch     2 | loss: 4.1682520CurrentTrain: epoch  8, batch     3 | loss: 1.7536412CurrentTrain: epoch  9, batch     0 | loss: 3.6154273CurrentTrain: epoch  9, batch     1 | loss: 4.1472964CurrentTrain: epoch  9, batch     2 | loss: 2.5744801CurrentTrain: epoch  9, batch     3 | loss: 4.4011669
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: the tcp protocol has a three state system for opening a connection .
Head Entity: tcp
Tail Entity: three state system
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: the theory of relativity revolutionized our understanding of space and time.  
Head Entity: theory of relativity  
Tail Entity: understanding of space and time  

Relation: main subject  
Context: the novel explores the complexities of human relationships in a dystopian society.  
Head Entity: novel  
Tail Entity: complexities of human relationships  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: the two teams had met in two previous matches , including in the 2006 world cup group stage , won by brazil 1–0 .
Head Entity: 2006 world cup
Tail Entity: brazil
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: In the 2020 UEFA European Championship, Italy and England faced off in the final, with Italy emerging victorious after a tense penalty shootout.  
Head Entity: 2020 UEFA European Championship  
Tail Entity: Italy  

Relation: participating team  
Context: The 2018 FIFA World Cup saw France and Croatia compete in an exhilarating final, where France claimed their second title.  
Head Entity: 2018 FIFA World Cup  
Tail Entity: France  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: among the competing artists was previous eurovision song contest entrant robin juhkental who represented estonia in 2010 as part of the group malcolm lincoln .
Head Entity: malcolm lincoln
Tail Entity: robin juhkental
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The famous painting "The Last Supper" features several apostles, including Peter, who is depicted prominently on the left side of the artwork.  
Head Entity: The Last Supper  
Tail Entity: Peter  

Relation: has part  
Context: The smartphone model Galaxy S21 comes with a high-resolution camera that includes a wide-angle lens, which is essential for capturing stunning landscapes.  
Head Entity: Galaxy S21  
Tail Entity: wide-angle lens  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: gyllenhaal was nominated for an academy award for best supporting actress for her performance .
Head Entity: gyllenhaal
Tail Entity: academy award for best supporting actress
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film was nominated for several prestigious awards, including the Golden Globe for Best Motion Picture.  
Head Entity: The film  
Tail Entity: Golden Globe for Best Motion Picture  

Relation: nominated for  
Context: She was nominated for the Grammy Award for Best New Artist after her debut album received critical acclaim.  
Head Entity: She  
Tail Entity: Grammy Award for Best New Artist  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: in 2007 , the county municipality created the transit authority called " skyss " that would market public transport while is would be operated by private companies based on public service obligation contracts .
Head Entity: skyss
Tail Entity: transit authority
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone known as the iPhone was first released by Apple Inc. in 2007, revolutionizing the mobile phone industry and setting new standards for design and functionality.  
Head Entity: iPhone  
Tail Entity: smartphone  

Relation: instance of  
Context: The Great Wall of China is a series of fortifications made of various materials, built to protect the Chinese states and empires from invasions and raids.  
Head Entity: Great Wall of China  
Tail Entity: fortification
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: abus valley ( ) is an ice - free valley southeast of turnstile ridge at the north end of britannia range .
Head Entity: turnstile ridge
Tail Entity: britannia range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the sierra nevada ( ) is a mountain range in the western united states, primarily in the state of california, and it includes the famous yosemite national park, which is located within the range.  
Head Entity: sierra nevada  
Tail Entity: yosemite national park  

Relation: mountain range  
Context: the appalachian mountains ( ) extend from the canadian province of quebec down to alabama, making it one of the longest mountain ranges in north america.  
Head Entity: appalachian mountains  
Tail Entity: alabama  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: future films include " mcqueen " , ian bonhote ’s documentary about the fashion designer alexander mcqueen , brad anderson ’s thriller " beirut " , and mark pellington ’s drama " nostalgia " .
Head Entity: nostalgia
Tail Entity: mark pellington
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: the upcoming movie "the great adventure" is penned by renowned screenwriter jessica taylor, who has previously won multiple awards for her work in the industry.  
Head Entity: the great adventure  
Tail Entity: jessica taylor  

Relation: screenwriter  
Context: in the latest film festival, "shadows of the past" received critical acclaim, thanks to its brilliant script written by the talented screenwriter robert king.  
Head Entity: shadows of the past  
Tail Entity: robert king  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the publisher found success in 1993 with tarō gomi 's " everyone poops " , a japanese language import that sold over one million copies and is now their best - known title .
Head Entity: everyone poops
Tail Entity: japanese language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The animated series "Avatar: The Last Airbender" features a rich narrative and is primarily produced in English, appealing to a global audience.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  

Relation: language of work or name  
Context: The famous novel "Cien años de soledad" by Gabriel García Márquez is a cornerstone of Latin American literature and is originally written in Spanish.  
Head Entity: Cien años de soledad  
Tail Entity: Spanish  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: the aircraft was a boeing 707 - 321c that first entered service with pan american world airways ( pan am ) in 1963 .
Head Entity: boeing 707 - 321c
Tail Entity: pan american world airways
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: the new electric bus fleet is managed by the city transportation authority, which ensures efficient public transit services.  
Head Entity: electric bus fleet  
Tail Entity: city transportation authority  

Relation: operator  
Context: the state-of-the-art research facility is operated by the national science foundation, focusing on advanced materials and nanotechnology.  
Head Entity: research facility  
Tail Entity: national science foundation  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: baianism is a term applied to the theology of catholic theologian michael baius ( 1513 - 1589 ) .
Head Entity: michael baius
Tail Entity: catholic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the mosque is a place of worship for followers of islam, where they gather for prayers and community events.  
Head Entity: mosque  
Tail Entity: islam  

Relation: religion  
Context: the teachings of buddha form the foundation of buddhism, which emphasizes mindfulness and compassion.  
Head Entity: buddha  
Tail Entity: buddhism  
Mixup data size:  497
MixupTrain:  epoch  0, batch     0 | loss: 1.7735884MixupTrain:  epoch  0, batch     1 | loss: 2.3803111MixupTrain:  epoch  0, batch     2 | loss: 1.9135132MixupTrain:  epoch  0, batch     3 | loss: 1.9990061MixupTrain:  epoch  0, batch     4 | loss: 2.2351980MixupTrain:  epoch  0, batch     5 | loss: 2.0553254MixupTrain:  epoch  0, batch     6 | loss: 2.3141718MixupTrain:  epoch  0, batch     7 | loss: 1.9026317MixupTrain:  epoch  0, batch     8 | loss: 1.7558731MixupTrain:  epoch  0, batch     9 | loss: 2.0550665MixupTrain:  epoch  0, batch    10 | loss: 1.9267970MixupTrain:  epoch  0, batch    11 | loss: 2.6691706MixupTrain:  epoch  0, batch    12 | loss: 1.7854847MixupTrain:  epoch  0, batch    13 | loss: 1.5717769MixupTrain:  epoch  0, batch    14 | loss: 1.8341946MixupTrain:  epoch  0, batch    15 | loss: 2.0926413MixupTrain:  epoch  0, batch    16 | loss: 1.9735785MixupTrain:  epoch  0, batch    17 | loss: 2.1408438MixupTrain:  epoch  0, batch    18 | loss: 2.3878075MixupTrain:  epoch  0, batch    19 | loss: 2.1352222MixupTrain:  epoch  0, batch    20 | loss: 2.1894894MixupTrain:  epoch  0, batch    21 | loss: 1.9755594MixupTrain:  epoch  0, batch    22 | loss: 2.0301681MixupTrain:  epoch  0, batch    23 | loss: 1.9948029MixupTrain:  epoch  0, batch    24 | loss: 2.1123243MixupTrain:  epoch  0, batch    25 | loss: 2.1492513MixupTrain:  epoch  0, batch    26 | loss: 2.0123647MixupTrain:  epoch  0, batch    27 | loss: 1.6196741MixupTrain:  epoch  0, batch    28 | loss: 2.0408640MixupTrain:  epoch  0, batch    29 | loss: 2.0500988MixupTrain:  epoch  0, batch    30 | loss: 2.6002563MixupTrain:  epoch  0, batch    31 | loss: 2.8260643
MemoryTrain:  epoch  0, batch     0 | loss: 2.6352503MemoryTrain:  epoch  0, batch     1 | loss: 2.6051707MemoryTrain:  epoch  0, batch     2 | loss: 2.3957293MemoryTrain:  epoch  0, batch     3 | loss: 2.7897413MemoryTrain:  epoch  0, batch     4 | loss: 1.9612188MemoryTrain:  epoch  0, batch     5 | loss: 1.8556154MemoryTrain:  epoch  0, batch     6 | loss: 1.6531942MemoryTrain:  epoch  0, batch     7 | loss: 2.0794032MemoryTrain:  epoch  0, batch     8 | loss: 1.9853741MemoryTrain:  epoch  0, batch     9 | loss: 2.4486494MemoryTrain:  epoch  0, batch    10 | loss: 1.7480152MemoryTrain:  epoch  0, batch    11 | loss: 2.9799647MemoryTrain:  epoch  0, batch    12 | loss: 1.7842665MemoryTrain:  epoch  0, batch    13 | loss: 1.8451033MemoryTrain:  epoch  1, batch     0 | loss: 1.7596873MemoryTrain:  epoch  1, batch     1 | loss: 1.8080816MemoryTrain:  epoch  1, batch     2 | loss: 1.8632121MemoryTrain:  epoch  1, batch     3 | loss: 2.4113696MemoryTrain:  epoch  1, batch     4 | loss: 1.7001877MemoryTrain:  epoch  1, batch     5 | loss: 2.0540960MemoryTrain:  epoch  1, batch     6 | loss: 2.6708040MemoryTrain:  epoch  1, batch     7 | loss: 1.3263161MemoryTrain:  epoch  1, batch     8 | loss: 2.2931769MemoryTrain:  epoch  1, batch     9 | loss: 1.9744047MemoryTrain:  epoch  1, batch    10 | loss: 2.0071955MemoryTrain:  epoch  1, batch    11 | loss: 1.5624909MemoryTrain:  epoch  1, batch    12 | loss: 2.0089159MemoryTrain:  epoch  1, batch    13 | loss: 1.8432274MemoryTrain:  epoch  2, batch     0 | loss: 1.9589335MemoryTrain:  epoch  2, batch     1 | loss: 1.5990469MemoryTrain:  epoch  2, batch     2 | loss: 1.7759109MemoryTrain:  epoch  2, batch     3 | loss: 1.6973484MemoryTrain:  epoch  2, batch     4 | loss: 1.6674170MemoryTrain:  epoch  2, batch     5 | loss: 1.7287964MemoryTrain:  epoch  2, batch     6 | loss: 2.3593154MemoryTrain:  epoch  2, batch     7 | loss: 1.5060365MemoryTrain:  epoch  2, batch     8 | loss: 1.2521110MemoryTrain:  epoch  2, batch     9 | loss: 2.1162431MemoryTrain:  epoch  2, batch    10 | loss: 1.6973751MemoryTrain:  epoch  2, batch    11 | loss: 1.3248144MemoryTrain:  epoch  2, batch    12 | loss: 1.7579832MemoryTrain:  epoch  2, batch    13 | loss: 1.1420436MemoryTrain:  epoch  3, batch     0 | loss: 1.7338061MemoryTrain:  epoch  3, batch     1 | loss: 1.3493515MemoryTrain:  epoch  3, batch     2 | loss: 1.6012245MemoryTrain:  epoch  3, batch     3 | loss: 1.5326326MemoryTrain:  epoch  3, batch     4 | loss: 2.0537994MemoryTrain:  epoch  3, batch     5 | loss: 1.2141407MemoryTrain:  epoch  3, batch     6 | loss: 1.4815235MemoryTrain:  epoch  3, batch     7 | loss: 1.9266925MemoryTrain:  epoch  3, batch     8 | loss: 1.6124744MemoryTrain:  epoch  3, batch     9 | loss: 1.2879051MemoryTrain:  epoch  3, batch    10 | loss: 1.4283271MemoryTrain:  epoch  3, batch    11 | loss: 1.9159094MemoryTrain:  epoch  3, batch    12 | loss: 1.7103316MemoryTrain:  epoch  3, batch    13 | loss: 1.2269721MemoryTrain:  epoch  4, batch     0 | loss: 1.3329867MemoryTrain:  epoch  4, batch     1 | loss: 1.4146025MemoryTrain:  epoch  4, batch     2 | loss: 1.6381507MemoryTrain:  epoch  4, batch     3 | loss: 1.8582836MemoryTrain:  epoch  4, batch     4 | loss: 1.6432734MemoryTrain:  epoch  4, batch     5 | loss: 1.4601851MemoryTrain:  epoch  4, batch     6 | loss: 1.3856852MemoryTrain:  epoch  4, batch     7 | loss: 1.2037947MemoryTrain:  epoch  4, batch     8 | loss: 1.9510651MemoryTrain:  epoch  4, batch     9 | loss: 1.2831984MemoryTrain:  epoch  4, batch    10 | loss: 1.7242948MemoryTrain:  epoch  4, batch    11 | loss: 1.4984484MemoryTrain:  epoch  4, batch    12 | loss: 1.4365383MemoryTrain:  epoch  4, batch    13 | loss: 1.2119148MemoryTrain:  epoch  5, batch     0 | loss: 1.2891730MemoryTrain:  epoch  5, batch     1 | loss: 1.4636847MemoryTrain:  epoch  5, batch     2 | loss: 1.2978566MemoryTrain:  epoch  5, batch     3 | loss: 1.5298216MemoryTrain:  epoch  5, batch     4 | loss: 1.7824526MemoryTrain:  epoch  5, batch     5 | loss: 1.6078038MemoryTrain:  epoch  5, batch     6 | loss: 1.4146272MemoryTrain:  epoch  5, batch     7 | loss: 1.5948917MemoryTrain:  epoch  5, batch     8 | loss: 1.3132424MemoryTrain:  epoch  5, batch     9 | loss: 1.5028567MemoryTrain:  epoch  5, batch    10 | loss: 1.2553867MemoryTrain:  epoch  5, batch    11 | loss: 1.4862098MemoryTrain:  epoch  5, batch    12 | loss: 1.3142178MemoryTrain:  epoch  5, batch    13 | loss: 1.1648202MemoryTrain:  epoch  6, batch     0 | loss: 1.3405480MemoryTrain:  epoch  6, batch     1 | loss: 1.5707088MemoryTrain:  epoch  6, batch     2 | loss: 1.5222723MemoryTrain:  epoch  6, batch     3 | loss: 1.3904140MemoryTrain:  epoch  6, batch     4 | loss: 1.4982781MemoryTrain:  epoch  6, batch     5 | loss: 1.4979161MemoryTrain:  epoch  6, batch     6 | loss: 1.3365130MemoryTrain:  epoch  6, batch     7 | loss: 1.3548610MemoryTrain:  epoch  6, batch     8 | loss: 1.2476029MemoryTrain:  epoch  6, batch     9 | loss: 1.2835051MemoryTrain:  epoch  6, batch    10 | loss: 1.2612047MemoryTrain:  epoch  6, batch    11 | loss: 1.3414141MemoryTrain:  epoch  6, batch    12 | loss: 1.2183797MemoryTrain:  epoch  6, batch    13 | loss: 1.1837933MemoryTrain:  epoch  7, batch     0 | loss: 1.1870289MemoryTrain:  epoch  7, batch     1 | loss: 1.2444632MemoryTrain:  epoch  7, batch     2 | loss: 1.4433894MemoryTrain:  epoch  7, batch     3 | loss: 1.2617774MemoryTrain:  epoch  7, batch     4 | loss: 1.4693267MemoryTrain:  epoch  7, batch     5 | loss: 1.5280980MemoryTrain:  epoch  7, batch     6 | loss: 1.3715551MemoryTrain:  epoch  7, batch     7 | loss: 1.3267992MemoryTrain:  epoch  7, batch     8 | loss: 1.4012837MemoryTrain:  epoch  7, batch     9 | loss: 1.2910860MemoryTrain:  epoch  7, batch    10 | loss: 1.3973025MemoryTrain:  epoch  7, batch    11 | loss: 1.5051425MemoryTrain:  epoch  7, batch    12 | loss: 1.3515215MemoryTrain:  epoch  7, batch    13 | loss: 1.5919800MemoryTrain:  epoch  8, batch     0 | loss: 1.2928013MemoryTrain:  epoch  8, batch     1 | loss: 1.4600430MemoryTrain:  epoch  8, batch     2 | loss: 1.3076615MemoryTrain:  epoch  8, batch     3 | loss: 1.4710896MemoryTrain:  epoch  8, batch     4 | loss: 1.4720494MemoryTrain:  epoch  8, batch     5 | loss: 1.2354391MemoryTrain:  epoch  8, batch     6 | loss: 1.3149233MemoryTrain:  epoch  8, batch     7 | loss: 1.2732153MemoryTrain:  epoch  8, batch     8 | loss: 1.3198328MemoryTrain:  epoch  8, batch     9 | loss: 1.2176781MemoryTrain:  epoch  8, batch    10 | loss: 1.2803479MemoryTrain:  epoch  8, batch    11 | loss: 1.2498974MemoryTrain:  epoch  8, batch    12 | loss: 1.2946016MemoryTrain:  epoch  8, batch    13 | loss: 1.1626860MemoryTrain:  epoch  9, batch     0 | loss: 1.2957457MemoryTrain:  epoch  9, batch     1 | loss: 1.5485899MemoryTrain:  epoch  9, batch     2 | loss: 1.2263911MemoryTrain:  epoch  9, batch     3 | loss: 1.1854521MemoryTrain:  epoch  9, batch     4 | loss: 1.2826202MemoryTrain:  epoch  9, batch     5 | loss: 1.2861950MemoryTrain:  epoch  9, batch     6 | loss: 1.3236442MemoryTrain:  epoch  9, batch     7 | loss: 1.2248682MemoryTrain:  epoch  9, batch     8 | loss: 1.4049286MemoryTrain:  epoch  9, batch     9 | loss: 1.2218819MemoryTrain:  epoch  9, batch    10 | loss: 1.3326726MemoryTrain:  epoch  9, batch    11 | loss: 1.2134335MemoryTrain:  epoch  9, batch    12 | loss: 1.2523613MemoryTrain:  epoch  9, batch    13 | loss: 1.3052506
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 10.42%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 14.06%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 13.75%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 13.54%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 22.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 32.03%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 38.19%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 43.12%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 47.73%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 52.08%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 53.85%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 51.79%   [EVAL] batch:   14 | acc: 25.00%,  total acc: 50.00%   [EVAL] batch:   15 | acc: 12.50%,  total acc: 47.66%   [EVAL] batch:   16 | acc: 37.50%,  total acc: 47.06%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 45.83%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 45.39%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 48.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 50.60%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 52.56%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 54.62%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 56.51%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 58.00%   [EVAL] batch:   25 | acc: 43.75%,  total acc: 57.45%   [EVAL] batch:   26 | acc: 37.50%,  total acc: 56.71%   [EVAL] batch:   27 | acc: 37.50%,  total acc: 56.03%   [EVAL] batch:   28 | acc: 43.75%,  total acc: 55.60%   [EVAL] batch:   29 | acc: 18.75%,  total acc: 54.37%   [EVAL] batch:   30 | acc: 31.25%,  total acc: 53.63%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 54.30%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 55.49%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 56.62%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 57.68%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 58.51%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 59.12%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 59.87%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 60.58%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 60.47%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 60.67%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 60.71%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 60.76%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 60.94%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 60.42%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 60.46%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 60.37%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 60.29%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 60.20%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 59.75%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 59.93%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 60.34%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 60.73%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 61.34%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 61.93%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 62.83%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 63.25%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 63.77%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 64.06%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 64.65%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 64.92%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 64.68%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 66.15%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 66.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 68.30%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 73.16%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 74.31%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 75.66%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 74.72%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 73.64%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 73.70%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 74.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.24%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 75.93%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 76.34%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 76.94%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 77.71%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 78.23%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 79.55%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 80.15%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 80.54%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 81.08%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 81.59%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 81.91%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 82.37%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 83.08%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 83.48%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 83.58%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 83.66%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 83.29%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 82.85%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 82.55%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 82.53%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 82.12%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 81.99%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 81.97%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 82.19%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 81.83%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 81.36%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 81.36%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 81.03%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 80.28%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 79.98%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 79.92%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 79.64%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 79.07%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 78.12%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 77.40%   [EVAL] batch:   65 | acc: 25.00%,  total acc: 76.61%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 75.65%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 75.37%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 74.73%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 74.73%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 74.47%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 74.31%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 74.40%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 74.41%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 74.50%   [EVAL] batch:   75 | acc: 43.75%,  total acc: 74.10%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 73.94%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 73.64%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 73.26%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 73.12%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 73.15%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 73.09%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 72.82%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 72.25%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 72.06%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 71.73%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 71.62%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 71.66%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 71.91%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 72.22%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 72.46%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 72.76%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 72.98%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 73.20%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 73.49%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 73.76%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 74.03%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 74.30%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 74.56%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 74.81%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 75.18%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 75.18%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 75.30%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 75.48%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 75.53%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 75.12%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 74.59%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 74.20%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 73.64%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 73.20%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 72.71%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 72.51%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 72.64%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 72.88%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 72.90%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 73.02%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 73.20%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 73.21%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 72.60%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 72.11%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 71.52%   [EVAL] batch:  122 | acc: 0.00%,  total acc: 70.93%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 70.41%   [EVAL] batch:  124 | acc: 0.00%,  total acc: 69.85%   [EVAL] batch:  125 | acc: 43.75%,  total acc: 69.64%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 69.34%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 68.99%   [EVAL] batch:  128 | acc: 18.75%,  total acc: 68.60%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 68.46%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 68.27%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 68.42%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 68.61%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 68.89%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 69.12%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 69.34%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 69.47%   [EVAL] batch:  138 | acc: 50.00%,  total acc: 69.33%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 69.33%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 69.41%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 69.45%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 69.49%   [EVAL] batch:  143 | acc: 81.25%,  total acc: 69.57%   [EVAL] batch:  144 | acc: 37.50%,  total acc: 69.35%   [EVAL] batch:  145 | acc: 62.50%,  total acc: 69.31%   [EVAL] batch:  146 | acc: 37.50%,  total acc: 69.09%   [EVAL] batch:  147 | acc: 31.25%,  total acc: 68.83%   [EVAL] batch:  148 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:  149 | acc: 25.00%,  total acc: 68.46%   [EVAL] batch:  150 | acc: 62.50%,  total acc: 68.42%   [EVAL] batch:  151 | acc: 75.00%,  total acc: 68.46%   [EVAL] batch:  152 | acc: 68.75%,  total acc: 68.46%   [EVAL] batch:  153 | acc: 62.50%,  total acc: 68.43%   [EVAL] batch:  154 | acc: 50.00%,  total acc: 68.31%   [EVAL] batch:  155 | acc: 56.25%,  total acc: 68.23%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 68.27%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 68.16%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 68.08%   [EVAL] batch:  159 | acc: 56.25%,  total acc: 68.01%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 67.97%   [EVAL] batch:  161 | acc: 50.00%,  total acc: 67.86%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 67.83%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 67.80%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 67.80%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 67.77%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 67.81%   [EVAL] batch:  167 | acc: 68.75%,  total acc: 67.82%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 67.75%   [EVAL] batch:  169 | acc: 12.50%,  total acc: 67.43%   [EVAL] batch:  170 | acc: 12.50%,  total acc: 67.11%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 66.79%   [EVAL] batch:  172 | acc: 25.00%,  total acc: 66.55%   [EVAL] batch:  173 | acc: 6.25%,  total acc: 66.20%   [EVAL] batch:  174 | acc: 18.75%,  total acc: 65.93%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 66.12%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 66.31%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 66.50%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 66.69%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 66.88%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 67.06%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 67.24%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 67.28%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 67.43%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 67.57%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 67.57%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 67.71%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 67.65%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 67.46%   [EVAL] batch:  189 | acc: 50.00%,  total acc: 67.37%   [EVAL] batch:  190 | acc: 25.00%,  total acc: 67.15%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 66.93%   [EVAL] batch:  192 | acc: 37.50%,  total acc: 66.77%   [EVAL] batch:  193 | acc: 25.00%,  total acc: 66.56%   [EVAL] batch:  194 | acc: 62.50%,  total acc: 66.54%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 66.45%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 66.50%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 66.54%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 66.55%   [EVAL] batch:  199 | acc: 68.75%,  total acc: 66.56%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 66.64%   [EVAL] batch:  201 | acc: 43.75%,  total acc: 66.52%   [EVAL] batch:  202 | acc: 50.00%,  total acc: 66.44%   [EVAL] batch:  203 | acc: 75.00%,  total acc: 66.48%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 66.52%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 66.54%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 66.83%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 66.99%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 67.11%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 67.24%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 67.39%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 67.37%   [EVAL] batch:  213 | acc: 75.00%,  total acc: 67.41%   [EVAL] batch:  214 | acc: 75.00%,  total acc: 67.44%   [EVAL] batch:  215 | acc: 75.00%,  total acc: 67.48%   [EVAL] batch:  216 | acc: 68.75%,  total acc: 67.48%   [EVAL] batch:  217 | acc: 81.25%,  total acc: 67.55%   [EVAL] batch:  218 | acc: 81.25%,  total acc: 67.61%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 67.76%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 67.90%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 68.05%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 68.16%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 68.28%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 68.42%   [EVAL] batch:  225 | acc: 43.75%,  total acc: 68.31%   [EVAL] batch:  226 | acc: 37.50%,  total acc: 68.17%   [EVAL] batch:  227 | acc: 31.25%,  total acc: 68.01%   [EVAL] batch:  228 | acc: 37.50%,  total acc: 67.88%   [EVAL] batch:  229 | acc: 18.75%,  total acc: 67.66%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 67.64%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 67.75%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 67.86%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 67.98%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 68.11%   [EVAL] batch:  235 | acc: 81.25%,  total acc: 68.17%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 68.22%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 68.28%   [EVAL] batch:  238 | acc: 62.50%,  total acc: 68.25%   [EVAL] batch:  239 | acc: 31.25%,  total acc: 68.10%   [EVAL] batch:  240 | acc: 31.25%,  total acc: 67.95%   [EVAL] batch:  241 | acc: 56.25%,  total acc: 67.90%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 67.88%   [EVAL] batch:  243 | acc: 31.25%,  total acc: 67.73%   [EVAL] batch:  244 | acc: 56.25%,  total acc: 67.68%   [EVAL] batch:  245 | acc: 62.50%,  total acc: 67.66%   [EVAL] batch:  246 | acc: 50.00%,  total acc: 67.59%   [EVAL] batch:  247 | acc: 68.75%,  total acc: 67.59%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 67.67%   [EVAL] batch:  249 | acc: 50.00%,  total acc: 67.60%   [EVAL] batch:  250 | acc: 100.00%,  total acc: 67.73%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 67.83%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 67.93%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 68.01%   [EVAL] batch:  254 | acc: 87.50%,  total acc: 68.09%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 68.21%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 68.17%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 68.19%   [EVAL] batch:  258 | acc: 50.00%,  total acc: 68.12%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 68.08%   [EVAL] batch:  260 | acc: 37.50%,  total acc: 67.96%   [EVAL] batch:  261 | acc: 62.50%,  total acc: 67.94%   [EVAL] batch:  262 | acc: 75.00%,  total acc: 67.97%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 68.02%   [EVAL] batch:  264 | acc: 87.50%,  total acc: 68.09%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 68.12%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 68.16%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 68.17%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 68.17%   [EVAL] batch:  269 | acc: 75.00%,  total acc: 68.19%   [EVAL] batch:  270 | acc: 81.25%,  total acc: 68.24%   [EVAL] batch:  271 | acc: 81.25%,  total acc: 68.29%   [EVAL] batch:  272 | acc: 62.50%,  total acc: 68.27%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 68.34%   [EVAL] batch:  274 | acc: 56.25%,  total acc: 68.30%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 68.41%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 68.52%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 68.64%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 68.86%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 68.97%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 68.97%   [EVAL] batch:  282 | acc: 81.25%,  total acc: 69.02%   [EVAL] batch:  283 | acc: 50.00%,  total acc: 68.95%   [EVAL] batch:  284 | acc: 75.00%,  total acc: 68.97%   [EVAL] batch:  285 | acc: 68.75%,  total acc: 68.97%   [EVAL] batch:  286 | acc: 56.25%,  total acc: 68.92%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 68.92%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 69.03%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 69.24%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 69.45%   [EVAL] batch:  293 | acc: 87.50%,  total acc: 69.52%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 69.62%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 69.72%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 69.82%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 69.92%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 70.03%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 70.12%   [EVAL] batch:  300 | acc: 87.50%,  total acc: 70.18%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 70.26%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 70.34%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 70.42%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 70.45%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 70.53%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 70.60%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 70.68%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 70.75%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 70.85%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 70.94%   [EVAL] batch:  311 | acc: 93.75%,  total acc: 71.01%   [EVAL] batch:  312 | acc: 56.25%,  total acc: 70.97%   [EVAL] batch:  313 | acc: 6.25%,  total acc: 70.76%   [EVAL] batch:  314 | acc: 12.50%,  total acc: 70.58%   [EVAL] batch:  315 | acc: 12.50%,  total acc: 70.39%   [EVAL] batch:  316 | acc: 0.00%,  total acc: 70.17%   [EVAL] batch:  317 | acc: 6.25%,  total acc: 69.97%   [EVAL] batch:  318 | acc: 37.50%,  total acc: 69.87%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 69.88%   [EVAL] batch:  320 | acc: 87.50%,  total acc: 69.94%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 69.99%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 70.07%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 70.08%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 70.13%   [EVAL] batch:  325 | acc: 81.25%,  total acc: 70.17%   [EVAL] batch:  326 | acc: 43.75%,  total acc: 70.09%   [EVAL] batch:  327 | acc: 75.00%,  total acc: 70.10%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 70.12%   [EVAL] batch:  329 | acc: 68.75%,  total acc: 70.11%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 70.11%   [EVAL] batch:  331 | acc: 12.50%,  total acc: 69.94%   [EVAL] batch:  332 | acc: 18.75%,  total acc: 69.78%   [EVAL] batch:  333 | acc: 18.75%,  total acc: 69.63%   [EVAL] batch:  334 | acc: 12.50%,  total acc: 69.46%   [EVAL] batch:  335 | acc: 18.75%,  total acc: 69.31%   [EVAL] batch:  336 | acc: 6.25%,  total acc: 69.12%   [EVAL] batch:  337 | acc: 12.50%,  total acc: 68.95%   [EVAL] batch:  338 | acc: 0.00%,  total acc: 68.75%   [EVAL] batch:  339 | acc: 6.25%,  total acc: 68.57%   [EVAL] batch:  340 | acc: 0.00%,  total acc: 68.37%   [EVAL] batch:  341 | acc: 12.50%,  total acc: 68.20%   [EVAL] batch:  342 | acc: 6.25%,  total acc: 68.02%   [EVAL] batch:  343 | acc: 12.50%,  total acc: 67.86%   [EVAL] batch:  344 | acc: 75.00%,  total acc: 67.88%   [EVAL] batch:  345 | acc: 87.50%,  total acc: 67.94%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 67.99%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 68.07%   [EVAL] batch:  348 | acc: 81.25%,  total acc: 68.11%   [EVAL] batch:  349 | acc: 62.50%,  total acc: 68.09%   [EVAL] batch:  350 | acc: 93.75%,  total acc: 68.16%   [EVAL] batch:  351 | acc: 93.75%,  total acc: 68.24%   [EVAL] batch:  352 | acc: 93.75%,  total acc: 68.31%   [EVAL] batch:  353 | acc: 87.50%,  total acc: 68.36%   [EVAL] batch:  354 | acc: 93.75%,  total acc: 68.43%   [EVAL] batch:  355 | acc: 100.00%,  total acc: 68.52%   [EVAL] batch:  356 | acc: 100.00%,  total acc: 68.61%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 68.68%   [EVAL] batch:  358 | acc: 75.00%,  total acc: 68.70%   [EVAL] batch:  359 | acc: 93.75%,  total acc: 68.77%   [EVAL] batch:  360 | acc: 87.50%,  total acc: 68.82%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 68.87%   [EVAL] batch:  362 | acc: 56.25%,  total acc: 68.84%   [EVAL] batch:  363 | acc: 37.50%,  total acc: 68.75%   [EVAL] batch:  364 | acc: 37.50%,  total acc: 68.66%   [EVAL] batch:  365 | acc: 81.25%,  total acc: 68.70%   [EVAL] batch:  366 | acc: 50.00%,  total acc: 68.65%   [EVAL] batch:  367 | acc: 68.75%,  total acc: 68.65%   [EVAL] batch:  368 | acc: 56.25%,  total acc: 68.61%   [EVAL] batch:  369 | acc: 43.75%,  total acc: 68.55%   [EVAL] batch:  370 | acc: 50.00%,  total acc: 68.50%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 68.45%   [EVAL] batch:  372 | acc: 62.50%,  total acc: 68.43%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 68.47%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 68.48%   [EVAL] batch:  375 | acc: 6.25%,  total acc: 68.32%   [EVAL] batch:  376 | acc: 6.25%,  total acc: 68.15%   [EVAL] batch:  377 | acc: 18.75%,  total acc: 68.02%   [EVAL] batch:  378 | acc: 25.00%,  total acc: 67.91%   [EVAL] batch:  379 | acc: 12.50%,  total acc: 67.76%   [EVAL] batch:  380 | acc: 12.50%,  total acc: 67.62%   [EVAL] batch:  381 | acc: 75.00%,  total acc: 67.64%   [EVAL] batch:  382 | acc: 100.00%,  total acc: 67.72%   [EVAL] batch:  383 | acc: 87.50%,  total acc: 67.77%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 67.82%   [EVAL] batch:  385 | acc: 93.75%,  total acc: 67.89%   [EVAL] batch:  386 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:  387 | acc: 75.00%,  total acc: 67.99%   [EVAL] batch:  388 | acc: 25.00%,  total acc: 67.88%   [EVAL] batch:  389 | acc: 25.00%,  total acc: 67.77%   [EVAL] batch:  390 | acc: 12.50%,  total acc: 67.63%   [EVAL] batch:  391 | acc: 37.50%,  total acc: 67.55%   [EVAL] batch:  392 | acc: 25.00%,  total acc: 67.45%   [EVAL] batch:  393 | acc: 37.50%,  total acc: 67.37%   [EVAL] batch:  394 | acc: 100.00%,  total acc: 67.45%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 67.53%   [EVAL] batch:  396 | acc: 93.75%,  total acc: 67.60%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 67.68%   [EVAL] batch:  398 | acc: 100.00%,  total acc: 67.76%   [EVAL] batch:  399 | acc: 93.75%,  total acc: 67.83%   [EVAL] batch:  400 | acc: 43.75%,  total acc: 67.77%   [EVAL] batch:  401 | acc: 37.50%,  total acc: 67.69%   [EVAL] batch:  402 | acc: 37.50%,  total acc: 67.62%   [EVAL] batch:  403 | acc: 43.75%,  total acc: 67.56%   [EVAL] batch:  404 | acc: 18.75%,  total acc: 67.44%   [EVAL] batch:  405 | acc: 31.25%,  total acc: 67.35%   [EVAL] batch:  406 | acc: 75.00%,  total acc: 67.37%   [EVAL] batch:  407 | acc: 93.75%,  total acc: 67.43%   [EVAL] batch:  408 | acc: 93.75%,  total acc: 67.50%   [EVAL] batch:  409 | acc: 93.75%,  total acc: 67.56%   [EVAL] batch:  410 | acc: 87.50%,  total acc: 67.61%   [EVAL] batch:  411 | acc: 81.25%,  total acc: 67.64%   [EVAL] batch:  412 | acc: 87.50%,  total acc: 67.69%   [EVAL] batch:  413 | acc: 87.50%,  total acc: 67.74%   [EVAL] batch:  414 | acc: 56.25%,  total acc: 67.71%   [EVAL] batch:  415 | acc: 68.75%,  total acc: 67.71%   [EVAL] batch:  416 | acc: 62.50%,  total acc: 67.70%   [EVAL] batch:  417 | acc: 62.50%,  total acc: 67.69%   [EVAL] batch:  418 | acc: 68.75%,  total acc: 67.69%   [EVAL] batch:  419 | acc: 37.50%,  total acc: 67.62%   [EVAL] batch:  420 | acc: 62.50%,  total acc: 67.61%   [EVAL] batch:  421 | acc: 56.25%,  total acc: 67.58%   [EVAL] batch:  422 | acc: 56.25%,  total acc: 67.55%   [EVAL] batch:  423 | acc: 56.25%,  total acc: 67.53%   [EVAL] batch:  424 | acc: 37.50%,  total acc: 67.46%   [EVAL] batch:  425 | acc: 68.75%,  total acc: 67.46%   [EVAL] batch:  426 | acc: 81.25%,  total acc: 67.49%   [EVAL] batch:  427 | acc: 81.25%,  total acc: 67.52%   [EVAL] batch:  428 | acc: 93.75%,  total acc: 67.58%   [EVAL] batch:  429 | acc: 93.75%,  total acc: 67.65%   [EVAL] batch:  430 | acc: 93.75%,  total acc: 67.71%   [EVAL] batch:  431 | acc: 81.25%,  total acc: 67.74%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 67.78%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 67.84%   [EVAL] batch:  434 | acc: 81.25%,  total acc: 67.87%   [EVAL] batch:  435 | acc: 100.00%,  total acc: 67.95%   [EVAL] batch:  436 | acc: 81.25%,  total acc: 67.98%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 67.94%   
cur_acc:  ['0.9504', '0.7292', '0.7837', '0.7560', '0.8849', '0.6240', '0.6468']
his_acc:  ['0.9504', '0.8365', '0.7989', '0.7595', '0.7702', '0.7228', '0.6794']
CurrentTrain: epoch  0, batch     0 | loss: 4.6822524CurrentTrain: epoch  0, batch     1 | loss: 5.0561075CurrentTrain: epoch  0, batch     2 | loss: 6.1007977CurrentTrain: epoch  0, batch     3 | loss: 6.6888824CurrentTrain: epoch  1, batch     0 | loss: 4.5253210CurrentTrain: epoch  1, batch     1 | loss: 3.7597334CurrentTrain: epoch  1, batch     2 | loss: 4.2713766CurrentTrain: epoch  1, batch     3 | loss: 3.9650488CurrentTrain: epoch  2, batch     0 | loss: 4.2600307CurrentTrain: epoch  2, batch     1 | loss: 3.3649199CurrentTrain: epoch  2, batch     2 | loss: 3.6200056CurrentTrain: epoch  2, batch     3 | loss: 1.8095951CurrentTrain: epoch  3, batch     0 | loss: 3.2172518CurrentTrain: epoch  3, batch     1 | loss: 3.4586461CurrentTrain: epoch  3, batch     2 | loss: 3.5847392CurrentTrain: epoch  3, batch     3 | loss: 3.9604616CurrentTrain: epoch  4, batch     0 | loss: 3.8308966CurrentTrain: epoch  4, batch     1 | loss: 2.5728765CurrentTrain: epoch  4, batch     2 | loss: 2.8101401CurrentTrain: epoch  4, batch     3 | loss: 3.4947386CurrentTrain: epoch  5, batch     0 | loss: 3.0017891CurrentTrain: epoch  5, batch     1 | loss: 2.7368650CurrentTrain: epoch  5, batch     2 | loss: 2.8476515CurrentTrain: epoch  5, batch     3 | loss: 2.3272033CurrentTrain: epoch  6, batch     0 | loss: 2.9227967CurrentTrain: epoch  6, batch     1 | loss: 2.3356991CurrentTrain: epoch  6, batch     2 | loss: 2.6032901CurrentTrain: epoch  6, batch     3 | loss: 2.6404092CurrentTrain: epoch  7, batch     0 | loss: 2.8611836CurrentTrain: epoch  7, batch     1 | loss: 2.2918854CurrentTrain: epoch  7, batch     2 | loss: 2.2863066CurrentTrain: epoch  7, batch     3 | loss: 2.3797116CurrentTrain: epoch  8, batch     0 | loss: 2.2905934CurrentTrain: epoch  8, batch     1 | loss: 2.3101249CurrentTrain: epoch  8, batch     2 | loss: 2.4664676CurrentTrain: epoch  8, batch     3 | loss: 2.4421625CurrentTrain: epoch  9, batch     0 | loss: 2.4114342CurrentTrain: epoch  9, batch     1 | loss: 2.2895231CurrentTrain: epoch  9, batch     2 | loss: 2.1383014CurrentTrain: epoch  9, batch     3 | loss: 2.2173631
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the cantons of créteil are administrative divisions of the val - de - marne department , île - de - france region , northern france .
Head Entity: île - de - france
Tail Entity: val - de - marne
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The provinces of Canada are the primary administrative divisions of the country, with Ontario being one of the most populous provinces.  
Head Entity: Canada  
Tail Entity: Ontario  

Relation: contains administrative territorial entity  
Context: The states of Australia are the major political divisions, with New South Wales being the most populous state in the country.  
Head Entity: Australia  
Tail Entity: New South Wales  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: enzo is an italian given name derivative of heinz , a form of henry .
Head Entity: henry
Tail Entity: heinz
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as each other due to their close genetic relationship.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: In some circles, the terms "soda" and "pop" are said to be the same as each other, though this varies by region.  
Head Entity: soda  
Tail Entity: pop  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: the filming was primarily held at pollachi . it fetched pawan kalyan the filmfare award for best actor – telugu award at 59th filmfare awards south .
Head Entity: filmfare award for best actor – telugu
Tail Entity: pawan kalyan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: in the thrilling finale of the championship, the underdog team triumphed, securing the title and making history as they celebrated their victory at the stadium. This remarkable achievement earned them the prestigious trophy of the national football league.  
Head Entity: national football league  
Tail Entity: underdog team  

Relation: winner  
Context: during the annual science fair, the innovative project on renewable energy solutions impressed the judges, leading to a unanimous decision to award the top prize to the talented young scientist.  
Head Entity: top prize  
Tail Entity: talented young scientist  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: the railroad car they were deported in was attached to the end of the last train out of drancy which also carried drancy commandant ss hauptsturmführer alois brunner and other german military personnel .
Head Entity: alois brunner
Tail Entity: hauptsturmführer
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: General John Smith was awarded the title of Major General for his exceptional leadership during the conflict, which significantly contributed to the victory of his battalion.  
Head Entity: John Smith  
Tail Entity: Major General  

Relation: military rank  
Context: During the ceremony, Colonel Jane Doe was recognized for her outstanding service and was promoted to the rank of Brigadier General, marking a significant milestone in her military career.  
Head Entity: Jane Doe  
Tail Entity: Brigadier General  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: tokyopop also released a collection of karin short stories under the title : " chibi vampire : airmail " on august 31 , 2010 .
Head Entity: karin
Tail Entity: tokyopop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: Penguin Random House announced the release of a new novel by the acclaimed author John Grisham, set to hit the shelves on June 15, 2021.  
Head Entity: John Grisham  
Tail Entity: Penguin Random House  

Relation: publisher  
Context: The popular online platform Wattpad has been instrumental in launching the careers of many writers, including Anna Todd, whose series "After" gained immense popularity.  
Head Entity: Anna Todd  
Tail Entity: Wattpad  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " muzai moratorium " and " shōso strip " were released on vinyl in 2008 , as a part of her 10-year anniversary celebrations .
Head Entity: muzai moratorium
Tail Entity: shōso strip
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first chapter of the book introduces the main character, while the second chapter follows with a detailed backstory.  
Head Entity: first chapter  
Tail Entity: second chapter  

Relation: followed by  
Context: In the concert lineup, the opening act performed a lively set, followed by the headliner who captivated the audience with their greatest hits.  
Head Entity: opening act  
Tail Entity: headliner  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: karen archey is a north american art critic and curator based in new york city and berlin .
Head Entity: karen archey
Tail Entity: new york city
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: john smith is a software engineer who has been working in san francisco for the past five years.  
Head Entity: john smith  
Tail Entity: san francisco  

Relation: work location  
Context: the headquarters of tech innovations is located in austin, where many employees collaborate on new projects.  
Head Entity: tech innovations  
Tail Entity: austin  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: Maria Gonzalez is a renowned chef known for her innovative culinary techniques and delicious recipes.  
Head Entity: Maria Gonzalez  
Tail Entity: chef  

Relation: occupation  
Context: Dr. James Smith has dedicated his life to research and is a leading scientist in the field of renewable energy.  
Head Entity: Dr. James Smith  
Tail Entity: scientist  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a World Heritage Site, attracting tourists from around the globe.  
Head Entity: Petra  
Tail Entity: World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: beverly bower ( september 30 , 1925 – march 24 , 2002 ) was an american operatic soprano who had an active international opera career from the mid-1950s through the early 1970s .
Head Entity: beverly bower
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti ( october 12 , 1935 – september 6 , 2007 ) was an italian tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey ( born march 27 , 1969 ) is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: singer  
Mixup data size:  558
MixupTrain:  epoch  0, batch     0 | loss: 2.4323461MixupTrain:  epoch  0, batch     1 | loss: 1.9375122MixupTrain:  epoch  0, batch     2 | loss: 1.3329872MixupTrain:  epoch  0, batch     3 | loss: 2.1173055MixupTrain:  epoch  0, batch     4 | loss: 2.2497026MixupTrain:  epoch  0, batch     5 | loss: 1.8587375MixupTrain:  epoch  0, batch     6 | loss: 1.7772978MixupTrain:  epoch  0, batch     7 | loss: 2.4058170MixupTrain:  epoch  0, batch     8 | loss: 1.9681893MixupTrain:  epoch  0, batch     9 | loss: 1.9801540MixupTrain:  epoch  0, batch    10 | loss: 1.9783288MixupTrain:  epoch  0, batch    11 | loss: 1.6956443MixupTrain:  epoch  0, batch    12 | loss: 2.3513743MixupTrain:  epoch  0, batch    13 | loss: 1.6807402MixupTrain:  epoch  0, batch    14 | loss: 1.7262662MixupTrain:  epoch  0, batch    15 | loss: 1.9031209MixupTrain:  epoch  0, batch    16 | loss: 1.6561915MixupTrain:  epoch  0, batch    17 | loss: 1.9986598MixupTrain:  epoch  0, batch    18 | loss: 1.7564147MixupTrain:  epoch  0, batch    19 | loss: 2.0236604MixupTrain:  epoch  0, batch    20 | loss: 2.2071906MixupTrain:  epoch  0, batch    21 | loss: 1.6857945MixupTrain:  epoch  0, batch    22 | loss: 1.8772086MixupTrain:  epoch  0, batch    23 | loss: 1.6750969MixupTrain:  epoch  0, batch    24 | loss: 2.0714662MixupTrain:  epoch  0, batch    25 | loss: 1.7640488MixupTrain:  epoch  0, batch    26 | loss: 1.9154555MixupTrain:  epoch  0, batch    27 | loss: 1.7681670MixupTrain:  epoch  0, batch    28 | loss: 1.7896345MixupTrain:  epoch  0, batch    29 | loss: 1.7080280MixupTrain:  epoch  0, batch    30 | loss: 1.5723239MixupTrain:  epoch  0, batch    31 | loss: 1.9660806MixupTrain:  epoch  0, batch    32 | loss: 2.1122166MixupTrain:  epoch  0, batch    33 | loss: 1.7479110MixupTrain:  epoch  0, batch    34 | loss: 1.5399084
MemoryTrain:  epoch  0, batch     0 | loss: 1.7472355MemoryTrain:  epoch  0, batch     1 | loss: 1.9574591MemoryTrain:  epoch  0, batch     2 | loss: 1.8527300MemoryTrain:  epoch  0, batch     3 | loss: 1.3385663MemoryTrain:  epoch  0, batch     4 | loss: 1.6633813MemoryTrain:  epoch  0, batch     5 | loss: 2.5667148MemoryTrain:  epoch  0, batch     6 | loss: 1.9064233MemoryTrain:  epoch  0, batch     7 | loss: 1.6301425MemoryTrain:  epoch  0, batch     8 | loss: 1.5896685MemoryTrain:  epoch  0, batch     9 | loss: 1.8737676MemoryTrain:  epoch  0, batch    10 | loss: 1.8708978MemoryTrain:  epoch  0, batch    11 | loss: 1.7370152MemoryTrain:  epoch  0, batch    12 | loss: 2.9868808MemoryTrain:  epoch  0, batch    13 | loss: 2.1326208MemoryTrain:  epoch  0, batch    14 | loss: 2.5924020MemoryTrain:  epoch  1, batch     0 | loss: 1.9732242MemoryTrain:  epoch  1, batch     1 | loss: 1.3247721MemoryTrain:  epoch  1, batch     2 | loss: 2.0764477MemoryTrain:  epoch  1, batch     3 | loss: 1.7367598MemoryTrain:  epoch  1, batch     4 | loss: 1.8899696MemoryTrain:  epoch  1, batch     5 | loss: 2.0330627MemoryTrain:  epoch  1, batch     6 | loss: 1.4846790MemoryTrain:  epoch  1, batch     7 | loss: 2.3142710MemoryTrain:  epoch  1, batch     8 | loss: 1.4330348MemoryTrain:  epoch  1, batch     9 | loss: 1.7353542MemoryTrain:  epoch  1, batch    10 | loss: 1.4034562MemoryTrain:  epoch  1, batch    11 | loss: 1.3526313MemoryTrain:  epoch  1, batch    12 | loss: 1.9032276MemoryTrain:  epoch  1, batch    13 | loss: 1.7147470MemoryTrain:  epoch  1, batch    14 | loss: 2.1037736MemoryTrain:  epoch  2, batch     0 | loss: 1.3136258MemoryTrain:  epoch  2, batch     1 | loss: 1.3785847MemoryTrain:  epoch  2, batch     2 | loss: 1.5402524MemoryTrain:  epoch  2, batch     3 | loss: 1.5660996MemoryTrain:  epoch  2, batch     4 | loss: 2.0304301MemoryTrain:  epoch  2, batch     5 | loss: 1.9130325MemoryTrain:  epoch  2, batch     6 | loss: 1.5012589MemoryTrain:  epoch  2, batch     7 | loss: 2.0628667MemoryTrain:  epoch  2, batch     8 | loss: 1.4132853MemoryTrain:  epoch  2, batch     9 | loss: 1.5081307MemoryTrain:  epoch  2, batch    10 | loss: 1.5858629MemoryTrain:  epoch  2, batch    11 | loss: 1.4903179MemoryTrain:  epoch  2, batch    12 | loss: 1.3201432MemoryTrain:  epoch  2, batch    13 | loss: 1.3488729MemoryTrain:  epoch  2, batch    14 | loss: 1.2544003MemoryTrain:  epoch  3, batch     0 | loss: 1.5320462MemoryTrain:  epoch  3, batch     1 | loss: 1.4006987MemoryTrain:  epoch  3, batch     2 | loss: 1.5599365MemoryTrain:  epoch  3, batch     3 | loss: 1.5718949MemoryTrain:  epoch  3, batch     4 | loss: 1.2076871MemoryTrain:  epoch  3, batch     5 | loss: 1.6788541MemoryTrain:  epoch  3, batch     6 | loss: 1.6156061MemoryTrain:  epoch  3, batch     7 | loss: 1.4796020MemoryTrain:  epoch  3, batch     8 | loss: 1.3101212MemoryTrain:  epoch  3, batch     9 | loss: 1.4320136MemoryTrain:  epoch  3, batch    10 | loss: 1.6004622MemoryTrain:  epoch  3, batch    11 | loss: 1.2252659MemoryTrain:  epoch  3, batch    12 | loss: 1.2469423MemoryTrain:  epoch  3, batch    13 | loss: 1.4723611MemoryTrain:  epoch  3, batch    14 | loss: 1.5687938MemoryTrain:  epoch  4, batch     0 | loss: 1.4138850MemoryTrain:  epoch  4, batch     1 | loss: 1.2197144MemoryTrain:  epoch  4, batch     2 | loss: 1.4276736MemoryTrain:  epoch  4, batch     3 | loss: 1.2430917MemoryTrain:  epoch  4, batch     4 | loss: 1.5292935MemoryTrain:  epoch  4, batch     5 | loss: 1.1754655MemoryTrain:  epoch  4, batch     6 | loss: 1.3763976MemoryTrain:  epoch  4, batch     7 | loss: 1.3953354MemoryTrain:  epoch  4, batch     8 | loss: 1.3255436MemoryTrain:  epoch  4, batch     9 | loss: 1.2402222MemoryTrain:  epoch  4, batch    10 | loss: 1.4072808MemoryTrain:  epoch  4, batch    11 | loss: 1.3134303MemoryTrain:  epoch  4, batch    12 | loss: 1.4775001MemoryTrain:  epoch  4, batch    13 | loss: 1.5720172MemoryTrain:  epoch  4, batch    14 | loss: 1.4349031MemoryTrain:  epoch  5, batch     0 | loss: 1.3841255MemoryTrain:  epoch  5, batch     1 | loss: 1.3687466MemoryTrain:  epoch  5, batch     2 | loss: 1.3516268MemoryTrain:  epoch  5, batch     3 | loss: 1.2542733MemoryTrain:  epoch  5, batch     4 | loss: 1.3364120MemoryTrain:  epoch  5, batch     5 | loss: 1.7365949MemoryTrain:  epoch  5, batch     6 | loss: 1.5090575MemoryTrain:  epoch  5, batch     7 | loss: 1.2916995MemoryTrain:  epoch  5, batch     8 | loss: 1.2568381MemoryTrain:  epoch  5, batch     9 | loss: 1.4261616MemoryTrain:  epoch  5, batch    10 | loss: 1.3905140MemoryTrain:  epoch  5, batch    11 | loss: 1.2527393MemoryTrain:  epoch  5, batch    12 | loss: 1.2780297MemoryTrain:  epoch  5, batch    13 | loss: 1.2372677MemoryTrain:  epoch  5, batch    14 | loss: 1.3328913MemoryTrain:  epoch  6, batch     0 | loss: 1.3725815MemoryTrain:  epoch  6, batch     1 | loss: 1.2684853MemoryTrain:  epoch  6, batch     2 | loss: 1.3874478MemoryTrain:  epoch  6, batch     3 | loss: 1.2396526MemoryTrain:  epoch  6, batch     4 | loss: 1.2381926MemoryTrain:  epoch  6, batch     5 | loss: 1.2940495MemoryTrain:  epoch  6, batch     6 | loss: 1.3895621MemoryTrain:  epoch  6, batch     7 | loss: 1.2632040MemoryTrain:  epoch  6, batch     8 | loss: 1.2853283MemoryTrain:  epoch  6, batch     9 | loss: 1.4063350MemoryTrain:  epoch  6, batch    10 | loss: 1.4131221MemoryTrain:  epoch  6, batch    11 | loss: 1.2579094MemoryTrain:  epoch  6, batch    12 | loss: 1.3257521MemoryTrain:  epoch  6, batch    13 | loss: 1.2798510MemoryTrain:  epoch  6, batch    14 | loss: 1.2481391MemoryTrain:  epoch  7, batch     0 | loss: 1.3909527MemoryTrain:  epoch  7, batch     1 | loss: 1.3488991MemoryTrain:  epoch  7, batch     2 | loss: 1.2528024MemoryTrain:  epoch  7, batch     3 | loss: 1.2979499MemoryTrain:  epoch  7, batch     4 | loss: 1.2666086MemoryTrain:  epoch  7, batch     5 | loss: 1.2073567MemoryTrain:  epoch  7, batch     6 | loss: 1.2615440MemoryTrain:  epoch  7, batch     7 | loss: 1.2290986MemoryTrain:  epoch  7, batch     8 | loss: 1.3349383MemoryTrain:  epoch  7, batch     9 | loss: 1.2654893MemoryTrain:  epoch  7, batch    10 | loss: 1.2316594MemoryTrain:  epoch  7, batch    11 | loss: 1.2761471MemoryTrain:  epoch  7, batch    12 | loss: 1.2436707MemoryTrain:  epoch  7, batch    13 | loss: 1.1926466MemoryTrain:  epoch  7, batch    14 | loss: 1.2319814MemoryTrain:  epoch  8, batch     0 | loss: 1.2325983MemoryTrain:  epoch  8, batch     1 | loss: 1.2248870MemoryTrain:  epoch  8, batch     2 | loss: 1.2458805MemoryTrain:  epoch  8, batch     3 | loss: 1.2686241MemoryTrain:  epoch  8, batch     4 | loss: 1.2007039MemoryTrain:  epoch  8, batch     5 | loss: 1.2262131MemoryTrain:  epoch  8, batch     6 | loss: 1.2367262MemoryTrain:  epoch  8, batch     7 | loss: 1.2627138MemoryTrain:  epoch  8, batch     8 | loss: 1.2083614MemoryTrain:  epoch  8, batch     9 | loss: 1.2622901MemoryTrain:  epoch  8, batch    10 | loss: 1.1790118MemoryTrain:  epoch  8, batch    11 | loss: 1.2874286MemoryTrain:  epoch  8, batch    12 | loss: 1.2546723MemoryTrain:  epoch  8, batch    13 | loss: 1.3118567MemoryTrain:  epoch  8, batch    14 | loss: 1.2902031MemoryTrain:  epoch  9, batch     0 | loss: 1.2445636MemoryTrain:  epoch  9, batch     1 | loss: 1.2378030MemoryTrain:  epoch  9, batch     2 | loss: 1.2113018MemoryTrain:  epoch  9, batch     3 | loss: 1.2369077MemoryTrain:  epoch  9, batch     4 | loss: 1.2443495MemoryTrain:  epoch  9, batch     5 | loss: 1.2283887MemoryTrain:  epoch  9, batch     6 | loss: 1.2445371MemoryTrain:  epoch  9, batch     7 | loss: 1.2150635MemoryTrain:  epoch  9, batch     8 | loss: 1.1872426MemoryTrain:  epoch  9, batch     9 | loss: 1.2468095MemoryTrain:  epoch  9, batch    10 | loss: 1.2329617MemoryTrain:  epoch  9, batch    11 | loss: 1.2305316MemoryTrain:  epoch  9, batch    12 | loss: 1.2197636MemoryTrain:  epoch  9, batch    13 | loss: 1.1816336MemoryTrain:  epoch  9, batch    14 | loss: 1.2781875
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 45.83%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 42.19%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 42.50%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 44.79%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 45.54%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 46.88%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 49.31%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 49.43%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 50.52%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 52.40%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 53.12%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 53.75%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 54.30%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 54.41%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 54.17%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 57.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 59.82%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 61.65%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 63.04%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 64.32%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 65.75%   [EVAL] batch:   25 | acc: 56.25%,  total acc: 65.38%   [EVAL] batch:   26 | acc: 31.25%,  total acc: 64.12%   [EVAL] batch:   27 | acc: 37.50%,  total acc: 63.17%   [EVAL] batch:   28 | acc: 56.25%,  total acc: 62.93%   [EVAL] batch:   29 | acc: 43.75%,  total acc: 62.29%   [EVAL] batch:   30 | acc: 43.75%,  total acc: 61.69%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 61.72%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 61.74%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 60.85%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 60.54%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 59.55%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 58.78%   [EVAL] batch:   37 | acc: 37.50%,  total acc: 58.22%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 58.33%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 58.28%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 58.08%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 57.89%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 58.14%   [EVAL] batch:   43 | acc: 43.75%,  total acc: 57.81%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 58.47%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 58.83%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 59.04%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 59.38%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 59.69%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 60.12%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 60.91%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 61.66%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 62.38%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 63.08%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 63.75%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 64.40%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 65.02%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 65.41%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 65.89%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 66.35%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 66.80%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 67.34%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 67.06%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 68.75%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 66.48%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 64.58%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 64.42%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 66.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 68.33%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 69.92%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 71.32%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 72.57%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 74.01%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 72.81%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 73.21%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 72.73%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 72.01%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 72.40%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 72.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.80%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 74.54%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 75.65%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.46%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 77.02%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 77.73%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 78.41%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 79.04%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 79.46%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 79.86%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 80.41%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 80.59%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 80.93%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 80.96%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 81.11%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 80.83%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 80.98%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 80.32%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 79.95%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 79.72%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 79.25%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 78.97%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 79.25%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 78.94%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 78.52%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 78.68%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 78.40%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 77.91%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 77.65%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 77.71%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 77.77%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 77.62%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 77.08%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 76.17%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 75.19%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 74.24%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 73.23%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 72.70%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 72.01%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 72.05%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 71.83%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 71.61%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 71.58%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 71.62%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 71.75%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 71.55%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 71.51%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 71.39%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 71.04%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 71.02%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 71.06%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 70.96%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 70.71%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 70.24%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 70.07%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 69.77%   [EVAL] batch:   86 | acc: 81.25%,  total acc: 69.90%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 69.89%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 70.15%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 70.49%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 70.47%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 70.72%   [EVAL] batch:   92 | acc: 87.50%,  total acc: 70.90%   [EVAL] batch:   93 | acc: 81.25%,  total acc: 71.01%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 71.32%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 71.61%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 71.91%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 72.19%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 72.47%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 72.75%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 72.96%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 73.16%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 73.18%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 73.32%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 73.51%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 73.58%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 73.31%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 72.86%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 72.48%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 71.93%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 71.51%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 71.21%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 71.07%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 71.22%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 71.47%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 71.61%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 71.74%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 71.85%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 71.25%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 70.76%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 70.18%   [EVAL] batch:  122 | acc: 0.00%,  total acc: 69.61%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 69.10%   [EVAL] batch:  124 | acc: 0.00%,  total acc: 68.55%   [EVAL] batch:  125 | acc: 50.00%,  total acc: 68.40%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 68.11%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 67.82%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 67.49%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 67.36%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 67.18%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 67.33%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 67.53%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 67.68%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 67.78%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 67.97%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 68.20%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 68.34%   [EVAL] batch:  138 | acc: 50.00%,  total acc: 68.21%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 68.21%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 68.31%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 68.35%   [EVAL] batch:  142 | acc: 62.50%,  total acc: 68.31%   [EVAL] batch:  143 | acc: 75.00%,  total acc: 68.36%   [EVAL] batch:  144 | acc: 50.00%,  total acc: 68.23%   [EVAL] batch:  145 | acc: 56.25%,  total acc: 68.15%   [EVAL] batch:  146 | acc: 43.75%,  total acc: 67.98%   [EVAL] batch:  147 | acc: 43.75%,  total acc: 67.82%   [EVAL] batch:  148 | acc: 56.25%,  total acc: 67.74%   [EVAL] batch:  149 | acc: 18.75%,  total acc: 67.42%   [EVAL] batch:  150 | acc: 56.25%,  total acc: 67.34%   [EVAL] batch:  151 | acc: 50.00%,  total acc: 67.23%   [EVAL] batch:  152 | acc: 62.50%,  total acc: 67.20%   [EVAL] batch:  153 | acc: 56.25%,  total acc: 67.13%   [EVAL] batch:  154 | acc: 37.50%,  total acc: 66.94%   [EVAL] batch:  155 | acc: 37.50%,  total acc: 66.75%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 66.72%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 66.61%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 66.51%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 66.41%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 66.38%   [EVAL] batch:  161 | acc: 43.75%,  total acc: 66.24%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 66.22%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 66.20%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 66.25%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 66.23%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 66.28%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 66.33%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 66.27%   [EVAL] batch:  169 | acc: 12.50%,  total acc: 65.96%   [EVAL] batch:  170 | acc: 12.50%,  total acc: 65.64%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 65.33%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 65.14%   [EVAL] batch:  173 | acc: 6.25%,  total acc: 64.80%   [EVAL] batch:  174 | acc: 18.75%,  total acc: 64.54%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 64.74%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 64.94%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 65.13%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 65.33%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 65.52%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 65.71%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 65.90%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 65.92%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 66.07%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 66.15%   [EVAL] batch:  185 | acc: 62.50%,  total acc: 66.13%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 66.24%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 66.09%   [EVAL] batch:  188 | acc: 12.50%,  total acc: 65.81%   [EVAL] batch:  189 | acc: 12.50%,  total acc: 65.53%   [EVAL] batch:  190 | acc: 6.25%,  total acc: 65.22%   [EVAL] batch:  191 | acc: 6.25%,  total acc: 64.91%   [EVAL] batch:  192 | acc: 0.00%,  total acc: 64.57%   [EVAL] batch:  193 | acc: 12.50%,  total acc: 64.30%   [EVAL] batch:  194 | acc: 62.50%,  total acc: 64.29%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 64.22%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 64.28%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 64.33%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 64.35%   [EVAL] batch:  199 | acc: 68.75%,  total acc: 64.38%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 64.46%   [EVAL] batch:  201 | acc: 62.50%,  total acc: 64.45%   [EVAL] batch:  202 | acc: 50.00%,  total acc: 64.38%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 64.46%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 64.54%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 64.59%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 64.70%   [EVAL] batch:  207 | acc: 93.75%,  total acc: 64.84%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 65.01%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 65.15%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 65.28%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 65.45%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 65.49%   [EVAL] batch:  213 | acc: 87.50%,  total acc: 65.60%   [EVAL] batch:  214 | acc: 75.00%,  total acc: 65.64%   [EVAL] batch:  215 | acc: 81.25%,  total acc: 65.71%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 65.75%   [EVAL] batch:  217 | acc: 87.50%,  total acc: 65.85%   [EVAL] batch:  218 | acc: 75.00%,  total acc: 65.90%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 66.05%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 66.20%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 66.33%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 66.45%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 66.60%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 66.75%   [EVAL] batch:  225 | acc: 43.75%,  total acc: 66.65%   [EVAL] batch:  226 | acc: 31.25%,  total acc: 66.49%   [EVAL] batch:  227 | acc: 31.25%,  total acc: 66.34%   [EVAL] batch:  228 | acc: 37.50%,  total acc: 66.21%   [EVAL] batch:  229 | acc: 18.75%,  total acc: 66.01%   [EVAL] batch:  230 | acc: 56.25%,  total acc: 65.96%   [EVAL] batch:  231 | acc: 81.25%,  total acc: 66.03%   [EVAL] batch:  232 | acc: 75.00%,  total acc: 66.07%   [EVAL] batch:  233 | acc: 81.25%,  total acc: 66.13%   [EVAL] batch:  234 | acc: 81.25%,  total acc: 66.20%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 66.18%   [EVAL] batch:  236 | acc: 56.25%,  total acc: 66.14%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 66.15%   [EVAL] batch:  238 | acc: 68.75%,  total acc: 66.16%   [EVAL] batch:  239 | acc: 43.75%,  total acc: 66.07%   [EVAL] batch:  240 | acc: 43.75%,  total acc: 65.98%   [EVAL] batch:  241 | acc: 68.75%,  total acc: 65.99%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 65.97%   [EVAL] batch:  243 | acc: 25.00%,  total acc: 65.80%   [EVAL] batch:  244 | acc: 56.25%,  total acc: 65.77%   [EVAL] batch:  245 | acc: 62.50%,  total acc: 65.75%   [EVAL] batch:  246 | acc: 50.00%,  total acc: 65.69%   [EVAL] batch:  247 | acc: 68.75%,  total acc: 65.70%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 65.79%   [EVAL] batch:  249 | acc: 31.25%,  total acc: 65.65%   [EVAL] batch:  250 | acc: 100.00%,  total acc: 65.79%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 65.90%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 66.01%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 66.09%   [EVAL] batch:  254 | acc: 87.50%,  total acc: 66.18%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 66.31%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 66.29%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 66.33%   [EVAL] batch:  258 | acc: 50.00%,  total acc: 66.26%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 66.23%   [EVAL] batch:  260 | acc: 37.50%,  total acc: 66.12%   [EVAL] batch:  261 | acc: 62.50%,  total acc: 66.10%   [EVAL] batch:  262 | acc: 75.00%,  total acc: 66.14%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 66.17%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 66.20%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 66.24%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 66.29%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 66.30%   [EVAL] batch:  268 | acc: 56.25%,  total acc: 66.26%   [EVAL] batch:  269 | acc: 25.00%,  total acc: 66.11%   [EVAL] batch:  270 | acc: 25.00%,  total acc: 65.96%   [EVAL] batch:  271 | acc: 25.00%,  total acc: 65.81%   [EVAL] batch:  272 | acc: 18.75%,  total acc: 65.64%   [EVAL] batch:  273 | acc: 50.00%,  total acc: 65.58%   [EVAL] batch:  274 | acc: 18.75%,  total acc: 65.41%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 65.53%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 65.66%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 65.78%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 65.91%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 66.03%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 66.15%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 66.16%   [EVAL] batch:  282 | acc: 81.25%,  total acc: 66.21%   [EVAL] batch:  283 | acc: 50.00%,  total acc: 66.15%   [EVAL] batch:  284 | acc: 68.75%,  total acc: 66.16%   [EVAL] batch:  285 | acc: 68.75%,  total acc: 66.17%   [EVAL] batch:  286 | acc: 56.25%,  total acc: 66.14%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 66.15%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 66.26%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 66.38%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 66.49%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 66.59%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 66.70%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 66.79%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 66.91%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 67.02%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 67.13%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 67.24%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 67.35%   [EVAL] batch:  299 | acc: 93.75%,  total acc: 67.44%   [EVAL] batch:  300 | acc: 87.50%,  total acc: 67.50%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 67.59%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 67.68%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 67.76%   [EVAL] batch:  304 | acc: 87.50%,  total acc: 67.83%   [EVAL] batch:  305 | acc: 81.25%,  total acc: 67.87%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 67.96%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 68.04%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 68.12%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 68.23%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 68.33%   [EVAL] batch:  311 | acc: 93.75%,  total acc: 68.41%   [EVAL] batch:  312 | acc: 62.50%,  total acc: 68.39%   [EVAL] batch:  313 | acc: 6.25%,  total acc: 68.19%   [EVAL] batch:  314 | acc: 12.50%,  total acc: 68.02%   [EVAL] batch:  315 | acc: 6.25%,  total acc: 67.82%   [EVAL] batch:  316 | acc: 0.00%,  total acc: 67.61%   [EVAL] batch:  317 | acc: 12.50%,  total acc: 67.43%   [EVAL] batch:  318 | acc: 37.50%,  total acc: 67.34%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 67.36%   [EVAL] batch:  320 | acc: 87.50%,  total acc: 67.43%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 67.49%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 67.57%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 67.59%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 67.65%   [EVAL] batch:  325 | acc: 81.25%,  total acc: 67.70%   [EVAL] batch:  326 | acc: 50.00%,  total acc: 67.64%   [EVAL] batch:  327 | acc: 75.00%,  total acc: 67.66%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 67.69%   [EVAL] batch:  329 | acc: 62.50%,  total acc: 67.67%   [EVAL] batch:  330 | acc: 62.50%,  total acc: 67.65%   [EVAL] batch:  331 | acc: 12.50%,  total acc: 67.49%   [EVAL] batch:  332 | acc: 12.50%,  total acc: 67.32%   [EVAL] batch:  333 | acc: 18.75%,  total acc: 67.18%   [EVAL] batch:  334 | acc: 6.25%,  total acc: 67.00%   [EVAL] batch:  335 | acc: 12.50%,  total acc: 66.83%   [EVAL] batch:  336 | acc: 12.50%,  total acc: 66.67%   [EVAL] batch:  337 | acc: 6.25%,  total acc: 66.49%   [EVAL] batch:  338 | acc: 0.00%,  total acc: 66.30%   [EVAL] batch:  339 | acc: 6.25%,  total acc: 66.12%   [EVAL] batch:  340 | acc: 0.00%,  total acc: 65.93%   [EVAL] batch:  341 | acc: 12.50%,  total acc: 65.77%   [EVAL] batch:  342 | acc: 6.25%,  total acc: 65.60%   [EVAL] batch:  343 | acc: 12.50%,  total acc: 65.44%   [EVAL] batch:  344 | acc: 93.75%,  total acc: 65.53%   [EVAL] batch:  345 | acc: 87.50%,  total acc: 65.59%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 65.65%   [EVAL] batch:  347 | acc: 87.50%,  total acc: 65.71%   [EVAL] batch:  348 | acc: 87.50%,  total acc: 65.78%   [EVAL] batch:  349 | acc: 81.25%,  total acc: 65.82%   [EVAL] batch:  350 | acc: 93.75%,  total acc: 65.90%   [EVAL] batch:  351 | acc: 93.75%,  total acc: 65.98%   [EVAL] batch:  352 | acc: 93.75%,  total acc: 66.06%   [EVAL] batch:  353 | acc: 87.50%,  total acc: 66.12%   [EVAL] batch:  354 | acc: 93.75%,  total acc: 66.20%   [EVAL] batch:  355 | acc: 100.00%,  total acc: 66.29%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 66.35%   [EVAL] batch:  357 | acc: 56.25%,  total acc: 66.32%   [EVAL] batch:  358 | acc: 62.50%,  total acc: 66.31%   [EVAL] batch:  359 | acc: 62.50%,  total acc: 66.30%   [EVAL] batch:  360 | acc: 62.50%,  total acc: 66.29%   [EVAL] batch:  361 | acc: 56.25%,  total acc: 66.26%   [EVAL] batch:  362 | acc: 43.75%,  total acc: 66.20%   [EVAL] batch:  363 | acc: 43.75%,  total acc: 66.14%   [EVAL] batch:  364 | acc: 50.00%,  total acc: 66.10%   [EVAL] batch:  365 | acc: 68.75%,  total acc: 66.10%   [EVAL] batch:  366 | acc: 43.75%,  total acc: 66.04%   [EVAL] batch:  367 | acc: 68.75%,  total acc: 66.05%   [EVAL] batch:  368 | acc: 56.25%,  total acc: 66.02%   [EVAL] batch:  369 | acc: 50.00%,  total acc: 65.98%   [EVAL] batch:  370 | acc: 50.00%,  total acc: 65.94%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 65.89%   [EVAL] batch:  372 | acc: 50.00%,  total acc: 65.85%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 65.89%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 65.92%   [EVAL] batch:  375 | acc: 6.25%,  total acc: 65.76%   [EVAL] batch:  376 | acc: 6.25%,  total acc: 65.60%   [EVAL] batch:  377 | acc: 25.00%,  total acc: 65.49%   [EVAL] batch:  378 | acc: 25.00%,  total acc: 65.39%   [EVAL] batch:  379 | acc: 18.75%,  total acc: 65.26%   [EVAL] batch:  380 | acc: 6.25%,  total acc: 65.11%   [EVAL] batch:  381 | acc: 68.75%,  total acc: 65.12%   [EVAL] batch:  382 | acc: 87.50%,  total acc: 65.18%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 65.25%   [EVAL] batch:  384 | acc: 75.00%,  total acc: 65.28%   [EVAL] batch:  385 | acc: 68.75%,  total acc: 65.28%   [EVAL] batch:  386 | acc: 87.50%,  total acc: 65.34%   [EVAL] batch:  387 | acc: 56.25%,  total acc: 65.32%   [EVAL] batch:  388 | acc: 25.00%,  total acc: 65.22%   [EVAL] batch:  389 | acc: 25.00%,  total acc: 65.11%   [EVAL] batch:  390 | acc: 6.25%,  total acc: 64.96%   [EVAL] batch:  391 | acc: 37.50%,  total acc: 64.89%   [EVAL] batch:  392 | acc: 25.00%,  total acc: 64.79%   [EVAL] batch:  393 | acc: 37.50%,  total acc: 64.72%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 64.79%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 64.88%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 64.97%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 65.06%   [EVAL] batch:  398 | acc: 100.00%,  total acc: 65.15%   [EVAL] batch:  399 | acc: 93.75%,  total acc: 65.22%   [EVAL] batch:  400 | acc: 25.00%,  total acc: 65.12%   [EVAL] batch:  401 | acc: 18.75%,  total acc: 65.00%   [EVAL] batch:  402 | acc: 25.00%,  total acc: 64.90%   [EVAL] batch:  403 | acc: 31.25%,  total acc: 64.82%   [EVAL] batch:  404 | acc: 12.50%,  total acc: 64.69%   [EVAL] batch:  405 | acc: 37.50%,  total acc: 64.62%   [EVAL] batch:  406 | acc: 75.00%,  total acc: 64.65%   [EVAL] batch:  407 | acc: 93.75%,  total acc: 64.72%   [EVAL] batch:  408 | acc: 93.75%,  total acc: 64.79%   [EVAL] batch:  409 | acc: 93.75%,  total acc: 64.86%   [EVAL] batch:  410 | acc: 87.50%,  total acc: 64.92%   [EVAL] batch:  411 | acc: 81.25%,  total acc: 64.96%   [EVAL] batch:  412 | acc: 81.25%,  total acc: 65.00%   [EVAL] batch:  413 | acc: 81.25%,  total acc: 65.04%   [EVAL] batch:  414 | acc: 62.50%,  total acc: 65.03%   [EVAL] batch:  415 | acc: 75.00%,  total acc: 65.05%   [EVAL] batch:  416 | acc: 68.75%,  total acc: 65.06%   [EVAL] batch:  417 | acc: 62.50%,  total acc: 65.06%   [EVAL] batch:  418 | acc: 68.75%,  total acc: 65.07%   [EVAL] batch:  419 | acc: 37.50%,  total acc: 65.00%   [EVAL] batch:  420 | acc: 62.50%,  total acc: 64.99%   [EVAL] batch:  421 | acc: 56.25%,  total acc: 64.97%   [EVAL] batch:  422 | acc: 50.00%,  total acc: 64.94%   [EVAL] batch:  423 | acc: 50.00%,  total acc: 64.90%   [EVAL] batch:  424 | acc: 43.75%,  total acc: 64.85%   [EVAL] batch:  425 | acc: 68.75%,  total acc: 64.86%   [EVAL] batch:  426 | acc: 62.50%,  total acc: 64.86%   [EVAL] batch:  427 | acc: 68.75%,  total acc: 64.87%   [EVAL] batch:  428 | acc: 93.75%,  total acc: 64.93%   [EVAL] batch:  429 | acc: 93.75%,  total acc: 65.00%   [EVAL] batch:  430 | acc: 87.50%,  total acc: 65.05%   [EVAL] batch:  431 | acc: 75.00%,  total acc: 65.08%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 65.13%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 65.19%   [EVAL] batch:  434 | acc: 87.50%,  total acc: 65.24%   [EVAL] batch:  435 | acc: 100.00%,  total acc: 65.32%   [EVAL] batch:  436 | acc: 81.25%,  total acc: 65.36%   [EVAL] batch:  437 | acc: 56.25%,  total acc: 65.34%   [EVAL] batch:  438 | acc: 50.00%,  total acc: 65.30%   [EVAL] batch:  439 | acc: 56.25%,  total acc: 65.28%   [EVAL] batch:  440 | acc: 43.75%,  total acc: 65.24%   [EVAL] batch:  441 | acc: 37.50%,  total acc: 65.17%   [EVAL] batch:  442 | acc: 43.75%,  total acc: 65.12%   [EVAL] batch:  443 | acc: 50.00%,  total acc: 65.09%   [EVAL] batch:  444 | acc: 56.25%,  total acc: 65.07%   [EVAL] batch:  445 | acc: 62.50%,  total acc: 65.06%   [EVAL] batch:  446 | acc: 68.75%,  total acc: 65.07%   [EVAL] batch:  447 | acc: 56.25%,  total acc: 65.05%   [EVAL] batch:  448 | acc: 25.00%,  total acc: 64.96%   [EVAL] batch:  449 | acc: 100.00%,  total acc: 65.04%   [EVAL] batch:  450 | acc: 62.50%,  total acc: 65.04%   [EVAL] batch:  451 | acc: 56.25%,  total acc: 65.02%   [EVAL] batch:  452 | acc: 68.75%,  total acc: 65.02%   [EVAL] batch:  453 | acc: 56.25%,  total acc: 65.01%   [EVAL] batch:  454 | acc: 43.75%,  total acc: 64.96%   [EVAL] batch:  455 | acc: 81.25%,  total acc: 64.99%   [EVAL] batch:  456 | acc: 87.50%,  total acc: 65.04%   [EVAL] batch:  457 | acc: 93.75%,  total acc: 65.11%   [EVAL] batch:  458 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:  459 | acc: 93.75%,  total acc: 65.24%   [EVAL] batch:  460 | acc: 93.75%,  total acc: 65.31%   [EVAL] batch:  461 | acc: 100.00%,  total acc: 65.38%   [EVAL] batch:  462 | acc: 75.00%,  total acc: 65.40%   [EVAL] batch:  463 | acc: 50.00%,  total acc: 65.37%   [EVAL] batch:  464 | acc: 25.00%,  total acc: 65.28%   [EVAL] batch:  465 | acc: 56.25%,  total acc: 65.26%   [EVAL] batch:  466 | acc: 43.75%,  total acc: 65.22%   [EVAL] batch:  467 | acc: 56.25%,  total acc: 65.20%   [EVAL] batch:  468 | acc: 37.50%,  total acc: 65.14%   [EVAL] batch:  469 | acc: 75.00%,  total acc: 65.16%   [EVAL] batch:  470 | acc: 43.75%,  total acc: 65.11%   [EVAL] batch:  471 | acc: 31.25%,  total acc: 65.04%   [EVAL] batch:  472 | acc: 43.75%,  total acc: 65.00%   [EVAL] batch:  473 | acc: 25.00%,  total acc: 64.91%   [EVAL] batch:  474 | acc: 37.50%,  total acc: 64.86%   [EVAL] batch:  475 | acc: 43.75%,  total acc: 64.81%   [EVAL] batch:  476 | acc: 62.50%,  total acc: 64.81%   [EVAL] batch:  477 | acc: 50.00%,  total acc: 64.78%   [EVAL] batch:  478 | acc: 62.50%,  total acc: 64.77%   [EVAL] batch:  479 | acc: 50.00%,  total acc: 64.74%   [EVAL] batch:  480 | acc: 50.00%,  total acc: 64.71%   [EVAL] batch:  481 | acc: 75.00%,  total acc: 64.73%   [EVAL] batch:  482 | acc: 81.25%,  total acc: 64.76%   [EVAL] batch:  483 | acc: 75.00%,  total acc: 64.79%   [EVAL] batch:  484 | acc: 62.50%,  total acc: 64.78%   [EVAL] batch:  485 | acc: 81.25%,  total acc: 64.81%   [EVAL] batch:  486 | acc: 75.00%,  total acc: 64.84%   [EVAL] batch:  487 | acc: 93.75%,  total acc: 64.89%   [EVAL] batch:  488 | acc: 100.00%,  total acc: 64.97%   [EVAL] batch:  489 | acc: 100.00%,  total acc: 65.04%   [EVAL] batch:  490 | acc: 100.00%,  total acc: 65.11%   [EVAL] batch:  491 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:  492 | acc: 100.00%,  total acc: 65.25%   [EVAL] batch:  493 | acc: 100.00%,  total acc: 65.32%   [EVAL] batch:  494 | acc: 100.00%,  total acc: 65.39%   [EVAL] batch:  495 | acc: 81.25%,  total acc: 65.42%   [EVAL] batch:  496 | acc: 100.00%,  total acc: 65.49%   [EVAL] batch:  497 | acc: 87.50%,  total acc: 65.54%   [EVAL] batch:  498 | acc: 100.00%,  total acc: 65.61%   [EVAL] batch:  499 | acc: 100.00%,  total acc: 65.67%   
cur_acc:  ['0.9504', '0.7292', '0.7837', '0.7560', '0.8849', '0.6240', '0.6468', '0.6706']
his_acc:  ['0.9504', '0.8365', '0.7989', '0.7595', '0.7702', '0.7228', '0.6794', '0.6567']
--------Round  3
seed:  400
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 0 1 2 5 3 4 6]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 13.0163383CurrentTrain: epoch  0, batch     1 | loss: 12.6199055CurrentTrain: epoch  0, batch     2 | loss: 12.0659122CurrentTrain: epoch  0, batch     3 | loss: 12.0403185CurrentTrain: epoch  0, batch     4 | loss: 11.8768311CurrentTrain: epoch  0, batch     5 | loss: 11.7116613CurrentTrain: epoch  0, batch     6 | loss: 11.7157192CurrentTrain: epoch  0, batch     7 | loss: 11.3147545CurrentTrain: epoch  0, batch     8 | loss: 11.4094534CurrentTrain: epoch  0, batch     9 | loss: 11.2277946CurrentTrain: epoch  0, batch    10 | loss: 11.0203552CurrentTrain: epoch  0, batch    11 | loss: 10.5956554CurrentTrain: epoch  0, batch    12 | loss: 10.9211311CurrentTrain: epoch  0, batch    13 | loss: 10.7034130CurrentTrain: epoch  0, batch    14 | loss: 10.5737906CurrentTrain: epoch  0, batch    15 | loss: 10.5752964CurrentTrain: epoch  0, batch    16 | loss: 10.7940025CurrentTrain: epoch  0, batch    17 | loss: 10.5792618CurrentTrain: epoch  0, batch    18 | loss: 10.9380417CurrentTrain: epoch  0, batch    19 | loss: 10.1757383CurrentTrain: epoch  0, batch    20 | loss: 10.2315102CurrentTrain: epoch  0, batch    21 | loss: 10.3067427CurrentTrain: epoch  0, batch    22 | loss: 10.5523834CurrentTrain: epoch  0, batch    23 | loss: 10.5063620CurrentTrain: epoch  0, batch    24 | loss: 9.7297564CurrentTrain: epoch  0, batch    25 | loss: 10.0788116CurrentTrain: epoch  0, batch    26 | loss: 10.0636282CurrentTrain: epoch  0, batch    27 | loss: 9.4162531CurrentTrain: epoch  0, batch    28 | loss: 10.0759029CurrentTrain: epoch  0, batch    29 | loss: 9.3998032CurrentTrain: epoch  0, batch    30 | loss: 9.1157265CurrentTrain: epoch  0, batch    31 | loss: 9.8245325CurrentTrain: epoch  0, batch    32 | loss: 9.2154570CurrentTrain: epoch  0, batch    33 | loss: 9.6751699CurrentTrain: epoch  0, batch    34 | loss: 10.3828888CurrentTrain: epoch  0, batch    35 | loss: 9.6718035CurrentTrain: epoch  0, batch    36 | loss: 9.3169308CurrentTrain: epoch  0, batch    37 | loss: 9.3295126CurrentTrain: epoch  0, batch    38 | loss: 9.3993340CurrentTrain: epoch  0, batch    39 | loss: 8.8432589CurrentTrain: epoch  0, batch    40 | loss: 9.3864031CurrentTrain: epoch  0, batch    41 | loss: 8.8405275CurrentTrain: epoch  0, batch    42 | loss: 9.6477613CurrentTrain: epoch  0, batch    43 | loss: 9.1011858CurrentTrain: epoch  0, batch    44 | loss: 8.4170284CurrentTrain: epoch  0, batch    45 | loss: 9.7504997CurrentTrain: epoch  0, batch    46 | loss: 9.0205469CurrentTrain: epoch  0, batch    47 | loss: 9.0162640CurrentTrain: epoch  0, batch    48 | loss: 8.7220411CurrentTrain: epoch  0, batch    49 | loss: 8.7937851CurrentTrain: epoch  0, batch    50 | loss: 8.1781006CurrentTrain: epoch  0, batch    51 | loss: 8.4636955CurrentTrain: epoch  0, batch    52 | loss: 8.9256325CurrentTrain: epoch  0, batch    53 | loss: 7.6919208CurrentTrain: epoch  0, batch    54 | loss: 8.2043610CurrentTrain: epoch  0, batch    55 | loss: 8.5009098CurrentTrain: epoch  0, batch    56 | loss: 8.1992073CurrentTrain: epoch  0, batch    57 | loss: 8.5525846CurrentTrain: epoch  0, batch    58 | loss: 8.6235027CurrentTrain: epoch  0, batch    59 | loss: 8.9326553CurrentTrain: epoch  0, batch    60 | loss: 7.3446383CurrentTrain: epoch  0, batch    61 | loss: 7.6555519CurrentTrain: epoch  0, batch    62 | loss: 9.2307558CurrentTrain: epoch  1, batch     0 | loss: 7.9879603CurrentTrain: epoch  1, batch     1 | loss: 8.0525990CurrentTrain: epoch  1, batch     2 | loss: 7.0878687CurrentTrain: epoch  1, batch     3 | loss: 8.1120319CurrentTrain: epoch  1, batch     4 | loss: 8.0961752CurrentTrain: epoch  1, batch     5 | loss: 7.6915245CurrentTrain: epoch  1, batch     6 | loss: 7.1398592CurrentTrain: epoch  1, batch     7 | loss: 7.1556044CurrentTrain: epoch  1, batch     8 | loss: 7.4782257CurrentTrain: epoch  1, batch     9 | loss: 8.3168697CurrentTrain: epoch  1, batch    10 | loss: 7.0727654CurrentTrain: epoch  1, batch    11 | loss: 7.3950396CurrentTrain: epoch  1, batch    12 | loss: 7.3605399CurrentTrain: epoch  1, batch    13 | loss: 8.0578642CurrentTrain: epoch  1, batch    14 | loss: 7.4617534CurrentTrain: epoch  1, batch    15 | loss: 8.6566992CurrentTrain: epoch  1, batch    16 | loss: 7.0673656CurrentTrain: epoch  1, batch    17 | loss: 7.5902052CurrentTrain: epoch  1, batch    18 | loss: 7.7627311CurrentTrain: epoch  1, batch    19 | loss: 7.1827917CurrentTrain: epoch  1, batch    20 | loss: 7.6774087CurrentTrain: epoch  1, batch    21 | loss: 7.4480209CurrentTrain: epoch  1, batch    22 | loss: 7.0330477CurrentTrain: epoch  1, batch    23 | loss: 6.9588623CurrentTrain: epoch  1, batch    24 | loss: 6.8513927CurrentTrain: epoch  1, batch    25 | loss: 6.4217920CurrentTrain: epoch  1, batch    26 | loss: 6.7420459CurrentTrain: epoch  1, batch    27 | loss: 6.9646845CurrentTrain: epoch  1, batch    28 | loss: 6.6440148CurrentTrain: epoch  1, batch    29 | loss: 7.1710587CurrentTrain: epoch  1, batch    30 | loss: 6.9588919CurrentTrain: epoch  1, batch    31 | loss: 7.0668430CurrentTrain: epoch  1, batch    32 | loss: 6.6530962CurrentTrain: epoch  1, batch    33 | loss: 7.1381364CurrentTrain: epoch  1, batch    34 | loss: 6.8076553CurrentTrain: epoch  1, batch    35 | loss: 7.5922327CurrentTrain: epoch  1, batch    36 | loss: 7.0507717CurrentTrain: epoch  1, batch    37 | loss: 7.2319298CurrentTrain: epoch  1, batch    38 | loss: 6.5506759CurrentTrain: epoch  1, batch    39 | loss: 6.1676712CurrentTrain: epoch  1, batch    40 | loss: 6.4682131CurrentTrain: epoch  1, batch    41 | loss: 6.5197330CurrentTrain: epoch  1, batch    42 | loss: 6.4888239CurrentTrain: epoch  1, batch    43 | loss: 6.7314525CurrentTrain: epoch  1, batch    44 | loss: 5.9978232CurrentTrain: epoch  1, batch    45 | loss: 6.3152504CurrentTrain: epoch  1, batch    46 | loss: 7.5495934CurrentTrain: epoch  1, batch    47 | loss: 6.3168917CurrentTrain: epoch  1, batch    48 | loss: 6.4961138CurrentTrain: epoch  1, batch    49 | loss: 5.9808931CurrentTrain: epoch  1, batch    50 | loss: 6.5988693CurrentTrain: epoch  1, batch    51 | loss: 6.5492983CurrentTrain: epoch  1, batch    52 | loss: 5.8307743CurrentTrain: epoch  1, batch    53 | loss: 6.0652308CurrentTrain: epoch  1, batch    54 | loss: 6.1071124CurrentTrain: epoch  1, batch    55 | loss: 6.0025845CurrentTrain: epoch  1, batch    56 | loss: 6.3767405CurrentTrain: epoch  1, batch    57 | loss: 5.6210580CurrentTrain: epoch  1, batch    58 | loss: 6.4323883CurrentTrain: epoch  1, batch    59 | loss: 7.1498237CurrentTrain: epoch  1, batch    60 | loss: 5.8531494CurrentTrain: epoch  1, batch    61 | loss: 6.7285686CurrentTrain: epoch  1, batch    62 | loss: 5.0404530CurrentTrain: epoch  2, batch     0 | loss: 5.4454432CurrentTrain: epoch  2, batch     1 | loss: 4.9075985CurrentTrain: epoch  2, batch     2 | loss: 6.3146763CurrentTrain: epoch  2, batch     3 | loss: 6.5142512CurrentTrain: epoch  2, batch     4 | loss: 6.2288685CurrentTrain: epoch  2, batch     5 | loss: 6.2334175CurrentTrain: epoch  2, batch     6 | loss: 5.9763336CurrentTrain: epoch  2, batch     7 | loss: 6.5718465CurrentTrain: epoch  2, batch     8 | loss: 5.7546291CurrentTrain: epoch  2, batch     9 | loss: 6.2642736CurrentTrain: epoch  2, batch    10 | loss: 5.7182574CurrentTrain: epoch  2, batch    11 | loss: 6.9638443CurrentTrain: epoch  2, batch    12 | loss: 6.0211391CurrentTrain: epoch  2, batch    13 | loss: 5.4548316CurrentTrain: epoch  2, batch    14 | loss: 5.9079914CurrentTrain: epoch  2, batch    15 | loss: 5.8308740CurrentTrain: epoch  2, batch    16 | loss: 5.8758178CurrentTrain: epoch  2, batch    17 | loss: 5.6289186CurrentTrain: epoch  2, batch    18 | loss: 5.9335093CurrentTrain: epoch  2, batch    19 | loss: 5.9112482CurrentTrain: epoch  2, batch    20 | loss: 5.9476185CurrentTrain: epoch  2, batch    21 | loss: 5.5129890CurrentTrain: epoch  2, batch    22 | loss: 5.9723134CurrentTrain: epoch  2, batch    23 | loss: 4.8421016CurrentTrain: epoch  2, batch    24 | loss: 5.8800306CurrentTrain: epoch  2, batch    25 | loss: 5.6323175CurrentTrain: epoch  2, batch    26 | loss: 5.6983461CurrentTrain: epoch  2, batch    27 | loss: 6.0546923CurrentTrain: epoch  2, batch    28 | loss: 4.8972812CurrentTrain: epoch  2, batch    29 | loss: 5.9092054CurrentTrain: epoch  2, batch    30 | loss: 5.5169172CurrentTrain: epoch  2, batch    31 | loss: 5.6554718CurrentTrain: epoch  2, batch    32 | loss: 5.7843008CurrentTrain: epoch  2, batch    33 | loss: 5.8972001CurrentTrain: epoch  2, batch    34 | loss: 5.4559469CurrentTrain: epoch  2, batch    35 | loss: 5.1755171CurrentTrain: epoch  2, batch    36 | loss: 6.3438158CurrentTrain: epoch  2, batch    37 | loss: 5.5228128CurrentTrain: epoch  2, batch    38 | loss: 5.9130793CurrentTrain: epoch  2, batch    39 | loss: 5.6892157CurrentTrain: epoch  2, batch    40 | loss: 5.8517613CurrentTrain: epoch  2, batch    41 | loss: 5.4226789CurrentTrain: epoch  2, batch    42 | loss: 6.0207844CurrentTrain: epoch  2, batch    43 | loss: 5.4998798CurrentTrain: epoch  2, batch    44 | loss: 5.9154634CurrentTrain: epoch  2, batch    45 | loss: 5.8236847CurrentTrain: epoch  2, batch    46 | loss: 6.3937573CurrentTrain: epoch  2, batch    47 | loss: 5.4377441CurrentTrain: epoch  2, batch    48 | loss: 5.5710521CurrentTrain: epoch  2, batch    49 | loss: 5.0997224CurrentTrain: epoch  2, batch    50 | loss: 4.9223881CurrentTrain: epoch  2, batch    51 | loss: 5.5958815CurrentTrain: epoch  2, batch    52 | loss: 5.6638579CurrentTrain: epoch  2, batch    53 | loss: 4.9979072CurrentTrain: epoch  2, batch    54 | loss: 5.4874077CurrentTrain: epoch  2, batch    55 | loss: 5.4113307CurrentTrain: epoch  2, batch    56 | loss: 5.9852219CurrentTrain: epoch  2, batch    57 | loss: 5.4816155CurrentTrain: epoch  2, batch    58 | loss: 5.9823399CurrentTrain: epoch  2, batch    59 | loss: 5.4865499CurrentTrain: epoch  2, batch    60 | loss: 5.7388287CurrentTrain: epoch  2, batch    61 | loss: 4.9262161CurrentTrain: epoch  2, batch    62 | loss: 5.0331478CurrentTrain: epoch  3, batch     0 | loss: 6.3029137CurrentTrain: epoch  3, batch     1 | loss: 5.7216387CurrentTrain: epoch  3, batch     2 | loss: 5.0312481CurrentTrain: epoch  3, batch     3 | loss: 4.8524761CurrentTrain: epoch  3, batch     4 | loss: 5.5363026CurrentTrain: epoch  3, batch     5 | loss: 5.1026130CurrentTrain: epoch  3, batch     6 | loss: 5.1273217CurrentTrain: epoch  3, batch     7 | loss: 5.1390800CurrentTrain: epoch  3, batch     8 | loss: 4.9871058CurrentTrain: epoch  3, batch     9 | loss: 5.0237207CurrentTrain: epoch  3, batch    10 | loss: 5.0749788CurrentTrain: epoch  3, batch    11 | loss: 5.3320212CurrentTrain: epoch  3, batch    12 | loss: 5.1734877CurrentTrain: epoch  3, batch    13 | loss: 4.8679562CurrentTrain: epoch  3, batch    14 | loss: 5.0801640CurrentTrain: epoch  3, batch    15 | loss: 5.5650396CurrentTrain: epoch  3, batch    16 | loss: 5.3742208CurrentTrain: epoch  3, batch    17 | loss: 5.6085558CurrentTrain: epoch  3, batch    18 | loss: 5.5561571CurrentTrain: epoch  3, batch    19 | loss: 6.1174564CurrentTrain: epoch  3, batch    20 | loss: 5.2580628CurrentTrain: epoch  3, batch    21 | loss: 5.1812811CurrentTrain: epoch  3, batch    22 | loss: 5.1566429CurrentTrain: epoch  3, batch    23 | loss: 4.9755640CurrentTrain: epoch  3, batch    24 | loss: 5.3557119CurrentTrain: epoch  3, batch    25 | loss: 5.0741925CurrentTrain: epoch  3, batch    26 | loss: 5.0615997CurrentTrain: epoch  3, batch    27 | loss: 5.1230707CurrentTrain: epoch  3, batch    28 | loss: 5.0109453CurrentTrain: epoch  3, batch    29 | loss: 4.8408637CurrentTrain: epoch  3, batch    30 | loss: 4.8909459CurrentTrain: epoch  3, batch    31 | loss: 4.8581867CurrentTrain: epoch  3, batch    32 | loss: 5.0653358CurrentTrain: epoch  3, batch    33 | loss: 4.8782420CurrentTrain: epoch  3, batch    34 | loss: 5.1278591CurrentTrain: epoch  3, batch    35 | loss: 4.9610806CurrentTrain: epoch  3, batch    36 | loss: 5.0037055CurrentTrain: epoch  3, batch    37 | loss: 5.4498491CurrentTrain: epoch  3, batch    38 | loss: 4.8497562CurrentTrain: epoch  3, batch    39 | loss: 5.3052073CurrentTrain: epoch  3, batch    40 | loss: 5.2236443CurrentTrain: epoch  3, batch    41 | loss: 5.2712879CurrentTrain: epoch  3, batch    42 | loss: 4.8263354CurrentTrain: epoch  3, batch    43 | loss: 5.1508207CurrentTrain: epoch  3, batch    44 | loss: 4.7250090CurrentTrain: epoch  3, batch    45 | loss: 4.8379955CurrentTrain: epoch  3, batch    46 | loss: 4.6312809CurrentTrain: epoch  3, batch    47 | loss: 4.7047396CurrentTrain: epoch  3, batch    48 | loss: 4.7200289CurrentTrain: epoch  3, batch    49 | loss: 5.0705032CurrentTrain: epoch  3, batch    50 | loss: 4.9980288CurrentTrain: epoch  3, batch    51 | loss: 4.5091114CurrentTrain: epoch  3, batch    52 | loss: 5.0454359CurrentTrain: epoch  3, batch    53 | loss: 5.0011635CurrentTrain: epoch  3, batch    54 | loss: 4.7571702CurrentTrain: epoch  3, batch    55 | loss: 4.8129802CurrentTrain: epoch  3, batch    56 | loss: 4.6012855CurrentTrain: epoch  3, batch    57 | loss: 4.9763546CurrentTrain: epoch  3, batch    58 | loss: 4.9819260CurrentTrain: epoch  3, batch    59 | loss: 4.5328007CurrentTrain: epoch  3, batch    60 | loss: 4.9037757CurrentTrain: epoch  3, batch    61 | loss: 4.4719691CurrentTrain: epoch  3, batch    62 | loss: 4.4731388CurrentTrain: epoch  4, batch     0 | loss: 4.6557159CurrentTrain: epoch  4, batch     1 | loss: 4.5732646CurrentTrain: epoch  4, batch     2 | loss: 4.5830159CurrentTrain: epoch  4, batch     3 | loss: 5.2998857CurrentTrain: epoch  4, batch     4 | loss: 4.8397975CurrentTrain: epoch  4, batch     5 | loss: 4.3901906CurrentTrain: epoch  4, batch     6 | loss: 4.5829554CurrentTrain: epoch  4, batch     7 | loss: 4.5000620CurrentTrain: epoch  4, batch     8 | loss: 4.7744551CurrentTrain: epoch  4, batch     9 | loss: 4.6752682CurrentTrain: epoch  4, batch    10 | loss: 4.6147223CurrentTrain: epoch  4, batch    11 | loss: 4.5848150CurrentTrain: epoch  4, batch    12 | loss: 4.6820555CurrentTrain: epoch  4, batch    13 | loss: 4.4903717CurrentTrain: epoch  4, batch    14 | loss: 4.6498337CurrentTrain: epoch  4, batch    15 | loss: 4.6735868CurrentTrain: epoch  4, batch    16 | loss: 5.0803051CurrentTrain: epoch  4, batch    17 | loss: 4.6287327CurrentTrain: epoch  4, batch    18 | loss: 5.5040083CurrentTrain: epoch  4, batch    19 | loss: 5.0829663CurrentTrain: epoch  4, batch    20 | loss: 4.7849865CurrentTrain: epoch  4, batch    21 | loss: 4.7332354CurrentTrain: epoch  4, batch    22 | loss: 4.6845446CurrentTrain: epoch  4, batch    23 | loss: 5.1012058CurrentTrain: epoch  4, batch    24 | loss: 4.5878587CurrentTrain: epoch  4, batch    25 | loss: 4.6816211CurrentTrain: epoch  4, batch    26 | loss: 4.8145752CurrentTrain: epoch  4, batch    27 | loss: 4.5949354CurrentTrain: epoch  4, batch    28 | loss: 4.4955673CurrentTrain: epoch  4, batch    29 | loss: 4.5668268CurrentTrain: epoch  4, batch    30 | loss: 4.4923868CurrentTrain: epoch  4, batch    31 | loss: 4.6007223CurrentTrain: epoch  4, batch    32 | loss: 4.4224377CurrentTrain: epoch  4, batch    33 | loss: 4.5180526CurrentTrain: epoch  4, batch    34 | loss: 4.7888808CurrentTrain: epoch  4, batch    35 | loss: 4.7077165CurrentTrain: epoch  4, batch    36 | loss: 4.4855580CurrentTrain: epoch  4, batch    37 | loss: 4.5417657CurrentTrain: epoch  4, batch    38 | loss: 4.7252569CurrentTrain: epoch  4, batch    39 | loss: 4.5979476CurrentTrain: epoch  4, batch    40 | loss: 5.1830559CurrentTrain: epoch  4, batch    41 | loss: 4.3749223CurrentTrain: epoch  4, batch    42 | loss: 4.5182323CurrentTrain: epoch  4, batch    43 | loss: 4.5562153CurrentTrain: epoch  4, batch    44 | loss: 4.3744497CurrentTrain: epoch  4, batch    45 | loss: 4.9379973CurrentTrain: epoch  4, batch    46 | loss: 4.4636011CurrentTrain: epoch  4, batch    47 | loss: 4.5049038CurrentTrain: epoch  4, batch    48 | loss: 4.7212591CurrentTrain: epoch  4, batch    49 | loss: 4.5256844CurrentTrain: epoch  4, batch    50 | loss: 4.3687172CurrentTrain: epoch  4, batch    51 | loss: 4.8821826CurrentTrain: epoch  4, batch    52 | loss: 4.5441489CurrentTrain: epoch  4, batch    53 | loss: 4.7711105CurrentTrain: epoch  4, batch    54 | loss: 4.4265985CurrentTrain: epoch  4, batch    55 | loss: 4.5676975CurrentTrain: epoch  4, batch    56 | loss: 4.4339113CurrentTrain: epoch  4, batch    57 | loss: 4.3820200CurrentTrain: epoch  4, batch    58 | loss: 4.4971147CurrentTrain: epoch  4, batch    59 | loss: 4.5237112CurrentTrain: epoch  4, batch    60 | loss: 4.3650055CurrentTrain: epoch  4, batch    61 | loss: 4.7597218CurrentTrain: epoch  4, batch    62 | loss: 6.1370821CurrentTrain: epoch  5, batch     0 | loss: 4.6162891CurrentTrain: epoch  5, batch     1 | loss: 4.4431658CurrentTrain: epoch  5, batch     2 | loss: 4.4501176CurrentTrain: epoch  5, batch     3 | loss: 4.4677620CurrentTrain: epoch  5, batch     4 | loss: 4.8876553CurrentTrain: epoch  5, batch     5 | loss: 4.8273168CurrentTrain: epoch  5, batch     6 | loss: 4.4210038CurrentTrain: epoch  5, batch     7 | loss: 4.3456850CurrentTrain: epoch  5, batch     8 | loss: 4.2868052CurrentTrain: epoch  5, batch     9 | loss: 4.4469519CurrentTrain: epoch  5, batch    10 | loss: 4.3676329CurrentTrain: epoch  5, batch    11 | loss: 4.5220051CurrentTrain: epoch  5, batch    12 | loss: 4.4897709CurrentTrain: epoch  5, batch    13 | loss: 4.3677130CurrentTrain: epoch  5, batch    14 | loss: 4.7754126CurrentTrain: epoch  5, batch    15 | loss: 4.4896665CurrentTrain: epoch  5, batch    16 | loss: 4.5273061CurrentTrain: epoch  5, batch    17 | loss: 4.3024125CurrentTrain: epoch  5, batch    18 | loss: 4.3640485CurrentTrain: epoch  5, batch    19 | loss: 4.5582809CurrentTrain: epoch  5, batch    20 | loss: 4.6912231CurrentTrain: epoch  5, batch    21 | loss: 4.1878343CurrentTrain: epoch  5, batch    22 | loss: 5.0525026CurrentTrain: epoch  5, batch    23 | loss: 4.2951250CurrentTrain: epoch  5, batch    24 | loss: 4.2718611CurrentTrain: epoch  5, batch    25 | loss: 4.4471188CurrentTrain: epoch  5, batch    26 | loss: 4.2767057CurrentTrain: epoch  5, batch    27 | loss: 4.4349551CurrentTrain: epoch  5, batch    28 | loss: 4.3625484CurrentTrain: epoch  5, batch    29 | loss: 4.5278549CurrentTrain: epoch  5, batch    30 | loss: 4.5796638CurrentTrain: epoch  5, batch    31 | loss: 4.9794617CurrentTrain: epoch  5, batch    32 | loss: 4.3698750CurrentTrain: epoch  5, batch    33 | loss: 4.3616138CurrentTrain: epoch  5, batch    34 | loss: 4.2850790CurrentTrain: epoch  5, batch    35 | loss: 4.3145657CurrentTrain: epoch  5, batch    36 | loss: 4.2724962CurrentTrain: epoch  5, batch    37 | loss: 4.3011656CurrentTrain: epoch  5, batch    38 | loss: 4.2503157CurrentTrain: epoch  5, batch    39 | loss: 4.4485044CurrentTrain: epoch  5, batch    40 | loss: 4.3382912CurrentTrain: epoch  5, batch    41 | loss: 4.2659211CurrentTrain: epoch  5, batch    42 | loss: 4.5288086CurrentTrain: epoch  5, batch    43 | loss: 4.2693639CurrentTrain: epoch  5, batch    44 | loss: 4.2490177CurrentTrain: epoch  5, batch    45 | loss: 4.2710314CurrentTrain: epoch  5, batch    46 | loss: 4.2728200CurrentTrain: epoch  5, batch    47 | loss: 4.6554918CurrentTrain: epoch  5, batch    48 | loss: 4.4824314CurrentTrain: epoch  5, batch    49 | loss: 4.3666315CurrentTrain: epoch  5, batch    50 | loss: 4.3702631CurrentTrain: epoch  5, batch    51 | loss: 4.3142304CurrentTrain: epoch  5, batch    52 | loss: 4.3498263CurrentTrain: epoch  5, batch    53 | loss: 4.1862283CurrentTrain: epoch  5, batch    54 | loss: 4.3526607CurrentTrain: epoch  5, batch    55 | loss: 4.3916883CurrentTrain: epoch  5, batch    56 | loss: 4.1678643CurrentTrain: epoch  5, batch    57 | loss: 4.2739911CurrentTrain: epoch  5, batch    58 | loss: 4.4282579CurrentTrain: epoch  5, batch    59 | loss: 4.2351685CurrentTrain: epoch  5, batch    60 | loss: 4.2072854CurrentTrain: epoch  5, batch    61 | loss: 4.3151298CurrentTrain: epoch  5, batch    62 | loss: 4.1833243CurrentTrain: epoch  6, batch     0 | loss: 4.1997261CurrentTrain: epoch  6, batch     1 | loss: 4.5483751CurrentTrain: epoch  6, batch     2 | loss: 4.3387227CurrentTrain: epoch  6, batch     3 | loss: 4.2568321CurrentTrain: epoch  6, batch     4 | loss: 4.1629438CurrentTrain: epoch  6, batch     5 | loss: 4.2245402CurrentTrain: epoch  6, batch     6 | loss: 4.2529659CurrentTrain: epoch  6, batch     7 | loss: 4.2715220CurrentTrain: epoch  6, batch     8 | loss: 4.2247143CurrentTrain: epoch  6, batch     9 | loss: 4.3152542CurrentTrain: epoch  6, batch    10 | loss: 4.2049809CurrentTrain: epoch  6, batch    11 | loss: 4.2584529CurrentTrain: epoch  6, batch    12 | loss: 4.3013659CurrentTrain: epoch  6, batch    13 | loss: 4.2746205CurrentTrain: epoch  6, batch    14 | loss: 4.2286959CurrentTrain: epoch  6, batch    15 | loss: 4.2050200CurrentTrain: epoch  6, batch    16 | loss: 4.2240801CurrentTrain: epoch  6, batch    17 | loss: 4.2684155CurrentTrain: epoch  6, batch    18 | loss: 4.2682362CurrentTrain: epoch  6, batch    19 | loss: 4.3323679CurrentTrain: epoch  6, batch    20 | loss: 4.3209991CurrentTrain: epoch  6, batch    21 | loss: 4.3182249CurrentTrain: epoch  6, batch    22 | loss: 4.2125921CurrentTrain: epoch  6, batch    23 | loss: 4.1995311CurrentTrain: epoch  6, batch    24 | loss: 4.3042164CurrentTrain: epoch  6, batch    25 | loss: 4.1721926CurrentTrain: epoch  6, batch    26 | loss: 4.2230606CurrentTrain: epoch  6, batch    27 | loss: 4.0922346CurrentTrain: epoch  6, batch    28 | loss: 4.2518711CurrentTrain: epoch  6, batch    29 | loss: 4.2361364CurrentTrain: epoch  6, batch    30 | loss: 4.2389393CurrentTrain: epoch  6, batch    31 | loss: 4.1700511CurrentTrain: epoch  6, batch    32 | loss: 4.2079763CurrentTrain: epoch  6, batch    33 | loss: 4.1918688CurrentTrain: epoch  6, batch    34 | loss: 4.2089133CurrentTrain: epoch  6, batch    35 | loss: 4.1478100CurrentTrain: epoch  6, batch    36 | loss: 4.2722807CurrentTrain: epoch  6, batch    37 | loss: 4.4093790CurrentTrain: epoch  6, batch    38 | loss: 4.2784891CurrentTrain: epoch  6, batch    39 | loss: 4.1558704CurrentTrain: epoch  6, batch    40 | loss: 4.1625533CurrentTrain: epoch  6, batch    41 | loss: 4.1899495CurrentTrain: epoch  6, batch    42 | loss: 4.3058548CurrentTrain: epoch  6, batch    43 | loss: 4.2684093CurrentTrain: epoch  6, batch    44 | loss: 4.6100454CurrentTrain: epoch  6, batch    45 | loss: 4.1536198CurrentTrain: epoch  6, batch    46 | loss: 4.2276030CurrentTrain: epoch  6, batch    47 | loss: 4.1755733CurrentTrain: epoch  6, batch    48 | loss: 4.2767191CurrentTrain: epoch  6, batch    49 | loss: 4.0517817CurrentTrain: epoch  6, batch    50 | loss: 4.2241964CurrentTrain: epoch  6, batch    51 | loss: 4.1424618CurrentTrain: epoch  6, batch    52 | loss: 4.2819443CurrentTrain: epoch  6, batch    53 | loss: 4.1762838CurrentTrain: epoch  6, batch    54 | loss: 4.2179060CurrentTrain: epoch  6, batch    55 | loss: 4.3106499CurrentTrain: epoch  6, batch    56 | loss: 4.1704974CurrentTrain: epoch  6, batch    57 | loss: 4.1281013CurrentTrain: epoch  6, batch    58 | loss: 4.1252289CurrentTrain: epoch  6, batch    59 | loss: 4.1823907CurrentTrain: epoch  6, batch    60 | loss: 4.1213398CurrentTrain: epoch  6, batch    61 | loss: 4.1490664CurrentTrain: epoch  6, batch    62 | loss: 4.0871601CurrentTrain: epoch  7, batch     0 | loss: 4.1010799CurrentTrain: epoch  7, batch     1 | loss: 4.1361580CurrentTrain: epoch  7, batch     2 | loss: 4.1655388CurrentTrain: epoch  7, batch     3 | loss: 4.1649618CurrentTrain: epoch  7, batch     4 | loss: 4.1123571CurrentTrain: epoch  7, batch     5 | loss: 4.1500578CurrentTrain: epoch  7, batch     6 | loss: 4.1575961CurrentTrain: epoch  7, batch     7 | loss: 4.1696606CurrentTrain: epoch  7, batch     8 | loss: 4.1688795CurrentTrain: epoch  7, batch     9 | loss: 4.1771545CurrentTrain: epoch  7, batch    10 | loss: 4.3963723CurrentTrain: epoch  7, batch    11 | loss: 4.1789608CurrentTrain: epoch  7, batch    12 | loss: 4.2169571CurrentTrain: epoch  7, batch    13 | loss: 4.1523190CurrentTrain: epoch  7, batch    14 | loss: 4.1040277CurrentTrain: epoch  7, batch    15 | loss: 4.1130247CurrentTrain: epoch  7, batch    16 | loss: 4.1500554CurrentTrain: epoch  7, batch    17 | loss: 4.1159039CurrentTrain: epoch  7, batch    18 | loss: 4.0675788CurrentTrain: epoch  7, batch    19 | loss: 4.1458225CurrentTrain: epoch  7, batch    20 | loss: 4.1239519CurrentTrain: epoch  7, batch    21 | loss: 4.1087470CurrentTrain: epoch  7, batch    22 | loss: 4.1581497CurrentTrain: epoch  7, batch    23 | loss: 4.1472502CurrentTrain: epoch  7, batch    24 | loss: 4.1958704CurrentTrain: epoch  7, batch    25 | loss: 4.1474137CurrentTrain: epoch  7, batch    26 | loss: 4.1595812CurrentTrain: epoch  7, batch    27 | loss: 4.1296039CurrentTrain: epoch  7, batch    28 | loss: 4.1229744CurrentTrain: epoch  7, batch    29 | loss: 4.1445699CurrentTrain: epoch  7, batch    30 | loss: 4.1619930CurrentTrain: epoch  7, batch    31 | loss: 4.1267996CurrentTrain: epoch  7, batch    32 | loss: 4.1388278CurrentTrain: epoch  7, batch    33 | loss: 4.1448669CurrentTrain: epoch  7, batch    34 | loss: 4.0878649CurrentTrain: epoch  7, batch    35 | loss: 4.0868707CurrentTrain: epoch  7, batch    36 | loss: 4.1011586CurrentTrain: epoch  7, batch    37 | loss: 4.1261625CurrentTrain: epoch  7, batch    38 | loss: 4.1025777CurrentTrain: epoch  7, batch    39 | loss: 4.2630191CurrentTrain: epoch  7, batch    40 | loss: 4.1435232CurrentTrain: epoch  7, batch    41 | loss: 4.2468104CurrentTrain: epoch  7, batch    42 | loss: 4.1045246CurrentTrain: epoch  7, batch    43 | loss: 4.0600600CurrentTrain: epoch  7, batch    44 | loss: 4.0662184CurrentTrain: epoch  7, batch    45 | loss: 4.1640120CurrentTrain: epoch  7, batch    46 | loss: 4.0871325CurrentTrain: epoch  7, batch    47 | loss: 4.1231055CurrentTrain: epoch  7, batch    48 | loss: 4.1133871CurrentTrain: epoch  7, batch    49 | loss: 4.0700130CurrentTrain: epoch  7, batch    50 | loss: 4.0802460CurrentTrain: epoch  7, batch    51 | loss: 4.0527763CurrentTrain: epoch  7, batch    52 | loss: 4.0881052CurrentTrain: epoch  7, batch    53 | loss: 4.0739756CurrentTrain: epoch  7, batch    54 | loss: 4.1297054CurrentTrain: epoch  7, batch    55 | loss: 4.1258292CurrentTrain: epoch  7, batch    56 | loss: 4.0912414CurrentTrain: epoch  7, batch    57 | loss: 4.0931654CurrentTrain: epoch  7, batch    58 | loss: 4.0938740CurrentTrain: epoch  7, batch    59 | loss: 4.1178732CurrentTrain: epoch  7, batch    60 | loss: 3.9950352CurrentTrain: epoch  7, batch    61 | loss: 4.1191854CurrentTrain: epoch  7, batch    62 | loss: 4.0728960CurrentTrain: epoch  8, batch     0 | loss: 4.0347185CurrentTrain: epoch  8, batch     1 | loss: 4.1392326CurrentTrain: epoch  8, batch     2 | loss: 4.0185556CurrentTrain: epoch  8, batch     3 | loss: 4.1204948CurrentTrain: epoch  8, batch     4 | loss: 4.0855937CurrentTrain: epoch  8, batch     5 | loss: 4.0570641CurrentTrain: epoch  8, batch     6 | loss: 4.0716877CurrentTrain: epoch  8, batch     7 | loss: 4.0814586CurrentTrain: epoch  8, batch     8 | loss: 4.0935001CurrentTrain: epoch  8, batch     9 | loss: 4.0804472CurrentTrain: epoch  8, batch    10 | loss: 4.1365490CurrentTrain: epoch  8, batch    11 | loss: 4.0994873CurrentTrain: epoch  8, batch    12 | loss: 4.1165018CurrentTrain: epoch  8, batch    13 | loss: 4.1160135CurrentTrain: epoch  8, batch    14 | loss: 4.0616169CurrentTrain: epoch  8, batch    15 | loss: 4.0917645CurrentTrain: epoch  8, batch    16 | loss: 4.0564280CurrentTrain: epoch  8, batch    17 | loss: 4.1034164CurrentTrain: epoch  8, batch    18 | loss: 4.0925159CurrentTrain: epoch  8, batch    19 | loss: 4.0804758CurrentTrain: epoch  8, batch    20 | loss: 3.9945593CurrentTrain: epoch  8, batch    21 | loss: 4.0735455CurrentTrain: epoch  8, batch    22 | loss: 4.1108756CurrentTrain: epoch  8, batch    23 | loss: 4.0840435CurrentTrain: epoch  8, batch    24 | loss: 4.0545988CurrentTrain: epoch  8, batch    25 | loss: 4.0614214CurrentTrain: epoch  8, batch    26 | loss: 4.0657911CurrentTrain: epoch  8, batch    27 | loss: 4.0842695CurrentTrain: epoch  8, batch    28 | loss: 4.0179901CurrentTrain: epoch  8, batch    29 | loss: 4.1218529CurrentTrain: epoch  8, batch    30 | loss: 4.0218077CurrentTrain: epoch  8, batch    31 | loss: 4.1185269CurrentTrain: epoch  8, batch    32 | loss: 4.0568333CurrentTrain: epoch  8, batch    33 | loss: 4.0477395CurrentTrain: epoch  8, batch    34 | loss: 4.0923657CurrentTrain: epoch  8, batch    35 | loss: 4.1413498CurrentTrain: epoch  8, batch    36 | loss: 4.1129713CurrentTrain: epoch  8, batch    37 | loss: 4.0107069CurrentTrain: epoch  8, batch    38 | loss: 4.0190964CurrentTrain: epoch  8, batch    39 | loss: 4.1144495CurrentTrain: epoch  8, batch    40 | loss: 4.1125631CurrentTrain: epoch  8, batch    41 | loss: 4.0480323CurrentTrain: epoch  8, batch    42 | loss: 4.1411824CurrentTrain: epoch  8, batch    43 | loss: 4.0858355CurrentTrain: epoch  8, batch    44 | loss: 4.0816565CurrentTrain: epoch  8, batch    45 | loss: 4.0515556CurrentTrain: epoch  8, batch    46 | loss: 4.0877295CurrentTrain: epoch  8, batch    47 | loss: 4.0704093CurrentTrain: epoch  8, batch    48 | loss: 4.0653114CurrentTrain: epoch  8, batch    49 | loss: 4.0346823CurrentTrain: epoch  8, batch    50 | loss: 4.0570421CurrentTrain: epoch  8, batch    51 | loss: 4.0269356CurrentTrain: epoch  8, batch    52 | loss: 4.1165462CurrentTrain: epoch  8, batch    53 | loss: 4.1023836CurrentTrain: epoch  8, batch    54 | loss: 4.0718656CurrentTrain: epoch  8, batch    55 | loss: 4.1047935CurrentTrain: epoch  8, batch    56 | loss: 4.0182238CurrentTrain: epoch  8, batch    57 | loss: 4.0639119CurrentTrain: epoch  8, batch    58 | loss: 4.0357046CurrentTrain: epoch  8, batch    59 | loss: 4.0684872CurrentTrain: epoch  8, batch    60 | loss: 4.0099335CurrentTrain: epoch  8, batch    61 | loss: 4.0572243CurrentTrain: epoch  8, batch    62 | loss: 4.0461006CurrentTrain: epoch  9, batch     0 | loss: 4.0029459CurrentTrain: epoch  9, batch     1 | loss: 4.0228820CurrentTrain: epoch  9, batch     2 | loss: 4.0711746CurrentTrain: epoch  9, batch     3 | loss: 4.1111612CurrentTrain: epoch  9, batch     4 | loss: 4.0515847CurrentTrain: epoch  9, batch     5 | loss: 4.0397844CurrentTrain: epoch  9, batch     6 | loss: 4.0330095CurrentTrain: epoch  9, batch     7 | loss: 4.0333700CurrentTrain: epoch  9, batch     8 | loss: 4.0721488CurrentTrain: epoch  9, batch     9 | loss: 4.0279179CurrentTrain: epoch  9, batch    10 | loss: 4.0350451CurrentTrain: epoch  9, batch    11 | loss: 4.0466766CurrentTrain: epoch  9, batch    12 | loss: 4.0539227CurrentTrain: epoch  9, batch    13 | loss: 4.0531425CurrentTrain: epoch  9, batch    14 | loss: 4.0215445CurrentTrain: epoch  9, batch    15 | loss: 4.0484800CurrentTrain: epoch  9, batch    16 | loss: 4.0989938CurrentTrain: epoch  9, batch    17 | loss: 4.0514755CurrentTrain: epoch  9, batch    18 | loss: 4.0472946CurrentTrain: epoch  9, batch    19 | loss: 3.9942024CurrentTrain: epoch  9, batch    20 | loss: 4.0391331CurrentTrain: epoch  9, batch    21 | loss: 4.0472417CurrentTrain: epoch  9, batch    22 | loss: 4.0140572CurrentTrain: epoch  9, batch    23 | loss: 4.0429153CurrentTrain: epoch  9, batch    24 | loss: 4.0817146CurrentTrain: epoch  9, batch    25 | loss: 4.0624733CurrentTrain: epoch  9, batch    26 | loss: 4.0351276CurrentTrain: epoch  9, batch    27 | loss: 4.0890388CurrentTrain: epoch  9, batch    28 | loss: 4.0098138CurrentTrain: epoch  9, batch    29 | loss: 4.0356112CurrentTrain: epoch  9, batch    30 | loss: 4.0307255CurrentTrain: epoch  9, batch    31 | loss: 3.9856801CurrentTrain: epoch  9, batch    32 | loss: 4.0960884CurrentTrain: epoch  9, batch    33 | loss: 4.0704517CurrentTrain: epoch  9, batch    34 | loss: 4.0412312CurrentTrain: epoch  9, batch    35 | loss: 4.0162067CurrentTrain: epoch  9, batch    36 | loss: 4.0505533CurrentTrain: epoch  9, batch    37 | loss: 4.0411754CurrentTrain: epoch  9, batch    38 | loss: 4.0267277CurrentTrain: epoch  9, batch    39 | loss: 4.0357656CurrentTrain: epoch  9, batch    40 | loss: 4.0260754CurrentTrain: epoch  9, batch    41 | loss: 4.0607376CurrentTrain: epoch  9, batch    42 | loss: 4.0371218CurrentTrain: epoch  9, batch    43 | loss: 4.0122709CurrentTrain: epoch  9, batch    44 | loss: 4.0240102CurrentTrain: epoch  9, batch    45 | loss: 4.0936379CurrentTrain: epoch  9, batch    46 | loss: 4.0184832CurrentTrain: epoch  9, batch    47 | loss: 4.0845137CurrentTrain: epoch  9, batch    48 | loss: 4.0504594CurrentTrain: epoch  9, batch    49 | loss: 4.0533590CurrentTrain: epoch  9, batch    50 | loss: 4.0286517CurrentTrain: epoch  9, batch    51 | loss: 4.0625648CurrentTrain: epoch  9, batch    52 | loss: 4.0370636CurrentTrain: epoch  9, batch    53 | loss: 4.0891809CurrentTrain: epoch  9, batch    54 | loss: 4.0363798CurrentTrain: epoch  9, batch    55 | loss: 4.0694332CurrentTrain: epoch  9, batch    56 | loss: 4.0508866CurrentTrain: epoch  9, batch    57 | loss: 4.0384955CurrentTrain: epoch  9, batch    58 | loss: 4.0822439CurrentTrain: epoch  9, batch    59 | loss: 4.0134645CurrentTrain: epoch  9, batch    60 | loss: 4.0556998CurrentTrain: epoch  9, batch    61 | loss: 4.0424232CurrentTrain: epoch  9, batch    62 | loss: 4.0788822
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: while the milanese painting scene was dominated by neoclassic painters andrea appiani and luigi sabatelli , giovannni migliara stayed with the historical themes and medieval subtlety of romanticism .
Head Entity: andrea appiani
Tail Entity: neoclassic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: during the early 20th century, the surrealist movement gained prominence, with artists like salvador dalí and max ernst pushing the boundaries of imagination and reality.  
Head Entity: salvador dalí  
Tail Entity: surrealist  

Relation: movement  
Context: the beat generation was a literary movement that emerged in the 1950s, characterized by authors such as jack kerouac and allen ginsberg who challenged conventional norms.  
Head Entity: jack kerouac  
Tail Entity: beat generation  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: faldu was born on 1 august 1957 to a family of farmer in kalavad , jamnagar district , bombay state ( present - day gujarat ) .
Head Entity: kalavad
Tail Entity: jamnagar district
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: The city of Springfield is known for its beautiful parks and vibrant community, situated in the heart of Sangamon County, Illinois.  
Head Entity: Springfield  
Tail Entity: Sangamon County  

Relation: located in the administrative territorial entity  
Context: The historic town of Williamsburg is a popular tourist destination, located within the boundaries of James City County, Virginia.  
Head Entity: Williamsburg  
Tail Entity: James City County  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the nation as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: he is an ancestor of querl dox , brainiac 5 of the legion of super - heroes .
Head Entity: brainiac 5
Tail Entity: legion of super - heroes
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: She has been a prominent member of the local art collective for over a decade.  
Head Entity: She  
Tail Entity: local art collective  

Relation: member of  
Context: The band has released several albums and is known for its unique sound, with John being a key member of the group.  
Head Entity: John  
Tail Entity: the group  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: on july 2 , 1935 , the rip van winkle bridge over the hudson river between catskill and greenport was opened to traffic .
Head Entity: rip van winkle bridge
Tail Entity: hudson river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: The Golden Gate Bridge spans the Golden Gate Strait, connecting San Francisco to Marin County, and is a vital link for traffic crossing over the waterway.  
Head Entity: Golden Gate Bridge  
Tail Entity: Golden Gate Strait  

Relation: crosses  
Context: The Channel Tunnel, also known as the Chunnel, runs beneath the English Channel, providing a crucial transportation route between the UK and mainland Europe.  
Head Entity: Channel Tunnel  
Tail Entity: English Channel  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: the municipality contains the lower reaches of the maracanã river , which is joined by the caripi river before widening into maracanã bay and emptying into the atlantic ocean beside maiandeua island .
Head Entity: maracanã river
Tail Entity: caripi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The Amazon River, one of the largest rivers in the world, receives numerous tributaries, including the Madeira River, which significantly contributes to its flow.  
Head Entity: Amazon River  
Tail Entity: Madeira River  

Relation: tributary  
Context: The Mississippi River is fed by many smaller rivers, with the Missouri River being one of its major tributaries that enhances its water volume.  
Head Entity: Mississippi River  
Tail Entity: Missouri River  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: the result was widely interpreted as a personal rebuke to then chief minister shane stone .
Head Entity: shane stone
Tail Entity: chief minister
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: After serving as the mayor for over a decade, John Smith announced his retirement from the position.  
Head Entity: John Smith  
Tail Entity: mayor  

Relation: position held  
Context: The former president of the organization, Maria Lopez, was recognized for her contributions during her tenure.  
Head Entity: Maria Lopez  
Tail Entity: president
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: released by looking glass studios in 1998 and powered by their own in - house developed dark engine , " thief : the dark project " was considered by many to be a revolutionary game .
Head Entity: thief : the dark project
Tail Entity: looking glass studios
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: The popular game "The Witcher 3: Wild Hunt" was developed by CD Projekt Red, a Polish video game developer known for its rich storytelling and immersive gameplay.  
Head Entity: The Witcher 3: Wild Hunt  
Tail Entity: CD Projekt Red  

Relation: developer  
Context: "Minecraft," a sandbox video game that allows players to build and explore virtual worlds, was created by Markus Persson and later developed and published by Mojang Studios.  
Head Entity: Minecraft  
Tail Entity: Mojang Studios  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: neptune is the second studio album by the london - based band the duke spirit and the last with the original lead guitarist , dan higgins .
Head Entity: the duke spirit
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the tech startup was founded in silicon valley by a group of innovative engineers and entrepreneurs.  
Head Entity: the tech startup  
Tail Entity: silicon valley  

Relation: location of formation  
Context: the famous rock band was established in new york city, where they quickly gained a following.  
Head Entity: the famous rock band  
Tail Entity: new york city  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: during the same month , the band supported kings of leon 's tour of the united states , and coldplay 's tour of the united kingdom .
Head Entity: kings of leon
Tail Entity: united states
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous Italian dish, pizza, has gained popularity worldwide, but its roots can be traced back to Naples, Italy.  
Head Entity: pizza  
Tail Entity: Italy  

Relation: country of origin  
Context: The iconic brand, Rolex, is renowned for its luxury watches, which are crafted in Switzerland, known for its precision and quality in watchmaking.  
Head Entity: Rolex  
Tail Entity: Switzerland  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.79%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 94.08%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.60%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.84%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.79%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.68%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.61%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.96%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.40%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.61%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.67%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.98%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 96.08%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.16%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.33%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.28%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 96.35%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.43%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.45%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 96.27%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 96.23%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.30%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 95.91%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.94%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.91%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.97%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.24%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.79%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 94.08%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.60%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.84%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.79%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.68%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.61%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.96%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.40%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.61%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.67%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.98%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 96.08%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.16%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.33%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.28%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 96.35%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.43%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.45%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 96.27%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 96.23%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.30%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 95.91%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.94%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.91%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.97%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.24%   
cur_acc:  ['0.9524']
his_acc:  ['0.9524']
CurrentTrain: epoch  0, batch     0 | loss: 7.8943725CurrentTrain: epoch  0, batch     1 | loss: 6.9180565CurrentTrain: epoch  0, batch     2 | loss: 7.2132220CurrentTrain: epoch  0, batch     3 | loss: 4.6009197CurrentTrain: epoch  1, batch     0 | loss: 6.7501907CurrentTrain: epoch  1, batch     1 | loss: 6.6359982CurrentTrain: epoch  1, batch     2 | loss: 6.2727442CurrentTrain: epoch  1, batch     3 | loss: 5.4995012CurrentTrain: epoch  2, batch     0 | loss: 5.5703516CurrentTrain: epoch  2, batch     1 | loss: 5.3562317CurrentTrain: epoch  2, batch     2 | loss: 5.6872144CurrentTrain: epoch  2, batch     3 | loss: 5.4189954CurrentTrain: epoch  3, batch     0 | loss: 5.6874437CurrentTrain: epoch  3, batch     1 | loss: 4.8185987CurrentTrain: epoch  3, batch     2 | loss: 4.8261337CurrentTrain: epoch  3, batch     3 | loss: 2.8866830CurrentTrain: epoch  4, batch     0 | loss: 4.9987926CurrentTrain: epoch  4, batch     1 | loss: 4.9065204CurrentTrain: epoch  4, batch     2 | loss: 4.0699606CurrentTrain: epoch  4, batch     3 | loss: 3.2667127CurrentTrain: epoch  5, batch     0 | loss: 4.4402065CurrentTrain: epoch  5, batch     1 | loss: 4.5096202CurrentTrain: epoch  5, batch     2 | loss: 4.5770831CurrentTrain: epoch  5, batch     3 | loss: 3.4589410CurrentTrain: epoch  6, batch     0 | loss: 4.4036255CurrentTrain: epoch  6, batch     1 | loss: 4.7067065CurrentTrain: epoch  6, batch     2 | loss: 3.9709716CurrentTrain: epoch  6, batch     3 | loss: 2.0988100CurrentTrain: epoch  7, batch     0 | loss: 4.3812003CurrentTrain: epoch  7, batch     1 | loss: 3.3087802CurrentTrain: epoch  7, batch     2 | loss: 3.9763150CurrentTrain: epoch  7, batch     3 | loss: 6.1657257CurrentTrain: epoch  8, batch     0 | loss: 4.0517063CurrentTrain: epoch  8, batch     1 | loss: 3.6716805CurrentTrain: epoch  8, batch     2 | loss: 3.2682781CurrentTrain: epoch  8, batch     3 | loss: 4.4396667CurrentTrain: epoch  9, batch     0 | loss: 3.1506281CurrentTrain: epoch  9, batch     1 | loss: 4.0802774CurrentTrain: epoch  9, batch     2 | loss: 2.8969245CurrentTrain: epoch  9, batch     3 | loss: 6.0384769
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: bryson 's best known work is his 1985 book " evil angels " which chronicles the story of lindy chamberlain 's trial for murder , following the death of her baby daughter , azaria .
Head Entity: evil angels
Tail Entity: death
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: in her groundbreaking 2010 book "the immortal life of henrietta lacks," rebecca skloot explores the life of henrietta lacks and the impact of her cells on medical research.  
Head Entity: the immortal life of henrietta lacks  
Tail Entity: henrietta lacks  

Relation: main subject  
Context: "sapiens: a brief history of humankind" by yuval noah harari examines the history and evolution of our species from the emergence of Homo sapiens to the present day.  
Head Entity: sapiens: a brief history of humankind  
Tail Entity: Homo sapiens  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: he represented romania at euro 2000 , his first major tournament where they beat england 3–2 , and drew 1–1 against germany .
Head Entity: euro 2000
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: The United States women's national soccer team has consistently performed well in international competitions, winning the FIFA Women's World Cup multiple times, including in 2019 where they defeated the Netherlands 2-0.  
Head Entity: FIFA Women's World Cup  
Tail Entity: Netherlands  

Relation: participating team  
Context: In the 2021 Tokyo Olympics, the Australian basketball team showcased their skills, ultimately facing the United States in the gold medal match, where they put up a strong fight but lost 86-69.  
Head Entity: Tokyo Olympics  
Tail Entity: United States  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: at state level , the parliament of victoria consists of the legislative assembly ( the lower house ) and the legislative council ( the upper house ) .
Head Entity: parliament of victoria
Tail Entity: legislative council
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The human body is composed of various systems, including the circulatory system, which is essential for transporting blood throughout the body.  
Head Entity: human body  
Tail Entity: circulatory system  

Relation: has part  
Context: The solar system includes several celestial bodies, with the Earth being one of the planets that orbits the Sun.  
Head Entity: solar system  
Tail Entity: Earth  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: he was nominated for the academy award for best picture , along with steven spielberg , ian bryce , mark gordon for the film " saving private ryan " .
Head Entity: ian bryce
Tail Entity: academy award for best picture
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film "The Shape of Water" received multiple nominations, including a nod for best director, with Guillermo del Toro being nominated for the prestigious award.  
Head Entity: Guillermo del Toro  
Tail Entity: best director  

Relation: nominated for  
Context: In 2020, the popular series "Succession" was nominated for several Emmy Awards, with Jeremy Strong receiving a nomination for his outstanding performance.  
Head Entity: Jeremy Strong  
Tail Entity: Emmy Award  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: in 2007 , the county municipality created the transit authority called " skyss " that would market public transport while is would be operated by private companies based on public service obligation contracts .
Head Entity: skyss
Tail Entity: transit authority
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone known as the iPhone was first released by Apple Inc. in 2007, revolutionizing the mobile phone industry and setting new standards for design and functionality.  
Head Entity: iPhone  
Tail Entity: smartphone  

Relation: instance of  
Context: The Great Wall of China is a series of fortifications made of various materials, built to protect the Chinese states from invasions and raids, and is considered one of the most iconic structures in the world.  
Head Entity: Great Wall of China  
Tail Entity: fortification
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: abus valley ( ) is an ice - free valley southeast of turnstile ridge at the north end of britannia range .
Head Entity: turnstile ridge
Tail Entity: britannia range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the majestic peaks of the sierra nevada ( ) are known for their stunning beauty and are part of the larger cascade range .  
Head Entity: sierra nevada  
Tail Entity: cascade range  

Relation: mountain range  
Context: the famous rock formations of the appalachian mountains ( ) stretch across several states and are a significant part of the blue ridge range .  
Head Entity: appalachian mountains  
Tail Entity: blue ridge range  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: anders matthesen has also released several cds with his radio material , in addition to the animated movie " terkel in trouble " , based on one of these .
Head Entity: terkel in trouble
Tail Entity: anders matthesen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: the acclaimed film "inception" features a complex narrative that was intricately crafted by its talented screenwriter, who is known for his unique storytelling style.  
Head Entity: inception  
Tail Entity: christopher nolan  

Relation: screenwriter  
Context: the beloved animated feature "finding nemo" was brought to life through the creative vision of its screenwriter, who has a knack for creating heartwarming tales.  
Head Entity: finding nemo  
Tail Entity: andrew stanton  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the publisher found success in 1993 with tarō gomi 's " everyone poops " , a japanese language import that sold over one million copies and is now their best - known title .
Head Entity: everyone poops
Tail Entity: japanese language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The animated series "Avatar: The Last Airbender" features a rich narrative and is primarily presented in English, captivating audiences worldwide.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  

Relation: language of work or name  
Context: The famous novel "Cien años de soledad" by Gabriel García Márquez is celebrated for its magical realism and is originally written in Spanish, reflecting the culture of Latin America.  
Head Entity: Cien años de soledad  
Tail Entity: Spanish  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: the aircraft was a boeing 707 - 321c that first entered service with pan american world airways ( pan am ) in 1963 .
Head Entity: boeing 707 - 321c
Tail Entity: pan american world airways
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: the new electric bus fleet is managed by the city transportation authority, which ensures efficient public transit services.  
Head Entity: electric bus fleet  
Tail Entity: city transportation authority  

Relation: operator  
Context: the state-of-the-art research facility is operated by the national science foundation, focusing on advanced materials and nanotechnology.  
Head Entity: research facility  
Tail Entity: national science foundation  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: the most important lama of this series was the third changkya , rolpai dorje , who was preceptor to the qianlong emperor of china , and chief representative of tibetan buddhism at the qing court .
Head Entity: rolpai dorje
Tail Entity: tibetan buddhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the cathedral of notre-dame is a famous example of french gothic architecture and serves as the seat of the archbishop of paris, representing the roman catholic faith in the heart of the city.  
Head Entity: cathedral of notre-dame  
Tail Entity: roman catholic faith  

Relation: religion  
Context: the dalai lama is a prominent figure in the world, known for his teachings on compassion and non-violence, and is the spiritual leader of tibetan buddhism.  
Head Entity: dalai lama  
Tail Entity: tibetan buddhism  
Mixup data size:  198
MixupTrain:  epoch  0, batch     0 | loss: 6.2580787MixupTrain:  epoch  0, batch     1 | loss: 5.8056257MixupTrain:  epoch  0, batch     2 | loss: 5.6003579MixupTrain:  epoch  0, batch     3 | loss: 5.2630112MixupTrain:  epoch  0, batch     4 | loss: 5.4366362MixupTrain:  epoch  0, batch     5 | loss: 5.1920047MixupTrain:  epoch  0, batch     6 | loss: 5.2733691MixupTrain:  epoch  0, batch     7 | loss: 5.1229015MixupTrain:  epoch  0, batch     8 | loss: 4.6453106MixupTrain:  epoch  0, batch     9 | loss: 4.4321239MixupTrain:  epoch  0, batch    10 | loss: 4.4932234MixupTrain:  epoch  0, batch    11 | loss: 4.2669367MixupTrain:  epoch  0, batch    12 | loss: 3.5212261
MemoryTrain:  epoch  0, batch     0 | loss: 3.3993139MemoryTrain:  epoch  0, batch     1 | loss: 3.9445992MemoryTrain:  epoch  0, batch     2 | loss: 3.6619666MemoryTrain:  epoch  0, batch     3 | loss: 3.3985701MemoryTrain:  epoch  1, batch     0 | loss: 2.8255992MemoryTrain:  epoch  1, batch     1 | loss: 3.7734363MemoryTrain:  epoch  1, batch     2 | loss: 2.9789674MemoryTrain:  epoch  1, batch     3 | loss: 2.6540380MemoryTrain:  epoch  2, batch     0 | loss: 3.0381489MemoryTrain:  epoch  2, batch     1 | loss: 2.3473988MemoryTrain:  epoch  2, batch     2 | loss: 2.2684216MemoryTrain:  epoch  2, batch     3 | loss: 1.7690759MemoryTrain:  epoch  3, batch     0 | loss: 1.5873721MemoryTrain:  epoch  3, batch     1 | loss: 2.3366518MemoryTrain:  epoch  3, batch     2 | loss: 2.9887633MemoryTrain:  epoch  3, batch     3 | loss: 1.8214490MemoryTrain:  epoch  4, batch     0 | loss: 2.0014477MemoryTrain:  epoch  4, batch     1 | loss: 1.7671752MemoryTrain:  epoch  4, batch     2 | loss: 1.9443890MemoryTrain:  epoch  4, batch     3 | loss: 2.3393078MemoryTrain:  epoch  5, batch     0 | loss: 1.7713031MemoryTrain:  epoch  5, batch     1 | loss: 1.6850344MemoryTrain:  epoch  5, batch     2 | loss: 1.9467080MemoryTrain:  epoch  5, batch     3 | loss: 1.8977464MemoryTrain:  epoch  6, batch     0 | loss: 1.6788459MemoryTrain:  epoch  6, batch     1 | loss: 2.3070953MemoryTrain:  epoch  6, batch     2 | loss: 1.2791569MemoryTrain:  epoch  6, batch     3 | loss: 1.6489990MemoryTrain:  epoch  7, batch     0 | loss: 1.6576387MemoryTrain:  epoch  7, batch     1 | loss: 1.7324885MemoryTrain:  epoch  7, batch     2 | loss: 1.6049333MemoryTrain:  epoch  7, batch     3 | loss: 1.6259153MemoryTrain:  epoch  8, batch     0 | loss: 1.6568402MemoryTrain:  epoch  8, batch     1 | loss: 1.7369046MemoryTrain:  epoch  8, batch     2 | loss: 1.4810709MemoryTrain:  epoch  8, batch     3 | loss: 1.5204146MemoryTrain:  epoch  9, batch     0 | loss: 1.5801390MemoryTrain:  epoch  9, batch     1 | loss: 1.6810963MemoryTrain:  epoch  9, batch     2 | loss: 1.3344977MemoryTrain:  epoch  9, batch     3 | loss: 1.5607541
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 65.00%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 64.58%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 73.61%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 75.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 77.84%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 77.40%   [EVAL] batch:   13 | acc: 6.25%,  total acc: 72.32%   [EVAL] batch:   14 | acc: 18.75%,  total acc: 68.75%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 65.62%   [EVAL] batch:   16 | acc: 6.25%,  total acc: 62.13%   [EVAL] batch:   17 | acc: 0.00%,  total acc: 58.68%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 57.24%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 59.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 61.01%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 64.13%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 66.75%   [EVAL] batch:   25 | acc: 62.50%,  total acc: 66.59%   [EVAL] batch:   26 | acc: 43.75%,  total acc: 65.74%   [EVAL] batch:   27 | acc: 50.00%,  total acc: 65.18%   [EVAL] batch:   28 | acc: 68.75%,  total acc: 65.30%   [EVAL] batch:   29 | acc: 37.50%,  total acc: 64.38%   [EVAL] batch:   30 | acc: 50.00%,  total acc: 63.91%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 64.65%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 65.53%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 66.36%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 67.14%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 67.88%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 70.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 71.80%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 72.47%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 73.11%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 73.72%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 74.17%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 74.73%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 75.27%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 75.39%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 75.77%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 76.12%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 76.23%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 76.44%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 76.65%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 77.50%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 77.68%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 77.85%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 77.91%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 78.18%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 78.28%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 78.02%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 77.48%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 86.36%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 87.95%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 88.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 89.34%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.13%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 90.31%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 90.48%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 91.03%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 91.15%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 91.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.35%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.96%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 92.24%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.74%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.18%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.39%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 93.58%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.91%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.06%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 94.05%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.20%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 94.04%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.18%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.31%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 94.29%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 94.28%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 94.27%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.39%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 94.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.36%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.35%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 94.34%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.44%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.20%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 94.30%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 93.86%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 93.45%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 93.25%   [EVAL] batch:   63 | acc: 62.50%,  total acc: 92.77%   [EVAL] batch:   64 | acc: 56.25%,  total acc: 92.21%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 92.14%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 91.51%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 91.18%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 90.85%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 90.98%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 91.02%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 90.89%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 91.01%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 91.13%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   75 | acc: 0.00%,  total acc: 90.05%   [EVAL] batch:   76 | acc: 12.50%,  total acc: 89.04%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 88.22%   [EVAL] batch:   78 | acc: 12.50%,  total acc: 87.26%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 86.17%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 85.19%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 84.98%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 85.17%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 85.34%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 85.44%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 85.61%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 85.70%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 85.72%   [EVAL] batch:   88 | acc: 43.75%,  total acc: 85.25%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 84.86%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 84.62%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 84.31%   [EVAL] batch:   92 | acc: 31.25%,  total acc: 83.74%   [EVAL] batch:   93 | acc: 68.75%,  total acc: 83.58%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 83.85%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 83.89%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 83.99%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 84.15%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 84.31%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 84.47%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 84.77%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 84.92%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 85.06%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 85.20%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 85.28%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 85.55%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 85.68%   [EVAL] batch:  110 | acc: 81.25%,  total acc: 85.64%   [EVAL] batch:  111 | acc: 87.50%,  total acc: 85.66%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 85.79%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 85.64%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 85.76%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 85.78%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 85.90%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 85.96%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 85.92%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 85.99%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 86.00%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 85.96%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 85.87%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 85.79%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 85.75%   
cur_acc:  ['0.9524', '0.7748']
his_acc:  ['0.9524', '0.8575']
CurrentTrain: epoch  0, batch     0 | loss: 5.9801693CurrentTrain: epoch  0, batch     1 | loss: 5.5001669CurrentTrain: epoch  0, batch     2 | loss: 6.1062393CurrentTrain: epoch  0, batch     3 | loss: 5.9405818CurrentTrain: epoch  1, batch     0 | loss: 5.0927620CurrentTrain: epoch  1, batch     1 | loss: 5.9122210CurrentTrain: epoch  1, batch     2 | loss: 4.8011494CurrentTrain: epoch  1, batch     3 | loss: 2.6619134CurrentTrain: epoch  2, batch     0 | loss: 4.1228685CurrentTrain: epoch  2, batch     1 | loss: 4.5016026CurrentTrain: epoch  2, batch     2 | loss: 4.6929774CurrentTrain: epoch  2, batch     3 | loss: 3.2621369CurrentTrain: epoch  3, batch     0 | loss: 4.1582646CurrentTrain: epoch  3, batch     1 | loss: 4.0803771CurrentTrain: epoch  3, batch     2 | loss: 4.2786036CurrentTrain: epoch  3, batch     3 | loss: 5.1381102CurrentTrain: epoch  4, batch     0 | loss: 3.6181748CurrentTrain: epoch  4, batch     1 | loss: 3.9869411CurrentTrain: epoch  4, batch     2 | loss: 3.9051547CurrentTrain: epoch  4, batch     3 | loss: 3.6567669CurrentTrain: epoch  5, batch     0 | loss: 3.8506205CurrentTrain: epoch  5, batch     1 | loss: 3.3474936CurrentTrain: epoch  5, batch     2 | loss: 3.1209717CurrentTrain: epoch  5, batch     3 | loss: 5.2710342CurrentTrain: epoch  6, batch     0 | loss: 3.7133069CurrentTrain: epoch  6, batch     1 | loss: 3.2634730CurrentTrain: epoch  6, batch     2 | loss: 3.5553818CurrentTrain: epoch  6, batch     3 | loss: 1.8377011CurrentTrain: epoch  7, batch     0 | loss: 3.4533610CurrentTrain: epoch  7, batch     1 | loss: 3.1663046CurrentTrain: epoch  7, batch     2 | loss: 3.6207428CurrentTrain: epoch  7, batch     3 | loss: 1.9261203CurrentTrain: epoch  8, batch     0 | loss: 2.8287039CurrentTrain: epoch  8, batch     1 | loss: 2.9275246CurrentTrain: epoch  8, batch     2 | loss: 3.6871765CurrentTrain: epoch  8, batch     3 | loss: 3.1141992CurrentTrain: epoch  9, batch     0 | loss: 3.1540723CurrentTrain: epoch  9, batch     1 | loss: 2.9093859CurrentTrain: epoch  9, batch     2 | loss: 3.2441607CurrentTrain: epoch  9, batch     3 | loss: 2.0848594
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: peugeot took a similar step in 2010 when replacing the 407 and long - running but unpopular 607 with a single model , the 508 .
Head Entity: 508
Tail Entity: 407
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: In the 2021 season, the team introduced the new model, the X5, which follows the successful launch of the X3 in 2019.  
Head Entity: X5  
Tail Entity: X3  

Relation: follows  
Context: The latest smartphone, the Galaxy Z, follows the previous model, the Galaxy S, which was released just a year earlier.  
Head Entity: Galaxy Z  
Tail Entity: Galaxy S  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: in 1992 , with grand ceremony , the orioles began their season in a brand new ballpark , oriole park at camden yards , and thus retiring memorial stadium in the major league baseball world .
Head Entity: memorial stadium
Tail Entity: baseball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: The 2020 Summer Olympics, originally scheduled to be held in Tokyo, were postponed due to the COVID-19 pandemic, but the athletes continued to train for their respective sports.  
Head Entity: athletes  
Tail Entity: Olympics  

Relation: sport  
Context: After years of dedication and hard work, she finally qualified for the national swimming championships, showcasing her talent in the sport.  
Head Entity: national swimming championships  
Tail Entity: swimming  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael, the youngest son, had always looked up to his father, robert, for guidance and support.  
Head Entity: michael  
Tail Entity: robert  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: since 2009 , culshaw has starred in the bbc one comedy sketch show " the impressions show " alongside debra stephenson .
Head Entity: the impressions show
Tail Entity: bbc one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: The popular series "Breaking Bad" first aired on AMC, captivating audiences with its intense storytelling and character development.  
Head Entity: Breaking Bad  
Tail Entity: AMC  

Relation: original network  
Context: "Friends" became a cultural phenomenon after it premiered on NBC, showcasing the lives of six friends living in New York City.  
Head Entity: Friends  
Tail Entity: NBC  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: the 1954 film , directed by kadri venkata reddy , " peddamanushulu " was honoured with the first president 's silver medal for best feature film in telugu .
Head Entity: peddamanushulu
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: the 1994 animated film "The Lion King," produced by Walt Disney, was originally created in English before being dubbed into several other languages.  
Head Entity: The Lion King  
Tail Entity: English  

Relation: original language of film or TV show  
Context: the critically acclaimed 2001 film "Amélie," directed by Jean-Pierre Jeunet, was filmed in French and has since gained a global audience.  
Head Entity: Amélie  
Tail Entity: French  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: the league champions for the fifth time in their history ( and the second season running ) were taunton town , but runners - up mangotsfield united took promotion to the southern league .
Head Entity: mangotsfield united
Tail Entity: southern league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: After an impressive season, the team secured a spot in the prestigious league, where they will compete against top clubs, including their rivals, the city wanderers.  
Head Entity: city wanderers  
Tail Entity: prestigious league  

Relation: league  
Context: The young striker made headlines when he signed with the national league team, hoping to showcase his talent and help them achieve promotion.  
Head Entity: national league team  
Tail Entity: promotion  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: other teammates included dominique leray and élisabeth riffiod , whose son boris diaw currently plays in the nba for the spurs ( 2014 ) .
Head Entity: boris diaw
Tail Entity: élisabeth riffiod
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: in a recent interview, actress jennifer garner spoke fondly of her children, including violet affleck, whom she describes as a wonderful daughter.  
Head Entity: violet affleck  
Tail Entity: jennifer garner  

Relation: mother  
Context: during the family reunion, it was heartwarming to see how much emily loved her mother, sarah, as they shared stories and laughter together.  
Head Entity: emily  
Tail Entity: sarah  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres. he has dedicated his life to music and is known for his mastery of the cello.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: a u.s. government - funded $ 36 million bridge over the panj river connects sher khan bandar in afghanistan with nizhniy pyanzh in tajikistan , which transport more than 150 trucks or 1,000 cars daily .
Head Entity: sher khan bandar
Tail Entity: afghanistan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the ancient city of petra, located in southern jordan, is renowned for its rock-cut architecture and water conduit system, attracting thousands of tourists each year.  
Head Entity: petra  
Tail Entity: jordan  

Relation: country  
Context: the great wall of china, a series of fortifications made of various materials, stretches across northern china and is a UNESCO World Heritage site, symbolizing the country's historical strength.  
Head Entity: great wall of china  
Tail Entity: china  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: klaus kinski , who would play dracula himself nine years later in " nosferatu the vampyre " , is also featured in the film as renfield .
Head Entity: nosferatu the vampyre
Tail Entity: renfield
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: in the animated series "avatar: the last airbender," zuko is a complex character who struggles with his identity and family legacy.  
Head Entity: avatar: the last airbender  
Tail Entity: zuko  

Relation: characters  
Context: the novel "pride and prejudice" features elizabeth bennet, a strong-willed protagonist who navigates societal expectations and personal relationships.  
Head Entity: pride and prejudice  
Tail Entity: elizabeth bennet  
Mixup data size:  258
MixupTrain:  epoch  0, batch     0 | loss: 3.0181472MixupTrain:  epoch  0, batch     1 | loss: 2.9902183MixupTrain:  epoch  0, batch     2 | loss: 2.7708224MixupTrain:  epoch  0, batch     3 | loss: 2.5254345MixupTrain:  epoch  0, batch     4 | loss: 3.0508418MixupTrain:  epoch  0, batch     5 | loss: 3.0066132MixupTrain:  epoch  0, batch     6 | loss: 3.1790224MixupTrain:  epoch  0, batch     7 | loss: 3.0534552MixupTrain:  epoch  0, batch     8 | loss: 2.9994587MixupTrain:  epoch  0, batch     9 | loss: 2.7257275MixupTrain:  epoch  0, batch    10 | loss: 3.0866313MixupTrain:  epoch  0, batch    11 | loss: 3.1334504MixupTrain:  epoch  0, batch    12 | loss: 3.2410361MixupTrain:  epoch  0, batch    13 | loss: 3.1353845MixupTrain:  epoch  0, batch    14 | loss: 2.4289924MixupTrain:  epoch  0, batch    15 | loss: 2.8260055MixupTrain:  epoch  0, batch    16 | loss: 1.7166131
MemoryTrain:  epoch  0, batch     0 | loss: 1.8763070MemoryTrain:  epoch  0, batch     1 | loss: 2.8459616MemoryTrain:  epoch  0, batch     2 | loss: 3.3824446MemoryTrain:  epoch  0, batch     3 | loss: 3.4195910MemoryTrain:  epoch  0, batch     4 | loss: 3.6774704MemoryTrain:  epoch  0, batch     5 | loss: 4.0625110MemoryTrain:  epoch  1, batch     0 | loss: 3.0742002MemoryTrain:  epoch  1, batch     1 | loss: 3.0682449MemoryTrain:  epoch  1, batch     2 | loss: 3.2606266MemoryTrain:  epoch  1, batch     3 | loss: 1.6933730MemoryTrain:  epoch  1, batch     4 | loss: 2.6872368MemoryTrain:  epoch  1, batch     5 | loss: 1.9601727MemoryTrain:  epoch  2, batch     0 | loss: 2.6716928MemoryTrain:  epoch  2, batch     1 | loss: 3.0413742MemoryTrain:  epoch  2, batch     2 | loss: 2.3013215MemoryTrain:  epoch  2, batch     3 | loss: 2.1090508MemoryTrain:  epoch  2, batch     4 | loss: 2.5634911MemoryTrain:  epoch  2, batch     5 | loss: 1.6552769MemoryTrain:  epoch  3, batch     0 | loss: 2.7019799MemoryTrain:  epoch  3, batch     1 | loss: 2.0882783MemoryTrain:  epoch  3, batch     2 | loss: 2.1574340MemoryTrain:  epoch  3, batch     3 | loss: 1.8732708MemoryTrain:  epoch  3, batch     4 | loss: 1.9181675MemoryTrain:  epoch  3, batch     5 | loss: 3.2026460MemoryTrain:  epoch  4, batch     0 | loss: 1.5081255MemoryTrain:  epoch  4, batch     1 | loss: 2.2668028MemoryTrain:  epoch  4, batch     2 | loss: 2.9841118MemoryTrain:  epoch  4, batch     3 | loss: 1.8906424MemoryTrain:  epoch  4, batch     4 | loss: 2.0364151MemoryTrain:  epoch  4, batch     5 | loss: 1.4166112MemoryTrain:  epoch  5, batch     0 | loss: 2.4494426MemoryTrain:  epoch  5, batch     1 | loss: 1.6675084MemoryTrain:  epoch  5, batch     2 | loss: 1.4538643MemoryTrain:  epoch  5, batch     3 | loss: 2.7049084MemoryTrain:  epoch  5, batch     4 | loss: 1.7721230MemoryTrain:  epoch  5, batch     5 | loss: 1.3489665MemoryTrain:  epoch  6, batch     0 | loss: 2.3621619MemoryTrain:  epoch  6, batch     1 | loss: 1.5037026MemoryTrain:  epoch  6, batch     2 | loss: 1.5852940MemoryTrain:  epoch  6, batch     3 | loss: 2.1824918MemoryTrain:  epoch  6, batch     4 | loss: 1.7163609MemoryTrain:  epoch  6, batch     5 | loss: 1.3998566MemoryTrain:  epoch  7, batch     0 | loss: 1.6711755MemoryTrain:  epoch  7, batch     1 | loss: 1.4675345MemoryTrain:  epoch  7, batch     2 | loss: 1.4613854MemoryTrain:  epoch  7, batch     3 | loss: 2.1172504MemoryTrain:  epoch  7, batch     4 | loss: 1.8433766MemoryTrain:  epoch  7, batch     5 | loss: 1.3243488MemoryTrain:  epoch  8, batch     0 | loss: 1.6118362MemoryTrain:  epoch  8, batch     1 | loss: 1.4682127MemoryTrain:  epoch  8, batch     2 | loss: 1.5881256MemoryTrain:  epoch  8, batch     3 | loss: 1.5461432MemoryTrain:  epoch  8, batch     4 | loss: 1.5885137MemoryTrain:  epoch  8, batch     5 | loss: 2.0174539MemoryTrain:  epoch  9, batch     0 | loss: 1.8622963MemoryTrain:  epoch  9, batch     1 | loss: 1.5164057MemoryTrain:  epoch  9, batch     2 | loss: 1.6358178MemoryTrain:  epoch  9, batch     3 | loss: 1.4398420MemoryTrain:  epoch  9, batch     4 | loss: 1.3222094MemoryTrain:  epoch  9, batch     5 | loss: 1.2766269
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 69.64%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 70.31%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 73.86%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 74.48%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 74.52%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 75.45%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 76.25%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 75.78%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 76.10%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 76.39%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 76.32%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 77.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 78.57%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 79.26%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 80.16%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 80.99%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 81.50%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 81.97%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 82.18%   [EVAL] batch:   27 | acc: 81.25%,  total acc: 82.14%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 82.33%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 82.66%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 83.01%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 83.82%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 84.11%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 84.63%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 84.87%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 84.62%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 83.91%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 83.54%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 83.04%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 82.41%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 82.39%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 82.78%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 83.02%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 83.38%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 83.72%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 84.12%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 84.07%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 83.65%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 83.37%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 83.22%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 83.41%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 83.04%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 82.68%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 82.65%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 82.20%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 82.40%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 82.38%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 82.66%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 82.04%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 80.73%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 80.77%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 81.70%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 84.72%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 85.53%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 85.31%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 85.80%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 85.60%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 85.68%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 85.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.30%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 87.05%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 87.28%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.48%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 88.83%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 89.15%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 89.86%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 89.97%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.22%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.47%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 90.55%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.77%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 90.84%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 91.05%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 91.11%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 91.03%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 90.96%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 90.89%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 91.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.18%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 91.11%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 91.16%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 91.20%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 91.02%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 90.79%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 89.76%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 89.19%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 88.85%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 88.11%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 87.40%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 87.10%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 86.52%   [EVAL] batch:   64 | acc: 43.75%,  total acc: 85.87%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 85.89%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 85.26%   [EVAL] batch:   67 | acc: 62.50%,  total acc: 84.93%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 84.69%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 84.95%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 84.90%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 85.10%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 85.30%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 85.50%   [EVAL] batch:   75 | acc: 0.00%,  total acc: 84.38%   [EVAL] batch:   76 | acc: 12.50%,  total acc: 83.44%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 82.69%   [EVAL] batch:   78 | acc: 18.75%,  total acc: 81.88%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 80.86%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 79.86%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 79.80%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 80.05%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 80.28%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 80.44%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 80.67%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 80.82%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 80.82%   [EVAL] batch:   88 | acc: 31.25%,  total acc: 80.27%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 80.00%   [EVAL] batch:   90 | acc: 50.00%,  total acc: 79.67%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 79.48%   [EVAL] batch:   92 | acc: 50.00%,  total acc: 79.17%   [EVAL] batch:   93 | acc: 68.75%,  total acc: 79.06%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 79.28%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 79.43%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 79.57%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 79.72%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 79.92%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 80.12%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 80.32%   [EVAL] batch:  101 | acc: 81.25%,  total acc: 80.33%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 80.40%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 80.59%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 80.77%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 80.90%   [EVAL] batch:  106 | acc: 75.00%,  total acc: 80.84%   [EVAL] batch:  107 | acc: 62.50%,  total acc: 80.67%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 80.50%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 80.17%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 79.84%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 79.58%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 79.65%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 79.55%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 79.73%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 79.80%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 79.91%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 80.03%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 80.04%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 80.16%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 80.22%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 80.23%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 80.23%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 80.29%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 80.35%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 80.31%   [EVAL] batch:  126 | acc: 62.50%,  total acc: 80.17%   [EVAL] batch:  127 | acc: 75.00%,  total acc: 80.13%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 80.09%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 80.00%   [EVAL] batch:  130 | acc: 56.25%,  total acc: 79.82%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 79.78%   [EVAL] batch:  132 | acc: 75.00%,  total acc: 79.75%   [EVAL] batch:  133 | acc: 68.75%,  total acc: 79.66%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 79.72%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 79.83%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 79.84%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 79.80%   [EVAL] batch:  138 | acc: 87.50%,  total acc: 79.86%   [EVAL] batch:  139 | acc: 87.50%,  total acc: 79.91%   [EVAL] batch:  140 | acc: 68.75%,  total acc: 79.83%   [EVAL] batch:  141 | acc: 81.25%,  total acc: 79.84%   [EVAL] batch:  142 | acc: 81.25%,  total acc: 79.85%   [EVAL] batch:  143 | acc: 75.00%,  total acc: 79.82%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 79.96%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 80.09%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 80.19%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 80.32%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 80.45%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 80.54%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 80.63%   [EVAL] batch:  151 | acc: 87.50%,  total acc: 80.67%   [EVAL] batch:  152 | acc: 81.25%,  total acc: 80.68%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 80.72%   [EVAL] batch:  154 | acc: 87.50%,  total acc: 80.77%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 80.81%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 80.89%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 81.01%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 81.09%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 81.17%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 81.33%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 81.40%   [EVAL] batch:  163 | acc: 75.00%,  total acc: 81.36%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 81.21%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 81.14%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 81.03%   [EVAL] batch:  167 | acc: 56.25%,  total acc: 80.88%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 80.88%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 80.99%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 81.07%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 81.18%   [EVAL] batch:  172 | acc: 100.00%,  total acc: 81.29%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 81.36%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 81.43%   [EVAL] batch:  175 | acc: 81.25%,  total acc: 81.43%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 81.32%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 81.22%   [EVAL] batch:  179 | acc: 93.75%,  total acc: 81.28%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 81.18%   [EVAL] batch:  181 | acc: 62.50%,  total acc: 81.08%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 81.08%   [EVAL] batch:  183 | acc: 56.25%,  total acc: 80.94%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 81.01%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 81.01%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 81.12%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 80.92%   
cur_acc:  ['0.9524', '0.7748', '0.8204']
his_acc:  ['0.9524', '0.8575', '0.8092']
CurrentTrain: epoch  0, batch     0 | loss: 5.2302980CurrentTrain: epoch  0, batch     1 | loss: 6.6753435CurrentTrain: epoch  0, batch     2 | loss: 5.9311171CurrentTrain: epoch  0, batch     3 | loss: 4.4945827CurrentTrain: epoch  1, batch     0 | loss: 5.1878052CurrentTrain: epoch  1, batch     1 | loss: 5.6382222CurrentTrain: epoch  1, batch     2 | loss: 4.1872072CurrentTrain: epoch  1, batch     3 | loss: 3.9942212CurrentTrain: epoch  2, batch     0 | loss: 4.3084030CurrentTrain: epoch  2, batch     1 | loss: 4.8300591CurrentTrain: epoch  2, batch     2 | loss: 4.7191401CurrentTrain: epoch  2, batch     3 | loss: 1.7751238CurrentTrain: epoch  3, batch     0 | loss: 4.7247038CurrentTrain: epoch  3, batch     1 | loss: 4.5998263CurrentTrain: epoch  3, batch     2 | loss: 3.6255598CurrentTrain: epoch  3, batch     3 | loss: 4.7553296CurrentTrain: epoch  4, batch     0 | loss: 3.6858122CurrentTrain: epoch  4, batch     1 | loss: 4.0314875CurrentTrain: epoch  4, batch     2 | loss: 4.3404465CurrentTrain: epoch  4, batch     3 | loss: 4.2505713CurrentTrain: epoch  5, batch     0 | loss: 3.2688832CurrentTrain: epoch  5, batch     1 | loss: 3.9961710CurrentTrain: epoch  5, batch     2 | loss: 3.5630422CurrentTrain: epoch  5, batch     3 | loss: 4.5275340CurrentTrain: epoch  6, batch     0 | loss: 3.2150817CurrentTrain: epoch  6, batch     1 | loss: 3.9750481CurrentTrain: epoch  6, batch     2 | loss: 2.9860137CurrentTrain: epoch  6, batch     3 | loss: 5.1936665CurrentTrain: epoch  7, batch     0 | loss: 2.8384562CurrentTrain: epoch  7, batch     1 | loss: 3.5072563CurrentTrain: epoch  7, batch     2 | loss: 3.6224787CurrentTrain: epoch  7, batch     3 | loss: 1.9145434CurrentTrain: epoch  8, batch     0 | loss: 3.3230429CurrentTrain: epoch  8, batch     1 | loss: 3.0336728CurrentTrain: epoch  8, batch     2 | loss: 2.9272649CurrentTrain: epoch  8, batch     3 | loss: 2.8496900CurrentTrain: epoch  9, batch     0 | loss: 2.5674543CurrentTrain: epoch  9, batch     1 | loss: 3.6845956CurrentTrain: epoch  9, batch     2 | loss: 2.4413612CurrentTrain: epoch  9, batch     3 | loss: 3.3498173
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: george lawson sheldon ( may 31 , 1870 – april 4 , 1960 ) was 14th governor of nebraska with service from 1907 until 1909 .
Head Entity: governor
Tail Entity: nebraska
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The Environmental Protection Agency (EPA) is responsible for enforcing regulations related to environmental protection in the United States.  
Head Entity: Environmental Protection Agency  
Tail Entity: United States  

Relation: applies to jurisdiction  
Context: The California Consumer Privacy Act (CCPA) provides residents of California with specific rights regarding their personal information.  
Head Entity: California Consumer Privacy Act  
Tail Entity: California  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: it is prominently featured on jean michel jarre 's albums " oxygène " ( 1977 ) and " équinoxe " ( 1978 ) .
Head Entity: équinoxe
Tail Entity: jean michel jarre
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: The legendary band Queen is known for their iconic song "Bohemian Rhapsody," which showcases Freddie Mercury's extraordinary vocal talent.  
Head Entity: Bohemian Rhapsody  
Tail Entity: Freddie Mercury  

Relation: performer  
Context: Taylor Swift captivated audiences with her heartfelt performance of "Love Story" during the awards ceremony last night.  
Head Entity: Love Story  
Tail Entity: Taylor Swift  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the glasflügel h-201 standard " libelle " ( german : " dragonfly " ) is an early composite standard class single - seat sailplane produced by glasflügel from 1967 .
Head Entity: dragonfly
Tail Entity: glasflügel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The iPhone 13, a smartphone developed by Apple, was released in September 2021 and features advanced camera technology and improved battery life.  
Head Entity: iPhone 13  
Tail Entity: Apple  

Relation: manufacturer  
Context: The Tesla Model S is an all-electric luxury sedan that was first introduced by Tesla, Inc. in 2012, showcasing innovative technology and performance.  
Head Entity: Tesla Model S  
Tail Entity: Tesla, Inc.  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: at the 2016 summer olympics , heath and schofield won the silver k-2 200 event , finishing behind spain 's saúl craviotto and cristian toro .
Head Entity: 2016 summer olympics
Tail Entity: spain
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: during the 2020 presidential election, the democratic party nominated joe biden as their candidate, while the republican party supported donald trump.  
Head Entity: 2020 presidential election  
Tail Entity: democratic party  

Relation: participant  
Context: in the 2022 world cup, france emerged as the champion, defeating argentina in a thrilling final match.  
Head Entity: 2022 world cup  
Tail Entity: france  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the application is compatible with various operating systems, including macOS, Windows, and Ubuntu.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: This game runs smoothly on multiple operating systems, such as Linux, Windows 10, and older versions of Windows.  
Head Entity: game  
Tail Entity: Linux  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in his next bout , he won the wbc light welterweight title by defeating roger mayweather for a second time .
Head Entity: roger mayweather
Tail Entity: light welterweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: The team secured their place in the tournament by qualifying under the UEFA Champions League regulations.  
Head Entity: UEFA Champions League  
Tail Entity: tournament  

Relation: competition class  
Context: The athlete was classified under the FIS Alpine Skiing World Cup, allowing him to compete at the highest level.  
Head Entity: FIS Alpine Skiing World Cup  
Tail Entity: highest level  
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: swartz served as swedish prime minister for only seven months before being forced from power after the swedish elections of 1917 and replaced by the liberal government of nils edén .
Head Entity: swedish elections of 1917
Tail Entity: nils edén
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: in the recent municipal elections, the voters chose maria gonzalez as the new mayor of the city, marking a significant shift in local governance.  
Head Entity: municipal elections  
Tail Entity: maria gonzalez  

Relation: successful candidate  
Context: after a closely contested race, the citizens of springfield elected john doe as their new governor, succeeding the long-serving incumbent.  
Head Entity: springfield elections  
Tail Entity: john doe  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: although the latter 's eldest son , lǫgmaðr , appears to have succeeded to the kingship , he was soon forced to contend with factions supporting his younger brothers : haraldr , and óláfr .
Head Entity: lǫgmaðr
Tail Entity: óláfr
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: In the royal family of the ancient kingdom, the two brothers, Eirik and Sigurd, were known for their fierce rivalry, yet they shared the same lineage and upbringing under their parents, King Harald and Queen Ingrid.  
Head Entity: Eirik  
Tail Entity: Sigurd  

Relation: sibling  
Context: During the family reunion, it was evident that both Anna and her brother, Mark, inherited their parents' artistic talents, showcasing their skills in painting and music.  
Head Entity: Anna  
Tail Entity: Mark  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: iain harrison is a competitive shooter and former british army captain .
Head Entity: iain harrison
Tail Entity: british army
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: sergeant johnson served in the united states marine corps during his military career.  
Head Entity: sergeant johnson  
Tail Entity: united states marine corps  

Relation: military branch  
Context: general smith was a prominent figure in the royal air force, leading several key missions.  
Head Entity: general smith  
Tail Entity: royal air force  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: ziad rahbani is the son of the lebanese famous composer assi rahbani and nouhad haddad , the famous lebanese female singer known as fairuz .
Head Entity: assi rahbani
Tail Entity: ziad rahbani
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: in the animated series, the character simon is the son of the adventurous couple, marie and tom, who often embark on thrilling quests together.  
Head Entity: marie  
Tail Entity: simon  

Relation: child  
Context: the renowned physicist albert einstein had a daughter named lieserl, who was born in 1902 and is often mentioned in discussions about his family life.  
Head Entity: albert einstein  
Tail Entity: lieserl
Mixup data size:  319
MixupTrain:  epoch  0, batch     0 | loss: 2.5081050MixupTrain:  epoch  0, batch     1 | loss: 3.2685660MixupTrain:  epoch  0, batch     2 | loss: 3.1439048MixupTrain:  epoch  0, batch     3 | loss: 3.3397386MixupTrain:  epoch  0, batch     4 | loss: 2.8961268MixupTrain:  epoch  0, batch     5 | loss: 2.2023406MixupTrain:  epoch  0, batch     6 | loss: 2.4540887MixupTrain:  epoch  0, batch     7 | loss: 2.6164654MixupTrain:  epoch  0, batch     8 | loss: 3.3686697MixupTrain:  epoch  0, batch     9 | loss: 2.9152777MixupTrain:  epoch  0, batch    10 | loss: 3.1944059MixupTrain:  epoch  0, batch    11 | loss: 2.4760215MixupTrain:  epoch  0, batch    12 | loss: 2.9540098MixupTrain:  epoch  0, batch    13 | loss: 2.8206838MixupTrain:  epoch  0, batch    14 | loss: 2.5383149MixupTrain:  epoch  0, batch    15 | loss: 2.5018758MixupTrain:  epoch  0, batch    16 | loss: 2.0531729MixupTrain:  epoch  0, batch    17 | loss: 2.6738641MixupTrain:  epoch  0, batch    18 | loss: 2.8239528MixupTrain:  epoch  0, batch    19 | loss: 1.9469154
MemoryTrain:  epoch  0, batch     0 | loss: 2.4790070MemoryTrain:  epoch  0, batch     1 | loss: 2.9636369MemoryTrain:  epoch  0, batch     2 | loss: 3.4866383MemoryTrain:  epoch  0, batch     3 | loss: 3.1235898MemoryTrain:  epoch  0, batch     4 | loss: 3.0577955MemoryTrain:  epoch  0, batch     5 | loss: 2.6085496MemoryTrain:  epoch  0, batch     6 | loss: 3.3775766MemoryTrain:  epoch  0, batch     7 | loss: 3.4074862MemoryTrain:  epoch  1, batch     0 | loss: 2.4100268MemoryTrain:  epoch  1, batch     1 | loss: 3.4380512MemoryTrain:  epoch  1, batch     2 | loss: 2.6050234MemoryTrain:  epoch  1, batch     3 | loss: 2.4127288MemoryTrain:  epoch  1, batch     4 | loss: 3.2358072MemoryTrain:  epoch  1, batch     5 | loss: 3.0088687MemoryTrain:  epoch  1, batch     6 | loss: 2.4315071MemoryTrain:  epoch  1, batch     7 | loss: 2.5564232MemoryTrain:  epoch  2, batch     0 | loss: 2.1080709MemoryTrain:  epoch  2, batch     1 | loss: 2.2737827MemoryTrain:  epoch  2, batch     2 | loss: 3.0451784MemoryTrain:  epoch  2, batch     3 | loss: 2.0718255MemoryTrain:  epoch  2, batch     4 | loss: 2.5751204MemoryTrain:  epoch  2, batch     5 | loss: 2.1293654MemoryTrain:  epoch  2, batch     6 | loss: 1.9085660MemoryTrain:  epoch  2, batch     7 | loss: 2.3111229MemoryTrain:  epoch  3, batch     0 | loss: 2.3232305MemoryTrain:  epoch  3, batch     1 | loss: 2.4221172MemoryTrain:  epoch  3, batch     2 | loss: 1.9758847MemoryTrain:  epoch  3, batch     3 | loss: 2.4194241MemoryTrain:  epoch  3, batch     4 | loss: 1.8365518MemoryTrain:  epoch  3, batch     5 | loss: 1.8233589MemoryTrain:  epoch  3, batch     6 | loss: 1.7127522MemoryTrain:  epoch  3, batch     7 | loss: 2.3854027MemoryTrain:  epoch  4, batch     0 | loss: 2.0670972MemoryTrain:  epoch  4, batch     1 | loss: 2.7046859MemoryTrain:  epoch  4, batch     2 | loss: 1.8066269MemoryTrain:  epoch  4, batch     3 | loss: 1.9178438MemoryTrain:  epoch  4, batch     4 | loss: 1.6050465MemoryTrain:  epoch  4, batch     5 | loss: 2.0621715MemoryTrain:  epoch  4, batch     6 | loss: 1.9213170MemoryTrain:  epoch  4, batch     7 | loss: 1.4700799MemoryTrain:  epoch  5, batch     0 | loss: 1.7973621MemoryTrain:  epoch  5, batch     1 | loss: 2.3900163MemoryTrain:  epoch  5, batch     2 | loss: 1.7856923MemoryTrain:  epoch  5, batch     3 | loss: 1.9271320MemoryTrain:  epoch  5, batch     4 | loss: 1.8330579MemoryTrain:  epoch  5, batch     5 | loss: 1.5818963MemoryTrain:  epoch  5, batch     6 | loss: 1.6896186MemoryTrain:  epoch  5, batch     7 | loss: 2.1055579MemoryTrain:  epoch  6, batch     0 | loss: 1.7332056MemoryTrain:  epoch  6, batch     1 | loss: 2.2572124MemoryTrain:  epoch  6, batch     2 | loss: 1.4704741MemoryTrain:  epoch  6, batch     3 | loss: 1.7681866MemoryTrain:  epoch  6, batch     4 | loss: 1.7388506MemoryTrain:  epoch  6, batch     5 | loss: 1.6757014MemoryTrain:  epoch  6, batch     6 | loss: 1.5653958MemoryTrain:  epoch  6, batch     7 | loss: 1.3963003MemoryTrain:  epoch  7, batch     0 | loss: 1.6111836MemoryTrain:  epoch  7, batch     1 | loss: 1.7300656MemoryTrain:  epoch  7, batch     2 | loss: 1.5556498MemoryTrain:  epoch  7, batch     3 | loss: 1.5768056MemoryTrain:  epoch  7, batch     4 | loss: 1.4900616MemoryTrain:  epoch  7, batch     5 | loss: 1.3881973MemoryTrain:  epoch  7, batch     6 | loss: 1.6347129MemoryTrain:  epoch  7, batch     7 | loss: 1.2612798MemoryTrain:  epoch  8, batch     0 | loss: 1.4676418MemoryTrain:  epoch  8, batch     1 | loss: 1.4166602MemoryTrain:  epoch  8, batch     2 | loss: 1.8502715MemoryTrain:  epoch  8, batch     3 | loss: 1.5525101MemoryTrain:  epoch  8, batch     4 | loss: 1.3498927MemoryTrain:  epoch  8, batch     5 | loss: 1.5719048MemoryTrain:  epoch  8, batch     6 | loss: 1.4560519MemoryTrain:  epoch  8, batch     7 | loss: 1.3734822MemoryTrain:  epoch  9, batch     0 | loss: 1.4825599MemoryTrain:  epoch  9, batch     1 | loss: 1.6470180MemoryTrain:  epoch  9, batch     2 | loss: 1.2909853MemoryTrain:  epoch  9, batch     3 | loss: 1.3888208MemoryTrain:  epoch  9, batch     4 | loss: 1.3597733MemoryTrain:  epoch  9, batch     5 | loss: 1.4667815MemoryTrain:  epoch  9, batch     6 | loss: 1.4867290MemoryTrain:  epoch  9, batch     7 | loss: 1.4653153
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 78.98%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 78.37%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 78.57%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 78.33%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 79.04%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 79.86%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 79.28%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 77.81%   [EVAL] batch:   20 | acc: 50.00%,  total acc: 76.49%   [EVAL] batch:   21 | acc: 50.00%,  total acc: 75.28%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 74.18%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 74.22%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 74.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 75.93%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 76.79%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 77.59%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.03%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 79.49%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 80.11%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 80.70%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 81.77%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 82.26%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 82.73%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 82.85%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 82.62%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 82.89%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 83.14%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 82.67%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 81.94%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 80.57%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 79.92%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 79.82%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 78.95%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 78.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 78.43%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 78.73%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 79.01%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 79.05%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 79.32%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 79.58%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 79.17%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 78.34%   [EVAL] batch:   58 | acc: 6.25%,  total acc: 77.12%   [EVAL] batch:   59 | acc: 25.00%,  total acc: 76.25%   [EVAL] batch:   60 | acc: 12.50%,  total acc: 75.20%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 74.50%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 73.41%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 80.00%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 79.33%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 80.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 82.72%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 84.21%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 82.88%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 83.07%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 83.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.89%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.49%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.13%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.09%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.52%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 87.32%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 87.85%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.18%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 88.32%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.62%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.91%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 89.02%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 89.29%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 89.39%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 89.35%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 89.17%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 88.86%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 88.16%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 87.50%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 87.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 87.13%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 87.14%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 87.26%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 87.38%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 87.39%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 86.95%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 85.99%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 85.38%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 84.90%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 84.02%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 83.06%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 82.74%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 82.32%   [EVAL] batch:   64 | acc: 50.00%,  total acc: 81.83%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 81.82%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 81.34%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 80.97%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 80.80%   [EVAL] batch:   69 | acc: 56.25%,  total acc: 80.45%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 80.28%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 79.86%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 79.11%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 79.05%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 78.83%   [EVAL] batch:   75 | acc: 0.00%,  total acc: 77.80%   [EVAL] batch:   76 | acc: 6.25%,  total acc: 76.87%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 76.20%   [EVAL] batch:   78 | acc: 6.25%,  total acc: 75.32%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 74.38%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 73.46%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 73.48%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 73.80%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 74.34%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 74.64%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 74.86%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   88 | acc: 37.50%,  total acc: 74.58%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 74.38%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 74.18%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 74.05%   [EVAL] batch:   92 | acc: 31.25%,  total acc: 73.59%   [EVAL] batch:   93 | acc: 62.50%,  total acc: 73.47%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 73.75%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 73.96%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 74.16%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 74.43%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 74.68%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 74.94%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 75.12%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 75.12%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 75.24%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 75.42%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 75.65%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 75.88%   [EVAL] batch:  106 | acc: 75.00%,  total acc: 75.88%   [EVAL] batch:  107 | acc: 62.50%,  total acc: 75.75%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 75.57%   [EVAL] batch:  109 | acc: 50.00%,  total acc: 75.34%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 75.11%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 74.94%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 74.84%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 74.95%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 74.95%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 75.05%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 75.21%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 75.36%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 75.46%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 75.51%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 75.60%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 75.80%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 75.64%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 75.39%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 75.24%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 75.10%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 75.05%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 74.81%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 74.76%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 74.81%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 74.91%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 75.05%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 75.14%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 75.27%   [EVAL] batch:  137 | acc: 56.25%,  total acc: 75.14%   [EVAL] batch:  138 | acc: 37.50%,  total acc: 74.87%   [EVAL] batch:  139 | acc: 43.75%,  total acc: 74.64%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 74.34%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 73.99%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 73.73%   [EVAL] batch:  143 | acc: 50.00%,  total acc: 73.57%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 73.75%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 73.93%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 74.06%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 74.24%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 74.41%   [EVAL] batch:  149 | acc: 81.25%,  total acc: 74.46%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 74.59%   [EVAL] batch:  151 | acc: 87.50%,  total acc: 74.67%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 74.67%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 74.76%   [EVAL] batch:  154 | acc: 87.50%,  total acc: 74.84%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 74.92%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 75.04%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 75.20%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 75.31%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 75.43%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 75.54%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 75.66%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 75.81%   [EVAL] batch:  163 | acc: 75.00%,  total acc: 75.80%   [EVAL] batch:  164 | acc: 50.00%,  total acc: 75.64%   [EVAL] batch:  165 | acc: 75.00%,  total acc: 75.64%   [EVAL] batch:  166 | acc: 56.25%,  total acc: 75.52%   [EVAL] batch:  167 | acc: 50.00%,  total acc: 75.37%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 75.37%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 75.51%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 75.58%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 75.73%   [EVAL] batch:  172 | acc: 93.75%,  total acc: 75.83%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 75.93%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 76.00%   [EVAL] batch:  175 | acc: 75.00%,  total acc: 75.99%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 75.95%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 75.91%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 75.87%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 75.94%   [EVAL] batch:  180 | acc: 50.00%,  total acc: 75.79%   [EVAL] batch:  181 | acc: 56.25%,  total acc: 75.69%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 75.75%   [EVAL] batch:  183 | acc: 68.75%,  total acc: 75.71%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 75.78%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 75.84%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 75.94%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 75.96%   [EVAL] batch:  188 | acc: 75.00%,  total acc: 75.96%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 75.92%   [EVAL] batch:  190 | acc: 81.25%,  total acc: 75.95%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 75.98%   [EVAL] batch:  192 | acc: 68.75%,  total acc: 75.94%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 75.97%   [EVAL] batch:  194 | acc: 93.75%,  total acc: 76.06%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 76.05%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 75.98%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 76.07%   [EVAL] batch:  198 | acc: 87.50%,  total acc: 76.13%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 76.16%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 76.15%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 76.18%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 76.14%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 76.16%   [EVAL] batch:  204 | acc: 93.75%,  total acc: 76.25%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 76.33%   [EVAL] batch:  206 | acc: 50.00%,  total acc: 76.21%   [EVAL] batch:  207 | acc: 56.25%,  total acc: 76.11%   [EVAL] batch:  208 | acc: 31.25%,  total acc: 75.90%   [EVAL] batch:  209 | acc: 62.50%,  total acc: 75.83%   [EVAL] batch:  210 | acc: 50.00%,  total acc: 75.71%   [EVAL] batch:  211 | acc: 81.25%,  total acc: 75.74%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 75.79%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 75.91%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 76.02%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 76.13%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 76.24%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 76.35%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 76.43%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 76.53%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 76.64%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 76.75%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 76.85%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 76.95%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 77.06%   [EVAL] batch:  225 | acc: 93.75%,  total acc: 77.13%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 77.18%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 77.17%   [EVAL] batch:  228 | acc: 87.50%,  total acc: 77.21%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 77.28%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 77.30%   [EVAL] batch:  231 | acc: 50.00%,  total acc: 77.18%   [EVAL] batch:  232 | acc: 31.25%,  total acc: 76.98%   [EVAL] batch:  233 | acc: 37.50%,  total acc: 76.82%   [EVAL] batch:  234 | acc: 68.75%,  total acc: 76.78%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 76.69%   [EVAL] batch:  236 | acc: 25.00%,  total acc: 76.48%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 76.47%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 76.52%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 76.61%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 76.66%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 76.70%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 76.77%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 76.82%   [EVAL] batch:  244 | acc: 37.50%,  total acc: 76.66%   [EVAL] batch:  245 | acc: 12.50%,  total acc: 76.40%   [EVAL] batch:  246 | acc: 6.25%,  total acc: 76.11%   [EVAL] batch:  247 | acc: 25.00%,  total acc: 75.91%   [EVAL] batch:  248 | acc: 25.00%,  total acc: 75.70%   [EVAL] batch:  249 | acc: 18.75%,  total acc: 75.48%   
cur_acc:  ['0.9524', '0.7748', '0.8204', '0.7341']
his_acc:  ['0.9524', '0.8575', '0.8092', '0.7548']
CurrentTrain: epoch  0, batch     0 | loss: 4.7652664CurrentTrain: epoch  0, batch     1 | loss: 5.2985921CurrentTrain: epoch  0, batch     2 | loss: 5.4998531CurrentTrain: epoch  0, batch     3 | loss: 2.9793365CurrentTrain: epoch  1, batch     0 | loss: 4.2123051CurrentTrain: epoch  1, batch     1 | loss: 4.0888481CurrentTrain: epoch  1, batch     2 | loss: 3.8029990CurrentTrain: epoch  1, batch     3 | loss: 3.3051174CurrentTrain: epoch  2, batch     0 | loss: 2.9438162CurrentTrain: epoch  2, batch     1 | loss: 3.1256883CurrentTrain: epoch  2, batch     2 | loss: 3.4393363CurrentTrain: epoch  2, batch     3 | loss: 4.6664038CurrentTrain: epoch  3, batch     0 | loss: 3.0968058CurrentTrain: epoch  3, batch     1 | loss: 2.9994743CurrentTrain: epoch  3, batch     2 | loss: 2.4846334CurrentTrain: epoch  3, batch     3 | loss: 3.2475467CurrentTrain: epoch  4, batch     0 | loss: 2.8296313CurrentTrain: epoch  4, batch     1 | loss: 2.8437657CurrentTrain: epoch  4, batch     2 | loss: 2.2801085CurrentTrain: epoch  4, batch     3 | loss: 1.7872465CurrentTrain: epoch  5, batch     0 | loss: 2.3286891CurrentTrain: epoch  5, batch     1 | loss: 2.9183450CurrentTrain: epoch  5, batch     2 | loss: 2.2446640CurrentTrain: epoch  5, batch     3 | loss: 1.7684619CurrentTrain: epoch  6, batch     0 | loss: 2.0917554CurrentTrain: epoch  6, batch     1 | loss: 2.4356742CurrentTrain: epoch  6, batch     2 | loss: 2.3097835CurrentTrain: epoch  6, batch     3 | loss: 3.7796962CurrentTrain: epoch  7, batch     0 | loss: 2.5980380CurrentTrain: epoch  7, batch     1 | loss: 2.1569605CurrentTrain: epoch  7, batch     2 | loss: 2.1669846CurrentTrain: epoch  7, batch     3 | loss: 1.7661897CurrentTrain: epoch  8, batch     0 | loss: 2.0536366CurrentTrain: epoch  8, batch     1 | loss: 1.9684309CurrentTrain: epoch  8, batch     2 | loss: 2.0787194CurrentTrain: epoch  8, batch     3 | loss: 3.3796399CurrentTrain: epoch  9, batch     0 | loss: 2.0168443CurrentTrain: epoch  9, batch     1 | loss: 2.1537259CurrentTrain: epoch  9, batch     2 | loss: 1.9251971CurrentTrain: epoch  9, batch     3 | loss: 2.1308270
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: karl johan aarønes ( 8 may 1900 – 12 august 1969 ) was a norwegian politician for the labour party .
Head Entity: karl johan aarønes
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: elizabeth warren is a prominent member of the democratic party, advocating for progressive policies and reforms.  
Head Entity: elizabeth warren  
Tail Entity: democratic party  

Relation: member of political party  
Context: during his tenure, barack obama was a key figure in the democratic party, leading numerous initiatives and campaigns.  
Head Entity: barack obama  
Tail Entity: democratic party  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: the script for " the great santini " was adapted by carlino from the 1976 novel by pat conroy , with assistance from an un - credited herman raucher .
Head Entity: the great santini
Tail Entity: pat conroy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film "inception" draws heavily from the concepts presented in the 1982 novel "neuromancer" by william gibson, influencing its narrative structure and themes.  
Head Entity: inception  
Tail Entity: william gibson  

Relation: after a work by  
Context: the musical "hamilton" is inspired by the biography of alexander hamilton written by ron chernow, which provided the foundation for its storytelling and character development.  
Head Entity: hamilton  
Tail Entity: ron chernow  
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: the influential victorian art critic john ruskin expounded theories about social justice in " unto this last " ( 1860 ) .
Head Entity: john ruskin
Tail Entity: unto this last
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: the renowned physicist albert einstein published his groundbreaking paper on the theory of relativity in "on the electrodynamics of moving bodies" (1905).  
Head Entity: albert einstein  
Tail Entity: on the electrodynamics of moving bodies  

Relation: notable work  
Context: the celebrated author j.k. rowling captivated readers worldwide with her first book in the harry potter series, "harry potter and the philosopher's stone" (1997).  
Head Entity: j.k. rowling  
Tail Entity: harry potter and the philosopher's stone  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: at the request of the stockholm institute for bible translation , mirza khazar translated the bible – the new testament and old testament – into the azerbaijani language .
Head Entity: institute for bible translation
Tail Entity: stockholm
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: the global technology company, headquartered in san francisco, has been a leader in software development for over two decades.  
Head Entity: global technology company  
Tail Entity: san francisco  

Relation: headquarters location  
Context: after relocating its main office to new york city, the fashion brand saw a significant increase in sales and brand recognition.  
Head Entity: fashion brand  
Tail Entity: new york city  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most recognized as the wolf, which belongs to the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of plants, the family Rosaceae encompasses a variety of species, including roses and strawberries, which are categorized under the order Rosales.  
Head Entity: Rosaceae  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: he later collaborated with vito volterra and explored laplace transforms and other parts of functional analysis .
Head Entity: vito volterra
Tail Entity: functional analysis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: Marie Curie conducted pioneering research on radioactivity, which laid the groundwork for advancements in nuclear physics.  
Head Entity: Marie Curie  
Tail Entity: nuclear physics  

Relation: field of work  
Context: The company specializes in artificial intelligence and machine learning, providing innovative solutions for various industries.  
Head Entity: The company  
Tail Entity: artificial intelligence  
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: wjmi ( 99.7 fm ) is a radio station licensed to jackson , mississippi , united states , with a mainstream urban musical format .
Head Entity: wjmi
Tail Entity: jackson , mississippi
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: kxyz ( 101.5 fm ) is a radio station licensed to transmit to the city of austin , texas , known for its eclectic music scene.  
Head Entity: kxyz  
Tail Entity: austin , texas  

Relation: licensed to broadcast to  
Context: wxyz-tv is a television station licensed to serve the area of rochester , new york , providing local news and entertainment programming.  
Head Entity: wxyz-tv  
Tail Entity: rochester , new york  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: tau² eridani ( τ² eridani , abbreviated tau² eri , τ² eri ) , also named angetenar , is a star in the constellation of eridanus .
Head Entity: angetenar
Tail Entity: eridanus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: betelgeuse ( α orionis , also known as alpha orionis ) is a red supergiant star located in the constellation of orion.  
Head Entity: betelgeuse  
Tail Entity: orion  

Relation: constellation  
Context: the star vega, also designated alpha lyrae, is the brightest star in the constellation of lyra.  
Head Entity: vega  
Tail Entity: lyra  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: services to the island run from san juan 's luis muñoz marín international airport , ceiba airport or isla grande airport ( 20- to 30-minute flight ) .
Head Entity: isla grande airport
Tail Entity: san juan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The city of Barcelona is well-connected to various destinations through the El Prat Airport, which offers flights to numerous European cities.  
Head Entity: El Prat Airport  
Tail Entity: Barcelona  

Relation: place served by transport hub  
Context: The central train station in Zurich provides access to multiple Swiss cities and international destinations, making it a key transport hub in the region.  
Head Entity: Zurich central train station  
Tail Entity: Zurich  
Mixup data size:  378
MixupTrain:  epoch  0, batch     0 | loss: 1.9907522MixupTrain:  epoch  0, batch     1 | loss: 2.1891159MixupTrain:  epoch  0, batch     2 | loss: 2.4240764MixupTrain:  epoch  0, batch     3 | loss: 2.1025287MixupTrain:  epoch  0, batch     4 | loss: 2.0455358MixupTrain:  epoch  0, batch     5 | loss: 1.9343889MixupTrain:  epoch  0, batch     6 | loss: 2.2190250MixupTrain:  epoch  0, batch     7 | loss: 1.9539808MixupTrain:  epoch  0, batch     8 | loss: 2.2207111MixupTrain:  epoch  0, batch     9 | loss: 1.9188116MixupTrain:  epoch  0, batch    10 | loss: 2.0126029MixupTrain:  epoch  0, batch    11 | loss: 2.0020278MixupTrain:  epoch  0, batch    12 | loss: 1.9935220MixupTrain:  epoch  0, batch    13 | loss: 1.8596575MixupTrain:  epoch  0, batch    14 | loss: 1.9542496MixupTrain:  epoch  0, batch    15 | loss: 1.8162322MixupTrain:  epoch  0, batch    16 | loss: 1.9544731MixupTrain:  epoch  0, batch    17 | loss: 1.8120219MixupTrain:  epoch  0, batch    18 | loss: 2.1971128MixupTrain:  epoch  0, batch    19 | loss: 1.7017414MixupTrain:  epoch  0, batch    20 | loss: 1.6671843MixupTrain:  epoch  0, batch    21 | loss: 1.7973687MixupTrain:  epoch  0, batch    22 | loss: 1.7696108MixupTrain:  epoch  0, batch    23 | loss: 1.7091290
MemoryTrain:  epoch  0, batch     0 | loss: 1.5786867MemoryTrain:  epoch  0, batch     1 | loss: 2.5986502MemoryTrain:  epoch  0, batch     2 | loss: 1.5373458MemoryTrain:  epoch  0, batch     3 | loss: 2.6680868MemoryTrain:  epoch  0, batch     4 | loss: 2.0743167MemoryTrain:  epoch  0, batch     5 | loss: 2.2789106MemoryTrain:  epoch  0, batch     6 | loss: 2.3685117MemoryTrain:  epoch  0, batch     7 | loss: 2.1209030MemoryTrain:  epoch  0, batch     8 | loss: 3.0623260MemoryTrain:  epoch  0, batch     9 | loss: 1.9212866MemoryTrain:  epoch  1, batch     0 | loss: 1.6614909MemoryTrain:  epoch  1, batch     1 | loss: 1.8784802MemoryTrain:  epoch  1, batch     2 | loss: 2.0179014MemoryTrain:  epoch  1, batch     3 | loss: 2.3578365MemoryTrain:  epoch  1, batch     4 | loss: 1.8396704MemoryTrain:  epoch  1, batch     5 | loss: 1.8034941MemoryTrain:  epoch  1, batch     6 | loss: 2.0130818MemoryTrain:  epoch  1, batch     7 | loss: 1.4211813MemoryTrain:  epoch  1, batch     8 | loss: 1.9890220MemoryTrain:  epoch  1, batch     9 | loss: 1.8333287MemoryTrain:  epoch  2, batch     0 | loss: 1.8925014MemoryTrain:  epoch  2, batch     1 | loss: 1.5602741MemoryTrain:  epoch  2, batch     2 | loss: 1.4901057MemoryTrain:  epoch  2, batch     3 | loss: 1.4485456MemoryTrain:  epoch  2, batch     4 | loss: 1.7590606MemoryTrain:  epoch  2, batch     5 | loss: 1.3482821MemoryTrain:  epoch  2, batch     6 | loss: 1.7979262MemoryTrain:  epoch  2, batch     7 | loss: 1.6721380MemoryTrain:  epoch  2, batch     8 | loss: 1.5330822MemoryTrain:  epoch  2, batch     9 | loss: 2.0607307MemoryTrain:  epoch  3, batch     0 | loss: 1.7943978MemoryTrain:  epoch  3, batch     1 | loss: 1.5323536MemoryTrain:  epoch  3, batch     2 | loss: 1.4261968MemoryTrain:  epoch  3, batch     3 | loss: 1.5401365MemoryTrain:  epoch  3, batch     4 | loss: 1.4392842MemoryTrain:  epoch  3, batch     5 | loss: 1.5634336MemoryTrain:  epoch  3, batch     6 | loss: 1.6063631MemoryTrain:  epoch  3, batch     7 | loss: 1.5205685MemoryTrain:  epoch  3, batch     8 | loss: 1.3591542MemoryTrain:  epoch  3, batch     9 | loss: 1.8878683MemoryTrain:  epoch  4, batch     0 | loss: 1.5260615MemoryTrain:  epoch  4, batch     1 | loss: 1.5763016MemoryTrain:  epoch  4, batch     2 | loss: 1.3883787MemoryTrain:  epoch  4, batch     3 | loss: 1.5141902MemoryTrain:  epoch  4, batch     4 | loss: 1.2682950MemoryTrain:  epoch  4, batch     5 | loss: 1.3809838MemoryTrain:  epoch  4, batch     6 | loss: 1.4195168MemoryTrain:  epoch  4, batch     7 | loss: 1.7671231MemoryTrain:  epoch  4, batch     8 | loss: 1.4377534MemoryTrain:  epoch  4, batch     9 | loss: 1.2969948MemoryTrain:  epoch  5, batch     0 | loss: 1.4307580MemoryTrain:  epoch  5, batch     1 | loss: 1.5551288MemoryTrain:  epoch  5, batch     2 | loss: 1.3816953MemoryTrain:  epoch  5, batch     3 | loss: 1.4323967MemoryTrain:  epoch  5, batch     4 | loss: 1.3742841MemoryTrain:  epoch  5, batch     5 | loss: 1.3103952MemoryTrain:  epoch  5, batch     6 | loss: 1.2754266MemoryTrain:  epoch  5, batch     7 | loss: 1.3709512MemoryTrain:  epoch  5, batch     8 | loss: 1.3285668MemoryTrain:  epoch  5, batch     9 | loss: 1.3967680MemoryTrain:  epoch  6, batch     0 | loss: 1.3685009MemoryTrain:  epoch  6, batch     1 | loss: 1.4153593MemoryTrain:  epoch  6, batch     2 | loss: 1.3699510MemoryTrain:  epoch  6, batch     3 | loss: 1.4206704MemoryTrain:  epoch  6, batch     4 | loss: 1.3140969MemoryTrain:  epoch  6, batch     5 | loss: 1.2752494MemoryTrain:  epoch  6, batch     6 | loss: 1.3329341MemoryTrain:  epoch  6, batch     7 | loss: 1.3189986MemoryTrain:  epoch  6, batch     8 | loss: 1.3030500MemoryTrain:  epoch  6, batch     9 | loss: 1.6535867MemoryTrain:  epoch  7, batch     0 | loss: 1.3363992MemoryTrain:  epoch  7, batch     1 | loss: 1.2224140MemoryTrain:  epoch  7, batch     2 | loss: 1.2773991MemoryTrain:  epoch  7, batch     3 | loss: 1.3953502MemoryTrain:  epoch  7, batch     4 | loss: 1.3946712MemoryTrain:  epoch  7, batch     5 | loss: 1.3914361MemoryTrain:  epoch  7, batch     6 | loss: 1.4047275MemoryTrain:  epoch  7, batch     7 | loss: 1.3384575MemoryTrain:  epoch  7, batch     8 | loss: 1.2498908MemoryTrain:  epoch  7, batch     9 | loss: 1.2593616MemoryTrain:  epoch  8, batch     0 | loss: 1.3614001MemoryTrain:  epoch  8, batch     1 | loss: 1.2306929MemoryTrain:  epoch  8, batch     2 | loss: 1.2835064MemoryTrain:  epoch  8, batch     3 | loss: 1.3228908MemoryTrain:  epoch  8, batch     4 | loss: 1.2583580MemoryTrain:  epoch  8, batch     5 | loss: 1.3070199MemoryTrain:  epoch  8, batch     6 | loss: 1.2892473MemoryTrain:  epoch  8, batch     7 | loss: 1.3026323MemoryTrain:  epoch  8, batch     8 | loss: 1.2607698MemoryTrain:  epoch  8, batch     9 | loss: 1.3094778MemoryTrain:  epoch  9, batch     0 | loss: 1.2734127MemoryTrain:  epoch  9, batch     1 | loss: 1.2123657MemoryTrain:  epoch  9, batch     2 | loss: 1.2987897MemoryTrain:  epoch  9, batch     3 | loss: 1.2476263MemoryTrain:  epoch  9, batch     4 | loss: 1.2829180MemoryTrain:  epoch  9, batch     5 | loss: 1.2709600MemoryTrain:  epoch  9, batch     6 | loss: 1.2895569MemoryTrain:  epoch  9, batch     7 | loss: 1.3154659MemoryTrain:  epoch  9, batch     8 | loss: 1.3168664MemoryTrain:  epoch  9, batch     9 | loss: 1.2356113
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 93.06%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 93.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 92.61%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 92.79%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 91.96%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 91.41%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 90.81%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 89.58%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 88.82%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 88.12%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 88.10%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 88.35%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 88.04%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 87.76%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 87.96%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.39%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.79%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 89.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 89.52%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 89.26%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 89.39%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 88.97%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 89.11%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 88.37%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 88.01%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 87.66%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.98%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.57%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 88.84%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 89.10%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 89.20%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 89.44%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 89.67%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 89.89%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.31%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 90.50%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 90.44%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.50%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 90.68%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 90.74%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 90.57%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 90.68%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 90.57%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 90.68%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 89.98%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 73.61%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 69.89%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 70.54%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 71.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 74.63%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 75.69%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 76.97%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 76.25%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 76.49%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 76.14%   [EVAL] batch:   22 | acc: 37.50%,  total acc: 74.46%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 74.48%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.96%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 76.85%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.23%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.64%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.27%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 80.87%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 81.43%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 81.79%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 82.29%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 82.77%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 83.06%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.49%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.91%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 84.15%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 84.74%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 84.94%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 84.65%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 84.04%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 83.33%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 83.42%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 83.12%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 83.09%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 83.05%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 82.90%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 82.64%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 82.05%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 82.14%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 81.80%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 80.93%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 80.51%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 80.31%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 79.71%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 79.13%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 78.77%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 78.32%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 77.40%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 77.46%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 77.15%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 76.84%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 76.63%   [EVAL] batch:   69 | acc: 43.75%,  total acc: 76.16%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 76.06%   [EVAL] batch:   71 | acc: 37.50%,  total acc: 75.52%   [EVAL] batch:   72 | acc: 18.75%,  total acc: 74.74%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 74.75%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 74.42%   [EVAL] batch:   75 | acc: 0.00%,  total acc: 73.44%   [EVAL] batch:   76 | acc: 6.25%,  total acc: 72.56%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 71.88%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 70.97%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 70.08%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 69.21%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 69.28%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 69.65%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 70.01%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 70.29%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 70.64%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 70.91%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 70.95%   [EVAL] batch:   88 | acc: 12.50%,  total acc: 70.29%   [EVAL] batch:   89 | acc: 37.50%,  total acc: 69.93%   [EVAL] batch:   90 | acc: 43.75%,  total acc: 69.64%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 69.57%   [EVAL] batch:   92 | acc: 12.50%,  total acc: 68.95%   [EVAL] batch:   93 | acc: 56.25%,  total acc: 68.82%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 69.40%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 69.65%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 69.90%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 70.20%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 70.50%   [EVAL] batch:  100 | acc: 43.75%,  total acc: 70.24%   [EVAL] batch:  101 | acc: 43.75%,  total acc: 69.98%   [EVAL] batch:  102 | acc: 31.25%,  total acc: 69.60%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 69.41%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 69.29%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 69.16%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 69.16%   [EVAL] batch:  107 | acc: 56.25%,  total acc: 69.04%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 68.98%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 68.86%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 68.69%   [EVAL] batch:  111 | acc: 56.25%,  total acc: 68.58%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 68.70%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 68.80%   [EVAL] batch:  115 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 68.91%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 68.96%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 69.12%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 69.32%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 69.52%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 69.67%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 69.77%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 69.91%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 70.15%   [EVAL] batch:  125 | acc: 37.50%,  total acc: 69.89%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 69.64%   [EVAL] batch:  127 | acc: 18.75%,  total acc: 69.24%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 69.14%   [EVAL] batch:  129 | acc: 31.25%,  total acc: 68.85%   [EVAL] batch:  130 | acc: 31.25%,  total acc: 68.56%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 68.61%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 68.80%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 68.98%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 69.21%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 69.39%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 69.57%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 69.57%   [EVAL] batch:  138 | acc: 50.00%,  total acc: 69.42%   [EVAL] batch:  139 | acc: 50.00%,  total acc: 69.29%   [EVAL] batch:  140 | acc: 50.00%,  total acc: 69.15%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 68.97%   [EVAL] batch:  142 | acc: 56.25%,  total acc: 68.88%   [EVAL] batch:  143 | acc: 56.25%,  total acc: 68.79%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 69.01%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 69.22%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 69.39%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 69.59%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 69.80%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 69.96%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 70.12%   [EVAL] batch:  151 | acc: 87.50%,  total acc: 70.23%   [EVAL] batch:  152 | acc: 81.25%,  total acc: 70.30%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 70.41%   [EVAL] batch:  154 | acc: 81.25%,  total acc: 70.48%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 70.59%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 70.74%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 70.93%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 71.07%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 71.21%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 71.35%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 71.49%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 71.63%   [EVAL] batch:  163 | acc: 75.00%,  total acc: 71.65%   [EVAL] batch:  164 | acc: 50.00%,  total acc: 71.52%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 71.50%   [EVAL] batch:  166 | acc: 56.25%,  total acc: 71.41%   [EVAL] batch:  167 | acc: 56.25%,  total acc: 71.32%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 71.34%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 71.51%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 71.60%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 71.77%   [EVAL] batch:  172 | acc: 100.00%,  total acc: 71.93%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 72.05%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 72.11%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 72.09%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 72.03%   [EVAL] batch:  177 | acc: 75.00%,  total acc: 72.05%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 72.03%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 72.12%   [EVAL] batch:  180 | acc: 43.75%,  total acc: 71.96%   [EVAL] batch:  181 | acc: 43.75%,  total acc: 71.81%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 71.79%   [EVAL] batch:  183 | acc: 50.00%,  total acc: 71.67%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 71.76%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 71.89%   [EVAL] batch:  187 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:  188 | acc: 68.75%,  total acc: 71.86%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:  190 | acc: 81.25%,  total acc: 71.92%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 71.97%   [EVAL] batch:  192 | acc: 68.75%,  total acc: 71.96%   [EVAL] batch:  193 | acc: 93.75%,  total acc: 72.07%   [EVAL] batch:  194 | acc: 93.75%,  total acc: 72.18%   [EVAL] batch:  195 | acc: 81.25%,  total acc: 72.23%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 72.24%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 72.35%   [EVAL] batch:  198 | acc: 93.75%,  total acc: 72.46%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 72.50%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 72.51%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 72.56%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 72.57%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 72.55%   [EVAL] batch:  204 | acc: 93.75%,  total acc: 72.65%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 72.75%   [EVAL] batch:  206 | acc: 43.75%,  total acc: 72.61%   [EVAL] batch:  207 | acc: 62.50%,  total acc: 72.57%   [EVAL] batch:  208 | acc: 37.50%,  total acc: 72.40%   [EVAL] batch:  209 | acc: 56.25%,  total acc: 72.32%   [EVAL] batch:  210 | acc: 62.50%,  total acc: 72.27%   [EVAL] batch:  211 | acc: 68.75%,  total acc: 72.26%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 72.27%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 72.37%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 72.63%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 72.75%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 72.88%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 72.97%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 73.10%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 73.22%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 73.34%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 73.46%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 73.58%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 73.69%   [EVAL] batch:  225 | acc: 93.75%,  total acc: 73.78%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 73.84%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 73.79%   [EVAL] batch:  228 | acc: 87.50%,  total acc: 73.85%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 73.94%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 73.97%   [EVAL] batch:  231 | acc: 50.00%,  total acc: 73.87%   [EVAL] batch:  232 | acc: 31.25%,  total acc: 73.69%   [EVAL] batch:  233 | acc: 31.25%,  total acc: 73.50%   [EVAL] batch:  234 | acc: 56.25%,  total acc: 73.43%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 73.38%   [EVAL] batch:  236 | acc: 31.25%,  total acc: 73.21%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 73.21%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 73.27%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 73.39%   [EVAL] batch:  240 | acc: 75.00%,  total acc: 73.39%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 73.45%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 73.53%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 73.59%   [EVAL] batch:  244 | acc: 25.00%,  total acc: 73.39%   [EVAL] batch:  245 | acc: 12.50%,  total acc: 73.15%   [EVAL] batch:  246 | acc: 0.00%,  total acc: 72.85%   [EVAL] batch:  247 | acc: 12.50%,  total acc: 72.61%   [EVAL] batch:  248 | acc: 12.50%,  total acc: 72.36%   [EVAL] batch:  249 | acc: 12.50%,  total acc: 72.12%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 72.21%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 72.30%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 72.38%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 72.44%   [EVAL] batch:  254 | acc: 100.00%,  total acc: 72.55%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:  256 | acc: 87.50%,  total acc: 72.71%   [EVAL] batch:  257 | acc: 100.00%,  total acc: 72.82%   [EVAL] batch:  258 | acc: 81.25%,  total acc: 72.85%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 72.93%   [EVAL] batch:  260 | acc: 87.50%,  total acc: 72.99%   [EVAL] batch:  261 | acc: 87.50%,  total acc: 73.04%   [EVAL] batch:  262 | acc: 100.00%,  total acc: 73.15%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 73.18%   [EVAL] batch:  264 | acc: 93.75%,  total acc: 73.25%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 73.28%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 73.31%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 73.30%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 73.30%   [EVAL] batch:  269 | acc: 75.00%,  total acc: 73.31%   [EVAL] batch:  270 | acc: 87.50%,  total acc: 73.36%   [EVAL] batch:  271 | acc: 93.75%,  total acc: 73.44%   [EVAL] batch:  272 | acc: 81.25%,  total acc: 73.47%   [EVAL] batch:  273 | acc: 81.25%,  total acc: 73.49%   [EVAL] batch:  274 | acc: 68.75%,  total acc: 73.48%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 73.57%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 73.67%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 73.76%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 73.86%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 73.95%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 74.04%   [EVAL] batch:  281 | acc: 81.25%,  total acc: 74.07%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 74.14%   [EVAL] batch:  283 | acc: 75.00%,  total acc: 74.14%   [EVAL] batch:  284 | acc: 93.75%,  total acc: 74.21%   [EVAL] batch:  285 | acc: 62.50%,  total acc: 74.17%   [EVAL] batch:  286 | acc: 75.00%,  total acc: 74.17%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 74.18%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 74.26%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 74.35%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 74.44%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 74.53%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 74.62%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 74.68%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 74.77%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 74.85%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 74.94%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 75.02%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 75.10%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 75.19%   [EVAL] batch:  300 | acc: 87.50%,  total acc: 75.23%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 75.29%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 75.37%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 75.43%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 75.45%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 75.51%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 75.57%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 75.61%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 75.65%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 75.71%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 75.76%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 75.80%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 75.72%   
cur_acc:  ['0.9524', '0.7748', '0.8204', '0.7341', '0.8998']
his_acc:  ['0.9524', '0.8575', '0.8092', '0.7548', '0.7572']
CurrentTrain: epoch  0, batch     0 | loss: 5.2015443CurrentTrain: epoch  0, batch     1 | loss: 6.3902016CurrentTrain: epoch  0, batch     2 | loss: 6.2811165CurrentTrain: epoch  0, batch     3 | loss: 4.2609138CurrentTrain: epoch  1, batch     0 | loss: 5.3784399CurrentTrain: epoch  1, batch     1 | loss: 5.0168905CurrentTrain: epoch  1, batch     2 | loss: 4.1392393CurrentTrain: epoch  1, batch     3 | loss: 3.9403234CurrentTrain: epoch  2, batch     0 | loss: 3.6952350CurrentTrain: epoch  2, batch     1 | loss: 4.9137859CurrentTrain: epoch  2, batch     2 | loss: 3.7941294CurrentTrain: epoch  2, batch     3 | loss: 3.3118753CurrentTrain: epoch  3, batch     0 | loss: 2.9569271CurrentTrain: epoch  3, batch     1 | loss: 4.1670227CurrentTrain: epoch  3, batch     2 | loss: 3.6155543CurrentTrain: epoch  3, batch     3 | loss: 2.0355959CurrentTrain: epoch  4, batch     0 | loss: 3.0014558CurrentTrain: epoch  4, batch     1 | loss: 3.1091104CurrentTrain: epoch  4, batch     2 | loss: 3.2627091CurrentTrain: epoch  4, batch     3 | loss: 3.2498195CurrentTrain: epoch  5, batch     0 | loss: 2.6624346CurrentTrain: epoch  5, batch     1 | loss: 2.8328147CurrentTrain: epoch  5, batch     2 | loss: 2.7935224CurrentTrain: epoch  5, batch     3 | loss: 2.8054006CurrentTrain: epoch  6, batch     0 | loss: 2.6869469CurrentTrain: epoch  6, batch     1 | loss: 2.9811659CurrentTrain: epoch  6, batch     2 | loss: 2.3179667CurrentTrain: epoch  6, batch     3 | loss: 1.9966071CurrentTrain: epoch  7, batch     0 | loss: 2.3564074CurrentTrain: epoch  7, batch     1 | loss: 2.8749030CurrentTrain: epoch  7, batch     2 | loss: 2.3328915CurrentTrain: epoch  7, batch     3 | loss: 1.8569610CurrentTrain: epoch  8, batch     0 | loss: 2.2768610CurrentTrain: epoch  8, batch     1 | loss: 2.5682189CurrentTrain: epoch  8, batch     2 | loss: 2.3397183CurrentTrain: epoch  8, batch     3 | loss: 1.8458607CurrentTrain: epoch  9, batch     0 | loss: 2.2821784CurrentTrain: epoch  9, batch     1 | loss: 2.2818284CurrentTrain: epoch  9, batch     2 | loss: 2.4148917CurrentTrain: epoch  9, batch     3 | loss: 3.2586348
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: rohan is the son of narayana murthy , founder of infosys , and his wife sudha murthy , an engineer and author of note .
Head Entity: narayana murthy
Tail Entity: sudha murthy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: elon musk, the CEO of spacex and tesla, was previously married to talulah riley, a talented actress and writer.  
Head Entity: elon musk  
Tail Entity: talulah riley  

Relation: spouse  
Context: barack obama, the 44th president of the united states, has been married to michelle obama since 1992, and they have two daughters together.  
Head Entity: barack obama  
Tail Entity: michelle obama  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: they briefly reformed in 1992 , when sub pop re - released " primal rock therapy " on cd with five additional unreleased tracks .
Head Entity: primal rock therapy
Tail Entity: sub pop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: The album "Future Nostalgia" was released under the Warner Records label, showcasing Dua Lipa's unique sound and style.  
Head Entity: Future Nostalgia  
Tail Entity: Warner Records  

Relation: record label  
Context: After signing with Columbia Records, the band released their highly anticipated debut album, which quickly climbed the charts.  
Head Entity: debut album  
Tail Entity: Columbia Records  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: distributed by paramount pictures , " the italian job " was theatrically released in the united states on may 30 , 2003 , and grossed over $ 176 million worldwide .
Head Entity: the italian job
Tail Entity: paramount pictures
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: the film was distributed by universal pictures and received critical acclaim upon its release in 2015.  
Head Entity: the film  
Tail Entity: universal pictures  

Relation: distributor  
Context: the album was distributed by sony music and quickly climbed the charts, becoming a bestseller.  
Head Entity: the album  
Tail Entity: sony music  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: in 1796 carondelet commissioned pintado to conduct a survey of lands granted to ambrosio longue - épée in iberville county on the west bank of the mississippi river opposite baton rouge .
Head Entity: baton rouge
Tail Entity: mississippi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: the city of new orleans is situated on the banks of the mississippi river, making it a vibrant hub for trade and culture.  
Head Entity: new orleans  
Tail Entity: mississippi river  

Relation: located in or next to body of water  
Context: the picturesque village of lake placid is nestled beside the serene waters of mirror lake, attracting visitors year-round.  
Head Entity: lake placid  
Tail Entity: mirror lake  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: cbs and viacom split again in 2005 , but both are still owned by national amusements .
Head Entity: national amusements
Tail Entity: viacom
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Relation: subsidiary  
Context: In 2018, Facebook acquired Instagram, which now operates as a subsidiary under the social media giant.  
Head Entity: Facebook  
Tail Entity: Instagram  

Relation: subsidiary  
Context: Toyota has several subsidiaries, including Lexus, which focuses on luxury vehicles.  
Head Entity: Toyota  
Tail Entity: Lexus  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: he challenged seven other swimmers in heat four including ukraine 's rostyslav svanidze , a top 16 finalist in atlanta four years earlier .
Head Entity: rostyslav svanidze
Tail Entity: ukraine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: after years of living in the united states, the renowned artist decided to apply for dual citizenship, allowing him to retain his ties to his homeland, spain.  
Head Entity: the renowned artist  
Tail Entity: spain  

Relation: country of citizenship  
Context: during the international conference, the delegate from canada proudly spoke about the rights and privileges of being a citizen of his country.  
Head Entity: the delegate from canada  
Tail Entity: canada  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: buck moved to austin in the mid-1970s and joined the blues rock group , the fabulous thunderbirds , along with keith ferguson , jimmie vaughan , and kim wilson .
Head Entity: the fabulous thunderbirds
Tail Entity: blues rock
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the album "thriller" by michael jackson is widely regarded as a landmark in pop music history, blending elements of rock, funk, and soul.  
Head Entity: michael jackson  
Tail Entity: pop  

Relation: genre  
Context: the film "inception," directed by christopher nolan, is a complex narrative that combines science fiction with psychological thriller elements.  
Head Entity: inception  
Tail Entity: science fiction  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: san lázaro is 660 km far from asunción and is located at the exact site of the confluence of the apa river with paraguay river , bordering brazil - chaco .
Head Entity: apa river
Tail Entity: paraguay river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: the mississippi river flows into the gulf of mexico, creating a rich delta ecosystem that supports diverse wildlife and plant species.  
Head Entity: mississippi river  
Tail Entity: gulf of mexico  

Relation: mouth of the watercourse  
Context: the rhine river, which runs through several countries in europe, ultimately empties into the north sea, providing a vital shipping route for international trade.  
Head Entity: rhine river  
Tail Entity: north sea  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: benjamin van leer ( born 9 april 1992 in houten ) is a dutch professional footballer who currently plays as a goalkeeper for ajax in the dutch eredivisie .
Head Entity: benjamin van leer
Tail Entity: goalkeeper
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: lebron james, born on december 30, 1984, is an american professional basketball player who currently plays as a forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: serena williams, born on september 26, 1981, is an american professional tennis player who is known for her powerful serve and currently competes in singles and doubles events.  
Head Entity: serena williams  
Tail Entity: tennis player  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: hammami made his international debut for tunisia in 2007 , and represented them at the africa cup of nations in 2010 and 2013 .
Head Entity: 2010
Tail Entity: africa cup of nations
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2021 UEFA European Championship was held in various cities across Europe, showcasing top national teams competing for the title.  
Head Entity: 2021  
Tail Entity: UEFA European Championship  

Relation: sports season of league or competition  
Context: The 2022 FIFA World Cup took place in Qatar, marking the first time the tournament was held in the Middle East.  
Head Entity: 2022  
Tail Entity: FIFA World Cup  
Mixup data size:  438
MixupTrain:  epoch  0, batch     0 | loss: 2.0799488MixupTrain:  epoch  0, batch     1 | loss: 2.1444147MixupTrain:  epoch  0, batch     2 | loss: 2.3668500MixupTrain:  epoch  0, batch     3 | loss: 2.3543142MixupTrain:  epoch  0, batch     4 | loss: 2.4866078MixupTrain:  epoch  0, batch     5 | loss: 2.2192858MixupTrain:  epoch  0, batch     6 | loss: 2.1962010MixupTrain:  epoch  0, batch     7 | loss: 1.8777645MixupTrain:  epoch  0, batch     8 | loss: 2.0372727MixupTrain:  epoch  0, batch     9 | loss: 2.8353008MixupTrain:  epoch  0, batch    10 | loss: 2.3340659MixupTrain:  epoch  0, batch    11 | loss: 2.0857599MixupTrain:  epoch  0, batch    12 | loss: 1.9701590MixupTrain:  epoch  0, batch    13 | loss: 2.2660295MixupTrain:  epoch  0, batch    14 | loss: 2.3037567MixupTrain:  epoch  0, batch    15 | loss: 1.7796506MixupTrain:  epoch  0, batch    16 | loss: 1.9989659MixupTrain:  epoch  0, batch    17 | loss: 2.5226595MixupTrain:  epoch  0, batch    18 | loss: 1.9959037MixupTrain:  epoch  0, batch    19 | loss: 1.8954208MixupTrain:  epoch  0, batch    20 | loss: 2.4757149MixupTrain:  epoch  0, batch    21 | loss: 2.1479692MixupTrain:  epoch  0, batch    22 | loss: 2.0481161MixupTrain:  epoch  0, batch    23 | loss: 1.9299640MixupTrain:  epoch  0, batch    24 | loss: 2.0831060MixupTrain:  epoch  0, batch    25 | loss: 2.2141437MixupTrain:  epoch  0, batch    26 | loss: 2.0323715MixupTrain:  epoch  0, batch    27 | loss: 1.5662571
MemoryTrain:  epoch  0, batch     0 | loss: 1.7426445MemoryTrain:  epoch  0, batch     1 | loss: 2.5554271MemoryTrain:  epoch  0, batch     2 | loss: 1.9508516MemoryTrain:  epoch  0, batch     3 | loss: 2.9661922MemoryTrain:  epoch  0, batch     4 | loss: 2.1829922MemoryTrain:  epoch  0, batch     5 | loss: 1.7088256MemoryTrain:  epoch  0, batch     6 | loss: 2.1544366MemoryTrain:  epoch  0, batch     7 | loss: 1.8108425MemoryTrain:  epoch  0, batch     8 | loss: 2.2011397MemoryTrain:  epoch  0, batch     9 | loss: 2.4125817MemoryTrain:  epoch  0, batch    10 | loss: 2.1550455MemoryTrain:  epoch  0, batch    11 | loss: 2.8931179MemoryTrain:  epoch  1, batch     0 | loss: 1.5607332MemoryTrain:  epoch  1, batch     1 | loss: 2.7867677MemoryTrain:  epoch  1, batch     2 | loss: 2.2164397MemoryTrain:  epoch  1, batch     3 | loss: 2.7828355MemoryTrain:  epoch  1, batch     4 | loss: 1.4208304MemoryTrain:  epoch  1, batch     5 | loss: 1.7782301MemoryTrain:  epoch  1, batch     6 | loss: 1.7847694MemoryTrain:  epoch  1, batch     7 | loss: 1.5206121MemoryTrain:  epoch  1, batch     8 | loss: 1.4690346MemoryTrain:  epoch  1, batch     9 | loss: 1.4209683MemoryTrain:  epoch  1, batch    10 | loss: 2.4503236MemoryTrain:  epoch  1, batch    11 | loss: 3.6351087MemoryTrain:  epoch  2, batch     0 | loss: 1.6977857MemoryTrain:  epoch  2, batch     1 | loss: 1.9384997MemoryTrain:  epoch  2, batch     2 | loss: 1.3200935MemoryTrain:  epoch  2, batch     3 | loss: 1.4950016MemoryTrain:  epoch  2, batch     4 | loss: 1.7477481MemoryTrain:  epoch  2, batch     5 | loss: 1.4777637MemoryTrain:  epoch  2, batch     6 | loss: 1.8134143MemoryTrain:  epoch  2, batch     7 | loss: 1.7448738MemoryTrain:  epoch  2, batch     8 | loss: 1.5851122MemoryTrain:  epoch  2, batch     9 | loss: 1.7863144MemoryTrain:  epoch  2, batch    10 | loss: 1.6761571MemoryTrain:  epoch  2, batch    11 | loss: 3.4292870MemoryTrain:  epoch  3, batch     0 | loss: 1.9487276MemoryTrain:  epoch  3, batch     1 | loss: 1.5978978MemoryTrain:  epoch  3, batch     2 | loss: 1.4256862MemoryTrain:  epoch  3, batch     3 | loss: 1.6584599MemoryTrain:  epoch  3, batch     4 | loss: 1.3995399MemoryTrain:  epoch  3, batch     5 | loss: 1.7604362MemoryTrain:  epoch  3, batch     6 | loss: 1.6026077MemoryTrain:  epoch  3, batch     7 | loss: 2.0522156MemoryTrain:  epoch  3, batch     8 | loss: 1.5535707MemoryTrain:  epoch  3, batch     9 | loss: 1.5102156MemoryTrain:  epoch  3, batch    10 | loss: 1.3188231MemoryTrain:  epoch  3, batch    11 | loss: 2.7951155MemoryTrain:  epoch  4, batch     0 | loss: 1.4087167MemoryTrain:  epoch  4, batch     1 | loss: 1.5436094MemoryTrain:  epoch  4, batch     2 | loss: 1.6416476MemoryTrain:  epoch  4, batch     3 | loss: 1.4326059MemoryTrain:  epoch  4, batch     4 | loss: 1.5186698MemoryTrain:  epoch  4, batch     5 | loss: 1.2774184MemoryTrain:  epoch  4, batch     6 | loss: 1.4912454MemoryTrain:  epoch  4, batch     7 | loss: 2.0171790MemoryTrain:  epoch  4, batch     8 | loss: 1.6859167MemoryTrain:  epoch  4, batch     9 | loss: 1.5226405MemoryTrain:  epoch  4, batch    10 | loss: 1.5316465MemoryTrain:  epoch  4, batch    11 | loss: 2.0526011MemoryTrain:  epoch  5, batch     0 | loss: 1.8124497MemoryTrain:  epoch  5, batch     1 | loss: 1.5318938MemoryTrain:  epoch  5, batch     2 | loss: 1.5916753MemoryTrain:  epoch  5, batch     3 | loss: 1.3583837MemoryTrain:  epoch  5, batch     4 | loss: 1.2423272MemoryTrain:  epoch  5, batch     5 | loss: 1.4507689MemoryTrain:  epoch  5, batch     6 | loss: 1.7498957MemoryTrain:  epoch  5, batch     7 | loss: 1.3643924MemoryTrain:  epoch  5, batch     8 | loss: 1.2976484MemoryTrain:  epoch  5, batch     9 | loss: 1.3273753MemoryTrain:  epoch  5, batch    10 | loss: 1.3094678MemoryTrain:  epoch  5, batch    11 | loss: 1.5141546MemoryTrain:  epoch  6, batch     0 | loss: 1.3300028MemoryTrain:  epoch  6, batch     1 | loss: 1.5002218MemoryTrain:  epoch  6, batch     2 | loss: 1.3269227MemoryTrain:  epoch  6, batch     3 | loss: 1.3559879MemoryTrain:  epoch  6, batch     4 | loss: 1.3397245MemoryTrain:  epoch  6, batch     5 | loss: 1.3983047MemoryTrain:  epoch  6, batch     6 | loss: 1.2697957MemoryTrain:  epoch  6, batch     7 | loss: 1.3376052MemoryTrain:  epoch  6, batch     8 | loss: 1.4727194MemoryTrain:  epoch  6, batch     9 | loss: 1.2759762MemoryTrain:  epoch  6, batch    10 | loss: 1.5236619MemoryTrain:  epoch  6, batch    11 | loss: 1.3509196MemoryTrain:  epoch  7, batch     0 | loss: 1.3056059MemoryTrain:  epoch  7, batch     1 | loss: 1.2611294MemoryTrain:  epoch  7, batch     2 | loss: 1.3322374MemoryTrain:  epoch  7, batch     3 | loss: 1.2574739MemoryTrain:  epoch  7, batch     4 | loss: 1.2959218MemoryTrain:  epoch  7, batch     5 | loss: 1.5037434MemoryTrain:  epoch  7, batch     6 | loss: 1.4059423MemoryTrain:  epoch  7, batch     7 | loss: 1.3754795MemoryTrain:  epoch  7, batch     8 | loss: 1.2575469MemoryTrain:  epoch  7, batch     9 | loss: 1.2777399MemoryTrain:  epoch  7, batch    10 | loss: 1.3607532MemoryTrain:  epoch  7, batch    11 | loss: 1.3244715MemoryTrain:  epoch  8, batch     0 | loss: 1.3451669MemoryTrain:  epoch  8, batch     1 | loss: 1.3144135MemoryTrain:  epoch  8, batch     2 | loss: 1.3837283MemoryTrain:  epoch  8, batch     3 | loss: 1.2497590MemoryTrain:  epoch  8, batch     4 | loss: 1.3016753MemoryTrain:  epoch  8, batch     5 | loss: 1.3240381MemoryTrain:  epoch  8, batch     6 | loss: 1.2256373MemoryTrain:  epoch  8, batch     7 | loss: 1.2466178MemoryTrain:  epoch  8, batch     8 | loss: 1.3255632MemoryTrain:  epoch  8, batch     9 | loss: 1.2982655MemoryTrain:  epoch  8, batch    10 | loss: 1.4030980MemoryTrain:  epoch  8, batch    11 | loss: 1.3175628MemoryTrain:  epoch  9, batch     0 | loss: 1.2472703MemoryTrain:  epoch  9, batch     1 | loss: 1.2943405MemoryTrain:  epoch  9, batch     2 | loss: 1.2230325MemoryTrain:  epoch  9, batch     3 | loss: 1.3465219MemoryTrain:  epoch  9, batch     4 | loss: 1.3718926MemoryTrain:  epoch  9, batch     5 | loss: 1.2779353MemoryTrain:  epoch  9, batch     6 | loss: 1.2587740MemoryTrain:  epoch  9, batch     7 | loss: 1.2558329MemoryTrain:  epoch  9, batch     8 | loss: 1.3046858MemoryTrain:  epoch  9, batch     9 | loss: 1.2489876MemoryTrain:  epoch  9, batch    10 | loss: 1.3619843MemoryTrain:  epoch  9, batch    11 | loss: 1.1882172
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 83.52%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 85.10%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 83.04%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 83.09%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 82.29%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.44%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 83.52%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 83.70%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 83.85%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 84.00%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 84.13%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 84.26%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 84.60%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.13%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 84.58%   [EVAL] batch:   30 | acc: 56.25%,  total acc: 83.67%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 83.01%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 82.39%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 81.43%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 80.89%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 80.38%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 79.90%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 79.77%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 79.81%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 80.00%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 80.34%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 80.36%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 80.52%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 80.54%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 79.58%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 78.67%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 77.93%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 77.73%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 76.66%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 76.25%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 76.72%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 77.16%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 77.59%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 78.01%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 78.41%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 79.06%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 79.20%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 79.13%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 79.27%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 79.30%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 79.44%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 78.77%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 68.18%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 66.15%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 66.83%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 68.30%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 69.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 71.48%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 72.79%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 73.96%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 75.33%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 74.69%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 74.72%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 74.18%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 74.22%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 74.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.72%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 76.62%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.46%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 78.23%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.64%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.27%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 80.87%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 80.51%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 80.36%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 80.21%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 80.41%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 80.59%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.09%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 81.56%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 81.86%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 82.29%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 82.56%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 82.78%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 82.61%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 82.05%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 81.38%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 81.76%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 81.38%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 81.37%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 81.37%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 81.37%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 81.13%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 80.57%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 80.69%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 80.37%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 79.31%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 78.50%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 78.23%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 77.46%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 76.71%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 76.29%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 75.88%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 75.19%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 75.09%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 74.72%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 74.45%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 74.28%   [EVAL] batch:   69 | acc: 31.25%,  total acc: 73.66%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 73.33%   [EVAL] batch:   71 | acc: 37.50%,  total acc: 72.83%   [EVAL] batch:   72 | acc: 18.75%,  total acc: 72.09%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 71.88%   [EVAL] batch:   74 | acc: 37.50%,  total acc: 71.42%   [EVAL] batch:   75 | acc: 0.00%,  total acc: 70.48%   [EVAL] batch:   76 | acc: 0.00%,  total acc: 69.56%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 68.91%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 68.04%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 67.19%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 66.36%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 66.46%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 66.87%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 67.26%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 67.57%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 67.95%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 68.25%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 68.39%   [EVAL] batch:   88 | acc: 6.25%,  total acc: 67.70%   [EVAL] batch:   89 | acc: 37.50%,  total acc: 67.36%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 67.24%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 67.12%   [EVAL] batch:   92 | acc: 18.75%,  total acc: 66.60%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 66.42%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 66.78%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 67.06%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 67.27%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 67.47%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 67.80%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 68.12%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 68.01%   [EVAL] batch:  101 | acc: 43.75%,  total acc: 67.77%   [EVAL] batch:  102 | acc: 31.25%,  total acc: 67.42%   [EVAL] batch:  103 | acc: 31.25%,  total acc: 67.07%   [EVAL] batch:  104 | acc: 50.00%,  total acc: 66.90%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 66.80%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 66.76%   [EVAL] batch:  107 | acc: 56.25%,  total acc: 66.67%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 66.63%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 66.53%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 66.44%   [EVAL] batch:  111 | acc: 56.25%,  total acc: 66.35%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 66.54%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 66.45%   [EVAL] batch:  114 | acc: 75.00%,  total acc: 66.52%   [EVAL] batch:  115 | acc: 56.25%,  total acc: 66.43%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 66.51%   [EVAL] batch:  117 | acc: 68.75%,  total acc: 66.53%   [EVAL] batch:  118 | acc: 50.00%,  total acc: 66.39%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 66.61%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 66.84%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 67.01%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 67.07%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 67.24%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 67.50%   [EVAL] batch:  125 | acc: 37.50%,  total acc: 67.26%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 67.03%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 66.75%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 66.57%   [EVAL] batch:  129 | acc: 18.75%,  total acc: 66.20%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 65.89%   [EVAL] batch:  131 | acc: 50.00%,  total acc: 65.77%   [EVAL] batch:  132 | acc: 68.75%,  total acc: 65.79%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 65.86%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 65.97%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 66.18%   [EVAL] batch:  136 | acc: 68.75%,  total acc: 66.20%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 66.08%   [EVAL] batch:  138 | acc: 31.25%,  total acc: 65.83%   [EVAL] batch:  139 | acc: 43.75%,  total acc: 65.67%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 65.43%   [EVAL] batch:  141 | acc: 31.25%,  total acc: 65.18%   [EVAL] batch:  142 | acc: 56.25%,  total acc: 65.12%   [EVAL] batch:  143 | acc: 50.00%,  total acc: 65.02%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 65.26%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 65.50%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 65.69%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 65.92%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 66.15%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 66.33%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 66.51%   [EVAL] batch:  151 | acc: 87.50%,  total acc: 66.65%   [EVAL] batch:  152 | acc: 87.50%,  total acc: 66.79%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 66.92%   [EVAL] batch:  154 | acc: 93.75%,  total acc: 67.10%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 67.23%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 67.40%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 67.60%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 67.77%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 67.93%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 68.09%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 68.25%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 68.17%   [EVAL] batch:  163 | acc: 25.00%,  total acc: 67.91%   [EVAL] batch:  164 | acc: 12.50%,  total acc: 67.58%   [EVAL] batch:  165 | acc: 18.75%,  total acc: 67.28%   [EVAL] batch:  166 | acc: 12.50%,  total acc: 66.95%   [EVAL] batch:  167 | acc: 6.25%,  total acc: 66.59%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 66.53%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 66.62%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 66.70%   [EVAL] batch:  171 | acc: 87.50%,  total acc: 66.82%   [EVAL] batch:  172 | acc: 75.00%,  total acc: 66.87%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 66.85%   [EVAL] batch:  174 | acc: 62.50%,  total acc: 66.82%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 66.73%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 66.74%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 66.71%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 66.72%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 66.74%   [EVAL] batch:  180 | acc: 31.25%,  total acc: 66.54%   [EVAL] batch:  181 | acc: 43.75%,  total acc: 66.41%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 66.50%   [EVAL] batch:  183 | acc: 50.00%,  total acc: 66.41%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 66.49%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 66.60%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 66.68%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 66.62%   [EVAL] batch:  188 | acc: 56.25%,  total acc: 66.57%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 66.58%   [EVAL] batch:  190 | acc: 68.75%,  total acc: 66.59%   [EVAL] batch:  191 | acc: 62.50%,  total acc: 66.57%   [EVAL] batch:  192 | acc: 68.75%,  total acc: 66.58%   [EVAL] batch:  193 | acc: 68.75%,  total acc: 66.59%   [EVAL] batch:  194 | acc: 93.75%,  total acc: 66.73%   [EVAL] batch:  195 | acc: 87.50%,  total acc: 66.84%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 66.85%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 66.92%   [EVAL] batch:  198 | acc: 93.75%,  total acc: 67.05%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 67.16%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 67.16%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 67.23%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 67.24%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 67.22%   [EVAL] batch:  204 | acc: 93.75%,  total acc: 67.35%   [EVAL] batch:  205 | acc: 87.50%,  total acc: 67.45%   [EVAL] batch:  206 | acc: 43.75%,  total acc: 67.33%   [EVAL] batch:  207 | acc: 62.50%,  total acc: 67.31%   [EVAL] batch:  208 | acc: 37.50%,  total acc: 67.17%   [EVAL] batch:  209 | acc: 56.25%,  total acc: 67.11%   [EVAL] batch:  210 | acc: 62.50%,  total acc: 67.09%   [EVAL] batch:  211 | acc: 75.00%,  total acc: 67.13%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 67.11%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 67.23%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 67.38%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 67.48%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 67.63%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 67.78%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 67.89%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 68.04%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 68.18%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 68.33%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 68.47%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 68.61%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  225 | acc: 93.75%,  total acc: 68.86%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 68.94%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 68.94%   [EVAL] batch:  228 | acc: 87.50%,  total acc: 69.02%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 69.13%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 69.18%   [EVAL] batch:  231 | acc: 43.75%,  total acc: 69.07%   [EVAL] batch:  232 | acc: 31.25%,  total acc: 68.91%   [EVAL] batch:  233 | acc: 31.25%,  total acc: 68.75%   [EVAL] batch:  234 | acc: 62.50%,  total acc: 68.72%   [EVAL] batch:  235 | acc: 68.75%,  total acc: 68.72%   [EVAL] batch:  236 | acc: 31.25%,  total acc: 68.57%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 68.57%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 68.65%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 68.78%   [EVAL] batch:  240 | acc: 75.00%,  total acc: 68.80%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 68.88%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 68.96%   [EVAL] batch:  243 | acc: 68.75%,  total acc: 68.95%   [EVAL] batch:  244 | acc: 6.25%,  total acc: 68.70%   [EVAL] batch:  245 | acc: 12.50%,  total acc: 68.47%   [EVAL] batch:  246 | acc: 0.00%,  total acc: 68.19%   [EVAL] batch:  247 | acc: 0.00%,  total acc: 67.92%   [EVAL] batch:  248 | acc: 6.25%,  total acc: 67.67%   [EVAL] batch:  249 | acc: 6.25%,  total acc: 67.42%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 67.53%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 67.63%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 67.74%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 67.81%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 67.92%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 68.04%   [EVAL] batch:  256 | acc: 87.50%,  total acc: 68.12%   [EVAL] batch:  257 | acc: 100.00%,  total acc: 68.24%   [EVAL] batch:  258 | acc: 81.25%,  total acc: 68.29%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 68.39%   [EVAL] batch:  260 | acc: 87.50%,  total acc: 68.46%   [EVAL] batch:  261 | acc: 87.50%,  total acc: 68.54%   [EVAL] batch:  262 | acc: 100.00%,  total acc: 68.65%   [EVAL] batch:  263 | acc: 87.50%,  total acc: 68.73%   [EVAL] batch:  264 | acc: 93.75%,  total acc: 68.82%   [EVAL] batch:  265 | acc: 87.50%,  total acc: 68.89%   [EVAL] batch:  266 | acc: 87.50%,  total acc: 68.96%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 68.96%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 68.98%   [EVAL] batch:  269 | acc: 75.00%,  total acc: 69.00%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 69.12%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 69.23%   [EVAL] batch:  272 | acc: 81.25%,  total acc: 69.28%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 69.34%   [EVAL] batch:  274 | acc: 75.00%,  total acc: 69.36%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 69.47%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 69.58%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 69.69%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 69.80%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 69.91%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 70.02%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 70.01%   [EVAL] batch:  282 | acc: 81.25%,  total acc: 70.05%   [EVAL] batch:  283 | acc: 68.75%,  total acc: 70.05%   [EVAL] batch:  284 | acc: 75.00%,  total acc: 70.07%   [EVAL] batch:  285 | acc: 62.50%,  total acc: 70.04%   [EVAL] batch:  286 | acc: 56.25%,  total acc: 69.99%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 69.99%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 70.09%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 70.19%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 70.30%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 70.40%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 70.50%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 70.58%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 70.68%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 70.78%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 70.88%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 70.97%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 71.07%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 71.17%   [EVAL] batch:  300 | acc: 87.50%,  total acc: 71.22%   [EVAL] batch:  301 | acc: 87.50%,  total acc: 71.27%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 71.35%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 71.42%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 71.45%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 71.53%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 71.60%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 71.65%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 71.70%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 71.79%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 71.89%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 71.94%   [EVAL] batch:  312 | acc: 93.75%,  total acc: 72.00%   [EVAL] batch:  313 | acc: 68.75%,  total acc: 71.99%   [EVAL] batch:  314 | acc: 100.00%,  total acc: 72.08%   [EVAL] batch:  315 | acc: 68.75%,  total acc: 72.07%   [EVAL] batch:  316 | acc: 81.25%,  total acc: 72.10%   [EVAL] batch:  317 | acc: 68.75%,  total acc: 72.09%   [EVAL] batch:  318 | acc: 75.00%,  total acc: 72.10%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 72.19%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 72.28%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 72.37%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 72.42%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 72.46%   [EVAL] batch:  325 | acc: 81.25%,  total acc: 72.49%   [EVAL] batch:  326 | acc: 62.50%,  total acc: 72.46%   [EVAL] batch:  327 | acc: 87.50%,  total acc: 72.50%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 72.51%   [EVAL] batch:  329 | acc: 93.75%,  total acc: 72.58%   [EVAL] batch:  330 | acc: 75.00%,  total acc: 72.58%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 72.65%   [EVAL] batch:  332 | acc: 81.25%,  total acc: 72.67%   [EVAL] batch:  333 | acc: 87.50%,  total acc: 72.72%   [EVAL] batch:  334 | acc: 81.25%,  total acc: 72.74%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 72.82%   [EVAL] batch:  336 | acc: 75.00%,  total acc: 72.83%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 72.89%   [EVAL] batch:  338 | acc: 93.75%,  total acc: 72.95%   [EVAL] batch:  339 | acc: 81.25%,  total acc: 72.98%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 73.06%   [EVAL] batch:  341 | acc: 81.25%,  total acc: 73.08%   [EVAL] batch:  342 | acc: 68.75%,  total acc: 73.07%   [EVAL] batch:  343 | acc: 43.75%,  total acc: 72.98%   [EVAL] batch:  344 | acc: 75.00%,  total acc: 72.99%   [EVAL] batch:  345 | acc: 43.75%,  total acc: 72.90%   [EVAL] batch:  346 | acc: 56.25%,  total acc: 72.86%   [EVAL] batch:  347 | acc: 68.75%,  total acc: 72.84%   [EVAL] batch:  348 | acc: 68.75%,  total acc: 72.83%   [EVAL] batch:  349 | acc: 68.75%,  total acc: 72.82%   [EVAL] batch:  350 | acc: 75.00%,  total acc: 72.83%   [EVAL] batch:  351 | acc: 87.50%,  total acc: 72.87%   [EVAL] batch:  352 | acc: 87.50%,  total acc: 72.91%   [EVAL] batch:  353 | acc: 93.75%,  total acc: 72.97%   [EVAL] batch:  354 | acc: 75.00%,  total acc: 72.98%   [EVAL] batch:  355 | acc: 93.75%,  total acc: 73.03%   [EVAL] batch:  356 | acc: 62.50%,  total acc: 73.00%   [EVAL] batch:  357 | acc: 37.50%,  total acc: 72.91%   [EVAL] batch:  358 | acc: 37.50%,  total acc: 72.81%   [EVAL] batch:  359 | acc: 62.50%,  total acc: 72.78%   [EVAL] batch:  360 | acc: 31.25%,  total acc: 72.66%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 72.60%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 72.61%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 72.68%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 72.76%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 72.83%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 72.91%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 72.98%   [EVAL] batch:  368 | acc: 93.75%,  total acc: 73.04%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 73.09%   [EVAL] batch:  370 | acc: 75.00%,  total acc: 73.10%   [EVAL] batch:  371 | acc: 81.25%,  total acc: 73.12%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 73.16%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 73.22%   
cur_acc:  ['0.9524', '0.7748', '0.8204', '0.7341', '0.8998', '0.7877']
his_acc:  ['0.9524', '0.8575', '0.8092', '0.7548', '0.7572', '0.7322']
CurrentTrain: epoch  0, batch     0 | loss: 6.2519364CurrentTrain: epoch  0, batch     1 | loss: 5.4617219CurrentTrain: epoch  0, batch     2 | loss: 6.3007040CurrentTrain: epoch  0, batch     3 | loss: 6.7687798CurrentTrain: epoch  1, batch     0 | loss: 4.2172470CurrentTrain: epoch  1, batch     1 | loss: 5.6361637CurrentTrain: epoch  1, batch     2 | loss: 4.3304949CurrentTrain: epoch  1, batch     3 | loss: 5.3115478CurrentTrain: epoch  2, batch     0 | loss: 4.4913683CurrentTrain: epoch  2, batch     1 | loss: 4.2234817CurrentTrain: epoch  2, batch     2 | loss: 3.8678257CurrentTrain: epoch  2, batch     3 | loss: 5.0399952CurrentTrain: epoch  3, batch     0 | loss: 4.1156764CurrentTrain: epoch  3, batch     1 | loss: 3.4434538CurrentTrain: epoch  3, batch     2 | loss: 3.6073213CurrentTrain: epoch  3, batch     3 | loss: 4.9882979CurrentTrain: epoch  4, batch     0 | loss: 3.9370472CurrentTrain: epoch  4, batch     1 | loss: 2.7848468CurrentTrain: epoch  4, batch     2 | loss: 3.5827100CurrentTrain: epoch  4, batch     3 | loss: 2.2456794CurrentTrain: epoch  5, batch     0 | loss: 3.0351629CurrentTrain: epoch  5, batch     1 | loss: 3.2367506CurrentTrain: epoch  5, batch     2 | loss: 2.9007449CurrentTrain: epoch  5, batch     3 | loss: 4.5610304CurrentTrain: epoch  6, batch     0 | loss: 2.9244847CurrentTrain: epoch  6, batch     1 | loss: 3.1503239CurrentTrain: epoch  6, batch     2 | loss: 2.6672096CurrentTrain: epoch  6, batch     3 | loss: 3.6552629CurrentTrain: epoch  7, batch     0 | loss: 3.0819755CurrentTrain: epoch  7, batch     1 | loss: 2.6537035CurrentTrain: epoch  7, batch     2 | loss: 2.6412039CurrentTrain: epoch  7, batch     3 | loss: 1.8588796CurrentTrain: epoch  8, batch     0 | loss: 2.6168847CurrentTrain: epoch  8, batch     1 | loss: 2.7817791CurrentTrain: epoch  8, batch     2 | loss: 2.3782561CurrentTrain: epoch  8, batch     3 | loss: 2.6008523CurrentTrain: epoch  9, batch     0 | loss: 2.4871211CurrentTrain: epoch  9, batch     1 | loss: 2.2855773CurrentTrain: epoch  9, batch     2 | loss: 2.4461699CurrentTrain: epoch  9, batch     3 | loss: 1.9538832
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the cantons of créteil are administrative divisions of the val - de - marne department , île - de - france region , northern france .
Head Entity: île - de - france
Tail Entity: val - de - marne
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The provinces of Canada are the primary administrative divisions of the country, with Ontario being one of the most populous provinces.  
Head Entity: Canada  
Tail Entity: Ontario  

Relation: contains administrative territorial entity  
Context: The states of Australia are significant divisions within the country, with New South Wales being the most populous state.  
Head Entity: Australia  
Tail Entity: New South Wales  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: this contrasts with the common usage of harem as an english loan - word , which implies a female - only enclave or seraglio .
Head Entity: harem
Tail Entity: seraglio
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: Some researchers argue that the term "artificial intelligence" is often used interchangeably with "machine learning," although they are not exactly the same.  
Head Entity: artificial intelligence  
Tail Entity: machine learning  

Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are frequently said to be the same as they share a common ancestor, yet their classifications differ in the scientific community.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: he considers his father to be biggest influence on his career as his brother javier castellano recipient of four eclipse award for outstanding jockey in the row ( 2013 , 2014,2015 and 2016 ) .
Head Entity: eclipse award for outstanding jockey
Tail Entity: javier castellano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: In the thrilling finale of the championship, Sarah Thompson emerged victorious, claiming the title of best player in the tournament, while her teammate, Mark Johnson, was awarded the runner-up position.  
Head Entity: best player in the tournament  
Tail Entity: Sarah Thompson  

Relation: winner  
Context: The annual science fair concluded with Emily Chen taking home the grand prize for her innovative project on renewable energy, while her classmate, David Lee, received an honorable mention for his work on robotics.  
Head Entity: grand prize  
Tail Entity: Emily Chen  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: air commodore arthur kellam tylee obe ( 24 april 1887 – 13 april 1961 ) was canadian officer who served in the royal flying corps during world war i.
Head Entity: arthur kellam tylee
Tail Entity: air commodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: general john smith was appointed as the chief of staff of the army in 2020, overseeing all military operations.  
Head Entity: john smith  
Tail Entity: general  

Relation: military rank  
Context: colonel jane doe led her battalion with distinction during the peacekeeping mission in the region.  
Head Entity: jane doe  
Tail Entity: colonel  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: he also worked on activision games like " gun " , " " and " the amazing spider - man " .
Head Entity: gun
Tail Entity: activision
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: The latest novel by J.K. Rowling was released by Bloomsbury Publishing, captivating readers worldwide.  
Head Entity: J.K. Rowling  
Tail Entity: Bloomsbury Publishing  

Relation: publisher  
Context: The popular science magazine was launched by National Geographic, providing insights into nature and exploration.  
Head Entity: science magazine  
Tail Entity: National Geographic  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: following this song were the # 21 " she wants to be wanted again " , the # 2 " loved too much " and the # 17 " i have to surrender " .
Head Entity: she wants to be wanted again
Tail Entity: loved too much
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The top three movies of the year were "Inception," followed by "The Social Network" and "Toy Story 3."  
Head Entity: Inception  
Tail Entity: The Social Network  

Relation: followed by  
Context: The conference schedule included a keynote speech, followed by a panel discussion and then a networking session.  
Head Entity: keynote speech  
Tail Entity: panel discussion  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: he was also associated with robert wilkinson in producing " londina illustrata " , an illustrated account of ancient buildings in london and westminster in two volumes ( 1819–25 ) .
Head Entity: robert wilkinson
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: The headquarters of the company is situated in Silicon Valley, where they have been developing innovative technology solutions for over a decade.  
Head Entity: the company  
Tail Entity: Silicon Valley  

Relation: work location  
Context: During her time at the university, she conducted groundbreaking research in marine biology while based in the coastal city of Miami.  
Head Entity: she  
Tail Entity: Miami  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: Maria Gonzalez is a renowned chef known for her innovative culinary techniques and delicious recipes.  
Head Entity: Maria Gonzalez  
Tail Entity: chef  

Relation: occupation  
Context: Dr. James Smith has dedicated his life to research in the field of astrophysics, contributing significantly to our understanding of black holes.  
Head Entity: Dr. James Smith  
Tail Entity: astrophysicist  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a World Heritage Site, attracting millions of tourists each year.  
Head Entity: ancient city of Petra  
Tail Entity: World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: ada adini ( 1855 – february 1924 ) was an american operatic soprano who had an active international career from 1876 up into the first decade of the 20th century .
Head Entity: ada adini
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti (1935 – 2007) was an italian operatic tenor who became one of the most celebrated and influential tenors of the 20th century.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey, born in 1969, is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: singer  
Mixup data size:  499
MixupTrain:  epoch  0, batch     0 | loss: 1.9293444MixupTrain:  epoch  0, batch     1 | loss: 2.1196117MixupTrain:  epoch  0, batch     2 | loss: 2.1312045MixupTrain:  epoch  0, batch     3 | loss: 1.9188830MixupTrain:  epoch  0, batch     4 | loss: 2.0888337MixupTrain:  epoch  0, batch     5 | loss: 1.9266501MixupTrain:  epoch  0, batch     6 | loss: 1.8428025MixupTrain:  epoch  0, batch     7 | loss: 2.1608112MixupTrain:  epoch  0, batch     8 | loss: 1.7466436MixupTrain:  epoch  0, batch     9 | loss: 2.1385730MixupTrain:  epoch  0, batch    10 | loss: 2.1758150MixupTrain:  epoch  0, batch    11 | loss: 1.5370774MixupTrain:  epoch  0, batch    12 | loss: 1.5814900MixupTrain:  epoch  0, batch    13 | loss: 1.4635031MixupTrain:  epoch  0, batch    14 | loss: 2.2831527MixupTrain:  epoch  0, batch    15 | loss: 1.7473431MixupTrain:  epoch  0, batch    16 | loss: 2.3447414MixupTrain:  epoch  0, batch    17 | loss: 1.9631012MixupTrain:  epoch  0, batch    18 | loss: 1.7966323MixupTrain:  epoch  0, batch    19 | loss: 1.8337868MixupTrain:  epoch  0, batch    20 | loss: 1.7410827MixupTrain:  epoch  0, batch    21 | loss: 1.7279135MixupTrain:  epoch  0, batch    22 | loss: 1.6751400MixupTrain:  epoch  0, batch    23 | loss: 1.7663623MixupTrain:  epoch  0, batch    24 | loss: 1.6182701MixupTrain:  epoch  0, batch    25 | loss: 1.7131311MixupTrain:  epoch  0, batch    26 | loss: 1.4812146MixupTrain:  epoch  0, batch    27 | loss: 1.6166260MixupTrain:  epoch  0, batch    28 | loss: 1.8130190MixupTrain:  epoch  0, batch    29 | loss: 1.6843922MixupTrain:  epoch  0, batch    30 | loss: 1.5741115MixupTrain:  epoch  0, batch    31 | loss: 1.7430449
MemoryTrain:  epoch  0, batch     0 | loss: 1.5405660MemoryTrain:  epoch  0, batch     1 | loss: 1.9342599MemoryTrain:  epoch  0, batch     2 | loss: 1.5183380MemoryTrain:  epoch  0, batch     3 | loss: 2.0322757MemoryTrain:  epoch  0, batch     4 | loss: 1.4495726MemoryTrain:  epoch  0, batch     5 | loss: 2.0029669MemoryTrain:  epoch  0, batch     6 | loss: 1.8238506MemoryTrain:  epoch  0, batch     7 | loss: 1.9010626MemoryTrain:  epoch  0, batch     8 | loss: 1.8003256MemoryTrain:  epoch  0, batch     9 | loss: 2.3224773MemoryTrain:  epoch  0, batch    10 | loss: 1.8793403MemoryTrain:  epoch  0, batch    11 | loss: 2.5943458MemoryTrain:  epoch  0, batch    12 | loss: 1.9094481MemoryTrain:  epoch  0, batch    13 | loss: 4.0520473MemoryTrain:  epoch  1, batch     0 | loss: 1.7589586MemoryTrain:  epoch  1, batch     1 | loss: 1.9414392MemoryTrain:  epoch  1, batch     2 | loss: 1.7622509MemoryTrain:  epoch  1, batch     3 | loss: 1.6752920MemoryTrain:  epoch  1, batch     4 | loss: 1.7522318MemoryTrain:  epoch  1, batch     5 | loss: 1.5001237MemoryTrain:  epoch  1, batch     6 | loss: 1.6618083MemoryTrain:  epoch  1, batch     7 | loss: 1.4779878MemoryTrain:  epoch  1, batch     8 | loss: 1.4771075MemoryTrain:  epoch  1, batch     9 | loss: 1.6631913MemoryTrain:  epoch  1, batch    10 | loss: 1.7053437MemoryTrain:  epoch  1, batch    11 | loss: 1.3867151MemoryTrain:  epoch  1, batch    12 | loss: 1.3866961MemoryTrain:  epoch  1, batch    13 | loss: 1.6618637MemoryTrain:  epoch  2, batch     0 | loss: 1.5026644MemoryTrain:  epoch  2, batch     1 | loss: 1.5197318MemoryTrain:  epoch  2, batch     2 | loss: 1.3851726MemoryTrain:  epoch  2, batch     3 | loss: 1.3342462MemoryTrain:  epoch  2, batch     4 | loss: 1.3337841MemoryTrain:  epoch  2, batch     5 | loss: 1.5532125MemoryTrain:  epoch  2, batch     6 | loss: 1.3798239MemoryTrain:  epoch  2, batch     7 | loss: 1.3885480MemoryTrain:  epoch  2, batch     8 | loss: 1.5862774MemoryTrain:  epoch  2, batch     9 | loss: 1.5152632MemoryTrain:  epoch  2, batch    10 | loss: 2.0184867MemoryTrain:  epoch  2, batch    11 | loss: 1.7033881MemoryTrain:  epoch  2, batch    12 | loss: 1.4637411MemoryTrain:  epoch  2, batch    13 | loss: 1.3344429MemoryTrain:  epoch  3, batch     0 | loss: 1.4892681MemoryTrain:  epoch  3, batch     1 | loss: 1.5710266MemoryTrain:  epoch  3, batch     2 | loss: 1.3146842MemoryTrain:  epoch  3, batch     3 | loss: 1.4166150MemoryTrain:  epoch  3, batch     4 | loss: 1.4010838MemoryTrain:  epoch  3, batch     5 | loss: 1.5237138MemoryTrain:  epoch  3, batch     6 | loss: 1.3430395MemoryTrain:  epoch  3, batch     7 | loss: 1.2948869MemoryTrain:  epoch  3, batch     8 | loss: 1.5892200MemoryTrain:  epoch  3, batch     9 | loss: 1.4322805MemoryTrain:  epoch  3, batch    10 | loss: 1.3876406MemoryTrain:  epoch  3, batch    11 | loss: 1.4242439MemoryTrain:  epoch  3, batch    12 | loss: 1.3244350MemoryTrain:  epoch  3, batch    13 | loss: 2.2693651MemoryTrain:  epoch  4, batch     0 | loss: 1.4809110MemoryTrain:  epoch  4, batch     1 | loss: 1.4197948MemoryTrain:  epoch  4, batch     2 | loss: 1.5903847MemoryTrain:  epoch  4, batch     3 | loss: 1.3961043MemoryTrain:  epoch  4, batch     4 | loss: 1.2855265MemoryTrain:  epoch  4, batch     5 | loss: 1.2666677MemoryTrain:  epoch  4, batch     6 | loss: 1.2207282MemoryTrain:  epoch  4, batch     7 | loss: 1.4686551MemoryTrain:  epoch  4, batch     8 | loss: 1.2867911MemoryTrain:  epoch  4, batch     9 | loss: 1.3544011MemoryTrain:  epoch  4, batch    10 | loss: 1.3015293MemoryTrain:  epoch  4, batch    11 | loss: 1.5309627MemoryTrain:  epoch  4, batch    12 | loss: 1.2488722MemoryTrain:  epoch  4, batch    13 | loss: 1.1584840MemoryTrain:  epoch  5, batch     0 | loss: 1.2597225MemoryTrain:  epoch  5, batch     1 | loss: 1.3645381MemoryTrain:  epoch  5, batch     2 | loss: 1.4246148MemoryTrain:  epoch  5, batch     3 | loss: 1.3691230MemoryTrain:  epoch  5, batch     4 | loss: 1.4233767MemoryTrain:  epoch  5, batch     5 | loss: 1.2949054MemoryTrain:  epoch  5, batch     6 | loss: 1.3429384MemoryTrain:  epoch  5, batch     7 | loss: 1.2811363MemoryTrain:  epoch  5, batch     8 | loss: 1.4577484MemoryTrain:  epoch  5, batch     9 | loss: 1.3257148MemoryTrain:  epoch  5, batch    10 | loss: 1.2614484MemoryTrain:  epoch  5, batch    11 | loss: 1.3117988MemoryTrain:  epoch  5, batch    12 | loss: 1.2283499MemoryTrain:  epoch  5, batch    13 | loss: 1.3991885MemoryTrain:  epoch  6, batch     0 | loss: 1.2644416MemoryTrain:  epoch  6, batch     1 | loss: 1.2870203MemoryTrain:  epoch  6, batch     2 | loss: 1.3157737MemoryTrain:  epoch  6, batch     3 | loss: 1.3069857MemoryTrain:  epoch  6, batch     4 | loss: 1.2576802MemoryTrain:  epoch  6, batch     5 | loss: 1.3005338MemoryTrain:  epoch  6, batch     6 | loss: 1.3065592MemoryTrain:  epoch  6, batch     7 | loss: 1.3590380MemoryTrain:  epoch  6, batch     8 | loss: 1.3469439MemoryTrain:  epoch  6, batch     9 | loss: 1.2574391MemoryTrain:  epoch  6, batch    10 | loss: 1.4555287MemoryTrain:  epoch  6, batch    11 | loss: 1.3866451MemoryTrain:  epoch  6, batch    12 | loss: 1.2357427MemoryTrain:  epoch  6, batch    13 | loss: 1.2344961MemoryTrain:  epoch  7, batch     0 | loss: 1.2078848MemoryTrain:  epoch  7, batch     1 | loss: 1.3040098MemoryTrain:  epoch  7, batch     2 | loss: 1.3377161MemoryTrain:  epoch  7, batch     3 | loss: 1.2773827MemoryTrain:  epoch  7, batch     4 | loss: 1.3424430MemoryTrain:  epoch  7, batch     5 | loss: 1.3600571MemoryTrain:  epoch  7, batch     6 | loss: 1.3663343MemoryTrain:  epoch  7, batch     7 | loss: 1.2613548MemoryTrain:  epoch  7, batch     8 | loss: 1.3334123MemoryTrain:  epoch  7, batch     9 | loss: 1.2746462MemoryTrain:  epoch  7, batch    10 | loss: 1.2652857MemoryTrain:  epoch  7, batch    11 | loss: 1.2948656MemoryTrain:  epoch  7, batch    12 | loss: 1.2366509MemoryTrain:  epoch  7, batch    13 | loss: 1.3998384MemoryTrain:  epoch  8, batch     0 | loss: 1.2410300MemoryTrain:  epoch  8, batch     1 | loss: 1.2966368MemoryTrain:  epoch  8, batch     2 | loss: 1.2488484MemoryTrain:  epoch  8, batch     3 | loss: 1.2034402MemoryTrain:  epoch  8, batch     4 | loss: 1.2599142MemoryTrain:  epoch  8, batch     5 | loss: 1.2534699MemoryTrain:  epoch  8, batch     6 | loss: 1.2774162MemoryTrain:  epoch  8, batch     7 | loss: 1.2668310MemoryTrain:  epoch  8, batch     8 | loss: 1.3907125MemoryTrain:  epoch  8, batch     9 | loss: 1.2535346MemoryTrain:  epoch  8, batch    10 | loss: 1.2263899MemoryTrain:  epoch  8, batch    11 | loss: 1.3513656MemoryTrain:  epoch  8, batch    12 | loss: 1.3340439MemoryTrain:  epoch  8, batch    13 | loss: 1.3161250MemoryTrain:  epoch  9, batch     0 | loss: 1.2675292MemoryTrain:  epoch  9, batch     1 | loss: 1.2834749MemoryTrain:  epoch  9, batch     2 | loss: 1.2775810MemoryTrain:  epoch  9, batch     3 | loss: 1.2478597MemoryTrain:  epoch  9, batch     4 | loss: 1.2285429MemoryTrain:  epoch  9, batch     5 | loss: 1.2175424MemoryTrain:  epoch  9, batch     6 | loss: 1.2599094MemoryTrain:  epoch  9, batch     7 | loss: 1.2330804MemoryTrain:  epoch  9, batch     8 | loss: 1.3025922MemoryTrain:  epoch  9, batch     9 | loss: 1.2705662MemoryTrain:  epoch  9, batch    10 | loss: 1.2444109MemoryTrain:  epoch  9, batch    11 | loss: 1.2890940MemoryTrain:  epoch  9, batch    12 | loss: 1.3084358MemoryTrain:  epoch  9, batch    13 | loss: 1.2813715
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 66.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 68.18%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 68.23%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 70.19%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 69.64%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 69.17%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 69.14%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 69.49%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 70.49%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 71.71%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 72.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 73.81%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 74.72%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 75.82%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 76.56%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 77.50%   [EVAL] batch:   25 | acc: 62.50%,  total acc: 76.92%   [EVAL] batch:   26 | acc: 43.75%,  total acc: 75.69%   [EVAL] batch:   27 | acc: 75.00%,  total acc: 75.67%   [EVAL] batch:   28 | acc: 68.75%,  total acc: 75.43%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 75.42%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 75.60%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 75.98%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 75.95%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 75.55%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 75.36%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 74.83%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 74.32%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 74.67%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 75.31%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 75.46%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 75.60%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 75.58%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 75.85%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 75.69%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 75.54%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 75.27%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 75.13%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 74.87%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 75.49%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 75.96%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 76.42%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 76.85%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 77.27%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 78.07%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 78.34%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 78.71%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 79.06%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 79.41%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 79.74%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 79.27%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 69.64%   [EVAL] batch:    7 | acc: 25.00%,  total acc: 64.06%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 63.89%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 61.25%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 61.36%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 59.90%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 60.10%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 64.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 66.80%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 68.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 71.38%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 70.31%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 70.54%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 70.17%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 69.29%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 70.05%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 70.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 73.66%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 74.35%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 75.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 76.01%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 76.76%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 77.46%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 77.21%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 77.14%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 77.43%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 77.70%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 78.69%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.22%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 79.57%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 79.76%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 80.09%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 80.26%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 79.86%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 79.62%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 78.99%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 78.39%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 78.06%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 77.50%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 77.45%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 77.52%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 77.48%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 77.20%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 76.70%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 76.79%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 76.32%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 75.32%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 74.58%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 74.17%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 73.46%   [EVAL] batch:   61 | acc: 12.50%,  total acc: 72.48%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 72.22%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 71.88%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 71.35%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 71.40%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 71.18%   [EVAL] batch:   67 | acc: 62.50%,  total acc: 71.05%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 71.01%   [EVAL] batch:   69 | acc: 25.00%,  total acc: 70.36%   [EVAL] batch:   70 | acc: 18.75%,  total acc: 69.63%   [EVAL] batch:   71 | acc: 12.50%,  total acc: 68.84%   [EVAL] batch:   72 | acc: 12.50%,  total acc: 68.07%   [EVAL] batch:   73 | acc: 31.25%,  total acc: 67.57%   [EVAL] batch:   74 | acc: 12.50%,  total acc: 66.83%   [EVAL] batch:   75 | acc: 0.00%,  total acc: 65.95%   [EVAL] batch:   76 | acc: 0.00%,  total acc: 65.10%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 64.58%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 63.77%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 62.97%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 62.27%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 62.35%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 62.80%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 63.24%   [EVAL] batch:   84 | acc: 100.00%,  total acc: 63.68%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 64.10%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 64.44%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 64.49%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 63.76%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 63.40%   [EVAL] batch:   90 | acc: 31.25%,  total acc: 63.05%   [EVAL] batch:   91 | acc: 37.50%,  total acc: 62.77%   [EVAL] batch:   92 | acc: 12.50%,  total acc: 62.23%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 61.90%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 62.30%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 62.63%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 62.89%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 63.14%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 63.51%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 63.88%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 63.74%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 63.66%   [EVAL] batch:  102 | acc: 37.50%,  total acc: 63.41%   [EVAL] batch:  103 | acc: 37.50%,  total acc: 63.16%   [EVAL] batch:  104 | acc: 50.00%,  total acc: 63.04%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 62.97%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 63.03%   [EVAL] batch:  107 | acc: 68.75%,  total acc: 63.08%   [EVAL] batch:  108 | acc: 68.75%,  total acc: 63.13%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 63.07%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 63.01%   [EVAL] batch:  111 | acc: 56.25%,  total acc: 62.95%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 63.05%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 62.94%   [EVAL] batch:  114 | acc: 50.00%,  total acc: 62.83%   [EVAL] batch:  115 | acc: 56.25%,  total acc: 62.77%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 62.77%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 62.76%   [EVAL] batch:  118 | acc: 43.75%,  total acc: 62.61%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 62.86%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 63.12%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 63.32%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 63.47%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 63.66%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 63.95%   [EVAL] batch:  125 | acc: 18.75%,  total acc: 63.59%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 63.29%   [EVAL] batch:  127 | acc: 0.00%,  total acc: 62.79%   [EVAL] batch:  128 | acc: 12.50%,  total acc: 62.40%   [EVAL] batch:  129 | acc: 0.00%,  total acc: 61.92%   [EVAL] batch:  130 | acc: 12.50%,  total acc: 61.55%   [EVAL] batch:  131 | acc: 50.00%,  total acc: 61.46%   [EVAL] batch:  132 | acc: 68.75%,  total acc: 61.51%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 61.61%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 61.76%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 61.99%   [EVAL] batch:  136 | acc: 62.50%,  total acc: 62.00%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 61.91%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 61.65%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 61.43%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 61.21%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 60.96%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 60.80%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 60.63%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 60.86%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 61.13%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 61.35%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 61.61%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 61.87%   [EVAL] batch:  149 | acc: 87.50%,  total acc: 62.04%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 62.25%   [EVAL] batch:  151 | acc: 87.50%,  total acc: 62.42%   [EVAL] batch:  152 | acc: 87.50%,  total acc: 62.58%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 62.74%   [EVAL] batch:  154 | acc: 81.25%,  total acc: 62.86%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 63.02%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 63.22%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 63.45%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 63.64%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 63.83%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 64.01%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 64.20%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 64.15%   [EVAL] batch:  163 | acc: 25.00%,  total acc: 63.91%   [EVAL] batch:  164 | acc: 12.50%,  total acc: 63.60%   [EVAL] batch:  165 | acc: 25.00%,  total acc: 63.37%   [EVAL] batch:  166 | acc: 12.50%,  total acc: 63.06%   [EVAL] batch:  167 | acc: 12.50%,  total acc: 62.76%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 62.76%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 62.83%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 62.94%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 63.01%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 63.04%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 62.97%   [EVAL] batch:  174 | acc: 68.75%,  total acc: 63.00%   [EVAL] batch:  175 | acc: 43.75%,  total acc: 62.89%   [EVAL] batch:  176 | acc: 75.00%,  total acc: 62.96%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 62.96%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 62.99%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 63.02%   [EVAL] batch:  180 | acc: 31.25%,  total acc: 62.85%   [EVAL] batch:  181 | acc: 37.50%,  total acc: 62.71%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 62.74%   [EVAL] batch:  183 | acc: 50.00%,  total acc: 62.67%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 62.74%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 62.90%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 62.97%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 62.90%   [EVAL] batch:  188 | acc: 50.00%,  total acc: 62.83%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 62.86%   [EVAL] batch:  190 | acc: 62.50%,  total acc: 62.86%   [EVAL] batch:  191 | acc: 62.50%,  total acc: 62.86%   [EVAL] batch:  192 | acc: 56.25%,  total acc: 62.82%   [EVAL] batch:  193 | acc: 68.75%,  total acc: 62.85%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 62.98%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 63.04%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 63.07%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 63.16%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 63.19%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 63.25%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 63.31%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 63.40%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 63.39%   [EVAL] batch:  203 | acc: 75.00%,  total acc: 63.45%   [EVAL] batch:  204 | acc: 87.50%,  total acc: 63.57%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 63.71%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 63.59%   [EVAL] batch:  207 | acc: 31.25%,  total acc: 63.43%   [EVAL] batch:  208 | acc: 43.75%,  total acc: 63.34%   [EVAL] batch:  209 | acc: 37.50%,  total acc: 63.21%   [EVAL] batch:  210 | acc: 62.50%,  total acc: 63.21%   [EVAL] batch:  211 | acc: 43.75%,  total acc: 63.12%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 63.06%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 63.20%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 63.37%   [EVAL] batch:  215 | acc: 68.75%,  total acc: 63.40%   [EVAL] batch:  216 | acc: 93.75%,  total acc: 63.54%   [EVAL] batch:  217 | acc: 81.25%,  total acc: 63.62%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 63.76%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 63.92%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 64.08%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 64.25%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 64.41%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 64.56%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 64.72%   [EVAL] batch:  225 | acc: 93.75%,  total acc: 64.85%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 64.95%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 64.97%   [EVAL] batch:  228 | acc: 87.50%,  total acc: 65.07%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 65.19%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 65.26%   [EVAL] batch:  231 | acc: 50.00%,  total acc: 65.19%   [EVAL] batch:  232 | acc: 37.50%,  total acc: 65.08%   [EVAL] batch:  233 | acc: 43.75%,  total acc: 64.98%   [EVAL] batch:  234 | acc: 68.75%,  total acc: 65.00%   [EVAL] batch:  235 | acc: 68.75%,  total acc: 65.02%   [EVAL] batch:  236 | acc: 50.00%,  total acc: 64.95%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 65.05%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 65.14%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 65.29%   [EVAL] batch:  240 | acc: 68.75%,  total acc: 65.30%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 65.39%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 65.48%   [EVAL] batch:  243 | acc: 75.00%,  total acc: 65.52%   [EVAL] batch:  244 | acc: 18.75%,  total acc: 65.33%   [EVAL] batch:  245 | acc: 12.50%,  total acc: 65.12%   [EVAL] batch:  246 | acc: 6.25%,  total acc: 64.88%   [EVAL] batch:  247 | acc: 18.75%,  total acc: 64.69%   [EVAL] batch:  248 | acc: 18.75%,  total acc: 64.51%   [EVAL] batch:  249 | acc: 18.75%,  total acc: 64.33%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 64.44%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 64.56%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 64.65%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 64.74%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 64.85%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 64.99%   [EVAL] batch:  256 | acc: 87.50%,  total acc: 65.08%   [EVAL] batch:  257 | acc: 100.00%,  total acc: 65.21%   [EVAL] batch:  258 | acc: 81.25%,  total acc: 65.28%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 65.38%   [EVAL] batch:  260 | acc: 81.25%,  total acc: 65.45%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 65.51%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 65.61%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 65.67%   [EVAL] batch:  264 | acc: 93.75%,  total acc: 65.78%   [EVAL] batch:  265 | acc: 87.50%,  total acc: 65.86%   [EVAL] batch:  266 | acc: 87.50%,  total acc: 65.94%   [EVAL] batch:  267 | acc: 87.50%,  total acc: 66.02%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 66.08%   [EVAL] batch:  269 | acc: 56.25%,  total acc: 66.04%   [EVAL] batch:  270 | acc: 68.75%,  total acc: 66.05%   [EVAL] batch:  271 | acc: 75.00%,  total acc: 66.08%   [EVAL] batch:  272 | acc: 31.25%,  total acc: 65.96%   [EVAL] batch:  273 | acc: 81.25%,  total acc: 66.01%   [EVAL] batch:  274 | acc: 50.00%,  total acc: 65.95%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 66.08%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 66.20%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 66.32%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 66.44%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 66.56%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 66.68%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:  282 | acc: 81.25%,  total acc: 66.72%   [EVAL] batch:  283 | acc: 68.75%,  total acc: 66.73%   [EVAL] batch:  284 | acc: 75.00%,  total acc: 66.75%   [EVAL] batch:  285 | acc: 62.50%,  total acc: 66.74%   [EVAL] batch:  286 | acc: 56.25%,  total acc: 66.70%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 66.71%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 66.83%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 66.94%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 67.05%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 67.17%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 67.28%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 67.37%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 67.48%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 67.59%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 67.70%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 67.81%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 67.91%   [EVAL] batch:  299 | acc: 93.75%,  total acc: 68.00%   [EVAL] batch:  300 | acc: 87.50%,  total acc: 68.06%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 68.15%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 68.23%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 68.32%   [EVAL] batch:  304 | acc: 87.50%,  total acc: 68.38%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 68.44%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 68.53%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 68.59%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 68.65%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 68.73%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 68.81%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 68.85%   [EVAL] batch:  312 | acc: 93.75%,  total acc: 68.93%   [EVAL] batch:  313 | acc: 75.00%,  total acc: 68.95%   [EVAL] batch:  314 | acc: 93.75%,  total acc: 69.03%   [EVAL] batch:  315 | acc: 75.00%,  total acc: 69.05%   [EVAL] batch:  316 | acc: 81.25%,  total acc: 69.09%   [EVAL] batch:  317 | acc: 81.25%,  total acc: 69.12%   [EVAL] batch:  318 | acc: 75.00%,  total acc: 69.14%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 69.24%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 69.33%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 69.35%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 69.41%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 69.46%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 69.50%   [EVAL] batch:  325 | acc: 62.50%,  total acc: 69.48%   [EVAL] batch:  326 | acc: 37.50%,  total acc: 69.38%   [EVAL] batch:  327 | acc: 56.25%,  total acc: 69.34%   [EVAL] batch:  328 | acc: 68.75%,  total acc: 69.34%   [EVAL] batch:  329 | acc: 75.00%,  total acc: 69.36%   [EVAL] batch:  330 | acc: 56.25%,  total acc: 69.32%   [EVAL] batch:  331 | acc: 81.25%,  total acc: 69.35%   [EVAL] batch:  332 | acc: 81.25%,  total acc: 69.39%   [EVAL] batch:  333 | acc: 93.75%,  total acc: 69.46%   [EVAL] batch:  334 | acc: 81.25%,  total acc: 69.50%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 69.59%   [EVAL] batch:  336 | acc: 87.50%,  total acc: 69.64%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 69.71%   [EVAL] batch:  338 | acc: 81.25%,  total acc: 69.75%   [EVAL] batch:  339 | acc: 81.25%,  total acc: 69.78%   [EVAL] batch:  340 | acc: 93.75%,  total acc: 69.85%   [EVAL] batch:  341 | acc: 93.75%,  total acc: 69.92%   [EVAL] batch:  342 | acc: 62.50%,  total acc: 69.90%   [EVAL] batch:  343 | acc: 43.75%,  total acc: 69.82%   [EVAL] batch:  344 | acc: 75.00%,  total acc: 69.84%   [EVAL] batch:  345 | acc: 43.75%,  total acc: 69.76%   [EVAL] batch:  346 | acc: 62.50%,  total acc: 69.74%   [EVAL] batch:  347 | acc: 62.50%,  total acc: 69.72%   [EVAL] batch:  348 | acc: 62.50%,  total acc: 69.70%   [EVAL] batch:  349 | acc: 68.75%,  total acc: 69.70%   [EVAL] batch:  350 | acc: 81.25%,  total acc: 69.73%   [EVAL] batch:  351 | acc: 87.50%,  total acc: 69.78%   [EVAL] batch:  352 | acc: 93.75%,  total acc: 69.85%   [EVAL] batch:  353 | acc: 81.25%,  total acc: 69.88%   [EVAL] batch:  354 | acc: 81.25%,  total acc: 69.91%   [EVAL] batch:  355 | acc: 93.75%,  total acc: 69.98%   [EVAL] batch:  356 | acc: 68.75%,  total acc: 69.98%   [EVAL] batch:  357 | acc: 50.00%,  total acc: 69.92%   [EVAL] batch:  358 | acc: 43.75%,  total acc: 69.85%   [EVAL] batch:  359 | acc: 62.50%,  total acc: 69.83%   [EVAL] batch:  360 | acc: 31.25%,  total acc: 69.72%   [EVAL] batch:  361 | acc: 62.50%,  total acc: 69.70%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 69.70%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 69.78%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 69.86%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 69.95%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 70.03%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 70.11%   [EVAL] batch:  368 | acc: 93.75%,  total acc: 70.17%   [EVAL] batch:  369 | acc: 81.25%,  total acc: 70.20%   [EVAL] batch:  370 | acc: 62.50%,  total acc: 70.18%   [EVAL] batch:  371 | acc: 81.25%,  total acc: 70.21%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 70.24%   [EVAL] batch:  373 | acc: 75.00%,  total acc: 70.25%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 70.27%   [EVAL] batch:  375 | acc: 50.00%,  total acc: 70.21%   [EVAL] batch:  376 | acc: 68.75%,  total acc: 70.21%   [EVAL] batch:  377 | acc: 75.00%,  total acc: 70.22%   [EVAL] batch:  378 | acc: 68.75%,  total acc: 70.22%   [EVAL] batch:  379 | acc: 68.75%,  total acc: 70.21%   [EVAL] batch:  380 | acc: 81.25%,  total acc: 70.24%   [EVAL] batch:  381 | acc: 68.75%,  total acc: 70.24%   [EVAL] batch:  382 | acc: 87.50%,  total acc: 70.28%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 70.28%   [EVAL] batch:  384 | acc: 68.75%,  total acc: 70.28%   [EVAL] batch:  385 | acc: 43.75%,  total acc: 70.21%   [EVAL] batch:  386 | acc: 68.75%,  total acc: 70.20%   [EVAL] batch:  387 | acc: 93.75%,  total acc: 70.26%   [EVAL] batch:  388 | acc: 62.50%,  total acc: 70.24%   [EVAL] batch:  389 | acc: 62.50%,  total acc: 70.22%   [EVAL] batch:  390 | acc: 68.75%,  total acc: 70.22%   [EVAL] batch:  391 | acc: 75.00%,  total acc: 70.23%   [EVAL] batch:  392 | acc: 87.50%,  total acc: 70.28%   [EVAL] batch:  393 | acc: 93.75%,  total acc: 70.34%   [EVAL] batch:  394 | acc: 87.50%,  total acc: 70.38%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 70.45%   [EVAL] batch:  396 | acc: 93.75%,  total acc: 70.51%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 70.59%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 70.65%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 70.72%   [EVAL] batch:  400 | acc: 62.50%,  total acc: 70.70%   [EVAL] batch:  401 | acc: 43.75%,  total acc: 70.63%   [EVAL] batch:  402 | acc: 75.00%,  total acc: 70.64%   [EVAL] batch:  403 | acc: 68.75%,  total acc: 70.64%   [EVAL] batch:  404 | acc: 75.00%,  total acc: 70.65%   [EVAL] batch:  405 | acc: 81.25%,  total acc: 70.67%   [EVAL] batch:  406 | acc: 87.50%,  total acc: 70.72%   [EVAL] batch:  407 | acc: 75.00%,  total acc: 70.73%   [EVAL] batch:  408 | acc: 62.50%,  total acc: 70.71%   [EVAL] batch:  409 | acc: 68.75%,  total acc: 70.70%   [EVAL] batch:  410 | acc: 56.25%,  total acc: 70.67%   [EVAL] batch:  411 | acc: 56.25%,  total acc: 70.63%   [EVAL] batch:  412 | acc: 87.50%,  total acc: 70.67%   [EVAL] batch:  413 | acc: 87.50%,  total acc: 70.71%   [EVAL] batch:  414 | acc: 87.50%,  total acc: 70.75%   [EVAL] batch:  415 | acc: 81.25%,  total acc: 70.78%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 70.80%   [EVAL] batch:  417 | acc: 75.00%,  total acc: 70.81%   [EVAL] batch:  418 | acc: 87.50%,  total acc: 70.85%   [EVAL] batch:  419 | acc: 68.75%,  total acc: 70.85%   [EVAL] batch:  420 | acc: 68.75%,  total acc: 70.84%   [EVAL] batch:  421 | acc: 62.50%,  total acc: 70.82%   [EVAL] batch:  422 | acc: 68.75%,  total acc: 70.82%   [EVAL] batch:  423 | acc: 62.50%,  total acc: 70.80%   [EVAL] batch:  424 | acc: 81.25%,  total acc: 70.82%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 70.89%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 70.96%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 71.03%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 71.10%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 71.16%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 71.23%   [EVAL] batch:  431 | acc: 100.00%,  total acc: 71.30%   [EVAL] batch:  432 | acc: 93.75%,  total acc: 71.35%   [EVAL] batch:  433 | acc: 100.00%,  total acc: 71.41%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 71.48%   [EVAL] batch:  435 | acc: 100.00%,  total acc: 71.55%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 71.61%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 71.56%   
cur_acc:  ['0.9524', '0.7748', '0.8204', '0.7341', '0.8998', '0.7877', '0.7927']
his_acc:  ['0.9524', '0.8575', '0.8092', '0.7548', '0.7572', '0.7322', '0.7156']
CurrentTrain: epoch  0, batch     0 | loss: 6.4367185CurrentTrain: epoch  0, batch     1 | loss: 6.5174990CurrentTrain: epoch  0, batch     2 | loss: 6.5903311CurrentTrain: epoch  0, batch     3 | loss: 5.2869854CurrentTrain: epoch  1, batch     0 | loss: 6.3835292CurrentTrain: epoch  1, batch     1 | loss: 5.7012744CurrentTrain: epoch  1, batch     2 | loss: 4.5582905CurrentTrain: epoch  1, batch     3 | loss: 3.6529880CurrentTrain: epoch  2, batch     0 | loss: 5.3626022CurrentTrain: epoch  2, batch     1 | loss: 4.1518731CurrentTrain: epoch  2, batch     2 | loss: 4.8169670CurrentTrain: epoch  2, batch     3 | loss: 6.6817679CurrentTrain: epoch  3, batch     0 | loss: 4.2152691CurrentTrain: epoch  3, batch     1 | loss: 4.0847926CurrentTrain: epoch  3, batch     2 | loss: 4.8235817CurrentTrain: epoch  3, batch     3 | loss: 7.8763433CurrentTrain: epoch  4, batch     0 | loss: 3.6937921CurrentTrain: epoch  4, batch     1 | loss: 4.8169098CurrentTrain: epoch  4, batch     2 | loss: 3.4600215CurrentTrain: epoch  4, batch     3 | loss: 2.8116367CurrentTrain: epoch  5, batch     0 | loss: 3.5460300CurrentTrain: epoch  5, batch     1 | loss: 3.7859888CurrentTrain: epoch  5, batch     2 | loss: 3.9654014CurrentTrain: epoch  5, batch     3 | loss: 2.5953422CurrentTrain: epoch  6, batch     0 | loss: 3.8193235CurrentTrain: epoch  6, batch     1 | loss: 3.0167780CurrentTrain: epoch  6, batch     2 | loss: 3.5808790CurrentTrain: epoch  6, batch     3 | loss: 2.5856452CurrentTrain: epoch  7, batch     0 | loss: 2.9349341CurrentTrain: epoch  7, batch     1 | loss: 3.8194647CurrentTrain: epoch  7, batch     2 | loss: 3.2582543CurrentTrain: epoch  7, batch     3 | loss: 1.9002993CurrentTrain: epoch  8, batch     0 | loss: 2.7292237CurrentTrain: epoch  8, batch     1 | loss: 2.8970604CurrentTrain: epoch  8, batch     2 | loss: 3.3794494CurrentTrain: epoch  8, batch     3 | loss: 1.9871788CurrentTrain: epoch  9, batch     0 | loss: 3.0608819CurrentTrain: epoch  9, batch     1 | loss: 2.4905071CurrentTrain: epoch  9, batch     2 | loss: 2.7547510CurrentTrain: epoch  9, batch     3 | loss: 1.8356910
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: the andesite line , a zone of intense volcanic and seismic activity , is a major regional distinction in the pacific .
Head Entity: andesite line
Tail Entity: pacific
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The Great Barrier Reef, a stunning natural wonder, stretches along the northeastern coast of Australia.  
Head Entity: Great Barrier Reef  
Tail Entity: Australia  

Relation: located on terrain feature  
Context: Mount Everest, the highest peak in the world, is part of the Himalayas and attracts climbers from around the globe.  
Head Entity: Mount Everest  
Tail Entity: Himalayas  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, held in paris in 2015, saw numerous world leaders and organizations come together to discuss global warming and its impacts.  
Head Entity: united nations climate change conference  
Tail Entity: paris agreement  

Relation: participant of  
Context: the 2022 fifa world cup in qatar featured teams from around the globe competing for the prestigious trophy, with many fans attending the matches.  
Head Entity: 2022 fifa world cup  
Tail Entity: qatar
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: luna subsequently had a supporting role in the comedy film " gravy " , directed by james roday , which was released in october 2015 .
Head Entity: gravy
Tail Entity: james roday
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: the acclaimed drama series "the crown" was brought to life by the visionary director peter morgan, who skillfully navigated the complexities of the royal family.  
Head Entity: the crown  
Tail Entity: peter morgan  

Relation: director  
Context: in the thrilling action movie "inception," the brilliant director christopher nolan crafted a mind-bending narrative that captivated audiences worldwide.  
Head Entity: inception  
Tail Entity: christopher nolan  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: the station is owned by the hearst television subsidiary of the hearst corporation .
Head Entity: hearst television
Tail Entity: hearst corporation
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: the famous painting is owned by the metropolitan museum of art in new york.  
Head Entity: metropolitan museum of art  
Tail Entity: famous painting  

Relation: owned by  
Context: the luxury car brand is owned by a major automotive conglomerate based in germany.  
Head Entity: major automotive conglomerate  
Tail Entity: luxury car brand  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: remains of taxa that can be confidently assigned to istiodactylidae have been found in the uk and china , in rocks dating from the early cretaceous period ( barremian to aptian stage ) .
Head Entity: barremian
Tail Entity: early cretaceous
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The Great Wall of China is a series of fortifications made of various materials, and it is part of the UNESCO World Heritage Sites.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Sites  

Relation: part of  
Context: The human heart is a vital organ that is part of the circulatory system, responsible for pumping blood throughout the body.  
Head Entity: human heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: the brewery closed in 1996 and is now the site of the halliwell jones stadium , home of warrington wolves , as well as a tesco store .
Head Entity: halliwell jones stadium
Tail Entity: warrington wolves
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was transformed into a cultural center, which now hosts the local art gallery and serves as the headquarters for the community theater group.  
Head Entity: cultural center  
Tail Entity: community theater group  

Relation: occupant  
Context: After the renovation, the old factory became a vibrant co-working space, attracting freelancers and startups from the tech industry.  
Head Entity: co-working space  
Tail Entity: tech industry
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: the art nouveau style was brought to italy by figures such as giuseppe sommaruga and ernesto basile ( who respectively designed the palazzo castiglioni and expanded the palazzo montecitorio in rome ) .
Head Entity: palazzo castiglioni
Tail Entity: giuseppe sommaruga
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: The iconic Sydney Opera House was designed by Danish architect Jørn Utzon, whose innovative approach transformed the landscape of modern architecture.  
Head Entity: Sydney Opera House  
Tail Entity: Jørn Utzon  

Relation: architect  
Context: The Guggenheim Museum in Bilbao, a masterpiece of contemporary architecture, was created by the renowned architect Frank Gehry, showcasing his unique deconstructivist style.  
Head Entity: Guggenheim Museum  
Tail Entity: Frank Gehry  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: cyrus alexander was born in pennsylvania , and his family soon moved to illinois .
Head Entity: cyrus alexander
Tail Entity: illinois
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: after living in new york for several years, emily decided to relocate to california for a fresh start.  
Head Entity: emily  
Tail Entity: california  

Relation: residence  
Context: during his childhood, michael spent most of his time in texas before his family moved to florida.  
Head Entity: michael  
Tail Entity: florida  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Ludwig van Beethoven created some of the most celebrated symphonies in classical music history.  
Head Entity: symphonies  
Tail Entity: Ludwig van Beethoven  

Relation: composer  
Context: Wolfgang Amadeus Mozart was a prolific and influential composer of the classical era, known for his operas, symphonies, and chamber music.  
Head Entity: operas  
Tail Entity: Wolfgang Amadeus Mozart  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: one polish archer qualified for the women 's individual recurve by obtaining one of the eight olympic places available from the 2015 world archery championships in copenhagen , denmark .
Head Entity: 2015 world archery championships
Tail Entity: copenhagen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: The annual music festival will take place in the beautiful city of Barcelona, Spain, attracting thousands of visitors from around the world.  
Head Entity: annual music festival  
Tail Entity: Barcelona  

Relation: location  
Context: The historic battle was fought near the town of Gettysburg, Pennsylvania, which is now a popular tourist destination.  
Head Entity: historic battle  
Tail Entity: Gettysburg  
Mixup data size:  559
MixupTrain:  epoch  0, batch     0 | loss: 2.0377039MixupTrain:  epoch  0, batch     1 | loss: 2.0721572MixupTrain:  epoch  0, batch     2 | loss: 2.1402539MixupTrain:  epoch  0, batch     3 | loss: 1.8810028MixupTrain:  epoch  0, batch     4 | loss: 2.9293207MixupTrain:  epoch  0, batch     5 | loss: 2.2074436MixupTrain:  epoch  0, batch     6 | loss: 2.1530608MixupTrain:  epoch  0, batch     7 | loss: 2.5007412MixupTrain:  epoch  0, batch     8 | loss: 2.2152385MixupTrain:  epoch  0, batch     9 | loss: 2.1227793MixupTrain:  epoch  0, batch    10 | loss: 1.8460603MixupTrain:  epoch  0, batch    11 | loss: 2.1304531MixupTrain:  epoch  0, batch    12 | loss: 1.8497148MixupTrain:  epoch  0, batch    13 | loss: 2.4850657MixupTrain:  epoch  0, batch    14 | loss: 2.2020327MixupTrain:  epoch  0, batch    15 | loss: 2.2815258MixupTrain:  epoch  0, batch    16 | loss: 1.7731829MixupTrain:  epoch  0, batch    17 | loss: 2.6286264MixupTrain:  epoch  0, batch    18 | loss: 1.7317256MixupTrain:  epoch  0, batch    19 | loss: 1.8950234MixupTrain:  epoch  0, batch    20 | loss: 1.9818617MixupTrain:  epoch  0, batch    21 | loss: 1.8896599MixupTrain:  epoch  0, batch    22 | loss: 2.2177950MixupTrain:  epoch  0, batch    23 | loss: 1.9832611MixupTrain:  epoch  0, batch    24 | loss: 1.7270903MixupTrain:  epoch  0, batch    25 | loss: 2.7706196MixupTrain:  epoch  0, batch    26 | loss: 2.6070273MixupTrain:  epoch  0, batch    27 | loss: 2.1701162MixupTrain:  epoch  0, batch    28 | loss: 2.1256090MixupTrain:  epoch  0, batch    29 | loss: 2.1840828MixupTrain:  epoch  0, batch    30 | loss: 2.0257023MixupTrain:  epoch  0, batch    31 | loss: 1.7191351MixupTrain:  epoch  0, batch    32 | loss: 1.8944513MixupTrain:  epoch  0, batch    33 | loss: 1.7507580MixupTrain:  epoch  0, batch    34 | loss: 1.7608573
MemoryTrain:  epoch  0, batch     0 | loss: 1.6369238MemoryTrain:  epoch  0, batch     1 | loss: 1.5462584MemoryTrain:  epoch  0, batch     2 | loss: 1.9079835MemoryTrain:  epoch  0, batch     3 | loss: 1.9300660MemoryTrain:  epoch  0, batch     4 | loss: 1.6410360MemoryTrain:  epoch  0, batch     5 | loss: 3.0781324MemoryTrain:  epoch  0, batch     6 | loss: 1.5745444MemoryTrain:  epoch  0, batch     7 | loss: 2.1714220MemoryTrain:  epoch  0, batch     8 | loss: 1.8397574MemoryTrain:  epoch  0, batch     9 | loss: 3.2093134MemoryTrain:  epoch  0, batch    10 | loss: 2.1176982MemoryTrain:  epoch  0, batch    11 | loss: 2.1315267MemoryTrain:  epoch  0, batch    12 | loss: 2.9202521MemoryTrain:  epoch  0, batch    13 | loss: 2.2203178MemoryTrain:  epoch  0, batch    14 | loss: 3.4357355MemoryTrain:  epoch  1, batch     0 | loss: 1.4072895MemoryTrain:  epoch  1, batch     1 | loss: 2.4912724MemoryTrain:  epoch  1, batch     2 | loss: 1.8420877MemoryTrain:  epoch  1, batch     3 | loss: 1.8351400MemoryTrain:  epoch  1, batch     4 | loss: 2.4056273MemoryTrain:  epoch  1, batch     5 | loss: 1.7299197MemoryTrain:  epoch  1, batch     6 | loss: 1.4724225MemoryTrain:  epoch  1, batch     7 | loss: 2.3160570MemoryTrain:  epoch  1, batch     8 | loss: 1.4617696MemoryTrain:  epoch  1, batch     9 | loss: 2.0454855MemoryTrain:  epoch  1, batch    10 | loss: 1.5968207MemoryTrain:  epoch  1, batch    11 | loss: 1.8768375MemoryTrain:  epoch  1, batch    12 | loss: 1.4393466MemoryTrain:  epoch  1, batch    13 | loss: 2.5423651MemoryTrain:  epoch  1, batch    14 | loss: 2.4920974MemoryTrain:  epoch  2, batch     0 | loss: 1.9047155MemoryTrain:  epoch  2, batch     1 | loss: 1.8257201MemoryTrain:  epoch  2, batch     2 | loss: 1.9177754MemoryTrain:  epoch  2, batch     3 | loss: 1.4542614MemoryTrain:  epoch  2, batch     4 | loss: 1.4127562MemoryTrain:  epoch  2, batch     5 | loss: 2.4253366MemoryTrain:  epoch  2, batch     6 | loss: 1.4486140MemoryTrain:  epoch  2, batch     7 | loss: 1.8172836MemoryTrain:  epoch  2, batch     8 | loss: 2.1494308MemoryTrain:  epoch  2, batch     9 | loss: 1.6744585MemoryTrain:  epoch  2, batch    10 | loss: 2.7026930MemoryTrain:  epoch  2, batch    11 | loss: 1.6291113MemoryTrain:  epoch  2, batch    12 | loss: 1.6713306MemoryTrain:  epoch  2, batch    13 | loss: 1.2889185MemoryTrain:  epoch  2, batch    14 | loss: 1.5727315MemoryTrain:  epoch  3, batch     0 | loss: 1.7454748MemoryTrain:  epoch  3, batch     1 | loss: 1.3036927MemoryTrain:  epoch  3, batch     2 | loss: 1.2697062MemoryTrain:  epoch  3, batch     3 | loss: 2.1535904MemoryTrain:  epoch  3, batch     4 | loss: 2.0644250MemoryTrain:  epoch  3, batch     5 | loss: 1.2521785MemoryTrain:  epoch  3, batch     6 | loss: 1.5089662MemoryTrain:  epoch  3, batch     7 | loss: 1.6072226MemoryTrain:  epoch  3, batch     8 | loss: 1.3636446MemoryTrain:  epoch  3, batch     9 | loss: 1.8046848MemoryTrain:  epoch  3, batch    10 | loss: 2.0602379MemoryTrain:  epoch  3, batch    11 | loss: 1.7145690MemoryTrain:  epoch  3, batch    12 | loss: 2.2522087MemoryTrain:  epoch  3, batch    13 | loss: 1.4724190MemoryTrain:  epoch  3, batch    14 | loss: 1.4661826MemoryTrain:  epoch  4, batch     0 | loss: 2.5181799MemoryTrain:  epoch  4, batch     1 | loss: 1.4505196MemoryTrain:  epoch  4, batch     2 | loss: 1.3659155MemoryTrain:  epoch  4, batch     3 | loss: 1.6653998MemoryTrain:  epoch  4, batch     4 | loss: 1.4298794MemoryTrain:  epoch  4, batch     5 | loss: 1.4407513MemoryTrain:  epoch  4, batch     6 | loss: 1.2429521MemoryTrain:  epoch  4, batch     7 | loss: 1.6376300MemoryTrain:  epoch  4, batch     8 | loss: 1.7226694MemoryTrain:  epoch  4, batch     9 | loss: 1.9215374MemoryTrain:  epoch  4, batch    10 | loss: 1.6899583MemoryTrain:  epoch  4, batch    11 | loss: 1.3431168MemoryTrain:  epoch  4, batch    12 | loss: 1.2558607MemoryTrain:  epoch  4, batch    13 | loss: 1.7250981MemoryTrain:  epoch  4, batch    14 | loss: 1.4234707MemoryTrain:  epoch  5, batch     0 | loss: 1.6520463MemoryTrain:  epoch  5, batch     1 | loss: 1.5629375MemoryTrain:  epoch  5, batch     2 | loss: 1.3118340MemoryTrain:  epoch  5, batch     3 | loss: 1.2886379MemoryTrain:  epoch  5, batch     4 | loss: 1.4440162MemoryTrain:  epoch  5, batch     5 | loss: 1.2025957MemoryTrain:  epoch  5, batch     6 | loss: 1.5821970MemoryTrain:  epoch  5, batch     7 | loss: 1.4182147MemoryTrain:  epoch  5, batch     8 | loss: 2.3341265MemoryTrain:  epoch  5, batch     9 | loss: 1.3123912MemoryTrain:  epoch  5, batch    10 | loss: 1.8070546MemoryTrain:  epoch  5, batch    11 | loss: 1.3178595MemoryTrain:  epoch  5, batch    12 | loss: 1.8754900MemoryTrain:  epoch  5, batch    13 | loss: 1.3234494MemoryTrain:  epoch  5, batch    14 | loss: 1.4584327MemoryTrain:  epoch  6, batch     0 | loss: 1.6252387MemoryTrain:  epoch  6, batch     1 | loss: 1.2386816MemoryTrain:  epoch  6, batch     2 | loss: 1.4974756MemoryTrain:  epoch  6, batch     3 | loss: 1.2388560MemoryTrain:  epoch  6, batch     4 | loss: 1.5968983MemoryTrain:  epoch  6, batch     5 | loss: 1.2868385MemoryTrain:  epoch  6, batch     6 | loss: 1.2939323MemoryTrain:  epoch  6, batch     7 | loss: 1.7052548MemoryTrain:  epoch  6, batch     8 | loss: 1.3198588MemoryTrain:  epoch  6, batch     9 | loss: 1.2601680MemoryTrain:  epoch  6, batch    10 | loss: 1.2423913MemoryTrain:  epoch  6, batch    11 | loss: 1.4508057MemoryTrain:  epoch  6, batch    12 | loss: 2.3378601MemoryTrain:  epoch  6, batch    13 | loss: 1.2235062MemoryTrain:  epoch  6, batch    14 | loss: 1.2348044MemoryTrain:  epoch  7, batch     0 | loss: 1.5878628MemoryTrain:  epoch  7, batch     1 | loss: 1.4246575MemoryTrain:  epoch  7, batch     2 | loss: 1.2885684MemoryTrain:  epoch  7, batch     3 | loss: 1.2325294MemoryTrain:  epoch  7, batch     4 | loss: 1.2859784MemoryTrain:  epoch  7, batch     5 | loss: 1.3531289MemoryTrain:  epoch  7, batch     6 | loss: 1.4602809MemoryTrain:  epoch  7, batch     7 | loss: 1.4712335MemoryTrain:  epoch  7, batch     8 | loss: 1.6539381MemoryTrain:  epoch  7, batch     9 | loss: 1.3095566MemoryTrain:  epoch  7, batch    10 | loss: 1.4689896MemoryTrain:  epoch  7, batch    11 | loss: 1.8828063MemoryTrain:  epoch  7, batch    12 | loss: 1.2241950MemoryTrain:  epoch  7, batch    13 | loss: 1.1894544MemoryTrain:  epoch  7, batch    14 | loss: 1.2933303MemoryTrain:  epoch  8, batch     0 | loss: 1.7269791MemoryTrain:  epoch  8, batch     1 | loss: 1.2159499MemoryTrain:  epoch  8, batch     2 | loss: 1.4622082MemoryTrain:  epoch  8, batch     3 | loss: 1.2067629MemoryTrain:  epoch  8, batch     4 | loss: 1.4619057MemoryTrain:  epoch  8, batch     5 | loss: 1.5089996MemoryTrain:  epoch  8, batch     6 | loss: 1.2478653MemoryTrain:  epoch  8, batch     7 | loss: 1.2362556MemoryTrain:  epoch  8, batch     8 | loss: 1.4591255MemoryTrain:  epoch  8, batch     9 | loss: 1.2557051MemoryTrain:  epoch  8, batch    10 | loss: 1.2587652MemoryTrain:  epoch  8, batch    11 | loss: 1.3261833MemoryTrain:  epoch  8, batch    12 | loss: 1.4109768MemoryTrain:  epoch  8, batch    13 | loss: 1.2403359MemoryTrain:  epoch  8, batch    14 | loss: 1.3607159MemoryTrain:  epoch  9, batch     0 | loss: 1.2648280MemoryTrain:  epoch  9, batch     1 | loss: 1.4577618MemoryTrain:  epoch  9, batch     2 | loss: 1.2273569MemoryTrain:  epoch  9, batch     3 | loss: 1.2573822MemoryTrain:  epoch  9, batch     4 | loss: 1.6222358MemoryTrain:  epoch  9, batch     5 | loss: 1.4189259MemoryTrain:  epoch  9, batch     6 | loss: 1.2365475MemoryTrain:  epoch  9, batch     7 | loss: 1.4144021MemoryTrain:  epoch  9, batch     8 | loss: 1.2073221MemoryTrain:  epoch  9, batch     9 | loss: 1.2401377MemoryTrain:  epoch  9, batch    10 | loss: 1.6069355MemoryTrain:  epoch  9, batch    11 | loss: 1.4199400MemoryTrain:  epoch  9, batch    12 | loss: 1.2411124MemoryTrain:  epoch  9, batch    13 | loss: 1.2657819MemoryTrain:  epoch  9, batch    14 | loss: 1.2449436
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 18.75%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 21.88%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 21.25%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 19.79%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 27.68%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 34.38%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 40.28%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 44.38%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 47.73%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 51.04%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 53.85%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 58.33%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 60.16%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 61.03%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 60.62%   [EVAL] batch:   20 | acc: 18.75%,  total acc: 58.63%   [EVAL] batch:   21 | acc: 25.00%,  total acc: 57.10%   [EVAL] batch:   22 | acc: 12.50%,  total acc: 55.16%   [EVAL] batch:   23 | acc: 18.75%,  total acc: 53.65%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 52.50%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 50.72%   [EVAL] batch:   26 | acc: 18.75%,  total acc: 49.54%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 47.77%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 46.34%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 45.00%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 43.55%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 44.14%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 45.64%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 46.88%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 48.39%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 49.65%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 50.68%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 51.48%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 52.40%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 53.59%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 54.57%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 55.36%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 56.40%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 56.96%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 57.22%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 57.20%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 57.58%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 57.42%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 57.78%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 57.63%   [EVAL] batch:   50 | acc: 31.25%,  total acc: 57.11%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 57.09%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 57.19%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 57.06%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 57.05%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 57.25%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 57.13%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 57.00%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 56.78%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 57.08%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 57.07%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 57.46%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 56.94%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 25.00%,  total acc: 64.84%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 63.89%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 60.62%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 58.52%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 56.25%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 56.73%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 58.93%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 60.83%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 62.89%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 64.71%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 66.32%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 68.09%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 67.81%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 68.45%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 68.18%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 67.93%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 68.49%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 69.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 70.43%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 71.53%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 72.32%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 73.06%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 73.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 74.80%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 75.59%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 76.33%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 76.29%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 76.25%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 77.03%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 77.47%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 78.04%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 78.59%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 78.96%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 79.36%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 79.55%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 79.31%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 79.08%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 78.59%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 77.99%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 77.93%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 77.38%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 77.33%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 77.40%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 77.71%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 77.43%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 77.05%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 77.12%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 76.43%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 75.43%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 74.68%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 74.17%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 73.46%   [EVAL] batch:   61 | acc: 12.50%,  total acc: 72.48%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 72.22%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 71.88%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 71.25%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 71.21%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 70.90%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 70.86%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:   69 | acc: 31.25%,  total acc: 70.27%   [EVAL] batch:   70 | acc: 43.75%,  total acc: 69.89%   [EVAL] batch:   71 | acc: 18.75%,  total acc: 69.18%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 68.58%   [EVAL] batch:   73 | acc: 37.50%,  total acc: 68.16%   [EVAL] batch:   74 | acc: 31.25%,  total acc: 67.67%   [EVAL] batch:   75 | acc: 0.00%,  total acc: 66.78%   [EVAL] batch:   76 | acc: 6.25%,  total acc: 65.99%   [EVAL] batch:   77 | acc: 12.50%,  total acc: 65.30%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 64.48%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 63.67%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 62.96%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 63.03%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 63.48%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 63.91%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 64.26%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 64.68%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 65.01%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 65.13%   [EVAL] batch:   88 | acc: 6.25%,  total acc: 64.47%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 64.10%   [EVAL] batch:   90 | acc: 31.25%,  total acc: 63.74%   [EVAL] batch:   91 | acc: 37.50%,  total acc: 63.45%   [EVAL] batch:   92 | acc: 12.50%,  total acc: 62.90%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 62.50%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 62.83%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 63.15%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 63.34%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 63.58%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 63.83%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 64.19%   [EVAL] batch:  100 | acc: 0.00%,  total acc: 63.55%   [EVAL] batch:  101 | acc: 6.25%,  total acc: 62.99%   [EVAL] batch:  102 | acc: 0.00%,  total acc: 62.38%   [EVAL] batch:  103 | acc: 0.00%,  total acc: 61.78%   [EVAL] batch:  104 | acc: 0.00%,  total acc: 61.19%   [EVAL] batch:  105 | acc: 0.00%,  total acc: 60.61%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 60.57%   [EVAL] batch:  107 | acc: 62.50%,  total acc: 60.59%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 60.61%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 60.57%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 60.53%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 60.55%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 60.79%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 60.75%   [EVAL] batch:  114 | acc: 50.00%,  total acc: 60.65%   [EVAL] batch:  115 | acc: 56.25%,  total acc: 60.61%   [EVAL] batch:  116 | acc: 68.75%,  total acc: 60.68%   [EVAL] batch:  117 | acc: 56.25%,  total acc: 60.65%   [EVAL] batch:  118 | acc: 43.75%,  total acc: 60.50%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 60.78%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 61.05%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 61.27%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 61.38%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 61.59%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 61.85%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 61.56%   [EVAL] batch:  126 | acc: 12.50%,  total acc: 61.17%   [EVAL] batch:  127 | acc: 12.50%,  total acc: 60.79%   [EVAL] batch:  128 | acc: 6.25%,  total acc: 60.37%   [EVAL] batch:  129 | acc: 0.00%,  total acc: 59.90%   [EVAL] batch:  130 | acc: 12.50%,  total acc: 59.54%   [EVAL] batch:  131 | acc: 50.00%,  total acc: 59.47%   [EVAL] batch:  132 | acc: 68.75%,  total acc: 59.54%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 59.65%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 59.81%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 60.06%   [EVAL] batch:  136 | acc: 68.75%,  total acc: 60.13%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 60.05%   [EVAL] batch:  138 | acc: 31.25%,  total acc: 59.85%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 59.64%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 59.44%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 59.20%   [EVAL] batch:  142 | acc: 50.00%,  total acc: 59.13%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 58.98%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 59.22%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 59.50%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 59.74%   [EVAL] batch:  147 | acc: 93.75%,  total acc: 59.97%   [EVAL] batch:  148 | acc: 93.75%,  total acc: 60.19%   [EVAL] batch:  149 | acc: 81.25%,  total acc: 60.33%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 60.55%   [EVAL] batch:  151 | acc: 87.50%,  total acc: 60.73%   [EVAL] batch:  152 | acc: 87.50%,  total acc: 60.91%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 61.08%   [EVAL] batch:  154 | acc: 87.50%,  total acc: 61.25%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 61.42%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 61.62%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 61.87%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 62.07%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 62.27%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 62.46%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 62.65%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 62.62%   [EVAL] batch:  163 | acc: 25.00%,  total acc: 62.39%   [EVAL] batch:  164 | acc: 12.50%,  total acc: 62.08%   [EVAL] batch:  165 | acc: 12.50%,  total acc: 61.78%   [EVAL] batch:  166 | acc: 6.25%,  total acc: 61.45%   [EVAL] batch:  167 | acc: 0.00%,  total acc: 61.09%   [EVAL] batch:  168 | acc: 50.00%,  total acc: 61.02%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 61.10%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 61.22%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 61.30%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 61.34%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 61.28%   [EVAL] batch:  174 | acc: 68.75%,  total acc: 61.32%   [EVAL] batch:  175 | acc: 37.50%,  total acc: 61.19%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 61.09%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 60.96%   [EVAL] batch:  178 | acc: 43.75%,  total acc: 60.86%   [EVAL] batch:  179 | acc: 50.00%,  total acc: 60.80%   [EVAL] batch:  180 | acc: 18.75%,  total acc: 60.57%   [EVAL] batch:  181 | acc: 31.25%,  total acc: 60.41%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 60.48%   [EVAL] batch:  183 | acc: 50.00%,  total acc: 60.43%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 60.57%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 60.75%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 60.83%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 60.84%   [EVAL] batch:  188 | acc: 62.50%,  total acc: 60.85%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 60.92%   [EVAL] batch:  190 | acc: 75.00%,  total acc: 60.99%   [EVAL] batch:  191 | acc: 62.50%,  total acc: 61.00%   [EVAL] batch:  192 | acc: 68.75%,  total acc: 61.04%   [EVAL] batch:  193 | acc: 87.50%,  total acc: 61.18%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 61.31%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 61.32%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 61.36%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 61.43%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 61.46%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 61.53%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 61.60%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 61.70%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 61.73%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 61.83%   [EVAL] batch:  204 | acc: 87.50%,  total acc: 61.95%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 62.11%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 61.99%   [EVAL] batch:  207 | acc: 31.25%,  total acc: 61.84%   [EVAL] batch:  208 | acc: 18.75%,  total acc: 61.63%   [EVAL] batch:  209 | acc: 12.50%,  total acc: 61.40%   [EVAL] batch:  210 | acc: 43.75%,  total acc: 61.32%   [EVAL] batch:  211 | acc: 37.50%,  total acc: 61.20%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 61.15%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 61.30%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 61.48%   [EVAL] batch:  215 | acc: 68.75%,  total acc: 61.52%   [EVAL] batch:  216 | acc: 93.75%,  total acc: 61.66%   [EVAL] batch:  217 | acc: 75.00%,  total acc: 61.73%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 61.87%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 62.05%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 62.22%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 62.39%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 62.56%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 62.72%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 62.89%   [EVAL] batch:  225 | acc: 93.75%,  total acc: 63.03%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 63.13%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 63.16%   [EVAL] batch:  228 | acc: 87.50%,  total acc: 63.26%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 63.40%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 63.47%   [EVAL] batch:  231 | acc: 50.00%,  total acc: 63.42%   [EVAL] batch:  232 | acc: 31.25%,  total acc: 63.28%   [EVAL] batch:  233 | acc: 37.50%,  total acc: 63.17%   [EVAL] batch:  234 | acc: 56.25%,  total acc: 63.14%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 63.14%   [EVAL] batch:  236 | acc: 37.50%,  total acc: 63.03%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 63.10%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 63.21%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 63.36%   [EVAL] batch:  240 | acc: 75.00%,  total acc: 63.41%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 63.51%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 63.61%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 63.68%   [EVAL] batch:  244 | acc: 25.00%,  total acc: 63.52%   [EVAL] batch:  245 | acc: 12.50%,  total acc: 63.31%   [EVAL] batch:  246 | acc: 6.25%,  total acc: 63.08%   [EVAL] batch:  247 | acc: 25.00%,  total acc: 62.93%   [EVAL] batch:  248 | acc: 18.75%,  total acc: 62.75%   [EVAL] batch:  249 | acc: 18.75%,  total acc: 62.58%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 62.70%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 62.82%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 62.92%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 63.02%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 63.14%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 63.28%   [EVAL] batch:  256 | acc: 75.00%,  total acc: 63.33%   [EVAL] batch:  257 | acc: 93.75%,  total acc: 63.44%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 63.49%   [EVAL] batch:  259 | acc: 87.50%,  total acc: 63.58%   [EVAL] batch:  260 | acc: 75.00%,  total acc: 63.63%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 63.67%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 63.76%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 63.83%   [EVAL] batch:  264 | acc: 87.50%,  total acc: 63.92%   [EVAL] batch:  265 | acc: 87.50%,  total acc: 64.00%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 64.07%   [EVAL] batch:  267 | acc: 87.50%,  total acc: 64.16%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 64.20%   [EVAL] batch:  269 | acc: 56.25%,  total acc: 64.17%   [EVAL] batch:  270 | acc: 62.50%,  total acc: 64.16%   [EVAL] batch:  271 | acc: 62.50%,  total acc: 64.15%   [EVAL] batch:  272 | acc: 43.75%,  total acc: 64.08%   [EVAL] batch:  273 | acc: 81.25%,  total acc: 64.14%   [EVAL] batch:  274 | acc: 50.00%,  total acc: 64.09%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 64.22%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 64.35%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 64.48%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 64.61%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 64.73%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 64.86%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 64.89%   [EVAL] batch:  282 | acc: 81.25%,  total acc: 64.95%   [EVAL] batch:  283 | acc: 68.75%,  total acc: 64.96%   [EVAL] batch:  284 | acc: 75.00%,  total acc: 65.00%   [EVAL] batch:  285 | acc: 62.50%,  total acc: 64.99%   [EVAL] batch:  286 | acc: 56.25%,  total acc: 64.96%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 64.97%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 65.10%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 65.22%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 65.34%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 65.45%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 65.57%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 65.67%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 65.78%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 65.90%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 66.01%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 66.13%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 66.24%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 66.35%   [EVAL] batch:  300 | acc: 87.50%,  total acc: 66.42%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 66.51%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 66.60%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 66.69%   [EVAL] batch:  304 | acc: 87.50%,  total acc: 66.76%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 66.85%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 66.94%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 67.03%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 67.09%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 67.18%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 67.28%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 67.33%   [EVAL] batch:  312 | acc: 93.75%,  total acc: 67.41%   [EVAL] batch:  313 | acc: 81.25%,  total acc: 67.46%   [EVAL] batch:  314 | acc: 100.00%,  total acc: 67.56%   [EVAL] batch:  315 | acc: 75.00%,  total acc: 67.58%   [EVAL] batch:  316 | acc: 68.75%,  total acc: 67.59%   [EVAL] batch:  317 | acc: 81.25%,  total acc: 67.63%   [EVAL] batch:  318 | acc: 81.25%,  total acc: 67.67%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 67.77%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 67.87%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 67.92%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 67.98%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 68.04%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 68.08%   [EVAL] batch:  325 | acc: 50.00%,  total acc: 68.02%   [EVAL] batch:  326 | acc: 37.50%,  total acc: 67.93%   [EVAL] batch:  327 | acc: 56.25%,  total acc: 67.89%   [EVAL] batch:  328 | acc: 68.75%,  total acc: 67.90%   [EVAL] batch:  329 | acc: 75.00%,  total acc: 67.92%   [EVAL] batch:  330 | acc: 62.50%,  total acc: 67.90%   [EVAL] batch:  331 | acc: 62.50%,  total acc: 67.88%   [EVAL] batch:  332 | acc: 56.25%,  total acc: 67.85%   [EVAL] batch:  333 | acc: 68.75%,  total acc: 67.85%   [EVAL] batch:  334 | acc: 43.75%,  total acc: 67.78%   [EVAL] batch:  335 | acc: 68.75%,  total acc: 67.78%   [EVAL] batch:  336 | acc: 62.50%,  total acc: 67.77%   [EVAL] batch:  337 | acc: 75.00%,  total acc: 67.79%   [EVAL] batch:  338 | acc: 68.75%,  total acc: 67.79%   [EVAL] batch:  339 | acc: 75.00%,  total acc: 67.81%   [EVAL] batch:  340 | acc: 81.25%,  total acc: 67.85%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 67.87%   [EVAL] batch:  342 | acc: 50.00%,  total acc: 67.82%   [EVAL] batch:  343 | acc: 43.75%,  total acc: 67.75%   [EVAL] batch:  344 | acc: 62.50%,  total acc: 67.74%   [EVAL] batch:  345 | acc: 43.75%,  total acc: 67.67%   [EVAL] batch:  346 | acc: 62.50%,  total acc: 67.65%   [EVAL] batch:  347 | acc: 62.50%,  total acc: 67.64%   [EVAL] batch:  348 | acc: 62.50%,  total acc: 67.62%   [EVAL] batch:  349 | acc: 50.00%,  total acc: 67.57%   [EVAL] batch:  350 | acc: 81.25%,  total acc: 67.61%   [EVAL] batch:  351 | acc: 87.50%,  total acc: 67.67%   [EVAL] batch:  352 | acc: 87.50%,  total acc: 67.72%   [EVAL] batch:  353 | acc: 87.50%,  total acc: 67.78%   [EVAL] batch:  354 | acc: 75.00%,  total acc: 67.80%   [EVAL] batch:  355 | acc: 87.50%,  total acc: 67.85%   [EVAL] batch:  356 | acc: 56.25%,  total acc: 67.82%   [EVAL] batch:  357 | acc: 37.50%,  total acc: 67.74%   [EVAL] batch:  358 | acc: 43.75%,  total acc: 67.67%   [EVAL] batch:  359 | acc: 50.00%,  total acc: 67.62%   [EVAL] batch:  360 | acc: 37.50%,  total acc: 67.54%   [EVAL] batch:  361 | acc: 43.75%,  total acc: 67.47%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 67.46%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 67.55%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 67.73%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 67.81%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 67.90%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 67.99%   [EVAL] batch:  369 | acc: 81.25%,  total acc: 68.02%   [EVAL] batch:  370 | acc: 62.50%,  total acc: 68.01%   [EVAL] batch:  371 | acc: 87.50%,  total acc: 68.06%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 68.10%   [EVAL] batch:  373 | acc: 68.75%,  total acc: 68.10%   [EVAL] batch:  374 | acc: 68.75%,  total acc: 68.10%   [EVAL] batch:  375 | acc: 62.50%,  total acc: 68.09%   [EVAL] batch:  376 | acc: 81.25%,  total acc: 68.12%   [EVAL] batch:  377 | acc: 81.25%,  total acc: 68.15%   [EVAL] batch:  378 | acc: 81.25%,  total acc: 68.19%   [EVAL] batch:  379 | acc: 75.00%,  total acc: 68.21%   [EVAL] batch:  380 | acc: 87.50%,  total acc: 68.26%   [EVAL] batch:  381 | acc: 56.25%,  total acc: 68.23%   [EVAL] batch:  382 | acc: 68.75%,  total acc: 68.23%   [EVAL] batch:  383 | acc: 75.00%,  total acc: 68.25%   [EVAL] batch:  384 | acc: 68.75%,  total acc: 68.25%   [EVAL] batch:  385 | acc: 37.50%,  total acc: 68.17%   [EVAL] batch:  386 | acc: 62.50%,  total acc: 68.15%   [EVAL] batch:  387 | acc: 93.75%,  total acc: 68.22%   [EVAL] batch:  388 | acc: 62.50%,  total acc: 68.20%   [EVAL] batch:  389 | acc: 62.50%,  total acc: 68.19%   [EVAL] batch:  390 | acc: 62.50%,  total acc: 68.17%   [EVAL] batch:  391 | acc: 68.75%,  total acc: 68.18%   [EVAL] batch:  392 | acc: 68.75%,  total acc: 68.18%   [EVAL] batch:  393 | acc: 100.00%,  total acc: 68.26%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 68.32%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 68.40%   [EVAL] batch:  396 | acc: 93.75%,  total acc: 68.47%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 68.55%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 68.61%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 68.69%   [EVAL] batch:  400 | acc: 62.50%,  total acc: 68.67%   [EVAL] batch:  401 | acc: 56.25%,  total acc: 68.64%   [EVAL] batch:  402 | acc: 81.25%,  total acc: 68.67%   [EVAL] batch:  403 | acc: 62.50%,  total acc: 68.66%   [EVAL] batch:  404 | acc: 75.00%,  total acc: 68.67%   [EVAL] batch:  405 | acc: 81.25%,  total acc: 68.70%   [EVAL] batch:  406 | acc: 100.00%,  total acc: 68.78%   [EVAL] batch:  407 | acc: 68.75%,  total acc: 68.78%   [EVAL] batch:  408 | acc: 50.00%,  total acc: 68.73%   [EVAL] batch:  409 | acc: 56.25%,  total acc: 68.70%   [EVAL] batch:  410 | acc: 56.25%,  total acc: 68.67%   [EVAL] batch:  411 | acc: 62.50%,  total acc: 68.66%   [EVAL] batch:  412 | acc: 50.00%,  total acc: 68.61%   [EVAL] batch:  413 | acc: 43.75%,  total acc: 68.55%   [EVAL] batch:  414 | acc: 31.25%,  total acc: 68.46%   [EVAL] batch:  415 | acc: 31.25%,  total acc: 68.37%   [EVAL] batch:  416 | acc: 43.75%,  total acc: 68.32%   [EVAL] batch:  417 | acc: 37.50%,  total acc: 68.24%   [EVAL] batch:  418 | acc: 43.75%,  total acc: 68.18%   [EVAL] batch:  419 | acc: 75.00%,  total acc: 68.20%   [EVAL] batch:  420 | acc: 68.75%,  total acc: 68.20%   [EVAL] batch:  421 | acc: 56.25%,  total acc: 68.17%   [EVAL] batch:  422 | acc: 68.75%,  total acc: 68.17%   [EVAL] batch:  423 | acc: 75.00%,  total acc: 68.19%   [EVAL] batch:  424 | acc: 81.25%,  total acc: 68.22%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 68.30%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 68.37%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 68.44%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 68.52%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 68.59%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 68.66%   [EVAL] batch:  431 | acc: 100.00%,  total acc: 68.74%   [EVAL] batch:  432 | acc: 93.75%,  total acc: 68.79%   [EVAL] batch:  433 | acc: 100.00%,  total acc: 68.87%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 68.94%   [EVAL] batch:  435 | acc: 100.00%,  total acc: 69.01%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 69.08%   [EVAL] batch:  437 | acc: 56.25%,  total acc: 69.05%   [EVAL] batch:  438 | acc: 18.75%,  total acc: 68.94%   [EVAL] batch:  439 | acc: 18.75%,  total acc: 68.82%   [EVAL] batch:  440 | acc: 37.50%,  total acc: 68.75%   [EVAL] batch:  441 | acc: 12.50%,  total acc: 68.62%   [EVAL] batch:  442 | acc: 18.75%,  total acc: 68.51%   [EVAL] batch:  443 | acc: 43.75%,  total acc: 68.45%   [EVAL] batch:  444 | acc: 75.00%,  total acc: 68.47%   [EVAL] batch:  445 | acc: 81.25%,  total acc: 68.50%   [EVAL] batch:  446 | acc: 87.50%,  total acc: 68.54%   [EVAL] batch:  447 | acc: 87.50%,  total acc: 68.58%   [EVAL] batch:  448 | acc: 75.00%,  total acc: 68.60%   [EVAL] batch:  449 | acc: 87.50%,  total acc: 68.64%   [EVAL] batch:  450 | acc: 100.00%,  total acc: 68.71%   [EVAL] batch:  451 | acc: 81.25%,  total acc: 68.74%   [EVAL] batch:  452 | acc: 87.50%,  total acc: 68.78%   [EVAL] batch:  453 | acc: 81.25%,  total acc: 68.81%   [EVAL] batch:  454 | acc: 81.25%,  total acc: 68.83%   [EVAL] batch:  455 | acc: 87.50%,  total acc: 68.87%   [EVAL] batch:  456 | acc: 25.00%,  total acc: 68.78%   [EVAL] batch:  457 | acc: 31.25%,  total acc: 68.70%   [EVAL] batch:  458 | acc: 18.75%,  total acc: 68.59%   [EVAL] batch:  459 | acc: 18.75%,  total acc: 68.48%   [EVAL] batch:  460 | acc: 18.75%,  total acc: 68.37%   [EVAL] batch:  461 | acc: 12.50%,  total acc: 68.25%   [EVAL] batch:  462 | acc: 25.00%,  total acc: 68.16%   [EVAL] batch:  463 | acc: 6.25%,  total acc: 68.02%   [EVAL] batch:  464 | acc: 12.50%,  total acc: 67.90%   [EVAL] batch:  465 | acc: 0.00%,  total acc: 67.76%   [EVAL] batch:  466 | acc: 6.25%,  total acc: 67.63%   [EVAL] batch:  467 | acc: 6.25%,  total acc: 67.49%   [EVAL] batch:  468 | acc: 18.75%,  total acc: 67.39%   [EVAL] batch:  469 | acc: 87.50%,  total acc: 67.43%   [EVAL] batch:  470 | acc: 93.75%,  total acc: 67.49%   [EVAL] batch:  471 | acc: 93.75%,  total acc: 67.55%   [EVAL] batch:  472 | acc: 100.00%,  total acc: 67.61%   [EVAL] batch:  473 | acc: 87.50%,  total acc: 67.66%   [EVAL] batch:  474 | acc: 87.50%,  total acc: 67.70%   [EVAL] batch:  475 | acc: 81.25%,  total acc: 67.73%   [EVAL] batch:  476 | acc: 93.75%,  total acc: 67.78%   [EVAL] batch:  477 | acc: 93.75%,  total acc: 67.83%   [EVAL] batch:  478 | acc: 93.75%,  total acc: 67.89%   [EVAL] batch:  479 | acc: 93.75%,  total acc: 67.94%   [EVAL] batch:  480 | acc: 100.00%,  total acc: 68.01%   [EVAL] batch:  481 | acc: 75.00%,  total acc: 68.02%   [EVAL] batch:  482 | acc: 56.25%,  total acc: 68.00%   [EVAL] batch:  483 | acc: 56.25%,  total acc: 67.98%   [EVAL] batch:  484 | acc: 81.25%,  total acc: 68.00%   [EVAL] batch:  485 | acc: 43.75%,  total acc: 67.95%   [EVAL] batch:  486 | acc: 68.75%,  total acc: 67.95%   [EVAL] batch:  487 | acc: 37.50%,  total acc: 67.89%   [EVAL] batch:  488 | acc: 43.75%,  total acc: 67.84%   [EVAL] batch:  489 | acc: 56.25%,  total acc: 67.82%   [EVAL] batch:  490 | acc: 68.75%,  total acc: 67.82%   [EVAL] batch:  491 | acc: 43.75%,  total acc: 67.77%   [EVAL] batch:  492 | acc: 62.50%,  total acc: 67.76%   [EVAL] batch:  493 | acc: 62.50%,  total acc: 67.75%   [EVAL] batch:  494 | acc: 50.00%,  total acc: 67.71%   [EVAL] batch:  495 | acc: 50.00%,  total acc: 67.68%   [EVAL] batch:  496 | acc: 50.00%,  total acc: 67.64%   [EVAL] batch:  497 | acc: 68.75%,  total acc: 67.65%   [EVAL] batch:  498 | acc: 75.00%,  total acc: 67.66%   [EVAL] batch:  499 | acc: 62.50%,  total acc: 67.65%   
cur_acc:  ['0.9524', '0.7748', '0.8204', '0.7341', '0.8998', '0.7877', '0.7927', '0.5694']
his_acc:  ['0.9524', '0.8575', '0.8092', '0.7548', '0.7572', '0.7322', '0.7156', '0.6765']
--------Round  4
seed:  500
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 5 6 4 2 1 3 0]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 13.1388245CurrentTrain: epoch  0, batch     1 | loss: 12.7849798CurrentTrain: epoch  0, batch     2 | loss: 12.9057236CurrentTrain: epoch  0, batch     3 | loss: 12.5117340CurrentTrain: epoch  0, batch     4 | loss: 11.9298286CurrentTrain: epoch  0, batch     5 | loss: 11.8650351CurrentTrain: epoch  0, batch     6 | loss: 11.9230852CurrentTrain: epoch  0, batch     7 | loss: 11.4831820CurrentTrain: epoch  0, batch     8 | loss: 11.5512753CurrentTrain: epoch  0, batch     9 | loss: 11.5413036CurrentTrain: epoch  0, batch    10 | loss: 11.2489624CurrentTrain: epoch  0, batch    11 | loss: 11.1673622CurrentTrain: epoch  0, batch    12 | loss: 11.2556858CurrentTrain: epoch  0, batch    13 | loss: 10.8612690CurrentTrain: epoch  0, batch    14 | loss: 10.6481457CurrentTrain: epoch  0, batch    15 | loss: 10.7434654CurrentTrain: epoch  0, batch    16 | loss: 10.3818245CurrentTrain: epoch  0, batch    17 | loss: 10.3972111CurrentTrain: epoch  0, batch    18 | loss: 9.9402771CurrentTrain: epoch  0, batch    19 | loss: 10.0884342CurrentTrain: epoch  0, batch    20 | loss: 10.3795223CurrentTrain: epoch  0, batch    21 | loss: 9.8598509CurrentTrain: epoch  0, batch    22 | loss: 10.4190836CurrentTrain: epoch  0, batch    23 | loss: 10.2717772CurrentTrain: epoch  0, batch    24 | loss: 10.2067451CurrentTrain: epoch  0, batch    25 | loss: 9.9175606CurrentTrain: epoch  0, batch    26 | loss: 10.0795670CurrentTrain: epoch  0, batch    27 | loss: 9.8407469CurrentTrain: epoch  0, batch    28 | loss: 9.7256308CurrentTrain: epoch  0, batch    29 | loss: 9.3624039CurrentTrain: epoch  0, batch    30 | loss: 9.8317490CurrentTrain: epoch  0, batch    31 | loss: 9.4567080CurrentTrain: epoch  0, batch    32 | loss: 9.4634066CurrentTrain: epoch  0, batch    33 | loss: 8.7165203CurrentTrain: epoch  0, batch    34 | loss: 9.8625154CurrentTrain: epoch  0, batch    35 | loss: 9.6619740CurrentTrain: epoch  0, batch    36 | loss: 9.5153713CurrentTrain: epoch  0, batch    37 | loss: 8.6071281CurrentTrain: epoch  0, batch    38 | loss: 9.0698519CurrentTrain: epoch  0, batch    39 | loss: 8.9396420CurrentTrain: epoch  0, batch    40 | loss: 8.8390846CurrentTrain: epoch  0, batch    41 | loss: 8.9303284CurrentTrain: epoch  0, batch    42 | loss: 9.5840073CurrentTrain: epoch  0, batch    43 | loss: 8.7586975CurrentTrain: epoch  0, batch    44 | loss: 9.1357765CurrentTrain: epoch  0, batch    45 | loss: 8.8551540CurrentTrain: epoch  0, batch    46 | loss: 8.1454182CurrentTrain: epoch  0, batch    47 | loss: 8.8841124CurrentTrain: epoch  0, batch    48 | loss: 8.6134186CurrentTrain: epoch  0, batch    49 | loss: 9.1056118CurrentTrain: epoch  0, batch    50 | loss: 7.9175262CurrentTrain: epoch  0, batch    51 | loss: 8.3604050CurrentTrain: epoch  0, batch    52 | loss: 8.5199938CurrentTrain: epoch  0, batch    53 | loss: 8.8677492CurrentTrain: epoch  0, batch    54 | loss: 9.1207256CurrentTrain: epoch  0, batch    55 | loss: 9.0709553CurrentTrain: epoch  0, batch    56 | loss: 7.8411303CurrentTrain: epoch  0, batch    57 | loss: 7.4882898CurrentTrain: epoch  0, batch    58 | loss: 8.0309467CurrentTrain: epoch  0, batch    59 | loss: 7.9618502CurrentTrain: epoch  0, batch    60 | loss: 8.5632133CurrentTrain: epoch  0, batch    61 | loss: 7.4672027CurrentTrain: epoch  0, batch    62 | loss: 7.8579330CurrentTrain: epoch  1, batch     0 | loss: 8.0023470CurrentTrain: epoch  1, batch     1 | loss: 7.2889409CurrentTrain: epoch  1, batch     2 | loss: 6.6413703CurrentTrain: epoch  1, batch     3 | loss: 8.5545187CurrentTrain: epoch  1, batch     4 | loss: 8.0961943CurrentTrain: epoch  1, batch     5 | loss: 6.9052701CurrentTrain: epoch  1, batch     6 | loss: 7.5643535CurrentTrain: epoch  1, batch     7 | loss: 7.0409012CurrentTrain: epoch  1, batch     8 | loss: 7.2642221CurrentTrain: epoch  1, batch     9 | loss: 7.4273510CurrentTrain: epoch  1, batch    10 | loss: 7.3170781CurrentTrain: epoch  1, batch    11 | loss: 7.5283036CurrentTrain: epoch  1, batch    12 | loss: 7.0201311CurrentTrain: epoch  1, batch    13 | loss: 7.4172487CurrentTrain: epoch  1, batch    14 | loss: 7.6339054CurrentTrain: epoch  1, batch    15 | loss: 8.0372849CurrentTrain: epoch  1, batch    16 | loss: 8.0770102CurrentTrain: epoch  1, batch    17 | loss: 7.7442856CurrentTrain: epoch  1, batch    18 | loss: 7.4176044CurrentTrain: epoch  1, batch    19 | loss: 7.4216852CurrentTrain: epoch  1, batch    20 | loss: 7.4294748CurrentTrain: epoch  1, batch    21 | loss: 7.0691729CurrentTrain: epoch  1, batch    22 | loss: 7.8901715CurrentTrain: epoch  1, batch    23 | loss: 7.5206890CurrentTrain: epoch  1, batch    24 | loss: 7.1091967CurrentTrain: epoch  1, batch    25 | loss: 5.9188924CurrentTrain: epoch  1, batch    26 | loss: 6.7212381CurrentTrain: epoch  1, batch    27 | loss: 7.2966795CurrentTrain: epoch  1, batch    28 | loss: 6.7685442CurrentTrain: epoch  1, batch    29 | loss: 6.5011292CurrentTrain: epoch  1, batch    30 | loss: 6.7055378CurrentTrain: epoch  1, batch    31 | loss: 7.1384621CurrentTrain: epoch  1, batch    32 | loss: 6.6881208CurrentTrain: epoch  1, batch    33 | loss: 6.4085283CurrentTrain: epoch  1, batch    34 | loss: 7.5877533CurrentTrain: epoch  1, batch    35 | loss: 6.8008814CurrentTrain: epoch  1, batch    36 | loss: 6.3800602CurrentTrain: epoch  1, batch    37 | loss: 6.8626385CurrentTrain: epoch  1, batch    38 | loss: 6.1607695CurrentTrain: epoch  1, batch    39 | loss: 6.0986290CurrentTrain: epoch  1, batch    40 | loss: 6.6480985CurrentTrain: epoch  1, batch    41 | loss: 6.6116362CurrentTrain: epoch  1, batch    42 | loss: 6.2152967CurrentTrain: epoch  1, batch    43 | loss: 6.5072460CurrentTrain: epoch  1, batch    44 | loss: 6.6181130CurrentTrain: epoch  1, batch    45 | loss: 6.4451094CurrentTrain: epoch  1, batch    46 | loss: 6.6407666CurrentTrain: epoch  1, batch    47 | loss: 6.5261245CurrentTrain: epoch  1, batch    48 | loss: 6.1680775CurrentTrain: epoch  1, batch    49 | loss: 6.4360890CurrentTrain: epoch  1, batch    50 | loss: 7.1438656CurrentTrain: epoch  1, batch    51 | loss: 6.5381994CurrentTrain: epoch  1, batch    52 | loss: 5.7783389CurrentTrain: epoch  1, batch    53 | loss: 6.8103008CurrentTrain: epoch  1, batch    54 | loss: 6.2121224CurrentTrain: epoch  1, batch    55 | loss: 5.6850295CurrentTrain: epoch  1, batch    56 | loss: 6.7402887CurrentTrain: epoch  1, batch    57 | loss: 7.2114339CurrentTrain: epoch  1, batch    58 | loss: 6.6185975CurrentTrain: epoch  1, batch    59 | loss: 6.6943841CurrentTrain: epoch  1, batch    60 | loss: 6.8661141CurrentTrain: epoch  1, batch    61 | loss: 6.1044903CurrentTrain: epoch  1, batch    62 | loss: 6.7946711CurrentTrain: epoch  2, batch     0 | loss: 7.0297480CurrentTrain: epoch  2, batch     1 | loss: 5.9303808CurrentTrain: epoch  2, batch     2 | loss: 5.7815051CurrentTrain: epoch  2, batch     3 | loss: 6.1026034CurrentTrain: epoch  2, batch     4 | loss: 6.6440806CurrentTrain: epoch  2, batch     5 | loss: 5.9529610CurrentTrain: epoch  2, batch     6 | loss: 6.2884717CurrentTrain: epoch  2, batch     7 | loss: 5.7944546CurrentTrain: epoch  2, batch     8 | loss: 5.9668941CurrentTrain: epoch  2, batch     9 | loss: 5.8245096CurrentTrain: epoch  2, batch    10 | loss: 6.2771273CurrentTrain: epoch  2, batch    11 | loss: 6.2376184CurrentTrain: epoch  2, batch    12 | loss: 5.8588171CurrentTrain: epoch  2, batch    13 | loss: 5.7950196CurrentTrain: epoch  2, batch    14 | loss: 5.8424625CurrentTrain: epoch  2, batch    15 | loss: 6.2074385CurrentTrain: epoch  2, batch    16 | loss: 5.5627737CurrentTrain: epoch  2, batch    17 | loss: 5.4884968CurrentTrain: epoch  2, batch    18 | loss: 5.4850726CurrentTrain: epoch  2, batch    19 | loss: 5.4644117CurrentTrain: epoch  2, batch    20 | loss: 5.5655203CurrentTrain: epoch  2, batch    21 | loss: 5.8668833CurrentTrain: epoch  2, batch    22 | loss: 6.2684450CurrentTrain: epoch  2, batch    23 | loss: 5.7106724CurrentTrain: epoch  2, batch    24 | loss: 5.8643460CurrentTrain: epoch  2, batch    25 | loss: 6.4105539CurrentTrain: epoch  2, batch    26 | loss: 5.6377010CurrentTrain: epoch  2, batch    27 | loss: 5.8807964CurrentTrain: epoch  2, batch    28 | loss: 5.6081982CurrentTrain: epoch  2, batch    29 | loss: 5.8834944CurrentTrain: epoch  2, batch    30 | loss: 6.0341282CurrentTrain: epoch  2, batch    31 | loss: 5.4162874CurrentTrain: epoch  2, batch    32 | loss: 6.5676899CurrentTrain: epoch  2, batch    33 | loss: 6.2103415CurrentTrain: epoch  2, batch    34 | loss: 5.5706944CurrentTrain: epoch  2, batch    35 | loss: 6.5546579CurrentTrain: epoch  2, batch    36 | loss: 5.3392887CurrentTrain: epoch  2, batch    37 | loss: 5.9421148CurrentTrain: epoch  2, batch    38 | loss: 5.4537096CurrentTrain: epoch  2, batch    39 | loss: 5.2925167CurrentTrain: epoch  2, batch    40 | loss: 5.6200538CurrentTrain: epoch  2, batch    41 | loss: 5.8636303CurrentTrain: epoch  2, batch    42 | loss: 4.8369617CurrentTrain: epoch  2, batch    43 | loss: 5.5217810CurrentTrain: epoch  2, batch    44 | loss: 5.3199215CurrentTrain: epoch  2, batch    45 | loss: 5.4904332CurrentTrain: epoch  2, batch    46 | loss: 5.6736956CurrentTrain: epoch  2, batch    47 | loss: 5.3047080CurrentTrain: epoch  2, batch    48 | loss: 5.1410904CurrentTrain: epoch  2, batch    49 | loss: 5.3257599CurrentTrain: epoch  2, batch    50 | loss: 5.5017357CurrentTrain: epoch  2, batch    51 | loss: 6.0407972CurrentTrain: epoch  2, batch    52 | loss: 5.1868277CurrentTrain: epoch  2, batch    53 | loss: 5.1619816CurrentTrain: epoch  2, batch    54 | loss: 4.9635267CurrentTrain: epoch  2, batch    55 | loss: 5.8813105CurrentTrain: epoch  2, batch    56 | loss: 5.6249633CurrentTrain: epoch  2, batch    57 | loss: 6.4462519CurrentTrain: epoch  2, batch    58 | loss: 6.5497637CurrentTrain: epoch  2, batch    59 | loss: 5.3337555CurrentTrain: epoch  2, batch    60 | loss: 5.7773867CurrentTrain: epoch  2, batch    61 | loss: 5.1092706CurrentTrain: epoch  2, batch    62 | loss: 4.6728725CurrentTrain: epoch  3, batch     0 | loss: 5.6518712CurrentTrain: epoch  3, batch     1 | loss: 5.4335423CurrentTrain: epoch  3, batch     2 | loss: 5.8856125CurrentTrain: epoch  3, batch     3 | loss: 6.9416294CurrentTrain: epoch  3, batch     4 | loss: 5.1880941CurrentTrain: epoch  3, batch     5 | loss: 5.6151733CurrentTrain: epoch  3, batch     6 | loss: 5.2825775CurrentTrain: epoch  3, batch     7 | loss: 5.0925374CurrentTrain: epoch  3, batch     8 | loss: 4.7875624CurrentTrain: epoch  3, batch     9 | loss: 5.4743681CurrentTrain: epoch  3, batch    10 | loss: 4.6886292CurrentTrain: epoch  3, batch    11 | loss: 5.1568513CurrentTrain: epoch  3, batch    12 | loss: 4.9165659CurrentTrain: epoch  3, batch    13 | loss: 5.2973723CurrentTrain: epoch  3, batch    14 | loss: 5.0496273CurrentTrain: epoch  3, batch    15 | loss: 5.5406914CurrentTrain: epoch  3, batch    16 | loss: 5.2316957CurrentTrain: epoch  3, batch    17 | loss: 4.9170413CurrentTrain: epoch  3, batch    18 | loss: 5.3565197CurrentTrain: epoch  3, batch    19 | loss: 4.9022121CurrentTrain: epoch  3, batch    20 | loss: 5.4596038CurrentTrain: epoch  3, batch    21 | loss: 5.2852912CurrentTrain: epoch  3, batch    22 | loss: 4.8973203CurrentTrain: epoch  3, batch    23 | loss: 5.7202144CurrentTrain: epoch  3, batch    24 | loss: 5.1401958CurrentTrain: epoch  3, batch    25 | loss: 4.9194989CurrentTrain: epoch  3, batch    26 | loss: 5.9852290CurrentTrain: epoch  3, batch    27 | loss: 4.9901042CurrentTrain: epoch  3, batch    28 | loss: 5.4415903CurrentTrain: epoch  3, batch    29 | loss: 4.7369900CurrentTrain: epoch  3, batch    30 | loss: 5.0772638CurrentTrain: epoch  3, batch    31 | loss: 5.3997326CurrentTrain: epoch  3, batch    32 | loss: 4.8914866CurrentTrain: epoch  3, batch    33 | loss: 5.1362829CurrentTrain: epoch  3, batch    34 | loss: 4.8438835CurrentTrain: epoch  3, batch    35 | loss: 5.3054156CurrentTrain: epoch  3, batch    36 | loss: 4.9646311CurrentTrain: epoch  3, batch    37 | loss: 4.7947750CurrentTrain: epoch  3, batch    38 | loss: 5.6840773CurrentTrain: epoch  3, batch    39 | loss: 4.9636011CurrentTrain: epoch  3, batch    40 | loss: 4.6332989CurrentTrain: epoch  3, batch    41 | loss: 4.9181070CurrentTrain: epoch  3, batch    42 | loss: 4.5869241CurrentTrain: epoch  3, batch    43 | loss: 4.8300180CurrentTrain: epoch  3, batch    44 | loss: 5.5458689CurrentTrain: epoch  3, batch    45 | loss: 4.7591820CurrentTrain: epoch  3, batch    46 | loss: 4.7875943CurrentTrain: epoch  3, batch    47 | loss: 4.6219692CurrentTrain: epoch  3, batch    48 | loss: 4.8111157CurrentTrain: epoch  3, batch    49 | loss: 5.2280025CurrentTrain: epoch  3, batch    50 | loss: 4.8902364CurrentTrain: epoch  3, batch    51 | loss: 5.0062237CurrentTrain: epoch  3, batch    52 | loss: 4.8744516CurrentTrain: epoch  3, batch    53 | loss: 5.0932374CurrentTrain: epoch  3, batch    54 | loss: 5.2036066CurrentTrain: epoch  3, batch    55 | loss: 4.9879193CurrentTrain: epoch  3, batch    56 | loss: 4.9884167CurrentTrain: epoch  3, batch    57 | loss: 5.1053019CurrentTrain: epoch  3, batch    58 | loss: 5.3648491CurrentTrain: epoch  3, batch    59 | loss: 4.6962037CurrentTrain: epoch  3, batch    60 | loss: 4.6980743CurrentTrain: epoch  3, batch    61 | loss: 4.8098621CurrentTrain: epoch  3, batch    62 | loss: 4.5836024CurrentTrain: epoch  4, batch     0 | loss: 4.8010368CurrentTrain: epoch  4, batch     1 | loss: 4.8933053CurrentTrain: epoch  4, batch     2 | loss: 4.5850115CurrentTrain: epoch  4, batch     3 | loss: 4.5079212CurrentTrain: epoch  4, batch     4 | loss: 4.7690673CurrentTrain: epoch  4, batch     5 | loss: 4.6357203CurrentTrain: epoch  4, batch     6 | loss: 4.6278453CurrentTrain: epoch  4, batch     7 | loss: 4.8174319CurrentTrain: epoch  4, batch     8 | loss: 4.6093411CurrentTrain: epoch  4, batch     9 | loss: 4.5644593CurrentTrain: epoch  4, batch    10 | loss: 4.6870461CurrentTrain: epoch  4, batch    11 | loss: 4.7092447CurrentTrain: epoch  4, batch    12 | loss: 5.5816216CurrentTrain: epoch  4, batch    13 | loss: 4.4079990CurrentTrain: epoch  4, batch    14 | loss: 5.1305552CurrentTrain: epoch  4, batch    15 | loss: 5.3566427CurrentTrain: epoch  4, batch    16 | loss: 4.7696152CurrentTrain: epoch  4, batch    17 | loss: 4.8244257CurrentTrain: epoch  4, batch    18 | loss: 4.7178216CurrentTrain: epoch  4, batch    19 | loss: 4.5891471CurrentTrain: epoch  4, batch    20 | loss: 4.7267885CurrentTrain: epoch  4, batch    21 | loss: 4.5045776CurrentTrain: epoch  4, batch    22 | loss: 4.8099384CurrentTrain: epoch  4, batch    23 | loss: 4.5774112CurrentTrain: epoch  4, batch    24 | loss: 4.4449902CurrentTrain: epoch  4, batch    25 | loss: 4.5282745CurrentTrain: epoch  4, batch    26 | loss: 4.5538597CurrentTrain: epoch  4, batch    27 | loss: 4.3374228CurrentTrain: epoch  4, batch    28 | loss: 4.7434788CurrentTrain: epoch  4, batch    29 | loss: 4.5013890CurrentTrain: epoch  4, batch    30 | loss: 4.4564662CurrentTrain: epoch  4, batch    31 | loss: 4.9688263CurrentTrain: epoch  4, batch    32 | loss: 4.7383199CurrentTrain: epoch  4, batch    33 | loss: 4.6673956CurrentTrain: epoch  4, batch    34 | loss: 4.4880629CurrentTrain: epoch  4, batch    35 | loss: 4.8178225CurrentTrain: epoch  4, batch    36 | loss: 4.4693375CurrentTrain: epoch  4, batch    37 | loss: 4.4511971CurrentTrain: epoch  4, batch    38 | loss: 4.7182975CurrentTrain: epoch  4, batch    39 | loss: 5.2619753CurrentTrain: epoch  4, batch    40 | loss: 4.4471579CurrentTrain: epoch  4, batch    41 | loss: 4.4209518CurrentTrain: epoch  4, batch    42 | loss: 4.4576578CurrentTrain: epoch  4, batch    43 | loss: 4.7504401CurrentTrain: epoch  4, batch    44 | loss: 4.4173059CurrentTrain: epoch  4, batch    45 | loss: 4.4361916CurrentTrain: epoch  4, batch    46 | loss: 4.3578029CurrentTrain: epoch  4, batch    47 | loss: 4.4274168CurrentTrain: epoch  4, batch    48 | loss: 4.5976686CurrentTrain: epoch  4, batch    49 | loss: 4.3173065CurrentTrain: epoch  4, batch    50 | loss: 4.6773205CurrentTrain: epoch  4, batch    51 | loss: 4.3106289CurrentTrain: epoch  4, batch    52 | loss: 4.4211721CurrentTrain: epoch  4, batch    53 | loss: 4.4155550CurrentTrain: epoch  4, batch    54 | loss: 4.6843529CurrentTrain: epoch  4, batch    55 | loss: 4.3852797CurrentTrain: epoch  4, batch    56 | loss: 4.3080373CurrentTrain: epoch  4, batch    57 | loss: 4.1609879CurrentTrain: epoch  4, batch    58 | loss: 4.3871732CurrentTrain: epoch  4, batch    59 | loss: 4.3347135CurrentTrain: epoch  4, batch    60 | loss: 4.6457882CurrentTrain: epoch  4, batch    61 | loss: 4.4420810CurrentTrain: epoch  4, batch    62 | loss: 4.2473497CurrentTrain: epoch  5, batch     0 | loss: 4.4041233CurrentTrain: epoch  5, batch     1 | loss: 4.5511618CurrentTrain: epoch  5, batch     2 | loss: 4.4610701CurrentTrain: epoch  5, batch     3 | loss: 4.3177381CurrentTrain: epoch  5, batch     4 | loss: 4.4426522CurrentTrain: epoch  5, batch     5 | loss: 4.4306111CurrentTrain: epoch  5, batch     6 | loss: 4.3065529CurrentTrain: epoch  5, batch     7 | loss: 4.4895844CurrentTrain: epoch  5, batch     8 | loss: 4.4057131CurrentTrain: epoch  5, batch     9 | loss: 4.2931690CurrentTrain: epoch  5, batch    10 | loss: 4.4318609CurrentTrain: epoch  5, batch    11 | loss: 4.2504711CurrentTrain: epoch  5, batch    12 | loss: 4.3023109CurrentTrain: epoch  5, batch    13 | loss: 5.1122518CurrentTrain: epoch  5, batch    14 | loss: 4.3248143CurrentTrain: epoch  5, batch    15 | loss: 4.3096104CurrentTrain: epoch  5, batch    16 | loss: 4.3719959CurrentTrain: epoch  5, batch    17 | loss: 4.5347366CurrentTrain: epoch  5, batch    18 | loss: 4.4269600CurrentTrain: epoch  5, batch    19 | loss: 4.6696172CurrentTrain: epoch  5, batch    20 | loss: 4.3252659CurrentTrain: epoch  5, batch    21 | loss: 4.4890270CurrentTrain: epoch  5, batch    22 | loss: 4.6169491CurrentTrain: epoch  5, batch    23 | loss: 4.4271016CurrentTrain: epoch  5, batch    24 | loss: 4.4697046CurrentTrain: epoch  5, batch    25 | loss: 4.3600140CurrentTrain: epoch  5, batch    26 | loss: 4.2296114CurrentTrain: epoch  5, batch    27 | loss: 4.2862463CurrentTrain: epoch  5, batch    28 | loss: 4.3556128CurrentTrain: epoch  5, batch    29 | loss: 4.2771254CurrentTrain: epoch  5, batch    30 | loss: 4.2438002CurrentTrain: epoch  5, batch    31 | loss: 4.2969475CurrentTrain: epoch  5, batch    32 | loss: 4.2867522CurrentTrain: epoch  5, batch    33 | loss: 4.2542362CurrentTrain: epoch  5, batch    34 | loss: 4.2758341CurrentTrain: epoch  5, batch    35 | loss: 4.3192697CurrentTrain: epoch  5, batch    36 | loss: 4.2482672CurrentTrain: epoch  5, batch    37 | loss: 4.4036913CurrentTrain: epoch  5, batch    38 | loss: 4.2010584CurrentTrain: epoch  5, batch    39 | loss: 4.3069611CurrentTrain: epoch  5, batch    40 | loss: 4.4458547CurrentTrain: epoch  5, batch    41 | loss: 4.2864475CurrentTrain: epoch  5, batch    42 | loss: 4.5610385CurrentTrain: epoch  5, batch    43 | loss: 4.2047853CurrentTrain: epoch  5, batch    44 | loss: 4.4427953CurrentTrain: epoch  5, batch    45 | loss: 4.2673922CurrentTrain: epoch  5, batch    46 | loss: 4.3305492CurrentTrain: epoch  5, batch    47 | loss: 4.3019495CurrentTrain: epoch  5, batch    48 | loss: 4.2784748CurrentTrain: epoch  5, batch    49 | loss: 4.3247213CurrentTrain: epoch  5, batch    50 | loss: 4.2103825CurrentTrain: epoch  5, batch    51 | loss: 4.2075415CurrentTrain: epoch  5, batch    52 | loss: 4.3143892CurrentTrain: epoch  5, batch    53 | loss: 4.1767654CurrentTrain: epoch  5, batch    54 | loss: 4.1823077CurrentTrain: epoch  5, batch    55 | loss: 4.6182003CurrentTrain: epoch  5, batch    56 | loss: 4.2139983CurrentTrain: epoch  5, batch    57 | loss: 4.2910738CurrentTrain: epoch  5, batch    58 | loss: 4.2031569CurrentTrain: epoch  5, batch    59 | loss: 4.2383461CurrentTrain: epoch  5, batch    60 | loss: 4.2057347CurrentTrain: epoch  5, batch    61 | loss: 4.1955066CurrentTrain: epoch  5, batch    62 | loss: 4.1604815CurrentTrain: epoch  6, batch     0 | loss: 4.3960114CurrentTrain: epoch  6, batch     1 | loss: 4.2375283CurrentTrain: epoch  6, batch     2 | loss: 4.1973009CurrentTrain: epoch  6, batch     3 | loss: 4.1660299CurrentTrain: epoch  6, batch     4 | loss: 4.2154174CurrentTrain: epoch  6, batch     5 | loss: 4.1775084CurrentTrain: epoch  6, batch     6 | loss: 4.2104921CurrentTrain: epoch  6, batch     7 | loss: 4.1785374CurrentTrain: epoch  6, batch     8 | loss: 4.1890888CurrentTrain: epoch  6, batch     9 | loss: 4.2677193CurrentTrain: epoch  6, batch    10 | loss: 4.2268639CurrentTrain: epoch  6, batch    11 | loss: 4.2158747CurrentTrain: epoch  6, batch    12 | loss: 4.2755380CurrentTrain: epoch  6, batch    13 | loss: 4.1290617CurrentTrain: epoch  6, batch    14 | loss: 4.1642013CurrentTrain: epoch  6, batch    15 | loss: 4.2100816CurrentTrain: epoch  6, batch    16 | loss: 4.1139231CurrentTrain: epoch  6, batch    17 | loss: 4.1513510CurrentTrain: epoch  6, batch    18 | loss: 4.2391391CurrentTrain: epoch  6, batch    19 | loss: 4.1126585CurrentTrain: epoch  6, batch    20 | loss: 4.3787303CurrentTrain: epoch  6, batch    21 | loss: 4.0742884CurrentTrain: epoch  6, batch    22 | loss: 4.1353445CurrentTrain: epoch  6, batch    23 | loss: 4.4132895CurrentTrain: epoch  6, batch    24 | loss: 4.2481046CurrentTrain: epoch  6, batch    25 | loss: 4.1920671CurrentTrain: epoch  6, batch    26 | loss: 4.1614838CurrentTrain: epoch  6, batch    27 | loss: 4.1906638CurrentTrain: epoch  6, batch    28 | loss: 4.1967421CurrentTrain: epoch  6, batch    29 | loss: 4.1922626CurrentTrain: epoch  6, batch    30 | loss: 4.1082726CurrentTrain: epoch  6, batch    31 | loss: 4.1195917CurrentTrain: epoch  6, batch    32 | loss: 4.2039723CurrentTrain: epoch  6, batch    33 | loss: 4.1924324CurrentTrain: epoch  6, batch    34 | loss: 4.2154112CurrentTrain: epoch  6, batch    35 | loss: 4.2633085CurrentTrain: epoch  6, batch    36 | loss: 4.2327056CurrentTrain: epoch  6, batch    37 | loss: 4.1997132CurrentTrain: epoch  6, batch    38 | loss: 4.1645637CurrentTrain: epoch  6, batch    39 | loss: 4.2060747CurrentTrain: epoch  6, batch    40 | loss: 4.2435589CurrentTrain: epoch  6, batch    41 | loss: 4.2896709CurrentTrain: epoch  6, batch    42 | loss: 4.1563306CurrentTrain: epoch  6, batch    43 | loss: 4.1975703CurrentTrain: epoch  6, batch    44 | loss: 4.1283698CurrentTrain: epoch  6, batch    45 | loss: 4.2093296CurrentTrain: epoch  6, batch    46 | loss: 4.2299590CurrentTrain: epoch  6, batch    47 | loss: 4.3285313CurrentTrain: epoch  6, batch    48 | loss: 4.2300777CurrentTrain: epoch  6, batch    49 | loss: 4.1956968CurrentTrain: epoch  6, batch    50 | loss: 4.3696747CurrentTrain: epoch  6, batch    51 | loss: 4.2152719CurrentTrain: epoch  6, batch    52 | loss: 4.1259623CurrentTrain: epoch  6, batch    53 | loss: 4.1310301CurrentTrain: epoch  6, batch    54 | loss: 4.1598501CurrentTrain: epoch  6, batch    55 | loss: 4.1167669CurrentTrain: epoch  6, batch    56 | loss: 4.5312757CurrentTrain: epoch  6, batch    57 | loss: 4.1371589CurrentTrain: epoch  6, batch    58 | loss: 4.1890984CurrentTrain: epoch  6, batch    59 | loss: 4.2450743CurrentTrain: epoch  6, batch    60 | loss: 4.2902088CurrentTrain: epoch  6, batch    61 | loss: 4.0764751CurrentTrain: epoch  6, batch    62 | loss: 4.0197849CurrentTrain: epoch  7, batch     0 | loss: 4.1563926CurrentTrain: epoch  7, batch     1 | loss: 4.0819678CurrentTrain: epoch  7, batch     2 | loss: 4.1319685CurrentTrain: epoch  7, batch     3 | loss: 4.1500840CurrentTrain: epoch  7, batch     4 | loss: 4.1970587CurrentTrain: epoch  7, batch     5 | loss: 4.1479063CurrentTrain: epoch  7, batch     6 | loss: 4.2366323CurrentTrain: epoch  7, batch     7 | loss: 4.1377363CurrentTrain: epoch  7, batch     8 | loss: 4.3026352CurrentTrain: epoch  7, batch     9 | loss: 4.1041584CurrentTrain: epoch  7, batch    10 | loss: 4.1611004CurrentTrain: epoch  7, batch    11 | loss: 4.1347985CurrentTrain: epoch  7, batch    12 | loss: 4.1751862CurrentTrain: epoch  7, batch    13 | loss: 4.0876932CurrentTrain: epoch  7, batch    14 | loss: 4.1778927CurrentTrain: epoch  7, batch    15 | loss: 4.1319404CurrentTrain: epoch  7, batch    16 | loss: 4.1003494CurrentTrain: epoch  7, batch    17 | loss: 4.1294403CurrentTrain: epoch  7, batch    18 | loss: 4.1845317CurrentTrain: epoch  7, batch    19 | loss: 4.0992990CurrentTrain: epoch  7, batch    20 | loss: 4.2002349CurrentTrain: epoch  7, batch    21 | loss: 4.1552734CurrentTrain: epoch  7, batch    22 | loss: 4.1309052CurrentTrain: epoch  7, batch    23 | loss: 4.1362944CurrentTrain: epoch  7, batch    24 | loss: 4.1316881CurrentTrain: epoch  7, batch    25 | loss: 4.0695858CurrentTrain: epoch  7, batch    26 | loss: 4.1511602CurrentTrain: epoch  7, batch    27 | loss: 4.1290603CurrentTrain: epoch  7, batch    28 | loss: 4.1237683CurrentTrain: epoch  7, batch    29 | loss: 4.0954690CurrentTrain: epoch  7, batch    30 | loss: 4.1796513CurrentTrain: epoch  7, batch    31 | loss: 4.2281222CurrentTrain: epoch  7, batch    32 | loss: 4.1120195CurrentTrain: epoch  7, batch    33 | loss: 4.1253281CurrentTrain: epoch  7, batch    34 | loss: 4.1471128CurrentTrain: epoch  7, batch    35 | loss: 4.1631918CurrentTrain: epoch  7, batch    36 | loss: 4.2082253CurrentTrain: epoch  7, batch    37 | loss: 4.1760349CurrentTrain: epoch  7, batch    38 | loss: 4.1692696CurrentTrain: epoch  7, batch    39 | loss: 4.0753059CurrentTrain: epoch  7, batch    40 | loss: 4.2072906CurrentTrain: epoch  7, batch    41 | loss: 4.1816344CurrentTrain: epoch  7, batch    42 | loss: 4.1505451CurrentTrain: epoch  7, batch    43 | loss: 4.1115670CurrentTrain: epoch  7, batch    44 | loss: 4.1055841CurrentTrain: epoch  7, batch    45 | loss: 4.1115656CurrentTrain: epoch  7, batch    46 | loss: 4.0741816CurrentTrain: epoch  7, batch    47 | loss: 4.1408272CurrentTrain: epoch  7, batch    48 | loss: 4.0947094CurrentTrain: epoch  7, batch    49 | loss: 4.1867628CurrentTrain: epoch  7, batch    50 | loss: 4.1052370CurrentTrain: epoch  7, batch    51 | loss: 4.1588712CurrentTrain: epoch  7, batch    52 | loss: 4.1036606CurrentTrain: epoch  7, batch    53 | loss: 4.2084637CurrentTrain: epoch  7, batch    54 | loss: 4.1326284CurrentTrain: epoch  7, batch    55 | loss: 4.1335220CurrentTrain: epoch  7, batch    56 | loss: 4.0699034CurrentTrain: epoch  7, batch    57 | loss: 4.0541286CurrentTrain: epoch  7, batch    58 | loss: 4.2645664CurrentTrain: epoch  7, batch    59 | loss: 4.5137196CurrentTrain: epoch  7, batch    60 | loss: 4.1093321CurrentTrain: epoch  7, batch    61 | loss: 4.1173463CurrentTrain: epoch  7, batch    62 | loss: 4.1052322CurrentTrain: epoch  8, batch     0 | loss: 4.0975728CurrentTrain: epoch  8, batch     1 | loss: 4.0863972CurrentTrain: epoch  8, batch     2 | loss: 4.1076813CurrentTrain: epoch  8, batch     3 | loss: 4.1086116CurrentTrain: epoch  8, batch     4 | loss: 4.1138601CurrentTrain: epoch  8, batch     5 | loss: 4.0528617CurrentTrain: epoch  8, batch     6 | loss: 4.1148582CurrentTrain: epoch  8, batch     7 | loss: 4.0689392CurrentTrain: epoch  8, batch     8 | loss: 4.1632895CurrentTrain: epoch  8, batch     9 | loss: 4.0851631CurrentTrain: epoch  8, batch    10 | loss: 4.0843124CurrentTrain: epoch  8, batch    11 | loss: 4.1562071CurrentTrain: epoch  8, batch    12 | loss: 4.0921345CurrentTrain: epoch  8, batch    13 | loss: 4.1331944CurrentTrain: epoch  8, batch    14 | loss: 4.1125641CurrentTrain: epoch  8, batch    15 | loss: 4.0850868CurrentTrain: epoch  8, batch    16 | loss: 4.1162338CurrentTrain: epoch  8, batch    17 | loss: 4.1193562CurrentTrain: epoch  8, batch    18 | loss: 4.1344271CurrentTrain: epoch  8, batch    19 | loss: 4.0832577CurrentTrain: epoch  8, batch    20 | loss: 4.1579437CurrentTrain: epoch  8, batch    21 | loss: 4.1169434CurrentTrain: epoch  8, batch    22 | loss: 4.0908046CurrentTrain: epoch  8, batch    23 | loss: 3.9966383CurrentTrain: epoch  8, batch    24 | loss: 4.0772076CurrentTrain: epoch  8, batch    25 | loss: 4.3103075CurrentTrain: epoch  8, batch    26 | loss: 4.1203284CurrentTrain: epoch  8, batch    27 | loss: 4.1111732CurrentTrain: epoch  8, batch    28 | loss: 4.0534563CurrentTrain: epoch  8, batch    29 | loss: 4.0081744CurrentTrain: epoch  8, batch    30 | loss: 4.0624290CurrentTrain: epoch  8, batch    31 | loss: 4.0515327CurrentTrain: epoch  8, batch    32 | loss: 4.0879774CurrentTrain: epoch  8, batch    33 | loss: 4.1260104CurrentTrain: epoch  8, batch    34 | loss: 4.1401467CurrentTrain: epoch  8, batch    35 | loss: 4.0542622CurrentTrain: epoch  8, batch    36 | loss: 4.1153493CurrentTrain: epoch  8, batch    37 | loss: 4.1532183CurrentTrain: epoch  8, batch    38 | loss: 4.0666733CurrentTrain: epoch  8, batch    39 | loss: 4.1065979CurrentTrain: epoch  8, batch    40 | loss: 4.0535994CurrentTrain: epoch  8, batch    41 | loss: 4.0859685CurrentTrain: epoch  8, batch    42 | loss: 4.0880508CurrentTrain: epoch  8, batch    43 | loss: 4.0569777CurrentTrain: epoch  8, batch    44 | loss: 4.1314411CurrentTrain: epoch  8, batch    45 | loss: 4.2752185CurrentTrain: epoch  8, batch    46 | loss: 4.0819011CurrentTrain: epoch  8, batch    47 | loss: 4.0441656CurrentTrain: epoch  8, batch    48 | loss: 4.0257549CurrentTrain: epoch  8, batch    49 | loss: 4.0861912CurrentTrain: epoch  8, batch    50 | loss: 4.0745263CurrentTrain: epoch  8, batch    51 | loss: 4.0664997CurrentTrain: epoch  8, batch    52 | loss: 4.0781765CurrentTrain: epoch  8, batch    53 | loss: 4.1032438CurrentTrain: epoch  8, batch    54 | loss: 4.0929828CurrentTrain: epoch  8, batch    55 | loss: 4.0385261CurrentTrain: epoch  8, batch    56 | loss: 4.0965805CurrentTrain: epoch  8, batch    57 | loss: 4.1261120CurrentTrain: epoch  8, batch    58 | loss: 4.0795956CurrentTrain: epoch  8, batch    59 | loss: 4.0909052CurrentTrain: epoch  8, batch    60 | loss: 4.0853586CurrentTrain: epoch  8, batch    61 | loss: 4.0406847CurrentTrain: epoch  8, batch    62 | loss: 4.0455952CurrentTrain: epoch  9, batch     0 | loss: 4.0373926CurrentTrain: epoch  9, batch     1 | loss: 4.0240850CurrentTrain: epoch  9, batch     2 | loss: 4.0574083CurrentTrain: epoch  9, batch     3 | loss: 4.0656080CurrentTrain: epoch  9, batch     4 | loss: 4.0559897CurrentTrain: epoch  9, batch     5 | loss: 4.0920787CurrentTrain: epoch  9, batch     6 | loss: 4.0305090CurrentTrain: epoch  9, batch     7 | loss: 4.0581565CurrentTrain: epoch  9, batch     8 | loss: 4.0323844CurrentTrain: epoch  9, batch     9 | loss: 4.1131363CurrentTrain: epoch  9, batch    10 | loss: 4.0882559CurrentTrain: epoch  9, batch    11 | loss: 4.0775299CurrentTrain: epoch  9, batch    12 | loss: 4.0570383CurrentTrain: epoch  9, batch    13 | loss: 4.0532041CurrentTrain: epoch  9, batch    14 | loss: 4.0321226CurrentTrain: epoch  9, batch    15 | loss: 4.0289955CurrentTrain: epoch  9, batch    16 | loss: 4.0810194CurrentTrain: epoch  9, batch    17 | loss: 4.0548725CurrentTrain: epoch  9, batch    18 | loss: 4.0372438CurrentTrain: epoch  9, batch    19 | loss: 4.0112534CurrentTrain: epoch  9, batch    20 | loss: 4.0470557CurrentTrain: epoch  9, batch    21 | loss: 4.1547065CurrentTrain: epoch  9, batch    22 | loss: 4.0457792CurrentTrain: epoch  9, batch    23 | loss: 4.0594521CurrentTrain: epoch  9, batch    24 | loss: 4.0217524CurrentTrain: epoch  9, batch    25 | loss: 4.0364876CurrentTrain: epoch  9, batch    26 | loss: 4.0116987CurrentTrain: epoch  9, batch    27 | loss: 4.0730314CurrentTrain: epoch  9, batch    28 | loss: 4.0236311CurrentTrain: epoch  9, batch    29 | loss: 4.0874610CurrentTrain: epoch  9, batch    30 | loss: 4.0681210CurrentTrain: epoch  9, batch    31 | loss: 4.0502062CurrentTrain: epoch  9, batch    32 | loss: 4.0701218CurrentTrain: epoch  9, batch    33 | loss: 4.0595493CurrentTrain: epoch  9, batch    34 | loss: 4.4482889CurrentTrain: epoch  9, batch    35 | loss: 4.0558748CurrentTrain: epoch  9, batch    36 | loss: 4.0537753CurrentTrain: epoch  9, batch    37 | loss: 4.0017395CurrentTrain: epoch  9, batch    38 | loss: 4.0841761CurrentTrain: epoch  9, batch    39 | loss: 4.0539198CurrentTrain: epoch  9, batch    40 | loss: 4.0434656CurrentTrain: epoch  9, batch    41 | loss: 3.9868302CurrentTrain: epoch  9, batch    42 | loss: 4.0193486CurrentTrain: epoch  9, batch    43 | loss: 4.0902357CurrentTrain: epoch  9, batch    44 | loss: 4.0624065CurrentTrain: epoch  9, batch    45 | loss: 3.9990277CurrentTrain: epoch  9, batch    46 | loss: 4.0743237CurrentTrain: epoch  9, batch    47 | loss: 4.0442991CurrentTrain: epoch  9, batch    48 | loss: 4.0491238CurrentTrain: epoch  9, batch    49 | loss: 4.0383520CurrentTrain: epoch  9, batch    50 | loss: 4.0441761CurrentTrain: epoch  9, batch    51 | loss: 4.0146337CurrentTrain: epoch  9, batch    52 | loss: 4.0872555CurrentTrain: epoch  9, batch    53 | loss: 4.0104561CurrentTrain: epoch  9, batch    54 | loss: 3.9943166CurrentTrain: epoch  9, batch    55 | loss: 4.0693169CurrentTrain: epoch  9, batch    56 | loss: 4.0173635CurrentTrain: epoch  9, batch    57 | loss: 4.0567145CurrentTrain: epoch  9, batch    58 | loss: 4.0222445CurrentTrain: epoch  9, batch    59 | loss: 4.0378170CurrentTrain: epoch  9, batch    60 | loss: 3.9852717CurrentTrain: epoch  9, batch    61 | loss: 4.0403280CurrentTrain: epoch  9, batch    62 | loss: 4.0194607
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: while the milanese painting scene was dominated by neoclassic painters andrea appiani and luigi sabatelli , giovannni migliara stayed with the historical themes and medieval subtlety of romanticism .
Head Entity: andrea appiani
Tail Entity: neoclassic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: during the early 20th century, the surrealist movement gained prominence, with artists like salvador dalí and max ernst pushing the boundaries of creativity and imagination.  
Head Entity: salvador dalí  
Tail Entity: surrealist  

Relation: movement  
Context: the beat generation was a literary movement that emerged in the 1950s, characterized by authors such as jack kerouac and allen ginsberg who challenged conventional norms.  
Head Entity: jack kerouac  
Tail Entity: beat generation  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: martin linge was born in norddal , møre og romsdal county , norway .
Head Entity: norddal
Tail Entity: møre og romsdal
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: the city of barcelona is situated in the province of catalonia, spain.  
Head Entity: barcelona  
Tail Entity: catalonia  

Relation: located in the administrative territorial entity  
Context: the university of toronto is located in the province of ontario, canada.  
Head Entity: toronto  
Tail Entity: ontario  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the nation as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: fukumura ( along with iikubo haruna ) was appointed a sub - leader of morning musume at tanaka reina 's graduation concert on may 21 , 2013 .
Head Entity: iikubo haruna
Tail Entity: morning musume
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: After years of dedication and hard work, Sarah was finally recognized as a member of the prestigious National Art Society during the annual gala.  
Head Entity: Sarah  
Tail Entity: National Art Society  

Relation: member of  
Context: The talented musician joined the band last year and quickly became a valued member of The Jazz Collective, contributing to their latest album.  
Head Entity: The talented musician  
Tail Entity: The Jazz Collective  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: the solkan bridge ( , ) is a arch bridge over the soča river near nova gorica in western slovenia ( by railway terminology it is a viaduct ) .
Head Entity: solkan bridge
Tail Entity: soča river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: the golden gate bridge ( , ) spans the golden gate strait, the entrance to san francisco bay from the pacific ocean.  
Head Entity: golden gate bridge  
Tail Entity: golden gate strait  

Relation: crosses  
Context: the channel tunnel ( , ) runs beneath the english channel, connecting folkestone in the uk with coquelles near calais in france.  
Head Entity: channel tunnel  
Tail Entity: english channel  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: the koksoak river arises at the confluence of its two main tributaries , the rivière aux mélèzes to the west and the caniapiscau river to the south .
Head Entity: koksoak river
Tail Entity: caniapiscau river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The Missouri River is fed by several tributaries, including the Yellowstone River, which flows in from the north.  
Head Entity: Missouri River  
Tail Entity: Yellowstone River  

Relation: tributary  
Context: The Amazon River has numerous tributaries, one of the largest being the Madeira River, which significantly contributes to its flow.  
Head Entity: Amazon River  
Tail Entity: Madeira River  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: sir arthur stanley , the governor of victoria , and sir edmund barton , a justice of the high court and a former prime minister of australia , were also in attendance .
Head Entity: sir edmund barton
Tail Entity: prime minister of australia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: after serving as the mayor of new york city, bill de blasio was elected as the 109th mayor of the city, succeeding michael bloomberg.  
Head Entity: bill de blasio  
Tail Entity: mayor of new york city  

Relation: position held  
Context: during her tenure, angela merkel served as the chancellor of germany, leading the country through numerous crises and becoming one of the world's most powerful women.  
Head Entity: angela merkel  
Tail Entity: chancellor of germany  
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: section 8 : prejudice is a science fiction , first - person shooter video game developed by timegate studios .
Head Entity: section 8
Tail Entity: timegate studios
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: The popular mobile game Clash of Clans was developed by Supercell, a Finnish video game company.  
Head Entity: Clash of Clans  
Tail Entity: Supercell  

Relation: developer  
Context: The innovative electric vehicle known as the Tesla Model S was developed by Tesla, Inc., which is known for its cutting-edge technology.  
Head Entity: Tesla Model S  
Tail Entity: Tesla, Inc.  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: neptune is the second studio album by the london - based band the duke spirit and the last with the original lead guitarist , dan higgins .
Head Entity: the duke spirit
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the tech startup was founded in a small garage in silicon valley, where innovative ideas flourished.  
Head Entity: the tech startup  
Tail Entity: silicon valley  

Relation: location of formation  
Context: the famous rock band was established in a vibrant music scene in seattle during the early 1990s.  
Head Entity: the famous rock band  
Tail Entity: seattle  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: " shining time station " won a number of awards and significantly increased the popularity of the " thomas " media franchise in the united states .
Head Entity: shining time station
Tail Entity: united states
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous dish "sushi" is traditionally associated with Japan and has gained immense popularity worldwide.  
Head Entity: sushi  
Tail Entity: Japan  

Relation: country of origin  
Context: The iconic brand "Guinness" is known for its rich stout beer, which originated in Ireland and is now enjoyed globally.  
Head Entity: Guinness  
Tail Entity: Ireland  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.79%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 93.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 94.60%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.84%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.79%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.68%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 94.42%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.40%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.92%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.08%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.22%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.39%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.51%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.62%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.73%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.93%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.02%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 96.11%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.20%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.14%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 96.09%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.17%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.20%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 96.03%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.99%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 95.91%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 95.69%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.76%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 95.62%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.70%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.67%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.94%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.79%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 93.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 94.60%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.84%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.79%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.68%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 94.42%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.40%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.92%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.08%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.22%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.39%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.51%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.62%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.73%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.93%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.02%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 96.11%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.20%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.14%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 96.09%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.17%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.20%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 96.03%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.99%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 95.91%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 95.69%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.76%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 95.62%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.70%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.67%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.94%   
cur_acc:  ['0.9494']
his_acc:  ['0.9494']
CurrentTrain: epoch  0, batch     0 | loss: 6.7617860CurrentTrain: epoch  0, batch     1 | loss: 5.8129759CurrentTrain: epoch  0, batch     2 | loss: 5.9303284CurrentTrain: epoch  0, batch     3 | loss: 6.8097248CurrentTrain: epoch  1, batch     0 | loss: 5.4410038CurrentTrain: epoch  1, batch     1 | loss: 6.0467753CurrentTrain: epoch  1, batch     2 | loss: 4.7915559CurrentTrain: epoch  1, batch     3 | loss: 2.2638454CurrentTrain: epoch  2, batch     0 | loss: 4.1524782CurrentTrain: epoch  2, batch     1 | loss: 3.3855271CurrentTrain: epoch  2, batch     2 | loss: 4.5356088CurrentTrain: epoch  2, batch     3 | loss: 2.3129163CurrentTrain: epoch  3, batch     0 | loss: 3.2679260CurrentTrain: epoch  3, batch     1 | loss: 3.6866484CurrentTrain: epoch  3, batch     2 | loss: 3.4993196CurrentTrain: epoch  3, batch     3 | loss: 2.4692090CurrentTrain: epoch  4, batch     0 | loss: 3.7445288CurrentTrain: epoch  4, batch     1 | loss: 2.7856598CurrentTrain: epoch  4, batch     2 | loss: 2.8095338CurrentTrain: epoch  4, batch     3 | loss: 2.8586824CurrentTrain: epoch  5, batch     0 | loss: 3.5943353CurrentTrain: epoch  5, batch     1 | loss: 2.6661358CurrentTrain: epoch  5, batch     2 | loss: 2.3486371CurrentTrain: epoch  5, batch     3 | loss: 2.2537508CurrentTrain: epoch  6, batch     0 | loss: 2.4191017CurrentTrain: epoch  6, batch     1 | loss: 2.4616079CurrentTrain: epoch  6, batch     2 | loss: 2.5501480CurrentTrain: epoch  6, batch     3 | loss: 5.0923634CurrentTrain: epoch  7, batch     0 | loss: 2.2315207CurrentTrain: epoch  7, batch     1 | loss: 2.5714612CurrentTrain: epoch  7, batch     2 | loss: 2.1157789CurrentTrain: epoch  7, batch     3 | loss: 2.5225744CurrentTrain: epoch  8, batch     0 | loss: 2.1054246CurrentTrain: epoch  8, batch     1 | loss: 2.2189493CurrentTrain: epoch  8, batch     2 | loss: 2.6301539CurrentTrain: epoch  8, batch     3 | loss: 2.5566807CurrentTrain: epoch  9, batch     0 | loss: 2.2916720CurrentTrain: epoch  9, batch     1 | loss: 2.1404009CurrentTrain: epoch  9, batch     2 | loss: 2.2246671CurrentTrain: epoch  9, batch     3 | loss: 2.2527089
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: alongside evan durbin and hugh gaitskell , he brought the thinking of john maynard keynes to the labour party , especially in relation to price determination .
Head Entity: hugh gaitskell
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: After serving as the mayor, she became a prominent figure in the local chapter of the Green Party, advocating for environmental policies and community engagement.  
Head Entity: she  
Tail Entity: Green Party  

Relation: member of political party  
Context: During his tenure in the Senate, he was known for his strong support of the Democratic Party's initiatives, often collaborating with other members to push for healthcare reform.  
Head Entity: he  
Tail Entity: Democratic Party  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: the original story of real life escape of betty mahmoody is depicted in the movie " not without my daughter " which itself was based on betty mahmoody 's book of the same name .
Head Entity: not without my daughter
Tail Entity: betty mahmoody
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film "the great gatsby" draws heavily from f. scott fitzgerald's classic novel, capturing the essence of the roaring twenties and the complexities of love and ambition.  
Head Entity: the great gatsby  
Tail Entity: f. scott fitzgerald  

Relation: after a work by  
Context: the animated feature "the lion king" was inspired by shakespeare's play "hamlet," incorporating themes of betrayal, revenge, and the struggle for power.  
Head Entity: the lion king  
Tail Entity: shakespeare
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: the group named itself after the popular children 's book harold and the purple crayon by crockett johnson , whose protagonist , harold , uses a purple crayon to draw his imagination into reality .
Head Entity: crockett johnson
Tail Entity: harold and the purple crayon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: The renowned physicist Albert Einstein is best known for his groundbreaking theory of relativity, which revolutionized our understanding of space, time, and gravity.  
Head Entity: Albert Einstein  
Tail Entity: theory of relativity  

Relation: notable work  
Context: The famous painter Vincent van Gogh created numerous masterpieces, but his most celebrated painting, "Starry Night," remains a significant influence in the art world.  
Head Entity: Vincent van Gogh  
Tail Entity: Starry Night  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: at the request of the stockholm institute for bible translation , mirza khazar translated the bible – the new testament and old testament – into the azerbaijani language .
Head Entity: institute for bible translation
Tail Entity: stockholm
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: the global technology company, headquartered in san francisco, has been a leader in software development for over two decades.  
Head Entity: global technology company  
Tail Entity: san francisco  

Relation: headquarters location  
Context: after relocating its main office to new york city, the fashion brand saw a significant increase in sales and brand recognition.  
Head Entity: fashion brand  
Tail Entity: new york city  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most recognized as the wolf, which belongs to the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of plants, the family Rosaceae encompasses a variety of species, including roses and strawberries, which are categorized under the order Rosales.  
Head Entity: Rosaceae  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: he later collaborated with vito volterra and explored laplace transforms and other parts of functional analysis .
Head Entity: vito volterra
Tail Entity: functional analysis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: Marie Curie conducted pioneering research on radioactivity, which laid the groundwork for advancements in nuclear physics.  
Head Entity: Marie Curie  
Tail Entity: nuclear physics  

Relation: field of work  
Context: The company specializes in artificial intelligence and machine learning, providing innovative solutions for various industries.  
Head Entity: The company  
Tail Entity: artificial intelligence
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: wjmi ( 99.7 fm ) is a radio station licensed to jackson , mississippi , united states , with a mainstream urban musical format .
Head Entity: wjmi
Tail Entity: jackson , mississippi
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: kxyz ( 101.5 fm ) is a radio station licensed to transmit to the city of austin , texas , known for its eclectic music scene.  
Head Entity: kxyz  
Tail Entity: austin , texas  

Relation: licensed to broadcast to  
Context: wxyz-tv is a television station licensed to air programs in the region of phoenix , arizona , serving a diverse audience with local news and entertainment.  
Head Entity: wxyz-tv  
Tail Entity: phoenix , arizona  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: ngc 473 is a lenticular galaxy in the constellation of pisces .
Head Entity: ngc 473
Tail Entity: pisces
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: the orion nebula is located in the constellation of orion .  
Head Entity: orion nebula  
Tail Entity: orion  

Relation: constellation  
Context: the star sirius is part of the constellation canis major .  
Head Entity: sirius  
Tail Entity: canis major  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: services to the island run from san juan 's luis muñoz marín international airport , ceiba airport or isla grande airport ( 20- to 30-minute flight ) .
Head Entity: isla grande airport
Tail Entity: san juan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The city of Barcelona is well-connected to various destinations through the El Prat Airport, which offers flights to numerous European cities.  
Head Entity: El Prat Airport  
Tail Entity: Barcelona  

Relation: place served by transport hub  
Context: The central train station in Zurich provides access to multiple Swiss cities and international destinations, making it a key transport hub in the region.  
Head Entity: Zurich central train station  
Tail Entity: Zurich  
Mixup data size:  199
MixupTrain:  epoch  0, batch     0 | loss: 5.6861707MixupTrain:  epoch  0, batch     1 | loss: 5.0297664MixupTrain:  epoch  0, batch     2 | loss: 6.0755962MixupTrain:  epoch  0, batch     3 | loss: 5.0280990MixupTrain:  epoch  0, batch     4 | loss: 5.1107519MixupTrain:  epoch  0, batch     5 | loss: 5.0368379MixupTrain:  epoch  0, batch     6 | loss: 5.1357284MixupTrain:  epoch  0, batch     7 | loss: 4.6170256MixupTrain:  epoch  0, batch     8 | loss: 4.6120297MixupTrain:  epoch  0, batch     9 | loss: 4.2163027MixupTrain:  epoch  0, batch    10 | loss: 4.3115687MixupTrain:  epoch  0, batch    11 | loss: 4.3335538MixupTrain:  epoch  0, batch    12 | loss: 4.2147829
MemoryTrain:  epoch  0, batch     0 | loss: 4.9553223MemoryTrain:  epoch  0, batch     1 | loss: 4.5195284MemoryTrain:  epoch  0, batch     2 | loss: 3.5019646MemoryTrain:  epoch  0, batch     3 | loss: 3.3967967MemoryTrain:  epoch  1, batch     0 | loss: 3.3833714MemoryTrain:  epoch  1, batch     1 | loss: 2.9693792MemoryTrain:  epoch  1, batch     2 | loss: 4.1714983MemoryTrain:  epoch  1, batch     3 | loss: 3.6912794MemoryTrain:  epoch  2, batch     0 | loss: 2.5433238MemoryTrain:  epoch  2, batch     1 | loss: 2.8452828MemoryTrain:  epoch  2, batch     2 | loss: 2.8328133MemoryTrain:  epoch  2, batch     3 | loss: 3.0329189MemoryTrain:  epoch  3, batch     0 | loss: 1.8662646MemoryTrain:  epoch  3, batch     1 | loss: 2.5742626MemoryTrain:  epoch  3, batch     2 | loss: 2.1012740MemoryTrain:  epoch  3, batch     3 | loss: 3.0918911MemoryTrain:  epoch  4, batch     0 | loss: 1.7991076MemoryTrain:  epoch  4, batch     1 | loss: 2.0624099MemoryTrain:  epoch  4, batch     2 | loss: 3.1904373MemoryTrain:  epoch  4, batch     3 | loss: 1.9897773MemoryTrain:  epoch  5, batch     0 | loss: 2.2064989MemoryTrain:  epoch  5, batch     1 | loss: 1.6381642MemoryTrain:  epoch  5, batch     2 | loss: 2.3826594MemoryTrain:  epoch  5, batch     3 | loss: 1.9897046MemoryTrain:  epoch  6, batch     0 | loss: 1.7866759MemoryTrain:  epoch  6, batch     1 | loss: 2.3760159MemoryTrain:  epoch  6, batch     2 | loss: 1.9080937MemoryTrain:  epoch  6, batch     3 | loss: 1.7328324MemoryTrain:  epoch  7, batch     0 | loss: 2.2308187MemoryTrain:  epoch  7, batch     1 | loss: 1.5103656MemoryTrain:  epoch  7, batch     2 | loss: 1.5393810MemoryTrain:  epoch  7, batch     3 | loss: 1.6905441MemoryTrain:  epoch  8, batch     0 | loss: 1.8169711MemoryTrain:  epoch  8, batch     1 | loss: 1.6412749MemoryTrain:  epoch  8, batch     2 | loss: 1.5089498MemoryTrain:  epoch  8, batch     3 | loss: 1.5817084MemoryTrain:  epoch  9, batch     0 | loss: 1.6209428MemoryTrain:  epoch  9, batch     1 | loss: 1.4994125MemoryTrain:  epoch  9, batch     2 | loss: 1.6628746MemoryTrain:  epoch  9, batch     3 | loss: 1.4271278
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 92.61%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 90.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 90.23%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 89.34%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 88.89%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 88.49%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 86.88%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 86.90%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 86.08%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 84.78%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 84.11%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 83.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.65%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.26%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.34%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.83%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.29%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 86.33%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 86.36%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 86.21%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 86.11%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 85.64%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 85.53%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.90%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 86.59%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 86.90%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 87.21%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 87.78%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 88.04%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 88.30%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.78%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 89.00%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 89.22%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 89.42%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 89.50%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 89.70%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 89.77%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 89.96%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 90.13%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 90.19%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 90.25%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 90.42%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 90.57%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 89.98%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 79.86%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 77.84%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 77.60%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 78.37%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 79.46%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 80.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 81.64%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 82.72%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 84.21%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 84.69%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 85.12%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 84.66%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 84.78%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 84.90%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.58%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 86.38%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 86.64%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.89%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 88.26%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 88.60%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 89.36%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 89.64%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.90%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.40%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 90.70%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 90.77%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 90.83%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 90.56%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 90.49%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 90.43%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 90.50%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 90.32%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 90.14%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 90.21%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 90.16%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 89.66%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 89.62%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 89.69%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 89.66%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 89.83%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 89.79%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 89.96%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 90.12%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 90.04%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 89.96%   [EVAL] batch:   66 | acc: 87.50%,  total acc: 89.93%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 89.98%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 90.04%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 90.09%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 90.23%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 90.36%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 90.50%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 90.54%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 90.58%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 90.30%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 90.26%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 90.38%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 90.11%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 89.92%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 89.81%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 89.63%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 89.38%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 89.14%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 88.90%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 88.59%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 88.22%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 88.21%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 88.34%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 88.47%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 88.60%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 88.72%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 88.84%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 88.96%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 88.82%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 88.87%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 88.79%   [EVAL] batch:   97 | acc: 75.00%,  total acc: 88.65%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 88.70%   [EVAL] batch:   99 | acc: 56.25%,  total acc: 88.38%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 88.49%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 88.60%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 88.71%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 88.82%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 88.93%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 89.03%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 89.14%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 89.24%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 89.33%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 89.43%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 89.53%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 89.62%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 89.71%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 89.80%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 89.92%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 89.96%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 90.04%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 90.13%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 90.21%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 90.19%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 90.27%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 90.35%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 90.42%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 90.45%   
cur_acc:  ['0.9494', '0.8998']
his_acc:  ['0.9494', '0.9045']
CurrentTrain: epoch  0, batch     0 | loss: 6.4584141CurrentTrain: epoch  0, batch     1 | loss: 7.8125224CurrentTrain: epoch  0, batch     2 | loss: 7.0033045CurrentTrain: epoch  0, batch     3 | loss: 5.6606679CurrentTrain: epoch  1, batch     0 | loss: 5.3702269CurrentTrain: epoch  1, batch     1 | loss: 6.3687506CurrentTrain: epoch  1, batch     2 | loss: 6.2946739CurrentTrain: epoch  1, batch     3 | loss: 6.1488848CurrentTrain: epoch  2, batch     0 | loss: 5.0956507CurrentTrain: epoch  2, batch     1 | loss: 5.7189531CurrentTrain: epoch  2, batch     2 | loss: 4.8682647CurrentTrain: epoch  2, batch     3 | loss: 3.9398603CurrentTrain: epoch  3, batch     0 | loss: 4.0403175CurrentTrain: epoch  3, batch     1 | loss: 4.9883332CurrentTrain: epoch  3, batch     2 | loss: 4.5916042CurrentTrain: epoch  3, batch     3 | loss: 7.8707113CurrentTrain: epoch  4, batch     0 | loss: 4.5615396CurrentTrain: epoch  4, batch     1 | loss: 4.0743933CurrentTrain: epoch  4, batch     2 | loss: 4.0867987CurrentTrain: epoch  4, batch     3 | loss: 4.0208578CurrentTrain: epoch  5, batch     0 | loss: 3.8258481CurrentTrain: epoch  5, batch     1 | loss: 4.4307165CurrentTrain: epoch  5, batch     2 | loss: 3.6181273CurrentTrain: epoch  5, batch     3 | loss: 3.4775863CurrentTrain: epoch  6, batch     0 | loss: 3.4628429CurrentTrain: epoch  6, batch     1 | loss: 3.6733770CurrentTrain: epoch  6, batch     2 | loss: 4.5587955CurrentTrain: epoch  6, batch     3 | loss: 3.8069367CurrentTrain: epoch  7, batch     0 | loss: 2.9660876CurrentTrain: epoch  7, batch     1 | loss: 3.1755080CurrentTrain: epoch  7, batch     2 | loss: 4.8748565CurrentTrain: epoch  7, batch     3 | loss: 2.3980994CurrentTrain: epoch  8, batch     0 | loss: 3.7586865CurrentTrain: epoch  8, batch     1 | loss: 3.7101197CurrentTrain: epoch  8, batch     2 | loss: 2.9314485CurrentTrain: epoch  8, batch     3 | loss: 2.2718041CurrentTrain: epoch  9, batch     0 | loss: 3.7133670CurrentTrain: epoch  9, batch     1 | loss: 3.0157924CurrentTrain: epoch  9, batch     2 | loss: 3.0642421CurrentTrain: epoch  9, batch     3 | loss: 2.0966809
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: it is situated in the historic lower lusatia region , about south of beeskow , and north of cottbus .
Head Entity: cottbus
Tail Entity: lower lusatia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The ancient ruins are found on the slopes of the majestic Andes mountains, providing a breathtaking view of the valley below.  
Head Entity: Andes mountains  
Tail Entity: ancient ruins  

Relation: located on terrain feature  
Context: The village is nestled in the heart of the lush Black Forest, surrounded by towering trees and serene lakes.  
Head Entity: Black Forest  
Tail Entity: village  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, held in paris in 2015, saw numerous world leaders and organizations come together to discuss global warming and its impacts.  
Head Entity: united nations climate change conference  
Tail Entity: paris agreement  

Relation: participant of  
Context: the 2022 fifa world cup in qatar featured teams from around the globe competing for the prestigious trophy, showcasing the best of international football.  
Head Entity: 2022 fifa world cup  
Tail Entity: qatar
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: the festival opened with " whiplash " directed by damien chazelle and closed with musical drama " rudderless " directed by william h. macy .
Head Entity: whiplash
Tail Entity: damien chazelle
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: the critically acclaimed series "breaking bad" was brought to life by the visionary director vince gilligan, while the spin-off "better call saul" continued his legacy under the direction of peter gould.  
Head Entity: breaking bad  
Tail Entity: vince gilligan  

Relation: director  
Context: the animated feature "spider-man: into the spider-verse" was directed by bob persichetti, peter ramsey, and rodney rothman, showcasing a unique visual style that captivated audiences.  
Head Entity: spider-man: into the spider-verse  
Tail Entity: bob persichetti  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: as of april 2013 , sony isp , known as so - net , released a new fibre service to tokyo for small businesses and homes and also made it available to six surrounding prefectures .
Head Entity: so - net
Tail Entity: sony
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: In 2020, the popular video game developer, Bungie, announced that it had become an independent studio, having been owned by Activision for several years prior.  
Head Entity: Bungie  
Tail Entity: Activision  

Relation: owned by  
Context: The luxury fashion brand Gucci, known for its high-end products, is owned by the French conglomerate Kering, which also owns several other prestigious labels.  
Head Entity: Gucci  
Tail Entity: Kering  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: remains of taxa that can be confidently assigned to istiodactylidae have been found in the uk and china , in rocks dating from the early cretaceous period ( barremian to aptian stage ) .
Head Entity: barremian
Tail Entity: early cretaceous
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The Great Wall of China is a series of fortifications made of various materials, and it is part of the UNESCO World Heritage Sites.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Sites  

Relation: part of  
Context: The human heart is a vital organ that is part of the circulatory system, responsible for pumping blood throughout the body.  
Head Entity: human heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: it served as the home of k league side gyeongnam fc and national league side changwon city fc prior to the construction of the changwon football center in 2009 .
Head Entity: changwon football center
Tail Entity: gyeongnam fc
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was once the residence of the famous artist Pablo Picasso before it was converted into a museum.  
Head Entity: museum  
Tail Entity: Pablo Picasso  

Relation: occupant  
Context: After the renovation, the old factory became the headquarters for a tech startup that focuses on artificial intelligence solutions.  
Head Entity: tech startup  
Tail Entity: old factory  
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: reading town hall was built in several phases between 1786 and 1897 , although the principal facade was designed by alfred waterhouse in 1875 .
Head Entity: reading town hall
Tail Entity: alfred waterhouse
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: the new art museum was designed by renowned architect zaha hadid, whose innovative style has transformed modern architecture.  
Head Entity: new art museum  
Tail Entity: zaha hadid  

Relation: architect  
Context: the iconic sydney opera house was the brainchild of architect jørn utzon, who won the design competition in 1957.  
Head Entity: sydney opera house  
Tail Entity: jørn utzon  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: fantcha was born as francelina durão almeida in mindelo on são vicente island , cape verde .
Head Entity: fantcha
Tail Entity: são vicente island
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: after living in new york city for several years, john decided to move back to his hometown in los angeles.  
Head Entity: john  
Tail Entity: los angeles  

Relation: residence  
Context: the famous author spent most of her life in a quaint cottage located in the picturesque village of haworth.  
Head Entity: the famous author  
Tail Entity: haworth  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Ludwig van Beethoven created some of the most celebrated symphonies in classical music history.  
Head Entity: symphonies  
Tail Entity: Ludwig van Beethoven  

Relation: composer  
Context: Wolfgang Amadeus Mozart was a prolific and influential composer of the classical era, known for his operas and symphonies.  
Head Entity: operas  
Tail Entity: Wolfgang Amadeus Mozart  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: one polish archer qualified for the women 's individual recurve by obtaining one of the eight olympic places available from the 2015 world archery championships in copenhagen , denmark .
Head Entity: 2015 world archery championships
Tail Entity: copenhagen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: The annual music festival will take place in the vibrant city of Austin, Texas, attracting thousands of fans from around the world.  
Head Entity: annual music festival  
Tail Entity: Austin, Texas  

Relation: location  
Context: The historic battle was fought near the banks of the Potomac River, a significant site in American history.  
Head Entity: historic battle  
Tail Entity: Potomac River  
Mixup data size:  259
MixupTrain:  epoch  0, batch     0 | loss: 3.1870203MixupTrain:  epoch  0, batch     1 | loss: 3.1682096MixupTrain:  epoch  0, batch     2 | loss: 3.3284232MixupTrain:  epoch  0, batch     3 | loss: 2.7748493MixupTrain:  epoch  0, batch     4 | loss: 3.8950827MixupTrain:  epoch  0, batch     5 | loss: 3.1864699MixupTrain:  epoch  0, batch     6 | loss: 3.7145605MixupTrain:  epoch  0, batch     7 | loss: 3.3252765MixupTrain:  epoch  0, batch     8 | loss: 3.6079709MixupTrain:  epoch  0, batch     9 | loss: 2.8190518MixupTrain:  epoch  0, batch    10 | loss: 3.3456177MixupTrain:  epoch  0, batch    11 | loss: 2.6584690MixupTrain:  epoch  0, batch    12 | loss: 2.7185142MixupTrain:  epoch  0, batch    13 | loss: 2.8798070MixupTrain:  epoch  0, batch    14 | loss: 3.9371526MixupTrain:  epoch  0, batch    15 | loss: 2.4907183MixupTrain:  epoch  0, batch    16 | loss: 2.3173881
MemoryTrain:  epoch  0, batch     0 | loss: 2.7304189MemoryTrain:  epoch  0, batch     1 | loss: 3.3622794MemoryTrain:  epoch  0, batch     2 | loss: 4.2891221MemoryTrain:  epoch  0, batch     3 | loss: 3.5788734MemoryTrain:  epoch  0, batch     4 | loss: 3.6265311MemoryTrain:  epoch  0, batch     5 | loss: 3.0405710MemoryTrain:  epoch  1, batch     0 | loss: 3.0176640MemoryTrain:  epoch  1, batch     1 | loss: 2.8193870MemoryTrain:  epoch  1, batch     2 | loss: 3.5756199MemoryTrain:  epoch  1, batch     3 | loss: 2.8616667MemoryTrain:  epoch  1, batch     4 | loss: 3.1826973MemoryTrain:  epoch  1, batch     5 | loss: 2.6927493MemoryTrain:  epoch  2, batch     0 | loss: 3.0856733MemoryTrain:  epoch  2, batch     1 | loss: 2.1096749MemoryTrain:  epoch  2, batch     2 | loss: 2.9245040MemoryTrain:  epoch  2, batch     3 | loss: 2.9684620MemoryTrain:  epoch  2, batch     4 | loss: 2.7117393MemoryTrain:  epoch  2, batch     5 | loss: 2.0703130MemoryTrain:  epoch  3, batch     0 | loss: 2.3249943MemoryTrain:  epoch  3, batch     1 | loss: 2.1056631MemoryTrain:  epoch  3, batch     2 | loss: 2.4931881MemoryTrain:  epoch  3, batch     3 | loss: 2.1673656MemoryTrain:  epoch  3, batch     4 | loss: 2.5453086MemoryTrain:  epoch  3, batch     5 | loss: 2.0947404MemoryTrain:  epoch  4, batch     0 | loss: 2.3176138MemoryTrain:  epoch  4, batch     1 | loss: 2.1174412MemoryTrain:  epoch  4, batch     2 | loss: 2.6643147MemoryTrain:  epoch  4, batch     3 | loss: 1.9556402MemoryTrain:  epoch  4, batch     4 | loss: 1.9249600MemoryTrain:  epoch  4, batch     5 | loss: 1.8477329MemoryTrain:  epoch  5, batch     0 | loss: 2.3699074MemoryTrain:  epoch  5, batch     1 | loss: 1.8920476MemoryTrain:  epoch  5, batch     2 | loss: 2.3294954MemoryTrain:  epoch  5, batch     3 | loss: 2.2707572MemoryTrain:  epoch  5, batch     4 | loss: 1.7780901MemoryTrain:  epoch  5, batch     5 | loss: 1.6139072MemoryTrain:  epoch  6, batch     0 | loss: 2.3254678MemoryTrain:  epoch  6, batch     1 | loss: 1.7165790MemoryTrain:  epoch  6, batch     2 | loss: 1.9690776MemoryTrain:  epoch  6, batch     3 | loss: 1.9685247MemoryTrain:  epoch  6, batch     4 | loss: 1.8479700MemoryTrain:  epoch  6, batch     5 | loss: 2.1689975MemoryTrain:  epoch  7, batch     0 | loss: 2.1619115MemoryTrain:  epoch  7, batch     1 | loss: 2.1954865MemoryTrain:  epoch  7, batch     2 | loss: 1.6508446MemoryTrain:  epoch  7, batch     3 | loss: 1.8131042MemoryTrain:  epoch  7, batch     4 | loss: 1.5895247MemoryTrain:  epoch  7, batch     5 | loss: 1.8687710MemoryTrain:  epoch  8, batch     0 | loss: 1.7637200MemoryTrain:  epoch  8, batch     1 | loss: 1.9085721MemoryTrain:  epoch  8, batch     2 | loss: 1.9947876MemoryTrain:  epoch  8, batch     3 | loss: 1.8810560MemoryTrain:  epoch  8, batch     4 | loss: 1.4559577MemoryTrain:  epoch  8, batch     5 | loss: 1.5789230MemoryTrain:  epoch  9, batch     0 | loss: 1.8435904MemoryTrain:  epoch  9, batch     1 | loss: 1.8188225MemoryTrain:  epoch  9, batch     2 | loss: 1.5919785MemoryTrain:  epoch  9, batch     3 | loss: 1.8633900MemoryTrain:  epoch  9, batch     4 | loss: 1.4355843MemoryTrain:  epoch  9, batch     5 | loss: 1.4451246
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 33.75%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 33.33%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 39.29%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 45.31%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 51.39%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 60.23%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 63.02%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 65.38%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 67.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 69.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 72.06%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 73.26%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 73.03%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 72.19%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 70.24%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 70.74%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 70.11%   [EVAL] batch:   23 | acc: 50.00%,  total acc: 69.27%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 66.35%   [EVAL] batch:   26 | acc: 31.25%,  total acc: 65.05%   [EVAL] batch:   27 | acc: 12.50%,  total acc: 63.17%   [EVAL] batch:   28 | acc: 18.75%,  total acc: 61.64%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 60.00%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 58.27%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 58.40%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 59.47%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 60.11%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 61.07%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 61.63%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 62.16%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 62.83%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 63.46%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 64.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 65.24%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 65.33%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 66.13%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 66.76%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 67.36%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 67.80%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 67.82%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 68.23%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 68.62%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 68.88%   [EVAL] batch:   50 | acc: 31.25%,  total acc: 68.14%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 67.79%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 67.33%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 67.01%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 67.05%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 66.96%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 66.45%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 66.06%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 65.68%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 65.37%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 65.52%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 64.88%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 77.27%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 76.04%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 76.92%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 81.62%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.44%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 83.52%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 83.70%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 83.85%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 84.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.19%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 85.49%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.78%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.69%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.11%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 87.87%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 88.04%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 88.37%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.68%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.98%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.26%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.79%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.03%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 90.26%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 90.48%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 90.56%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 90.49%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 90.29%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 90.36%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 90.43%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 90.50%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 90.20%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 90.02%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 89.86%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 89.58%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 88.98%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 88.84%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 88.82%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 88.79%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 88.98%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 88.96%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 89.14%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 89.31%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 89.38%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 89.36%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 89.42%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 89.46%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 89.61%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 89.67%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 89.46%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 89.17%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 88.89%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 88.61%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 88.43%   [EVAL] batch:   74 | acc: 43.75%,  total acc: 87.83%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 87.83%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 87.82%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 87.98%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 87.74%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 87.66%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 87.27%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 87.05%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 86.76%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 86.32%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 85.97%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 85.78%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 85.72%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 85.88%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 86.04%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 86.20%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 86.35%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 86.49%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 86.64%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 86.58%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 86.52%   [EVAL] batch:   96 | acc: 75.00%,  total acc: 86.40%   [EVAL] batch:   97 | acc: 75.00%,  total acc: 86.29%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 86.30%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 86.06%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 86.20%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 86.34%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 86.47%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 86.60%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 86.73%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 86.79%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 86.92%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 87.04%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 87.16%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 87.27%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 87.39%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 87.61%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 87.77%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 87.88%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 87.98%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 88.08%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 88.13%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 88.18%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 88.17%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 88.22%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 88.31%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 88.36%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 88.40%   [EVAL] batch:  125 | acc: 18.75%,  total acc: 87.85%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 87.50%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 87.06%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 86.63%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 86.30%   [EVAL] batch:  130 | acc: 31.25%,  total acc: 85.88%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 85.80%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 85.81%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 85.91%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 86.02%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 86.12%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 86.18%   [EVAL] batch:  137 | acc: 93.75%,  total acc: 86.23%   [EVAL] batch:  138 | acc: 93.75%,  total acc: 86.29%   [EVAL] batch:  139 | acc: 93.75%,  total acc: 86.34%   [EVAL] batch:  140 | acc: 100.00%,  total acc: 86.44%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 86.44%   [EVAL] batch:  142 | acc: 93.75%,  total acc: 86.49%   [EVAL] batch:  143 | acc: 68.75%,  total acc: 86.37%   [EVAL] batch:  144 | acc: 56.25%,  total acc: 86.16%   [EVAL] batch:  145 | acc: 31.25%,  total acc: 85.79%   [EVAL] batch:  146 | acc: 81.25%,  total acc: 85.76%   [EVAL] batch:  147 | acc: 56.25%,  total acc: 85.56%   [EVAL] batch:  148 | acc: 50.00%,  total acc: 85.32%   [EVAL] batch:  149 | acc: 56.25%,  total acc: 85.12%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 84.60%   [EVAL] batch:  151 | acc: 31.25%,  total acc: 84.25%   [EVAL] batch:  152 | acc: 12.50%,  total acc: 83.78%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 83.36%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 82.90%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 82.41%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 82.29%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 82.36%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 82.35%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 82.42%   [EVAL] batch:  160 | acc: 81.25%,  total acc: 82.41%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 82.41%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 82.44%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 82.47%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 82.58%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 82.68%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 82.60%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 82.70%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 82.77%   [EVAL] batch:  169 | acc: 93.75%,  total acc: 82.83%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 82.86%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 82.78%   [EVAL] batch:  172 | acc: 87.50%,  total acc: 82.80%   [EVAL] batch:  173 | acc: 87.50%,  total acc: 82.83%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 82.82%   [EVAL] batch:  175 | acc: 31.25%,  total acc: 82.53%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 82.34%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 82.13%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 81.95%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 81.88%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 81.77%   [EVAL] batch:  181 | acc: 37.50%,  total acc: 81.52%   [EVAL] batch:  182 | acc: 43.75%,  total acc: 81.32%   [EVAL] batch:  183 | acc: 43.75%,  total acc: 81.11%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 81.01%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 80.85%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 80.82%   [EVAL] batch:  187 | acc: 25.00%,  total acc: 80.52%   
cur_acc:  ['0.9494', '0.8998', '0.6488']
his_acc:  ['0.9494', '0.9045', '0.8052']
CurrentTrain: epoch  0, batch     0 | loss: 5.2245731CurrentTrain: epoch  0, batch     1 | loss: 5.4072843CurrentTrain: epoch  0, batch     2 | loss: 6.2503295CurrentTrain: epoch  0, batch     3 | loss: 4.5495276CurrentTrain: epoch  1, batch     0 | loss: 5.1240897CurrentTrain: epoch  1, batch     1 | loss: 4.3200388CurrentTrain: epoch  1, batch     2 | loss: 4.4834447CurrentTrain: epoch  1, batch     3 | loss: 4.9593315CurrentTrain: epoch  2, batch     0 | loss: 4.0483265CurrentTrain: epoch  2, batch     1 | loss: 4.2632570CurrentTrain: epoch  2, batch     2 | loss: 3.9835911CurrentTrain: epoch  2, batch     3 | loss: 6.6311746CurrentTrain: epoch  3, batch     0 | loss: 3.4295745CurrentTrain: epoch  3, batch     1 | loss: 3.9443140CurrentTrain: epoch  3, batch     2 | loss: 4.0662737CurrentTrain: epoch  3, batch     3 | loss: 5.4298811CurrentTrain: epoch  4, batch     0 | loss: 2.9661937CurrentTrain: epoch  4, batch     1 | loss: 3.3503723CurrentTrain: epoch  4, batch     2 | loss: 3.6392541CurrentTrain: epoch  4, batch     3 | loss: 2.4741805CurrentTrain: epoch  5, batch     0 | loss: 2.9506776CurrentTrain: epoch  5, batch     1 | loss: 2.7342987CurrentTrain: epoch  5, batch     2 | loss: 3.7407734CurrentTrain: epoch  5, batch     3 | loss: 2.9935350CurrentTrain: epoch  6, batch     0 | loss: 2.6756430CurrentTrain: epoch  6, batch     1 | loss: 2.6557183CurrentTrain: epoch  6, batch     2 | loss: 3.2966566CurrentTrain: epoch  6, batch     3 | loss: 2.4802098CurrentTrain: epoch  7, batch     0 | loss: 3.0530987CurrentTrain: epoch  7, batch     1 | loss: 2.3721349CurrentTrain: epoch  7, batch     2 | loss: 3.0348797CurrentTrain: epoch  7, batch     3 | loss: 2.5493922CurrentTrain: epoch  8, batch     0 | loss: 2.9096198CurrentTrain: epoch  8, batch     1 | loss: 2.4282322CurrentTrain: epoch  8, batch     2 | loss: 2.6177511CurrentTrain: epoch  8, batch     3 | loss: 3.0624788CurrentTrain: epoch  9, batch     0 | loss: 2.6676409CurrentTrain: epoch  9, batch     1 | loss: 2.6043770CurrentTrain: epoch  9, batch     2 | loss: 2.1808624CurrentTrain: epoch  9, batch     3 | loss: 1.8243339
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the cantons of créteil are administrative divisions of the val - de - marne department , île - de - france region , northern france .
Head Entity: île - de - france
Tail Entity: val - de - marne
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The provinces of Canada are significant administrative divisions within the country, with Ontario being one of the largest provinces.  
Head Entity: Canada  
Tail Entity: Ontario  

Relation: contains administrative territorial entity  
Context: The state of California is divided into several counties, including Los Angeles County, which is one of the most populous counties in the United States.  
Head Entity: California  
Tail Entity: Los Angeles County  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: heinz , is a german given name , a diminutive of heinrich and cognate of the given name henry .
Head Entity: heinrich
Tail Entity: henry
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as they share a common ancestor and many genetic traits.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: In various historical texts, the ancient city of Byzantium is said to be the same as the later known Constantinople, though some scholars debate this.  
Head Entity: Byzantium  
Tail Entity: Constantinople  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: in 2010 , ty burrell received a nomination for the television critics association award for individual achievement in comedy , winning in 2011 .
Head Entity: television critics association award for individual achievement in comedy
Tail Entity: ty burrell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: In 2022, the Boston Red Sox triumphed in the World Series, marking their fourth championship in the last two decades.  
Head Entity: World Series  
Tail Entity: Boston Red Sox  

Relation: winner  
Context: At the annual film festival, the documentary "Voices of Change" was awarded the Best Documentary prize, highlighting its impactful storytelling.  
Head Entity: Best Documentary prize  
Tail Entity: "Voices of Change"  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: the calais region was defended by the 15th army under " generaloberst " ( colonel general ) hans von salmuth , and normandy by the 7th army commanded by " generaloberst " friedrich dollmann .
Head Entity: hans von salmuth
Tail Entity: generaloberst
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: during the battle of stalingrad, the soviet forces were led by general vasily chuikov, who held the rank of major general, while the german troops were commanded by general friedrich paulus.  
Head Entity: vasily chuikov  
Tail Entity: major general  

Relation: military rank  
Context: admiral horatio nelson was a key figure in the british navy during the napoleonic wars, famously holding the rank of vice admiral at the time of the battle of trafalgar.  
Head Entity: horatio nelson  
Tail Entity: vice admiral  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: tokyopop also released a collection of karin short stories under the title : " chibi vampire : airmail " on august 31 , 2010 .
Head Entity: karin
Tail Entity: tokyopop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: Penguin Random House announced the release of a new novel by the acclaimed author John Grisham, set to hit the shelves on June 15, 2021.  
Head Entity: John Grisham  
Tail Entity: Penguin Random House  

Relation: publisher  
Context: The popular online platform Wattpad has been instrumental in launching the careers of many writers, including Anna Todd, whose series "After" gained immense popularity.  
Head Entity: Anna Todd  
Tail Entity: Wattpad  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " choose you " and " homesick " were released as the album 's second and third singles , respectively , and each attained moderate chart success .
Head Entity: choose you
Tail Entity: homesick
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first chapter of the book introduces the main character, while the second chapter delves into the backstory.  
Head Entity: first chapter  
Tail Entity: second chapter  

Relation: followed by  
Context: The opening act of the concert was a local band, followed by a well-known pop artist who energized the crowd.  
Head Entity: local band  
Tail Entity: well-known pop artist  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: there is a memoir of his life and work by gustav queck ( jena , 1852 ) .
Head Entity: gustav queck
Tail Entity: jena
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: The headquarters of the company is situated in San Francisco, where they have been operating since 2010.  
Head Entity: the company  
Tail Entity: San Francisco  

Relation: work location  
Context: During her time at the university, she conducted research in various labs located in Cambridge.  
Head Entity: she  
Tail Entity: Cambridge  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in enzyme catalysis.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a talented software engineer, has been working on innovative artificial intelligence projects for over a decade.  
Head Entity: john smith  
Tail Entity: software engineer  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: the george boxley cabin , davenport - bradfield house , and sheridan downtown commercial historic district are listed on the national register of historic places .
Head Entity: sheridan downtown commercial historic district
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: the ancient city of petra in jordan is recognized as a unesco world heritage site due to its archaeological significance.  
Head Entity: ancient city of petra  
Tail Entity: unesco world heritage site  

Relation: heritage designation  
Context: the great barrier reef, known for its stunning marine biodiversity, was designated a world heritage site by unesco in 1981.  
Head Entity: great barrier reef  
Tail Entity: world heritage site
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: ada adini ( 1855 – february 1924 ) was an american operatic soprano who had an active international career from 1876 up into the first decade of the 20th century .
Head Entity: ada adini
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti ( october 12, 1935 – september 6, 2007 ) was an italian operatic tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey ( born march 27, 1969 ) is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: soprano  
Mixup data size:  319
MixupTrain:  epoch  0, batch     0 | loss: 2.6585152MixupTrain:  epoch  0, batch     1 | loss: 2.4520186MixupTrain:  epoch  0, batch     2 | loss: 2.9422313MixupTrain:  epoch  0, batch     3 | loss: 2.6730325MixupTrain:  epoch  0, batch     4 | loss: 2.5357945MixupTrain:  epoch  0, batch     5 | loss: 3.3531790MixupTrain:  epoch  0, batch     6 | loss: 3.0137723MixupTrain:  epoch  0, batch     7 | loss: 2.6745407MixupTrain:  epoch  0, batch     8 | loss: 2.6618349MixupTrain:  epoch  0, batch     9 | loss: 2.6630455MixupTrain:  epoch  0, batch    10 | loss: 2.5304985MixupTrain:  epoch  0, batch    11 | loss: 2.5852941MixupTrain:  epoch  0, batch    12 | loss: 2.2338769MixupTrain:  epoch  0, batch    13 | loss: 2.6967616MixupTrain:  epoch  0, batch    14 | loss: 2.6142815MixupTrain:  epoch  0, batch    15 | loss: 2.6173187MixupTrain:  epoch  0, batch    16 | loss: 2.1259738MixupTrain:  epoch  0, batch    17 | loss: 2.3286245MixupTrain:  epoch  0, batch    18 | loss: 2.6502413MixupTrain:  epoch  0, batch    19 | loss: 2.4085354
MemoryTrain:  epoch  0, batch     0 | loss: 2.0653973MemoryTrain:  epoch  0, batch     1 | loss: 3.4832783MemoryTrain:  epoch  0, batch     2 | loss: 2.7382290MemoryTrain:  epoch  0, batch     3 | loss: 2.6650879MemoryTrain:  epoch  0, batch     4 | loss: 2.9510221MemoryTrain:  epoch  0, batch     5 | loss: 3.2808647MemoryTrain:  epoch  0, batch     6 | loss: 2.3743522MemoryTrain:  epoch  0, batch     7 | loss: 3.5300777MemoryTrain:  epoch  1, batch     0 | loss: 2.9598582MemoryTrain:  epoch  1, batch     1 | loss: 2.9354432MemoryTrain:  epoch  1, batch     2 | loss: 2.4170811MemoryTrain:  epoch  1, batch     3 | loss: 2.2782006MemoryTrain:  epoch  1, batch     4 | loss: 2.4305434MemoryTrain:  epoch  1, batch     5 | loss: 3.0974231MemoryTrain:  epoch  1, batch     6 | loss: 1.9746314MemoryTrain:  epoch  1, batch     7 | loss: 2.2600577MemoryTrain:  epoch  2, batch     0 | loss: 2.3651485MemoryTrain:  epoch  2, batch     1 | loss: 3.1834512MemoryTrain:  epoch  2, batch     2 | loss: 2.1360762MemoryTrain:  epoch  2, batch     3 | loss: 1.8383814MemoryTrain:  epoch  2, batch     4 | loss: 1.8417778MemoryTrain:  epoch  2, batch     5 | loss: 2.3140514MemoryTrain:  epoch  2, batch     6 | loss: 1.7712216MemoryTrain:  epoch  2, batch     7 | loss: 1.9435970MemoryTrain:  epoch  3, batch     0 | loss: 2.2910070MemoryTrain:  epoch  3, batch     1 | loss: 2.4204001MemoryTrain:  epoch  3, batch     2 | loss: 1.9841163MemoryTrain:  epoch  3, batch     3 | loss: 2.0692935MemoryTrain:  epoch  3, batch     4 | loss: 1.5579565MemoryTrain:  epoch  3, batch     5 | loss: 1.8118007MemoryTrain:  epoch  3, batch     6 | loss: 2.0735455MemoryTrain:  epoch  3, batch     7 | loss: 1.8579581MemoryTrain:  epoch  4, batch     0 | loss: 2.1484675MemoryTrain:  epoch  4, batch     1 | loss: 1.4136637MemoryTrain:  epoch  4, batch     2 | loss: 1.6880162MemoryTrain:  epoch  4, batch     3 | loss: 1.5460272MemoryTrain:  epoch  4, batch     4 | loss: 2.1416564MemoryTrain:  epoch  4, batch     5 | loss: 1.7695090MemoryTrain:  epoch  4, batch     6 | loss: 2.4650505MemoryTrain:  epoch  4, batch     7 | loss: 2.1498079MemoryTrain:  epoch  5, batch     0 | loss: 1.8193468MemoryTrain:  epoch  5, batch     1 | loss: 2.0919056MemoryTrain:  epoch  5, batch     2 | loss: 2.2055092MemoryTrain:  epoch  5, batch     3 | loss: 1.3488606MemoryTrain:  epoch  5, batch     4 | loss: 1.8172410MemoryTrain:  epoch  5, batch     5 | loss: 1.5396428MemoryTrain:  epoch  5, batch     6 | loss: 2.0588708MemoryTrain:  epoch  5, batch     7 | loss: 1.5253234MemoryTrain:  epoch  6, batch     0 | loss: 1.8662004MemoryTrain:  epoch  6, batch     1 | loss: 1.8527884MemoryTrain:  epoch  6, batch     2 | loss: 1.6393173MemoryTrain:  epoch  6, batch     3 | loss: 1.7686887MemoryTrain:  epoch  6, batch     4 | loss: 1.7630668MemoryTrain:  epoch  6, batch     5 | loss: 1.5295787MemoryTrain:  epoch  6, batch     6 | loss: 1.6180147MemoryTrain:  epoch  6, batch     7 | loss: 1.7229421MemoryTrain:  epoch  7, batch     0 | loss: 1.7711097MemoryTrain:  epoch  7, batch     1 | loss: 1.6385537MemoryTrain:  epoch  7, batch     2 | loss: 1.4778817MemoryTrain:  epoch  7, batch     3 | loss: 1.5131956MemoryTrain:  epoch  7, batch     4 | loss: 1.6127870MemoryTrain:  epoch  7, batch     5 | loss: 1.5402184MemoryTrain:  epoch  7, batch     6 | loss: 1.5215611MemoryTrain:  epoch  7, batch     7 | loss: 1.9962597MemoryTrain:  epoch  8, batch     0 | loss: 1.6743619MemoryTrain:  epoch  8, batch     1 | loss: 1.6628771MemoryTrain:  epoch  8, batch     2 | loss: 1.5973133MemoryTrain:  epoch  8, batch     3 | loss: 1.6064134MemoryTrain:  epoch  8, batch     4 | loss: 1.3548139MemoryTrain:  epoch  8, batch     5 | loss: 1.3834755MemoryTrain:  epoch  8, batch     6 | loss: 1.4627953MemoryTrain:  epoch  8, batch     7 | loss: 1.4393804MemoryTrain:  epoch  9, batch     0 | loss: 1.4555860MemoryTrain:  epoch  9, batch     1 | loss: 1.4920666MemoryTrain:  epoch  9, batch     2 | loss: 1.4911761MemoryTrain:  epoch  9, batch     3 | loss: 1.4264193MemoryTrain:  epoch  9, batch     4 | loss: 1.4363823MemoryTrain:  epoch  9, batch     5 | loss: 1.5311638MemoryTrain:  epoch  9, batch     6 | loss: 1.3978541MemoryTrain:  epoch  9, batch     7 | loss: 1.5083948
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 73.44%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 74.31%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 73.30%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 74.52%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 75.45%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 76.67%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 76.95%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 77.94%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 80.26%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 80.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 81.55%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.15%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 83.59%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.25%   [EVAL] batch:   25 | acc: 62.50%,  total acc: 83.41%   [EVAL] batch:   26 | acc: 37.50%,  total acc: 81.71%   [EVAL] batch:   27 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   28 | acc: 68.75%,  total acc: 80.82%   [EVAL] batch:   29 | acc: 43.75%,  total acc: 79.58%   [EVAL] batch:   30 | acc: 56.25%,  total acc: 78.83%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 78.52%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 78.22%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 76.65%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 75.54%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 74.13%   [EVAL] batch:   36 | acc: 25.00%,  total acc: 72.80%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 72.53%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 72.12%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 71.56%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 71.19%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 70.68%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 70.20%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 69.89%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 70.14%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 70.24%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 70.35%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 70.44%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 70.79%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 71.12%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 71.69%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 72.24%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 72.64%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 73.15%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 73.64%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 74.56%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 74.89%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 75.32%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 75.73%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 76.13%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 76.51%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 76.09%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 74.43%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 73.96%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 74.52%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 75.89%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 78.52%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 79.78%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.56%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 81.85%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 80.71%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 80.99%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.97%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 82.41%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 83.19%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.27%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.77%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 85.66%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 85.89%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 86.28%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.66%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.01%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.34%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.66%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.96%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 87.95%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.08%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 88.21%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 88.19%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 87.91%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 87.63%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 87.37%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 87.24%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 87.00%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 86.76%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 86.66%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 86.44%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 86.23%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 85.57%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 85.38%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 85.45%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 85.70%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 85.73%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 85.96%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 86.19%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 86.31%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 86.43%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 86.54%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 86.65%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 86.57%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 86.76%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 86.87%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 86.88%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 86.71%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 86.63%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 86.47%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 86.40%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 86.08%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 86.10%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 86.12%   [EVAL] batch:   77 | acc: 93.75%,  total acc: 86.22%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 86.16%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 86.09%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 85.96%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 85.75%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 85.39%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 84.67%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 84.26%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 83.87%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 83.48%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 83.38%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 83.57%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 84.10%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 84.27%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 84.21%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 84.24%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 84.02%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 83.80%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 83.84%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 83.44%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 83.60%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 83.76%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 83.92%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 84.07%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 84.23%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 84.81%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 85.08%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 85.16%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 85.29%   [EVAL] batch:  113 | acc: 93.75%,  total acc: 85.36%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 85.43%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 85.56%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 85.68%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 85.81%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 85.82%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 85.83%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 85.80%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 85.86%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 85.98%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 86.04%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 86.10%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 85.62%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 85.38%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 85.06%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 84.69%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 84.42%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 84.06%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 83.95%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 84.02%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 84.14%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 84.21%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 84.28%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 84.31%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 84.33%   [EVAL] batch:  138 | acc: 81.25%,  total acc: 84.31%   [EVAL] batch:  139 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 84.44%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 84.46%   [EVAL] batch:  142 | acc: 93.75%,  total acc: 84.53%   [EVAL] batch:  143 | acc: 68.75%,  total acc: 84.42%   [EVAL] batch:  144 | acc: 43.75%,  total acc: 84.14%   [EVAL] batch:  145 | acc: 37.50%,  total acc: 83.82%   [EVAL] batch:  146 | acc: 68.75%,  total acc: 83.72%   [EVAL] batch:  147 | acc: 62.50%,  total acc: 83.57%   [EVAL] batch:  148 | acc: 31.25%,  total acc: 83.22%   [EVAL] batch:  149 | acc: 50.00%,  total acc: 83.00%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 82.49%   [EVAL] batch:  151 | acc: 31.25%,  total acc: 82.15%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 81.74%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 81.25%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 80.81%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 80.29%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 80.18%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 80.26%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 80.27%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 80.35%   [EVAL] batch:  160 | acc: 81.25%,  total acc: 80.36%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 80.44%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 80.56%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 80.60%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 80.72%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 80.80%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 80.88%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 80.99%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 81.07%   [EVAL] batch:  169 | acc: 68.75%,  total acc: 80.99%   [EVAL] batch:  170 | acc: 62.50%,  total acc: 80.88%   [EVAL] batch:  171 | acc: 56.25%,  total acc: 80.74%   [EVAL] batch:  172 | acc: 75.00%,  total acc: 80.71%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 80.60%   [EVAL] batch:  174 | acc: 68.75%,  total acc: 80.54%   [EVAL] batch:  175 | acc: 37.50%,  total acc: 80.29%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 80.12%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 79.95%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 79.85%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 79.79%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 79.77%   [EVAL] batch:  181 | acc: 43.75%,  total acc: 79.57%   [EVAL] batch:  182 | acc: 43.75%,  total acc: 79.37%   [EVAL] batch:  183 | acc: 37.50%,  total acc: 79.14%   [EVAL] batch:  184 | acc: 56.25%,  total acc: 79.02%   [EVAL] batch:  185 | acc: 56.25%,  total acc: 78.90%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 78.88%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 78.69%   [EVAL] batch:  188 | acc: 81.25%,  total acc: 78.70%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 78.68%   [EVAL] batch:  190 | acc: 81.25%,  total acc: 78.70%   [EVAL] batch:  191 | acc: 68.75%,  total acc: 78.65%   [EVAL] batch:  192 | acc: 75.00%,  total acc: 78.63%   [EVAL] batch:  193 | acc: 62.50%,  total acc: 78.54%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 78.59%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 78.54%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 78.55%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 78.63%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 78.39%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 78.50%   [EVAL] batch:  200 | acc: 87.50%,  total acc: 78.54%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 78.59%   [EVAL] batch:  202 | acc: 93.75%,  total acc: 78.66%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 78.68%   [EVAL] batch:  204 | acc: 100.00%,  total acc: 78.78%   [EVAL] batch:  205 | acc: 100.00%,  total acc: 78.88%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 78.96%   [EVAL] batch:  207 | acc: 93.75%,  total acc: 79.03%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 79.13%   [EVAL] batch:  209 | acc: 100.00%,  total acc: 79.23%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 79.30%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 79.39%   [EVAL] batch:  212 | acc: 81.25%,  total acc: 79.40%   [EVAL] batch:  213 | acc: 56.25%,  total acc: 79.29%   [EVAL] batch:  214 | acc: 43.75%,  total acc: 79.13%   [EVAL] batch:  215 | acc: 75.00%,  total acc: 79.11%   [EVAL] batch:  216 | acc: 43.75%,  total acc: 78.95%   [EVAL] batch:  217 | acc: 62.50%,  total acc: 78.87%   [EVAL] batch:  218 | acc: 56.25%,  total acc: 78.77%   [EVAL] batch:  219 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:  220 | acc: 37.50%,  total acc: 78.56%   [EVAL] batch:  221 | acc: 31.25%,  total acc: 78.35%   [EVAL] batch:  222 | acc: 37.50%,  total acc: 78.17%   [EVAL] batch:  223 | acc: 25.00%,  total acc: 77.93%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 77.81%   [EVAL] batch:  225 | acc: 56.25%,  total acc: 77.71%   [EVAL] batch:  226 | acc: 50.00%,  total acc: 77.59%   [EVAL] batch:  227 | acc: 43.75%,  total acc: 77.44%   [EVAL] batch:  228 | acc: 68.75%,  total acc: 77.40%   [EVAL] batch:  229 | acc: 50.00%,  total acc: 77.28%   [EVAL] batch:  230 | acc: 37.50%,  total acc: 77.11%   [EVAL] batch:  231 | acc: 75.00%,  total acc: 77.10%   [EVAL] batch:  232 | acc: 75.00%,  total acc: 77.09%   [EVAL] batch:  233 | acc: 81.25%,  total acc: 77.11%   [EVAL] batch:  234 | acc: 75.00%,  total acc: 77.10%   [EVAL] batch:  235 | acc: 81.25%,  total acc: 77.12%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 77.16%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 77.23%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 77.33%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 77.40%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 77.49%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 77.58%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 77.67%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 77.77%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 77.86%   [EVAL] batch:  245 | acc: 93.75%,  total acc: 77.92%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 78.01%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 78.10%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 78.19%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 78.27%   
cur_acc:  ['0.9494', '0.8998', '0.6488', '0.7609']
his_acc:  ['0.9494', '0.9045', '0.8052', '0.7827']
CurrentTrain: epoch  0, batch     0 | loss: 6.0619617CurrentTrain: epoch  0, batch     1 | loss: 5.8808212CurrentTrain: epoch  0, batch     2 | loss: 5.8942127CurrentTrain: epoch  0, batch     3 | loss: 6.1264887CurrentTrain: epoch  1, batch     0 | loss: 4.8400311CurrentTrain: epoch  1, batch     1 | loss: 5.3752365CurrentTrain: epoch  1, batch     2 | loss: 5.4587326CurrentTrain: epoch  1, batch     3 | loss: 3.3528080CurrentTrain: epoch  2, batch     0 | loss: 4.2885480CurrentTrain: epoch  2, batch     1 | loss: 4.0625076CurrentTrain: epoch  2, batch     2 | loss: 5.6123266CurrentTrain: epoch  2, batch     3 | loss: 4.8938389CurrentTrain: epoch  3, batch     0 | loss: 3.9829233CurrentTrain: epoch  3, batch     1 | loss: 3.8531504CurrentTrain: epoch  3, batch     2 | loss: 4.9208202CurrentTrain: epoch  3, batch     3 | loss: 3.7594466CurrentTrain: epoch  4, batch     0 | loss: 4.1070747CurrentTrain: epoch  4, batch     1 | loss: 4.1344614CurrentTrain: epoch  4, batch     2 | loss: 3.7381098CurrentTrain: epoch  4, batch     3 | loss: 2.1728530CurrentTrain: epoch  5, batch     0 | loss: 3.2053556CurrentTrain: epoch  5, batch     1 | loss: 3.5548847CurrentTrain: epoch  5, batch     2 | loss: 4.1100140CurrentTrain: epoch  5, batch     3 | loss: 5.4281521CurrentTrain: epoch  6, batch     0 | loss: 3.5777073CurrentTrain: epoch  6, batch     1 | loss: 3.6101203CurrentTrain: epoch  6, batch     2 | loss: 3.0800138CurrentTrain: epoch  6, batch     3 | loss: 3.1244793CurrentTrain: epoch  7, batch     0 | loss: 3.1802688CurrentTrain: epoch  7, batch     1 | loss: 3.0925217CurrentTrain: epoch  7, batch     2 | loss: 3.7259512CurrentTrain: epoch  7, batch     3 | loss: 1.9032784CurrentTrain: epoch  8, batch     0 | loss: 3.3855567CurrentTrain: epoch  8, batch     1 | loss: 2.5456505CurrentTrain: epoch  8, batch     2 | loss: 3.1154385CurrentTrain: epoch  8, batch     3 | loss: 1.8535208CurrentTrain: epoch  9, batch     0 | loss: 2.6447854CurrentTrain: epoch  9, batch     1 | loss: 2.9197872CurrentTrain: epoch  9, batch     2 | loss: 2.8086176CurrentTrain: epoch  9, batch     3 | loss: 4.0264640
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: in 2004 the catalan government gave him the george cross .
Head Entity: george cross
Tail Entity: catalan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The new environmental regulations established by the European Union will be enforced across all member states starting next year.  
Head Entity: environmental regulations  
Tail Entity: European Union  

Relation: applies to jurisdiction  
Context: The Supreme Court's ruling on the matter will have implications for all states within the United States.  
Head Entity: Supreme Court  
Tail Entity: United States  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: it is prominently featured on jean michel jarre 's albums " oxygène " ( 1977 ) and " équinoxe " ( 1978 ) .
Head Entity: équinoxe
Tail Entity: jean michel jarre
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: The legendary band Queen is known for their iconic song "Bohemian Rhapsody," which showcases Freddie Mercury's extraordinary vocal talent.  
Head Entity: Bohemian Rhapsody  
Tail Entity: Freddie Mercury  

Relation: performer  
Context: Taylor Swift captivated audiences with her heartfelt performance of "Love Story" during the concert last summer.  
Head Entity: Love Story  
Tail Entity: Taylor Swift  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the next full - frame digital slrs were the canon eos-1ds of late 2002 , followed by kodak 's dcs pro 14n in 2003 .
Head Entity: dcs pro 14n
Tail Entity: kodak
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The latest smartphone model, the Galaxy S21, was released by Samsung in January 2021, showcasing advanced camera technology and performance.  
Head Entity: Galaxy S21  
Tail Entity: Samsung  

Relation: manufacturer  
Context: The iconic Model T was produced by Ford Motor Company, revolutionizing the automotive industry in the early 20th century.  
Head Entity: Model T  
Tail Entity: Ford Motor Company  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: norway was represented in the eurovision song contest 2005 by the song " in my dreams " performed by wig wam .
Head Entity: eurovision song contest 2005
Tail Entity: wig wam
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: the united states was represented in the 2020 summer olympics by a team of over 600 athletes competing in various sports.  
Head Entity: 2020 summer olympics  
Tail Entity: united states  

Relation: participant  
Context: the world health organization organized a global conference where numerous countries, including india, participated to discuss health policies.  
Head Entity: global conference  
Tail Entity: india  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the application is compatible with various operating systems, including macOS, Windows, and Ubuntu.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: This game runs smoothly on multiple operating systems, such as Linux, Windows 10, and the latest version of macOS.  
Head Entity: game  
Tail Entity: Windows 10  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in his next bout , he won the wbc light welterweight title by defeating roger mayweather for a second time .
Head Entity: roger mayweather
Tail Entity: light welterweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: The team secured their place in the tournament by qualifying under the UEFA Champions League regulations.  
Head Entity: UEFA Champions League  
Tail Entity: tournament  

Relation: competition class  
Context: The athlete competed in the 100 meters event, which is classified under the Olympic Games standards.  
Head Entity: Olympic Games  
Tail Entity: 100 meters event  
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: the qup gained 3 seats from its previous election result , and won a 10 % swing to it in primary votes , but failed to defeat the incumbent labor government led by ted theodore .
Head Entity: previous election
Tail Entity: ted theodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: After a closely contested campaign, the voters chose Sarah Johnson as their representative, marking a significant shift in the political landscape of the district.  
Head Entity: closely contested campaign  
Tail Entity: Sarah Johnson  

Relation: successful candidate  
Context: In the recent mayoral election, the community rallied behind Mark Thompson, who promised to bring new initiatives to the city, ultimately leading to his victory.  
Head Entity: recent mayoral election  
Tail Entity: Mark Thompson  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: although the latter 's eldest son , lǫgmaðr , appears to have succeeded to the kingship , he was soon forced to contend with factions supporting his younger brothers : haraldr , and óláfr .
Head Entity: lǫgmaðr
Tail Entity: óláfr
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: In the royal family of the ancient kingdom, the two brothers, Eirik and Sigurd, were known for their fierce loyalty to each other, having grown up under the same roof and shared the same parents.  
Head Entity: Eirik  
Tail Entity: Sigurd  

Relation: sibling  
Context: During the family reunion, it was evident that both Clara and her brother, James, inherited their parents' artistic talents, showcasing their skills in painting and music.  
Head Entity: Clara  
Tail Entity: James  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: iain harrison is a competitive shooter and former british army captain .
Head Entity: iain harrison
Tail Entity: british army
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: sergeant johnson served in the united states marine corps during his military career.  
Head Entity: sergeant johnson  
Tail Entity: united states marine corps  

Relation: military branch  
Context: general smith was a prominent leader in the royal air force, known for his strategic insights.  
Head Entity: general smith  
Tail Entity: royal air force  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: ziad rahbani is the son of the lebanese famous composer assi rahbani and nouhad haddad , the famous lebanese female singer known as fairuz .
Head Entity: assi rahbani
Tail Entity: ziad rahbani
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: in the animated series, the character simon is the son of the adventurous couple, marie and tom.  
Head Entity: marie  
Tail Entity: simon  

Relation: child  
Context: the renowned physicist albert einstein had a daughter named lieserl, who was born in 1902.  
Head Entity: albert einstein  
Tail Entity: lieserl  
Mixup data size:  378
MixupTrain:  epoch  0, batch     0 | loss: 2.2066982MixupTrain:  epoch  0, batch     1 | loss: 2.2745658MixupTrain:  epoch  0, batch     2 | loss: 1.9111586MixupTrain:  epoch  0, batch     3 | loss: 2.4027954MixupTrain:  epoch  0, batch     4 | loss: 2.7734656MixupTrain:  epoch  0, batch     5 | loss: 2.1664466MixupTrain:  epoch  0, batch     6 | loss: 2.2320045MixupTrain:  epoch  0, batch     7 | loss: 2.1454587MixupTrain:  epoch  0, batch     8 | loss: 2.3006889MixupTrain:  epoch  0, batch     9 | loss: 2.3649423MixupTrain:  epoch  0, batch    10 | loss: 2.2097103MixupTrain:  epoch  0, batch    11 | loss: 2.3601064MixupTrain:  epoch  0, batch    12 | loss: 2.1718903MixupTrain:  epoch  0, batch    13 | loss: 2.0404466MixupTrain:  epoch  0, batch    14 | loss: 1.9894918MixupTrain:  epoch  0, batch    15 | loss: 1.9122539MixupTrain:  epoch  0, batch    16 | loss: 1.9346489MixupTrain:  epoch  0, batch    17 | loss: 2.2798385MixupTrain:  epoch  0, batch    18 | loss: 2.2812135MixupTrain:  epoch  0, batch    19 | loss: 2.6730243MixupTrain:  epoch  0, batch    20 | loss: 2.1695862MixupTrain:  epoch  0, batch    21 | loss: 1.7710970MixupTrain:  epoch  0, batch    22 | loss: 2.2402063MixupTrain:  epoch  0, batch    23 | loss: 2.0093012
MemoryTrain:  epoch  0, batch     0 | loss: 1.8223779MemoryTrain:  epoch  0, batch     1 | loss: 1.8998706MemoryTrain:  epoch  0, batch     2 | loss: 2.2565198MemoryTrain:  epoch  0, batch     3 | loss: 1.7302685MemoryTrain:  epoch  0, batch     4 | loss: 2.0065436MemoryTrain:  epoch  0, batch     5 | loss: 2.3217046MemoryTrain:  epoch  0, batch     6 | loss: 2.0275140MemoryTrain:  epoch  0, batch     7 | loss: 2.7141356MemoryTrain:  epoch  0, batch     8 | loss: 2.6424632MemoryTrain:  epoch  0, batch     9 | loss: 2.8265057MemoryTrain:  epoch  1, batch     0 | loss: 1.7965806MemoryTrain:  epoch  1, batch     1 | loss: 2.7585168MemoryTrain:  epoch  1, batch     2 | loss: 1.5217516MemoryTrain:  epoch  1, batch     3 | loss: 1.9403931MemoryTrain:  epoch  1, batch     4 | loss: 2.1996431MemoryTrain:  epoch  1, batch     5 | loss: 1.8659787MemoryTrain:  epoch  1, batch     6 | loss: 1.4693387MemoryTrain:  epoch  1, batch     7 | loss: 1.6238232MemoryTrain:  epoch  1, batch     8 | loss: 2.2956991MemoryTrain:  epoch  1, batch     9 | loss: 1.8401315MemoryTrain:  epoch  2, batch     0 | loss: 1.9252714MemoryTrain:  epoch  2, batch     1 | loss: 1.6978221MemoryTrain:  epoch  2, batch     2 | loss: 2.0180159MemoryTrain:  epoch  2, batch     3 | loss: 1.6389925MemoryTrain:  epoch  2, batch     4 | loss: 1.6419528MemoryTrain:  epoch  2, batch     5 | loss: 1.6583650MemoryTrain:  epoch  2, batch     6 | loss: 1.6329216MemoryTrain:  epoch  2, batch     7 | loss: 1.4473643MemoryTrain:  epoch  2, batch     8 | loss: 1.7317290MemoryTrain:  epoch  2, batch     9 | loss: 1.4668379MemoryTrain:  epoch  3, batch     0 | loss: 1.4209108MemoryTrain:  epoch  3, batch     1 | loss: 1.4338262MemoryTrain:  epoch  3, batch     2 | loss: 1.8916218MemoryTrain:  epoch  3, batch     3 | loss: 1.5945606MemoryTrain:  epoch  3, batch     4 | loss: 1.3835313MemoryTrain:  epoch  3, batch     5 | loss: 1.5941112MemoryTrain:  epoch  3, batch     6 | loss: 1.6008766MemoryTrain:  epoch  3, batch     7 | loss: 1.7293380MemoryTrain:  epoch  3, batch     8 | loss: 1.6067560MemoryTrain:  epoch  3, batch     9 | loss: 1.7877516MemoryTrain:  epoch  4, batch     0 | loss: 1.6160120MemoryTrain:  epoch  4, batch     1 | loss: 1.2950363MemoryTrain:  epoch  4, batch     2 | loss: 1.5584822MemoryTrain:  epoch  4, batch     3 | loss: 1.6871932MemoryTrain:  epoch  4, batch     4 | loss: 1.4721210MemoryTrain:  epoch  4, batch     5 | loss: 1.3826058MemoryTrain:  epoch  4, batch     6 | loss: 1.4183161MemoryTrain:  epoch  4, batch     7 | loss: 1.3747801MemoryTrain:  epoch  4, batch     8 | loss: 1.7760057MemoryTrain:  epoch  4, batch     9 | loss: 1.2439028MemoryTrain:  epoch  5, batch     0 | loss: 1.3999743MemoryTrain:  epoch  5, batch     1 | loss: 1.3001958MemoryTrain:  epoch  5, batch     2 | loss: 1.4025623MemoryTrain:  epoch  5, batch     3 | loss: 1.6836874MemoryTrain:  epoch  5, batch     4 | loss: 1.5236719MemoryTrain:  epoch  5, batch     5 | loss: 1.3855021MemoryTrain:  epoch  5, batch     6 | loss: 1.3250544MemoryTrain:  epoch  5, batch     7 | loss: 1.4515276MemoryTrain:  epoch  5, batch     8 | loss: 1.4799809MemoryTrain:  epoch  5, batch     9 | loss: 1.6369269MemoryTrain:  epoch  6, batch     0 | loss: 1.3791691MemoryTrain:  epoch  6, batch     1 | loss: 1.3410394MemoryTrain:  epoch  6, batch     2 | loss: 1.5737250MemoryTrain:  epoch  6, batch     3 | loss: 1.3509710MemoryTrain:  epoch  6, batch     4 | loss: 1.5056102MemoryTrain:  epoch  6, batch     5 | loss: 1.2744617MemoryTrain:  epoch  6, batch     6 | loss: 1.3109096MemoryTrain:  epoch  6, batch     7 | loss: 1.5886428MemoryTrain:  epoch  6, batch     8 | loss: 1.4792349MemoryTrain:  epoch  6, batch     9 | loss: 1.5239754MemoryTrain:  epoch  7, batch     0 | loss: 1.3975375MemoryTrain:  epoch  7, batch     1 | loss: 1.3405149MemoryTrain:  epoch  7, batch     2 | loss: 1.3519242MemoryTrain:  epoch  7, batch     3 | loss: 1.3643314MemoryTrain:  epoch  7, batch     4 | loss: 1.3644755MemoryTrain:  epoch  7, batch     5 | loss: 1.5627279MemoryTrain:  epoch  7, batch     6 | loss: 1.5080063MemoryTrain:  epoch  7, batch     7 | loss: 1.3441577MemoryTrain:  epoch  7, batch     8 | loss: 1.3385172MemoryTrain:  epoch  7, batch     9 | loss: 1.3489819MemoryTrain:  epoch  8, batch     0 | loss: 1.4039606MemoryTrain:  epoch  8, batch     1 | loss: 1.3513209MemoryTrain:  epoch  8, batch     2 | loss: 1.4241507MemoryTrain:  epoch  8, batch     3 | loss: 1.3825059MemoryTrain:  epoch  8, batch     4 | loss: 1.4058365MemoryTrain:  epoch  8, batch     5 | loss: 1.2455881MemoryTrain:  epoch  8, batch     6 | loss: 1.2992154MemoryTrain:  epoch  8, batch     7 | loss: 1.4997282MemoryTrain:  epoch  8, batch     8 | loss: 1.2675827MemoryTrain:  epoch  8, batch     9 | loss: 1.2874799MemoryTrain:  epoch  9, batch     0 | loss: 1.3679996MemoryTrain:  epoch  9, batch     1 | loss: 1.3477373MemoryTrain:  epoch  9, batch     2 | loss: 1.2801445MemoryTrain:  epoch  9, batch     3 | loss: 1.3904024MemoryTrain:  epoch  9, batch     4 | loss: 1.3080305MemoryTrain:  epoch  9, batch     5 | loss: 1.3128035MemoryTrain:  epoch  9, batch     6 | loss: 1.2965328MemoryTrain:  epoch  9, batch     7 | loss: 1.2650888MemoryTrain:  epoch  9, batch     8 | loss: 1.3120933MemoryTrain:  epoch  9, batch     9 | loss: 1.2968773
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 80.56%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 80.62%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 78.65%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 79.33%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 80.83%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 80.86%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 81.99%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 82.14%   [EVAL] batch:   21 | acc: 56.25%,  total acc: 80.97%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 80.43%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 80.47%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 80.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 80.53%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.92%   [EVAL] batch:   28 | acc: 81.25%,  total acc: 81.90%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.06%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 84.56%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 85.81%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 86.18%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 86.59%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 86.76%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 86.92%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 87.07%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 86.25%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 85.19%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 84.31%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 83.85%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 83.42%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 82.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 82.84%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 83.05%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 83.25%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 83.22%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 83.41%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 83.48%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 83.11%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 82.97%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 83.05%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 82.71%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 82.79%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 82.16%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 81.65%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 75.62%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 74.43%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 72.92%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 73.08%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 74.55%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 75.83%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 76.95%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 77.94%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 78.82%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 79.28%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 79.06%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 78.69%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 77.72%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 78.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.09%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 79.63%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 80.13%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 80.60%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.85%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 82.42%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.95%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 83.46%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 84.20%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 84.63%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 85.03%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 86.13%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 86.31%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 86.48%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 86.22%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 85.56%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 84.38%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 83.38%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 82.68%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 81.63%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 80.75%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 80.51%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 80.53%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 80.42%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 80.21%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 79.77%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 79.58%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 79.71%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 79.85%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 80.19%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 80.31%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 80.53%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 80.75%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 80.95%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 81.15%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 81.35%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 81.63%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 81.62%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 81.89%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 82.07%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 81.96%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 81.78%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 81.68%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 81.51%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 81.42%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 81.08%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 81.09%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 81.17%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 81.17%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 81.09%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 81.02%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 80.79%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 80.57%   [EVAL] batch:   83 | acc: 43.75%,  total acc: 80.13%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 79.78%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 79.51%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 79.17%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 79.12%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 79.35%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 79.58%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 79.81%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 80.03%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 80.24%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 80.39%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 80.26%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 80.27%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 80.03%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 79.85%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 79.92%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 79.56%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 79.76%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 79.96%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 80.16%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 80.35%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 80.48%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 80.60%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 80.78%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 80.96%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 81.14%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 81.31%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 81.48%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 81.58%   [EVAL] batch:  112 | acc: 81.25%,  total acc: 81.58%   [EVAL] batch:  113 | acc: 75.00%,  total acc: 81.52%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 81.58%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 81.63%   [EVAL] batch:  116 | acc: 68.75%,  total acc: 81.52%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 81.57%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 81.62%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 81.67%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 81.66%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 81.76%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 81.91%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 82.01%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 82.10%   [EVAL] batch:  125 | acc: 18.75%,  total acc: 81.60%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 81.40%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 81.10%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 80.77%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 80.48%   [EVAL] batch:  130 | acc: 56.25%,  total acc: 80.30%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 80.21%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 80.26%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 80.41%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 80.51%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 80.61%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 80.66%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 80.66%   [EVAL] batch:  138 | acc: 81.25%,  total acc: 80.67%   [EVAL] batch:  139 | acc: 87.50%,  total acc: 80.71%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 80.76%   [EVAL] batch:  141 | acc: 93.75%,  total acc: 80.85%   [EVAL] batch:  142 | acc: 87.50%,  total acc: 80.90%   [EVAL] batch:  143 | acc: 68.75%,  total acc: 80.82%   [EVAL] batch:  144 | acc: 50.00%,  total acc: 80.60%   [EVAL] batch:  145 | acc: 31.25%,  total acc: 80.27%   [EVAL] batch:  146 | acc: 62.50%,  total acc: 80.14%   [EVAL] batch:  147 | acc: 62.50%,  total acc: 80.03%   [EVAL] batch:  148 | acc: 25.00%,  total acc: 79.66%   [EVAL] batch:  149 | acc: 56.25%,  total acc: 79.50%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 79.01%   [EVAL] batch:  151 | acc: 25.00%,  total acc: 78.66%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 78.27%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 77.80%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 77.38%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 76.88%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 76.75%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 76.86%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 76.89%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 76.99%   [EVAL] batch:  160 | acc: 81.25%,  total acc: 77.02%   [EVAL] batch:  161 | acc: 75.00%,  total acc: 77.01%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 77.11%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 77.17%   [EVAL] batch:  164 | acc: 93.75%,  total acc: 77.27%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 77.37%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 77.43%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 77.57%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 77.66%   [EVAL] batch:  169 | acc: 68.75%,  total acc: 77.61%   [EVAL] batch:  170 | acc: 56.25%,  total acc: 77.49%   [EVAL] batch:  171 | acc: 56.25%,  total acc: 77.36%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 77.31%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 77.23%   [EVAL] batch:  174 | acc: 68.75%,  total acc: 77.18%   [EVAL] batch:  175 | acc: 12.50%,  total acc: 76.81%   [EVAL] batch:  176 | acc: 31.25%,  total acc: 76.55%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 76.33%   [EVAL] batch:  178 | acc: 43.75%,  total acc: 76.15%   [EVAL] batch:  179 | acc: 37.50%,  total acc: 75.94%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 75.83%   [EVAL] batch:  181 | acc: 37.50%,  total acc: 75.62%   [EVAL] batch:  182 | acc: 43.75%,  total acc: 75.44%   [EVAL] batch:  183 | acc: 37.50%,  total acc: 75.24%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 75.17%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 75.03%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 75.03%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 74.90%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 74.97%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 74.97%   [EVAL] batch:  190 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 75.03%   [EVAL] batch:  192 | acc: 81.25%,  total acc: 75.06%   [EVAL] batch:  193 | acc: 68.75%,  total acc: 75.03%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 75.06%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 74.97%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 74.97%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 74.75%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 74.88%   [EVAL] batch:  200 | acc: 87.50%,  total acc: 74.94%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 75.03%   [EVAL] batch:  203 | acc: 75.00%,  total acc: 75.03%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 75.06%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 75.15%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 75.24%   [EVAL] batch:  207 | acc: 93.75%,  total acc: 75.33%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 75.45%   [EVAL] batch:  209 | acc: 100.00%,  total acc: 75.57%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 75.65%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 75.77%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 75.67%   [EVAL] batch:  213 | acc: 12.50%,  total acc: 75.38%   [EVAL] batch:  214 | acc: 18.75%,  total acc: 75.12%   [EVAL] batch:  215 | acc: 37.50%,  total acc: 74.94%   [EVAL] batch:  216 | acc: 18.75%,  total acc: 74.68%   [EVAL] batch:  217 | acc: 18.75%,  total acc: 74.43%   [EVAL] batch:  218 | acc: 50.00%,  total acc: 74.32%   [EVAL] batch:  219 | acc: 75.00%,  total acc: 74.32%   [EVAL] batch:  220 | acc: 43.75%,  total acc: 74.18%   [EVAL] batch:  221 | acc: 43.75%,  total acc: 74.04%   [EVAL] batch:  222 | acc: 43.75%,  total acc: 73.91%   [EVAL] batch:  223 | acc: 50.00%,  total acc: 73.80%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 73.69%   [EVAL] batch:  225 | acc: 56.25%,  total acc: 73.62%   [EVAL] batch:  226 | acc: 43.75%,  total acc: 73.49%   [EVAL] batch:  227 | acc: 43.75%,  total acc: 73.36%   [EVAL] batch:  228 | acc: 68.75%,  total acc: 73.34%   [EVAL] batch:  229 | acc: 50.00%,  total acc: 73.23%   [EVAL] batch:  230 | acc: 31.25%,  total acc: 73.05%   [EVAL] batch:  231 | acc: 75.00%,  total acc: 73.06%   [EVAL] batch:  232 | acc: 75.00%,  total acc: 73.07%   [EVAL] batch:  233 | acc: 75.00%,  total acc: 73.08%   [EVAL] batch:  234 | acc: 75.00%,  total acc: 73.09%   [EVAL] batch:  235 | acc: 81.25%,  total acc: 73.12%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 73.18%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 73.27%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 73.38%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 73.46%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 73.57%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 73.68%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 73.79%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 73.90%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 74.01%   [EVAL] batch:  245 | acc: 93.75%,  total acc: 74.09%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 74.16%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 74.27%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 74.37%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 74.48%   [EVAL] batch:  250 | acc: 81.25%,  total acc: 74.50%   [EVAL] batch:  251 | acc: 81.25%,  total acc: 74.53%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 74.56%   [EVAL] batch:  253 | acc: 68.75%,  total acc: 74.53%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 74.61%   [EVAL] batch:  255 | acc: 75.00%,  total acc: 74.61%   [EVAL] batch:  256 | acc: 81.25%,  total acc: 74.64%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 74.66%   [EVAL] batch:  258 | acc: 81.25%,  total acc: 74.69%   [EVAL] batch:  259 | acc: 81.25%,  total acc: 74.71%   [EVAL] batch:  260 | acc: 68.75%,  total acc: 74.69%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 74.67%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 74.71%   [EVAL] batch:  263 | acc: 93.75%,  total acc: 74.79%   [EVAL] batch:  264 | acc: 87.50%,  total acc: 74.83%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 74.86%   [EVAL] batch:  266 | acc: 100.00%,  total acc: 74.95%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 75.02%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 75.05%   [EVAL] batch:  269 | acc: 81.25%,  total acc: 75.07%   [EVAL] batch:  270 | acc: 75.00%,  total acc: 75.07%   [EVAL] batch:  271 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:  272 | acc: 68.75%,  total acc: 74.98%   [EVAL] batch:  273 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:  274 | acc: 68.75%,  total acc: 74.98%   [EVAL] batch:  275 | acc: 93.75%,  total acc: 75.05%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 75.14%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 75.22%   [EVAL] batch:  278 | acc: 81.25%,  total acc: 75.25%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 75.33%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 75.42%   [EVAL] batch:  281 | acc: 100.00%,  total acc: 75.51%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 75.60%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 75.68%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 75.77%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 75.85%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 75.94%   [EVAL] batch:  287 | acc: 100.00%,  total acc: 76.02%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 76.10%   [EVAL] batch:  289 | acc: 93.75%,  total acc: 76.16%   [EVAL] batch:  290 | acc: 81.25%,  total acc: 76.18%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 76.24%   [EVAL] batch:  292 | acc: 93.75%,  total acc: 76.30%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 76.36%   [EVAL] batch:  294 | acc: 50.00%,  total acc: 76.27%   [EVAL] batch:  295 | acc: 37.50%,  total acc: 76.14%   [EVAL] batch:  296 | acc: 43.75%,  total acc: 76.03%   [EVAL] batch:  297 | acc: 62.50%,  total acc: 75.99%   [EVAL] batch:  298 | acc: 62.50%,  total acc: 75.94%   [EVAL] batch:  299 | acc: 43.75%,  total acc: 75.83%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 75.89%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 75.95%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 76.01%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 76.03%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 76.09%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 76.12%   [EVAL] batch:  306 | acc: 62.50%,  total acc: 76.08%   [EVAL] batch:  307 | acc: 75.00%,  total acc: 76.08%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 76.11%   [EVAL] batch:  309 | acc: 62.50%,  total acc: 76.07%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 76.11%   [EVAL] batch:  311 | acc: 43.75%,  total acc: 76.00%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 75.92%   
cur_acc:  ['0.9494', '0.8998', '0.6488', '0.7609', '0.8165']
his_acc:  ['0.9494', '0.9045', '0.8052', '0.7827', '0.7592']
CurrentTrain: epoch  0, batch     0 | loss: 5.3870292CurrentTrain: epoch  0, batch     1 | loss: 7.2958002CurrentTrain: epoch  0, batch     2 | loss: 5.7196274CurrentTrain: epoch  0, batch     3 | loss: 4.1895113CurrentTrain: epoch  1, batch     0 | loss: 5.5726848CurrentTrain: epoch  1, batch     1 | loss: 4.7500644CurrentTrain: epoch  1, batch     2 | loss: 5.0444984CurrentTrain: epoch  1, batch     3 | loss: 5.0409956CurrentTrain: epoch  2, batch     0 | loss: 4.9810638CurrentTrain: epoch  2, batch     1 | loss: 4.0027103CurrentTrain: epoch  2, batch     2 | loss: 4.9822769CurrentTrain: epoch  2, batch     3 | loss: 2.5548353CurrentTrain: epoch  3, batch     0 | loss: 4.5057621CurrentTrain: epoch  3, batch     1 | loss: 2.8781943CurrentTrain: epoch  3, batch     2 | loss: 4.2324443CurrentTrain: epoch  3, batch     3 | loss: 3.6605008CurrentTrain: epoch  4, batch     0 | loss: 3.1648283CurrentTrain: epoch  4, batch     1 | loss: 4.2731333CurrentTrain: epoch  4, batch     2 | loss: 3.4030695CurrentTrain: epoch  4, batch     3 | loss: 2.7876358CurrentTrain: epoch  5, batch     0 | loss: 3.6658285CurrentTrain: epoch  5, batch     1 | loss: 3.1822052CurrentTrain: epoch  5, batch     2 | loss: 3.0648880CurrentTrain: epoch  5, batch     3 | loss: 5.7228961CurrentTrain: epoch  6, batch     0 | loss: 4.0374718CurrentTrain: epoch  6, batch     1 | loss: 2.5856988CurrentTrain: epoch  6, batch     2 | loss: 2.6661954CurrentTrain: epoch  6, batch     3 | loss: 3.2058430CurrentTrain: epoch  7, batch     0 | loss: 3.0995378CurrentTrain: epoch  7, batch     1 | loss: 3.0447545CurrentTrain: epoch  7, batch     2 | loss: 2.6664977CurrentTrain: epoch  7, batch     3 | loss: 2.1436834CurrentTrain: epoch  8, batch     0 | loss: 2.6867480CurrentTrain: epoch  8, batch     1 | loss: 2.7410240CurrentTrain: epoch  8, batch     2 | loss: 2.8899059CurrentTrain: epoch  8, batch     3 | loss: 1.9397094CurrentTrain: epoch  9, batch     0 | loss: 2.7603536CurrentTrain: epoch  9, batch     1 | loss: 2.5890329CurrentTrain: epoch  9, batch     2 | loss: 2.4096856CurrentTrain: epoch  9, batch     3 | loss: 2.1294250
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: the nikon d5500 features other improvements over its predecessor nikon d5300 .
Head Entity: nikon d5500
Tail Entity: nikon d5300
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: in the series of popular smartphone models, the iphone 12 follows the iphone 11, which was released the previous year.  
Head Entity: iphone 12  
Tail Entity: iphone 11  

Relation: follows  
Context: the latest installment in the series, the harry potter and the deathly hallows, follows the previous book, harry potter and the half-blood prince.  
Head Entity: harry potter and the deathly hallows  
Tail Entity: harry potter and the half-blood prince  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: dick packer was a u.s. soccer center forward who was a member of the u.s. team at the 1956 summer olympics .
Head Entity: dick packer
Tail Entity: soccer
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: serena williams is a professional tennis player who has won numerous grand slam titles throughout her career.  
Head Entity: serena williams  
Tail Entity: tennis  

Relation: sport  
Context: lebron james is known for his exceptional skills in basketball and has played for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: basketball  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael was the father of two daughters, emily and sarah, who both excelled in their studies.  
Head Entity: michael  
Tail Entity: emily  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: since 2009 , culshaw has starred in the bbc one comedy sketch show " the impressions show " alongside debra stephenson .
Head Entity: the impressions show
Tail Entity: bbc one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: The popular series "Breaking Bad" first aired on AMC, captivating audiences with its intense storytelling and character development.  
Head Entity: Breaking Bad  
Tail Entity: AMC  

Relation: original network  
Context: "Friends" became a cultural phenomenon after its debut on NBC, attracting millions of viewers each week during its run.  
Head Entity: Friends  
Tail Entity: NBC  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: he was also nominated twice for the filmfare best telugu actor award , for the films bhale bhale magadivoy and " gentleman " .
Head Entity: gentleman
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: The movie "Parasite" received critical acclaim and was originally produced in Korean, showcasing the talents of its director Bong Joon-ho.  
Head Entity: Parasite  
Tail Entity: Korean  

Relation: original language of film or TV show  
Context: The animated series "Avatar: The Last Airbender" was created in English and has gained a massive following since its release.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: the league champions for the fifth time in their history ( and the second season running ) were taunton town , but runners - up mangotsfield united took promotion to the southern league .
Head Entity: mangotsfield united
Tail Entity: southern league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: After an impressive season, the team secured a spot in the prestigious league, where they will compete against top clubs, including their rivals, the city wanderers.  
Head Entity: city wanderers  
Tail Entity: prestigious league  

Relation: league  
Context: The young athlete showcased his skills in the national league, where he played alongside some of the best players in the country, including his teammate, the star striker.  
Head Entity: star striker  
Tail Entity: national league  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: retrieved 29 september 2010 . the daughter of actress xenia desni , tamara desni was born in berlin .
Head Entity: tamara desni
Tail Entity: xenia desni
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: on 15th march 2015, the famous singer and actress, jennifer lopez, celebrated her birthday with her children, emme and max, who are her pride and joy.  
Head Entity: emme  
Tail Entity: jennifer lopez  

Relation: mother  
Context: during the interview, the renowned author spoke fondly of her upbringing and how her mother, elizabeth, inspired her to pursue writing.  
Head Entity: the renowned author  
Tail Entity: elizabeth  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres. he is particularly celebrated for his interpretations of classical cello concertos.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: the type locality given is " fan - si - pan mountains , tonkin , indo - china " ( mount fansipan , sa pa district , nghệ an province , northwestern vietnam ) .
Head Entity: sa pa district
Tail Entity: vietnam
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the capital city of the country is located in the central region, known for its rich history and cultural heritage, particularly in the area of traditional crafts.  
Head Entity: capital city  
Tail Entity: vietnam  

Relation: country  
Context: the ancient ruins are a testament to the civilization that once thrived in this region, which is now part of a modern nation-state recognized for its diverse landscapes and vibrant culture.  
Head Entity: ancient ruins  
Tail Entity: peru  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: hagrid suggests in " harry potter and the chamber of secrets " that " " they 're startin ' ter think the job 's jinxed .
Head Entity: harry potter and the chamber of secrets
Tail Entity: hagrid
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: In the animated series "Avatar: The Last Airbender," Aang, the last Airbender, teams up with Katara and Sokka to defeat the Fire Nation.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: Aang  

Relation: characters  
Context: The novel "Pride and Prejudice" features Elizabeth Bennet as she navigates societal expectations and her relationship with Mr. Darcy.  
Head Entity: Pride and Prejudice  
Tail Entity: Elizabeth Bennet  
Mixup data size:  438
MixupTrain:  epoch  0, batch     0 | loss: 2.4602574MixupTrain:  epoch  0, batch     1 | loss: 2.3456189MixupTrain:  epoch  0, batch     2 | loss: 2.4374089MixupTrain:  epoch  0, batch     3 | loss: 2.2839784MixupTrain:  epoch  0, batch     4 | loss: 2.1102320MixupTrain:  epoch  0, batch     5 | loss: 2.3524581MixupTrain:  epoch  0, batch     6 | loss: 2.1655631MixupTrain:  epoch  0, batch     7 | loss: 1.8816207MixupTrain:  epoch  0, batch     8 | loss: 2.1688356MixupTrain:  epoch  0, batch     9 | loss: 1.9061000MixupTrain:  epoch  0, batch    10 | loss: 2.4898805MixupTrain:  epoch  0, batch    11 | loss: 2.3529321MixupTrain:  epoch  0, batch    12 | loss: 2.2957081MixupTrain:  epoch  0, batch    13 | loss: 2.1676498MixupTrain:  epoch  0, batch    14 | loss: 2.6092978MixupTrain:  epoch  0, batch    15 | loss: 2.2992864MixupTrain:  epoch  0, batch    16 | loss: 1.9159157MixupTrain:  epoch  0, batch    17 | loss: 2.2213901MixupTrain:  epoch  0, batch    18 | loss: 2.1147335MixupTrain:  epoch  0, batch    19 | loss: 2.0967998MixupTrain:  epoch  0, batch    20 | loss: 2.1259933MixupTrain:  epoch  0, batch    21 | loss: 2.1319569MixupTrain:  epoch  0, batch    22 | loss: 2.2777496MixupTrain:  epoch  0, batch    23 | loss: 1.9853316MixupTrain:  epoch  0, batch    24 | loss: 1.9278594MixupTrain:  epoch  0, batch    25 | loss: 1.8413353MixupTrain:  epoch  0, batch    26 | loss: 1.8523081MixupTrain:  epoch  0, batch    27 | loss: 1.6818237
MemoryTrain:  epoch  0, batch     0 | loss: 2.7293730MemoryTrain:  epoch  0, batch     1 | loss: 1.7845738MemoryTrain:  epoch  0, batch     2 | loss: 1.7330407MemoryTrain:  epoch  0, batch     3 | loss: 2.6621988MemoryTrain:  epoch  0, batch     4 | loss: 1.7446324MemoryTrain:  epoch  0, batch     5 | loss: 1.8077044MemoryTrain:  epoch  0, batch     6 | loss: 2.1097090MemoryTrain:  epoch  0, batch     7 | loss: 1.8655788MemoryTrain:  epoch  0, batch     8 | loss: 2.3335018MemoryTrain:  epoch  0, batch     9 | loss: 2.9607978MemoryTrain:  epoch  0, batch    10 | loss: 2.6459842MemoryTrain:  epoch  0, batch    11 | loss: 2.4776230MemoryTrain:  epoch  1, batch     0 | loss: 1.6621958MemoryTrain:  epoch  1, batch     1 | loss: 2.1087370MemoryTrain:  epoch  1, batch     2 | loss: 1.9483165MemoryTrain:  epoch  1, batch     3 | loss: 1.6104625MemoryTrain:  epoch  1, batch     4 | loss: 1.8831985MemoryTrain:  epoch  1, batch     5 | loss: 2.3690515MemoryTrain:  epoch  1, batch     6 | loss: 1.8858948MemoryTrain:  epoch  1, batch     7 | loss: 1.9557588MemoryTrain:  epoch  1, batch     8 | loss: 2.2952118MemoryTrain:  epoch  1, batch     9 | loss: 2.0557084MemoryTrain:  epoch  1, batch    10 | loss: 1.8569994MemoryTrain:  epoch  1, batch    11 | loss: 1.7795897MemoryTrain:  epoch  2, batch     0 | loss: 1.4233220MemoryTrain:  epoch  2, batch     1 | loss: 2.3091028MemoryTrain:  epoch  2, batch     2 | loss: 1.3745921MemoryTrain:  epoch  2, batch     3 | loss: 1.9216655MemoryTrain:  epoch  2, batch     4 | loss: 1.9626994MemoryTrain:  epoch  2, batch     5 | loss: 1.8292862MemoryTrain:  epoch  2, batch     6 | loss: 1.4610827MemoryTrain:  epoch  2, batch     7 | loss: 1.9454427MemoryTrain:  epoch  2, batch     8 | loss: 1.2970841MemoryTrain:  epoch  2, batch     9 | loss: 1.6145568MemoryTrain:  epoch  2, batch    10 | loss: 1.7189202MemoryTrain:  epoch  2, batch    11 | loss: 1.2953218MemoryTrain:  epoch  3, batch     0 | loss: 1.7827698MemoryTrain:  epoch  3, batch     1 | loss: 1.3239067MemoryTrain:  epoch  3, batch     2 | loss: 1.6422410MemoryTrain:  epoch  3, batch     3 | loss: 1.3225434MemoryTrain:  epoch  3, batch     4 | loss: 1.9896619MemoryTrain:  epoch  3, batch     5 | loss: 1.5028744MemoryTrain:  epoch  3, batch     6 | loss: 1.6663203MemoryTrain:  epoch  3, batch     7 | loss: 1.3308315MemoryTrain:  epoch  3, batch     8 | loss: 1.8126699MemoryTrain:  epoch  3, batch     9 | loss: 1.6378692MemoryTrain:  epoch  3, batch    10 | loss: 1.2582142MemoryTrain:  epoch  3, batch    11 | loss: 2.0447314MemoryTrain:  epoch  4, batch     0 | loss: 1.5015594MemoryTrain:  epoch  4, batch     1 | loss: 1.2626091MemoryTrain:  epoch  4, batch     2 | loss: 1.9225887MemoryTrain:  epoch  4, batch     3 | loss: 1.3284760MemoryTrain:  epoch  4, batch     4 | loss: 1.6159443MemoryTrain:  epoch  4, batch     5 | loss: 1.6107780MemoryTrain:  epoch  4, batch     6 | loss: 1.2998565MemoryTrain:  epoch  4, batch     7 | loss: 1.9750679MemoryTrain:  epoch  4, batch     8 | loss: 1.4273021MemoryTrain:  epoch  4, batch     9 | loss: 1.8323040MemoryTrain:  epoch  4, batch    10 | loss: 1.2646675MemoryTrain:  epoch  4, batch    11 | loss: 1.2519224MemoryTrain:  epoch  5, batch     0 | loss: 1.5884154MemoryTrain:  epoch  5, batch     1 | loss: 1.7725452MemoryTrain:  epoch  5, batch     2 | loss: 1.4023514MemoryTrain:  epoch  5, batch     3 | loss: 1.3510506MemoryTrain:  epoch  5, batch     4 | loss: 1.2995067MemoryTrain:  epoch  5, batch     5 | loss: 1.5168813MemoryTrain:  epoch  5, batch     6 | loss: 1.7902718MemoryTrain:  epoch  5, batch     7 | loss: 1.3823476MemoryTrain:  epoch  5, batch     8 | loss: 1.2705072MemoryTrain:  epoch  5, batch     9 | loss: 1.3559397MemoryTrain:  epoch  5, batch    10 | loss: 1.3438058MemoryTrain:  epoch  5, batch    11 | loss: 1.2158220MemoryTrain:  epoch  6, batch     0 | loss: 1.3054864MemoryTrain:  epoch  6, batch     1 | loss: 1.5633210MemoryTrain:  epoch  6, batch     2 | loss: 1.5035956MemoryTrain:  epoch  6, batch     3 | loss: 1.4718776MemoryTrain:  epoch  6, batch     4 | loss: 1.6277519MemoryTrain:  epoch  6, batch     5 | loss: 1.4187441MemoryTrain:  epoch  6, batch     6 | loss: 1.3562385MemoryTrain:  epoch  6, batch     7 | loss: 1.3562627MemoryTrain:  epoch  6, batch     8 | loss: 1.3499799MemoryTrain:  epoch  6, batch     9 | loss: 1.3767052MemoryTrain:  epoch  6, batch    10 | loss: 1.2437960MemoryTrain:  epoch  6, batch    11 | loss: 1.2188574MemoryTrain:  epoch  7, batch     0 | loss: 1.8100481MemoryTrain:  epoch  7, batch     1 | loss: 1.2835186MemoryTrain:  epoch  7, batch     2 | loss: 1.2836545MemoryTrain:  epoch  7, batch     3 | loss: 1.6054764MemoryTrain:  epoch  7, batch     4 | loss: 1.3634853MemoryTrain:  epoch  7, batch     5 | loss: 1.2338963MemoryTrain:  epoch  7, batch     6 | loss: 1.3275287MemoryTrain:  epoch  7, batch     7 | loss: 1.3453790MemoryTrain:  epoch  7, batch     8 | loss: 1.3717420MemoryTrain:  epoch  7, batch     9 | loss: 1.3378971MemoryTrain:  epoch  7, batch    10 | loss: 1.2708898MemoryTrain:  epoch  7, batch    11 | loss: 1.2855674MemoryTrain:  epoch  8, batch     0 | loss: 1.4146607MemoryTrain:  epoch  8, batch     1 | loss: 1.3256273MemoryTrain:  epoch  8, batch     2 | loss: 1.2651044MemoryTrain:  epoch  8, batch     3 | loss: 1.3510250MemoryTrain:  epoch  8, batch     4 | loss: 1.2132833MemoryTrain:  epoch  8, batch     5 | loss: 1.7964299MemoryTrain:  epoch  8, batch     6 | loss: 1.2452919MemoryTrain:  epoch  8, batch     7 | loss: 1.3028032MemoryTrain:  epoch  8, batch     8 | loss: 1.4580204MemoryTrain:  epoch  8, batch     9 | loss: 1.3261994MemoryTrain:  epoch  8, batch    10 | loss: 1.2174395MemoryTrain:  epoch  8, batch    11 | loss: 1.2904757MemoryTrain:  epoch  9, batch     0 | loss: 1.2639351MemoryTrain:  epoch  9, batch     1 | loss: 1.4496675MemoryTrain:  epoch  9, batch     2 | loss: 1.2429312MemoryTrain:  epoch  9, batch     3 | loss: 1.2871574MemoryTrain:  epoch  9, batch     4 | loss: 1.3968644MemoryTrain:  epoch  9, batch     5 | loss: 1.2918074MemoryTrain:  epoch  9, batch     6 | loss: 1.4823656MemoryTrain:  epoch  9, batch     7 | loss: 1.2747314MemoryTrain:  epoch  9, batch     8 | loss: 1.3034263MemoryTrain:  epoch  9, batch     9 | loss: 1.3848346MemoryTrain:  epoch  9, batch    10 | loss: 1.2287229MemoryTrain:  epoch  9, batch    11 | loss: 1.2360673
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 27.08%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 25.00%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 28.75%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 27.08%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 33.04%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 39.84%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 44.44%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 48.75%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 52.84%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 55.21%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 56.73%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 54.91%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 54.58%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 54.69%   [EVAL] batch:   16 | acc: 37.50%,  total acc: 53.68%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 54.86%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 55.59%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 57.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 59.52%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 61.08%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 63.80%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 64.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 65.87%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 67.13%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 68.30%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 70.42%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 71.37%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 73.11%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 73.90%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 74.46%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 75.68%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 75.33%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 74.20%   [EVAL] batch:   39 | acc: 25.00%,  total acc: 72.97%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 71.95%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 71.28%   [EVAL] batch:   42 | acc: 25.00%,  total acc: 70.20%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 69.89%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 70.56%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 70.92%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 71.54%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 72.01%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 72.45%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 72.88%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 72.55%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 72.24%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 71.82%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 71.53%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 71.36%   [EVAL] batch:   55 | acc: 31.25%,  total acc: 70.65%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 70.50%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 70.69%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 70.76%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 70.52%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 70.59%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 70.87%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 70.14%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 69.44%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 68.12%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 67.61%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 65.62%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 65.87%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 67.86%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 69.58%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 71.09%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 72.43%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 73.61%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 74.38%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 75.30%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 74.46%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 75.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.20%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 76.85%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 77.46%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.02%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.44%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.08%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 80.68%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 81.61%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 82.12%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 82.60%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 83.06%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.49%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.91%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.30%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 84.52%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 84.59%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 84.80%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 84.58%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 83.97%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 83.51%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 83.20%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 82.65%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 82.00%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 81.62%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 81.49%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 81.13%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 80.57%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 80.47%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 80.48%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 80.17%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 80.30%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 80.31%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 80.43%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 80.34%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 80.46%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 80.66%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 80.87%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 81.16%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 81.16%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 81.43%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 81.61%   [EVAL] batch:   69 | acc: 37.50%,  total acc: 80.98%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 80.63%   [EVAL] batch:   71 | acc: 37.50%,  total acc: 80.03%   [EVAL] batch:   72 | acc: 31.25%,  total acc: 79.37%   [EVAL] batch:   73 | acc: 31.25%,  total acc: 78.72%   [EVAL] batch:   74 | acc: 25.00%,  total acc: 78.00%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 78.17%   [EVAL] batch:   77 | acc: 93.75%,  total acc: 78.37%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 78.32%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 78.28%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 78.24%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 77.90%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 77.64%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 77.01%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 76.69%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 76.45%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 76.15%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 76.14%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 76.40%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 76.67%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 76.92%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 77.17%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 77.42%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 77.59%   [EVAL] batch:   94 | acc: 62.50%,  total acc: 77.43%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 77.47%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 77.38%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 77.23%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 77.27%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 76.94%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 77.17%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 77.39%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 77.61%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 77.82%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 77.98%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 78.33%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 78.53%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 78.73%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 78.92%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 79.11%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 79.24%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 79.20%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 79.00%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 79.02%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 79.04%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 78.90%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 78.87%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 78.89%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 78.96%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 78.98%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 79.10%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 79.27%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 79.39%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 79.50%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 79.07%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 78.79%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 78.52%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 78.20%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 77.93%   [EVAL] batch:  130 | acc: 56.25%,  total acc: 77.77%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 77.65%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 77.68%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 77.85%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 77.96%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 78.08%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 78.10%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:  138 | acc: 75.00%,  total acc: 78.10%   [EVAL] batch:  139 | acc: 75.00%,  total acc: 78.08%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 78.15%   [EVAL] batch:  141 | acc: 81.25%,  total acc: 78.17%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 78.10%   [EVAL] batch:  143 | acc: 56.25%,  total acc: 77.95%   [EVAL] batch:  144 | acc: 43.75%,  total acc: 77.72%   [EVAL] batch:  145 | acc: 31.25%,  total acc: 77.40%   [EVAL] batch:  146 | acc: 68.75%,  total acc: 77.34%   [EVAL] batch:  147 | acc: 56.25%,  total acc: 77.20%   [EVAL] batch:  148 | acc: 31.25%,  total acc: 76.89%   [EVAL] batch:  149 | acc: 56.25%,  total acc: 76.75%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 76.28%   [EVAL] batch:  151 | acc: 25.00%,  total acc: 75.95%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 75.49%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 75.04%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 74.64%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 74.16%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 74.04%   [EVAL] batch:  157 | acc: 81.25%,  total acc: 74.09%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 74.14%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 74.26%   [EVAL] batch:  160 | acc: 81.25%,  total acc: 74.30%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 74.34%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 74.42%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 74.50%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 74.66%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 74.77%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 74.85%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 75.07%   [EVAL] batch:  169 | acc: 56.25%,  total acc: 74.96%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 74.74%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 74.53%   [EVAL] batch:  172 | acc: 62.50%,  total acc: 74.46%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 74.32%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 74.18%   [EVAL] batch:  175 | acc: 12.50%,  total acc: 73.83%   [EVAL] batch:  176 | acc: 37.50%,  total acc: 73.62%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 73.46%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 73.32%   [EVAL] batch:  179 | acc: 43.75%,  total acc: 73.16%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 73.10%   [EVAL] batch:  181 | acc: 37.50%,  total acc: 72.91%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 72.78%   [EVAL] batch:  183 | acc: 37.50%,  total acc: 72.59%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 72.53%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 72.41%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 72.39%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 72.27%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 72.35%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 72.34%   [EVAL] batch:  190 | acc: 75.00%,  total acc: 72.35%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 72.40%   [EVAL] batch:  192 | acc: 81.25%,  total acc: 72.44%   [EVAL] batch:  193 | acc: 62.50%,  total acc: 72.39%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 72.44%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 72.42%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 72.40%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 72.41%   [EVAL] batch:  198 | acc: 25.00%,  total acc: 72.17%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 72.31%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 72.36%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 72.43%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 72.48%   [EVAL] batch:  203 | acc: 75.00%,  total acc: 72.49%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 72.53%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 72.63%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 72.74%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 72.87%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 73.00%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 73.10%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 73.19%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 73.32%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 73.24%   [EVAL] batch:  213 | acc: 18.75%,  total acc: 72.98%   [EVAL] batch:  214 | acc: 18.75%,  total acc: 72.73%   [EVAL] batch:  215 | acc: 43.75%,  total acc: 72.60%   [EVAL] batch:  216 | acc: 18.75%,  total acc: 72.35%   [EVAL] batch:  217 | acc: 25.00%,  total acc: 72.13%   [EVAL] batch:  218 | acc: 25.00%,  total acc: 71.92%   [EVAL] batch:  219 | acc: 56.25%,  total acc: 71.85%   [EVAL] batch:  220 | acc: 37.50%,  total acc: 71.69%   [EVAL] batch:  221 | acc: 25.00%,  total acc: 71.48%   [EVAL] batch:  222 | acc: 18.75%,  total acc: 71.24%   [EVAL] batch:  223 | acc: 37.50%,  total acc: 71.09%   [EVAL] batch:  224 | acc: 25.00%,  total acc: 70.89%   [EVAL] batch:  225 | acc: 62.50%,  total acc: 70.85%   [EVAL] batch:  226 | acc: 68.75%,  total acc: 70.84%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 70.81%   [EVAL] batch:  228 | acc: 81.25%,  total acc: 70.85%   [EVAL] batch:  229 | acc: 68.75%,  total acc: 70.84%   [EVAL] batch:  230 | acc: 56.25%,  total acc: 70.78%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 70.77%   [EVAL] batch:  232 | acc: 75.00%,  total acc: 70.79%   [EVAL] batch:  233 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:  234 | acc: 62.50%,  total acc: 70.80%   [EVAL] batch:  235 | acc: 75.00%,  total acc: 70.82%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 70.89%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 70.98%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 71.10%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 71.20%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 71.29%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 71.41%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 71.53%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 71.64%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 71.76%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 71.80%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 71.89%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 71.93%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 72.04%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 72.15%   [EVAL] batch:  250 | acc: 75.00%,  total acc: 72.16%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 72.22%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 72.28%   [EVAL] batch:  253 | acc: 75.00%,  total acc: 72.29%   [EVAL] batch:  254 | acc: 87.50%,  total acc: 72.35%   [EVAL] batch:  255 | acc: 87.50%,  total acc: 72.41%   [EVAL] batch:  256 | acc: 87.50%,  total acc: 72.47%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 72.48%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 72.49%   [EVAL] batch:  259 | acc: 81.25%,  total acc: 72.52%   [EVAL] batch:  260 | acc: 68.75%,  total acc: 72.51%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 72.50%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 72.55%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 72.59%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 72.62%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 72.65%   [EVAL] batch:  266 | acc: 100.00%,  total acc: 72.75%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 72.83%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 72.86%   [EVAL] batch:  269 | acc: 81.25%,  total acc: 72.89%   [EVAL] batch:  270 | acc: 68.75%,  total acc: 72.88%   [EVAL] batch:  271 | acc: 50.00%,  total acc: 72.79%   [EVAL] batch:  272 | acc: 75.00%,  total acc: 72.80%   [EVAL] batch:  273 | acc: 75.00%,  total acc: 72.81%   [EVAL] batch:  274 | acc: 62.50%,  total acc: 72.77%   [EVAL] batch:  275 | acc: 93.75%,  total acc: 72.85%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 72.95%   [EVAL] batch:  277 | acc: 93.75%,  total acc: 73.02%   [EVAL] batch:  278 | acc: 93.75%,  total acc: 73.10%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 73.19%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 73.29%   [EVAL] batch:  281 | acc: 100.00%,  total acc: 73.38%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 73.48%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 73.57%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 73.66%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 73.75%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 73.85%   [EVAL] batch:  287 | acc: 100.00%,  total acc: 73.94%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 74.03%   [EVAL] batch:  289 | acc: 93.75%,  total acc: 74.09%   [EVAL] batch:  290 | acc: 81.25%,  total acc: 74.12%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 74.19%   [EVAL] batch:  292 | acc: 93.75%,  total acc: 74.25%   [EVAL] batch:  293 | acc: 75.00%,  total acc: 74.26%   [EVAL] batch:  294 | acc: 25.00%,  total acc: 74.09%   [EVAL] batch:  295 | acc: 18.75%,  total acc: 73.90%   [EVAL] batch:  296 | acc: 25.00%,  total acc: 73.74%   [EVAL] batch:  297 | acc: 37.50%,  total acc: 73.62%   [EVAL] batch:  298 | acc: 37.50%,  total acc: 73.49%   [EVAL] batch:  299 | acc: 18.75%,  total acc: 73.31%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 73.38%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 73.45%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 73.51%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 73.54%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 73.61%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 73.67%   [EVAL] batch:  306 | acc: 50.00%,  total acc: 73.60%   [EVAL] batch:  307 | acc: 18.75%,  total acc: 73.42%   [EVAL] batch:  308 | acc: 25.00%,  total acc: 73.26%   [EVAL] batch:  309 | acc: 31.25%,  total acc: 73.12%   [EVAL] batch:  310 | acc: 31.25%,  total acc: 72.99%   [EVAL] batch:  311 | acc: 18.75%,  total acc: 72.82%   [EVAL] batch:  312 | acc: 37.50%,  total acc: 72.70%   [EVAL] batch:  313 | acc: 43.75%,  total acc: 72.61%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 72.44%   [EVAL] batch:  315 | acc: 12.50%,  total acc: 72.25%   [EVAL] batch:  316 | acc: 31.25%,  total acc: 72.12%   [EVAL] batch:  317 | acc: 37.50%,  total acc: 72.01%   [EVAL] batch:  318 | acc: 37.50%,  total acc: 71.90%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 71.95%   [EVAL] batch:  320 | acc: 81.25%,  total acc: 71.98%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 72.01%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 72.08%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 72.13%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 72.15%   [EVAL] batch:  325 | acc: 56.25%,  total acc: 72.11%   [EVAL] batch:  326 | acc: 31.25%,  total acc: 71.98%   [EVAL] batch:  327 | acc: 43.75%,  total acc: 71.89%   [EVAL] batch:  328 | acc: 43.75%,  total acc: 71.81%   [EVAL] batch:  329 | acc: 68.75%,  total acc: 71.80%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 71.79%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 71.84%   [EVAL] batch:  332 | acc: 93.75%,  total acc: 71.90%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 71.99%   [EVAL] batch:  334 | acc: 87.50%,  total acc: 72.03%   [EVAL] batch:  335 | acc: 93.75%,  total acc: 72.10%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 72.18%   [EVAL] batch:  337 | acc: 81.25%,  total acc: 72.21%   [EVAL] batch:  338 | acc: 100.00%,  total acc: 72.29%   [EVAL] batch:  339 | acc: 100.00%,  total acc: 72.37%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 72.45%   [EVAL] batch:  341 | acc: 100.00%,  total acc: 72.53%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 72.61%   [EVAL] batch:  343 | acc: 100.00%,  total acc: 72.69%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 72.77%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 72.85%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 72.91%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 72.97%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 73.05%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 73.12%   [EVAL] batch:  350 | acc: 25.00%,  total acc: 72.99%   [EVAL] batch:  351 | acc: 18.75%,  total acc: 72.83%   [EVAL] batch:  352 | acc: 37.50%,  total acc: 72.73%   [EVAL] batch:  353 | acc: 50.00%,  total acc: 72.67%   [EVAL] batch:  354 | acc: 25.00%,  total acc: 72.54%   [EVAL] batch:  355 | acc: 25.00%,  total acc: 72.40%   [EVAL] batch:  356 | acc: 93.75%,  total acc: 72.46%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 72.52%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 72.58%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:  360 | acc: 93.75%,  total acc: 72.71%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 72.76%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 72.73%   [EVAL] batch:  363 | acc: 68.75%,  total acc: 72.72%   [EVAL] batch:  364 | acc: 56.25%,  total acc: 72.67%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 72.61%   [EVAL] batch:  366 | acc: 62.50%,  total acc: 72.58%   [EVAL] batch:  367 | acc: 50.00%,  total acc: 72.52%   [EVAL] batch:  368 | acc: 37.50%,  total acc: 72.43%   [EVAL] batch:  369 | acc: 87.50%,  total acc: 72.47%   [EVAL] batch:  370 | acc: 75.00%,  total acc: 72.47%   [EVAL] batch:  371 | acc: 56.25%,  total acc: 72.43%   [EVAL] batch:  372 | acc: 68.75%,  total acc: 72.42%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 72.46%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 72.43%   
cur_acc:  ['0.9494', '0.8998', '0.6488', '0.7609', '0.8165', '0.7014']
his_acc:  ['0.9494', '0.9045', '0.8052', '0.7827', '0.7592', '0.7243']
CurrentTrain: epoch  0, batch     0 | loss: 5.1035357CurrentTrain: epoch  0, batch     1 | loss: 5.9640050CurrentTrain: epoch  0, batch     2 | loss: 5.6103539CurrentTrain: epoch  0, batch     3 | loss: 9.1687469CurrentTrain: epoch  1, batch     0 | loss: 5.1041098CurrentTrain: epoch  1, batch     1 | loss: 4.0731821CurrentTrain: epoch  1, batch     2 | loss: 3.7793374CurrentTrain: epoch  1, batch     3 | loss: 4.1874166CurrentTrain: epoch  2, batch     0 | loss: 4.2394829CurrentTrain: epoch  2, batch     1 | loss: 3.7137718CurrentTrain: epoch  2, batch     2 | loss: 3.3204639CurrentTrain: epoch  2, batch     3 | loss: 3.7046065CurrentTrain: epoch  3, batch     0 | loss: 3.8467498CurrentTrain: epoch  3, batch     1 | loss: 2.8974900CurrentTrain: epoch  3, batch     2 | loss: 3.7892663CurrentTrain: epoch  3, batch     3 | loss: 4.0616899CurrentTrain: epoch  4, batch     0 | loss: 3.1108105CurrentTrain: epoch  4, batch     1 | loss: 3.6421134CurrentTrain: epoch  4, batch     2 | loss: 3.1195455CurrentTrain: epoch  4, batch     3 | loss: 2.9645534CurrentTrain: epoch  5, batch     0 | loss: 3.0176017CurrentTrain: epoch  5, batch     1 | loss: 3.2164176CurrentTrain: epoch  5, batch     2 | loss: 2.9279380CurrentTrain: epoch  5, batch     3 | loss: 2.0782800CurrentTrain: epoch  6, batch     0 | loss: 2.3280177CurrentTrain: epoch  6, batch     1 | loss: 2.7252192CurrentTrain: epoch  6, batch     2 | loss: 2.7143867CurrentTrain: epoch  6, batch     3 | loss: 2.5216401CurrentTrain: epoch  7, batch     0 | loss: 2.6743824CurrentTrain: epoch  7, batch     1 | loss: 2.1645460CurrentTrain: epoch  7, batch     2 | loss: 2.4485676CurrentTrain: epoch  7, batch     3 | loss: 2.2454429CurrentTrain: epoch  8, batch     0 | loss: 2.1115065CurrentTrain: epoch  8, batch     1 | loss: 2.3942091CurrentTrain: epoch  8, batch     2 | loss: 2.4609108CurrentTrain: epoch  8, batch     3 | loss: 1.9510615CurrentTrain: epoch  9, batch     0 | loss: 2.1362710CurrentTrain: epoch  9, batch     1 | loss: 2.2051783CurrentTrain: epoch  9, batch     2 | loss: 2.2057536CurrentTrain: epoch  9, batch     3 | loss: 1.7952166
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: rohan is the son of narayana murthy , founder of infosys , and his wife sudha murthy , an engineer and author of note .
Head Entity: narayana murthy
Tail Entity: sudha murthy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: elon musk, the CEO of spacex and tesla, was previously married to talulah riley, a talented actress and writer.  
Head Entity: elon musk  
Tail Entity: talulah riley  

Relation: spouse  
Context: barack obama, the 44th president of the united states, has been married to michelle obama since 1992, and they have two daughters together.  
Head Entity: barack obama  
Tail Entity: michelle obama  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: darlene released , " darlene mccoy " , on may 8 , 2007 with label emi gospel .
Head Entity: darlene mccoy
Tail Entity: emi gospel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: the band released their latest album, "echoes of the past," under the label universal music group in 2021.  
Head Entity: the band  
Tail Entity: universal music group  

Relation: record label  
Context: after signing a new contract, the artist announced that their upcoming single would be distributed by sony music entertainment.  
Head Entity: the artist  
Tail Entity: sony music entertainment  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: " the many adventures of winnie the pooh " is a 1977 american animated buddy musical comedy film produced by walt disney productions and distributed by buena vista distribution .
Head Entity: the many adventures of winnie the pooh
Tail Entity: buena vista distribution
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: "Inception" is a 2010 science fiction film directed by Christopher Nolan and distributed by Warner Bros. Pictures.  
Head Entity: Inception  
Tail Entity: Warner Bros. Pictures  

Relation: distributor  
Context: "The Dark Knight" is a 2008 superhero film directed by Christopher Nolan and distributed by Warner Bros. Pictures.  
Head Entity: The Dark Knight  
Tail Entity: Warner Bros. Pictures  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: in 1796 carondelet commissioned pintado to conduct a survey of lands granted to ambrosio longue - épée in iberville county on the west bank of the mississippi river opposite baton rouge .
Head Entity: baton rouge
Tail Entity: mississippi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: the city of new orleans is situated on the banks of the mississippi river, making it a vibrant hub for trade and culture.  
Head Entity: new orleans  
Tail Entity: mississippi river  

Relation: located in or next to body of water  
Context: the picturesque village of lake placid is nestled beside the serene waters of lake placid, attracting tourists year-round.  
Head Entity: lake placid  
Tail Entity: lake placid  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: following the sale of amstrad plc to rupert murdoch 's bskyb , viglen is now lord sugar 's sole it establishment .
Head Entity: bskyb
Tail Entity: amstrad
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Relation: subsidiary  
Context: In 2018, Facebook acquired WhatsApp, making it a subsidiary of the social media giant.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Relation: subsidiary  
Context: After the merger, the new company announced that its subsidiary, Zappos, would continue to operate independently.  
Head Entity: new company  
Tail Entity: Zappos  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: he challenged seven other swimmers in heat four including ukraine 's rostyslav svanidze , a top 16 finalist in atlanta four years earlier .
Head Entity: rostyslav svanidze
Tail Entity: ukraine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: after years of living in the united states, the renowned artist decided to apply for dual citizenship, allowing him to maintain his ties to his homeland, italy.  
Head Entity: the renowned artist  
Tail Entity: italy  

Relation: country of citizenship  
Context: during the international conference, the delegate from canada proudly spoke about the rights and privileges of being a citizen of his country, emphasizing the importance of citizenship in shaping national identity.  
Head Entity: the delegate from canada  
Tail Entity: canada  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: he is also a part of the atlanta - based hip - hop / r&b;/soul musical collective the dungeon family .
Head Entity: the dungeon family
Tail Entity: hip - hop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the band is known for their unique blend of rock and electronic music, often categorized under the genre of synth-pop.  
Head Entity: the band  
Tail Entity: synth-pop  

Relation: genre  
Context: she has made significant contributions to the world of classical music, particularly in the genre of opera.  
Head Entity: she  
Tail Entity: opera  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: in the west , the rivers rib , ash and stort flow south from the hundred parishes to meet the lea and then the thames .
Head Entity: rib
Tail Entity: lea
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: The river Seine flows through Paris and eventually empties into the English Channel, marking its mouth.  
Head Entity: Seine  
Tail Entity: English Channel  

Relation: mouth of the watercourse  
Context: The Mississippi River travels a long distance before reaching its mouth at the Gulf of Mexico, where it meets the sea.  
Head Entity: Mississippi River  
Tail Entity: Gulf of Mexico  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: bradley halliday ( born 10 july 1995 ) is an english professional footballer who plays as a defender or a midfielder for league two club cambridge united .
Head Entity: bradley halliday
Tail Entity: defender
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: alex morgan ( born 2 july 1989 ) is an american soccer player who plays as a forward for the national team and the club orlando pride in the national women's soccer league.  
Head Entity: alex morgan  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: lebron james ( born 30 december 1984 ) is an american professional basketball player who plays as a small forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: small forward  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: the stade louis ii in monaco was the venue for the uefa super cup every year since 1998 .
Head Entity: 1998
Tail Entity: uefa super cup
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2020 NBA season was significantly impacted by the COVID-19 pandemic, leading to a unique playoff format.  
Head Entity: 2020  
Tail Entity: NBA  

Relation: sports season of league or competition  
Context: The 2019 Rugby World Cup took place in Japan, showcasing teams from around the globe.  
Head Entity: 2019  
Tail Entity: Rugby World Cup  
Mixup data size:  499
MixupTrain:  epoch  0, batch     0 | loss: 2.3777317MixupTrain:  epoch  0, batch     1 | loss: 2.2531343MixupTrain:  epoch  0, batch     2 | loss: 1.7840609MixupTrain:  epoch  0, batch     3 | loss: 2.2202040MixupTrain:  epoch  0, batch     4 | loss: 1.9167224MixupTrain:  epoch  0, batch     5 | loss: 1.9130984MixupTrain:  epoch  0, batch     6 | loss: 2.2872405MixupTrain:  epoch  0, batch     7 | loss: 1.9845038MixupTrain:  epoch  0, batch     8 | loss: 1.7294843MixupTrain:  epoch  0, batch     9 | loss: 2.2461947MixupTrain:  epoch  0, batch    10 | loss: 2.1623580MixupTrain:  epoch  0, batch    11 | loss: 2.0246373MixupTrain:  epoch  0, batch    12 | loss: 1.7902626MixupTrain:  epoch  0, batch    13 | loss: 1.7189802MixupTrain:  epoch  0, batch    14 | loss: 1.9210941MixupTrain:  epoch  0, batch    15 | loss: 2.6342190MixupTrain:  epoch  0, batch    16 | loss: 1.9287908MixupTrain:  epoch  0, batch    17 | loss: 1.9710948MixupTrain:  epoch  0, batch    18 | loss: 2.1776097MixupTrain:  epoch  0, batch    19 | loss: 1.8360767MixupTrain:  epoch  0, batch    20 | loss: 1.9513877MixupTrain:  epoch  0, batch    21 | loss: 2.1904154MixupTrain:  epoch  0, batch    22 | loss: 2.3134783MixupTrain:  epoch  0, batch    23 | loss: 1.9536101MixupTrain:  epoch  0, batch    24 | loss: 2.0772719MixupTrain:  epoch  0, batch    25 | loss: 1.9134588MixupTrain:  epoch  0, batch    26 | loss: 1.7696071MixupTrain:  epoch  0, batch    27 | loss: 1.7295133MixupTrain:  epoch  0, batch    28 | loss: 1.9739080MixupTrain:  epoch  0, batch    29 | loss: 1.8075882MixupTrain:  epoch  0, batch    30 | loss: 2.0999665MixupTrain:  epoch  0, batch    31 | loss: 1.5507916
MemoryTrain:  epoch  0, batch     0 | loss: 1.4071869MemoryTrain:  epoch  0, batch     1 | loss: 1.5192881MemoryTrain:  epoch  0, batch     2 | loss: 2.1043847MemoryTrain:  epoch  0, batch     3 | loss: 2.5515261MemoryTrain:  epoch  0, batch     4 | loss: 2.4217496MemoryTrain:  epoch  0, batch     5 | loss: 1.5040163MemoryTrain:  epoch  0, batch     6 | loss: 2.2438705MemoryTrain:  epoch  0, batch     7 | loss: 1.8160998MemoryTrain:  epoch  0, batch     8 | loss: 1.7733920MemoryTrain:  epoch  0, batch     9 | loss: 2.6620150MemoryTrain:  epoch  0, batch    10 | loss: 2.3476510MemoryTrain:  epoch  0, batch    11 | loss: 1.8778653MemoryTrain:  epoch  0, batch    12 | loss: 2.7719364MemoryTrain:  epoch  0, batch    13 | loss: 1.7372389MemoryTrain:  epoch  1, batch     0 | loss: 1.7938136MemoryTrain:  epoch  1, batch     1 | loss: 2.0760930MemoryTrain:  epoch  1, batch     2 | loss: 1.5774893MemoryTrain:  epoch  1, batch     3 | loss: 1.6829702MemoryTrain:  epoch  1, batch     4 | loss: 1.7810092MemoryTrain:  epoch  1, batch     5 | loss: 1.5188700MemoryTrain:  epoch  1, batch     6 | loss: 2.2908511MemoryTrain:  epoch  1, batch     7 | loss: 1.6123862MemoryTrain:  epoch  1, batch     8 | loss: 1.5424122MemoryTrain:  epoch  1, batch     9 | loss: 1.7253532MemoryTrain:  epoch  1, batch    10 | loss: 1.8268247MemoryTrain:  epoch  1, batch    11 | loss: 1.7646830MemoryTrain:  epoch  1, batch    12 | loss: 1.5868365MemoryTrain:  epoch  1, batch    13 | loss: 1.2740555MemoryTrain:  epoch  2, batch     0 | loss: 1.7079921MemoryTrain:  epoch  2, batch     1 | loss: 1.6564294MemoryTrain:  epoch  2, batch     2 | loss: 1.5355070MemoryTrain:  epoch  2, batch     3 | loss: 1.6687698MemoryTrain:  epoch  2, batch     4 | loss: 1.5816011MemoryTrain:  epoch  2, batch     5 | loss: 1.6604154MemoryTrain:  epoch  2, batch     6 | loss: 1.4280128MemoryTrain:  epoch  2, batch     7 | loss: 1.2762592MemoryTrain:  epoch  2, batch     8 | loss: 1.6657062MemoryTrain:  epoch  2, batch     9 | loss: 1.4902270MemoryTrain:  epoch  2, batch    10 | loss: 1.6623205MemoryTrain:  epoch  2, batch    11 | loss: 1.6015141MemoryTrain:  epoch  2, batch    12 | loss: 1.2808123MemoryTrain:  epoch  2, batch    13 | loss: 1.2359920MemoryTrain:  epoch  3, batch     0 | loss: 1.3273447MemoryTrain:  epoch  3, batch     1 | loss: 1.4389485MemoryTrain:  epoch  3, batch     2 | loss: 1.4236922MemoryTrain:  epoch  3, batch     3 | loss: 1.4875989MemoryTrain:  epoch  3, batch     4 | loss: 1.3633786MemoryTrain:  epoch  3, batch     5 | loss: 1.6550586MemoryTrain:  epoch  3, batch     6 | loss: 1.4362997MemoryTrain:  epoch  3, batch     7 | loss: 1.7963955MemoryTrain:  epoch  3, batch     8 | loss: 1.2560043MemoryTrain:  epoch  3, batch     9 | loss: 1.4507414MemoryTrain:  epoch  3, batch    10 | loss: 1.2402911MemoryTrain:  epoch  3, batch    11 | loss: 1.5511031MemoryTrain:  epoch  3, batch    12 | loss: 1.3711826MemoryTrain:  epoch  3, batch    13 | loss: 1.2608455MemoryTrain:  epoch  4, batch     0 | loss: 1.5760622MemoryTrain:  epoch  4, batch     1 | loss: 1.5433260MemoryTrain:  epoch  4, batch     2 | loss: 1.2743223MemoryTrain:  epoch  4, batch     3 | loss: 1.2837431MemoryTrain:  epoch  4, batch     4 | loss: 1.2830133MemoryTrain:  epoch  4, batch     5 | loss: 1.3416699MemoryTrain:  epoch  4, batch     6 | loss: 1.5423205MemoryTrain:  epoch  4, batch     7 | loss: 1.2277546MemoryTrain:  epoch  4, batch     8 | loss: 1.4179332MemoryTrain:  epoch  4, batch     9 | loss: 1.3803725MemoryTrain:  epoch  4, batch    10 | loss: 1.4296496MemoryTrain:  epoch  4, batch    11 | loss: 1.3492742MemoryTrain:  epoch  4, batch    12 | loss: 1.2938709MemoryTrain:  epoch  4, batch    13 | loss: 1.3989449MemoryTrain:  epoch  5, batch     0 | loss: 1.3968673MemoryTrain:  epoch  5, batch     1 | loss: 1.3961385MemoryTrain:  epoch  5, batch     2 | loss: 1.2882907MemoryTrain:  epoch  5, batch     3 | loss: 1.2663983MemoryTrain:  epoch  5, batch     4 | loss: 1.4091597MemoryTrain:  epoch  5, batch     5 | loss: 1.3086522MemoryTrain:  epoch  5, batch     6 | loss: 1.2998525MemoryTrain:  epoch  5, batch     7 | loss: 1.3320975MemoryTrain:  epoch  5, batch     8 | loss: 1.2317781MemoryTrain:  epoch  5, batch     9 | loss: 1.2920927MemoryTrain:  epoch  5, batch    10 | loss: 1.3766929MemoryTrain:  epoch  5, batch    11 | loss: 1.2777822MemoryTrain:  epoch  5, batch    12 | loss: 1.2985144MemoryTrain:  epoch  5, batch    13 | loss: 1.4737798MemoryTrain:  epoch  6, batch     0 | loss: 1.2361906MemoryTrain:  epoch  6, batch     1 | loss: 1.2712511MemoryTrain:  epoch  6, batch     2 | loss: 1.4021311MemoryTrain:  epoch  6, batch     3 | loss: 1.2544862MemoryTrain:  epoch  6, batch     4 | loss: 1.2565360MemoryTrain:  epoch  6, batch     5 | loss: 1.3126273MemoryTrain:  epoch  6, batch     6 | loss: 1.2368747MemoryTrain:  epoch  6, batch     7 | loss: 1.4474330MemoryTrain:  epoch  6, batch     8 | loss: 1.2916498MemoryTrain:  epoch  6, batch     9 | loss: 1.2589562MemoryTrain:  epoch  6, batch    10 | loss: 1.3607712MemoryTrain:  epoch  6, batch    11 | loss: 1.3133898MemoryTrain:  epoch  6, batch    12 | loss: 1.1959728MemoryTrain:  epoch  6, batch    13 | loss: 1.3510609MemoryTrain:  epoch  7, batch     0 | loss: 1.3274753MemoryTrain:  epoch  7, batch     1 | loss: 1.2220094MemoryTrain:  epoch  7, batch     2 | loss: 1.2859119MemoryTrain:  epoch  7, batch     3 | loss: 1.2188144MemoryTrain:  epoch  7, batch     4 | loss: 1.2419624MemoryTrain:  epoch  7, batch     5 | loss: 1.3018095MemoryTrain:  epoch  7, batch     6 | loss: 1.2132668MemoryTrain:  epoch  7, batch     7 | loss: 1.2417942MemoryTrain:  epoch  7, batch     8 | loss: 1.3006675MemoryTrain:  epoch  7, batch     9 | loss: 1.2557646MemoryTrain:  epoch  7, batch    10 | loss: 1.3209217MemoryTrain:  epoch  7, batch    11 | loss: 1.3609698MemoryTrain:  epoch  7, batch    12 | loss: 1.3209599MemoryTrain:  epoch  7, batch    13 | loss: 1.5403601MemoryTrain:  epoch  8, batch     0 | loss: 1.2784030MemoryTrain:  epoch  8, batch     1 | loss: 1.3226247MemoryTrain:  epoch  8, batch     2 | loss: 1.2762141MemoryTrain:  epoch  8, batch     3 | loss: 1.3365484MemoryTrain:  epoch  8, batch     4 | loss: 1.2787266MemoryTrain:  epoch  8, batch     5 | loss: 1.2991340MemoryTrain:  epoch  8, batch     6 | loss: 1.2161903MemoryTrain:  epoch  8, batch     7 | loss: 1.2275383MemoryTrain:  epoch  8, batch     8 | loss: 1.4336772MemoryTrain:  epoch  8, batch     9 | loss: 1.2293239MemoryTrain:  epoch  8, batch    10 | loss: 1.2900243MemoryTrain:  epoch  8, batch    11 | loss: 1.2572823MemoryTrain:  epoch  8, batch    12 | loss: 1.2422581MemoryTrain:  epoch  8, batch    13 | loss: 1.2837138MemoryTrain:  epoch  9, batch     0 | loss: 1.2654014MemoryTrain:  epoch  9, batch     1 | loss: 1.2665464MemoryTrain:  epoch  9, batch     2 | loss: 1.2172270MemoryTrain:  epoch  9, batch     3 | loss: 1.2979151MemoryTrain:  epoch  9, batch     4 | loss: 1.2029603MemoryTrain:  epoch  9, batch     5 | loss: 1.2723562MemoryTrain:  epoch  9, batch     6 | loss: 1.2631449MemoryTrain:  epoch  9, batch     7 | loss: 1.2627242MemoryTrain:  epoch  9, batch     8 | loss: 1.2991289MemoryTrain:  epoch  9, batch     9 | loss: 1.2722373MemoryTrain:  epoch  9, batch    10 | loss: 1.2537193MemoryTrain:  epoch  9, batch    11 | loss: 1.2568460MemoryTrain:  epoch  9, batch    12 | loss: 1.3096968MemoryTrain:  epoch  9, batch    13 | loss: 1.1725628
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 53.12%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 57.29%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 71.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 73.86%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 76.04%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 77.40%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 73.75%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 74.61%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 74.31%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 74.34%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 74.69%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 75.30%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 75.57%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 75.82%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 76.56%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 77.00%   [EVAL] batch:   25 | acc: 25.00%,  total acc: 75.00%   [EVAL] batch:   26 | acc: 31.25%,  total acc: 73.38%   [EVAL] batch:   27 | acc: 37.50%,  total acc: 72.10%   [EVAL] batch:   28 | acc: 25.00%,  total acc: 70.47%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 69.79%   [EVAL] batch:   30 | acc: 25.00%,  total acc: 68.35%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 68.16%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 67.80%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 67.46%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 67.32%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 67.36%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 67.06%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 67.43%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 67.95%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 68.44%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 68.90%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 69.20%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 69.62%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 70.03%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 69.17%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 68.89%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 67.95%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 67.71%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 66.71%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 66.38%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 67.03%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 67.67%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 68.28%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 68.87%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 69.43%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 69.98%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 70.50%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 70.58%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 70.87%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 71.04%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 71.11%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 71.47%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 70.93%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 67.86%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 66.41%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 65.97%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 65.00%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 64.77%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 63.54%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 63.94%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 66.07%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 67.92%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 69.53%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 70.96%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 72.22%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 73.36%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 73.12%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 73.81%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 73.58%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 73.37%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 73.70%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 74.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 75.93%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 76.56%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 77.16%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 77.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 78.63%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 78.91%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 78.12%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 78.04%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 77.95%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 77.53%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 77.80%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 78.37%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 79.42%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 79.76%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 79.94%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 80.11%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 79.62%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 79.26%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 79.04%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 78.70%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 78.12%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 77.70%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 77.76%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 77.83%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 77.66%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 77.16%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 77.12%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 77.19%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 76.94%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 77.01%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 77.15%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 77.02%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 77.18%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 77.44%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 77.69%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 78.03%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 78.08%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 78.40%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 78.62%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 78.57%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 78.43%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 78.21%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 78.08%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 78.04%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 77.67%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 77.63%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 77.68%   [EVAL] batch:   77 | acc: 93.75%,  total acc: 77.88%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 77.77%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 77.66%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 77.62%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 77.52%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 77.33%   [EVAL] batch:   83 | acc: 43.75%,  total acc: 76.93%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 76.54%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 76.24%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 75.93%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 75.92%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 76.19%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 76.46%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 76.72%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 76.97%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 77.22%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 77.39%   [EVAL] batch:   94 | acc: 62.50%,  total acc: 77.24%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 77.28%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 77.19%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 77.04%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 76.75%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 76.98%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 77.21%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 77.43%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 77.64%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 77.80%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 77.95%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 78.15%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 78.36%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 78.56%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 78.94%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 79.07%   [EVAL] batch:  112 | acc: 81.25%,  total acc: 79.09%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 78.95%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 79.02%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 79.04%   [EVAL] batch:  116 | acc: 68.75%,  total acc: 78.95%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 79.03%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 78.94%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 79.01%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 79.03%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 79.15%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 79.32%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 79.44%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 79.55%   [EVAL] batch:  125 | acc: 0.00%,  total acc: 78.92%   [EVAL] batch:  126 | acc: 6.25%,  total acc: 78.35%   [EVAL] batch:  127 | acc: 18.75%,  total acc: 77.88%   [EVAL] batch:  128 | acc: 12.50%,  total acc: 77.37%   [EVAL] batch:  129 | acc: 31.25%,  total acc: 77.02%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 76.62%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 76.52%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 76.64%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 76.82%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 76.90%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 77.02%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 77.14%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 77.17%   [EVAL] batch:  138 | acc: 81.25%,  total acc: 77.20%   [EVAL] batch:  139 | acc: 81.25%,  total acc: 77.23%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 77.30%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 77.38%   [EVAL] batch:  142 | acc: 81.25%,  total acc: 77.40%   [EVAL] batch:  143 | acc: 50.00%,  total acc: 77.21%   [EVAL] batch:  144 | acc: 18.75%,  total acc: 76.81%   [EVAL] batch:  145 | acc: 12.50%,  total acc: 76.37%   [EVAL] batch:  146 | acc: 37.50%,  total acc: 76.11%   [EVAL] batch:  147 | acc: 31.25%,  total acc: 75.80%   [EVAL] batch:  148 | acc: 25.00%,  total acc: 75.46%   [EVAL] batch:  149 | acc: 31.25%,  total acc: 75.17%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 74.71%   [EVAL] batch:  151 | acc: 31.25%,  total acc: 74.42%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 73.98%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 73.54%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 73.15%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 72.68%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 72.61%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 72.71%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 72.76%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 72.89%   [EVAL] batch:  160 | acc: 81.25%,  total acc: 72.94%   [EVAL] batch:  161 | acc: 75.00%,  total acc: 72.96%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 73.08%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 73.17%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 73.33%   [EVAL] batch:  165 | acc: 87.50%,  total acc: 73.42%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 73.50%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 73.66%   [EVAL] batch:  168 | acc: 100.00%,  total acc: 73.82%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 73.75%   [EVAL] batch:  170 | acc: 62.50%,  total acc: 73.68%   [EVAL] batch:  171 | acc: 56.25%,  total acc: 73.58%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 73.55%   [EVAL] batch:  173 | acc: 56.25%,  total acc: 73.46%   [EVAL] batch:  174 | acc: 56.25%,  total acc: 73.36%   [EVAL] batch:  175 | acc: 12.50%,  total acc: 73.01%   [EVAL] batch:  176 | acc: 37.50%,  total acc: 72.81%   [EVAL] batch:  177 | acc: 31.25%,  total acc: 72.58%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 72.45%   [EVAL] batch:  179 | acc: 37.50%,  total acc: 72.26%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 72.17%   [EVAL] batch:  181 | acc: 37.50%,  total acc: 71.98%   [EVAL] batch:  182 | acc: 43.75%,  total acc: 71.82%   [EVAL] batch:  183 | acc: 37.50%,  total acc: 71.64%   [EVAL] batch:  184 | acc: 56.25%,  total acc: 71.55%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 71.44%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 71.46%   [EVAL] batch:  187 | acc: 31.25%,  total acc: 71.24%   [EVAL] batch:  188 | acc: 75.00%,  total acc: 71.26%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 71.25%   [EVAL] batch:  190 | acc: 68.75%,  total acc: 71.24%   [EVAL] batch:  191 | acc: 62.50%,  total acc: 71.19%   [EVAL] batch:  192 | acc: 62.50%,  total acc: 71.15%   [EVAL] batch:  193 | acc: 56.25%,  total acc: 71.07%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 71.09%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 71.08%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 71.07%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 71.09%   [EVAL] batch:  198 | acc: 25.00%,  total acc: 70.85%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 71.00%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 71.05%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 71.13%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 71.15%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 71.20%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 71.25%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 71.36%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 71.47%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 71.60%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:  209 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 71.98%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 72.11%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 72.04%   [EVAL] batch:  213 | acc: 12.50%,  total acc: 71.76%   [EVAL] batch:  214 | acc: 25.00%,  total acc: 71.54%   [EVAL] batch:  215 | acc: 25.00%,  total acc: 71.33%   [EVAL] batch:  216 | acc: 12.50%,  total acc: 71.05%   [EVAL] batch:  217 | acc: 18.75%,  total acc: 70.81%   [EVAL] batch:  218 | acc: 31.25%,  total acc: 70.63%   [EVAL] batch:  219 | acc: 56.25%,  total acc: 70.57%   [EVAL] batch:  220 | acc: 25.00%,  total acc: 70.36%   [EVAL] batch:  221 | acc: 18.75%,  total acc: 70.13%   [EVAL] batch:  222 | acc: 18.75%,  total acc: 69.90%   [EVAL] batch:  223 | acc: 25.00%,  total acc: 69.70%   [EVAL] batch:  224 | acc: 25.00%,  total acc: 69.50%   [EVAL] batch:  225 | acc: 56.25%,  total acc: 69.44%   [EVAL] batch:  226 | acc: 56.25%,  total acc: 69.38%   [EVAL] batch:  227 | acc: 43.75%,  total acc: 69.27%   [EVAL] batch:  228 | acc: 75.00%,  total acc: 69.30%   [EVAL] batch:  229 | acc: 62.50%,  total acc: 69.27%   [EVAL] batch:  230 | acc: 43.75%,  total acc: 69.16%   [EVAL] batch:  231 | acc: 75.00%,  total acc: 69.18%   [EVAL] batch:  232 | acc: 75.00%,  total acc: 69.21%   [EVAL] batch:  233 | acc: 68.75%,  total acc: 69.20%   [EVAL] batch:  234 | acc: 62.50%,  total acc: 69.18%   [EVAL] batch:  235 | acc: 75.00%,  total acc: 69.20%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 69.28%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 69.35%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 69.48%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 69.58%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 69.68%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 69.81%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 69.93%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 70.06%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 70.18%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 70.22%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 70.32%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 70.36%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 70.48%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 70.60%   [EVAL] batch:  250 | acc: 50.00%,  total acc: 70.52%   [EVAL] batch:  251 | acc: 62.50%,  total acc: 70.49%   [EVAL] batch:  252 | acc: 56.25%,  total acc: 70.43%   [EVAL] batch:  253 | acc: 50.00%,  total acc: 70.35%   [EVAL] batch:  254 | acc: 50.00%,  total acc: 70.27%   [EVAL] batch:  255 | acc: 68.75%,  total acc: 70.26%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 70.18%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 70.18%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 70.17%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 70.19%   [EVAL] batch:  260 | acc: 68.75%,  total acc: 70.19%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 70.18%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 70.27%   [EVAL] batch:  263 | acc: 62.50%,  total acc: 70.24%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 70.28%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 70.32%   [EVAL] batch:  266 | acc: 100.00%,  total acc: 70.44%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 70.52%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 70.56%   [EVAL] batch:  269 | acc: 81.25%,  total acc: 70.60%   [EVAL] batch:  270 | acc: 62.50%,  total acc: 70.57%   [EVAL] batch:  271 | acc: 50.00%,  total acc: 70.50%   [EVAL] batch:  272 | acc: 75.00%,  total acc: 70.51%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 70.57%   [EVAL] batch:  274 | acc: 68.75%,  total acc: 70.57%   [EVAL] batch:  275 | acc: 87.50%,  total acc: 70.63%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 70.74%   [EVAL] batch:  277 | acc: 93.75%,  total acc: 70.82%   [EVAL] batch:  278 | acc: 93.75%,  total acc: 70.90%   [EVAL] batch:  279 | acc: 93.75%,  total acc: 70.98%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:  281 | acc: 100.00%,  total acc: 71.19%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 71.29%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 71.39%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 71.49%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 71.59%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 71.69%   [EVAL] batch:  287 | acc: 100.00%,  total acc: 71.79%   [EVAL] batch:  288 | acc: 93.75%,  total acc: 71.86%   [EVAL] batch:  289 | acc: 93.75%,  total acc: 71.94%   [EVAL] batch:  290 | acc: 81.25%,  total acc: 71.97%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 72.05%   [EVAL] batch:  292 | acc: 93.75%,  total acc: 72.12%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 72.15%   [EVAL] batch:  294 | acc: 31.25%,  total acc: 72.01%   [EVAL] batch:  295 | acc: 18.75%,  total acc: 71.83%   [EVAL] batch:  296 | acc: 31.25%,  total acc: 71.70%   [EVAL] batch:  297 | acc: 50.00%,  total acc: 71.62%   [EVAL] batch:  298 | acc: 43.75%,  total acc: 71.53%   [EVAL] batch:  299 | acc: 50.00%,  total acc: 71.46%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 71.53%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 71.61%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 71.68%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 71.71%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 71.78%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 71.83%   [EVAL] batch:  306 | acc: 43.75%,  total acc: 71.74%   [EVAL] batch:  307 | acc: 25.00%,  total acc: 71.59%   [EVAL] batch:  308 | acc: 12.50%,  total acc: 71.40%   [EVAL] batch:  309 | acc: 25.00%,  total acc: 71.25%   [EVAL] batch:  310 | acc: 12.50%,  total acc: 71.06%   [EVAL] batch:  311 | acc: 25.00%,  total acc: 70.91%   [EVAL] batch:  312 | acc: 25.00%,  total acc: 70.77%   [EVAL] batch:  313 | acc: 37.50%,  total acc: 70.66%   [EVAL] batch:  314 | acc: 12.50%,  total acc: 70.48%   [EVAL] batch:  315 | acc: 18.75%,  total acc: 70.31%   [EVAL] batch:  316 | acc: 31.25%,  total acc: 70.19%   [EVAL] batch:  317 | acc: 25.00%,  total acc: 70.05%   [EVAL] batch:  318 | acc: 31.25%,  total acc: 69.93%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 69.96%   [EVAL] batch:  320 | acc: 81.25%,  total acc: 70.00%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 70.01%   [EVAL] batch:  322 | acc: 81.25%,  total acc: 70.05%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 70.10%   [EVAL] batch:  324 | acc: 68.75%,  total acc: 70.10%   [EVAL] batch:  325 | acc: 50.00%,  total acc: 70.03%   [EVAL] batch:  326 | acc: 25.00%,  total acc: 69.90%   [EVAL] batch:  327 | acc: 43.75%,  total acc: 69.82%   [EVAL] batch:  328 | acc: 37.50%,  total acc: 69.72%   [EVAL] batch:  329 | acc: 50.00%,  total acc: 69.66%   [EVAL] batch:  330 | acc: 56.25%,  total acc: 69.62%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 69.67%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 69.76%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 69.85%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 69.93%   [EVAL] batch:  335 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 70.09%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 70.14%   [EVAL] batch:  338 | acc: 100.00%,  total acc: 70.22%   [EVAL] batch:  339 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 70.40%   [EVAL] batch:  341 | acc: 100.00%,  total acc: 70.49%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 70.57%   [EVAL] batch:  343 | acc: 100.00%,  total acc: 70.66%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 70.74%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 70.89%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 70.96%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 71.02%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 71.11%   [EVAL] batch:  350 | acc: 12.50%,  total acc: 70.94%   [EVAL] batch:  351 | acc: 6.25%,  total acc: 70.76%   [EVAL] batch:  352 | acc: 31.25%,  total acc: 70.64%   [EVAL] batch:  353 | acc: 18.75%,  total acc: 70.50%   [EVAL] batch:  354 | acc: 6.25%,  total acc: 70.32%   [EVAL] batch:  355 | acc: 25.00%,  total acc: 70.19%   [EVAL] batch:  356 | acc: 93.75%,  total acc: 70.26%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 70.32%   [EVAL] batch:  358 | acc: 87.50%,  total acc: 70.37%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 70.45%   [EVAL] batch:  360 | acc: 93.75%,  total acc: 70.52%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 70.56%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 70.56%   [EVAL] batch:  363 | acc: 62.50%,  total acc: 70.54%   [EVAL] batch:  364 | acc: 50.00%,  total acc: 70.48%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 70.42%   [EVAL] batch:  366 | acc: 56.25%,  total acc: 70.38%   [EVAL] batch:  367 | acc: 37.50%,  total acc: 70.30%   [EVAL] batch:  368 | acc: 31.25%,  total acc: 70.19%   [EVAL] batch:  369 | acc: 87.50%,  total acc: 70.24%   [EVAL] batch:  370 | acc: 81.25%,  total acc: 70.27%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 70.19%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 70.22%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 70.25%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 70.23%   [EVAL] batch:  375 | acc: 56.25%,  total acc: 70.20%   [EVAL] batch:  376 | acc: 75.00%,  total acc: 70.21%   [EVAL] batch:  377 | acc: 43.75%,  total acc: 70.14%   [EVAL] batch:  378 | acc: 37.50%,  total acc: 70.05%   [EVAL] batch:  379 | acc: 68.75%,  total acc: 70.05%   [EVAL] batch:  380 | acc: 62.50%,  total acc: 70.03%   [EVAL] batch:  381 | acc: 93.75%,  total acc: 70.09%   [EVAL] batch:  382 | acc: 100.00%,  total acc: 70.17%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 70.23%   [EVAL] batch:  384 | acc: 81.25%,  total acc: 70.26%   [EVAL] batch:  385 | acc: 100.00%,  total acc: 70.34%   [EVAL] batch:  386 | acc: 100.00%,  total acc: 70.41%   [EVAL] batch:  387 | acc: 93.75%,  total acc: 70.47%   [EVAL] batch:  388 | acc: 43.75%,  total acc: 70.40%   [EVAL] batch:  389 | acc: 56.25%,  total acc: 70.37%   [EVAL] batch:  390 | acc: 87.50%,  total acc: 70.41%   [EVAL] batch:  391 | acc: 81.25%,  total acc: 70.44%   [EVAL] batch:  392 | acc: 62.50%,  total acc: 70.42%   [EVAL] batch:  393 | acc: 75.00%,  total acc: 70.43%   [EVAL] batch:  394 | acc: 81.25%,  total acc: 70.46%   [EVAL] batch:  395 | acc: 87.50%,  total acc: 70.50%   [EVAL] batch:  396 | acc: 81.25%,  total acc: 70.53%   [EVAL] batch:  397 | acc: 81.25%,  total acc: 70.56%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 70.61%   [EVAL] batch:  399 | acc: 87.50%,  total acc: 70.66%   [EVAL] batch:  400 | acc: 25.00%,  total acc: 70.54%   [EVAL] batch:  401 | acc: 31.25%,  total acc: 70.44%   [EVAL] batch:  402 | acc: 37.50%,  total acc: 70.36%   [EVAL] batch:  403 | acc: 25.00%,  total acc: 70.25%   [EVAL] batch:  404 | acc: 50.00%,  total acc: 70.20%   [EVAL] batch:  405 | acc: 25.00%,  total acc: 70.09%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 70.07%   [EVAL] batch:  407 | acc: 56.25%,  total acc: 70.04%   [EVAL] batch:  408 | acc: 56.25%,  total acc: 70.00%   [EVAL] batch:  409 | acc: 62.50%,  total acc: 69.98%   [EVAL] batch:  410 | acc: 68.75%,  total acc: 69.98%   [EVAL] batch:  411 | acc: 56.25%,  total acc: 69.95%   [EVAL] batch:  412 | acc: 81.25%,  total acc: 69.98%   [EVAL] batch:  413 | acc: 87.50%,  total acc: 70.02%   [EVAL] batch:  414 | acc: 87.50%,  total acc: 70.06%   [EVAL] batch:  415 | acc: 87.50%,  total acc: 70.10%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 70.13%   [EVAL] batch:  417 | acc: 87.50%,  total acc: 70.17%   [EVAL] batch:  418 | acc: 87.50%,  total acc: 70.21%   [EVAL] batch:  419 | acc: 31.25%,  total acc: 70.12%   [EVAL] batch:  420 | acc: 56.25%,  total acc: 70.09%   [EVAL] batch:  421 | acc: 25.00%,  total acc: 69.98%   [EVAL] batch:  422 | acc: 56.25%,  total acc: 69.95%   [EVAL] batch:  423 | acc: 18.75%,  total acc: 69.83%   [EVAL] batch:  424 | acc: 50.00%,  total acc: 69.78%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 69.85%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 69.92%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 69.99%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 70.06%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 70.13%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 70.20%   [EVAL] batch:  431 | acc: 100.00%,  total acc: 70.27%   [EVAL] batch:  432 | acc: 75.00%,  total acc: 70.28%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 70.32%   [EVAL] batch:  434 | acc: 81.25%,  total acc: 70.34%   [EVAL] batch:  435 | acc: 75.00%,  total acc: 70.36%   [EVAL] batch:  436 | acc: 93.75%,  total acc: 70.41%   [EVAL] batch:  437 | acc: 37.50%,  total acc: 70.33%   
cur_acc:  ['0.9494', '0.8998', '0.6488', '0.7609', '0.8165', '0.7014', '0.7093']
his_acc:  ['0.9494', '0.9045', '0.8052', '0.7827', '0.7592', '0.7243', '0.7033']
CurrentTrain: epoch  0, batch     0 | loss: 7.1172624CurrentTrain: epoch  0, batch     1 | loss: 6.3320355CurrentTrain: epoch  0, batch     2 | loss: 5.6161480CurrentTrain: epoch  0, batch     3 | loss: 4.8446088CurrentTrain: epoch  1, batch     0 | loss: 6.6056833CurrentTrain: epoch  1, batch     1 | loss: 4.8216896CurrentTrain: epoch  1, batch     2 | loss: 4.6833172CurrentTrain: epoch  1, batch     3 | loss: 5.9355211CurrentTrain: epoch  2, batch     0 | loss: 4.5434790CurrentTrain: epoch  2, batch     1 | loss: 4.6258750CurrentTrain: epoch  2, batch     2 | loss: 5.0871096CurrentTrain: epoch  2, batch     3 | loss: 2.4590874CurrentTrain: epoch  3, batch     0 | loss: 4.3371639CurrentTrain: epoch  3, batch     1 | loss: 5.1574545CurrentTrain: epoch  3, batch     2 | loss: 4.1261902CurrentTrain: epoch  3, batch     3 | loss: 2.2467816CurrentTrain: epoch  4, batch     0 | loss: 3.7510252CurrentTrain: epoch  4, batch     1 | loss: 3.3840199CurrentTrain: epoch  4, batch     2 | loss: 5.3648181CurrentTrain: epoch  4, batch     3 | loss: 3.9536281CurrentTrain: epoch  5, batch     0 | loss: 3.6677792CurrentTrain: epoch  5, batch     1 | loss: 3.5722818CurrentTrain: epoch  5, batch     2 | loss: 4.4314208CurrentTrain: epoch  5, batch     3 | loss: 5.0623856CurrentTrain: epoch  6, batch     0 | loss: 3.4739532CurrentTrain: epoch  6, batch     1 | loss: 3.5844724CurrentTrain: epoch  6, batch     2 | loss: 4.2438889CurrentTrain: epoch  6, batch     3 | loss: 2.3760271CurrentTrain: epoch  7, batch     0 | loss: 3.9281805CurrentTrain: epoch  7, batch     1 | loss: 3.6410375CurrentTrain: epoch  7, batch     2 | loss: 3.1968451CurrentTrain: epoch  7, batch     3 | loss: 2.9112282CurrentTrain: epoch  8, batch     0 | loss: 3.3599999CurrentTrain: epoch  8, batch     1 | loss: 3.1447034CurrentTrain: epoch  8, batch     2 | loss: 3.2863522CurrentTrain: epoch  8, batch     3 | loss: 4.4580760CurrentTrain: epoch  9, batch     0 | loss: 3.0410280CurrentTrain: epoch  9, batch     1 | loss: 3.7415247CurrentTrain: epoch  9, batch     2 | loss: 2.8332326CurrentTrain: epoch  9, batch     3 | loss: 4.0287995
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: the " tetrabiblos " was largely responsible for laying down the basic precepts of renaissance astrology , webster ( 1979 ) p.276 .
Head Entity: tetrabiblos
Tail Entity: astrology
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: In her groundbreaking book, "The Second Sex," Simone de Beauvoir explores the role of women in society and the concept of femininity, which has influenced feminist theory significantly.  
Head Entity: The Second Sex  
Tail Entity: feminist theory  

Relation: main subject  
Context: The documentary "Planet Earth" showcases the beauty and diversity of the natural world, focusing on various ecosystems and wildlife, which has sparked interest in environmental conservation.  
Head Entity: Planet Earth  
Tail Entity: environmental conservation  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: he represented romania at euro 2000 , his first major tournament where they beat england 3–2 , and drew 1–1 against germany .
Head Entity: euro 2000
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: The Brazilian national team showcased their skills at the 2014 FIFA World Cup, where they faced off against Germany in the semi-finals.  
Head Entity: 2014 FIFA World Cup  
Tail Entity: Germany  

Relation: participating team  
Context: During the 2021 Tokyo Olympics, the United States women's soccer team competed fiercely against Canada in the semi-finals.  
Head Entity: 2021 Tokyo Olympics  
Tail Entity: Canada  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: among the competing artists was previous eurovision song contest entrant robin juhkental who represented estonia in 2010 as part of the group malcolm lincoln .
Head Entity: malcolm lincoln
Tail Entity: robin juhkental
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The famous painting "The Last Supper" features several apostles, including Peter, who is depicted prominently on the left side of the artwork.  
Head Entity: The Last Supper  
Tail Entity: Peter  

Relation: has part  
Context: The smartphone model Galaxy S21 comes with a high-resolution camera that includes a wide-angle lens, allowing users to capture stunning landscapes.  
Head Entity: Galaxy S21  
Tail Entity: wide-angle lens  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: he was nominated for the academy award for best picture , along with steven spielberg , ian bryce , mark gordon for the film " saving private ryan " .
Head Entity: ian bryce
Tail Entity: academy award for best picture
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film "The Shape of Water" received multiple nominations, including a nod for best director for Guillermo del Toro at the Academy Awards.  
Head Entity: Guillermo del Toro  
Tail Entity: Academy Awards  

Relation: nominated for  
Context: The popular band was nominated for the Grammy Award for Best New Artist after their debut album topped the charts.  
Head Entity: the popular band  
Tail Entity: Grammy Award for Best New Artist  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: in 2007 , the county municipality created the transit authority called " skyss " that would market public transport while is would be operated by private companies based on public service obligation contracts .
Head Entity: skyss
Tail Entity: transit authority
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone known as the iPhone was first released by Apple Inc. in 2007, revolutionizing the mobile phone industry and setting new standards for design and functionality.  
Head Entity: iPhone  
Tail Entity: smartphone  

Relation: instance of  
Context: The Great Wall of China is a series of fortifications made of various materials, built to protect the Chinese states and empires from invasions and raids.  
Head Entity: Great Wall of China  
Tail Entity: fortification  
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: abus valley ( ) is an ice - free valley southeast of turnstile ridge at the north end of britannia range .
Head Entity: turnstile ridge
Tail Entity: britannia range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the sierra nevada ( ) is a mountain range in the western united states, primarily in the state of california, and it includes the famous yosemite national park, which is located in the eastern part of the range.  
Head Entity: sierra nevada  
Tail Entity: yosemite national park  

Relation: mountain range  
Context: the appalachian mountains ( ) extend from the canadian province of newfoundland and labrador down to alabama in the united states, encompassing a variety of subranges, including the great smoky mountains.  
Head Entity: appalachian mountains  
Tail Entity: great smoky mountains  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: anders matthesen has also released several cds with his radio material , in addition to the animated movie " terkel in trouble " , based on one of these .
Head Entity: terkel in trouble
Tail Entity: anders matthesen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: the acclaimed film "inception" features a complex narrative that keeps viewers on the edge of their seats, thanks to the brilliant writing of its screenwriter, christopher nolan.  
Head Entity: inception  
Tail Entity: christopher nolan  

Relation: screenwriter  
Context: the heartwarming story of "the pursuit of happyness" was brought to life by the talented screenwriter, steve conrad, who crafted a compelling script based on true events.  
Head Entity: the pursuit of happyness  
Tail Entity: steve conrad  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the publisher found success in 1993 with tarō gomi 's " everyone poops " , a japanese language import that sold over one million copies and is now their best - known title .
Head Entity: everyone poops
Tail Entity: japanese language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The animated series "Avatar: The Last Airbender" features a rich narrative and is primarily produced in English, appealing to a global audience.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  

Relation: language of work or name  
Context: The famous novel "Cien años de soledad" by Gabriel García Márquez is a cornerstone of Latin American literature and is originally written in Spanish.  
Head Entity: Cien años de soledad  
Tail Entity: Spanish  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: the aircraft was a boeing 707 - 321c that first entered service with pan american world airways ( pan am ) in 1963 .
Head Entity: boeing 707 - 321c
Tail Entity: pan american world airways
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: the new electric bus fleet is managed by the city transportation authority, which ensures efficient public transit services.  
Head Entity: electric bus fleet  
Tail Entity: city transportation authority  

Relation: operator  
Context: the state-of-the-art manufacturing plant is operated by tech innovations inc., specializing in advanced robotics.  
Head Entity: manufacturing plant  
Tail Entity: tech innovations inc.  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: the most important lama of this series was the third changkya , rolpai dorje , who was preceptor to the qianlong emperor of china , and chief representative of tibetan buddhism at the qing court .
Head Entity: rolpai dorje
Tail Entity: tibetan buddhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the cathedral of notre-dame is a famous example of french gothic architecture and serves as a central place of worship for the roman catholic community in paris.  
Head Entity: cathedral of notre-dame  
Tail Entity: roman catholic  

Relation: religion  
Context: the dalai lama is a prominent figure in the practice of tibetan buddhism, advocating for peace and compassion around the world.  
Head Entity: dalai lama  
Tail Entity: tibetan buddhism  
Mixup data size:  560
MixupTrain:  epoch  0, batch     0 | loss: 2.1996773MixupTrain:  epoch  0, batch     1 | loss: 2.5604145MixupTrain:  epoch  0, batch     2 | loss: 1.8084396MixupTrain:  epoch  0, batch     3 | loss: 1.4779812MixupTrain:  epoch  0, batch     4 | loss: 2.1441343MixupTrain:  epoch  0, batch     5 | loss: 1.9779472MixupTrain:  epoch  0, batch     6 | loss: 1.9682983MixupTrain:  epoch  0, batch     7 | loss: 2.1567926MixupTrain:  epoch  0, batch     8 | loss: 1.7663636MixupTrain:  epoch  0, batch     9 | loss: 2.0257809MixupTrain:  epoch  0, batch    10 | loss: 1.4260277MixupTrain:  epoch  0, batch    11 | loss: 2.2095659MixupTrain:  epoch  0, batch    12 | loss: 2.0229998MixupTrain:  epoch  0, batch    13 | loss: 2.4684470MixupTrain:  epoch  0, batch    14 | loss: 1.4322612MixupTrain:  epoch  0, batch    15 | loss: 1.9377490MixupTrain:  epoch  0, batch    16 | loss: 2.0595494MixupTrain:  epoch  0, batch    17 | loss: 1.7094530MixupTrain:  epoch  0, batch    18 | loss: 1.9495131MixupTrain:  epoch  0, batch    19 | loss: 1.8073815MixupTrain:  epoch  0, batch    20 | loss: 2.0911399MixupTrain:  epoch  0, batch    21 | loss: 1.7523088MixupTrain:  epoch  0, batch    22 | loss: 2.1637595MixupTrain:  epoch  0, batch    23 | loss: 1.8087475MixupTrain:  epoch  0, batch    24 | loss: 1.9398083MixupTrain:  epoch  0, batch    25 | loss: 1.6969082MixupTrain:  epoch  0, batch    26 | loss: 1.6615705MixupTrain:  epoch  0, batch    27 | loss: 2.0719362MixupTrain:  epoch  0, batch    28 | loss: 1.7641673MixupTrain:  epoch  0, batch    29 | loss: 2.3377831MixupTrain:  epoch  0, batch    30 | loss: 1.6047901MixupTrain:  epoch  0, batch    31 | loss: 1.7067986MixupTrain:  epoch  0, batch    32 | loss: 1.8615129MixupTrain:  epoch  0, batch    33 | loss: 1.5410556MixupTrain:  epoch  0, batch    34 | loss: 1.8150696
MemoryTrain:  epoch  0, batch     0 | loss: 2.0653572MemoryTrain:  epoch  0, batch     1 | loss: 1.6921266MemoryTrain:  epoch  0, batch     2 | loss: 2.0369945MemoryTrain:  epoch  0, batch     3 | loss: 1.9678028MemoryTrain:  epoch  0, batch     4 | loss: 1.5608941MemoryTrain:  epoch  0, batch     5 | loss: 2.1702046MemoryTrain:  epoch  0, batch     6 | loss: 2.8455896MemoryTrain:  epoch  0, batch     7 | loss: 1.8401558MemoryTrain:  epoch  0, batch     8 | loss: 2.3824422MemoryTrain:  epoch  0, batch     9 | loss: 1.6354337MemoryTrain:  epoch  0, batch    10 | loss: 1.8847942MemoryTrain:  epoch  0, batch    11 | loss: 2.3212497MemoryTrain:  epoch  0, batch    12 | loss: 2.7279530MemoryTrain:  epoch  0, batch    13 | loss: 2.3232207MemoryTrain:  epoch  0, batch    14 | loss: 2.6611950MemoryTrain:  epoch  1, batch     0 | loss: 1.5606649MemoryTrain:  epoch  1, batch     1 | loss: 1.3774558MemoryTrain:  epoch  1, batch     2 | loss: 2.3077607MemoryTrain:  epoch  1, batch     3 | loss: 1.7495750MemoryTrain:  epoch  1, batch     4 | loss: 2.1948009MemoryTrain:  epoch  1, batch     5 | loss: 2.2161610MemoryTrain:  epoch  1, batch     6 | loss: 1.9448780MemoryTrain:  epoch  1, batch     7 | loss: 1.9331317MemoryTrain:  epoch  1, batch     8 | loss: 1.9247576MemoryTrain:  epoch  1, batch     9 | loss: 1.9427347MemoryTrain:  epoch  1, batch    10 | loss: 1.6707716MemoryTrain:  epoch  1, batch    11 | loss: 1.9530630MemoryTrain:  epoch  1, batch    12 | loss: 2.5702367MemoryTrain:  epoch  1, batch    13 | loss: 1.5393920MemoryTrain:  epoch  1, batch    14 | loss: 2.0871878MemoryTrain:  epoch  2, batch     0 | loss: 1.5627017MemoryTrain:  epoch  2, batch     1 | loss: 1.5888380MemoryTrain:  epoch  2, batch     2 | loss: 2.9073668MemoryTrain:  epoch  2, batch     3 | loss: 1.5216156MemoryTrain:  epoch  2, batch     4 | loss: 1.6998303MemoryTrain:  epoch  2, batch     5 | loss: 1.5081949MemoryTrain:  epoch  2, batch     6 | loss: 1.9693274MemoryTrain:  epoch  2, batch     7 | loss: 2.1701894MemoryTrain:  epoch  2, batch     8 | loss: 1.6488702MemoryTrain:  epoch  2, batch     9 | loss: 1.3966110MemoryTrain:  epoch  2, batch    10 | loss: 1.6142501MemoryTrain:  epoch  2, batch    11 | loss: 1.5999463MemoryTrain:  epoch  2, batch    12 | loss: 1.8717635MemoryTrain:  epoch  2, batch    13 | loss: 1.7299347MemoryTrain:  epoch  2, batch    14 | loss: 1.3219393MemoryTrain:  epoch  3, batch     0 | loss: 1.4886982MemoryTrain:  epoch  3, batch     1 | loss: 1.8747551MemoryTrain:  epoch  3, batch     2 | loss: 2.1385143MemoryTrain:  epoch  3, batch     3 | loss: 1.3222468MemoryTrain:  epoch  3, batch     4 | loss: 1.7452161MemoryTrain:  epoch  3, batch     5 | loss: 1.3136668MemoryTrain:  epoch  3, batch     6 | loss: 1.3046551MemoryTrain:  epoch  3, batch     7 | loss: 1.3883870MemoryTrain:  epoch  3, batch     8 | loss: 1.2838768MemoryTrain:  epoch  3, batch     9 | loss: 1.4216537MemoryTrain:  epoch  3, batch    10 | loss: 1.9589696MemoryTrain:  epoch  3, batch    11 | loss: 1.5642965MemoryTrain:  epoch  3, batch    12 | loss: 1.8717850MemoryTrain:  epoch  3, batch    13 | loss: 1.6951598MemoryTrain:  epoch  3, batch    14 | loss: 1.2293334MemoryTrain:  epoch  4, batch     0 | loss: 1.5770880MemoryTrain:  epoch  4, batch     1 | loss: 1.2511084MemoryTrain:  epoch  4, batch     2 | loss: 1.3474534MemoryTrain:  epoch  4, batch     3 | loss: 1.4071866MemoryTrain:  epoch  4, batch     4 | loss: 1.2560114MemoryTrain:  epoch  4, batch     5 | loss: 2.4736912MemoryTrain:  epoch  4, batch     6 | loss: 1.3483740MemoryTrain:  epoch  4, batch     7 | loss: 1.3693035MemoryTrain:  epoch  4, batch     8 | loss: 1.8470144MemoryTrain:  epoch  4, batch     9 | loss: 1.8177738MemoryTrain:  epoch  4, batch    10 | loss: 1.2771226MemoryTrain:  epoch  4, batch    11 | loss: 1.4863651MemoryTrain:  epoch  4, batch    12 | loss: 1.2972062MemoryTrain:  epoch  4, batch    13 | loss: 1.3556106MemoryTrain:  epoch  4, batch    14 | loss: 1.3576287MemoryTrain:  epoch  5, batch     0 | loss: 1.2493682MemoryTrain:  epoch  5, batch     1 | loss: 1.9033780MemoryTrain:  epoch  5, batch     2 | loss: 1.8827603MemoryTrain:  epoch  5, batch     3 | loss: 1.2784783MemoryTrain:  epoch  5, batch     4 | loss: 1.3208491MemoryTrain:  epoch  5, batch     5 | loss: 1.6197505MemoryTrain:  epoch  5, batch     6 | loss: 1.3188336MemoryTrain:  epoch  5, batch     7 | loss: 1.4123199MemoryTrain:  epoch  5, batch     8 | loss: 1.4440981MemoryTrain:  epoch  5, batch     9 | loss: 1.3316933MemoryTrain:  epoch  5, batch    10 | loss: 1.3716296MemoryTrain:  epoch  5, batch    11 | loss: 1.2940629MemoryTrain:  epoch  5, batch    12 | loss: 1.6169956MemoryTrain:  epoch  5, batch    13 | loss: 1.4557549MemoryTrain:  epoch  5, batch    14 | loss: 1.2168522MemoryTrain:  epoch  6, batch     0 | loss: 1.2400360MemoryTrain:  epoch  6, batch     1 | loss: 1.2979033MemoryTrain:  epoch  6, batch     2 | loss: 1.3090649MemoryTrain:  epoch  6, batch     3 | loss: 1.2008011MemoryTrain:  epoch  6, batch     4 | loss: 1.3035433MemoryTrain:  epoch  6, batch     5 | loss: 1.3616509MemoryTrain:  epoch  6, batch     6 | loss: 1.7271318MemoryTrain:  epoch  6, batch     7 | loss: 1.4903187MemoryTrain:  epoch  6, batch     8 | loss: 1.2684237MemoryTrain:  epoch  6, batch     9 | loss: 1.2701542MemoryTrain:  epoch  6, batch    10 | loss: 1.5685961MemoryTrain:  epoch  6, batch    11 | loss: 1.4495997MemoryTrain:  epoch  6, batch    12 | loss: 1.3005154MemoryTrain:  epoch  6, batch    13 | loss: 1.4894836MemoryTrain:  epoch  6, batch    14 | loss: 1.2419714MemoryTrain:  epoch  7, batch     0 | loss: 1.2150799MemoryTrain:  epoch  7, batch     1 | loss: 1.2572485MemoryTrain:  epoch  7, batch     2 | loss: 1.7689700MemoryTrain:  epoch  7, batch     3 | loss: 1.2705305MemoryTrain:  epoch  7, batch     4 | loss: 1.2891108MemoryTrain:  epoch  7, batch     5 | loss: 1.2501236MemoryTrain:  epoch  7, batch     6 | loss: 1.4546576MemoryTrain:  epoch  7, batch     7 | loss: 1.2594441MemoryTrain:  epoch  7, batch     8 | loss: 1.2890098MemoryTrain:  epoch  7, batch     9 | loss: 1.2710173MemoryTrain:  epoch  7, batch    10 | loss: 1.2809901MemoryTrain:  epoch  7, batch    11 | loss: 1.3187621MemoryTrain:  epoch  7, batch    12 | loss: 1.2482204MemoryTrain:  epoch  7, batch    13 | loss: 1.2473356MemoryTrain:  epoch  7, batch    14 | loss: 1.3404751MemoryTrain:  epoch  8, batch     0 | loss: 1.3805709MemoryTrain:  epoch  8, batch     1 | loss: 1.4302700MemoryTrain:  epoch  8, batch     2 | loss: 1.2387426MemoryTrain:  epoch  8, batch     3 | loss: 1.3317012MemoryTrain:  epoch  8, batch     4 | loss: 1.2787516MemoryTrain:  epoch  8, batch     5 | loss: 1.4639478MemoryTrain:  epoch  8, batch     6 | loss: 1.4554093MemoryTrain:  epoch  8, batch     7 | loss: 1.2422847MemoryTrain:  epoch  8, batch     8 | loss: 1.2339771MemoryTrain:  epoch  8, batch     9 | loss: 1.2238600MemoryTrain:  epoch  8, batch    10 | loss: 1.2338963MemoryTrain:  epoch  8, batch    11 | loss: 1.2379934MemoryTrain:  epoch  8, batch    12 | loss: 1.5988414MemoryTrain:  epoch  8, batch    13 | loss: 1.2363582MemoryTrain:  epoch  8, batch    14 | loss: 1.3167988MemoryTrain:  epoch  9, batch     0 | loss: 1.3997602MemoryTrain:  epoch  9, batch     1 | loss: 1.4683974MemoryTrain:  epoch  9, batch     2 | loss: 1.2162035MemoryTrain:  epoch  9, batch     3 | loss: 1.2746794MemoryTrain:  epoch  9, batch     4 | loss: 1.3006319MemoryTrain:  epoch  9, batch     5 | loss: 1.2551932MemoryTrain:  epoch  9, batch     6 | loss: 1.2573782MemoryTrain:  epoch  9, batch     7 | loss: 1.5565031MemoryTrain:  epoch  9, batch     8 | loss: 1.2422423MemoryTrain:  epoch  9, batch     9 | loss: 1.2394447MemoryTrain:  epoch  9, batch    10 | loss: 1.2774614MemoryTrain:  epoch  9, batch    11 | loss: 1.2127988MemoryTrain:  epoch  9, batch    12 | loss: 1.3287678MemoryTrain:  epoch  9, batch    13 | loss: 1.2426677MemoryTrain:  epoch  9, batch    14 | loss: 1.1925066
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 45.83%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 53.12%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 53.75%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 58.93%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 61.72%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 63.19%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 63.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 64.77%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 64.90%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 62.50%   [EVAL] batch:   14 | acc: 25.00%,  total acc: 60.00%   [EVAL] batch:   15 | acc: 25.00%,  total acc: 57.81%   [EVAL] batch:   16 | acc: 31.25%,  total acc: 56.25%   [EVAL] batch:   17 | acc: 37.50%,  total acc: 55.21%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 54.28%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.33%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 59.94%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 61.68%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 63.28%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 64.50%   [EVAL] batch:   25 | acc: 43.75%,  total acc: 63.70%   [EVAL] batch:   26 | acc: 18.75%,  total acc: 62.04%   [EVAL] batch:   27 | acc: 31.25%,  total acc: 60.94%   [EVAL] batch:   28 | acc: 37.50%,  total acc: 60.13%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 58.54%   [EVAL] batch:   30 | acc: 31.25%,  total acc: 57.66%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 58.20%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 59.28%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 60.48%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 61.43%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 62.15%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 63.01%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 63.82%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 64.26%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 64.84%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 64.79%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 64.58%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 64.97%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 65.48%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 64.72%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 64.54%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 64.76%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 64.45%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 64.29%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 63.88%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 63.85%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 63.58%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 63.92%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 64.12%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 64.55%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 64.84%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 65.13%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 65.52%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 66.00%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 66.46%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 66.91%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 67.34%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 67.06%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 71.25%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 70.45%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 68.23%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 68.27%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 70.09%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 71.67%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 73.05%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 74.26%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 75.35%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 76.32%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 75.62%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 76.19%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 75.57%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 75.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.20%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 76.85%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 77.46%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.02%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.44%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 79.73%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 78.68%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 78.39%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 78.47%   [EVAL] batch:   36 | acc: 50.00%,  total acc: 77.70%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 77.96%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 78.53%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.06%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 79.42%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 79.61%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 79.94%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 80.11%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 79.48%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 79.12%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 78.78%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 78.32%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 77.75%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 77.57%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 77.64%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 77.71%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 77.55%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 77.05%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 77.01%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 76.75%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 76.19%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 76.06%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 76.15%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 76.13%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 76.01%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 76.09%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 76.37%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 76.63%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 76.99%   [EVAL] batch:   66 | acc: 87.50%,  total acc: 77.15%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 77.48%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 77.54%   [EVAL] batch:   69 | acc: 31.25%,  total acc: 76.88%   [EVAL] batch:   70 | acc: 25.00%,  total acc: 76.14%   [EVAL] batch:   71 | acc: 18.75%,  total acc: 75.35%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 74.66%   [EVAL] batch:   73 | acc: 18.75%,  total acc: 73.90%   [EVAL] batch:   74 | acc: 18.75%,  total acc: 73.17%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 73.27%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 73.38%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 73.72%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 73.66%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 73.75%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 73.77%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 73.78%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 73.57%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 73.07%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 72.79%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 72.46%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 72.20%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 72.23%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 72.54%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 72.85%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 73.15%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 73.72%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 73.94%   [EVAL] batch:   94 | acc: 62.50%,  total acc: 73.82%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 73.89%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 73.78%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 73.66%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 73.74%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 73.44%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 73.70%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 73.96%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 74.21%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 74.46%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 74.70%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 74.88%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 75.12%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 75.35%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 75.57%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 75.80%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 76.01%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 76.17%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 76.16%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 76.10%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 76.20%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 76.29%   [EVAL] batch:  116 | acc: 68.75%,  total acc: 76.23%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 76.32%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 76.31%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 76.46%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 76.50%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 76.64%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 76.83%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 76.92%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 77.05%   [EVAL] batch:  125 | acc: 0.00%,  total acc: 76.44%   [EVAL] batch:  126 | acc: 6.25%,  total acc: 75.89%   [EVAL] batch:  127 | acc: 0.00%,  total acc: 75.29%   [EVAL] batch:  128 | acc: 6.25%,  total acc: 74.76%   [EVAL] batch:  129 | acc: 0.00%,  total acc: 74.18%   [EVAL] batch:  130 | acc: 0.00%,  total acc: 73.62%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 73.53%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 73.59%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 73.79%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 73.89%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 74.03%   [EVAL] batch:  136 | acc: 75.00%,  total acc: 74.04%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 74.09%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 73.97%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 73.84%   [EVAL] batch:  140 | acc: 75.00%,  total acc: 73.85%   [EVAL] batch:  141 | acc: 56.25%,  total acc: 73.72%   [EVAL] batch:  142 | acc: 62.50%,  total acc: 73.65%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 73.31%   [EVAL] batch:  144 | acc: 18.75%,  total acc: 72.93%   [EVAL] batch:  145 | acc: 6.25%,  total acc: 72.47%   [EVAL] batch:  146 | acc: 31.25%,  total acc: 72.19%   [EVAL] batch:  147 | acc: 25.00%,  total acc: 71.88%   [EVAL] batch:  148 | acc: 12.50%,  total acc: 71.48%   [EVAL] batch:  149 | acc: 18.75%,  total acc: 71.12%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 70.70%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 70.35%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 69.89%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 69.48%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 69.07%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 68.63%   [EVAL] batch:  156 | acc: 50.00%,  total acc: 68.51%   [EVAL] batch:  157 | acc: 68.75%,  total acc: 68.51%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 68.59%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:  160 | acc: 81.25%,  total acc: 68.83%   [EVAL] batch:  161 | acc: 75.00%,  total acc: 68.87%   [EVAL] batch:  162 | acc: 81.25%,  total acc: 68.94%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 69.05%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 69.24%   [EVAL] batch:  165 | acc: 87.50%,  total acc: 69.35%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 69.50%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 69.68%   [EVAL] batch:  168 | acc: 100.00%,  total acc: 69.86%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 69.93%   [EVAL] batch:  170 | acc: 68.75%,  total acc: 69.92%   [EVAL] batch:  171 | acc: 81.25%,  total acc: 69.99%   [EVAL] batch:  172 | acc: 75.00%,  total acc: 70.01%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 69.97%   [EVAL] batch:  174 | acc: 62.50%,  total acc: 69.93%   [EVAL] batch:  175 | acc: 18.75%,  total acc: 69.64%   [EVAL] batch:  176 | acc: 37.50%,  total acc: 69.46%   [EVAL] batch:  177 | acc: 31.25%,  total acc: 69.24%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 69.13%   [EVAL] batch:  179 | acc: 37.50%,  total acc: 68.96%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 68.89%   [EVAL] batch:  181 | acc: 37.50%,  total acc: 68.72%   [EVAL] batch:  182 | acc: 43.75%,  total acc: 68.58%   [EVAL] batch:  183 | acc: 37.50%,  total acc: 68.41%   [EVAL] batch:  184 | acc: 56.25%,  total acc: 68.34%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 68.25%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 68.25%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 68.09%   [EVAL] batch:  188 | acc: 62.50%,  total acc: 68.06%   [EVAL] batch:  189 | acc: 43.75%,  total acc: 67.93%   [EVAL] batch:  190 | acc: 50.00%,  total acc: 67.83%   [EVAL] batch:  191 | acc: 56.25%,  total acc: 67.77%   [EVAL] batch:  192 | acc: 50.00%,  total acc: 67.68%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 67.53%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 67.60%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 67.57%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 67.58%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 67.61%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 67.43%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 67.59%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 67.60%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 67.64%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 67.58%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 67.56%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 67.53%   [EVAL] batch:  205 | acc: 87.50%,  total acc: 67.63%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 67.75%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 67.91%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 68.06%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 68.18%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 68.31%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 68.46%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 68.40%   [EVAL] batch:  213 | acc: 31.25%,  total acc: 68.22%   [EVAL] batch:  214 | acc: 25.00%,  total acc: 68.02%   [EVAL] batch:  215 | acc: 50.00%,  total acc: 67.94%   [EVAL] batch:  216 | acc: 31.25%,  total acc: 67.77%   [EVAL] batch:  217 | acc: 43.75%,  total acc: 67.66%   [EVAL] batch:  218 | acc: 56.25%,  total acc: 67.61%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 67.59%   [EVAL] batch:  220 | acc: 31.25%,  total acc: 67.42%   [EVAL] batch:  221 | acc: 12.50%,  total acc: 67.17%   [EVAL] batch:  222 | acc: 18.75%,  total acc: 66.96%   [EVAL] batch:  223 | acc: 25.00%,  total acc: 66.77%   [EVAL] batch:  224 | acc: 25.00%,  total acc: 66.58%   [EVAL] batch:  225 | acc: 56.25%,  total acc: 66.54%   [EVAL] batch:  226 | acc: 56.25%,  total acc: 66.49%   [EVAL] batch:  227 | acc: 43.75%,  total acc: 66.39%   [EVAL] batch:  228 | acc: 68.75%,  total acc: 66.40%   [EVAL] batch:  229 | acc: 56.25%,  total acc: 66.36%   [EVAL] batch:  230 | acc: 43.75%,  total acc: 66.26%   [EVAL] batch:  231 | acc: 75.00%,  total acc: 66.30%   [EVAL] batch:  232 | acc: 75.00%,  total acc: 66.34%   [EVAL] batch:  233 | acc: 81.25%,  total acc: 66.40%   [EVAL] batch:  234 | acc: 62.50%,  total acc: 66.38%   [EVAL] batch:  235 | acc: 75.00%,  total acc: 66.42%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 66.51%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 66.62%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 66.76%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 66.88%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 66.99%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 67.12%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 67.26%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 67.39%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 67.53%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 67.58%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 67.69%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 67.74%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 67.87%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 68.00%   [EVAL] batch:  250 | acc: 56.25%,  total acc: 67.95%   [EVAL] batch:  251 | acc: 68.75%,  total acc: 67.96%   [EVAL] batch:  252 | acc: 68.75%,  total acc: 67.96%   [EVAL] batch:  253 | acc: 56.25%,  total acc: 67.91%   [EVAL] batch:  254 | acc: 50.00%,  total acc: 67.84%   [EVAL] batch:  255 | acc: 75.00%,  total acc: 67.87%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 67.83%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 67.88%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 67.88%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 67.91%   [EVAL] batch:  260 | acc: 68.75%,  total acc: 67.91%   [EVAL] batch:  261 | acc: 62.50%,  total acc: 67.89%   [EVAL] batch:  262 | acc: 75.00%,  total acc: 67.92%   [EVAL] batch:  263 | acc: 37.50%,  total acc: 67.80%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 67.81%   [EVAL] batch:  265 | acc: 62.50%,  total acc: 67.79%   [EVAL] batch:  266 | acc: 43.75%,  total acc: 67.70%   [EVAL] batch:  267 | acc: 56.25%,  total acc: 67.65%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 67.66%   [EVAL] batch:  269 | acc: 56.25%,  total acc: 67.62%   [EVAL] batch:  270 | acc: 18.75%,  total acc: 67.44%   [EVAL] batch:  271 | acc: 43.75%,  total acc: 67.35%   [EVAL] batch:  272 | acc: 37.50%,  total acc: 67.24%   [EVAL] batch:  273 | acc: 50.00%,  total acc: 67.18%   [EVAL] batch:  274 | acc: 56.25%,  total acc: 67.14%   [EVAL] batch:  275 | acc: 87.50%,  total acc: 67.21%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 67.33%   [EVAL] batch:  277 | acc: 93.75%,  total acc: 67.42%   [EVAL] batch:  278 | acc: 81.25%,  total acc: 67.47%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 67.59%   [EVAL] batch:  280 | acc: 93.75%,  total acc: 67.68%   [EVAL] batch:  281 | acc: 100.00%,  total acc: 67.80%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 67.91%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 68.02%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 68.14%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 68.25%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 68.36%   [EVAL] batch:  287 | acc: 100.00%,  total acc: 68.47%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 68.58%   [EVAL] batch:  289 | acc: 93.75%,  total acc: 68.66%   [EVAL] batch:  290 | acc: 75.00%,  total acc: 68.69%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 68.77%   [EVAL] batch:  292 | acc: 93.75%,  total acc: 68.86%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 68.90%   [EVAL] batch:  294 | acc: 31.25%,  total acc: 68.77%   [EVAL] batch:  295 | acc: 18.75%,  total acc: 68.60%   [EVAL] batch:  296 | acc: 31.25%,  total acc: 68.48%   [EVAL] batch:  297 | acc: 37.50%,  total acc: 68.37%   [EVAL] batch:  298 | acc: 37.50%,  total acc: 68.27%   [EVAL] batch:  299 | acc: 31.25%,  total acc: 68.15%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 68.23%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 68.32%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 68.40%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 68.44%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 68.52%   [EVAL] batch:  305 | acc: 81.25%,  total acc: 68.57%   [EVAL] batch:  306 | acc: 43.75%,  total acc: 68.49%   [EVAL] batch:  307 | acc: 31.25%,  total acc: 68.36%   [EVAL] batch:  308 | acc: 12.50%,  total acc: 68.18%   [EVAL] batch:  309 | acc: 25.00%,  total acc: 68.04%   [EVAL] batch:  310 | acc: 18.75%,  total acc: 67.89%   [EVAL] batch:  311 | acc: 31.25%,  total acc: 67.77%   [EVAL] batch:  312 | acc: 18.75%,  total acc: 67.61%   [EVAL] batch:  313 | acc: 37.50%,  total acc: 67.52%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 67.36%   [EVAL] batch:  315 | acc: 12.50%,  total acc: 67.19%   [EVAL] batch:  316 | acc: 31.25%,  total acc: 67.07%   [EVAL] batch:  317 | acc: 25.00%,  total acc: 66.94%   [EVAL] batch:  318 | acc: 31.25%,  total acc: 66.83%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 66.88%   [EVAL] batch:  320 | acc: 75.00%,  total acc: 66.90%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 66.94%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 67.01%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 67.07%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 67.10%   [EVAL] batch:  325 | acc: 56.25%,  total acc: 67.06%   [EVAL] batch:  326 | acc: 18.75%,  total acc: 66.92%   [EVAL] batch:  327 | acc: 31.25%,  total acc: 66.81%   [EVAL] batch:  328 | acc: 37.50%,  total acc: 66.72%   [EVAL] batch:  329 | acc: 50.00%,  total acc: 66.67%   [EVAL] batch:  330 | acc: 50.00%,  total acc: 66.62%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 66.68%   [EVAL] batch:  332 | acc: 93.75%,  total acc: 66.76%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 66.86%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 66.94%   [EVAL] batch:  335 | acc: 93.75%,  total acc: 67.02%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 67.12%   [EVAL] batch:  337 | acc: 68.75%,  total acc: 67.12%   [EVAL] batch:  338 | acc: 68.75%,  total acc: 67.13%   [EVAL] batch:  339 | acc: 81.25%,  total acc: 67.17%   [EVAL] batch:  340 | acc: 75.00%,  total acc: 67.19%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 67.21%   [EVAL] batch:  342 | acc: 93.75%,  total acc: 67.29%   [EVAL] batch:  343 | acc: 81.25%,  total acc: 67.33%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 67.43%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 67.52%   [EVAL] batch:  346 | acc: 100.00%,  total acc: 67.62%   [EVAL] batch:  347 | acc: 87.50%,  total acc: 67.67%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 67.75%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 67.84%   [EVAL] batch:  350 | acc: 18.75%,  total acc: 67.70%   [EVAL] batch:  351 | acc: 6.25%,  total acc: 67.52%   [EVAL] batch:  352 | acc: 25.00%,  total acc: 67.40%   [EVAL] batch:  353 | acc: 18.75%,  total acc: 67.27%   [EVAL] batch:  354 | acc: 6.25%,  total acc: 67.10%   [EVAL] batch:  355 | acc: 25.00%,  total acc: 66.98%   [EVAL] batch:  356 | acc: 93.75%,  total acc: 67.05%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 67.13%   [EVAL] batch:  358 | acc: 87.50%,  total acc: 67.18%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 67.27%   [EVAL] batch:  360 | acc: 93.75%,  total acc: 67.35%   [EVAL] batch:  361 | acc: 81.25%,  total acc: 67.39%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 67.39%   [EVAL] batch:  363 | acc: 62.50%,  total acc: 67.38%   [EVAL] batch:  364 | acc: 56.25%,  total acc: 67.35%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 67.30%   [EVAL] batch:  366 | acc: 68.75%,  total acc: 67.30%   [EVAL] batch:  367 | acc: 43.75%,  total acc: 67.24%   [EVAL] batch:  368 | acc: 31.25%,  total acc: 67.14%   [EVAL] batch:  369 | acc: 75.00%,  total acc: 67.16%   [EVAL] batch:  370 | acc: 75.00%,  total acc: 67.18%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 67.12%   [EVAL] batch:  372 | acc: 75.00%,  total acc: 67.14%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 67.18%   [EVAL] batch:  374 | acc: 50.00%,  total acc: 67.13%   [EVAL] batch:  375 | acc: 56.25%,  total acc: 67.10%   [EVAL] batch:  376 | acc: 56.25%,  total acc: 67.08%   [EVAL] batch:  377 | acc: 43.75%,  total acc: 67.01%   [EVAL] batch:  378 | acc: 43.75%,  total acc: 66.95%   [EVAL] batch:  379 | acc: 68.75%,  total acc: 66.96%   [EVAL] batch:  380 | acc: 62.50%,  total acc: 66.95%   [EVAL] batch:  381 | acc: 93.75%,  total acc: 67.02%   [EVAL] batch:  382 | acc: 100.00%,  total acc: 67.10%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 67.17%   [EVAL] batch:  384 | acc: 81.25%,  total acc: 67.21%   [EVAL] batch:  385 | acc: 93.75%,  total acc: 67.28%   [EVAL] batch:  386 | acc: 100.00%,  total acc: 67.36%   [EVAL] batch:  387 | acc: 93.75%,  total acc: 67.43%   [EVAL] batch:  388 | acc: 50.00%,  total acc: 67.38%   [EVAL] batch:  389 | acc: 62.50%,  total acc: 67.37%   [EVAL] batch:  390 | acc: 93.75%,  total acc: 67.44%   [EVAL] batch:  391 | acc: 75.00%,  total acc: 67.46%   [EVAL] batch:  392 | acc: 62.50%,  total acc: 67.45%   [EVAL] batch:  393 | acc: 87.50%,  total acc: 67.50%   [EVAL] batch:  394 | acc: 81.25%,  total acc: 67.53%   [EVAL] batch:  395 | acc: 87.50%,  total acc: 67.58%   [EVAL] batch:  396 | acc: 68.75%,  total acc: 67.59%   [EVAL] batch:  397 | acc: 81.25%,  total acc: 67.62%   [EVAL] batch:  398 | acc: 87.50%,  total acc: 67.67%   [EVAL] batch:  399 | acc: 81.25%,  total acc: 67.70%   [EVAL] batch:  400 | acc: 31.25%,  total acc: 67.61%   [EVAL] batch:  401 | acc: 25.00%,  total acc: 67.51%   [EVAL] batch:  402 | acc: 31.25%,  total acc: 67.42%   [EVAL] batch:  403 | acc: 12.50%,  total acc: 67.28%   [EVAL] batch:  404 | acc: 31.25%,  total acc: 67.19%   [EVAL] batch:  405 | acc: 12.50%,  total acc: 67.06%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 67.05%   [EVAL] batch:  407 | acc: 62.50%,  total acc: 67.03%   [EVAL] batch:  408 | acc: 68.75%,  total acc: 67.04%   [EVAL] batch:  409 | acc: 56.25%,  total acc: 67.01%   [EVAL] batch:  410 | acc: 75.00%,  total acc: 67.03%   [EVAL] batch:  411 | acc: 56.25%,  total acc: 67.01%   [EVAL] batch:  412 | acc: 81.25%,  total acc: 67.04%   [EVAL] batch:  413 | acc: 62.50%,  total acc: 67.03%   [EVAL] batch:  414 | acc: 68.75%,  total acc: 67.03%   [EVAL] batch:  415 | acc: 62.50%,  total acc: 67.02%   [EVAL] batch:  416 | acc: 75.00%,  total acc: 67.04%   [EVAL] batch:  417 | acc: 75.00%,  total acc: 67.06%   [EVAL] batch:  418 | acc: 81.25%,  total acc: 67.09%   [EVAL] batch:  419 | acc: 31.25%,  total acc: 67.01%   [EVAL] batch:  420 | acc: 56.25%,  total acc: 66.98%   [EVAL] batch:  421 | acc: 37.50%,  total acc: 66.91%   [EVAL] batch:  422 | acc: 62.50%,  total acc: 66.90%   [EVAL] batch:  423 | acc: 18.75%,  total acc: 66.79%   [EVAL] batch:  424 | acc: 50.00%,  total acc: 66.75%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 66.83%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 66.91%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 66.98%   [EVAL] batch:  428 | acc: 93.75%,  total acc: 67.05%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 67.12%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 67.20%   [EVAL] batch:  431 | acc: 100.00%,  total acc: 67.27%   [EVAL] batch:  432 | acc: 56.25%,  total acc: 67.25%   [EVAL] batch:  433 | acc: 81.25%,  total acc: 67.28%   [EVAL] batch:  434 | acc: 81.25%,  total acc: 67.31%   [EVAL] batch:  435 | acc: 62.50%,  total acc: 67.30%   [EVAL] batch:  436 | acc: 93.75%,  total acc: 67.36%   [EVAL] batch:  437 | acc: 87.50%,  total acc: 67.41%   [EVAL] batch:  438 | acc: 43.75%,  total acc: 67.35%   [EVAL] batch:  439 | acc: 25.00%,  total acc: 67.26%   [EVAL] batch:  440 | acc: 62.50%,  total acc: 67.25%   [EVAL] batch:  441 | acc: 62.50%,  total acc: 67.24%   [EVAL] batch:  442 | acc: 75.00%,  total acc: 67.25%   [EVAL] batch:  443 | acc: 56.25%,  total acc: 67.23%   [EVAL] batch:  444 | acc: 81.25%,  total acc: 67.26%   [EVAL] batch:  445 | acc: 75.00%,  total acc: 67.28%   [EVAL] batch:  446 | acc: 81.25%,  total acc: 67.31%   [EVAL] batch:  447 | acc: 75.00%,  total acc: 67.33%   [EVAL] batch:  448 | acc: 68.75%,  total acc: 67.33%   [EVAL] batch:  449 | acc: 68.75%,  total acc: 67.33%   [EVAL] batch:  450 | acc: 43.75%,  total acc: 67.28%   [EVAL] batch:  451 | acc: 25.00%,  total acc: 67.19%   [EVAL] batch:  452 | acc: 18.75%,  total acc: 67.08%   [EVAL] batch:  453 | acc: 37.50%,  total acc: 67.02%   [EVAL] batch:  454 | acc: 37.50%,  total acc: 66.95%   [EVAL] batch:  455 | acc: 18.75%,  total acc: 66.84%   [EVAL] batch:  456 | acc: 75.00%,  total acc: 66.86%   [EVAL] batch:  457 | acc: 100.00%,  total acc: 66.94%   [EVAL] batch:  458 | acc: 100.00%,  total acc: 67.01%   [EVAL] batch:  459 | acc: 93.75%,  total acc: 67.07%   [EVAL] batch:  460 | acc: 100.00%,  total acc: 67.14%   [EVAL] batch:  461 | acc: 93.75%,  total acc: 67.19%   [EVAL] batch:  462 | acc: 81.25%,  total acc: 67.22%   [EVAL] batch:  463 | acc: 12.50%,  total acc: 67.11%   [EVAL] batch:  464 | acc: 37.50%,  total acc: 67.04%   [EVAL] batch:  465 | acc: 25.00%,  total acc: 66.95%   [EVAL] batch:  466 | acc: 31.25%,  total acc: 66.88%   [EVAL] batch:  467 | acc: 12.50%,  total acc: 66.76%   [EVAL] batch:  468 | acc: 56.25%,  total acc: 66.74%   [EVAL] batch:  469 | acc: 93.75%,  total acc: 66.80%   [EVAL] batch:  470 | acc: 93.75%,  total acc: 66.85%   [EVAL] batch:  471 | acc: 93.75%,  total acc: 66.91%   [EVAL] batch:  472 | acc: 87.50%,  total acc: 66.95%   [EVAL] batch:  473 | acc: 93.75%,  total acc: 67.01%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 67.08%   [EVAL] batch:  475 | acc: 81.25%,  total acc: 67.11%   [EVAL] batch:  476 | acc: 87.50%,  total acc: 67.15%   [EVAL] batch:  477 | acc: 81.25%,  total acc: 67.18%   [EVAL] batch:  478 | acc: 43.75%,  total acc: 67.13%   [EVAL] batch:  479 | acc: 87.50%,  total acc: 67.17%   [EVAL] batch:  480 | acc: 81.25%,  total acc: 67.20%   [EVAL] batch:  481 | acc: 50.00%,  total acc: 67.17%   [EVAL] batch:  482 | acc: 50.00%,  total acc: 67.13%   [EVAL] batch:  483 | acc: 62.50%,  total acc: 67.12%   [EVAL] batch:  484 | acc: 62.50%,  total acc: 67.11%   [EVAL] batch:  485 | acc: 43.75%,  total acc: 67.07%   [EVAL] batch:  486 | acc: 62.50%,  total acc: 67.06%   [EVAL] batch:  487 | acc: 50.00%,  total acc: 67.02%   [EVAL] batch:  488 | acc: 50.00%,  total acc: 66.99%   [EVAL] batch:  489 | acc: 75.00%,  total acc: 67.00%   [EVAL] batch:  490 | acc: 81.25%,  total acc: 67.03%   [EVAL] batch:  491 | acc: 81.25%,  total acc: 67.06%   [EVAL] batch:  492 | acc: 68.75%,  total acc: 67.06%   [EVAL] batch:  493 | acc: 81.25%,  total acc: 67.09%   [EVAL] batch:  494 | acc: 93.75%,  total acc: 67.15%   [EVAL] batch:  495 | acc: 93.75%,  total acc: 67.20%   [EVAL] batch:  496 | acc: 87.50%,  total acc: 67.24%   [EVAL] batch:  497 | acc: 93.75%,  total acc: 67.29%   [EVAL] batch:  498 | acc: 93.75%,  total acc: 67.35%   [EVAL] batch:  499 | acc: 100.00%,  total acc: 67.41%   
cur_acc:  ['0.9494', '0.8998', '0.6488', '0.7609', '0.8165', '0.7014', '0.7093', '0.6706']
his_acc:  ['0.9494', '0.9045', '0.8052', '0.7827', '0.7592', '0.7243', '0.7033', '0.6741']
--------Round  5
seed:  600
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 2 0 1 6 3 4 5]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 12.2965298CurrentTrain: epoch  0, batch     1 | loss: 12.4834290CurrentTrain: epoch  0, batch     2 | loss: 12.0952110CurrentTrain: epoch  0, batch     3 | loss: 11.9075642CurrentTrain: epoch  0, batch     4 | loss: 11.2454643CurrentTrain: epoch  0, batch     5 | loss: 10.9656963CurrentTrain: epoch  0, batch     6 | loss: 11.2470798CurrentTrain: epoch  0, batch     7 | loss: 10.8622456CurrentTrain: epoch  0, batch     8 | loss: 11.2785797CurrentTrain: epoch  0, batch     9 | loss: 11.1014748CurrentTrain: epoch  0, batch    10 | loss: 10.8399801CurrentTrain: epoch  0, batch    11 | loss: 10.8752069CurrentTrain: epoch  0, batch    12 | loss: 10.2859974CurrentTrain: epoch  0, batch    13 | loss: 10.1417799CurrentTrain: epoch  0, batch    14 | loss: 10.5376749CurrentTrain: epoch  0, batch    15 | loss: 10.3922138CurrentTrain: epoch  0, batch    16 | loss: 10.5399857CurrentTrain: epoch  0, batch    17 | loss: 9.8750267CurrentTrain: epoch  0, batch    18 | loss: 9.7673073CurrentTrain: epoch  0, batch    19 | loss: 10.1021709CurrentTrain: epoch  0, batch    20 | loss: 9.9306393CurrentTrain: epoch  0, batch    21 | loss: 10.1705170CurrentTrain: epoch  0, batch    22 | loss: 10.0502567CurrentTrain: epoch  0, batch    23 | loss: 9.7479000CurrentTrain: epoch  0, batch    24 | loss: 10.0742073CurrentTrain: epoch  0, batch    25 | loss: 9.7454491CurrentTrain: epoch  0, batch    26 | loss: 9.5857573CurrentTrain: epoch  0, batch    27 | loss: 9.6747751CurrentTrain: epoch  0, batch    28 | loss: 9.3128328CurrentTrain: epoch  0, batch    29 | loss: 9.4819736CurrentTrain: epoch  0, batch    30 | loss: 9.0278835CurrentTrain: epoch  0, batch    31 | loss: 9.3290424CurrentTrain: epoch  0, batch    32 | loss: 9.2657022CurrentTrain: epoch  0, batch    33 | loss: 8.9505014CurrentTrain: epoch  0, batch    34 | loss: 8.6289577CurrentTrain: epoch  0, batch    35 | loss: 9.2859802CurrentTrain: epoch  0, batch    36 | loss: 9.6654186CurrentTrain: epoch  0, batch    37 | loss: 9.4578695CurrentTrain: epoch  0, batch    38 | loss: 9.2093458CurrentTrain: epoch  0, batch    39 | loss: 9.1245651CurrentTrain: epoch  0, batch    40 | loss: 9.3738861CurrentTrain: epoch  0, batch    41 | loss: 9.1400013CurrentTrain: epoch  0, batch    42 | loss: 8.8175640CurrentTrain: epoch  0, batch    43 | loss: 8.3143272CurrentTrain: epoch  0, batch    44 | loss: 8.8983383CurrentTrain: epoch  0, batch    45 | loss: 9.0892496CurrentTrain: epoch  0, batch    46 | loss: 8.8708830CurrentTrain: epoch  0, batch    47 | loss: 8.6174421CurrentTrain: epoch  0, batch    48 | loss: 8.1667805CurrentTrain: epoch  0, batch    49 | loss: 9.2043972CurrentTrain: epoch  0, batch    50 | loss: 8.4157600CurrentTrain: epoch  0, batch    51 | loss: 8.9367085CurrentTrain: epoch  0, batch    52 | loss: 9.0297050CurrentTrain: epoch  0, batch    53 | loss: 7.8833079CurrentTrain: epoch  0, batch    54 | loss: 8.4152298CurrentTrain: epoch  0, batch    55 | loss: 8.5326900CurrentTrain: epoch  0, batch    56 | loss: 7.4350300CurrentTrain: epoch  0, batch    57 | loss: 7.6249142CurrentTrain: epoch  0, batch    58 | loss: 8.0712614CurrentTrain: epoch  0, batch    59 | loss: 7.9922600CurrentTrain: epoch  0, batch    60 | loss: 8.2426891CurrentTrain: epoch  0, batch    61 | loss: 7.3898997CurrentTrain: epoch  0, batch    62 | loss: 6.5954480CurrentTrain: epoch  1, batch     0 | loss: 8.1918335CurrentTrain: epoch  1, batch     1 | loss: 7.5942926CurrentTrain: epoch  1, batch     2 | loss: 7.7808933CurrentTrain: epoch  1, batch     3 | loss: 7.6944304CurrentTrain: epoch  1, batch     4 | loss: 6.5602551CurrentTrain: epoch  1, batch     5 | loss: 8.2695503CurrentTrain: epoch  1, batch     6 | loss: 6.6241579CurrentTrain: epoch  1, batch     7 | loss: 7.4858904CurrentTrain: epoch  1, batch     8 | loss: 7.1805124CurrentTrain: epoch  1, batch     9 | loss: 8.0636930CurrentTrain: epoch  1, batch    10 | loss: 7.5348883CurrentTrain: epoch  1, batch    11 | loss: 7.2859387CurrentTrain: epoch  1, batch    12 | loss: 6.9286613CurrentTrain: epoch  1, batch    13 | loss: 6.5625243CurrentTrain: epoch  1, batch    14 | loss: 6.8841376CurrentTrain: epoch  1, batch    15 | loss: 7.6287279CurrentTrain: epoch  1, batch    16 | loss: 7.0418406CurrentTrain: epoch  1, batch    17 | loss: 8.7767105CurrentTrain: epoch  1, batch    18 | loss: 6.6869802CurrentTrain: epoch  1, batch    19 | loss: 7.3058071CurrentTrain: epoch  1, batch    20 | loss: 7.1419439CurrentTrain: epoch  1, batch    21 | loss: 7.1662254CurrentTrain: epoch  1, batch    22 | loss: 6.8835869CurrentTrain: epoch  1, batch    23 | loss: 6.8345327CurrentTrain: epoch  1, batch    24 | loss: 7.1665678CurrentTrain: epoch  1, batch    25 | loss: 6.9707541CurrentTrain: epoch  1, batch    26 | loss: 7.1677818CurrentTrain: epoch  1, batch    27 | loss: 6.8365364CurrentTrain: epoch  1, batch    28 | loss: 6.5007362CurrentTrain: epoch  1, batch    29 | loss: 6.8679910CurrentTrain: epoch  1, batch    30 | loss: 6.2135053CurrentTrain: epoch  1, batch    31 | loss: 6.7480555CurrentTrain: epoch  1, batch    32 | loss: 6.5927987CurrentTrain: epoch  1, batch    33 | loss: 7.2398214CurrentTrain: epoch  1, batch    34 | loss: 6.6929302CurrentTrain: epoch  1, batch    35 | loss: 6.6445384CurrentTrain: epoch  1, batch    36 | loss: 6.8432589CurrentTrain: epoch  1, batch    37 | loss: 6.3646421CurrentTrain: epoch  1, batch    38 | loss: 7.2312636CurrentTrain: epoch  1, batch    39 | loss: 6.0918112CurrentTrain: epoch  1, batch    40 | loss: 5.6821370CurrentTrain: epoch  1, batch    41 | loss: 5.7684059CurrentTrain: epoch  1, batch    42 | loss: 6.8469362CurrentTrain: epoch  1, batch    43 | loss: 6.6728854CurrentTrain: epoch  1, batch    44 | loss: 6.4275990CurrentTrain: epoch  1, batch    45 | loss: 6.1009855CurrentTrain: epoch  1, batch    46 | loss: 6.4143648CurrentTrain: epoch  1, batch    47 | loss: 7.7045422CurrentTrain: epoch  1, batch    48 | loss: 6.4416509CurrentTrain: epoch  1, batch    49 | loss: 6.4232750CurrentTrain: epoch  1, batch    50 | loss: 6.9601336CurrentTrain: epoch  1, batch    51 | loss: 6.6006880CurrentTrain: epoch  1, batch    52 | loss: 7.1714087CurrentTrain: epoch  1, batch    53 | loss: 6.0388980CurrentTrain: epoch  1, batch    54 | loss: 6.7917185CurrentTrain: epoch  1, batch    55 | loss: 6.1866512CurrentTrain: epoch  1, batch    56 | loss: 5.5112934CurrentTrain: epoch  1, batch    57 | loss: 6.0432205CurrentTrain: epoch  1, batch    58 | loss: 5.6935062CurrentTrain: epoch  1, batch    59 | loss: 6.4118576CurrentTrain: epoch  1, batch    60 | loss: 5.8679614CurrentTrain: epoch  1, batch    61 | loss: 6.0778904CurrentTrain: epoch  1, batch    62 | loss: 6.9101577CurrentTrain: epoch  2, batch     0 | loss: 5.8655138CurrentTrain: epoch  2, batch     1 | loss: 5.5173922CurrentTrain: epoch  2, batch     2 | loss: 6.0007949CurrentTrain: epoch  2, batch     3 | loss: 6.1038103CurrentTrain: epoch  2, batch     4 | loss: 5.6880484CurrentTrain: epoch  2, batch     5 | loss: 5.9484677CurrentTrain: epoch  2, batch     6 | loss: 5.7803411CurrentTrain: epoch  2, batch     7 | loss: 5.7290220CurrentTrain: epoch  2, batch     8 | loss: 7.0488648CurrentTrain: epoch  2, batch     9 | loss: 5.6183476CurrentTrain: epoch  2, batch    10 | loss: 6.4982901CurrentTrain: epoch  2, batch    11 | loss: 6.1038513CurrentTrain: epoch  2, batch    12 | loss: 6.8258963CurrentTrain: epoch  2, batch    13 | loss: 6.0214844CurrentTrain: epoch  2, batch    14 | loss: 5.8689785CurrentTrain: epoch  2, batch    15 | loss: 5.9785137CurrentTrain: epoch  2, batch    16 | loss: 6.2043648CurrentTrain: epoch  2, batch    17 | loss: 6.4855871CurrentTrain: epoch  2, batch    18 | loss: 5.5492911CurrentTrain: epoch  2, batch    19 | loss: 5.8962126CurrentTrain: epoch  2, batch    20 | loss: 5.1572514CurrentTrain: epoch  2, batch    21 | loss: 5.9318871CurrentTrain: epoch  2, batch    22 | loss: 5.0774298CurrentTrain: epoch  2, batch    23 | loss: 5.7792239CurrentTrain: epoch  2, batch    24 | loss: 4.9663000CurrentTrain: epoch  2, batch    25 | loss: 5.4825420CurrentTrain: epoch  2, batch    26 | loss: 6.2756529CurrentTrain: epoch  2, batch    27 | loss: 5.6343489CurrentTrain: epoch  2, batch    28 | loss: 5.9343538CurrentTrain: epoch  2, batch    29 | loss: 6.5149965CurrentTrain: epoch  2, batch    30 | loss: 5.8647842CurrentTrain: epoch  2, batch    31 | loss: 5.1613607CurrentTrain: epoch  2, batch    32 | loss: 5.2221336CurrentTrain: epoch  2, batch    33 | loss: 5.5701065CurrentTrain: epoch  2, batch    34 | loss: 6.1252470CurrentTrain: epoch  2, batch    35 | loss: 5.2902145CurrentTrain: epoch  2, batch    36 | loss: 6.2563181CurrentTrain: epoch  2, batch    37 | loss: 5.0895414CurrentTrain: epoch  2, batch    38 | loss: 5.1048641CurrentTrain: epoch  2, batch    39 | loss: 5.6540604CurrentTrain: epoch  2, batch    40 | loss: 5.9310350CurrentTrain: epoch  2, batch    41 | loss: 7.0714035CurrentTrain: epoch  2, batch    42 | loss: 5.5134730CurrentTrain: epoch  2, batch    43 | loss: 5.0235977CurrentTrain: epoch  2, batch    44 | loss: 5.7268314CurrentTrain: epoch  2, batch    45 | loss: 5.2682266CurrentTrain: epoch  2, batch    46 | loss: 6.5448127CurrentTrain: epoch  2, batch    47 | loss: 5.4996505CurrentTrain: epoch  2, batch    48 | loss: 5.6406956CurrentTrain: epoch  2, batch    49 | loss: 5.0683818CurrentTrain: epoch  2, batch    50 | loss: 4.8355093CurrentTrain: epoch  2, batch    51 | loss: 5.7515306CurrentTrain: epoch  2, batch    52 | loss: 4.9263754CurrentTrain: epoch  2, batch    53 | loss: 5.2537413CurrentTrain: epoch  2, batch    54 | loss: 5.4499435CurrentTrain: epoch  2, batch    55 | loss: 5.6672134CurrentTrain: epoch  2, batch    56 | loss: 5.6944647CurrentTrain: epoch  2, batch    57 | loss: 5.7244306CurrentTrain: epoch  2, batch    58 | loss: 5.8584447CurrentTrain: epoch  2, batch    59 | loss: 5.1615500CurrentTrain: epoch  2, batch    60 | loss: 5.0430403CurrentTrain: epoch  2, batch    61 | loss: 4.8261280CurrentTrain: epoch  2, batch    62 | loss: 5.2907200CurrentTrain: epoch  3, batch     0 | loss: 5.0207214CurrentTrain: epoch  3, batch     1 | loss: 5.1220713CurrentTrain: epoch  3, batch     2 | loss: 5.6227708CurrentTrain: epoch  3, batch     3 | loss: 5.2462697CurrentTrain: epoch  3, batch     4 | loss: 5.3641901CurrentTrain: epoch  3, batch     5 | loss: 5.0561895CurrentTrain: epoch  3, batch     6 | loss: 5.1692991CurrentTrain: epoch  3, batch     7 | loss: 5.2486439CurrentTrain: epoch  3, batch     8 | loss: 5.3231707CurrentTrain: epoch  3, batch     9 | loss: 5.4000301CurrentTrain: epoch  3, batch    10 | loss: 4.9202180CurrentTrain: epoch  3, batch    11 | loss: 4.8496904CurrentTrain: epoch  3, batch    12 | loss: 5.1324558CurrentTrain: epoch  3, batch    13 | loss: 5.1913843CurrentTrain: epoch  3, batch    14 | loss: 5.4035559CurrentTrain: epoch  3, batch    15 | loss: 4.8002186CurrentTrain: epoch  3, batch    16 | loss: 5.4678478CurrentTrain: epoch  3, batch    17 | loss: 5.2902994CurrentTrain: epoch  3, batch    18 | loss: 5.1062918CurrentTrain: epoch  3, batch    19 | loss: 5.0993695CurrentTrain: epoch  3, batch    20 | loss: 5.1468639CurrentTrain: epoch  3, batch    21 | loss: 5.1003656CurrentTrain: epoch  3, batch    22 | loss: 4.7979088CurrentTrain: epoch  3, batch    23 | loss: 5.2690659CurrentTrain: epoch  3, batch    24 | loss: 5.1364212CurrentTrain: epoch  3, batch    25 | loss: 4.7517781CurrentTrain: epoch  3, batch    26 | loss: 4.6982050CurrentTrain: epoch  3, batch    27 | loss: 4.9872980CurrentTrain: epoch  3, batch    28 | loss: 4.6642761CurrentTrain: epoch  3, batch    29 | loss: 4.8110428CurrentTrain: epoch  3, batch    30 | loss: 4.9528327CurrentTrain: epoch  3, batch    31 | loss: 5.2016411CurrentTrain: epoch  3, batch    32 | loss: 5.1349483CurrentTrain: epoch  3, batch    33 | loss: 5.2128391CurrentTrain: epoch  3, batch    34 | loss: 4.8649731CurrentTrain: epoch  3, batch    35 | loss: 5.0524507CurrentTrain: epoch  3, batch    36 | loss: 5.3219366CurrentTrain: epoch  3, batch    37 | loss: 5.3696246CurrentTrain: epoch  3, batch    38 | loss: 4.8548889CurrentTrain: epoch  3, batch    39 | loss: 4.6685982CurrentTrain: epoch  3, batch    40 | loss: 5.0399666CurrentTrain: epoch  3, batch    41 | loss: 4.7712898CurrentTrain: epoch  3, batch    42 | loss: 4.7840943CurrentTrain: epoch  3, batch    43 | loss: 4.5776353CurrentTrain: epoch  3, batch    44 | loss: 4.7574940CurrentTrain: epoch  3, batch    45 | loss: 4.8919382CurrentTrain: epoch  3, batch    46 | loss: 5.6006451CurrentTrain: epoch  3, batch    47 | loss: 4.6306372CurrentTrain: epoch  3, batch    48 | loss: 4.6769624CurrentTrain: epoch  3, batch    49 | loss: 4.7599492CurrentTrain: epoch  3, batch    50 | loss: 6.0706258CurrentTrain: epoch  3, batch    51 | loss: 5.1103849CurrentTrain: epoch  3, batch    52 | loss: 5.0774813CurrentTrain: epoch  3, batch    53 | loss: 4.6837807CurrentTrain: epoch  3, batch    54 | loss: 4.6997328CurrentTrain: epoch  3, batch    55 | loss: 5.3951879CurrentTrain: epoch  3, batch    56 | loss: 6.0687799CurrentTrain: epoch  3, batch    57 | loss: 4.9001813CurrentTrain: epoch  3, batch    58 | loss: 4.8663855CurrentTrain: epoch  3, batch    59 | loss: 5.1136875CurrentTrain: epoch  3, batch    60 | loss: 5.3254986CurrentTrain: epoch  3, batch    61 | loss: 4.9946060CurrentTrain: epoch  3, batch    62 | loss: 4.8456163CurrentTrain: epoch  4, batch     0 | loss: 5.1949625CurrentTrain: epoch  4, batch     1 | loss: 4.8635063CurrentTrain: epoch  4, batch     2 | loss: 4.9270077CurrentTrain: epoch  4, batch     3 | loss: 5.4191160CurrentTrain: epoch  4, batch     4 | loss: 4.8277841CurrentTrain: epoch  4, batch     5 | loss: 4.9125128CurrentTrain: epoch  4, batch     6 | loss: 4.8040943CurrentTrain: epoch  4, batch     7 | loss: 5.0670180CurrentTrain: epoch  4, batch     8 | loss: 4.8978395CurrentTrain: epoch  4, batch     9 | loss: 4.6044979CurrentTrain: epoch  4, batch    10 | loss: 4.9593077CurrentTrain: epoch  4, batch    11 | loss: 4.9290314CurrentTrain: epoch  4, batch    12 | loss: 4.6032448CurrentTrain: epoch  4, batch    13 | loss: 4.6551309CurrentTrain: epoch  4, batch    14 | loss: 4.6996036CurrentTrain: epoch  4, batch    15 | loss: 5.3415670CurrentTrain: epoch  4, batch    16 | loss: 5.1025472CurrentTrain: epoch  4, batch    17 | loss: 4.5074606CurrentTrain: epoch  4, batch    18 | loss: 4.5513062CurrentTrain: epoch  4, batch    19 | loss: 4.5506258CurrentTrain: epoch  4, batch    20 | loss: 4.4842157CurrentTrain: epoch  4, batch    21 | loss: 4.8689475CurrentTrain: epoch  4, batch    22 | loss: 4.7060819CurrentTrain: epoch  4, batch    23 | loss: 4.5862589CurrentTrain: epoch  4, batch    24 | loss: 5.3302937CurrentTrain: epoch  4, batch    25 | loss: 4.6603255CurrentTrain: epoch  4, batch    26 | loss: 5.0366173CurrentTrain: epoch  4, batch    27 | loss: 4.5862484CurrentTrain: epoch  4, batch    28 | loss: 4.9139137CurrentTrain: epoch  4, batch    29 | loss: 4.7551022CurrentTrain: epoch  4, batch    30 | loss: 4.4929881CurrentTrain: epoch  4, batch    31 | loss: 4.8753338CurrentTrain: epoch  4, batch    32 | loss: 4.5705762CurrentTrain: epoch  4, batch    33 | loss: 4.7350240CurrentTrain: epoch  4, batch    34 | loss: 4.6381025CurrentTrain: epoch  4, batch    35 | loss: 4.7705097CurrentTrain: epoch  4, batch    36 | loss: 4.7124100CurrentTrain: epoch  4, batch    37 | loss: 4.8785372CurrentTrain: epoch  4, batch    38 | loss: 4.5673175CurrentTrain: epoch  4, batch    39 | loss: 4.6870489CurrentTrain: epoch  4, batch    40 | loss: 4.5102553CurrentTrain: epoch  4, batch    41 | loss: 4.4681306CurrentTrain: epoch  4, batch    42 | loss: 4.4590425CurrentTrain: epoch  4, batch    43 | loss: 4.4267783CurrentTrain: epoch  4, batch    44 | loss: 4.5960512CurrentTrain: epoch  4, batch    45 | loss: 4.5227222CurrentTrain: epoch  4, batch    46 | loss: 4.3492889CurrentTrain: epoch  4, batch    47 | loss: 4.7383118CurrentTrain: epoch  4, batch    48 | loss: 4.4230928CurrentTrain: epoch  4, batch    49 | loss: 4.5932527CurrentTrain: epoch  4, batch    50 | loss: 5.1545424CurrentTrain: epoch  4, batch    51 | loss: 4.5917253CurrentTrain: epoch  4, batch    52 | loss: 4.4887457CurrentTrain: epoch  4, batch    53 | loss: 4.7798882CurrentTrain: epoch  4, batch    54 | loss: 4.2410035CurrentTrain: epoch  4, batch    55 | loss: 4.2638969CurrentTrain: epoch  4, batch    56 | loss: 4.4690433CurrentTrain: epoch  4, batch    57 | loss: 4.5955410CurrentTrain: epoch  4, batch    58 | loss: 4.3520761CurrentTrain: epoch  4, batch    59 | loss: 4.5485091CurrentTrain: epoch  4, batch    60 | loss: 4.4351687CurrentTrain: epoch  4, batch    61 | loss: 4.4647694CurrentTrain: epoch  4, batch    62 | loss: 4.3814821CurrentTrain: epoch  5, batch     0 | loss: 4.5368962CurrentTrain: epoch  5, batch     1 | loss: 4.2870817CurrentTrain: epoch  5, batch     2 | loss: 4.5650425CurrentTrain: epoch  5, batch     3 | loss: 4.4902096CurrentTrain: epoch  5, batch     4 | loss: 4.1565247CurrentTrain: epoch  5, batch     5 | loss: 4.5668497CurrentTrain: epoch  5, batch     6 | loss: 4.2837133CurrentTrain: epoch  5, batch     7 | loss: 4.5002546CurrentTrain: epoch  5, batch     8 | loss: 4.2492905CurrentTrain: epoch  5, batch     9 | loss: 4.4635401CurrentTrain: epoch  5, batch    10 | loss: 4.4922371CurrentTrain: epoch  5, batch    11 | loss: 4.3092551CurrentTrain: epoch  5, batch    12 | loss: 4.3572273CurrentTrain: epoch  5, batch    13 | loss: 4.4755077CurrentTrain: epoch  5, batch    14 | loss: 4.3948984CurrentTrain: epoch  5, batch    15 | loss: 4.4436979CurrentTrain: epoch  5, batch    16 | loss: 4.2594748CurrentTrain: epoch  5, batch    17 | loss: 4.2777295CurrentTrain: epoch  5, batch    18 | loss: 4.4458652CurrentTrain: epoch  5, batch    19 | loss: 4.3671083CurrentTrain: epoch  5, batch    20 | loss: 4.3866692CurrentTrain: epoch  5, batch    21 | loss: 4.2885675CurrentTrain: epoch  5, batch    22 | loss: 4.2687950CurrentTrain: epoch  5, batch    23 | loss: 4.3454599CurrentTrain: epoch  5, batch    24 | loss: 4.3197145CurrentTrain: epoch  5, batch    25 | loss: 4.2945051CurrentTrain: epoch  5, batch    26 | loss: 4.4883890CurrentTrain: epoch  5, batch    27 | loss: 4.2936406CurrentTrain: epoch  5, batch    28 | loss: 4.4897037CurrentTrain: epoch  5, batch    29 | loss: 4.3417339CurrentTrain: epoch  5, batch    30 | loss: 4.2863617CurrentTrain: epoch  5, batch    31 | loss: 4.2567062CurrentTrain: epoch  5, batch    32 | loss: 4.4267302CurrentTrain: epoch  5, batch    33 | loss: 4.2546649CurrentTrain: epoch  5, batch    34 | loss: 4.5275564CurrentTrain: epoch  5, batch    35 | loss: 4.3250513CurrentTrain: epoch  5, batch    36 | loss: 4.3053470CurrentTrain: epoch  5, batch    37 | loss: 4.5942030CurrentTrain: epoch  5, batch    38 | loss: 4.2467604CurrentTrain: epoch  5, batch    39 | loss: 4.2946539CurrentTrain: epoch  5, batch    40 | loss: 4.4385500CurrentTrain: epoch  5, batch    41 | loss: 4.2773733CurrentTrain: epoch  5, batch    42 | loss: 4.3762183CurrentTrain: epoch  5, batch    43 | loss: 4.2601690CurrentTrain: epoch  5, batch    44 | loss: 4.3182144CurrentTrain: epoch  5, batch    45 | loss: 4.2511320CurrentTrain: epoch  5, batch    46 | loss: 4.2983880CurrentTrain: epoch  5, batch    47 | loss: 4.3447809CurrentTrain: epoch  5, batch    48 | loss: 4.3354826CurrentTrain: epoch  5, batch    49 | loss: 4.3565569CurrentTrain: epoch  5, batch    50 | loss: 4.9628706CurrentTrain: epoch  5, batch    51 | loss: 4.1922355CurrentTrain: epoch  5, batch    52 | loss: 4.3119445CurrentTrain: epoch  5, batch    53 | loss: 4.2776566CurrentTrain: epoch  5, batch    54 | loss: 4.2493091CurrentTrain: epoch  5, batch    55 | loss: 4.3045874CurrentTrain: epoch  5, batch    56 | loss: 4.2996397CurrentTrain: epoch  5, batch    57 | loss: 4.2441149CurrentTrain: epoch  5, batch    58 | loss: 4.2620564CurrentTrain: epoch  5, batch    59 | loss: 4.2422295CurrentTrain: epoch  5, batch    60 | loss: 4.3238115CurrentTrain: epoch  5, batch    61 | loss: 4.4341450CurrentTrain: epoch  5, batch    62 | loss: 4.1959515CurrentTrain: epoch  6, batch     0 | loss: 4.2629375CurrentTrain: epoch  6, batch     1 | loss: 4.2191291CurrentTrain: epoch  6, batch     2 | loss: 4.2238760CurrentTrain: epoch  6, batch     3 | loss: 4.1572552CurrentTrain: epoch  6, batch     4 | loss: 4.2549896CurrentTrain: epoch  6, batch     5 | loss: 4.3340120CurrentTrain: epoch  6, batch     6 | loss: 4.2464619CurrentTrain: epoch  6, batch     7 | loss: 4.1445494CurrentTrain: epoch  6, batch     8 | loss: 4.3785210CurrentTrain: epoch  6, batch     9 | loss: 4.3797770CurrentTrain: epoch  6, batch    10 | loss: 4.1665068CurrentTrain: epoch  6, batch    11 | loss: 4.1768122CurrentTrain: epoch  6, batch    12 | loss: 4.2892017CurrentTrain: epoch  6, batch    13 | loss: 4.2051268CurrentTrain: epoch  6, batch    14 | loss: 4.2309179CurrentTrain: epoch  6, batch    15 | loss: 4.5422492CurrentTrain: epoch  6, batch    16 | loss: 4.2347937CurrentTrain: epoch  6, batch    17 | loss: 4.2289505CurrentTrain: epoch  6, batch    18 | loss: 4.2416816CurrentTrain: epoch  6, batch    19 | loss: 4.4048071CurrentTrain: epoch  6, batch    20 | loss: 4.2819071CurrentTrain: epoch  6, batch    21 | loss: 4.1677990CurrentTrain: epoch  6, batch    22 | loss: 4.2619481CurrentTrain: epoch  6, batch    23 | loss: 4.2433596CurrentTrain: epoch  6, batch    24 | loss: 4.2467613CurrentTrain: epoch  6, batch    25 | loss: 4.2445250CurrentTrain: epoch  6, batch    26 | loss: 4.1298337CurrentTrain: epoch  6, batch    27 | loss: 4.2597651CurrentTrain: epoch  6, batch    28 | loss: 4.2812548CurrentTrain: epoch  6, batch    29 | loss: 4.6602001CurrentTrain: epoch  6, batch    30 | loss: 4.1683488CurrentTrain: epoch  6, batch    31 | loss: 4.3074255CurrentTrain: epoch  6, batch    32 | loss: 4.2238274CurrentTrain: epoch  6, batch    33 | loss: 4.1738138CurrentTrain: epoch  6, batch    34 | loss: 4.5439463CurrentTrain: epoch  6, batch    35 | loss: 4.1841669CurrentTrain: epoch  6, batch    36 | loss: 4.1677074CurrentTrain: epoch  6, batch    37 | loss: 4.4887838CurrentTrain: epoch  6, batch    38 | loss: 4.1890039CurrentTrain: epoch  6, batch    39 | loss: 4.2317858CurrentTrain: epoch  6, batch    40 | loss: 4.1696348CurrentTrain: epoch  6, batch    41 | loss: 4.2202263CurrentTrain: epoch  6, batch    42 | loss: 4.2755618CurrentTrain: epoch  6, batch    43 | loss: 4.1830630CurrentTrain: epoch  6, batch    44 | loss: 4.3524237CurrentTrain: epoch  6, batch    45 | loss: 4.2287331CurrentTrain: epoch  6, batch    46 | loss: 4.1581678CurrentTrain: epoch  6, batch    47 | loss: 4.3349371CurrentTrain: epoch  6, batch    48 | loss: 4.2385592CurrentTrain: epoch  6, batch    49 | loss: 4.2027283CurrentTrain: epoch  6, batch    50 | loss: 4.2668438CurrentTrain: epoch  6, batch    51 | loss: 4.1628132CurrentTrain: epoch  6, batch    52 | loss: 4.2335496CurrentTrain: epoch  6, batch    53 | loss: 4.2566614CurrentTrain: epoch  6, batch    54 | loss: 4.1160927CurrentTrain: epoch  6, batch    55 | loss: 4.1766739CurrentTrain: epoch  6, batch    56 | loss: 4.1193161CurrentTrain: epoch  6, batch    57 | loss: 4.2185683CurrentTrain: epoch  6, batch    58 | loss: 4.1587954CurrentTrain: epoch  6, batch    59 | loss: 4.1492834CurrentTrain: epoch  6, batch    60 | loss: 4.1741276CurrentTrain: epoch  6, batch    61 | loss: 4.1166177CurrentTrain: epoch  6, batch    62 | loss: 4.1849775CurrentTrain: epoch  7, batch     0 | loss: 4.1887035CurrentTrain: epoch  7, batch     1 | loss: 4.1111617CurrentTrain: epoch  7, batch     2 | loss: 4.0981727CurrentTrain: epoch  7, batch     3 | loss: 4.2171001CurrentTrain: epoch  7, batch     4 | loss: 4.3339176CurrentTrain: epoch  7, batch     5 | loss: 4.1278543CurrentTrain: epoch  7, batch     6 | loss: 4.1316886CurrentTrain: epoch  7, batch     7 | loss: 4.2087717CurrentTrain: epoch  7, batch     8 | loss: 4.1355901CurrentTrain: epoch  7, batch     9 | loss: 4.1568208CurrentTrain: epoch  7, batch    10 | loss: 4.1181326CurrentTrain: epoch  7, batch    11 | loss: 4.1829123CurrentTrain: epoch  7, batch    12 | loss: 4.1902142CurrentTrain: epoch  7, batch    13 | loss: 4.1643162CurrentTrain: epoch  7, batch    14 | loss: 4.3033791CurrentTrain: epoch  7, batch    15 | loss: 4.0770760CurrentTrain: epoch  7, batch    16 | loss: 4.2003269CurrentTrain: epoch  7, batch    17 | loss: 4.1508560CurrentTrain: epoch  7, batch    18 | loss: 4.1287918CurrentTrain: epoch  7, batch    19 | loss: 4.0715466CurrentTrain: epoch  7, batch    20 | loss: 4.1295567CurrentTrain: epoch  7, batch    21 | loss: 4.1010504CurrentTrain: epoch  7, batch    22 | loss: 4.1251516CurrentTrain: epoch  7, batch    23 | loss: 4.1776810CurrentTrain: epoch  7, batch    24 | loss: 4.1783342CurrentTrain: epoch  7, batch    25 | loss: 4.1433306CurrentTrain: epoch  7, batch    26 | loss: 4.1028190CurrentTrain: epoch  7, batch    27 | loss: 4.1240549CurrentTrain: epoch  7, batch    28 | loss: 4.1083632CurrentTrain: epoch  7, batch    29 | loss: 4.0715809CurrentTrain: epoch  7, batch    30 | loss: 4.1292048CurrentTrain: epoch  7, batch    31 | loss: 4.3273244CurrentTrain: epoch  7, batch    32 | loss: 4.1468282CurrentTrain: epoch  7, batch    33 | loss: 4.1609221CurrentTrain: epoch  7, batch    34 | loss: 4.1221161CurrentTrain: epoch  7, batch    35 | loss: 4.3665409CurrentTrain: epoch  7, batch    36 | loss: 4.1793079CurrentTrain: epoch  7, batch    37 | loss: 4.0516715CurrentTrain: epoch  7, batch    38 | loss: 4.1179132CurrentTrain: epoch  7, batch    39 | loss: 4.1186042CurrentTrain: epoch  7, batch    40 | loss: 4.1213021CurrentTrain: epoch  7, batch    41 | loss: 4.1465387CurrentTrain: epoch  7, batch    42 | loss: 4.1042371CurrentTrain: epoch  7, batch    43 | loss: 4.3234091CurrentTrain: epoch  7, batch    44 | loss: 4.1631231CurrentTrain: epoch  7, batch    45 | loss: 4.1744242CurrentTrain: epoch  7, batch    46 | loss: 4.2159219CurrentTrain: epoch  7, batch    47 | loss: 4.0789905CurrentTrain: epoch  7, batch    48 | loss: 4.1472950CurrentTrain: epoch  7, batch    49 | loss: 4.1321468CurrentTrain: epoch  7, batch    50 | loss: 4.0961046CurrentTrain: epoch  7, batch    51 | loss: 4.2012162CurrentTrain: epoch  7, batch    52 | loss: 4.1395960CurrentTrain: epoch  7, batch    53 | loss: 4.1012707CurrentTrain: epoch  7, batch    54 | loss: 4.1193123CurrentTrain: epoch  7, batch    55 | loss: 4.0426607CurrentTrain: epoch  7, batch    56 | loss: 4.0572391CurrentTrain: epoch  7, batch    57 | loss: 4.1381898CurrentTrain: epoch  7, batch    58 | loss: 4.0851154CurrentTrain: epoch  7, batch    59 | loss: 4.0625758CurrentTrain: epoch  7, batch    60 | loss: 4.1498876CurrentTrain: epoch  7, batch    61 | loss: 4.0436296CurrentTrain: epoch  7, batch    62 | loss: 4.0316544CurrentTrain: epoch  8, batch     0 | loss: 4.1166897CurrentTrain: epoch  8, batch     1 | loss: 4.1031346CurrentTrain: epoch  8, batch     2 | loss: 4.0576878CurrentTrain: epoch  8, batch     3 | loss: 4.1036930CurrentTrain: epoch  8, batch     4 | loss: 4.1060743CurrentTrain: epoch  8, batch     5 | loss: 4.1114597CurrentTrain: epoch  8, batch     6 | loss: 4.0787888CurrentTrain: epoch  8, batch     7 | loss: 4.1150789CurrentTrain: epoch  8, batch     8 | loss: 4.1028023CurrentTrain: epoch  8, batch     9 | loss: 4.1275663CurrentTrain: epoch  8, batch    10 | loss: 4.0841303CurrentTrain: epoch  8, batch    11 | loss: 4.1284142CurrentTrain: epoch  8, batch    12 | loss: 4.1414909CurrentTrain: epoch  8, batch    13 | loss: 4.1139221CurrentTrain: epoch  8, batch    14 | loss: 4.0809417CurrentTrain: epoch  8, batch    15 | loss: 4.1486216CurrentTrain: epoch  8, batch    16 | loss: 4.0870552CurrentTrain: epoch  8, batch    17 | loss: 4.2465758CurrentTrain: epoch  8, batch    18 | loss: 4.0942841CurrentTrain: epoch  8, batch    19 | loss: 4.1284041CurrentTrain: epoch  8, batch    20 | loss: 4.0909176CurrentTrain: epoch  8, batch    21 | loss: 4.0929346CurrentTrain: epoch  8, batch    22 | loss: 4.1060638CurrentTrain: epoch  8, batch    23 | loss: 4.1255226CurrentTrain: epoch  8, batch    24 | loss: 4.1148853CurrentTrain: epoch  8, batch    25 | loss: 4.0353909CurrentTrain: epoch  8, batch    26 | loss: 4.0674944CurrentTrain: epoch  8, batch    27 | loss: 4.0847602CurrentTrain: epoch  8, batch    28 | loss: 4.1367488CurrentTrain: epoch  8, batch    29 | loss: 4.0432305CurrentTrain: epoch  8, batch    30 | loss: 4.0733538CurrentTrain: epoch  8, batch    31 | loss: 4.1372228CurrentTrain: epoch  8, batch    32 | loss: 4.1022797CurrentTrain: epoch  8, batch    33 | loss: 4.0804696CurrentTrain: epoch  8, batch    34 | loss: 4.1270623CurrentTrain: epoch  8, batch    35 | loss: 4.2433028CurrentTrain: epoch  8, batch    36 | loss: 4.1033335CurrentTrain: epoch  8, batch    37 | loss: 4.0933719CurrentTrain: epoch  8, batch    38 | loss: 4.0743179CurrentTrain: epoch  8, batch    39 | loss: 4.1031399CurrentTrain: epoch  8, batch    40 | loss: 4.1441622CurrentTrain: epoch  8, batch    41 | loss: 4.0516214CurrentTrain: epoch  8, batch    42 | loss: 4.0299883CurrentTrain: epoch  8, batch    43 | loss: 4.1013908CurrentTrain: epoch  8, batch    44 | loss: 4.1028876CurrentTrain: epoch  8, batch    45 | loss: 4.0804992CurrentTrain: epoch  8, batch    46 | loss: 4.0650382CurrentTrain: epoch  8, batch    47 | loss: 4.0906410CurrentTrain: epoch  8, batch    48 | loss: 4.0651250CurrentTrain: epoch  8, batch    49 | loss: 4.1132302CurrentTrain: epoch  8, batch    50 | loss: 4.0591273CurrentTrain: epoch  8, batch    51 | loss: 4.0762229CurrentTrain: epoch  8, batch    52 | loss: 4.2939148CurrentTrain: epoch  8, batch    53 | loss: 4.0639324CurrentTrain: epoch  8, batch    54 | loss: 4.0794191CurrentTrain: epoch  8, batch    55 | loss: 4.0764050CurrentTrain: epoch  8, batch    56 | loss: 4.1182504CurrentTrain: epoch  8, batch    57 | loss: 4.1122932CurrentTrain: epoch  8, batch    58 | loss: 4.1281357CurrentTrain: epoch  8, batch    59 | loss: 4.0472927CurrentTrain: epoch  8, batch    60 | loss: 4.0617256CurrentTrain: epoch  8, batch    61 | loss: 4.0881443CurrentTrain: epoch  8, batch    62 | loss: 4.0942435CurrentTrain: epoch  9, batch     0 | loss: 4.0510817CurrentTrain: epoch  9, batch     1 | loss: 4.0609097CurrentTrain: epoch  9, batch     2 | loss: 4.0739336CurrentTrain: epoch  9, batch     3 | loss: 4.0768356CurrentTrain: epoch  9, batch     4 | loss: 4.0545235CurrentTrain: epoch  9, batch     5 | loss: 4.0795269CurrentTrain: epoch  9, batch     6 | loss: 4.0104961CurrentTrain: epoch  9, batch     7 | loss: 4.1750002CurrentTrain: epoch  9, batch     8 | loss: 4.1239038CurrentTrain: epoch  9, batch     9 | loss: 4.0122499CurrentTrain: epoch  9, batch    10 | loss: 4.0003586CurrentTrain: epoch  9, batch    11 | loss: 4.0921416CurrentTrain: epoch  9, batch    12 | loss: 4.0907211CurrentTrain: epoch  9, batch    13 | loss: 4.0782895CurrentTrain: epoch  9, batch    14 | loss: 4.0614347CurrentTrain: epoch  9, batch    15 | loss: 4.0547504CurrentTrain: epoch  9, batch    16 | loss: 4.0864944CurrentTrain: epoch  9, batch    17 | loss: 4.0417299CurrentTrain: epoch  9, batch    18 | loss: 4.0853739CurrentTrain: epoch  9, batch    19 | loss: 4.0575953CurrentTrain: epoch  9, batch    20 | loss: 4.1036139CurrentTrain: epoch  9, batch    21 | loss: 4.0537858CurrentTrain: epoch  9, batch    22 | loss: 4.0830650CurrentTrain: epoch  9, batch    23 | loss: 4.1298637CurrentTrain: epoch  9, batch    24 | loss: 4.1191983CurrentTrain: epoch  9, batch    25 | loss: 4.1559215CurrentTrain: epoch  9, batch    26 | loss: 4.0967350CurrentTrain: epoch  9, batch    27 | loss: 4.0148396CurrentTrain: epoch  9, batch    28 | loss: 4.0964622CurrentTrain: epoch  9, batch    29 | loss: 4.0640278CurrentTrain: epoch  9, batch    30 | loss: 4.0475144CurrentTrain: epoch  9, batch    31 | loss: 4.0494871CurrentTrain: epoch  9, batch    32 | loss: 4.0687575CurrentTrain: epoch  9, batch    33 | loss: 4.0515099CurrentTrain: epoch  9, batch    34 | loss: 4.0802155CurrentTrain: epoch  9, batch    35 | loss: 4.0540619CurrentTrain: epoch  9, batch    36 | loss: 4.0397110CurrentTrain: epoch  9, batch    37 | loss: 4.0586348CurrentTrain: epoch  9, batch    38 | loss: 4.0395756CurrentTrain: epoch  9, batch    39 | loss: 4.0153513CurrentTrain: epoch  9, batch    40 | loss: 4.0607710CurrentTrain: epoch  9, batch    41 | loss: 4.0281310CurrentTrain: epoch  9, batch    42 | loss: 4.0575757CurrentTrain: epoch  9, batch    43 | loss: 4.0395088CurrentTrain: epoch  9, batch    44 | loss: 4.0662408CurrentTrain: epoch  9, batch    45 | loss: 4.0513973CurrentTrain: epoch  9, batch    46 | loss: 4.0836287CurrentTrain: epoch  9, batch    47 | loss: 4.0669107CurrentTrain: epoch  9, batch    48 | loss: 4.0539217CurrentTrain: epoch  9, batch    49 | loss: 4.0296774CurrentTrain: epoch  9, batch    50 | loss: 4.0643358CurrentTrain: epoch  9, batch    51 | loss: 4.0507469CurrentTrain: epoch  9, batch    52 | loss: 4.0301514CurrentTrain: epoch  9, batch    53 | loss: 4.0341148CurrentTrain: epoch  9, batch    54 | loss: 4.0864000CurrentTrain: epoch  9, batch    55 | loss: 4.0240221CurrentTrain: epoch  9, batch    56 | loss: 4.0194221CurrentTrain: epoch  9, batch    57 | loss: 4.0383654CurrentTrain: epoch  9, batch    58 | loss: 3.9993286CurrentTrain: epoch  9, batch    59 | loss: 3.9947338CurrentTrain: epoch  9, batch    60 | loss: 4.0082340CurrentTrain: epoch  9, batch    61 | loss: 4.0502806CurrentTrain: epoch  9, batch    62 | loss: 4.0223160
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: while the milanese painting scene was dominated by neoclassic painters andrea appiani and luigi sabatelli , giovannni migliara stayed with the historical themes and medieval subtlety of romanticism .
Head Entity: andrea appiani
Tail Entity: neoclassic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: during the early 20th century, the surrealist movement gained prominence, with artists like salvador dalí and max ernst pushing the boundaries of imagination and reality.  
Head Entity: salvador dalí  
Tail Entity: surrealist  

Relation: movement  
Context: the beat generation was a literary movement that emerged in the 1950s, characterized by authors such as jack kerouac and allen ginsberg who challenged conventional norms.  
Head Entity: jack kerouac  
Tail Entity: beat generation  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: faldu was born on 1 august 1957 to a family of farmer in kalavad , jamnagar district , bombay state ( present - day gujarat ) .
Head Entity: kalavad
Tail Entity: jamnagar district
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: The city of Springfield is known for its beautiful parks and vibrant community, situated in the heart of Sangamon County, Illinois.  
Head Entity: Springfield  
Tail Entity: Sangamon County  

Relation: located in the administrative territorial entity  
Context: The historic town of Williamsburg is a popular tourist destination, located within the boundaries of James City County, Virginia.  
Head Entity: Williamsburg  
Tail Entity: James City County  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: in 2004 , the mayor of bremen , henning scherf , repeated this act when the concert " disenchanted amazon " was presented in the bremen municipal government palace .
Head Entity: bremen
Tail Entity: henning scherf
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: In 2021, the Prime Minister of Canada, Justin Trudeau, announced new policies during a press conference held in Ottawa.  
Head Entity: Canada  
Tail Entity: Justin Trudeau  

Relation: head of government  
Context: During the annual state address, the President of France, Emmanuel Macron, outlined his vision for the country's future in Paris.  
Head Entity: France  
Tail Entity: Emmanuel Macron  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: the fourth line - up of the sugababes , consisting of range , amelle berrabah and jade ewen , performed the song at the london superclub in march 2010 as part of a gig .
Head Entity: jade ewen
Tail Entity: sugababes
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: After years of hard work and dedication, Sarah finally became a member of the prestigious National Art Society, where she could showcase her talent alongside other renowned artists.  
Head Entity: Sarah  
Tail Entity: National Art Society  

Relation: member of  
Context: The talented musician joined the band last year, bringing her unique style to the group, and has since become a beloved member of the Jazz Collective.  
Head Entity: The talented musician  
Tail Entity: Jazz Collective  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: on july 2 , 1935 , the rip van winkle bridge over the hudson river between catskill and greenport was opened to traffic .
Head Entity: rip van winkle bridge
Tail Entity: hudson river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: The Golden Gate Bridge spans the Golden Gate Strait, connecting San Francisco to Marin County and allowing vehicles to cross over the waterway.  
Head Entity: Golden Gate Bridge  
Tail Entity: Golden Gate Strait  

Relation: crosses  
Context: The Channel Tunnel, also known as the Chunnel, runs beneath the English Channel, providing a passage for trains to cross under the sea between England and France.  
Head Entity: Channel Tunnel  
Tail Entity: English Channel  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: the municipality contains the lower reaches of the maracanã river , which is joined by the caripi river before widening into maracanã bay and emptying into the atlantic ocean beside maiandeua island .
Head Entity: maracanã river
Tail Entity: caripi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The Amazon River, one of the largest rivers in the world, receives numerous tributaries, including the Madeira River, which significantly contributes to its flow.  
Head Entity: Amazon River  
Tail Entity: Madeira River  

Relation: tributary  
Context: The Missouri River is known for its many tributaries, such as the Platte River, which merges with it before it continues its journey towards the Mississippi River.  
Head Entity: Missouri River  
Tail Entity: Platte River  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: the result was widely interpreted as a personal rebuke to then chief minister shane stone .
Head Entity: shane stone
Tail Entity: chief minister
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: After serving as the mayor for over a decade, John Smith announced his retirement from the position.  
Head Entity: John Smith  
Tail Entity: mayor  

Relation: position held  
Context: The former president of the organization, Maria Lopez, was recognized for her contributions during her tenure.  
Head Entity: Maria Lopez  
Tail Entity: president
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: in january 2009 , it was revealed that raw thrills and specular interactive came together to produce a spiritual successor to the midway arcade racer " hydro thunder " with a game entitled h2overdrive .
Head Entity: h2overdrive
Tail Entity: specular interactive
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: In 2015, the popular game "The Witcher 3: Wild Hunt" was released, developed by CD Projekt Red, which received critical acclaim for its storytelling and open-world design.  
Head Entity: The Witcher 3: Wild Hunt  
Tail Entity: CD Projekt Red  

Relation: developer  
Context: The innovative mobile game "Angry Birds" was created by Rovio Entertainment, which became a cultural phenomenon shortly after its launch in 2009.  
Head Entity: Angry Birds  
Tail Entity: Rovio Entertainment  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: neptune is the second studio album by the london - based band the duke spirit and the last with the original lead guitarist , dan higgins .
Head Entity: the duke spirit
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the tech startup was founded in silicon valley by a group of innovative engineers looking to revolutionize the industry.  
Head Entity: the tech startup  
Tail Entity: silicon valley  

Relation: location of formation  
Context: the famous rock band was established in new york city, where they quickly gained a following and changed the music scene.  
Head Entity: the famous rock band  
Tail Entity: new york city  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: " shining time station " won a number of awards and significantly increased the popularity of the " thomas " media franchise in the united states .
Head Entity: shining time station
Tail Entity: united states
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous dish "sushi" is traditionally associated with Japan and has gained immense popularity worldwide.  
Head Entity: sushi  
Tail Entity: Japan  

Relation: country of origin  
Context: The iconic brand "Guinness" is known for its rich stout beer, which originated in Ireland and is now enjoyed globally.  
Head Entity: Guinness  
Tail Entity: Ireland  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.79%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 94.08%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.60%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.84%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.79%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.68%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.61%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.96%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.40%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.61%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.67%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.98%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 95.93%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.02%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 95.97%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 96.14%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 96.09%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.17%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.20%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 96.03%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.99%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.80%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.76%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.72%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.80%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.87%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.90%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.14%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.79%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 94.08%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.60%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.84%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.79%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.68%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.61%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.96%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.40%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.61%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.67%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.98%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 95.93%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.02%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 95.97%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 96.14%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 96.09%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.17%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.20%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 96.03%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.99%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.80%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.76%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.72%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.80%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.87%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.90%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.14%   
cur_acc:  ['0.9514']
his_acc:  ['0.9514']
CurrentTrain: epoch  0, batch     0 | loss: 7.4188385CurrentTrain: epoch  0, batch     1 | loss: 6.6607924CurrentTrain: epoch  0, batch     2 | loss: 7.5862131CurrentTrain: epoch  0, batch     3 | loss: 8.8738594CurrentTrain: epoch  1, batch     0 | loss: 6.7590580CurrentTrain: epoch  1, batch     1 | loss: 5.4581628CurrentTrain: epoch  1, batch     2 | loss: 6.7130232CurrentTrain: epoch  1, batch     3 | loss: 3.9216621CurrentTrain: epoch  2, batch     0 | loss: 5.2730827CurrentTrain: epoch  2, batch     1 | loss: 6.0257902CurrentTrain: epoch  2, batch     2 | loss: 5.0569830CurrentTrain: epoch  2, batch     3 | loss: 4.6171689CurrentTrain: epoch  3, batch     0 | loss: 5.3508377CurrentTrain: epoch  3, batch     1 | loss: 4.4741292CurrentTrain: epoch  3, batch     2 | loss: 4.7147117CurrentTrain: epoch  3, batch     3 | loss: 5.3102965CurrentTrain: epoch  4, batch     0 | loss: 4.5222993CurrentTrain: epoch  4, batch     1 | loss: 5.3435555CurrentTrain: epoch  4, batch     2 | loss: 3.6499515CurrentTrain: epoch  4, batch     3 | loss: 7.1842699CurrentTrain: epoch  5, batch     0 | loss: 5.0626359CurrentTrain: epoch  5, batch     1 | loss: 4.7457471CurrentTrain: epoch  5, batch     2 | loss: 3.5699325CurrentTrain: epoch  5, batch     3 | loss: 3.5315878CurrentTrain: epoch  6, batch     0 | loss: 4.5820794CurrentTrain: epoch  6, batch     1 | loss: 4.0825291CurrentTrain: epoch  6, batch     2 | loss: 3.6819925CurrentTrain: epoch  6, batch     3 | loss: 4.3636127CurrentTrain: epoch  7, batch     0 | loss: 3.6457365CurrentTrain: epoch  7, batch     1 | loss: 4.8021441CurrentTrain: epoch  7, batch     2 | loss: 3.7917485CurrentTrain: epoch  7, batch     3 | loss: 1.9225965CurrentTrain: epoch  8, batch     0 | loss: 3.4084568CurrentTrain: epoch  8, batch     1 | loss: 4.6444454CurrentTrain: epoch  8, batch     2 | loss: 3.6596870CurrentTrain: epoch  8, batch     3 | loss: 2.5102472CurrentTrain: epoch  9, batch     0 | loss: 3.7779102CurrentTrain: epoch  9, batch     1 | loss: 3.5588121CurrentTrain: epoch  9, batch     2 | loss: 3.2689469CurrentTrain: epoch  9, batch     3 | loss: 4.5517502
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: in 2004 the catalan government gave him the george cross .
Head Entity: george cross
Tail Entity: catalan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The new environmental regulations introduced by the European Union will affect all member states.  
Head Entity: environmental regulations  
Tail Entity: European Union  

Relation: applies to jurisdiction  
Context: The Supreme Court's ruling has significant implications for the state of California's water rights.  
Head Entity: Supreme Court  
Tail Entity: California  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: it is prominently featured on jean michel jarre 's albums " oxygène " ( 1977 ) and " équinoxe " ( 1978 ) .
Head Entity: équinoxe
Tail Entity: jean michel jarre
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: The legendary band Queen is known for their iconic song "Bohemian Rhapsody," which showcases Freddie Mercury's extraordinary vocal talent.  
Head Entity: Bohemian Rhapsody  
Tail Entity: Freddie Mercury  

Relation: performer  
Context: Taylor Swift performed her hit single "Shake It Off" at the Grammy Awards, captivating the audience with her energetic stage presence.  
Head Entity: Shake It Off  
Tail Entity: Taylor Swift  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the glasflügel h-201 standard " libelle " ( german : " dragonfly " ) is an early composite standard class single - seat sailplane produced by glasflügel from 1967 .
Head Entity: dragonfly
Tail Entity: glasflügel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The iPhone 13, a smartphone developed by Apple, was released in September 2021 and features advanced camera technology and improved battery life.  
Head Entity: iPhone 13  
Tail Entity: Apple  

Relation: manufacturer  
Context: The Tesla Model S is an all-electric luxury sedan that was first introduced by Tesla, Inc. in 2012, showcasing innovative technology and performance.  
Head Entity: Tesla Model S  
Tail Entity: Tesla, Inc.  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: in contrast to gwladys and roger 's allegiance to the house of lancaster , their three sons were staunch yorkists during the wars of the roses .
Head Entity: wars of the roses
Tail Entity: house of lancaster
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: During the annual tech conference, several leading companies showcased their latest innovations, with Google and Microsoft being the most prominent participants.  
Head Entity: tech conference  
Tail Entity: Google  

Relation: participant  
Context: The local community organized a charity run, where various schools and organizations, including the Red Cross, participated to raise funds for the homeless.  
Head Entity: charity run  
Tail Entity: Red Cross  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the application is compatible with various operating systems, including macOS, Windows, and Ubuntu.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: Many developers prefer to use Linux as their operating system due to its flexibility and open-source nature.  
Head Entity: developers  
Tail Entity: Linux  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in his next bout , he won the wbc light welterweight title by defeating roger mayweather for a second time .
Head Entity: roger mayweather
Tail Entity: light welterweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: The team secured their place in the tournament by qualifying under the UEFA Champions League regulations.  
Head Entity: UEFA Champions League  
Tail Entity: tournament  

Relation: competition class  
Context: The athlete competed in the 100 meters dash, which is classified under the Olympic Games standards.  
Head Entity: Olympic Games  
Tail Entity: 100 meters dash  
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: the qup gained 3 seats from its previous election result , and won a 10 % swing to it in primary votes , but failed to defeat the incumbent labor government led by ted theodore .
Head Entity: previous election
Tail Entity: ted theodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: After a hard-fought campaign, the voters of the district chose Sarah Johnson as their representative, marking a significant shift in the political landscape.  
Head Entity: campaign  
Tail Entity: Sarah Johnson  

Relation: successful candidate  
Context: In the recent mayoral election, the community rallied behind Mark Thompson, who promised to bring change and transparency to the local government.  
Head Entity: mayoral election  
Tail Entity: Mark Thompson  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: the deaths of his brothers wenceslaus ii ( 1487 ) , casimir ii ( 1490 ) and władysław ( 1494 ) allowed jan v to reunificated the whole duchy of zator .
Head Entity: casimir ii
Tail Entity: wenceslaus ii
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: both elizabeth and her brother, charles, were known for their contributions to the arts and culture of their time.  
Head Entity: elizabeth  
Tail Entity: charles  

Relation: sibling  
Context: during the family reunion, it was heartwarming to see how much john and his sister, sarah, resembled each other in both looks and personality.  
Head Entity: john  
Tail Entity: sarah  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: iain harrison is a competitive shooter and former british army captain .
Head Entity: iain harrison
Tail Entity: british army
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: the united states navy is known for its naval operations and maritime security.  
Head Entity: united states navy  
Tail Entity: naval operations  

Relation: military branch  
Context: general patton served in the united states army during world war ii.  
Head Entity: general patton  
Tail Entity: united states army  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: ziad rahbani is the son of the lebanese famous composer assi rahbani and nouhad haddad , the famous lebanese female singer known as fairuz .
Head Entity: assi rahbani
Tail Entity: ziad rahbani
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: in the animated series, the character simon is the son of the adventurous couple, marie and john, who often embark on thrilling quests together.  
Head Entity: marie  
Tail Entity: simon  

Relation: child  
Context: the renowned physicist albert einstein had a daughter named lieserl, who was born in 1902 and later became a subject of much speculation.  
Head Entity: albert einstein  
Tail Entity: lieserl  
Mixup data size:  199
MixupTrain:  epoch  0, batch     0 | loss: 6.1940977MixupTrain:  epoch  0, batch     1 | loss: 5.8055910MixupTrain:  epoch  0, batch     2 | loss: 6.0441995MixupTrain:  epoch  0, batch     3 | loss: 5.3192917MixupTrain:  epoch  0, batch     4 | loss: 5.3362277MixupTrain:  epoch  0, batch     5 | loss: 5.2687946MixupTrain:  epoch  0, batch     6 | loss: 5.1563034MixupTrain:  epoch  0, batch     7 | loss: 5.7540538MixupTrain:  epoch  0, batch     8 | loss: 4.9282934MixupTrain:  epoch  0, batch     9 | loss: 4.7938204MixupTrain:  epoch  0, batch    10 | loss: 4.9841016MixupTrain:  epoch  0, batch    11 | loss: 4.6782019MixupTrain:  epoch  0, batch    12 | loss: 4.4023854
MemoryTrain:  epoch  0, batch     0 | loss: 4.2644901MemoryTrain:  epoch  0, batch     1 | loss: 4.5938587MemoryTrain:  epoch  0, batch     2 | loss: 4.5703278MemoryTrain:  epoch  0, batch     3 | loss: 4.0900736MemoryTrain:  epoch  1, batch     0 | loss: 5.0176654MemoryTrain:  epoch  1, batch     1 | loss: 3.4156818MemoryTrain:  epoch  1, batch     2 | loss: 3.1467323MemoryTrain:  epoch  1, batch     3 | loss: 3.1984115MemoryTrain:  epoch  2, batch     0 | loss: 3.3696115MemoryTrain:  epoch  2, batch     1 | loss: 2.6470690MemoryTrain:  epoch  2, batch     2 | loss: 3.3608165MemoryTrain:  epoch  2, batch     3 | loss: 3.2362161MemoryTrain:  epoch  3, batch     0 | loss: 2.9117050MemoryTrain:  epoch  3, batch     1 | loss: 2.7417722MemoryTrain:  epoch  3, batch     2 | loss: 2.7385488MemoryTrain:  epoch  3, batch     3 | loss: 3.2609594MemoryTrain:  epoch  4, batch     0 | loss: 2.5570655MemoryTrain:  epoch  4, batch     1 | loss: 2.6791172MemoryTrain:  epoch  4, batch     2 | loss: 2.8732524MemoryTrain:  epoch  4, batch     3 | loss: 2.6651123MemoryTrain:  epoch  5, batch     0 | loss: 2.7115712MemoryTrain:  epoch  5, batch     1 | loss: 2.2753384MemoryTrain:  epoch  5, batch     2 | loss: 2.4333677MemoryTrain:  epoch  5, batch     3 | loss: 2.4419498MemoryTrain:  epoch  6, batch     0 | loss: 2.0873101MemoryTrain:  epoch  6, batch     1 | loss: 2.4689732MemoryTrain:  epoch  6, batch     2 | loss: 2.4081266MemoryTrain:  epoch  6, batch     3 | loss: 1.8250093MemoryTrain:  epoch  7, batch     0 | loss: 2.1570299MemoryTrain:  epoch  7, batch     1 | loss: 2.0491834MemoryTrain:  epoch  7, batch     2 | loss: 2.0385733MemoryTrain:  epoch  7, batch     3 | loss: 1.7593501MemoryTrain:  epoch  8, batch     0 | loss: 1.9585390MemoryTrain:  epoch  8, batch     1 | loss: 2.2427690MemoryTrain:  epoch  8, batch     2 | loss: 2.0019810MemoryTrain:  epoch  8, batch     3 | loss: 1.7188342MemoryTrain:  epoch  9, batch     0 | loss: 1.7096751MemoryTrain:  epoch  9, batch     1 | loss: 1.9903203MemoryTrain:  epoch  9, batch     2 | loss: 2.0379131MemoryTrain:  epoch  9, batch     3 | loss: 1.4897503
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 60.94%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 64.58%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 67.86%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 73.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 75.52%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 77.40%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 77.23%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 78.68%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 79.86%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 80.26%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 80.31%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 80.36%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 80.68%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 80.98%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 81.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.73%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 82.41%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.04%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 83.62%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 84.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.68%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 84.96%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 85.85%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 86.63%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.99%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.34%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 87.19%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 86.89%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 86.90%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.06%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 87.22%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 87.22%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 87.09%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 87.23%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 87.37%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 87.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 87.75%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 87.86%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 87.97%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 87.85%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 87.95%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 87.95%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 87.28%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 86.21%   [EVAL] batch:   58 | acc: 12.50%,  total acc: 84.96%   [EVAL] batch:   59 | acc: 31.25%,  total acc: 84.06%   [EVAL] batch:   60 | acc: 6.25%,  total acc: 82.79%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 81.75%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 80.65%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 91.41%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 91.54%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 92.56%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 92.61%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 92.39%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 92.45%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.55%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 92.82%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.89%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.93%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.93%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.10%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.26%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 94.41%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.55%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.82%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.94%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 94.91%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 94.74%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 94.72%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 94.57%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 94.28%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 94.01%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.13%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 94.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.12%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 93.99%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 93.99%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.10%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 93.86%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.86%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 93.86%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 93.86%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 93.85%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 93.95%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 93.65%   [EVAL] batch:   63 | acc: 62.50%,  total acc: 93.16%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 92.79%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 92.33%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 91.88%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 91.64%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 91.39%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 91.43%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 91.29%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 91.15%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 91.10%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 91.13%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 91.08%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 90.95%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 90.91%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 90.71%   [EVAL] batch:   78 | acc: 100.00%,  total acc: 90.82%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 90.78%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 90.90%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 90.62%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 90.59%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 90.55%   [EVAL] batch:   84 | acc: 87.50%,  total acc: 90.51%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 90.41%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 90.45%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 90.34%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 90.45%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 90.56%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 90.66%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 90.76%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 90.86%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 90.89%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 90.99%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 91.08%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 91.17%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 91.26%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 91.35%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 91.44%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 91.46%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 91.48%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 91.26%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 91.17%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 91.19%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 91.16%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 91.18%   [EVAL] batch:  107 | acc: 81.25%,  total acc: 91.09%   [EVAL] batch:  108 | acc: 93.75%,  total acc: 91.11%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 91.14%   [EVAL] batch:  110 | acc: 93.75%,  total acc: 91.16%   [EVAL] batch:  111 | acc: 87.50%,  total acc: 91.13%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 91.21%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 91.17%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 91.22%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 91.19%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 91.21%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 91.07%   [EVAL] batch:  119 | acc: 25.00%,  total acc: 90.52%   [EVAL] batch:  120 | acc: 18.75%,  total acc: 89.93%   [EVAL] batch:  121 | acc: 12.50%,  total acc: 89.29%   [EVAL] batch:  122 | acc: 25.00%,  total acc: 88.77%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 88.10%   [EVAL] batch:  124 | acc: 31.25%,  total acc: 87.65%   
cur_acc:  ['0.9514', '0.8065']
his_acc:  ['0.9514', '0.8765']
CurrentTrain: epoch  0, batch     0 | loss: 6.7100725CurrentTrain: epoch  0, batch     1 | loss: 6.0597520CurrentTrain: epoch  0, batch     2 | loss: 6.7120123CurrentTrain: epoch  0, batch     3 | loss: 8.6393919CurrentTrain: epoch  1, batch     0 | loss: 5.9466105CurrentTrain: epoch  1, batch     1 | loss: 5.3398709CurrentTrain: epoch  1, batch     2 | loss: 5.5373602CurrentTrain: epoch  1, batch     3 | loss: 6.9850368CurrentTrain: epoch  2, batch     0 | loss: 5.1656981CurrentTrain: epoch  2, batch     1 | loss: 4.9049606CurrentTrain: epoch  2, batch     2 | loss: 5.3902259CurrentTrain: epoch  2, batch     3 | loss: 6.2050877CurrentTrain: epoch  3, batch     0 | loss: 4.8088665CurrentTrain: epoch  3, batch     1 | loss: 5.1097708CurrentTrain: epoch  3, batch     2 | loss: 4.3557296CurrentTrain: epoch  3, batch     3 | loss: 3.1047606CurrentTrain: epoch  4, batch     0 | loss: 4.5404997CurrentTrain: epoch  4, batch     1 | loss: 4.5960712CurrentTrain: epoch  4, batch     2 | loss: 4.0217695CurrentTrain: epoch  4, batch     3 | loss: 4.5745225CurrentTrain: epoch  5, batch     0 | loss: 4.0120983CurrentTrain: epoch  5, batch     1 | loss: 3.9356050CurrentTrain: epoch  5, batch     2 | loss: 4.4160442CurrentTrain: epoch  5, batch     3 | loss: 2.4438686CurrentTrain: epoch  6, batch     0 | loss: 4.0682130CurrentTrain: epoch  6, batch     1 | loss: 3.6002817CurrentTrain: epoch  6, batch     2 | loss: 3.7421207CurrentTrain: epoch  6, batch     3 | loss: 3.1392040CurrentTrain: epoch  7, batch     0 | loss: 3.4081435CurrentTrain: epoch  7, batch     1 | loss: 3.5841467CurrentTrain: epoch  7, batch     2 | loss: 3.6267629CurrentTrain: epoch  7, batch     3 | loss: 4.5950851CurrentTrain: epoch  8, batch     0 | loss: 3.1614637CurrentTrain: epoch  8, batch     1 | loss: 3.5696905CurrentTrain: epoch  8, batch     2 | loss: 3.3052955CurrentTrain: epoch  8, batch     3 | loss: 4.1488309CurrentTrain: epoch  9, batch     0 | loss: 3.1053936CurrentTrain: epoch  9, batch     1 | loss: 3.4133725CurrentTrain: epoch  9, batch     2 | loss: 3.1330481CurrentTrain: epoch  9, batch     3 | loss: 4.1723323
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: auerbach is prominently featured in the documentary film , " the first basket " , about jewish basketball history .
Head Entity: the first basket
Tail Entity: basketball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: the book "sapiens: a brief history of humankind" explores the evolution of human societies and cultures.  
Head Entity: sapiens: a brief history of humankind  
Tail Entity: human societies  

Relation: main subject  
Context: the documentary "our planet" showcases the beauty of nature and the impact of climate change on wildlife.  
Head Entity: our planet  
Tail Entity: nature  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: he represented romania at euro 2000 , his first major tournament where they beat england 3–2 , and drew 1–1 against germany .
Head Entity: euro 2000
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: The Brazilian national team showcased their skills at the 2014 FIFA World Cup, where they faced off against Germany in the semi-finals.  
Head Entity: 2014 FIFA World Cup  
Tail Entity: Germany  

Relation: participating team  
Context: During the 2021 Tokyo Olympics, the United States women's soccer team competed fiercely, ultimately playing against Canada in the semi-finals.  
Head Entity: 2021 Tokyo Olympics  
Tail Entity: Canada  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: at state level , the parliament of victoria consists of the legislative assembly ( the lower house ) and the legislative council ( the upper house ) .
Head Entity: parliament of victoria
Tail Entity: legislative council
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The human body is composed of various systems, including the circulatory system, which is essential for transporting blood throughout the body.  
Head Entity: human body  
Tail Entity: circulatory system  

Relation: has part  
Context: The solar system includes several celestial bodies, with the planet Earth being one of the key components that supports life.  
Head Entity: solar system  
Tail Entity: planet Earth  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: for her performance in the musical - drama " crazy heart " ( 2009 ) , she was nominated for the academy award for best supporting actress .
Head Entity: crazy heart
Tail Entity: academy award for best supporting actress
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film "The Shape of Water" received critical acclaim and was nominated for several prestigious awards, including the Academy Award for Best Picture.  
Head Entity: The Shape of Water  
Tail Entity: Academy Award for Best Picture  

Relation: nominated for  
Context: In 2021, the popular series "The Crown" was nominated for the Emmy Award for Outstanding Drama Series, showcasing its impact on television.  
Head Entity: The Crown  
Tail Entity: Emmy Award for Outstanding Drama Series  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: on march 2015 , cube entertainment launched the multi - national girl group clc including original members seunghee , yujin , seungyeon , sorn and yeeun .
Head Entity: clc
Tail Entity: girl group
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone brand Apple released its latest model, the iPhone 14, which features advanced technology and improved camera capabilities.  
Head Entity: iPhone 14  
Tail Entity: smartphone  

Relation: instance of  
Context: The famous painting "Starry Night" was created by the artist Vincent van Gogh and is considered a masterpiece of post-impressionism.  
Head Entity: Starry Night  
Tail Entity: painting  
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: brand peak is a sharp snow - covered peak located east - southeast of the eternity range and northwest of mount duemler , in palmer land .
Head Entity: mount duemler
Tail Entity: eternity range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the rocky mountains extend from Canada through the United States and into Mexico, forming a major mountain range in North America.  
Head Entity: rocky mountains  
Tail Entity: North America  

Relation: mountain range  
Context: the appalachian mountains are one of the oldest mountain ranges in the world, stretching from Canada down to Alabama.  
Head Entity: appalachian mountains  
Tail Entity: Alabama  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: later that year he had a minor hit film with " landru " , written by françoise sagan and starring charles denner , michèle morgan , danielle darrieux and hildegard knef .
Head Entity: " landru "
Tail Entity: françoise sagan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: The acclaimed film "Inception," directed by Christopher Nolan, features a complex narrative crafted by the talented screenwriter, who is none other than Nolan himself.  
Head Entity: "Inception"  
Tail Entity: Christopher Nolan  

Relation: screenwriter  
Context: The beloved animated movie "Toy Story" was brought to life through the imaginative script penned by the brilliant screenwriter, Joss Whedon, alongside his team.  
Head Entity: "Toy Story"  
Tail Entity: Joss Whedon  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the publisher found success in 1993 with tarō gomi 's " everyone poops " , a japanese language import that sold over one million copies and is now their best - known title .
Head Entity: everyone poops
Tail Entity: japanese language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The animated series "Avatar: The Last Airbender" features a rich narrative and is primarily produced in English, appealing to a global audience.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  

Relation: language of work or name  
Context: The famous novel "Cien años de soledad" by Gabriel García Márquez is a cornerstone of Latin American literature and is originally written in Spanish.  
Head Entity: Cien años de soledad  
Tail Entity: Spanish  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: the aircraft was a boeing 707 - 321c that first entered service with pan american world airways ( pan am ) in 1963 .
Head Entity: boeing 707 - 321c
Tail Entity: pan american world airways
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: the new electric bus fleet is managed by the city transportation authority, which ensures efficient public transit services.  
Head Entity: electric bus fleet  
Tail Entity: city transportation authority  

Relation: operator  
Context: the state-of-the-art research facility is operated by the national science foundation, providing resources for groundbreaking studies.  
Head Entity: research facility  
Tail Entity: national science foundation  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: the most important lama of this series was the third changkya , rolpai dorje , who was preceptor to the qianlong emperor of china , and chief representative of tibetan buddhism at the qing court .
Head Entity: rolpai dorje
Tail Entity: tibetan buddhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the cathedral of notre-dame is a famous example of french gothic architecture and serves as a major center for the roman catholic faith in paris.  
Head Entity: cathedral of notre-dame  
Tail Entity: roman catholic faith  

Relation: religion  
Context: the dalai lama is a prominent figure in the gelug school of tibetan buddhism, advocating for peace and compassion worldwide.  
Head Entity: dalai lama  
Tail Entity: tibetan buddhism  
Mixup data size:  259
MixupTrain:  epoch  0, batch     0 | loss: 3.7592298MixupTrain:  epoch  0, batch     1 | loss: 3.2806703MixupTrain:  epoch  0, batch     2 | loss: 3.7344153MixupTrain:  epoch  0, batch     3 | loss: 3.0951874MixupTrain:  epoch  0, batch     4 | loss: 3.4676948MixupTrain:  epoch  0, batch     5 | loss: 3.4066837MixupTrain:  epoch  0, batch     6 | loss: 2.8985486MixupTrain:  epoch  0, batch     7 | loss: 3.4849193MixupTrain:  epoch  0, batch     8 | loss: 3.5244668MixupTrain:  epoch  0, batch     9 | loss: 3.1882005MixupTrain:  epoch  0, batch    10 | loss: 3.0383708MixupTrain:  epoch  0, batch    11 | loss: 2.8890519MixupTrain:  epoch  0, batch    12 | loss: 3.1924223MixupTrain:  epoch  0, batch    13 | loss: 2.8016137MixupTrain:  epoch  0, batch    14 | loss: 2.4024901MixupTrain:  epoch  0, batch    15 | loss: 2.8126739MixupTrain:  epoch  0, batch    16 | loss: 2.8447990
MemoryTrain:  epoch  0, batch     0 | loss: 2.7349937MemoryTrain:  epoch  0, batch     1 | loss: 2.3761153MemoryTrain:  epoch  0, batch     2 | loss: 3.3735862MemoryTrain:  epoch  0, batch     3 | loss: 2.8257785MemoryTrain:  epoch  0, batch     4 | loss: 3.4229193MemoryTrain:  epoch  0, batch     5 | loss: 3.1943305MemoryTrain:  epoch  1, batch     0 | loss: 2.2394159MemoryTrain:  epoch  1, batch     1 | loss: 2.1574113MemoryTrain:  epoch  1, batch     2 | loss: 2.4157133MemoryTrain:  epoch  1, batch     3 | loss: 2.5470290MemoryTrain:  epoch  1, batch     4 | loss: 2.4229782MemoryTrain:  epoch  1, batch     5 | loss: 2.7781208MemoryTrain:  epoch  2, batch     0 | loss: 2.6255295MemoryTrain:  epoch  2, batch     1 | loss: 2.3221583MemoryTrain:  epoch  2, batch     2 | loss: 2.0583262MemoryTrain:  epoch  2, batch     3 | loss: 1.7910869MemoryTrain:  epoch  2, batch     4 | loss: 2.1190915MemoryTrain:  epoch  2, batch     5 | loss: 1.7769455MemoryTrain:  epoch  3, batch     0 | loss: 2.2495198MemoryTrain:  epoch  3, batch     1 | loss: 1.8752295MemoryTrain:  epoch  3, batch     2 | loss: 2.1660628MemoryTrain:  epoch  3, batch     3 | loss: 1.8801328MemoryTrain:  epoch  3, batch     4 | loss: 1.6965958MemoryTrain:  epoch  3, batch     5 | loss: 1.6434822MemoryTrain:  epoch  4, batch     0 | loss: 1.9018650MemoryTrain:  epoch  4, batch     1 | loss: 1.7169106MemoryTrain:  epoch  4, batch     2 | loss: 2.0669823MemoryTrain:  epoch  4, batch     3 | loss: 1.6913579MemoryTrain:  epoch  4, batch     4 | loss: 1.6242491MemoryTrain:  epoch  4, batch     5 | loss: 1.9823939MemoryTrain:  epoch  5, batch     0 | loss: 1.7721014MemoryTrain:  epoch  5, batch     1 | loss: 1.8954153MemoryTrain:  epoch  5, batch     2 | loss: 1.8130484MemoryTrain:  epoch  5, batch     3 | loss: 1.5940361MemoryTrain:  epoch  5, batch     4 | loss: 1.4560668MemoryTrain:  epoch  5, batch     5 | loss: 1.7326485MemoryTrain:  epoch  6, batch     0 | loss: 1.5754517MemoryTrain:  epoch  6, batch     1 | loss: 1.6355543MemoryTrain:  epoch  6, batch     2 | loss: 1.6176333MemoryTrain:  epoch  6, batch     3 | loss: 1.4792418MemoryTrain:  epoch  6, batch     4 | loss: 1.7432044MemoryTrain:  epoch  6, batch     5 | loss: 1.5680599MemoryTrain:  epoch  7, batch     0 | loss: 1.5773693MemoryTrain:  epoch  7, batch     1 | loss: 1.5135474MemoryTrain:  epoch  7, batch     2 | loss: 1.5298710MemoryTrain:  epoch  7, batch     3 | loss: 1.6567430MemoryTrain:  epoch  7, batch     4 | loss: 1.4399498MemoryTrain:  epoch  7, batch     5 | loss: 1.3466790MemoryTrain:  epoch  8, batch     0 | loss: 1.4229493MemoryTrain:  epoch  8, batch     1 | loss: 1.3470209MemoryTrain:  epoch  8, batch     2 | loss: 1.5461884MemoryTrain:  epoch  8, batch     3 | loss: 1.4173181MemoryTrain:  epoch  8, batch     4 | loss: 1.6060622MemoryTrain:  epoch  8, batch     5 | loss: 1.3409131MemoryTrain:  epoch  9, batch     0 | loss: 1.4963245MemoryTrain:  epoch  9, batch     1 | loss: 1.5208268MemoryTrain:  epoch  9, batch     2 | loss: 1.4385488MemoryTrain:  epoch  9, batch     3 | loss: 1.2752076MemoryTrain:  epoch  9, batch     4 | loss: 1.3685484MemoryTrain:  epoch  9, batch     5 | loss: 1.3693007
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.36%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 84.62%   [EVAL] batch:   13 | acc: 6.25%,  total acc: 79.02%   [EVAL] batch:   14 | acc: 6.25%,  total acc: 74.17%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 70.70%   [EVAL] batch:   16 | acc: 6.25%,  total acc: 66.91%   [EVAL] batch:   17 | acc: 0.00%,  total acc: 63.19%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 61.51%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 63.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 66.48%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 69.27%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 70.25%   [EVAL] batch:   25 | acc: 62.50%,  total acc: 69.95%   [EVAL] batch:   26 | acc: 43.75%,  total acc: 68.98%   [EVAL] batch:   27 | acc: 75.00%,  total acc: 69.20%   [EVAL] batch:   28 | acc: 81.25%,  total acc: 69.61%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 68.96%   [EVAL] batch:   30 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 69.53%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 70.27%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 70.96%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 71.61%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 72.22%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 72.97%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 73.68%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 74.36%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 74.84%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 75.30%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 75.89%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 76.45%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 76.99%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 77.17%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 77.26%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 77.30%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 77.62%   [EVAL] batch:   50 | acc: 50.00%,  total acc: 77.08%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 77.04%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 77.00%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 76.93%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 76.90%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 77.16%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 77.44%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 77.40%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 77.56%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 77.62%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 77.08%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 80.00%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 79.81%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 80.36%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 82.42%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 83.09%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 83.68%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 84.54%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 85.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 86.01%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 86.36%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 86.68%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 86.72%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 86.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.26%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 87.73%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 87.95%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 88.15%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.91%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.26%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 89.89%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 90.54%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 90.79%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 91.31%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.52%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 91.57%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 91.62%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 91.58%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 91.09%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 90.89%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 90.94%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 91.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.05%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 91.11%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 91.16%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 91.32%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.29%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 91.34%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 91.27%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 91.31%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 91.39%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 91.43%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 91.07%   [EVAL] batch:   63 | acc: 62.50%,  total acc: 90.62%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 90.29%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 90.15%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 89.93%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 89.71%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 89.58%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 89.64%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 89.70%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 89.41%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 89.47%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 89.53%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 89.50%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 89.39%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 89.45%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 89.26%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 89.00%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 89.12%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 88.41%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 87.80%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 87.35%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 86.69%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 86.05%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 85.49%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 85.09%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 85.25%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 85.51%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 85.67%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 85.82%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 85.90%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 86.05%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 86.20%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 86.34%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 86.48%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 86.62%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 86.75%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 86.82%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 86.89%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 86.83%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 86.84%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 86.96%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 87.03%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 87.03%   [EVAL] batch:  107 | acc: 75.00%,  total acc: 86.92%   [EVAL] batch:  108 | acc: 93.75%,  total acc: 86.98%   [EVAL] batch:  109 | acc: 75.00%,  total acc: 86.88%   [EVAL] batch:  110 | acc: 87.50%,  total acc: 86.88%   [EVAL] batch:  111 | acc: 75.00%,  total acc: 86.77%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 86.89%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 86.90%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 87.01%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 87.02%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 87.02%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 87.02%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 86.97%   [EVAL] batch:  119 | acc: 31.25%,  total acc: 86.51%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 86.16%   [EVAL] batch:  121 | acc: 25.00%,  total acc: 85.66%   [EVAL] batch:  122 | acc: 43.75%,  total acc: 85.32%   [EVAL] batch:  123 | acc: 18.75%,  total acc: 84.78%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 84.55%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 84.42%   [EVAL] batch:  126 | acc: 81.25%,  total acc: 84.40%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 84.35%   [EVAL] batch:  129 | acc: 100.00%,  total acc: 84.47%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 84.45%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 84.47%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 84.59%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 84.56%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 84.58%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 84.70%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 84.81%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 84.56%   [EVAL] batch:  138 | acc: 6.25%,  total acc: 83.99%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 83.44%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 82.98%   [EVAL] batch:  141 | acc: 6.25%,  total acc: 82.44%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 81.86%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 81.51%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 81.64%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 81.76%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 81.85%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 81.97%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 82.09%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 82.17%   [EVAL] batch:  150 | acc: 62.50%,  total acc: 82.04%   [EVAL] batch:  151 | acc: 43.75%,  total acc: 81.78%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 81.74%   [EVAL] batch:  153 | acc: 81.25%,  total acc: 81.74%   [EVAL] batch:  154 | acc: 50.00%,  total acc: 81.53%   [EVAL] batch:  155 | acc: 62.50%,  total acc: 81.41%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 81.49%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 81.57%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 81.64%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 81.72%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 81.79%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 81.91%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 82.02%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 82.13%   [EVAL] batch:  164 | acc: 93.75%,  total acc: 82.20%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 82.27%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 82.37%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 82.48%   [EVAL] batch:  168 | acc: 100.00%,  total acc: 82.58%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 82.57%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 82.57%   [EVAL] batch:  171 | acc: 81.25%,  total acc: 82.56%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 82.48%   [EVAL] batch:  173 | acc: 87.50%,  total acc: 82.51%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 82.57%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 82.39%   [EVAL] batch:  176 | acc: 75.00%,  total acc: 82.34%   [EVAL] batch:  177 | acc: 75.00%,  total acc: 82.30%   [EVAL] batch:  178 | acc: 81.25%,  total acc: 82.30%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 82.22%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 82.18%   [EVAL] batch:  181 | acc: 87.50%,  total acc: 82.21%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 82.21%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 82.27%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 82.23%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 82.26%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 82.25%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 82.05%   
cur_acc:  ['0.9514', '0.8065', '0.7708']
his_acc:  ['0.9514', '0.8765', '0.8205']
CurrentTrain: epoch  0, batch     0 | loss: 5.5364485CurrentTrain: epoch  0, batch     1 | loss: 5.5991502CurrentTrain: epoch  0, batch     2 | loss: 7.3877344CurrentTrain: epoch  0, batch     3 | loss: 9.2545958CurrentTrain: epoch  1, batch     0 | loss: 5.5004516CurrentTrain: epoch  1, batch     1 | loss: 5.0988264CurrentTrain: epoch  1, batch     2 | loss: 4.5913596CurrentTrain: epoch  1, batch     3 | loss: 6.5486526CurrentTrain: epoch  2, batch     0 | loss: 4.3021297CurrentTrain: epoch  2, batch     1 | loss: 4.4237795CurrentTrain: epoch  2, batch     2 | loss: 4.7663660CurrentTrain: epoch  2, batch     3 | loss: 4.5009933CurrentTrain: epoch  3, batch     0 | loss: 5.0688167CurrentTrain: epoch  3, batch     1 | loss: 3.9767663CurrentTrain: epoch  3, batch     2 | loss: 3.5664470CurrentTrain: epoch  3, batch     3 | loss: 2.2553411CurrentTrain: epoch  4, batch     0 | loss: 3.7224350CurrentTrain: epoch  4, batch     1 | loss: 4.3558121CurrentTrain: epoch  4, batch     2 | loss: 3.5307536CurrentTrain: epoch  4, batch     3 | loss: 3.3363354CurrentTrain: epoch  5, batch     0 | loss: 3.8192298CurrentTrain: epoch  5, batch     1 | loss: 3.6266932CurrentTrain: epoch  5, batch     2 | loss: 3.0324078CurrentTrain: epoch  5, batch     3 | loss: 3.0877745CurrentTrain: epoch  6, batch     0 | loss: 3.0614607CurrentTrain: epoch  6, batch     1 | loss: 3.3083324CurrentTrain: epoch  6, batch     2 | loss: 3.2502165CurrentTrain: epoch  6, batch     3 | loss: 2.0081000CurrentTrain: epoch  7, batch     0 | loss: 3.2511587CurrentTrain: epoch  7, batch     1 | loss: 3.1429715CurrentTrain: epoch  7, batch     2 | loss: 3.0639215CurrentTrain: epoch  7, batch     3 | loss: 4.8938527CurrentTrain: epoch  8, batch     0 | loss: 3.1464615CurrentTrain: epoch  8, batch     1 | loss: 2.8497396CurrentTrain: epoch  8, batch     2 | loss: 2.5356703CurrentTrain: epoch  8, batch     3 | loss: 3.0072155CurrentTrain: epoch  9, batch     0 | loss: 2.7652047CurrentTrain: epoch  9, batch     1 | loss: 2.7750692CurrentTrain: epoch  9, batch     2 | loss: 2.7286274CurrentTrain: epoch  9, batch     3 | loss: 1.8295219
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: ( 1988 ) , " night watch " ( 2004 ) , " day watch " ( 2006 ) and " legend № 17 " ( 2013 ) .
Head Entity: day watch
Tail Entity: night watch
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: ( 1995 ) , " The Phantom Menace " ( 1999 ) , " Attack of the Clones " ( 2002 ) and " Revenge of the Sith " ( 2005 ) .  
Head Entity: Attack of the Clones  
Tail Entity: The Phantom Menace  

Relation: follows  
Context: ( 2010 ) , " The Hunger Games " ( 2012 ) , " Catching Fire " ( 2013 ) and " Mockingjay " ( 2014 ) .  
Head Entity: Catching Fire  
Tail Entity: The Hunger Games  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: dick packer was a u.s. soccer center forward who was a member of the u.s. team at the 1956 summer olympics .
Head Entity: dick packer
Tail Entity: soccer
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: serena williams is a professional tennis player who has won numerous grand slam titles throughout her career.  
Head Entity: serena williams  
Tail Entity: tennis  

Relation: sport  
Context: lebron james is known for his exceptional skills in basketball and has played for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: basketball  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael was the father of laura, who had recently graduated from university.  
Head Entity: laura  
Tail Entity: michael  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: since 2009 , culshaw has starred in the bbc one comedy sketch show " the impressions show " alongside debra stephenson .
Head Entity: the impressions show
Tail Entity: bbc one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: The popular series "Breaking Bad" first aired on AMC, captivating audiences with its intense storytelling and character development.  
Head Entity: Breaking Bad  
Tail Entity: AMC  

Relation: original network  
Context: "Friends" became a cultural phenomenon after it premiered on NBC, showcasing the lives of six friends living in New York City.  
Head Entity: Friends  
Tail Entity: NBC  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: the 1954 film , directed by kadri venkata reddy , " peddamanushulu " was honoured with the first president 's silver medal for best feature film in telugu .
Head Entity: peddamanushulu
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: The 1994 animated film "The Lion King," produced by Walt Disney Feature Animation, was originally created in English.  
Head Entity: The Lion King  
Tail Entity: English  

Relation: original language of film or TV show  
Context: The critically acclaimed 2001 film "Amélie," directed by Jean-Pierre Jeunet, was originally filmed in French.  
Head Entity: Amélie  
Tail Entity: French  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: the league champions for the fifth time in their history ( and the second season running ) were taunton town , but runners - up mangotsfield united took promotion to the southern league .
Head Entity: mangotsfield united
Tail Entity: southern league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: After an impressive season, the team secured a spot in the prestigious league, where they will compete against top clubs, including their rivals, the city wanderers.  
Head Entity: city wanderers  
Tail Entity: prestigious league  

Relation: league  
Context: The young athlete showcased his skills in the national league, where he played alongside some of the best players in the country, including his teammate, the star striker.  
Head Entity: star striker  
Tail Entity: national league  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: other teammates included dominique leray and élisabeth riffiod , whose son boris diaw currently plays in the nba for the spurs ( 2014 ) .
Head Entity: boris diaw
Tail Entity: élisabeth riffiod
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: in a recent interview, actress jennifer garner spoke fondly of her children, including violet affleck, whom she describes as a wonderful daughter.  
Head Entity: violet affleck  
Tail Entity: jennifer garner  

Relation: mother  
Context: during the family reunion, it was heartwarming to see how much emma loved spending time with her mother, sarah connor, who always supported her dreams.  
Head Entity: emma  
Tail Entity: sarah connor  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres. he has dedicated his life to music and is known for his mastery of the cello.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: ploegsteert is a village in belgium located in the municipality of comines - warneton in the hainaut province and is the most westerly settlement of the walloon region .
Head Entity: hainaut
Tail Entity: belgium
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the city of prague is the capital of the czech republic and is known for its beautiful architecture and rich history.  
Head Entity: prague  
Tail Entity: czech republic  

Relation: country  
Context: the great barrier reef is located off the coast of australia and is the largest coral reef system in the world.  
Head Entity: great barrier reef  
Tail Entity: australia  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: ras thavas reappears later in the series to perform more mad science in the novel " synthetic men of mars " .
Head Entity: synthetic men of mars
Tail Entity: ras thavas
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: In the animated series "Avatar: The Last Airbender," Aang, the last Airbender, embarks on a journey to master all four elements.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: Aang  

Relation: characters  
Context: The novel "Pride and Prejudice" features Elizabeth Bennet as she navigates issues of class, marriage, and morality in early 19th century England.  
Head Entity: Pride and Prejudice  
Tail Entity: Elizabeth Bennet  
Mixup data size:  318
MixupTrain:  epoch  0, batch     0 | loss: 2.1398721MixupTrain:  epoch  0, batch     1 | loss: 2.7312673MixupTrain:  epoch  0, batch     2 | loss: 2.3569849MixupTrain:  epoch  0, batch     3 | loss: 2.4196044MixupTrain:  epoch  0, batch     4 | loss: 3.1704309MixupTrain:  epoch  0, batch     5 | loss: 2.4723976MixupTrain:  epoch  0, batch     6 | loss: 2.4544383MixupTrain:  epoch  0, batch     7 | loss: 2.7327761MixupTrain:  epoch  0, batch     8 | loss: 2.2643916MixupTrain:  epoch  0, batch     9 | loss: 2.5810040MixupTrain:  epoch  0, batch    10 | loss: 2.4908847MixupTrain:  epoch  0, batch    11 | loss: 2.8795660MixupTrain:  epoch  0, batch    12 | loss: 2.4757332MixupTrain:  epoch  0, batch    13 | loss: 2.4889985MixupTrain:  epoch  0, batch    14 | loss: 1.8921297MixupTrain:  epoch  0, batch    15 | loss: 2.3127188MixupTrain:  epoch  0, batch    16 | loss: 2.4085422MixupTrain:  epoch  0, batch    17 | loss: 2.2861605MixupTrain:  epoch  0, batch    18 | loss: 2.7994732MixupTrain:  epoch  0, batch    19 | loss: 2.2071364
MemoryTrain:  epoch  0, batch     0 | loss: 2.0847592MemoryTrain:  epoch  0, batch     1 | loss: 3.0038228MemoryTrain:  epoch  0, batch     2 | loss: 2.8525262MemoryTrain:  epoch  0, batch     3 | loss: 2.5052004MemoryTrain:  epoch  0, batch     4 | loss: 2.2190316MemoryTrain:  epoch  0, batch     5 | loss: 2.7458279MemoryTrain:  epoch  0, batch     6 | loss: 2.6622758MemoryTrain:  epoch  0, batch     7 | loss: 2.9286695MemoryTrain:  epoch  1, batch     0 | loss: 2.0398536MemoryTrain:  epoch  1, batch     1 | loss: 2.5133824MemoryTrain:  epoch  1, batch     2 | loss: 2.1809564MemoryTrain:  epoch  1, batch     3 | loss: 2.1609509MemoryTrain:  epoch  1, batch     4 | loss: 2.8095684MemoryTrain:  epoch  1, batch     5 | loss: 2.0679057MemoryTrain:  epoch  1, batch     6 | loss: 1.9705311MemoryTrain:  epoch  1, batch     7 | loss: 2.4351792MemoryTrain:  epoch  2, batch     0 | loss: 1.9086320MemoryTrain:  epoch  2, batch     1 | loss: 1.4938756MemoryTrain:  epoch  2, batch     2 | loss: 2.3239481MemoryTrain:  epoch  2, batch     3 | loss: 1.7915742MemoryTrain:  epoch  2, batch     4 | loss: 2.8753116MemoryTrain:  epoch  2, batch     5 | loss: 1.6108273MemoryTrain:  epoch  2, batch     6 | loss: 2.0261478MemoryTrain:  epoch  2, batch     7 | loss: 1.3768237MemoryTrain:  epoch  3, batch     0 | loss: 1.5030289MemoryTrain:  epoch  3, batch     1 | loss: 1.9071136MemoryTrain:  epoch  3, batch     2 | loss: 1.6797540MemoryTrain:  epoch  3, batch     3 | loss: 2.3416364MemoryTrain:  epoch  3, batch     4 | loss: 1.5020611MemoryTrain:  epoch  3, batch     5 | loss: 1.8635641MemoryTrain:  epoch  3, batch     6 | loss: 1.7868538MemoryTrain:  epoch  3, batch     7 | loss: 1.5201499MemoryTrain:  epoch  4, batch     0 | loss: 1.5591204MemoryTrain:  epoch  4, batch     1 | loss: 1.5900295MemoryTrain:  epoch  4, batch     2 | loss: 1.6471697MemoryTrain:  epoch  4, batch     3 | loss: 1.6251113MemoryTrain:  epoch  4, batch     4 | loss: 1.5056922MemoryTrain:  epoch  4, batch     5 | loss: 1.7831838MemoryTrain:  epoch  4, batch     6 | loss: 1.5273776MemoryTrain:  epoch  4, batch     7 | loss: 1.8160276MemoryTrain:  epoch  5, batch     0 | loss: 1.4606190MemoryTrain:  epoch  5, batch     1 | loss: 1.5945930MemoryTrain:  epoch  5, batch     2 | loss: 1.5153902MemoryTrain:  epoch  5, batch     3 | loss: 1.4764920MemoryTrain:  epoch  5, batch     4 | loss: 1.4444524MemoryTrain:  epoch  5, batch     5 | loss: 1.6463552MemoryTrain:  epoch  5, batch     6 | loss: 1.4659427MemoryTrain:  epoch  5, batch     7 | loss: 1.7104151MemoryTrain:  epoch  6, batch     0 | loss: 1.3940442MemoryTrain:  epoch  6, batch     1 | loss: 1.4582782MemoryTrain:  epoch  6, batch     2 | loss: 1.4602040MemoryTrain:  epoch  6, batch     3 | loss: 1.4834198MemoryTrain:  epoch  6, batch     4 | loss: 1.4417838MemoryTrain:  epoch  6, batch     5 | loss: 1.4500034MemoryTrain:  epoch  6, batch     6 | loss: 1.4680072MemoryTrain:  epoch  6, batch     7 | loss: 1.4114189MemoryTrain:  epoch  7, batch     0 | loss: 1.4414284MemoryTrain:  epoch  7, batch     1 | loss: 1.3189895MemoryTrain:  epoch  7, batch     2 | loss: 1.4268682MemoryTrain:  epoch  7, batch     3 | loss: 1.2919675MemoryTrain:  epoch  7, batch     4 | loss: 1.4289799MemoryTrain:  epoch  7, batch     5 | loss: 1.4320966MemoryTrain:  epoch  7, batch     6 | loss: 1.6114196MemoryTrain:  epoch  7, batch     7 | loss: 1.4165295MemoryTrain:  epoch  8, batch     0 | loss: 1.4974352MemoryTrain:  epoch  8, batch     1 | loss: 1.2448252MemoryTrain:  epoch  8, batch     2 | loss: 1.4846628MemoryTrain:  epoch  8, batch     3 | loss: 1.3512869MemoryTrain:  epoch  8, batch     4 | loss: 1.3618057MemoryTrain:  epoch  8, batch     5 | loss: 1.5437034MemoryTrain:  epoch  8, batch     6 | loss: 1.3717935MemoryTrain:  epoch  8, batch     7 | loss: 1.2337792MemoryTrain:  epoch  9, batch     0 | loss: 1.3059485MemoryTrain:  epoch  9, batch     1 | loss: 1.4640869MemoryTrain:  epoch  9, batch     2 | loss: 1.3662388MemoryTrain:  epoch  9, batch     3 | loss: 1.3842927MemoryTrain:  epoch  9, batch     4 | loss: 1.3777580MemoryTrain:  epoch  9, batch     5 | loss: 1.3203089MemoryTrain:  epoch  9, batch     6 | loss: 1.2503132MemoryTrain:  epoch  9, batch     7 | loss: 1.4543197
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 59.38%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 58.93%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 60.94%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 63.89%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 66.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 67.61%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 69.27%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 69.71%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 69.20%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 68.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 67.58%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 66.91%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 66.78%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 68.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 69.94%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 71.02%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 72.28%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 74.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 75.93%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 76.56%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 77.37%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 77.71%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 78.02%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 78.71%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 79.36%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 79.78%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 80.18%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 80.56%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 81.08%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 80.92%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 80.13%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 79.53%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 78.81%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 78.27%   [EVAL] batch:   42 | acc: 43.75%,  total acc: 77.47%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 77.70%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 78.19%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 78.40%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 78.86%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 79.30%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 79.59%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 79.88%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 79.53%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 79.33%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 79.13%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 79.05%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 79.09%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 78.57%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 78.18%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 78.02%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 77.75%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 77.92%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 78.18%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 78.23%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 77.58%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 77.78%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 75.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 76.14%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 75.52%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 75.48%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 76.79%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 77.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 79.30%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 80.15%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 80.90%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 81.91%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 81.88%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 82.44%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 82.67%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 82.88%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 83.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.65%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.26%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 84.60%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 84.91%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.89%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.33%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.74%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 87.13%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 87.32%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 87.67%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.01%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.32%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.62%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.91%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.18%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 89.43%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 89.53%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 89.63%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 89.72%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 89.54%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 88.96%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 88.93%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 89.03%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 89.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.22%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.30%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 89.39%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 89.55%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.62%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 89.47%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 88.90%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 88.67%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 88.65%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 88.52%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 88.21%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 87.70%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 87.11%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 86.83%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 86.46%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 85.63%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 85.20%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 85.05%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 85.18%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 85.12%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 84.81%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 84.93%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 84.80%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 84.75%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 84.70%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 84.74%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 84.62%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 84.41%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 84.49%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 83.84%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 83.21%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 82.81%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 82.06%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 81.40%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 80.89%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 80.54%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 80.76%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 80.97%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 81.11%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 81.32%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 81.45%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 81.58%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 81.78%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 81.97%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 82.15%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 82.33%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 82.51%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 82.69%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 82.80%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 82.90%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 82.83%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 82.87%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 83.04%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 83.14%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 82.77%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 82.29%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 82.00%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 81.48%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 80.69%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 80.48%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 80.54%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 80.71%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 80.77%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 80.82%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 80.88%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 80.88%   [EVAL] batch:  119 | acc: 18.75%,  total acc: 80.36%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 79.80%   [EVAL] batch:  121 | acc: 6.25%,  total acc: 79.20%   [EVAL] batch:  122 | acc: 31.25%,  total acc: 78.81%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 78.23%   [EVAL] batch:  124 | acc: 25.00%,  total acc: 77.80%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 77.73%   [EVAL] batch:  126 | acc: 62.50%,  total acc: 77.61%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 77.49%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 77.37%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 77.31%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 77.19%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 77.32%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 77.49%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 77.47%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 77.55%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 77.71%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 77.87%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 77.67%   [EVAL] batch:  138 | acc: 6.25%,  total acc: 77.16%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 76.70%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 76.24%   [EVAL] batch:  141 | acc: 12.50%,  total acc: 75.79%   [EVAL] batch:  142 | acc: 6.25%,  total acc: 75.31%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 74.96%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 75.13%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 75.30%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 75.43%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 75.59%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 75.76%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 75.88%   [EVAL] batch:  150 | acc: 62.50%,  total acc: 75.79%   [EVAL] batch:  151 | acc: 31.25%,  total acc: 75.49%   [EVAL] batch:  152 | acc: 68.75%,  total acc: 75.45%   [EVAL] batch:  153 | acc: 81.25%,  total acc: 75.49%   [EVAL] batch:  154 | acc: 50.00%,  total acc: 75.32%   [EVAL] batch:  155 | acc: 62.50%,  total acc: 75.24%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 75.36%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 75.47%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 75.59%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 75.70%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 75.78%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 75.93%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 76.07%   [EVAL] batch:  163 | acc: 81.25%,  total acc: 76.11%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 76.06%   [EVAL] batch:  165 | acc: 87.50%,  total acc: 76.13%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 76.20%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 76.30%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 76.26%   [EVAL] batch:  169 | acc: 12.50%,  total acc: 75.88%   [EVAL] batch:  170 | acc: 43.75%,  total acc: 75.69%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 75.36%   [EVAL] batch:  172 | acc: 18.75%,  total acc: 75.04%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 74.89%   [EVAL] batch:  174 | acc: 31.25%,  total acc: 74.64%   [EVAL] batch:  175 | acc: 43.75%,  total acc: 74.47%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 74.44%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 74.37%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 74.37%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 74.34%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 74.27%   [EVAL] batch:  181 | acc: 87.50%,  total acc: 74.35%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 74.42%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 74.52%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 74.53%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 74.60%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 74.63%   [EVAL] batch:  187 | acc: 87.50%,  total acc: 74.70%   [EVAL] batch:  188 | acc: 50.00%,  total acc: 74.57%   [EVAL] batch:  189 | acc: 81.25%,  total acc: 74.61%   [EVAL] batch:  190 | acc: 50.00%,  total acc: 74.48%   [EVAL] batch:  191 | acc: 56.25%,  total acc: 74.38%   [EVAL] batch:  192 | acc: 62.50%,  total acc: 74.32%   [EVAL] batch:  193 | acc: 50.00%,  total acc: 74.19%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 74.20%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 74.20%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 74.24%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 74.31%   [EVAL] batch:  198 | acc: 87.50%,  total acc: 74.37%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 74.38%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 74.38%   [EVAL] batch:  201 | acc: 50.00%,  total acc: 74.26%   [EVAL] batch:  202 | acc: 50.00%,  total acc: 74.14%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 74.11%   [EVAL] batch:  204 | acc: 50.00%,  total acc: 73.99%   [EVAL] batch:  205 | acc: 62.50%,  total acc: 73.94%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 74.03%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 74.16%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 74.28%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 74.38%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 74.50%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 74.62%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 74.71%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 74.80%   [EVAL] batch:  214 | acc: 93.75%,  total acc: 74.88%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  216 | acc: 87.50%,  total acc: 75.06%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 75.17%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 75.23%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 75.34%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 75.45%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 75.51%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 75.59%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 75.70%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 75.81%   [EVAL] batch:  225 | acc: 43.75%,  total acc: 75.66%   [EVAL] batch:  226 | acc: 56.25%,  total acc: 75.58%   [EVAL] batch:  227 | acc: 50.00%,  total acc: 75.47%   [EVAL] batch:  228 | acc: 68.75%,  total acc: 75.44%   [EVAL] batch:  229 | acc: 37.50%,  total acc: 75.27%   [EVAL] batch:  230 | acc: 68.75%,  total acc: 75.24%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 75.32%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 75.40%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 75.48%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 75.59%   [EVAL] batch:  235 | acc: 93.75%,  total acc: 75.66%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 75.74%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 75.71%   [EVAL] batch:  238 | acc: 75.00%,  total acc: 75.71%   [EVAL] batch:  239 | acc: 75.00%,  total acc: 75.70%   [EVAL] batch:  240 | acc: 68.75%,  total acc: 75.67%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 75.70%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 75.64%   [EVAL] batch:  243 | acc: 50.00%,  total acc: 75.54%   [EVAL] batch:  244 | acc: 81.25%,  total acc: 75.56%   [EVAL] batch:  245 | acc: 56.25%,  total acc: 75.48%   [EVAL] batch:  246 | acc: 68.75%,  total acc: 75.46%   [EVAL] batch:  247 | acc: 87.50%,  total acc: 75.50%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 75.60%   [EVAL] batch:  249 | acc: 68.75%,  total acc: 75.58%   
cur_acc:  ['0.9514', '0.8065', '0.7708', '0.7758']
his_acc:  ['0.9514', '0.8765', '0.8205', '0.7558']
CurrentTrain: epoch  0, batch     0 | loss: 6.6900129CurrentTrain: epoch  0, batch     1 | loss: 7.1123805CurrentTrain: epoch  0, batch     2 | loss: 5.5485950CurrentTrain: epoch  0, batch     3 | loss: 7.7155671CurrentTrain: epoch  1, batch     0 | loss: 6.5946121CurrentTrain: epoch  1, batch     1 | loss: 5.7762508CurrentTrain: epoch  1, batch     2 | loss: 5.2830591CurrentTrain: epoch  1, batch     3 | loss: 6.2753830CurrentTrain: epoch  2, batch     0 | loss: 5.1420822CurrentTrain: epoch  2, batch     1 | loss: 5.6568394CurrentTrain: epoch  2, batch     2 | loss: 5.1093235CurrentTrain: epoch  2, batch     3 | loss: 6.7006450CurrentTrain: epoch  3, batch     0 | loss: 4.9595213CurrentTrain: epoch  3, batch     1 | loss: 4.7122984CurrentTrain: epoch  3, batch     2 | loss: 4.9086308CurrentTrain: epoch  3, batch     3 | loss: 5.3220348CurrentTrain: epoch  4, batch     0 | loss: 3.9388595CurrentTrain: epoch  4, batch     1 | loss: 4.5893402CurrentTrain: epoch  4, batch     2 | loss: 4.4998407CurrentTrain: epoch  4, batch     3 | loss: 5.9300795CurrentTrain: epoch  5, batch     0 | loss: 4.6840749CurrentTrain: epoch  5, batch     1 | loss: 3.9176979CurrentTrain: epoch  5, batch     2 | loss: 3.5239162CurrentTrain: epoch  5, batch     3 | loss: 2.8522367CurrentTrain: epoch  6, batch     0 | loss: 5.0012598CurrentTrain: epoch  6, batch     1 | loss: 3.2150517CurrentTrain: epoch  6, batch     2 | loss: 3.5540695CurrentTrain: epoch  6, batch     3 | loss: 4.7266321CurrentTrain: epoch  7, batch     0 | loss: 4.0616374CurrentTrain: epoch  7, batch     1 | loss: 3.0064912CurrentTrain: epoch  7, batch     2 | loss: 4.3700533CurrentTrain: epoch  7, batch     3 | loss: 2.8525295CurrentTrain: epoch  8, batch     0 | loss: 3.4135418CurrentTrain: epoch  8, batch     1 | loss: 3.8543973CurrentTrain: epoch  8, batch     2 | loss: 3.6683855CurrentTrain: epoch  8, batch     3 | loss: 3.2602274CurrentTrain: epoch  9, batch     0 | loss: 3.5630684CurrentTrain: epoch  9, batch     1 | loss: 3.4934235CurrentTrain: epoch  9, batch     2 | loss: 3.3353872CurrentTrain: epoch  9, batch     3 | loss: 3.2020378
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: the andesite line , a zone of intense volcanic and seismic activity , is a major regional distinction in the pacific .
Head Entity: andesite line
Tail Entity: pacific
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The Great Barrier Reef, a stunning natural wonder, stretches along the northeastern coast of Australia.  
Head Entity: Great Barrier Reef  
Tail Entity: Australia  

Relation: located on terrain feature  
Context: Mount Everest, the highest peak in the world, is part of the Himalayas and attracts climbers from around the globe.  
Head Entity: Mount Everest  
Tail Entity: Himalayas  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, also known as cop26, took place in glasgow, scotland, where numerous world leaders gathered to discuss climate action.  
Head Entity: united nations climate change conference  
Tail Entity: cop26  

Relation: participant of  
Context: the world health organization organized a global summit on health equity, bringing together experts and leaders from various countries to address health disparities.  
Head Entity: world health organization  
Tail Entity: global summit on health equity  
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: luna subsequently had a supporting role in the comedy film " gravy " , directed by james roday , which was released in october 2015 .
Head Entity: gravy
Tail Entity: james roday
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: the acclaimed director christopher nolan is known for his work on the blockbuster film "inception," which was released in 2010.  
Head Entity: inception  
Tail Entity: christopher nolan  

Relation: director  
Context: in the latest season of the popular series "the crown," the talented director peter morgan brought a fresh perspective to the royal drama.  
Head Entity: the crown  
Tail Entity: peter morgan  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: as of april 2013 , sony isp , known as so - net , released a new fibre service to tokyo for small businesses and homes and also made it available to six surrounding prefectures .
Head Entity: so - net
Tail Entity: sony
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: In 2020, the popular video game developer, Bungie, announced that it would be self-publishing its games after separating from Activision, which had previously owned the rights to the Destiny franchise.  
Head Entity: Bungie  
Tail Entity: Activision  

Relation: owned by  
Context: The luxury fashion brand Gucci, known for its high-end products, is a subsidiary of the Kering Group, which owns several other prestigious labels in the fashion industry.  
Head Entity: Gucci  
Tail Entity: Kering Group  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: remains of taxa that can be confidently assigned to istiodactylidae have been found in the uk and china , in rocks dating from the early cretaceous period ( barremian to aptian stage ) .
Head Entity: barremian
Tail Entity: early cretaceous
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The Great Wall of China is a series of fortifications made of various materials, and it is part of the cultural heritage of the country.  
Head Entity: Great Wall of China  
Tail Entity: cultural heritage  

Relation: part of  
Context: The human heart is a vital organ that is part of the circulatory system, responsible for pumping blood throughout the body.  
Head Entity: human heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: the brewery closed in 1996 and is now the site of the halliwell jones stadium , home of warrington wolves , as well as a tesco store .
Head Entity: halliwell jones stadium
Tail Entity: warrington wolves
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was transformed into a cultural center, which now hosts the local art gallery and serves as the headquarters for the community theater group.  
Head Entity: cultural center  
Tail Entity: community theater group  

Relation: occupant  
Context: After the renovation, the old factory became a vibrant co-working space, attracting startups and freelancers from various industries.  
Head Entity: co-working space  
Tail Entity: startups
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: reading town hall was built in several phases between 1786 and 1897 , although the principal facade was designed by alfred waterhouse in 1875 .
Head Entity: reading town hall
Tail Entity: alfred waterhouse
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: the new art museum was designed by renowned architect zaha hadid, whose innovative style has transformed modern architecture.  
Head Entity: new art museum  
Tail Entity: zaha hadid  

Relation: architect  
Context: the iconic sydney opera house was the brainchild of architect jørn utzon, who won the design competition in 1957.  
Head Entity: sydney opera house  
Tail Entity: jørn utzon  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: cyrus alexander was born in pennsylvania , and his family soon moved to illinois .
Head Entity: cyrus alexander
Tail Entity: illinois
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: after living in new york for several years, emily decided to relocate to california for a change of scenery.  
Head Entity: emily  
Tail Entity: california  

Relation: residence  
Context: during his childhood, michael spent most of his time in texas before moving to florida for college.  
Head Entity: michael  
Tail Entity: florida  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Ludwig van Beethoven created some of the most celebrated symphonies in classical music history.  
Head Entity: symphonies  
Tail Entity: Ludwig van Beethoven  

Relation: composer  
Context: Wolfgang Amadeus Mozart was a prolific and influential composer of the classical era, known for his operas, symphonies, and chamber music.  
Head Entity: operas  
Tail Entity: Wolfgang Amadeus Mozart  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: in 1972 , terrorists from the japanese red army launched an attack that led to the deaths of at least 24 people at ben gurion airport .
Head Entity: attack
Tail Entity: ben gurion airport
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: The annual music festival will take place in the picturesque town of Coachella, attracting thousands of fans from around the world.  
Head Entity: music festival  
Tail Entity: Coachella  

Relation: location  
Context: The historic battle of Gettysburg was fought in Pennsylvania and is considered a turning point in the American Civil War.  
Head Entity: battle  
Tail Entity: Pennsylvania  
Mixup data size:  379
MixupTrain:  epoch  0, batch     0 | loss: 2.6164724MixupTrain:  epoch  0, batch     1 | loss: 3.1496890MixupTrain:  epoch  0, batch     2 | loss: 2.4744943MixupTrain:  epoch  0, batch     3 | loss: 2.7642310MixupTrain:  epoch  0, batch     4 | loss: 2.5763607MixupTrain:  epoch  0, batch     5 | loss: 2.3008055MixupTrain:  epoch  0, batch     6 | loss: 2.6377422MixupTrain:  epoch  0, batch     7 | loss: 2.1799513MixupTrain:  epoch  0, batch     8 | loss: 2.3767888MixupTrain:  epoch  0, batch     9 | loss: 2.5900536MixupTrain:  epoch  0, batch    10 | loss: 2.2655176MixupTrain:  epoch  0, batch    11 | loss: 2.7376337MixupTrain:  epoch  0, batch    12 | loss: 2.5243069MixupTrain:  epoch  0, batch    13 | loss: 2.7106363MixupTrain:  epoch  0, batch    14 | loss: 2.7994383MixupTrain:  epoch  0, batch    15 | loss: 2.2737339MixupTrain:  epoch  0, batch    16 | loss: 2.7039923MixupTrain:  epoch  0, batch    17 | loss: 2.1970995MixupTrain:  epoch  0, batch    18 | loss: 2.0204225MixupTrain:  epoch  0, batch    19 | loss: 2.5379628MixupTrain:  epoch  0, batch    20 | loss: 3.0374971MixupTrain:  epoch  0, batch    21 | loss: 2.9201959MixupTrain:  epoch  0, batch    22 | loss: 2.3416537MixupTrain:  epoch  0, batch    23 | loss: 2.1462214
MemoryTrain:  epoch  0, batch     0 | loss: 2.3911247MemoryTrain:  epoch  0, batch     1 | loss: 1.9444404MemoryTrain:  epoch  0, batch     2 | loss: 2.3287587MemoryTrain:  epoch  0, batch     3 | loss: 2.5789049MemoryTrain:  epoch  0, batch     4 | loss: 2.8402555MemoryTrain:  epoch  0, batch     5 | loss: 2.1819830MemoryTrain:  epoch  0, batch     6 | loss: 3.8737853MemoryTrain:  epoch  0, batch     7 | loss: 3.0835447MemoryTrain:  epoch  0, batch     8 | loss: 3.4303277MemoryTrain:  epoch  0, batch     9 | loss: 1.9642382MemoryTrain:  epoch  1, batch     0 | loss: 2.0491028MemoryTrain:  epoch  1, batch     1 | loss: 3.1564035MemoryTrain:  epoch  1, batch     2 | loss: 2.2818742MemoryTrain:  epoch  1, batch     3 | loss: 2.1750722MemoryTrain:  epoch  1, batch     4 | loss: 2.6665723MemoryTrain:  epoch  1, batch     5 | loss: 2.3591161MemoryTrain:  epoch  1, batch     6 | loss: 2.5937061MemoryTrain:  epoch  1, batch     7 | loss: 1.7389438MemoryTrain:  epoch  1, batch     8 | loss: 1.5362996MemoryTrain:  epoch  1, batch     9 | loss: 3.5352569MemoryTrain:  epoch  2, batch     0 | loss: 2.3681431MemoryTrain:  epoch  2, batch     1 | loss: 2.0832481MemoryTrain:  epoch  2, batch     2 | loss: 1.6934721MemoryTrain:  epoch  2, batch     3 | loss: 2.4365106MemoryTrain:  epoch  2, batch     4 | loss: 2.1018834MemoryTrain:  epoch  2, batch     5 | loss: 2.7118981MemoryTrain:  epoch  2, batch     6 | loss: 1.6119187MemoryTrain:  epoch  2, batch     7 | loss: 2.3842392MemoryTrain:  epoch  2, batch     8 | loss: 1.6287843MemoryTrain:  epoch  2, batch     9 | loss: 2.5807543MemoryTrain:  epoch  3, batch     0 | loss: 2.3316321MemoryTrain:  epoch  3, batch     1 | loss: 2.2606006MemoryTrain:  epoch  3, batch     2 | loss: 1.4437802MemoryTrain:  epoch  3, batch     3 | loss: 1.9827962MemoryTrain:  epoch  3, batch     4 | loss: 2.0914912MemoryTrain:  epoch  3, batch     5 | loss: 2.4444647MemoryTrain:  epoch  3, batch     6 | loss: 1.6385840MemoryTrain:  epoch  3, batch     7 | loss: 1.7698613MemoryTrain:  epoch  3, batch     8 | loss: 1.6814439MemoryTrain:  epoch  3, batch     9 | loss: 1.5507582MemoryTrain:  epoch  4, batch     0 | loss: 1.7457339MemoryTrain:  epoch  4, batch     1 | loss: 2.1220443MemoryTrain:  epoch  4, batch     2 | loss: 1.8509175MemoryTrain:  epoch  4, batch     3 | loss: 1.9203464MemoryTrain:  epoch  4, batch     4 | loss: 1.6023853MemoryTrain:  epoch  4, batch     5 | loss: 1.4685612MemoryTrain:  epoch  4, batch     6 | loss: 1.3844548MemoryTrain:  epoch  4, batch     7 | loss: 2.1746218MemoryTrain:  epoch  4, batch     8 | loss: 1.6504116MemoryTrain:  epoch  4, batch     9 | loss: 1.4837224MemoryTrain:  epoch  5, batch     0 | loss: 1.7630666MemoryTrain:  epoch  5, batch     1 | loss: 1.6841280MemoryTrain:  epoch  5, batch     2 | loss: 2.2317085MemoryTrain:  epoch  5, batch     3 | loss: 1.3783194MemoryTrain:  epoch  5, batch     4 | loss: 1.9481657MemoryTrain:  epoch  5, batch     5 | loss: 1.3846284MemoryTrain:  epoch  5, batch     6 | loss: 1.6999484MemoryTrain:  epoch  5, batch     7 | loss: 1.6392052MemoryTrain:  epoch  5, batch     8 | loss: 1.9482752MemoryTrain:  epoch  5, batch     9 | loss: 1.3118503MemoryTrain:  epoch  6, batch     0 | loss: 1.6979146MemoryTrain:  epoch  6, batch     1 | loss: 1.7550094MemoryTrain:  epoch  6, batch     2 | loss: 1.6731744MemoryTrain:  epoch  6, batch     3 | loss: 1.6622179MemoryTrain:  epoch  6, batch     4 | loss: 1.5495455MemoryTrain:  epoch  6, batch     5 | loss: 1.3846422MemoryTrain:  epoch  6, batch     6 | loss: 1.3780031MemoryTrain:  epoch  6, batch     7 | loss: 1.5731988MemoryTrain:  epoch  6, batch     8 | loss: 1.9158384MemoryTrain:  epoch  6, batch     9 | loss: 2.0815787MemoryTrain:  epoch  7, batch     0 | loss: 1.6305988MemoryTrain:  epoch  7, batch     1 | loss: 1.7125446MemoryTrain:  epoch  7, batch     2 | loss: 1.2388767MemoryTrain:  epoch  7, batch     3 | loss: 1.4836600MemoryTrain:  epoch  7, batch     4 | loss: 1.7794738MemoryTrain:  epoch  7, batch     5 | loss: 1.3759584MemoryTrain:  epoch  7, batch     6 | loss: 1.2650986MemoryTrain:  epoch  7, batch     7 | loss: 1.5924811MemoryTrain:  epoch  7, batch     8 | loss: 1.5969844MemoryTrain:  epoch  7, batch     9 | loss: 1.5495100MemoryTrain:  epoch  8, batch     0 | loss: 1.7406404MemoryTrain:  epoch  8, batch     1 | loss: 1.2716894MemoryTrain:  epoch  8, batch     2 | loss: 1.6048427MemoryTrain:  epoch  8, batch     3 | loss: 1.5866253MemoryTrain:  epoch  8, batch     4 | loss: 1.4809264MemoryTrain:  epoch  8, batch     5 | loss: 1.4326268MemoryTrain:  epoch  8, batch     6 | loss: 1.6008273MemoryTrain:  epoch  8, batch     7 | loss: 1.4559846MemoryTrain:  epoch  8, batch     8 | loss: 1.3056548MemoryTrain:  epoch  8, batch     9 | loss: 1.6142476MemoryTrain:  epoch  9, batch     0 | loss: 1.3873188MemoryTrain:  epoch  9, batch     1 | loss: 1.5076770MemoryTrain:  epoch  9, batch     2 | loss: 1.2870408MemoryTrain:  epoch  9, batch     3 | loss: 1.8933873MemoryTrain:  epoch  9, batch     4 | loss: 1.3406460MemoryTrain:  epoch  9, batch     5 | loss: 1.2488745MemoryTrain:  epoch  9, batch     6 | loss: 1.3449724MemoryTrain:  epoch  9, batch     7 | loss: 1.4585149MemoryTrain:  epoch  9, batch     8 | loss: 1.4359127MemoryTrain:  epoch  9, batch     9 | loss: 1.2306408
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 20.83%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 20.31%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 17.50%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 16.67%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 24.11%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 32.03%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 39.58%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 45.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 50.00%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 52.60%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 54.81%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 55.80%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 57.50%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 58.98%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 61.03%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 62.15%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   20 | acc: 56.25%,  total acc: 62.20%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 62.77%   [EVAL] batch:   23 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 63.00%   [EVAL] batch:   25 | acc: 12.50%,  total acc: 61.06%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 59.26%   [EVAL] batch:   27 | acc: 12.50%,  total acc: 57.59%   [EVAL] batch:   28 | acc: 12.50%,  total acc: 56.03%   [EVAL] batch:   29 | acc: 18.75%,  total acc: 54.79%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 53.02%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 53.32%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 54.55%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 55.70%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 56.96%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 57.64%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 58.45%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 59.21%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 59.94%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 60.94%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 61.74%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 61.90%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 62.79%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 63.35%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 63.47%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 63.45%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 63.56%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 63.67%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 64.03%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 63.88%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 63.48%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 63.46%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 63.80%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 63.89%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 63.98%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 64.17%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 63.93%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 63.69%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 63.24%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 63.44%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 63.32%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 63.61%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 63.10%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 73.75%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 73.86%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 72.40%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 72.60%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 74.11%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 75.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 76.95%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 77.94%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 78.82%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 79.93%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 79.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 80.40%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 80.71%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 80.73%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 81.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.73%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 82.41%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 83.19%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.27%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.77%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 85.66%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 86.07%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.82%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.17%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.11%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 88.39%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 88.37%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 88.49%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 88.19%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 88.04%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 87.50%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 87.24%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 87.37%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 87.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 87.38%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 87.62%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 87.73%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 87.73%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 87.83%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 86.75%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 86.33%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 86.07%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 85.69%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 85.22%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 84.96%   [EVAL] batch:   64 | acc: 50.00%,  total acc: 84.42%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 84.09%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 83.40%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 83.27%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 82.97%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 83.04%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 82.92%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 82.64%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 82.79%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 82.69%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 82.67%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 82.65%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 82.39%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 82.13%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 81.88%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 81.80%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 81.87%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 81.25%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 80.72%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 80.43%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 79.85%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 79.36%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 78.88%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 78.55%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 78.96%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 79.05%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 79.28%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 79.44%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 79.59%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 79.80%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 80.01%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 80.22%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 80.42%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 80.62%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 80.81%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 80.94%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 81.07%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 81.07%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 81.13%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 81.31%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 81.43%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 81.13%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 80.61%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 80.16%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 79.55%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 79.34%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 78.79%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 78.60%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 78.67%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 78.86%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 78.93%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 79.01%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 79.08%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 79.10%   [EVAL] batch:  119 | acc: 18.75%,  total acc: 78.59%   [EVAL] batch:  120 | acc: 18.75%,  total acc: 78.10%   [EVAL] batch:  121 | acc: 6.25%,  total acc: 77.51%   [EVAL] batch:  122 | acc: 31.25%,  total acc: 77.13%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 76.56%   [EVAL] batch:  124 | acc: 25.00%,  total acc: 76.15%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 76.09%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 75.94%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 75.83%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 75.73%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 75.72%   [EVAL] batch:  130 | acc: 56.25%,  total acc: 75.57%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 75.66%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 75.80%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 75.84%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 75.93%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 76.10%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 76.28%   [EVAL] batch:  137 | acc: 43.75%,  total acc: 76.04%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 75.58%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 75.09%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 74.60%   [EVAL] batch:  141 | acc: 6.25%,  total acc: 74.12%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 73.60%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 73.31%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 73.49%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 73.67%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 73.81%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 73.99%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 74.16%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 74.33%   [EVAL] batch:  150 | acc: 37.50%,  total acc: 74.09%   [EVAL] batch:  151 | acc: 31.25%,  total acc: 73.81%   [EVAL] batch:  152 | acc: 50.00%,  total acc: 73.65%   [EVAL] batch:  153 | acc: 62.50%,  total acc: 73.58%   [EVAL] batch:  154 | acc: 37.50%,  total acc: 73.35%   [EVAL] batch:  155 | acc: 56.25%,  total acc: 73.24%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 73.29%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 73.42%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 73.55%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 73.67%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 73.76%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 73.88%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 73.73%   [EVAL] batch:  163 | acc: 18.75%,  total acc: 73.40%   [EVAL] batch:  164 | acc: 12.50%,  total acc: 73.03%   [EVAL] batch:  165 | acc: 31.25%,  total acc: 72.78%   [EVAL] batch:  166 | acc: 18.75%,  total acc: 72.46%   [EVAL] batch:  167 | acc: 37.50%,  total acc: 72.25%   [EVAL] batch:  168 | acc: 31.25%,  total acc: 72.00%   [EVAL] batch:  169 | acc: 12.50%,  total acc: 71.65%   [EVAL] batch:  170 | acc: 43.75%,  total acc: 71.49%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 71.18%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 70.95%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 70.83%   [EVAL] batch:  174 | acc: 25.00%,  total acc: 70.57%   [EVAL] batch:  175 | acc: 43.75%,  total acc: 70.42%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 70.37%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 70.33%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 70.36%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 70.38%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 70.37%   [EVAL] batch:  181 | acc: 81.25%,  total acc: 70.43%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 70.53%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 70.65%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 70.74%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 70.83%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 70.89%   [EVAL] batch:  187 | acc: 87.50%,  total acc: 70.98%   [EVAL] batch:  188 | acc: 62.50%,  total acc: 70.93%   [EVAL] batch:  189 | acc: 81.25%,  total acc: 70.99%   [EVAL] batch:  190 | acc: 62.50%,  total acc: 70.94%   [EVAL] batch:  191 | acc: 56.25%,  total acc: 70.87%   [EVAL] batch:  192 | acc: 75.00%,  total acc: 70.89%   [EVAL] batch:  193 | acc: 50.00%,  total acc: 70.78%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 70.80%   [EVAL] batch:  195 | acc: 81.25%,  total acc: 70.85%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 70.91%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 70.96%   [EVAL] batch:  198 | acc: 81.25%,  total acc: 71.01%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 71.03%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 71.05%   [EVAL] batch:  201 | acc: 62.50%,  total acc: 71.01%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 70.94%   [EVAL] batch:  203 | acc: 75.00%,  total acc: 70.96%   [EVAL] batch:  204 | acc: 56.25%,  total acc: 70.88%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 70.87%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 70.95%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 71.23%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 71.34%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 71.48%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 71.61%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 71.71%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 71.82%   [EVAL] batch:  214 | acc: 87.50%,  total acc: 71.89%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:  216 | acc: 87.50%,  total acc: 72.09%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 72.22%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 72.32%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 72.44%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 72.57%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 72.64%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 72.73%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 72.82%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 72.94%   [EVAL] batch:  225 | acc: 37.50%,  total acc: 72.79%   [EVAL] batch:  226 | acc: 56.25%,  total acc: 72.71%   [EVAL] batch:  227 | acc: 43.75%,  total acc: 72.59%   [EVAL] batch:  228 | acc: 62.50%,  total acc: 72.54%   [EVAL] batch:  229 | acc: 37.50%,  total acc: 72.39%   [EVAL] batch:  230 | acc: 56.25%,  total acc: 72.32%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 72.41%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 72.51%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 72.62%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 72.74%   [EVAL] batch:  235 | acc: 87.50%,  total acc: 72.80%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 72.93%   [EVAL] batch:  238 | acc: 62.50%,  total acc: 72.88%   [EVAL] batch:  239 | acc: 68.75%,  total acc: 72.86%   [EVAL] batch:  240 | acc: 62.50%,  total acc: 72.82%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 72.83%   [EVAL] batch:  242 | acc: 56.25%,  total acc: 72.76%   [EVAL] batch:  243 | acc: 37.50%,  total acc: 72.62%   [EVAL] batch:  244 | acc: 68.75%,  total acc: 72.60%   [EVAL] batch:  245 | acc: 50.00%,  total acc: 72.51%   [EVAL] batch:  246 | acc: 56.25%,  total acc: 72.44%   [EVAL] batch:  247 | acc: 75.00%,  total acc: 72.45%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 72.52%   [EVAL] batch:  249 | acc: 56.25%,  total acc: 72.45%   [EVAL] batch:  250 | acc: 18.75%,  total acc: 72.24%   [EVAL] batch:  251 | acc: 25.00%,  total acc: 72.05%   [EVAL] batch:  252 | acc: 18.75%,  total acc: 71.84%   [EVAL] batch:  253 | acc: 18.75%,  total acc: 71.63%   [EVAL] batch:  254 | acc: 6.25%,  total acc: 71.37%   [EVAL] batch:  255 | acc: 12.50%,  total acc: 71.14%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 71.13%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 71.20%   [EVAL] batch:  258 | acc: 100.00%,  total acc: 71.31%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 71.39%   [EVAL] batch:  260 | acc: 100.00%,  total acc: 71.50%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 71.54%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 71.58%   [EVAL] batch:  263 | acc: 68.75%,  total acc: 71.57%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 71.60%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 71.64%   [EVAL] batch:  266 | acc: 93.75%,  total acc: 71.72%   [EVAL] batch:  267 | acc: 81.25%,  total acc: 71.76%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 71.75%   [EVAL] batch:  269 | acc: 62.50%,  total acc: 71.71%   [EVAL] batch:  270 | acc: 56.25%,  total acc: 71.66%   [EVAL] batch:  271 | acc: 68.75%,  total acc: 71.65%   [EVAL] batch:  272 | acc: 68.75%,  total acc: 71.63%   [EVAL] batch:  273 | acc: 56.25%,  total acc: 71.58%   [EVAL] batch:  274 | acc: 75.00%,  total acc: 71.59%   [EVAL] batch:  275 | acc: 12.50%,  total acc: 71.38%   [EVAL] batch:  276 | acc: 12.50%,  total acc: 71.16%   [EVAL] batch:  277 | acc: 12.50%,  total acc: 70.95%   [EVAL] batch:  278 | acc: 12.50%,  total acc: 70.74%   [EVAL] batch:  279 | acc: 18.75%,  total acc: 70.56%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 70.31%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 70.28%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 70.36%   [EVAL] batch:  283 | acc: 93.75%,  total acc: 70.44%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 70.55%   [EVAL] batch:  285 | acc: 81.25%,  total acc: 70.59%   [EVAL] batch:  286 | acc: 87.50%,  total acc: 70.64%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 70.70%   [EVAL] batch:  288 | acc: 87.50%,  total acc: 70.76%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 70.86%   [EVAL] batch:  290 | acc: 93.75%,  total acc: 70.94%   [EVAL] batch:  291 | acc: 68.75%,  total acc: 70.93%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 71.03%   [EVAL] batch:  293 | acc: 87.50%,  total acc: 71.09%   [EVAL] batch:  294 | acc: 68.75%,  total acc: 71.08%   [EVAL] batch:  295 | acc: 62.50%,  total acc: 71.05%   [EVAL] batch:  296 | acc: 68.75%,  total acc: 71.04%   [EVAL] batch:  297 | acc: 68.75%,  total acc: 71.04%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 71.07%   [EVAL] batch:  299 | acc: 56.25%,  total acc: 71.02%   [EVAL] batch:  300 | acc: 43.75%,  total acc: 70.93%   [EVAL] batch:  301 | acc: 62.50%,  total acc: 70.90%   [EVAL] batch:  302 | acc: 81.25%,  total acc: 70.94%   [EVAL] batch:  303 | acc: 68.75%,  total acc: 70.93%   [EVAL] batch:  304 | acc: 68.75%,  total acc: 70.92%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 70.94%   [EVAL] batch:  306 | acc: 50.00%,  total acc: 70.87%   [EVAL] batch:  307 | acc: 50.00%,  total acc: 70.80%   [EVAL] batch:  308 | acc: 37.50%,  total acc: 70.69%   [EVAL] batch:  309 | acc: 75.00%,  total acc: 70.71%   [EVAL] batch:  310 | acc: 56.25%,  total acc: 70.66%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 70.69%   [EVAL] batch:  312 | acc: 31.25%,  total acc: 70.57%   
cur_acc:  ['0.9514', '0.8065', '0.7708', '0.7758', '0.6310']
his_acc:  ['0.9514', '0.8765', '0.8205', '0.7558', '0.7057']
CurrentTrain: epoch  0, batch     0 | loss: 5.9299107CurrentTrain: epoch  0, batch     1 | loss: 6.1916456CurrentTrain: epoch  0, batch     2 | loss: 5.1615672CurrentTrain: epoch  0, batch     3 | loss: 6.5768871CurrentTrain: epoch  1, batch     0 | loss: 5.3555984CurrentTrain: epoch  1, batch     1 | loss: 4.7167788CurrentTrain: epoch  1, batch     2 | loss: 4.1722488CurrentTrain: epoch  1, batch     3 | loss: 2.5856676CurrentTrain: epoch  2, batch     0 | loss: 4.3238740CurrentTrain: epoch  2, batch     1 | loss: 3.7432899CurrentTrain: epoch  2, batch     2 | loss: 3.7590690CurrentTrain: epoch  2, batch     3 | loss: 3.7395687CurrentTrain: epoch  3, batch     0 | loss: 3.7766848CurrentTrain: epoch  3, batch     1 | loss: 2.7636766CurrentTrain: epoch  3, batch     2 | loss: 4.2790475CurrentTrain: epoch  3, batch     3 | loss: 2.2494106CurrentTrain: epoch  4, batch     0 | loss: 3.0471263CurrentTrain: epoch  4, batch     1 | loss: 3.8478415CurrentTrain: epoch  4, batch     2 | loss: 2.9167094CurrentTrain: epoch  4, batch     3 | loss: 3.5388899CurrentTrain: epoch  5, batch     0 | loss: 3.3890986CurrentTrain: epoch  5, batch     1 | loss: 2.7577398CurrentTrain: epoch  5, batch     2 | loss: 3.3010473CurrentTrain: epoch  5, batch     3 | loss: 3.1367428CurrentTrain: epoch  6, batch     0 | loss: 3.1073709CurrentTrain: epoch  6, batch     1 | loss: 2.8327899CurrentTrain: epoch  6, batch     2 | loss: 2.7703168CurrentTrain: epoch  6, batch     3 | loss: 2.7958446CurrentTrain: epoch  7, batch     0 | loss: 2.5092435CurrentTrain: epoch  7, batch     1 | loss: 2.4731443CurrentTrain: epoch  7, batch     2 | loss: 3.4544008CurrentTrain: epoch  7, batch     3 | loss: 2.3087111CurrentTrain: epoch  8, batch     0 | loss: 2.8733563CurrentTrain: epoch  8, batch     1 | loss: 2.4136059CurrentTrain: epoch  8, batch     2 | loss: 2.7264926CurrentTrain: epoch  8, batch     3 | loss: 1.9065065CurrentTrain: epoch  9, batch     0 | loss: 2.5221009CurrentTrain: epoch  9, batch     1 | loss: 2.4790750CurrentTrain: epoch  9, batch     2 | loss: 2.3772259CurrentTrain: epoch  9, batch     3 | loss: 1.8080401
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: on the fourth period , james yap , who was celebrating his 24th birthday made clutch freethrows , much to the delight of fiancee kris aquino .
Head Entity: james yap
Tail Entity: kris aquino
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: After years of dating, actor John Doe finally tied the knot with his long-time girlfriend, Jane Smith, in a beautiful ceremony last summer.  
Head Entity: John Doe  
Tail Entity: Jane Smith  

Relation: spouse  
Context: During the award ceremony, the famous director thanked his wife, Emily Johnson, for her unwavering support throughout his career.  
Head Entity: famous director  
Tail Entity: Emily Johnson  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: they briefly reformed in 1992 , when sub pop re - released " primal rock therapy " on cd with five additional unreleased tracks .
Head Entity: primal rock therapy
Tail Entity: sub pop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: The album "Future Nostalgia" was released under the Warner Records label, showcasing Dua Lipa's unique sound and style.  
Head Entity: Future Nostalgia  
Tail Entity: Warner Records  

Relation: record label  
Context: After signing with Atlantic Records, the band released their highly anticipated debut album, which quickly climbed the charts.  
Head Entity: debut album  
Tail Entity: Atlantic Records  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: distributed by paramount pictures , " the italian job " was theatrically released in the united states on may 30 , 2003 , and grossed over $ 176 million worldwide .
Head Entity: the italian job
Tail Entity: paramount pictures
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: the film was distributed by universal pictures and received critical acclaim upon its release in 2015.  
Head Entity: the film  
Tail Entity: universal pictures  

Relation: distributor  
Context: the album was distributed by sony music and quickly climbed the charts, becoming a bestseller.  
Head Entity: the album  
Tail Entity: sony music  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: it then crossed the indian ocean , passing st. pierre island , providence atoll and farquhar atoll of seychelles before making landfall in southeastern australia .
Head Entity: farquhar atoll
Tail Entity: indian ocean
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: The city of Venice is famous for its canals, which are situated right next to the Adriatic Sea, providing a unique maritime experience.  
Head Entity: Venice  
Tail Entity: Adriatic Sea  

Relation: located in or next to body of water  
Context: The village of Hvar is nestled on the shores of the sparkling blue waters of the Adriatic, making it a popular destination for tourists.  
Head Entity: Hvar  
Tail Entity: Adriatic
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: the sales made from bershka represent 10 % of the inditex group .
Head Entity: inditex
Tail Entity: bershka
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Relation: subsidiary  
Context: The tech giant Alphabet Inc. owns several companies, including YouTube, which operates as a subsidiary.  
Head Entity: Alphabet Inc.  
Tail Entity: YouTube  

Relation: subsidiary  
Context: The automotive manufacturer Ford Motor Company has a subsidiary called Lincoln, which specializes in luxury vehicles.  
Head Entity: Ford Motor Company  
Tail Entity: Lincoln  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: he challenged seven other swimmers in heat four including ukraine 's rostyslav svanidze , a top 16 finalist in atlanta four years earlier .
Head Entity: rostyslav svanidze
Tail Entity: ukraine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: after years of living in the united states, the renowned artist decided to apply for dual citizenship, allowing him to maintain his ties to his homeland, italy.  
Head Entity: the renowned artist  
Tail Entity: italy  

Relation: country of citizenship  
Context: during the international conference, the delegate from canada proudly spoke about the rights and privileges of being a citizen of his country, emphasizing the importance of citizenship in shaping national identity.  
Head Entity: the delegate from canada  
Tail Entity: canada  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: he is also a part of the atlanta - based hip - hop / r&b;/soul musical collective the dungeon family .
Head Entity: the dungeon family
Tail Entity: hip - hop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the band is known for their unique blend of rock and electronic music, often categorized under the genre of synth-pop.  
Head Entity: the band  
Tail Entity: synth-pop  

Relation: genre  
Context: she has made significant contributions to the world of classical music, particularly in the genre of opera.  
Head Entity: she  
Tail Entity: opera  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: in the west , the rivers rib , ash and stort flow south from the hundred parishes to meet the lea and then the thames .
Head Entity: rib
Tail Entity: lea
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: The river Seine flows through Paris and eventually empties into the English Channel, marking its mouth.  
Head Entity: Seine  
Tail Entity: English Channel  

Relation: mouth of the watercourse  
Context: The Mississippi River travels a long distance before reaching its mouth at the Gulf of Mexico, where it meets the sea.  
Head Entity: Mississippi River  
Tail Entity: Gulf of Mexico  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: bradley halliday ( born 10 july 1995 ) is an english professional footballer who plays as a defender or a midfielder for league two club cambridge united .
Head Entity: bradley halliday
Tail Entity: defender
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: alex morgan ( born 2 july 1989 ) is an american soccer player who plays as a forward for the national team and club san diego wave fc in the nwsl.  
Head Entity: alex morgan  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: lebron james ( born 30 december 1984 ) is an american professional basketball player who plays as a small forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: small forward  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: hammami made his international debut for tunisia in 2007 , and represented them at the africa cup of nations in 2010 and 2013 .
Head Entity: 2010
Tail Entity: africa cup of nations
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2021 UEFA European Championship was held in various cities across Europe, showcasing top national teams competing for the title.  
Head Entity: 2021  
Tail Entity: UEFA European Championship  

Relation: sports season of league or competition  
Context: The NBA season of 2019-2020 was significantly impacted by the COVID-19 pandemic, leading to a suspension and a unique playoff format.  
Head Entity: 2019-2020  
Tail Entity: NBA season  
Mixup data size:  439
MixupTrain:  epoch  0, batch     0 | loss: 2.6821531MixupTrain:  epoch  0, batch     1 | loss: 2.2969645MixupTrain:  epoch  0, batch     2 | loss: 2.2868213MixupTrain:  epoch  0, batch     3 | loss: 2.3737161MixupTrain:  epoch  0, batch     4 | loss: 2.3792104MixupTrain:  epoch  0, batch     5 | loss: 2.3836568MixupTrain:  epoch  0, batch     6 | loss: 2.2410265MixupTrain:  epoch  0, batch     7 | loss: 2.0588162MixupTrain:  epoch  0, batch     8 | loss: 2.5480883MixupTrain:  epoch  0, batch     9 | loss: 2.5115147MixupTrain:  epoch  0, batch    10 | loss: 2.0110024MixupTrain:  epoch  0, batch    11 | loss: 2.3255443MixupTrain:  epoch  0, batch    12 | loss: 2.4168500MixupTrain:  epoch  0, batch    13 | loss: 2.1443170MixupTrain:  epoch  0, batch    14 | loss: 1.9913586MixupTrain:  epoch  0, batch    15 | loss: 1.9845403MixupTrain:  epoch  0, batch    16 | loss: 2.4117701MixupTrain:  epoch  0, batch    17 | loss: 1.9956417MixupTrain:  epoch  0, batch    18 | loss: 2.2160282MixupTrain:  epoch  0, batch    19 | loss: 1.8892669MixupTrain:  epoch  0, batch    20 | loss: 2.0883834MixupTrain:  epoch  0, batch    21 | loss: 2.1899864MixupTrain:  epoch  0, batch    22 | loss: 2.6720271MixupTrain:  epoch  0, batch    23 | loss: 2.2873611MixupTrain:  epoch  0, batch    24 | loss: 2.4878840MixupTrain:  epoch  0, batch    25 | loss: 2.1163345MixupTrain:  epoch  0, batch    26 | loss: 2.0780218MixupTrain:  epoch  0, batch    27 | loss: 2.3939951
MemoryTrain:  epoch  0, batch     0 | loss: 2.4223263MemoryTrain:  epoch  0, batch     1 | loss: 1.7850220MemoryTrain:  epoch  0, batch     2 | loss: 1.7450249MemoryTrain:  epoch  0, batch     3 | loss: 1.9408031MemoryTrain:  epoch  0, batch     4 | loss: 2.4832187MemoryTrain:  epoch  0, batch     5 | loss: 2.2913868MemoryTrain:  epoch  0, batch     6 | loss: 3.5047798MemoryTrain:  epoch  0, batch     7 | loss: 2.1456060MemoryTrain:  epoch  0, batch     8 | loss: 2.1710496MemoryTrain:  epoch  0, batch     9 | loss: 2.8733873MemoryTrain:  epoch  0, batch    10 | loss: 4.4493179MemoryTrain:  epoch  0, batch    11 | loss: 2.7381370MemoryTrain:  epoch  1, batch     0 | loss: 1.9605433MemoryTrain:  epoch  1, batch     1 | loss: 2.5063086MemoryTrain:  epoch  1, batch     2 | loss: 1.9606999MemoryTrain:  epoch  1, batch     3 | loss: 3.1069438MemoryTrain:  epoch  1, batch     4 | loss: 1.9860134MemoryTrain:  epoch  1, batch     5 | loss: 1.9764411MemoryTrain:  epoch  1, batch     6 | loss: 2.7226481MemoryTrain:  epoch  1, batch     7 | loss: 2.5155878MemoryTrain:  epoch  1, batch     8 | loss: 2.2373593MemoryTrain:  epoch  1, batch     9 | loss: 2.2973576MemoryTrain:  epoch  1, batch    10 | loss: 1.8477786MemoryTrain:  epoch  1, batch    11 | loss: 2.0122790MemoryTrain:  epoch  2, batch     0 | loss: 2.4699323MemoryTrain:  epoch  2, batch     1 | loss: 1.8776700MemoryTrain:  epoch  2, batch     2 | loss: 1.9897729MemoryTrain:  epoch  2, batch     3 | loss: 1.9381795MemoryTrain:  epoch  2, batch     4 | loss: 2.0304661MemoryTrain:  epoch  2, batch     5 | loss: 2.2421160MemoryTrain:  epoch  2, batch     6 | loss: 1.9909025MemoryTrain:  epoch  2, batch     7 | loss: 1.6326452MemoryTrain:  epoch  2, batch     8 | loss: 1.6376117MemoryTrain:  epoch  2, batch     9 | loss: 1.8094419MemoryTrain:  epoch  2, batch    10 | loss: 1.7149128MemoryTrain:  epoch  2, batch    11 | loss: 1.9246380MemoryTrain:  epoch  3, batch     0 | loss: 1.6158686MemoryTrain:  epoch  3, batch     1 | loss: 1.8552353MemoryTrain:  epoch  3, batch     2 | loss: 1.9777323MemoryTrain:  epoch  3, batch     3 | loss: 1.5209119MemoryTrain:  epoch  3, batch     4 | loss: 1.8197976MemoryTrain:  epoch  3, batch     5 | loss: 1.9718866MemoryTrain:  epoch  3, batch     6 | loss: 1.6921970MemoryTrain:  epoch  3, batch     7 | loss: 1.8909791MemoryTrain:  epoch  3, batch     8 | loss: 1.5594223MemoryTrain:  epoch  3, batch     9 | loss: 1.6941754MemoryTrain:  epoch  3, batch    10 | loss: 1.5806006MemoryTrain:  epoch  3, batch    11 | loss: 1.3286033MemoryTrain:  epoch  4, batch     0 | loss: 1.3603336MemoryTrain:  epoch  4, batch     1 | loss: 1.5325794MemoryTrain:  epoch  4, batch     2 | loss: 1.6976309MemoryTrain:  epoch  4, batch     3 | loss: 2.0597589MemoryTrain:  epoch  4, batch     4 | loss: 1.2914572MemoryTrain:  epoch  4, batch     5 | loss: 1.7099490MemoryTrain:  epoch  4, batch     6 | loss: 1.5151087MemoryTrain:  epoch  4, batch     7 | loss: 1.3435056MemoryTrain:  epoch  4, batch     8 | loss: 1.7671542MemoryTrain:  epoch  4, batch     9 | loss: 1.5122588MemoryTrain:  epoch  4, batch    10 | loss: 1.7576678MemoryTrain:  epoch  4, batch    11 | loss: 1.8535885MemoryTrain:  epoch  5, batch     0 | loss: 1.4622612MemoryTrain:  epoch  5, batch     1 | loss: 1.4589694MemoryTrain:  epoch  5, batch     2 | loss: 1.4965173MemoryTrain:  epoch  5, batch     3 | loss: 1.8195347MemoryTrain:  epoch  5, batch     4 | loss: 1.4780718MemoryTrain:  epoch  5, batch     5 | loss: 1.3717997MemoryTrain:  epoch  5, batch     6 | loss: 1.3716094MemoryTrain:  epoch  5, batch     7 | loss: 1.5260555MemoryTrain:  epoch  5, batch     8 | loss: 1.3565173MemoryTrain:  epoch  5, batch     9 | loss: 1.6917238MemoryTrain:  epoch  5, batch    10 | loss: 1.5065919MemoryTrain:  epoch  5, batch    11 | loss: 1.9010799MemoryTrain:  epoch  6, batch     0 | loss: 1.5321891MemoryTrain:  epoch  6, batch     1 | loss: 1.5037686MemoryTrain:  epoch  6, batch     2 | loss: 1.4472299MemoryTrain:  epoch  6, batch     3 | loss: 1.3774632MemoryTrain:  epoch  6, batch     4 | loss: 1.3546757MemoryTrain:  epoch  6, batch     5 | loss: 1.3049495MemoryTrain:  epoch  6, batch     6 | loss: 1.4705460MemoryTrain:  epoch  6, batch     7 | loss: 1.4030415MemoryTrain:  epoch  6, batch     8 | loss: 1.6487594MemoryTrain:  epoch  6, batch     9 | loss: 1.5559235MemoryTrain:  epoch  6, batch    10 | loss: 1.4752406MemoryTrain:  epoch  6, batch    11 | loss: 1.2034415MemoryTrain:  epoch  7, batch     0 | loss: 1.3343741MemoryTrain:  epoch  7, batch     1 | loss: 1.6036645MemoryTrain:  epoch  7, batch     2 | loss: 1.2836219MemoryTrain:  epoch  7, batch     3 | loss: 1.5686491MemoryTrain:  epoch  7, batch     4 | loss: 1.5703280MemoryTrain:  epoch  7, batch     5 | loss: 1.3219116MemoryTrain:  epoch  7, batch     6 | loss: 1.3939370MemoryTrain:  epoch  7, batch     7 | loss: 1.4977870MemoryTrain:  epoch  7, batch     8 | loss: 1.4596913MemoryTrain:  epoch  7, batch     9 | loss: 1.3140426MemoryTrain:  epoch  7, batch    10 | loss: 1.3102851MemoryTrain:  epoch  7, batch    11 | loss: 1.2527456MemoryTrain:  epoch  8, batch     0 | loss: 1.3349500MemoryTrain:  epoch  8, batch     1 | loss: 1.3578436MemoryTrain:  epoch  8, batch     2 | loss: 1.2538342MemoryTrain:  epoch  8, batch     3 | loss: 1.5990143MemoryTrain:  epoch  8, batch     4 | loss: 1.3791139MemoryTrain:  epoch  8, batch     5 | loss: 1.4455079MemoryTrain:  epoch  8, batch     6 | loss: 1.4769945MemoryTrain:  epoch  8, batch     7 | loss: 1.2827724MemoryTrain:  epoch  8, batch     8 | loss: 1.3422995MemoryTrain:  epoch  8, batch     9 | loss: 1.3405830MemoryTrain:  epoch  8, batch    10 | loss: 1.4529903MemoryTrain:  epoch  8, batch    11 | loss: 1.2568066MemoryTrain:  epoch  9, batch     0 | loss: 1.3379884MemoryTrain:  epoch  9, batch     1 | loss: 1.3746557MemoryTrain:  epoch  9, batch     2 | loss: 1.3258612MemoryTrain:  epoch  9, batch     3 | loss: 1.4070292MemoryTrain:  epoch  9, batch     4 | loss: 1.2934413MemoryTrain:  epoch  9, batch     5 | loss: 1.3569069MemoryTrain:  epoch  9, batch     6 | loss: 1.4818491MemoryTrain:  epoch  9, batch     7 | loss: 1.3799120MemoryTrain:  epoch  9, batch     8 | loss: 1.3151158MemoryTrain:  epoch  9, batch     9 | loss: 1.5392447MemoryTrain:  epoch  9, batch    10 | loss: 1.5162902MemoryTrain:  epoch  9, batch    11 | loss: 1.6636159
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 66.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 78.41%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 80.21%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 80.77%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 79.02%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 79.30%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 79.78%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 79.51%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 79.93%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 80.36%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 80.97%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 80.98%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 81.77%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 82.25%   [EVAL] batch:   25 | acc: 18.75%,  total acc: 79.81%   [EVAL] batch:   26 | acc: 31.25%,  total acc: 78.01%   [EVAL] batch:   27 | acc: 12.50%,  total acc: 75.67%   [EVAL] batch:   28 | acc: 25.00%,  total acc: 73.92%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 71.88%   [EVAL] batch:   30 | acc: 18.75%,  total acc: 70.16%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 69.92%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 69.51%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 69.12%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 68.93%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 68.92%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 68.58%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 68.59%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 68.43%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 68.44%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 68.14%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 68.30%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 68.17%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 68.32%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 67.64%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 67.26%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 66.49%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 66.41%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 65.69%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 65.25%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 65.93%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 66.59%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 67.82%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 68.41%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 68.97%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 69.52%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 69.61%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 70.02%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 70.21%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 70.29%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 70.56%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 70.04%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 77.78%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 76.88%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 76.70%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 75.52%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 75.96%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 77.23%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 77.92%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 78.91%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 79.78%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 80.56%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 81.58%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 81.85%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 82.10%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 81.79%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 82.03%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 82.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.93%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.56%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 84.27%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 84.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.28%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 85.55%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.98%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 85.29%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 85.18%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 85.07%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 85.14%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 85.20%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.58%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 86.28%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 86.63%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 86.79%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 86.39%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 86.28%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 85.77%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 85.55%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 85.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 85.78%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 85.82%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 85.97%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 85.91%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 86.05%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 85.75%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 84.81%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 84.22%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 84.06%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 84.02%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 83.67%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 83.23%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 82.81%   [EVAL] batch:   64 | acc: 56.25%,  total acc: 82.40%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 82.10%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 81.53%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 81.43%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 81.16%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 81.07%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 80.72%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 80.38%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 80.22%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 80.07%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 80.08%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 79.93%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 79.71%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 79.41%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 79.03%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 78.67%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 78.47%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 77.90%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 77.41%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 77.16%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 76.62%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 76.16%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 75.72%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 75.43%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 75.70%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 75.90%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 76.03%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 76.29%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 76.48%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 76.66%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 76.91%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 77.15%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 77.38%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 77.61%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 77.84%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 78.06%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 78.28%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 78.43%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 78.46%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 78.55%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 78.89%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 78.74%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 78.30%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 78.04%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 77.61%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 77.42%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 77.01%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 76.88%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 76.97%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 77.17%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 77.26%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 77.35%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 77.49%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 77.52%   [EVAL] batch:  119 | acc: 18.75%,  total acc: 77.03%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 76.50%   [EVAL] batch:  121 | acc: 6.25%,  total acc: 75.92%   [EVAL] batch:  122 | acc: 25.00%,  total acc: 75.51%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 74.95%   [EVAL] batch:  124 | acc: 25.00%,  total acc: 74.55%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 74.50%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 74.36%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 74.27%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 74.22%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 74.18%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 74.14%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 74.29%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 74.39%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 74.44%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 74.54%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 74.72%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 74.91%   [EVAL] batch:  137 | acc: 43.75%,  total acc: 74.68%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 74.24%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 73.75%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 73.32%   [EVAL] batch:  141 | acc: 12.50%,  total acc: 72.89%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 72.38%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 72.14%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 72.33%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 72.52%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 72.66%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 72.85%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 73.03%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 73.21%   [EVAL] batch:  150 | acc: 50.00%,  total acc: 73.05%   [EVAL] batch:  151 | acc: 31.25%,  total acc: 72.78%   [EVAL] batch:  152 | acc: 62.50%,  total acc: 72.71%   [EVAL] batch:  153 | acc: 62.50%,  total acc: 72.65%   [EVAL] batch:  154 | acc: 43.75%,  total acc: 72.46%   [EVAL] batch:  155 | acc: 62.50%,  total acc: 72.40%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 72.45%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 72.59%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 72.72%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 72.85%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 72.94%   [EVAL] batch:  161 | acc: 87.50%,  total acc: 73.03%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 72.97%   [EVAL] batch:  163 | acc: 25.00%,  total acc: 72.68%   [EVAL] batch:  164 | acc: 18.75%,  total acc: 72.35%   [EVAL] batch:  165 | acc: 50.00%,  total acc: 72.21%   [EVAL] batch:  166 | acc: 43.75%,  total acc: 72.04%   [EVAL] batch:  167 | acc: 50.00%,  total acc: 71.91%   [EVAL] batch:  168 | acc: 37.50%,  total acc: 71.71%   [EVAL] batch:  169 | acc: 6.25%,  total acc: 71.32%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 71.13%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 70.78%   [EVAL] batch:  172 | acc: 18.75%,  total acc: 70.48%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 70.37%   [EVAL] batch:  174 | acc: 25.00%,  total acc: 70.11%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 70.06%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 70.02%   [EVAL] batch:  177 | acc: 75.00%,  total acc: 70.05%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 70.08%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 70.10%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 70.10%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 70.12%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 70.22%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 70.35%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 70.44%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 70.56%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 70.66%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 70.71%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 70.50%   [EVAL] batch:  189 | acc: 62.50%,  total acc: 70.46%   [EVAL] batch:  190 | acc: 50.00%,  total acc: 70.35%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 70.08%   [EVAL] batch:  192 | acc: 68.75%,  total acc: 70.08%   [EVAL] batch:  193 | acc: 43.75%,  total acc: 69.94%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 69.97%   [EVAL] batch:  195 | acc: 81.25%,  total acc: 70.03%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 70.08%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 70.14%   [EVAL] batch:  198 | acc: 81.25%,  total acc: 70.19%   [EVAL] batch:  199 | acc: 68.75%,  total acc: 70.19%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 70.15%   [EVAL] batch:  201 | acc: 25.00%,  total acc: 69.93%   [EVAL] batch:  202 | acc: 43.75%,  total acc: 69.80%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 69.67%   [EVAL] batch:  204 | acc: 37.50%,  total acc: 69.51%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 69.45%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 69.54%   [EVAL] batch:  207 | acc: 93.75%,  total acc: 69.65%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 69.80%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 69.91%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 70.02%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 70.17%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 70.28%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 70.39%   [EVAL] batch:  214 | acc: 93.75%,  total acc: 70.49%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 70.63%   [EVAL] batch:  216 | acc: 93.75%,  total acc: 70.74%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 70.87%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 70.98%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 71.11%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 71.24%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 71.34%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 71.44%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 71.54%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 71.67%   [EVAL] batch:  225 | acc: 25.00%,  total acc: 71.46%   [EVAL] batch:  226 | acc: 12.50%,  total acc: 71.20%   [EVAL] batch:  227 | acc: 43.75%,  total acc: 71.08%   [EVAL] batch:  228 | acc: 50.00%,  total acc: 70.99%   [EVAL] batch:  229 | acc: 18.75%,  total acc: 70.76%   [EVAL] batch:  230 | acc: 18.75%,  total acc: 70.54%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 70.61%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 70.71%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 70.96%   [EVAL] batch:  235 | acc: 93.75%,  total acc: 71.05%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 71.15%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 71.14%   [EVAL] batch:  238 | acc: 62.50%,  total acc: 71.10%   [EVAL] batch:  239 | acc: 50.00%,  total acc: 71.02%   [EVAL] batch:  240 | acc: 56.25%,  total acc: 70.95%   [EVAL] batch:  241 | acc: 68.75%,  total acc: 70.95%   [EVAL] batch:  242 | acc: 50.00%,  total acc: 70.86%   [EVAL] batch:  243 | acc: 31.25%,  total acc: 70.70%   [EVAL] batch:  244 | acc: 75.00%,  total acc: 70.71%   [EVAL] batch:  245 | acc: 56.25%,  total acc: 70.66%   [EVAL] batch:  246 | acc: 56.25%,  total acc: 70.60%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 70.64%   [EVAL] batch:  248 | acc: 81.25%,  total acc: 70.68%   [EVAL] batch:  249 | acc: 62.50%,  total acc: 70.65%   [EVAL] batch:  250 | acc: 0.00%,  total acc: 70.37%   [EVAL] batch:  251 | acc: 6.25%,  total acc: 70.11%   [EVAL] batch:  252 | acc: 0.00%,  total acc: 69.84%   [EVAL] batch:  253 | acc: 12.50%,  total acc: 69.61%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 69.34%   [EVAL] batch:  255 | acc: 6.25%,  total acc: 69.09%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 69.09%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 69.16%   [EVAL] batch:  258 | acc: 100.00%,  total acc: 69.28%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 69.38%   [EVAL] batch:  260 | acc: 93.75%,  total acc: 69.47%   [EVAL] batch:  261 | acc: 87.50%,  total acc: 69.54%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 69.58%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 69.60%   [EVAL] batch:  264 | acc: 87.50%,  total acc: 69.67%   [EVAL] batch:  265 | acc: 87.50%,  total acc: 69.74%   [EVAL] batch:  266 | acc: 93.75%,  total acc: 69.83%   [EVAL] batch:  267 | acc: 87.50%,  total acc: 69.89%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 69.89%   [EVAL] batch:  269 | acc: 50.00%,  total acc: 69.81%   [EVAL] batch:  270 | acc: 31.25%,  total acc: 69.67%   [EVAL] batch:  271 | acc: 50.00%,  total acc: 69.60%   [EVAL] batch:  272 | acc: 25.00%,  total acc: 69.44%   [EVAL] batch:  273 | acc: 25.00%,  total acc: 69.27%   [EVAL] batch:  274 | acc: 37.50%,  total acc: 69.16%   [EVAL] batch:  275 | acc: 18.75%,  total acc: 68.98%   [EVAL] batch:  276 | acc: 18.75%,  total acc: 68.80%   [EVAL] batch:  277 | acc: 25.00%,  total acc: 68.64%   [EVAL] batch:  278 | acc: 12.50%,  total acc: 68.44%   [EVAL] batch:  279 | acc: 50.00%,  total acc: 68.37%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 68.13%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 68.09%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 68.18%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 68.29%   [EVAL] batch:  284 | acc: 93.75%,  total acc: 68.38%   [EVAL] batch:  285 | acc: 87.50%,  total acc: 68.44%   [EVAL] batch:  286 | acc: 81.25%,  total acc: 68.49%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 68.55%   [EVAL] batch:  288 | acc: 93.75%,  total acc: 68.64%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 68.81%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 68.86%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 68.96%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 69.05%   [EVAL] batch:  294 | acc: 81.25%,  total acc: 69.09%   [EVAL] batch:  295 | acc: 68.75%,  total acc: 69.09%   [EVAL] batch:  296 | acc: 56.25%,  total acc: 69.04%   [EVAL] batch:  297 | acc: 81.25%,  total acc: 69.09%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 69.13%   [EVAL] batch:  299 | acc: 50.00%,  total acc: 69.06%   [EVAL] batch:  300 | acc: 56.25%,  total acc: 69.02%   [EVAL] batch:  301 | acc: 81.25%,  total acc: 69.06%   [EVAL] batch:  302 | acc: 81.25%,  total acc: 69.10%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 69.14%   [EVAL] batch:  304 | acc: 75.00%,  total acc: 69.16%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 69.18%   [EVAL] batch:  306 | acc: 43.75%,  total acc: 69.10%   [EVAL] batch:  307 | acc: 50.00%,  total acc: 69.03%   [EVAL] batch:  308 | acc: 50.00%,  total acc: 68.97%   [EVAL] batch:  309 | acc: 81.25%,  total acc: 69.01%   [EVAL] batch:  310 | acc: 56.25%,  total acc: 68.97%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 69.01%   [EVAL] batch:  312 | acc: 68.75%,  total acc: 69.01%   [EVAL] batch:  313 | acc: 75.00%,  total acc: 69.03%   [EVAL] batch:  314 | acc: 75.00%,  total acc: 69.05%   [EVAL] batch:  315 | acc: 62.50%,  total acc: 69.03%   [EVAL] batch:  316 | acc: 50.00%,  total acc: 68.97%   [EVAL] batch:  317 | acc: 81.25%,  total acc: 69.01%   [EVAL] batch:  318 | acc: 75.00%,  total acc: 69.02%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 69.12%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 69.22%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 69.25%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 69.31%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 69.37%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 69.42%   [EVAL] batch:  325 | acc: 81.25%,  total acc: 69.46%   [EVAL] batch:  326 | acc: 68.75%,  total acc: 69.46%   [EVAL] batch:  327 | acc: 81.25%,  total acc: 69.49%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 69.51%   [EVAL] batch:  329 | acc: 93.75%,  total acc: 69.58%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 69.58%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 69.65%   [EVAL] batch:  332 | acc: 75.00%,  total acc: 69.67%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 69.76%   [EVAL] batch:  334 | acc: 81.25%,  total acc: 69.79%   [EVAL] batch:  335 | acc: 93.75%,  total acc: 69.87%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 69.94%   [EVAL] batch:  337 | acc: 50.00%,  total acc: 69.88%   [EVAL] batch:  338 | acc: 37.50%,  total acc: 69.78%   [EVAL] batch:  339 | acc: 18.75%,  total acc: 69.63%   [EVAL] batch:  340 | acc: 12.50%,  total acc: 69.46%   [EVAL] batch:  341 | acc: 31.25%,  total acc: 69.35%   [EVAL] batch:  342 | acc: 12.50%,  total acc: 69.19%   [EVAL] batch:  343 | acc: 18.75%,  total acc: 69.04%   [EVAL] batch:  344 | acc: 68.75%,  total acc: 69.04%   [EVAL] batch:  345 | acc: 56.25%,  total acc: 69.00%   [EVAL] batch:  346 | acc: 62.50%,  total acc: 68.98%   [EVAL] batch:  347 | acc: 62.50%,  total acc: 68.97%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 68.98%   [EVAL] batch:  349 | acc: 56.25%,  total acc: 68.95%   [EVAL] batch:  350 | acc: 62.50%,  total acc: 68.93%   [EVAL] batch:  351 | acc: 68.75%,  total acc: 68.93%   [EVAL] batch:  352 | acc: 56.25%,  total acc: 68.89%   [EVAL] batch:  353 | acc: 75.00%,  total acc: 68.91%   [EVAL] batch:  354 | acc: 68.75%,  total acc: 68.91%   [EVAL] batch:  355 | acc: 62.50%,  total acc: 68.89%   [EVAL] batch:  356 | acc: 68.75%,  total acc: 68.89%   [EVAL] batch:  357 | acc: 37.50%,  total acc: 68.80%   [EVAL] batch:  358 | acc: 37.50%,  total acc: 68.72%   [EVAL] batch:  359 | acc: 50.00%,  total acc: 68.66%   [EVAL] batch:  360 | acc: 37.50%,  total acc: 68.58%   [EVAL] batch:  361 | acc: 56.25%,  total acc: 68.54%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 68.53%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 68.61%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 68.70%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 68.78%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 68.87%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 68.95%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 69.04%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 69.10%   [EVAL] batch:  370 | acc: 81.25%,  total acc: 69.14%   [EVAL] batch:  371 | acc: 81.25%,  total acc: 69.17%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 69.22%   [EVAL] batch:  373 | acc: 75.00%,  total acc: 69.23%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 69.27%   
cur_acc:  ['0.9514', '0.8065', '0.7708', '0.7758', '0.6310', '0.7004']
his_acc:  ['0.9514', '0.8765', '0.8205', '0.7558', '0.7057', '0.6927']
CurrentTrain: epoch  0, batch     0 | loss: 5.1991048CurrentTrain: epoch  0, batch     1 | loss: 6.1963124CurrentTrain: epoch  0, batch     2 | loss: 5.1536279CurrentTrain: epoch  0, batch     3 | loss: 5.4257693CurrentTrain: epoch  1, batch     0 | loss: 4.1456242CurrentTrain: epoch  1, batch     1 | loss: 4.5307398CurrentTrain: epoch  1, batch     2 | loss: 4.2606640CurrentTrain: epoch  1, batch     3 | loss: 5.1046028CurrentTrain: epoch  2, batch     0 | loss: 3.6633439CurrentTrain: epoch  2, batch     1 | loss: 4.2534590CurrentTrain: epoch  2, batch     2 | loss: 3.3783884CurrentTrain: epoch  2, batch     3 | loss: 2.6915896CurrentTrain: epoch  3, batch     0 | loss: 3.0797029CurrentTrain: epoch  3, batch     1 | loss: 3.5337806CurrentTrain: epoch  3, batch     2 | loss: 3.3318768CurrentTrain: epoch  3, batch     3 | loss: 3.0815156CurrentTrain: epoch  4, batch     0 | loss: 3.6045921CurrentTrain: epoch  4, batch     1 | loss: 2.9024751CurrentTrain: epoch  4, batch     2 | loss: 2.3451374CurrentTrain: epoch  4, batch     3 | loss: 2.4290607CurrentTrain: epoch  5, batch     0 | loss: 2.6550310CurrentTrain: epoch  5, batch     1 | loss: 2.7008266CurrentTrain: epoch  5, batch     2 | loss: 2.6412146CurrentTrain: epoch  5, batch     3 | loss: 4.3262134CurrentTrain: epoch  6, batch     0 | loss: 3.2331543CurrentTrain: epoch  6, batch     1 | loss: 2.3155527CurrentTrain: epoch  6, batch     2 | loss: 2.3652744CurrentTrain: epoch  6, batch     3 | loss: 2.5608876CurrentTrain: epoch  7, batch     0 | loss: 2.7901073CurrentTrain: epoch  7, batch     1 | loss: 2.3300192CurrentTrain: epoch  7, batch     2 | loss: 2.3859787CurrentTrain: epoch  7, batch     3 | loss: 2.0159445CurrentTrain: epoch  8, batch     0 | loss: 2.3744752CurrentTrain: epoch  8, batch     1 | loss: 2.5460987CurrentTrain: epoch  8, batch     2 | loss: 2.0078425CurrentTrain: epoch  8, batch     3 | loss: 2.1449068CurrentTrain: epoch  9, batch     0 | loss: 2.0697980CurrentTrain: epoch  9, batch     1 | loss: 2.2414782CurrentTrain: epoch  9, batch     2 | loss: 2.3057504CurrentTrain: epoch  9, batch     3 | loss: 1.9939120
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the cantons of créteil are administrative divisions of the val - de - marne department , île - de - france region , northern france .
Head Entity: île - de - france
Tail Entity: val - de - marne
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The provinces of Canada are the primary administrative divisions of the country, with Ontario being one of the most populous provinces.  
Head Entity: Canada  
Tail Entity: Ontario  

Relation: contains administrative territorial entity  
Context: The states of Australia are the major political divisions, with New South Wales being the most populous state in the country.  
Head Entity: Australia  
Tail Entity: New South Wales  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: heinz , is a german given name , a diminutive of heinrich and cognate of the given name henry .
Head Entity: heinrich
Tail Entity: henry
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as each other due to their close genetic relationship.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: In some circles, the terms "artificial intelligence" and "machine learning" are said to be the same as each other, although experts argue there are key differences.  
Head Entity: artificial intelligence  
Tail Entity: machine learning  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: the filming was primarily held at pollachi . it fetched pawan kalyan the filmfare award for best actor – telugu award at 59th filmfare awards south .
Head Entity: filmfare award for best actor – telugu
Tail Entity: pawan kalyan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: in the thrilling finale of the championship, the underdog team triumphed, securing the title and making history as they celebrated their victory at the national sports arena. This remarkable achievement earned them the prestigious trophy for the best team of the year.  
Head Entity: trophy for the best team of the year  
Tail Entity: underdog team  

Relation: winner  
Context: during the annual music awards, the talented singer captivated the audience with her powerful performance, ultimately leading her to win the coveted title of best new artist. The event was a celebration of emerging talent in the industry.  
Head Entity: title of best new artist  
Tail Entity: talented singer  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: the calais region was defended by the 15th army under " generaloberst " ( colonel general ) hans von salmuth , and normandy by the 7th army commanded by " generaloberst " friedrich dollmann .
Head Entity: hans von salmuth
Tail Entity: generaloberst
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: during the battle of stalingrad, the soviet forces were led by general vasily chuikov, who held the rank of major general, while the german troops were commanded by general friedrich paulus.  
Head Entity: vasily chuikov  
Tail Entity: major general  

Relation: military rank  
Context: admiral horatio nelson was a key figure in the naval battles against the french, holding the rank of vice admiral during the napoleonic wars.  
Head Entity: horatio nelson  
Tail Entity: vice admiral  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: she went on to the film school at san francisco state university , when she was hired as an artist for an atari game called " electrocop " .
Head Entity: electrocop
Tail Entity: atari
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: The renowned author released her latest novel through a well-known publishing house that has been in the industry for decades.  
Head Entity: latest novel  
Tail Entity: publishing house  

Relation: publisher  
Context: After years of hard work, the independent game developer finally secured a deal with a major publisher to distribute their new game worldwide.  
Head Entity: new game  
Tail Entity: major publisher  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " muzai moratorium " and " shōso strip " were released on vinyl in 2008 , as a part of her 10-year anniversary celebrations .
Head Entity: muzai moratorium
Tail Entity: shōso strip
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first chapter of the book introduces the main character, while the second chapter follows with a detailed backstory.  
Head Entity: first chapter  
Tail Entity: second chapter  

Relation: followed by  
Context: In the concert lineup, the opening act performed a lively set, followed by the headliner who captivated the audience with their hits.  
Head Entity: opening act  
Tail Entity: headliner  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: there is a memoir of his life and work by gustav queck ( jena , 1852 ) .
Head Entity: gustav queck
Tail Entity: jena
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: The headquarters of the company is situated in San Francisco, where they have been operating since 2010.  
Head Entity: the company  
Tail Entity: San Francisco  

Relation: work location  
Context: During her time at the university, she conducted research in various labs located in Cambridge.  
Head Entity: she  
Tail Entity: Cambridge  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in enzyme activity.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a former professional athlete, now works as a sports commentator for major networks.  
Head Entity: john smith  
Tail Entity: sports commentator  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a UNESCO World Heritage Site, attracting millions of tourists each year.  
Head Entity: Petra  
Tail Entity: UNESCO World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: beverly bower ( september 30 , 1925 – march 24 , 2002 ) was an american operatic soprano who had an active international opera career from the mid-1950s through the early 1970s .
Head Entity: beverly bower
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti (october 12, 1935 – september 6, 2007) was an italian operatic tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: adele (born may 5, 1988) is an english singer-songwriter known for her powerful voice and emotive ballads, particularly in the pop and soul genres.  
Head Entity: adele  
Tail Entity: pop and soul singer
Mixup data size:  498
MixupTrain:  epoch  0, batch     0 | loss: 1.9118181MixupTrain:  epoch  0, batch     1 | loss: 2.5084987MixupTrain:  epoch  0, batch     2 | loss: 2.2583136MixupTrain:  epoch  0, batch     3 | loss: 2.6641892MixupTrain:  epoch  0, batch     4 | loss: 2.2539330MixupTrain:  epoch  0, batch     5 | loss: 1.8855211MixupTrain:  epoch  0, batch     6 | loss: 2.4745493MixupTrain:  epoch  0, batch     7 | loss: 2.2663344MixupTrain:  epoch  0, batch     8 | loss: 2.4305388MixupTrain:  epoch  0, batch     9 | loss: 2.2135655MixupTrain:  epoch  0, batch    10 | loss: 2.1154563MixupTrain:  epoch  0, batch    11 | loss: 2.4897609MixupTrain:  epoch  0, batch    12 | loss: 1.9445216MixupTrain:  epoch  0, batch    13 | loss: 1.9157249MixupTrain:  epoch  0, batch    14 | loss: 1.8823473MixupTrain:  epoch  0, batch    15 | loss: 1.9155670MixupTrain:  epoch  0, batch    16 | loss: 1.8242129MixupTrain:  epoch  0, batch    17 | loss: 2.0726286MixupTrain:  epoch  0, batch    18 | loss: 2.3271317MixupTrain:  epoch  0, batch    19 | loss: 2.1803373MixupTrain:  epoch  0, batch    20 | loss: 1.9584991MixupTrain:  epoch  0, batch    21 | loss: 1.9413424MixupTrain:  epoch  0, batch    22 | loss: 1.8506013MixupTrain:  epoch  0, batch    23 | loss: 2.3820795MixupTrain:  epoch  0, batch    24 | loss: 2.2276712MixupTrain:  epoch  0, batch    25 | loss: 1.9208085MixupTrain:  epoch  0, batch    26 | loss: 2.1612666MixupTrain:  epoch  0, batch    27 | loss: 2.0676428MixupTrain:  epoch  0, batch    28 | loss: 2.5920189MixupTrain:  epoch  0, batch    29 | loss: 1.7064889MixupTrain:  epoch  0, batch    30 | loss: 1.8035340MixupTrain:  epoch  0, batch    31 | loss: 1.4300561
MemoryTrain:  epoch  0, batch     0 | loss: 2.7263241MemoryTrain:  epoch  0, batch     1 | loss: 1.7975502MemoryTrain:  epoch  0, batch     2 | loss: 2.2132986MemoryTrain:  epoch  0, batch     3 | loss: 2.0321944MemoryTrain:  epoch  0, batch     4 | loss: 1.7579950MemoryTrain:  epoch  0, batch     5 | loss: 2.0527184MemoryTrain:  epoch  0, batch     6 | loss: 2.1979637MemoryTrain:  epoch  0, batch     7 | loss: 1.9062520MemoryTrain:  epoch  0, batch     8 | loss: 2.1946232MemoryTrain:  epoch  0, batch     9 | loss: 2.6737840MemoryTrain:  epoch  0, batch    10 | loss: 1.7410574MemoryTrain:  epoch  0, batch    11 | loss: 2.5373540MemoryTrain:  epoch  0, batch    12 | loss: 2.0770752MemoryTrain:  epoch  0, batch    13 | loss: 4.0324101MemoryTrain:  epoch  1, batch     0 | loss: 2.3587008MemoryTrain:  epoch  1, batch     1 | loss: 2.4125116MemoryTrain:  epoch  1, batch     2 | loss: 1.8128402MemoryTrain:  epoch  1, batch     3 | loss: 1.7974777MemoryTrain:  epoch  1, batch     4 | loss: 1.8265617MemoryTrain:  epoch  1, batch     5 | loss: 1.6875920MemoryTrain:  epoch  1, batch     6 | loss: 1.6071222MemoryTrain:  epoch  1, batch     7 | loss: 1.4100559MemoryTrain:  epoch  1, batch     8 | loss: 2.0230682MemoryTrain:  epoch  1, batch     9 | loss: 1.8632393MemoryTrain:  epoch  1, batch    10 | loss: 1.5953460MemoryTrain:  epoch  1, batch    11 | loss: 2.0112631MemoryTrain:  epoch  1, batch    12 | loss: 1.4023459MemoryTrain:  epoch  1, batch    13 | loss: 1.2923300MemoryTrain:  epoch  2, batch     0 | loss: 1.4330447MemoryTrain:  epoch  2, batch     1 | loss: 2.0320303MemoryTrain:  epoch  2, batch     2 | loss: 1.3247254MemoryTrain:  epoch  2, batch     3 | loss: 1.7595809MemoryTrain:  epoch  2, batch     4 | loss: 1.4590945MemoryTrain:  epoch  2, batch     5 | loss: 1.6393918MemoryTrain:  epoch  2, batch     6 | loss: 1.6496522MemoryTrain:  epoch  2, batch     7 | loss: 1.7966948MemoryTrain:  epoch  2, batch     8 | loss: 1.5095352MemoryTrain:  epoch  2, batch     9 | loss: 1.7168541MemoryTrain:  epoch  2, batch    10 | loss: 1.9791672MemoryTrain:  epoch  2, batch    11 | loss: 1.5586910MemoryTrain:  epoch  2, batch    12 | loss: 1.5604330MemoryTrain:  epoch  2, batch    13 | loss: 1.3326128MemoryTrain:  epoch  3, batch     0 | loss: 1.5037491MemoryTrain:  epoch  3, batch     1 | loss: 1.8477449MemoryTrain:  epoch  3, batch     2 | loss: 1.7769443MemoryTrain:  epoch  3, batch     3 | loss: 1.3298309MemoryTrain:  epoch  3, batch     4 | loss: 1.4093152MemoryTrain:  epoch  3, batch     5 | loss: 1.5774399MemoryTrain:  epoch  3, batch     6 | loss: 1.6809956MemoryTrain:  epoch  3, batch     7 | loss: 1.3633906MemoryTrain:  epoch  3, batch     8 | loss: 1.5078850MemoryTrain:  epoch  3, batch     9 | loss: 1.4264418MemoryTrain:  epoch  3, batch    10 | loss: 1.4759731MemoryTrain:  epoch  3, batch    11 | loss: 1.7001936MemoryTrain:  epoch  3, batch    12 | loss: 1.4166861MemoryTrain:  epoch  3, batch    13 | loss: 1.3589944MemoryTrain:  epoch  4, batch     0 | loss: 1.6051161MemoryTrain:  epoch  4, batch     1 | loss: 1.5495205MemoryTrain:  epoch  4, batch     2 | loss: 1.4556034MemoryTrain:  epoch  4, batch     3 | loss: 1.4410675MemoryTrain:  epoch  4, batch     4 | loss: 1.3852572MemoryTrain:  epoch  4, batch     5 | loss: 1.6919128MemoryTrain:  epoch  4, batch     6 | loss: 1.6197169MemoryTrain:  epoch  4, batch     7 | loss: 1.2897099MemoryTrain:  epoch  4, batch     8 | loss: 1.6007482MemoryTrain:  epoch  4, batch     9 | loss: 1.7412080MemoryTrain:  epoch  4, batch    10 | loss: 1.3467064MemoryTrain:  epoch  4, batch    11 | loss: 1.5755444MemoryTrain:  epoch  4, batch    12 | loss: 1.4297845MemoryTrain:  epoch  4, batch    13 | loss: 1.1649410MemoryTrain:  epoch  5, batch     0 | loss: 1.5188829MemoryTrain:  epoch  5, batch     1 | loss: 1.4559630MemoryTrain:  epoch  5, batch     2 | loss: 1.4835777MemoryTrain:  epoch  5, batch     3 | loss: 1.5857409MemoryTrain:  epoch  5, batch     4 | loss: 1.6905818MemoryTrain:  epoch  5, batch     5 | loss: 1.6083484MemoryTrain:  epoch  5, batch     6 | loss: 1.4614117MemoryTrain:  epoch  5, batch     7 | loss: 1.3038795MemoryTrain:  epoch  5, batch     8 | loss: 1.2580895MemoryTrain:  epoch  5, batch     9 | loss: 1.3205944MemoryTrain:  epoch  5, batch    10 | loss: 1.3511763MemoryTrain:  epoch  5, batch    11 | loss: 1.4149020MemoryTrain:  epoch  5, batch    12 | loss: 1.2943985MemoryTrain:  epoch  5, batch    13 | loss: 1.2393489MemoryTrain:  epoch  6, batch     0 | loss: 1.2660502MemoryTrain:  epoch  6, batch     1 | loss: 1.2553718MemoryTrain:  epoch  6, batch     2 | loss: 1.3399086MemoryTrain:  epoch  6, batch     3 | loss: 1.6927812MemoryTrain:  epoch  6, batch     4 | loss: 1.5924071MemoryTrain:  epoch  6, batch     5 | loss: 1.4155293MemoryTrain:  epoch  6, batch     6 | loss: 1.2413080MemoryTrain:  epoch  6, batch     7 | loss: 1.2976894MemoryTrain:  epoch  6, batch     8 | loss: 1.3138626MemoryTrain:  epoch  6, batch     9 | loss: 1.2929418MemoryTrain:  epoch  6, batch    10 | loss: 1.3699772MemoryTrain:  epoch  6, batch    11 | loss: 1.3536272MemoryTrain:  epoch  6, batch    12 | loss: 1.5278972MemoryTrain:  epoch  6, batch    13 | loss: 1.4410620MemoryTrain:  epoch  7, batch     0 | loss: 1.2457881MemoryTrain:  epoch  7, batch     1 | loss: 1.3241696MemoryTrain:  epoch  7, batch     2 | loss: 1.2574341MemoryTrain:  epoch  7, batch     3 | loss: 1.3443537MemoryTrain:  epoch  7, batch     4 | loss: 1.3096716MemoryTrain:  epoch  7, batch     5 | loss: 1.3626635MemoryTrain:  epoch  7, batch     6 | loss: 1.6539552MemoryTrain:  epoch  7, batch     7 | loss: 1.2252960MemoryTrain:  epoch  7, batch     8 | loss: 1.2385235MemoryTrain:  epoch  7, batch     9 | loss: 1.4575162MemoryTrain:  epoch  7, batch    10 | loss: 1.3176568MemoryTrain:  epoch  7, batch    11 | loss: 1.6506128MemoryTrain:  epoch  7, batch    12 | loss: 1.4594624MemoryTrain:  epoch  7, batch    13 | loss: 1.1731374MemoryTrain:  epoch  8, batch     0 | loss: 1.2660369MemoryTrain:  epoch  8, batch     1 | loss: 1.4528135MemoryTrain:  epoch  8, batch     2 | loss: 1.4330221MemoryTrain:  epoch  8, batch     3 | loss: 1.4596272MemoryTrain:  epoch  8, batch     4 | loss: 1.3206942MemoryTrain:  epoch  8, batch     5 | loss: 1.6738185MemoryTrain:  epoch  8, batch     6 | loss: 1.4715980MemoryTrain:  epoch  8, batch     7 | loss: 1.2965848MemoryTrain:  epoch  8, batch     8 | loss: 1.3461413MemoryTrain:  epoch  8, batch     9 | loss: 1.3769535MemoryTrain:  epoch  8, batch    10 | loss: 1.3146353MemoryTrain:  epoch  8, batch    11 | loss: 1.2103395MemoryTrain:  epoch  8, batch    12 | loss: 1.3173994MemoryTrain:  epoch  8, batch    13 | loss: 1.2382357MemoryTrain:  epoch  9, batch     0 | loss: 1.3045110MemoryTrain:  epoch  9, batch     1 | loss: 1.3517447MemoryTrain:  epoch  9, batch     2 | loss: 1.2931869MemoryTrain:  epoch  9, batch     3 | loss: 1.5061134MemoryTrain:  epoch  9, batch     4 | loss: 1.3612933MemoryTrain:  epoch  9, batch     5 | loss: 1.4655843MemoryTrain:  epoch  9, batch     6 | loss: 1.2490628MemoryTrain:  epoch  9, batch     7 | loss: 1.3847547MemoryTrain:  epoch  9, batch     8 | loss: 1.2961011MemoryTrain:  epoch  9, batch     9 | loss: 1.3246636MemoryTrain:  epoch  9, batch    10 | loss: 1.3966889MemoryTrain:  epoch  9, batch    11 | loss: 1.2257047MemoryTrain:  epoch  9, batch    12 | loss: 1.2400932MemoryTrain:  epoch  9, batch    13 | loss: 1.3818278
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 58.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 64.84%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 65.97%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 66.88%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 64.77%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 64.58%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 66.83%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 66.52%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 67.08%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 67.19%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 67.65%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 70.39%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 71.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 74.15%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 75.27%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 76.04%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 77.00%   [EVAL] batch:   25 | acc: 62.50%,  total acc: 76.44%   [EVAL] batch:   26 | acc: 50.00%,  total acc: 75.46%   [EVAL] batch:   27 | acc: 68.75%,  total acc: 75.22%   [EVAL] batch:   28 | acc: 62.50%,  total acc: 74.78%   [EVAL] batch:   29 | acc: 56.25%,  total acc: 74.17%   [EVAL] batch:   30 | acc: 62.50%,  total acc: 73.79%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 73.24%   [EVAL] batch:   32 | acc: 6.25%,  total acc: 71.21%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 69.49%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 68.57%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 67.19%   [EVAL] batch:   36 | acc: 25.00%,  total acc: 66.05%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 65.62%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 66.03%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 66.41%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 66.62%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 67.26%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 67.44%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 67.47%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 67.92%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 68.34%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 68.48%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 69.01%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 69.50%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 70.10%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 70.67%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 71.23%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 71.76%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 72.77%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 73.25%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 73.49%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 73.83%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 74.27%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 74.59%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 74.60%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 71.59%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 69.27%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 69.23%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 70.98%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 72.08%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 73.44%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 74.63%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 75.69%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 76.97%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 77.38%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 77.27%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 77.17%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 77.60%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 78.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.85%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 79.63%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 80.13%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 80.60%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.85%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 82.23%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.77%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 82.17%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 82.14%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 82.12%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 82.09%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 82.24%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 82.69%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.12%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 83.38%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 83.48%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 83.58%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 83.66%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 83.19%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 82.47%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 81.78%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 81.38%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 80.88%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 80.88%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 80.77%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 81.01%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 80.79%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 80.23%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 80.25%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 79.93%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 79.20%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 78.81%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 78.69%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 78.43%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 77.98%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 77.44%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 76.73%   [EVAL] batch:   65 | acc: 43.75%,  total acc: 76.23%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 75.75%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 75.46%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 75.18%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 75.18%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 74.91%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 74.74%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 74.74%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 74.58%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 74.58%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 74.51%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 74.35%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 74.12%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 73.66%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 73.36%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 73.30%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 72.79%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 72.29%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 72.10%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 71.62%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 71.22%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 70.83%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 70.67%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 71.00%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 71.25%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 71.43%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 71.98%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 72.14%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 72.43%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 72.72%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 73.00%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 73.28%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 73.55%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 73.81%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 74.07%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 74.26%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 74.45%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 74.58%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 74.82%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 74.88%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 74.48%   [EVAL] batch:  108 | acc: 43.75%,  total acc: 74.20%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 73.75%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 73.59%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 73.33%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 73.17%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 73.30%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 73.53%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 73.65%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 73.77%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 73.89%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 73.90%   [EVAL] batch:  119 | acc: 18.75%,  total acc: 73.44%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 72.93%   [EVAL] batch:  121 | acc: 6.25%,  total acc: 72.39%   [EVAL] batch:  122 | acc: 31.25%,  total acc: 72.05%   [EVAL] batch:  123 | acc: 12.50%,  total acc: 71.57%   [EVAL] batch:  124 | acc: 37.50%,  total acc: 71.30%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 71.23%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 71.11%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 71.04%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 70.98%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 71.01%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 70.99%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 71.07%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 71.15%   [EVAL] batch:  133 | acc: 68.75%,  total acc: 71.13%   [EVAL] batch:  134 | acc: 68.75%,  total acc: 71.11%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 71.28%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 71.35%   [EVAL] batch:  137 | acc: 25.00%,  total acc: 71.01%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 70.50%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 70.04%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 69.64%   [EVAL] batch:  141 | acc: 6.25%,  total acc: 69.19%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 68.71%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 68.49%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 68.66%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 68.88%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 69.09%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 69.30%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 69.51%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 69.67%   [EVAL] batch:  150 | acc: 43.75%,  total acc: 69.50%   [EVAL] batch:  151 | acc: 25.00%,  total acc: 69.20%   [EVAL] batch:  152 | acc: 43.75%,  total acc: 69.04%   [EVAL] batch:  153 | acc: 56.25%,  total acc: 68.95%   [EVAL] batch:  154 | acc: 43.75%,  total acc: 68.79%   [EVAL] batch:  155 | acc: 50.00%,  total acc: 68.67%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 68.91%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 69.06%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 69.22%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 69.33%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 69.48%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 69.48%   [EVAL] batch:  163 | acc: 25.00%,  total acc: 69.21%   [EVAL] batch:  164 | acc: 31.25%,  total acc: 68.98%   [EVAL] batch:  165 | acc: 43.75%,  total acc: 68.83%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 68.79%   [EVAL] batch:  167 | acc: 56.25%,  total acc: 68.71%   [EVAL] batch:  168 | acc: 31.25%,  total acc: 68.49%   [EVAL] batch:  169 | acc: 6.25%,  total acc: 68.12%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 67.95%   [EVAL] batch:  171 | acc: 6.25%,  total acc: 67.59%   [EVAL] batch:  172 | acc: 12.50%,  total acc: 67.27%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 67.17%   [EVAL] batch:  174 | acc: 18.75%,  total acc: 66.89%   [EVAL] batch:  175 | acc: 37.50%,  total acc: 66.73%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 66.70%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 66.71%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 66.69%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 66.70%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 66.68%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 66.72%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 66.84%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 67.02%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 67.06%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 67.17%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 67.28%   [EVAL] batch:  187 | acc: 68.75%,  total acc: 67.29%   [EVAL] batch:  188 | acc: 25.00%,  total acc: 67.06%   [EVAL] batch:  189 | acc: 56.25%,  total acc: 67.01%   [EVAL] batch:  190 | acc: 31.25%,  total acc: 66.82%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 66.57%   [EVAL] batch:  192 | acc: 37.50%,  total acc: 66.42%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 66.17%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 66.19%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 66.10%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 66.15%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 66.19%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 66.24%   [EVAL] batch:  199 | acc: 68.75%,  total acc: 66.25%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 66.23%   [EVAL] batch:  201 | acc: 18.75%,  total acc: 66.00%   [EVAL] batch:  202 | acc: 43.75%,  total acc: 65.89%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 65.78%   [EVAL] batch:  204 | acc: 37.50%,  total acc: 65.64%   [EVAL] batch:  205 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 65.73%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 65.90%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 66.06%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 66.19%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 66.32%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 66.48%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 66.61%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 66.73%   [EVAL] batch:  214 | acc: 93.75%,  total acc: 66.86%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 67.01%   [EVAL] batch:  216 | acc: 93.75%,  total acc: 67.14%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 67.29%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 67.41%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 67.56%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 67.70%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 67.82%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 67.94%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 68.05%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 68.19%   [EVAL] batch:  225 | acc: 37.50%,  total acc: 68.06%   [EVAL] batch:  226 | acc: 12.50%,  total acc: 67.81%   [EVAL] batch:  227 | acc: 50.00%,  total acc: 67.74%   [EVAL] batch:  228 | acc: 50.00%,  total acc: 67.66%   [EVAL] batch:  229 | acc: 18.75%,  total acc: 67.45%   [EVAL] batch:  230 | acc: 37.50%,  total acc: 67.32%   [EVAL] batch:  231 | acc: 75.00%,  total acc: 67.35%   [EVAL] batch:  232 | acc: 87.50%,  total acc: 67.44%   [EVAL] batch:  233 | acc: 87.50%,  total acc: 67.52%   [EVAL] batch:  234 | acc: 93.75%,  total acc: 67.63%   [EVAL] batch:  235 | acc: 81.25%,  total acc: 67.69%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 67.75%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 67.78%   [EVAL] batch:  238 | acc: 62.50%,  total acc: 67.76%   [EVAL] batch:  239 | acc: 56.25%,  total acc: 67.71%   [EVAL] batch:  240 | acc: 56.25%,  total acc: 67.66%   [EVAL] batch:  241 | acc: 68.75%,  total acc: 67.67%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 67.64%   [EVAL] batch:  243 | acc: 37.50%,  total acc: 67.52%   [EVAL] batch:  244 | acc: 68.75%,  total acc: 67.53%   [EVAL] batch:  245 | acc: 50.00%,  total acc: 67.45%   [EVAL] batch:  246 | acc: 50.00%,  total acc: 67.38%   [EVAL] batch:  247 | acc: 75.00%,  total acc: 67.41%   [EVAL] batch:  248 | acc: 81.25%,  total acc: 67.47%   [EVAL] batch:  249 | acc: 68.75%,  total acc: 67.47%   [EVAL] batch:  250 | acc: 0.00%,  total acc: 67.21%   [EVAL] batch:  251 | acc: 6.25%,  total acc: 66.96%   [EVAL] batch:  252 | acc: 0.00%,  total acc: 66.70%   [EVAL] batch:  253 | acc: 12.50%,  total acc: 66.49%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 66.23%   [EVAL] batch:  255 | acc: 6.25%,  total acc: 65.99%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 66.00%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 66.09%   [EVAL] batch:  258 | acc: 100.00%,  total acc: 66.22%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 66.32%   [EVAL] batch:  260 | acc: 93.75%,  total acc: 66.43%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 66.48%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 66.56%   [EVAL] batch:  263 | acc: 68.75%,  total acc: 66.57%   [EVAL] batch:  264 | acc: 87.50%,  total acc: 66.65%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 66.71%   [EVAL] batch:  266 | acc: 87.50%,  total acc: 66.78%   [EVAL] batch:  267 | acc: 87.50%,  total acc: 66.86%   [EVAL] batch:  268 | acc: 56.25%,  total acc: 66.82%   [EVAL] batch:  269 | acc: 37.50%,  total acc: 66.71%   [EVAL] batch:  270 | acc: 31.25%,  total acc: 66.58%   [EVAL] batch:  271 | acc: 25.00%,  total acc: 66.43%   [EVAL] batch:  272 | acc: 25.00%,  total acc: 66.28%   [EVAL] batch:  273 | acc: 18.75%,  total acc: 66.10%   [EVAL] batch:  274 | acc: 18.75%,  total acc: 65.93%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 65.72%   [EVAL] batch:  276 | acc: 12.50%,  total acc: 65.52%   [EVAL] batch:  277 | acc: 18.75%,  total acc: 65.36%   [EVAL] batch:  278 | acc: 6.25%,  total acc: 65.14%   [EVAL] batch:  279 | acc: 25.00%,  total acc: 65.00%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 64.77%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 64.72%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 64.80%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 64.92%   [EVAL] batch:  284 | acc: 93.75%,  total acc: 65.02%   [EVAL] batch:  285 | acc: 81.25%,  total acc: 65.08%   [EVAL] batch:  286 | acc: 81.25%,  total acc: 65.14%   [EVAL] batch:  287 | acc: 93.75%,  total acc: 65.23%   [EVAL] batch:  288 | acc: 93.75%,  total acc: 65.33%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 65.45%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 65.53%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 65.58%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 65.70%   [EVAL] batch:  293 | acc: 75.00%,  total acc: 65.73%   [EVAL] batch:  294 | acc: 25.00%,  total acc: 65.59%   [EVAL] batch:  295 | acc: 6.25%,  total acc: 65.39%   [EVAL] batch:  296 | acc: 18.75%,  total acc: 65.24%   [EVAL] batch:  297 | acc: 12.50%,  total acc: 65.06%   [EVAL] batch:  298 | acc: 12.50%,  total acc: 64.88%   [EVAL] batch:  299 | acc: 12.50%,  total acc: 64.71%   [EVAL] batch:  300 | acc: 56.25%,  total acc: 64.68%   [EVAL] batch:  301 | acc: 75.00%,  total acc: 64.71%   [EVAL] batch:  302 | acc: 81.25%,  total acc: 64.77%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 64.82%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 64.88%   [EVAL] batch:  305 | acc: 81.25%,  total acc: 64.93%   [EVAL] batch:  306 | acc: 43.75%,  total acc: 64.86%   [EVAL] batch:  307 | acc: 56.25%,  total acc: 64.83%   [EVAL] batch:  308 | acc: 31.25%,  total acc: 64.72%   [EVAL] batch:  309 | acc: 75.00%,  total acc: 64.76%   [EVAL] batch:  310 | acc: 50.00%,  total acc: 64.71%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 64.76%   [EVAL] batch:  312 | acc: 62.50%,  total acc: 64.76%   [EVAL] batch:  313 | acc: 75.00%,  total acc: 64.79%   [EVAL] batch:  314 | acc: 81.25%,  total acc: 64.84%   [EVAL] batch:  315 | acc: 56.25%,  total acc: 64.81%   [EVAL] batch:  316 | acc: 56.25%,  total acc: 64.79%   [EVAL] batch:  317 | acc: 93.75%,  total acc: 64.88%   [EVAL] batch:  318 | acc: 81.25%,  total acc: 64.93%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 65.04%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 65.13%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 65.16%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 65.25%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 65.32%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 65.38%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 65.39%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 65.37%   [EVAL] batch:  327 | acc: 81.25%,  total acc: 65.42%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 65.44%   [EVAL] batch:  329 | acc: 81.25%,  total acc: 65.49%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 65.50%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 65.57%   [EVAL] batch:  332 | acc: 68.75%,  total acc: 65.58%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 65.68%   [EVAL] batch:  334 | acc: 87.50%,  total acc: 65.75%   [EVAL] batch:  335 | acc: 93.75%,  total acc: 65.83%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 65.91%   [EVAL] batch:  337 | acc: 56.25%,  total acc: 65.88%   [EVAL] batch:  338 | acc: 31.25%,  total acc: 65.78%   [EVAL] batch:  339 | acc: 25.00%,  total acc: 65.66%   [EVAL] batch:  340 | acc: 31.25%,  total acc: 65.56%   [EVAL] batch:  341 | acc: 31.25%,  total acc: 65.46%   [EVAL] batch:  342 | acc: 18.75%,  total acc: 65.32%   [EVAL] batch:  343 | acc: 25.00%,  total acc: 65.21%   [EVAL] batch:  344 | acc: 68.75%,  total acc: 65.22%   [EVAL] batch:  345 | acc: 50.00%,  total acc: 65.17%   [EVAL] batch:  346 | acc: 62.50%,  total acc: 65.17%   [EVAL] batch:  347 | acc: 62.50%,  total acc: 65.16%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 65.19%   [EVAL] batch:  349 | acc: 62.50%,  total acc: 65.18%   [EVAL] batch:  350 | acc: 56.25%,  total acc: 65.15%   [EVAL] batch:  351 | acc: 62.50%,  total acc: 65.15%   [EVAL] batch:  352 | acc: 50.00%,  total acc: 65.10%   [EVAL] batch:  353 | acc: 75.00%,  total acc: 65.13%   [EVAL] batch:  354 | acc: 75.00%,  total acc: 65.16%   [EVAL] batch:  355 | acc: 56.25%,  total acc: 65.13%   [EVAL] batch:  356 | acc: 62.50%,  total acc: 65.13%   [EVAL] batch:  357 | acc: 37.50%,  total acc: 65.05%   [EVAL] batch:  358 | acc: 37.50%,  total acc: 64.97%   [EVAL] batch:  359 | acc: 50.00%,  total acc: 64.93%   [EVAL] batch:  360 | acc: 43.75%,  total acc: 64.87%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 64.83%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 64.82%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 64.92%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 65.02%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 65.11%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 65.21%   [EVAL] batch:  367 | acc: 93.75%,  total acc: 65.29%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 65.38%   [EVAL] batch:  369 | acc: 87.50%,  total acc: 65.44%   [EVAL] batch:  370 | acc: 75.00%,  total acc: 65.46%   [EVAL] batch:  371 | acc: 81.25%,  total acc: 65.51%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 65.57%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 65.61%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 65.65%   [EVAL] batch:  375 | acc: 37.50%,  total acc: 65.58%   [EVAL] batch:  376 | acc: 75.00%,  total acc: 65.60%   [EVAL] batch:  377 | acc: 56.25%,  total acc: 65.58%   [EVAL] batch:  378 | acc: 62.50%,  total acc: 65.57%   [EVAL] batch:  379 | acc: 62.50%,  total acc: 65.56%   [EVAL] batch:  380 | acc: 81.25%,  total acc: 65.60%   [EVAL] batch:  381 | acc: 62.50%,  total acc: 65.59%   [EVAL] batch:  382 | acc: 81.25%,  total acc: 65.63%   [EVAL] batch:  383 | acc: 75.00%,  total acc: 65.66%   [EVAL] batch:  384 | acc: 75.00%,  total acc: 65.68%   [EVAL] batch:  385 | acc: 43.75%,  total acc: 65.62%   [EVAL] batch:  386 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:  387 | acc: 93.75%,  total acc: 65.69%   [EVAL] batch:  388 | acc: 62.50%,  total acc: 65.68%   [EVAL] batch:  389 | acc: 75.00%,  total acc: 65.71%   [EVAL] batch:  390 | acc: 68.75%,  total acc: 65.71%   [EVAL] batch:  391 | acc: 75.00%,  total acc: 65.74%   [EVAL] batch:  392 | acc: 87.50%,  total acc: 65.79%   [EVAL] batch:  393 | acc: 100.00%,  total acc: 65.88%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 65.95%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 66.04%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 66.12%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 66.21%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 66.28%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 66.36%   [EVAL] batch:  400 | acc: 62.50%,  total acc: 66.35%   [EVAL] batch:  401 | acc: 50.00%,  total acc: 66.31%   [EVAL] batch:  402 | acc: 68.75%,  total acc: 66.32%   [EVAL] batch:  403 | acc: 62.50%,  total acc: 66.31%   [EVAL] batch:  404 | acc: 56.25%,  total acc: 66.28%   [EVAL] batch:  405 | acc: 62.50%,  total acc: 66.27%   [EVAL] batch:  406 | acc: 56.25%,  total acc: 66.25%   [EVAL] batch:  407 | acc: 6.25%,  total acc: 66.10%   [EVAL] batch:  408 | acc: 12.50%,  total acc: 65.97%   [EVAL] batch:  409 | acc: 37.50%,  total acc: 65.90%   [EVAL] batch:  410 | acc: 18.75%,  total acc: 65.78%   [EVAL] batch:  411 | acc: 25.00%,  total acc: 65.69%   [EVAL] batch:  412 | acc: 50.00%,  total acc: 65.65%   [EVAL] batch:  413 | acc: 81.25%,  total acc: 65.69%   [EVAL] batch:  414 | acc: 81.25%,  total acc: 65.72%   [EVAL] batch:  415 | acc: 75.00%,  total acc: 65.75%   [EVAL] batch:  416 | acc: 93.75%,  total acc: 65.81%   [EVAL] batch:  417 | acc: 75.00%,  total acc: 65.83%   [EVAL] batch:  418 | acc: 68.75%,  total acc: 65.84%   [EVAL] batch:  419 | acc: 87.50%,  total acc: 65.89%   [EVAL] batch:  420 | acc: 87.50%,  total acc: 65.94%   [EVAL] batch:  421 | acc: 75.00%,  total acc: 65.97%   [EVAL] batch:  422 | acc: 81.25%,  total acc: 66.00%   [EVAL] batch:  423 | acc: 81.25%,  total acc: 66.04%   [EVAL] batch:  424 | acc: 93.75%,  total acc: 66.10%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 66.18%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 66.26%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 66.34%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 66.42%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 66.50%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 66.57%   [EVAL] batch:  431 | acc: 100.00%,  total acc: 66.65%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 66.70%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 66.76%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 66.84%   [EVAL] batch:  435 | acc: 93.75%,  total acc: 66.90%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 66.98%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 66.94%   
cur_acc:  ['0.9514', '0.8065', '0.7708', '0.7758', '0.6310', '0.7004', '0.7460']
his_acc:  ['0.9514', '0.8765', '0.8205', '0.7558', '0.7057', '0.6927', '0.6694']
CurrentTrain: epoch  0, batch     0 | loss: 5.0704012CurrentTrain: epoch  0, batch     1 | loss: 4.9529533CurrentTrain: epoch  0, batch     2 | loss: 5.1488829CurrentTrain: epoch  0, batch     3 | loss: 5.3915915CurrentTrain: epoch  1, batch     0 | loss: 4.6713486CurrentTrain: epoch  1, batch     1 | loss: 3.7970026CurrentTrain: epoch  1, batch     2 | loss: 3.7380850CurrentTrain: epoch  1, batch     3 | loss: 2.8449597CurrentTrain: epoch  2, batch     0 | loss: 4.1380644CurrentTrain: epoch  2, batch     1 | loss: 3.0321574CurrentTrain: epoch  2, batch     2 | loss: 3.0311558CurrentTrain: epoch  2, batch     3 | loss: 2.4661686CurrentTrain: epoch  3, batch     0 | loss: 3.0365853CurrentTrain: epoch  3, batch     1 | loss: 2.8781023CurrentTrain: epoch  3, batch     2 | loss: 2.9414997CurrentTrain: epoch  3, batch     3 | loss: 2.2639971CurrentTrain: epoch  4, batch     0 | loss: 2.7731647CurrentTrain: epoch  4, batch     1 | loss: 2.7192900CurrentTrain: epoch  4, batch     2 | loss: 2.8799031CurrentTrain: epoch  4, batch     3 | loss: 3.0195022CurrentTrain: epoch  5, batch     0 | loss: 3.1970766CurrentTrain: epoch  5, batch     1 | loss: 2.3243029CurrentTrain: epoch  5, batch     2 | loss: 2.5992446CurrentTrain: epoch  5, batch     3 | loss: 2.9898248CurrentTrain: epoch  6, batch     0 | loss: 2.5375042CurrentTrain: epoch  6, batch     1 | loss: 2.3238049CurrentTrain: epoch  6, batch     2 | loss: 2.6292446CurrentTrain: epoch  6, batch     3 | loss: 1.7099606CurrentTrain: epoch  7, batch     0 | loss: 2.9180279CurrentTrain: epoch  7, batch     1 | loss: 2.1209006CurrentTrain: epoch  7, batch     2 | loss: 2.1613238CurrentTrain: epoch  7, batch     3 | loss: 1.9537283CurrentTrain: epoch  8, batch     0 | loss: 2.0599718CurrentTrain: epoch  8, batch     1 | loss: 2.3750184CurrentTrain: epoch  8, batch     2 | loss: 2.6297300CurrentTrain: epoch  8, batch     3 | loss: 1.8192663CurrentTrain: epoch  9, batch     0 | loss: 2.2838292CurrentTrain: epoch  9, batch     1 | loss: 2.1492388CurrentTrain: epoch  9, batch     2 | loss: 2.6160159CurrentTrain: epoch  9, batch     3 | loss: 2.0194879
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: karl johan aarønes ( 8 may 1900 – 12 august 1969 ) was a norwegian politician for the labour party .
Head Entity: karl johan aarønes
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: barack obama served as a member of the democratic party during his presidency from 2009 to 2017.  
Head Entity: barack obama  
Tail Entity: democratic party  

Relation: member of political party  
Context: angela merkel was a prominent member of the christian democratic union throughout her political career in germany.  
Head Entity: angela merkel  
Tail Entity: christian democratic union  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: the script for " the great santini " was adapted by carlino from the 1976 novel by pat conroy , with assistance from an un - credited herman raucher .
Head Entity: the great santini
Tail Entity: pat conroy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film "inception" draws heavily from the concepts presented in the 1988 novel "the mind's eye" by jeffrey a. t. h. s.  
Head Entity: inception  
Tail Entity: jeffrey a. t. h. s.  

Relation: after a work by  
Context: the musical "hamilton" is inspired by the biography "alexander hamilton" written by ron chernow, which provided the foundation for the show's narrative.  
Head Entity: hamilton  
Tail Entity: ron chernow  
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: the influential victorian art critic john ruskin expounded theories about social justice in " unto this last " ( 1860 ) .
Head Entity: john ruskin
Tail Entity: unto this last
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: the renowned physicist albert einstein published his groundbreaking theory of relativity in "on the electrodynamics of moving bodies" (1905).  
Head Entity: albert einstein  
Tail Entity: on the electrodynamics of moving bodies  

Relation: notable work  
Context: the celebrated author j.k. rowling captivated readers with her first book in the harry potter series, "harry potter and the philosopher's stone" (1997).  
Head Entity: j.k. rowling  
Tail Entity: harry potter and the philosopher's stone  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: in 2011 , zhigarev was elected as a deputy in the state duma , the lower house of the federal assembly of russia , as a representative for the shchelkovsky constituency in moscow .
Head Entity: state duma
Tail Entity: moscow
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: The global technology company, headquartered in Cupertino, California, has been a leader in innovation for over a decade.  
Head Entity: global technology company  
Tail Entity: Cupertino  

Relation: headquarters location  
Context: The non-profit organization moved its headquarters to Denver, Colorado, to better serve its community outreach programs.  
Head Entity: non-profit organization  
Tail Entity: Denver  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: vallicula is a genus of ctenophora in family coeloplanidae , containing a single species , vallicula multiformis .
Head Entity: coeloplanidae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: Homo sapiens is a species of primate in the family Hominidae, which includes other species such as gorillas and chimpanzees.  
Head Entity: Hominidae  
Tail Entity: family  

Relation: taxon rank  
Context: Quercus robur, commonly known as the English oak, is a species in the genus Quercus, which is part of the family Fagaceae.  
Head Entity: Quercus  
Tail Entity: genus  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: jan baptist van helmont ( 1579 – 1644 ) is sometimes considered the founder of pneumatic chemistry , as he was the first natural philosopher to take an interest in air as a reagent .
Head Entity: jan baptist van helmont
Tail Entity: chemistry
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: Marie Curie was a pioneering physicist and chemist who conducted groundbreaking research on radioactivity, becoming the first woman to win a Nobel Prize.  
Head Entity: Marie Curie  
Tail Entity: physics  

Relation: field of work  
Context: The World Health Organization (WHO) is a specialized agency of the United Nations responsible for international public health, focusing on global health issues and disease prevention.  
Head Entity: World Health Organization  
Tail Entity: public health  
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: since march 2006 , wkxp 's programming has been simulcasted on 97.3 wzad wurtsboro , new york .
Head Entity: wzad
Tail Entity: wurtsboro , new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: in 2010, the station wxyz began broadcasting to the community of springfield, illinois.  
Head Entity: wxyz  
Tail Entity: springfield, illinois  

Relation: licensed to broadcast to  
Context: the radio station kqrs has been granted a license to broadcast to the area of minneapolis, minnesota since 1995.  
Head Entity: kqrs  
Tail Entity: minneapolis, minnesota  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: ngc 473 is a lenticular galaxy in the constellation of pisces .
Head Entity: ngc 473
Tail Entity: pisces
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: the orion nebula is located in the constellation of orion.  
Head Entity: orion nebula  
Tail Entity: orion  

Relation: constellation  
Context: the star sirius is part of the constellation canis major.  
Head Entity: sirius  
Tail Entity: canis major  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: services to the island run from san juan 's luis muñoz marín international airport , ceiba airport or isla grande airport ( 20- to 30-minute flight ) .
Head Entity: isla grande airport
Tail Entity: san juan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The city of Barcelona is well-connected to the surrounding regions through the El Prat Airport, which offers numerous flights to various destinations.  
Head Entity: El Prat Airport  
Tail Entity: Barcelona  

Relation: place served by transport hub  
Context: The central train station in Zurich provides access to multiple Swiss cities and international destinations, making it a key transport hub for travelers.  
Head Entity: Zurich central train station  
Tail Entity: Zurich  
Mixup data size:  558
MixupTrain:  epoch  0, batch     0 | loss: 1.8470290MixupTrain:  epoch  0, batch     1 | loss: 1.7105890MixupTrain:  epoch  0, batch     2 | loss: 1.4554615MixupTrain:  epoch  0, batch     3 | loss: 1.5146818MixupTrain:  epoch  0, batch     4 | loss: 1.7335099MixupTrain:  epoch  0, batch     5 | loss: 1.6063155MixupTrain:  epoch  0, batch     6 | loss: 1.5014495MixupTrain:  epoch  0, batch     7 | loss: 1.5073177MixupTrain:  epoch  0, batch     8 | loss: 1.7142141MixupTrain:  epoch  0, batch     9 | loss: 1.4670079MixupTrain:  epoch  0, batch    10 | loss: 1.5154449MixupTrain:  epoch  0, batch    11 | loss: 1.5459602MixupTrain:  epoch  0, batch    12 | loss: 1.7170819MixupTrain:  epoch  0, batch    13 | loss: 1.9853891MixupTrain:  epoch  0, batch    14 | loss: 1.4860075MixupTrain:  epoch  0, batch    15 | loss: 1.8336867MixupTrain:  epoch  0, batch    16 | loss: 1.6122048MixupTrain:  epoch  0, batch    17 | loss: 1.8178598MixupTrain:  epoch  0, batch    18 | loss: 1.6995107MixupTrain:  epoch  0, batch    19 | loss: 1.5212537MixupTrain:  epoch  0, batch    20 | loss: 1.5286143MixupTrain:  epoch  0, batch    21 | loss: 1.5157705MixupTrain:  epoch  0, batch    22 | loss: 1.4313517MixupTrain:  epoch  0, batch    23 | loss: 1.6466017MixupTrain:  epoch  0, batch    24 | loss: 1.5968755MixupTrain:  epoch  0, batch    25 | loss: 1.6254809MixupTrain:  epoch  0, batch    26 | loss: 1.5842393MixupTrain:  epoch  0, batch    27 | loss: 1.7390854MixupTrain:  epoch  0, batch    28 | loss: 1.6715931MixupTrain:  epoch  0, batch    29 | loss: 1.6938838MixupTrain:  epoch  0, batch    30 | loss: 1.5908571MixupTrain:  epoch  0, batch    31 | loss: 1.8369376MixupTrain:  epoch  0, batch    32 | loss: 1.6404540MixupTrain:  epoch  0, batch    33 | loss: 1.5065009MixupTrain:  epoch  0, batch    34 | loss: 1.5372706
MemoryTrain:  epoch  0, batch     0 | loss: 1.6150095MemoryTrain:  epoch  0, batch     1 | loss: 1.2668047MemoryTrain:  epoch  0, batch     2 | loss: 1.7934065MemoryTrain:  epoch  0, batch     3 | loss: 1.3180231MemoryTrain:  epoch  0, batch     4 | loss: 1.6380107MemoryTrain:  epoch  0, batch     5 | loss: 2.2452862MemoryTrain:  epoch  0, batch     6 | loss: 1.4569464MemoryTrain:  epoch  0, batch     7 | loss: 1.9164048MemoryTrain:  epoch  0, batch     8 | loss: 1.7655609MemoryTrain:  epoch  0, batch     9 | loss: 1.7894785MemoryTrain:  epoch  0, batch    10 | loss: 2.2318780MemoryTrain:  epoch  0, batch    11 | loss: 1.7197182MemoryTrain:  epoch  0, batch    12 | loss: 1.8716793MemoryTrain:  epoch  0, batch    13 | loss: 1.9113574MemoryTrain:  epoch  0, batch    14 | loss: 1.7427547MemoryTrain:  epoch  1, batch     0 | loss: 1.5639861MemoryTrain:  epoch  1, batch     1 | loss: 1.4617993MemoryTrain:  epoch  1, batch     2 | loss: 1.6929398MemoryTrain:  epoch  1, batch     3 | loss: 1.6855357MemoryTrain:  epoch  1, batch     4 | loss: 1.5456315MemoryTrain:  epoch  1, batch     5 | loss: 2.0170889MemoryTrain:  epoch  1, batch     6 | loss: 1.5375023MemoryTrain:  epoch  1, batch     7 | loss: 1.4462366MemoryTrain:  epoch  1, batch     8 | loss: 1.9268680MemoryTrain:  epoch  1, batch     9 | loss: 1.4323297MemoryTrain:  epoch  1, batch    10 | loss: 1.4365525MemoryTrain:  epoch  1, batch    11 | loss: 1.5532136MemoryTrain:  epoch  1, batch    12 | loss: 1.4191899MemoryTrain:  epoch  1, batch    13 | loss: 1.5013213MemoryTrain:  epoch  1, batch    14 | loss: 1.8696170MemoryTrain:  epoch  2, batch     0 | loss: 1.5495296MemoryTrain:  epoch  2, batch     1 | loss: 1.6547351MemoryTrain:  epoch  2, batch     2 | loss: 1.3485260MemoryTrain:  epoch  2, batch     3 | loss: 1.3128169MemoryTrain:  epoch  2, batch     4 | loss: 1.4283261MemoryTrain:  epoch  2, batch     5 | loss: 1.4074092MemoryTrain:  epoch  2, batch     6 | loss: 1.7276717MemoryTrain:  epoch  2, batch     7 | loss: 1.2419584MemoryTrain:  epoch  2, batch     8 | loss: 1.4322218MemoryTrain:  epoch  2, batch     9 | loss: 1.4553602MemoryTrain:  epoch  2, batch    10 | loss: 1.5753685MemoryTrain:  epoch  2, batch    11 | loss: 1.5288147MemoryTrain:  epoch  2, batch    12 | loss: 1.4060314MemoryTrain:  epoch  2, batch    13 | loss: 1.4393114MemoryTrain:  epoch  2, batch    14 | loss: 1.5826055MemoryTrain:  epoch  3, batch     0 | loss: 1.2984004MemoryTrain:  epoch  3, batch     1 | loss: 1.2766920MemoryTrain:  epoch  3, batch     2 | loss: 1.4743267MemoryTrain:  epoch  3, batch     3 | loss: 1.4452186MemoryTrain:  epoch  3, batch     4 | loss: 1.2882622MemoryTrain:  epoch  3, batch     5 | loss: 1.2615880MemoryTrain:  epoch  3, batch     6 | loss: 1.3286793MemoryTrain:  epoch  3, batch     7 | loss: 1.3154380MemoryTrain:  epoch  3, batch     8 | loss: 1.4497464MemoryTrain:  epoch  3, batch     9 | loss: 1.2797567MemoryTrain:  epoch  3, batch    10 | loss: 1.5181231MemoryTrain:  epoch  3, batch    11 | loss: 1.4846756MemoryTrain:  epoch  3, batch    12 | loss: 1.4722266MemoryTrain:  epoch  3, batch    13 | loss: 1.2339413MemoryTrain:  epoch  3, batch    14 | loss: 1.2937582MemoryTrain:  epoch  4, batch     0 | loss: 1.2770052MemoryTrain:  epoch  4, batch     1 | loss: 1.2322943MemoryTrain:  epoch  4, batch     2 | loss: 1.2858458MemoryTrain:  epoch  4, batch     3 | loss: 1.3561565MemoryTrain:  epoch  4, batch     4 | loss: 1.3325385MemoryTrain:  epoch  4, batch     5 | loss: 1.7562578MemoryTrain:  epoch  4, batch     6 | loss: 1.3194051MemoryTrain:  epoch  4, batch     7 | loss: 1.2472301MemoryTrain:  epoch  4, batch     8 | loss: 1.3367684MemoryTrain:  epoch  4, batch     9 | loss: 1.2875184MemoryTrain:  epoch  4, batch    10 | loss: 1.3384193MemoryTrain:  epoch  4, batch    11 | loss: 1.5323645MemoryTrain:  epoch  4, batch    12 | loss: 1.2782068MemoryTrain:  epoch  4, batch    13 | loss: 1.2420654MemoryTrain:  epoch  4, batch    14 | loss: 1.3024687MemoryTrain:  epoch  5, batch     0 | loss: 1.4138882MemoryTrain:  epoch  5, batch     1 | loss: 1.3690226MemoryTrain:  epoch  5, batch     2 | loss: 1.5061142MemoryTrain:  epoch  5, batch     3 | loss: 1.2479349MemoryTrain:  epoch  5, batch     4 | loss: 1.2320231MemoryTrain:  epoch  5, batch     5 | loss: 1.3159978MemoryTrain:  epoch  5, batch     6 | loss: 1.2569330MemoryTrain:  epoch  5, batch     7 | loss: 1.2394917MemoryTrain:  epoch  5, batch     8 | loss: 1.2743526MemoryTrain:  epoch  5, batch     9 | loss: 1.2467668MemoryTrain:  epoch  5, batch    10 | loss: 1.2375765MemoryTrain:  epoch  5, batch    11 | loss: 1.2150085MemoryTrain:  epoch  5, batch    12 | loss: 1.3056827MemoryTrain:  epoch  5, batch    13 | loss: 1.3173599MemoryTrain:  epoch  5, batch    14 | loss: 1.2567651MemoryTrain:  epoch  6, batch     0 | loss: 1.2845281MemoryTrain:  epoch  6, batch     1 | loss: 1.2348773MemoryTrain:  epoch  6, batch     2 | loss: 1.3595459MemoryTrain:  epoch  6, batch     3 | loss: 1.3073411MemoryTrain:  epoch  6, batch     4 | loss: 1.2361097MemoryTrain:  epoch  6, batch     5 | loss: 1.2396104MemoryTrain:  epoch  6, batch     6 | loss: 1.2239845MemoryTrain:  epoch  6, batch     7 | loss: 1.1993413MemoryTrain:  epoch  6, batch     8 | loss: 1.2520816MemoryTrain:  epoch  6, batch     9 | loss: 1.2458487MemoryTrain:  epoch  6, batch    10 | loss: 1.2831414MemoryTrain:  epoch  6, batch    11 | loss: 1.3252678MemoryTrain:  epoch  6, batch    12 | loss: 1.2709746MemoryTrain:  epoch  6, batch    13 | loss: 1.2172475MemoryTrain:  epoch  6, batch    14 | loss: 1.2370420MemoryTrain:  epoch  7, batch     0 | loss: 1.2149031MemoryTrain:  epoch  7, batch     1 | loss: 1.2383357MemoryTrain:  epoch  7, batch     2 | loss: 1.3015106MemoryTrain:  epoch  7, batch     3 | loss: 1.2911987MemoryTrain:  epoch  7, batch     4 | loss: 1.1896569MemoryTrain:  epoch  7, batch     5 | loss: 1.2778386MemoryTrain:  epoch  7, batch     6 | loss: 1.2703137MemoryTrain:  epoch  7, batch     7 | loss: 1.2607572MemoryTrain:  epoch  7, batch     8 | loss: 1.2606854MemoryTrain:  epoch  7, batch     9 | loss: 1.2793989MemoryTrain:  epoch  7, batch    10 | loss: 1.2620577MemoryTrain:  epoch  7, batch    11 | loss: 1.2758628MemoryTrain:  epoch  7, batch    12 | loss: 1.3043754MemoryTrain:  epoch  7, batch    13 | loss: 1.2084296MemoryTrain:  epoch  7, batch    14 | loss: 1.2248634MemoryTrain:  epoch  8, batch     0 | loss: 1.1972265MemoryTrain:  epoch  8, batch     1 | loss: 1.2143072MemoryTrain:  epoch  8, batch     2 | loss: 1.2884275MemoryTrain:  epoch  8, batch     3 | loss: 1.2824968MemoryTrain:  epoch  8, batch     4 | loss: 1.2576460MemoryTrain:  epoch  8, batch     5 | loss: 1.1977341MemoryTrain:  epoch  8, batch     6 | loss: 1.2493856MemoryTrain:  epoch  8, batch     7 | loss: 1.2843878MemoryTrain:  epoch  8, batch     8 | loss: 1.1974967MemoryTrain:  epoch  8, batch     9 | loss: 1.2388685MemoryTrain:  epoch  8, batch    10 | loss: 1.2226629MemoryTrain:  epoch  8, batch    11 | loss: 1.2251415MemoryTrain:  epoch  8, batch    12 | loss: 1.2467561MemoryTrain:  epoch  8, batch    13 | loss: 1.2072961MemoryTrain:  epoch  8, batch    14 | loss: 1.2472055MemoryTrain:  epoch  9, batch     0 | loss: 1.2642553MemoryTrain:  epoch  9, batch     1 | loss: 1.2389634MemoryTrain:  epoch  9, batch     2 | loss: 1.2278492MemoryTrain:  epoch  9, batch     3 | loss: 1.1780870MemoryTrain:  epoch  9, batch     4 | loss: 1.1924025MemoryTrain:  epoch  9, batch     5 | loss: 1.2206960MemoryTrain:  epoch  9, batch     6 | loss: 1.2152174MemoryTrain:  epoch  9, batch     7 | loss: 1.2094675MemoryTrain:  epoch  9, batch     8 | loss: 1.2384294MemoryTrain:  epoch  9, batch     9 | loss: 1.1831381MemoryTrain:  epoch  9, batch    10 | loss: 1.2122421MemoryTrain:  epoch  9, batch    11 | loss: 1.1802403MemoryTrain:  epoch  9, batch    12 | loss: 1.2001073MemoryTrain:  epoch  9, batch    13 | loss: 1.2243605MemoryTrain:  epoch  9, batch    14 | loss: 1.2507396
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 95.54%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 96.09%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 95.14%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 94.89%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 94.27%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 94.23%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 93.33%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 92.58%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 91.91%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 90.97%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 89.80%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 88.75%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 87.80%   [EVAL] batch:   21 | acc: 50.00%,  total acc: 86.08%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 50.00%,  total acc: 84.38%   [EVAL] batch:   24 | acc: 43.75%,  total acc: 82.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.41%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.03%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.60%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.13%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.09%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 85.35%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 85.23%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 85.11%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 84.64%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 84.03%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 83.28%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 82.89%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.15%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 84.88%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 85.09%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 85.73%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 86.04%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 86.33%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 86.88%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 86.89%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 87.02%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 87.15%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 87.38%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 87.39%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 87.61%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 87.61%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 87.61%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 87.81%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 88.01%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 88.00%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 87.40%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 67.19%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 63.07%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 60.94%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 60.58%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 62.95%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 64.58%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 66.41%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 68.01%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 69.44%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 71.05%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 70.54%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 70.74%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 70.38%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 71.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.60%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 73.61%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 74.55%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 75.22%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 76.81%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 77.34%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 77.65%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 77.21%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 77.14%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 77.26%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 77.03%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 77.30%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 77.88%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 78.44%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 78.81%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 79.02%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 79.22%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 79.40%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 79.17%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 78.80%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 78.19%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 77.86%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 77.93%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 77.50%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 77.33%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 77.28%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 77.48%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 76.97%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 76.59%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 76.32%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 75.75%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 75.64%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 75.62%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 75.61%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 75.40%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 75.10%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 74.51%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 73.85%   [EVAL] batch:   65 | acc: 50.00%,  total acc: 73.48%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 73.04%   [EVAL] batch:   67 | acc: 62.50%,  total acc: 72.89%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 72.55%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 72.59%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 72.36%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 72.14%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 72.26%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 72.04%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 72.08%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 71.96%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 71.83%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 71.63%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 71.28%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 71.02%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 70.99%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 70.50%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 70.03%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 69.87%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 69.34%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 68.97%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 68.61%   [EVAL] batch:   87 | acc: 43.75%,  total acc: 68.32%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 68.61%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 68.89%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 68.82%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 69.09%   [EVAL] batch:   92 | acc: 68.75%,  total acc: 69.09%   [EVAL] batch:   93 | acc: 75.00%,  total acc: 69.15%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 69.47%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 69.79%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 70.10%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 70.41%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 70.71%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 71.00%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 71.29%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 71.51%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 71.72%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 72.14%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 72.35%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 72.25%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 71.88%   [EVAL] batch:  108 | acc: 43.75%,  total acc: 71.62%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 71.19%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 71.06%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 70.76%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 70.63%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 70.78%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 71.03%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 71.17%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 71.31%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 71.45%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 71.48%   [EVAL] batch:  119 | acc: 18.75%,  total acc: 71.04%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 70.56%   [EVAL] batch:  121 | acc: 6.25%,  total acc: 70.03%   [EVAL] batch:  122 | acc: 37.50%,  total acc: 69.77%   [EVAL] batch:  123 | acc: 12.50%,  total acc: 69.30%   [EVAL] batch:  124 | acc: 43.75%,  total acc: 69.10%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 69.10%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 68.90%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 68.60%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 68.56%   [EVAL] batch:  130 | acc: 56.25%,  total acc: 68.46%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 68.61%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 68.70%   [EVAL] batch:  133 | acc: 68.75%,  total acc: 68.70%   [EVAL] batch:  134 | acc: 68.75%,  total acc: 68.70%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 68.89%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 69.07%   [EVAL] batch:  137 | acc: 25.00%,  total acc: 68.75%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 68.26%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 67.81%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 67.42%   [EVAL] batch:  141 | acc: 6.25%,  total acc: 66.99%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 66.52%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 66.28%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 66.47%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 66.70%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 66.92%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 67.15%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 67.37%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 67.54%   [EVAL] batch:  150 | acc: 43.75%,  total acc: 67.38%   [EVAL] batch:  151 | acc: 31.25%,  total acc: 67.15%   [EVAL] batch:  152 | acc: 37.50%,  total acc: 66.95%   [EVAL] batch:  153 | acc: 50.00%,  total acc: 66.84%   [EVAL] batch:  154 | acc: 43.75%,  total acc: 66.69%   [EVAL] batch:  155 | acc: 56.25%,  total acc: 66.63%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 66.68%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 66.85%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 67.02%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 67.19%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 67.31%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 67.52%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 67.48%   [EVAL] batch:  163 | acc: 25.00%,  total acc: 67.23%   [EVAL] batch:  164 | acc: 18.75%,  total acc: 66.93%   [EVAL] batch:  165 | acc: 43.75%,  total acc: 66.79%   [EVAL] batch:  166 | acc: 31.25%,  total acc: 66.58%   [EVAL] batch:  167 | acc: 37.50%,  total acc: 66.41%   [EVAL] batch:  168 | acc: 25.00%,  total acc: 66.16%   [EVAL] batch:  169 | acc: 12.50%,  total acc: 65.85%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 65.68%   [EVAL] batch:  171 | acc: 6.25%,  total acc: 65.33%   [EVAL] batch:  172 | acc: 18.75%,  total acc: 65.07%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 64.98%   [EVAL] batch:  174 | acc: 31.25%,  total acc: 64.79%   [EVAL] batch:  175 | acc: 43.75%,  total acc: 64.67%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 64.65%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 64.68%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 64.66%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 64.65%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 64.68%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 64.73%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 64.89%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 65.08%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 65.20%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 65.39%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 65.54%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 65.53%   [EVAL] batch:  188 | acc: 12.50%,  total acc: 65.24%   [EVAL] batch:  189 | acc: 37.50%,  total acc: 65.10%   [EVAL] batch:  190 | acc: 31.25%,  total acc: 64.92%   [EVAL] batch:  191 | acc: 12.50%,  total acc: 64.65%   [EVAL] batch:  192 | acc: 31.25%,  total acc: 64.48%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 64.24%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 64.26%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 64.19%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 64.24%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 64.30%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 64.35%   [EVAL] batch:  199 | acc: 68.75%,  total acc: 64.38%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 64.40%   [EVAL] batch:  201 | acc: 37.50%,  total acc: 64.26%   [EVAL] batch:  202 | acc: 50.00%,  total acc: 64.19%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 64.15%   [EVAL] batch:  204 | acc: 43.75%,  total acc: 64.05%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 64.08%   [EVAL] batch:  206 | acc: 81.25%,  total acc: 64.16%   [EVAL] batch:  207 | acc: 93.75%,  total acc: 64.30%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 64.47%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 64.61%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 64.75%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 64.92%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 65.05%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 65.19%   [EVAL] batch:  214 | acc: 93.75%,  total acc: 65.32%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 65.48%   [EVAL] batch:  216 | acc: 93.75%,  total acc: 65.61%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 65.77%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 65.90%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 66.05%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 66.20%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 66.36%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 66.48%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 66.60%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 66.75%   [EVAL] batch:  225 | acc: 43.75%,  total acc: 66.65%   [EVAL] batch:  226 | acc: 43.75%,  total acc: 66.55%   [EVAL] batch:  227 | acc: 56.25%,  total acc: 66.50%   [EVAL] batch:  228 | acc: 68.75%,  total acc: 66.51%   [EVAL] batch:  229 | acc: 43.75%,  total acc: 66.41%   [EVAL] batch:  230 | acc: 50.00%,  total acc: 66.34%   [EVAL] batch:  231 | acc: 81.25%,  total acc: 66.41%   [EVAL] batch:  232 | acc: 81.25%,  total acc: 66.47%   [EVAL] batch:  233 | acc: 87.50%,  total acc: 66.56%   [EVAL] batch:  234 | acc: 87.50%,  total acc: 66.65%   [EVAL] batch:  235 | acc: 75.00%,  total acc: 66.68%   [EVAL] batch:  236 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 66.65%   [EVAL] batch:  238 | acc: 62.50%,  total acc: 66.63%   [EVAL] batch:  239 | acc: 37.50%,  total acc: 66.51%   [EVAL] batch:  240 | acc: 56.25%,  total acc: 66.47%   [EVAL] batch:  241 | acc: 68.75%,  total acc: 66.48%   [EVAL] batch:  242 | acc: 43.75%,  total acc: 66.38%   [EVAL] batch:  243 | acc: 31.25%,  total acc: 66.24%   [EVAL] batch:  244 | acc: 56.25%,  total acc: 66.20%   [EVAL] batch:  245 | acc: 56.25%,  total acc: 66.16%   [EVAL] batch:  246 | acc: 50.00%,  total acc: 66.09%   [EVAL] batch:  247 | acc: 68.75%,  total acc: 66.10%   [EVAL] batch:  248 | acc: 81.25%,  total acc: 66.16%   [EVAL] batch:  249 | acc: 43.75%,  total acc: 66.07%   [EVAL] batch:  250 | acc: 0.00%,  total acc: 65.81%   [EVAL] batch:  251 | acc: 6.25%,  total acc: 65.58%   [EVAL] batch:  252 | acc: 0.00%,  total acc: 65.32%   [EVAL] batch:  253 | acc: 12.50%,  total acc: 65.11%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 64.85%   [EVAL] batch:  255 | acc: 6.25%,  total acc: 64.62%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 64.64%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 64.73%   [EVAL] batch:  258 | acc: 100.00%,  total acc: 64.86%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 64.98%   [EVAL] batch:  260 | acc: 93.75%,  total acc: 65.09%   [EVAL] batch:  261 | acc: 93.75%,  total acc: 65.20%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 65.28%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 65.25%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 65.31%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 65.37%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 65.40%   [EVAL] batch:  267 | acc: 75.00%,  total acc: 65.44%   [EVAL] batch:  268 | acc: 56.25%,  total acc: 65.40%   [EVAL] batch:  269 | acc: 43.75%,  total acc: 65.32%   [EVAL] batch:  270 | acc: 31.25%,  total acc: 65.20%   [EVAL] batch:  271 | acc: 43.75%,  total acc: 65.12%   [EVAL] batch:  272 | acc: 25.00%,  total acc: 64.97%   [EVAL] batch:  273 | acc: 18.75%,  total acc: 64.80%   [EVAL] batch:  274 | acc: 37.50%,  total acc: 64.70%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 64.49%   [EVAL] batch:  276 | acc: 18.75%,  total acc: 64.33%   [EVAL] batch:  277 | acc: 18.75%,  total acc: 64.16%   [EVAL] batch:  278 | acc: 6.25%,  total acc: 63.96%   [EVAL] batch:  279 | acc: 12.50%,  total acc: 63.77%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 63.55%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 63.50%   [EVAL] batch:  282 | acc: 81.25%,  total acc: 63.56%   [EVAL] batch:  283 | acc: 93.75%,  total acc: 63.67%   [EVAL] batch:  284 | acc: 93.75%,  total acc: 63.77%   [EVAL] batch:  285 | acc: 87.50%,  total acc: 63.85%   [EVAL] batch:  286 | acc: 81.25%,  total acc: 63.92%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 64.00%   [EVAL] batch:  288 | acc: 87.50%,  total acc: 64.08%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 64.20%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 64.28%   [EVAL] batch:  291 | acc: 75.00%,  total acc: 64.32%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 64.44%   [EVAL] batch:  293 | acc: 87.50%,  total acc: 64.52%   [EVAL] batch:  294 | acc: 37.50%,  total acc: 64.43%   [EVAL] batch:  295 | acc: 6.25%,  total acc: 64.23%   [EVAL] batch:  296 | acc: 25.00%,  total acc: 64.10%   [EVAL] batch:  297 | acc: 18.75%,  total acc: 63.95%   [EVAL] batch:  298 | acc: 31.25%,  total acc: 63.84%   [EVAL] batch:  299 | acc: 12.50%,  total acc: 63.67%   [EVAL] batch:  300 | acc: 43.75%,  total acc: 63.60%   [EVAL] batch:  301 | acc: 62.50%,  total acc: 63.60%   [EVAL] batch:  302 | acc: 75.00%,  total acc: 63.63%   [EVAL] batch:  303 | acc: 62.50%,  total acc: 63.63%   [EVAL] batch:  304 | acc: 75.00%,  total acc: 63.67%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 63.71%   [EVAL] batch:  306 | acc: 37.50%,  total acc: 63.62%   [EVAL] batch:  307 | acc: 56.25%,  total acc: 63.60%   [EVAL] batch:  308 | acc: 31.25%,  total acc: 63.49%   [EVAL] batch:  309 | acc: 75.00%,  total acc: 63.53%   [EVAL] batch:  310 | acc: 50.00%,  total acc: 63.48%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 63.54%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 63.50%   [EVAL] batch:  313 | acc: 50.00%,  total acc: 63.46%   [EVAL] batch:  314 | acc: 43.75%,  total acc: 63.39%   [EVAL] batch:  315 | acc: 43.75%,  total acc: 63.33%   [EVAL] batch:  316 | acc: 37.50%,  total acc: 63.25%   [EVAL] batch:  317 | acc: 81.25%,  total acc: 63.31%   [EVAL] batch:  318 | acc: 50.00%,  total acc: 63.26%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 63.38%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 63.49%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 63.55%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 63.64%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 63.73%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 63.81%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 63.82%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 63.80%   [EVAL] batch:  327 | acc: 81.25%,  total acc: 63.85%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 63.89%   [EVAL] batch:  329 | acc: 81.25%,  total acc: 63.94%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 63.95%   [EVAL] batch:  331 | acc: 81.25%,  total acc: 64.01%   [EVAL] batch:  332 | acc: 62.50%,  total acc: 64.00%   [EVAL] batch:  333 | acc: 93.75%,  total acc: 64.09%   [EVAL] batch:  334 | acc: 87.50%,  total acc: 64.16%   [EVAL] batch:  335 | acc: 93.75%,  total acc: 64.25%   [EVAL] batch:  336 | acc: 81.25%,  total acc: 64.30%   [EVAL] batch:  337 | acc: 56.25%,  total acc: 64.28%   [EVAL] batch:  338 | acc: 31.25%,  total acc: 64.18%   [EVAL] batch:  339 | acc: 25.00%,  total acc: 64.06%   [EVAL] batch:  340 | acc: 18.75%,  total acc: 63.93%   [EVAL] batch:  341 | acc: 31.25%,  total acc: 63.83%   [EVAL] batch:  342 | acc: 25.00%,  total acc: 63.72%   [EVAL] batch:  343 | acc: 25.00%,  total acc: 63.61%   [EVAL] batch:  344 | acc: 62.50%,  total acc: 63.61%   [EVAL] batch:  345 | acc: 43.75%,  total acc: 63.55%   [EVAL] batch:  346 | acc: 62.50%,  total acc: 63.54%   [EVAL] batch:  347 | acc: 62.50%,  total acc: 63.54%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 63.57%   [EVAL] batch:  349 | acc: 62.50%,  total acc: 63.57%   [EVAL] batch:  350 | acc: 62.50%,  total acc: 63.57%   [EVAL] batch:  351 | acc: 68.75%,  total acc: 63.58%   [EVAL] batch:  352 | acc: 56.25%,  total acc: 63.56%   [EVAL] batch:  353 | acc: 75.00%,  total acc: 63.59%   [EVAL] batch:  354 | acc: 68.75%,  total acc: 63.61%   [EVAL] batch:  355 | acc: 56.25%,  total acc: 63.59%   [EVAL] batch:  356 | acc: 62.50%,  total acc: 63.59%   [EVAL] batch:  357 | acc: 37.50%,  total acc: 63.51%   [EVAL] batch:  358 | acc: 37.50%,  total acc: 63.44%   [EVAL] batch:  359 | acc: 50.00%,  total acc: 63.40%   [EVAL] batch:  360 | acc: 37.50%,  total acc: 63.33%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 63.29%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 63.31%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 63.41%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 63.51%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 63.61%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 63.71%   [EVAL] batch:  367 | acc: 93.75%,  total acc: 63.79%   [EVAL] batch:  368 | acc: 93.75%,  total acc: 63.87%   [EVAL] batch:  369 | acc: 81.25%,  total acc: 63.92%   [EVAL] batch:  370 | acc: 62.50%,  total acc: 63.92%   [EVAL] batch:  371 | acc: 81.25%,  total acc: 63.96%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 64.02%   [EVAL] batch:  373 | acc: 75.00%,  total acc: 64.05%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 64.10%   [EVAL] batch:  375 | acc: 37.50%,  total acc: 64.03%   [EVAL] batch:  376 | acc: 75.00%,  total acc: 64.06%   [EVAL] batch:  377 | acc: 62.50%,  total acc: 64.05%   [EVAL] batch:  378 | acc: 56.25%,  total acc: 64.03%   [EVAL] batch:  379 | acc: 75.00%,  total acc: 64.06%   [EVAL] batch:  380 | acc: 68.75%,  total acc: 64.07%   [EVAL] batch:  381 | acc: 56.25%,  total acc: 64.05%   [EVAL] batch:  382 | acc: 68.75%,  total acc: 64.07%   [EVAL] batch:  383 | acc: 75.00%,  total acc: 64.10%   [EVAL] batch:  384 | acc: 62.50%,  total acc: 64.09%   [EVAL] batch:  385 | acc: 43.75%,  total acc: 64.04%   [EVAL] batch:  386 | acc: 62.50%,  total acc: 64.03%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 64.09%   [EVAL] batch:  388 | acc: 62.50%,  total acc: 64.09%   [EVAL] batch:  389 | acc: 62.50%,  total acc: 64.09%   [EVAL] batch:  390 | acc: 68.75%,  total acc: 64.10%   [EVAL] batch:  391 | acc: 75.00%,  total acc: 64.13%   [EVAL] batch:  392 | acc: 75.00%,  total acc: 64.15%   [EVAL] batch:  393 | acc: 100.00%,  total acc: 64.24%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 64.32%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 64.41%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 64.50%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 64.59%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 64.66%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 64.75%   [EVAL] batch:  400 | acc: 62.50%,  total acc: 64.74%   [EVAL] batch:  401 | acc: 43.75%,  total acc: 64.69%   [EVAL] batch:  402 | acc: 68.75%,  total acc: 64.70%   [EVAL] batch:  403 | acc: 75.00%,  total acc: 64.73%   [EVAL] batch:  404 | acc: 56.25%,  total acc: 64.71%   [EVAL] batch:  405 | acc: 81.25%,  total acc: 64.75%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 64.74%   [EVAL] batch:  407 | acc: 12.50%,  total acc: 64.61%   [EVAL] batch:  408 | acc: 25.00%,  total acc: 64.52%   [EVAL] batch:  409 | acc: 62.50%,  total acc: 64.51%   [EVAL] batch:  410 | acc: 25.00%,  total acc: 64.42%   [EVAL] batch:  411 | acc: 50.00%,  total acc: 64.38%   [EVAL] batch:  412 | acc: 43.75%,  total acc: 64.33%   [EVAL] batch:  413 | acc: 68.75%,  total acc: 64.34%   [EVAL] batch:  414 | acc: 81.25%,  total acc: 64.38%   [EVAL] batch:  415 | acc: 75.00%,  total acc: 64.41%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 64.46%   [EVAL] batch:  417 | acc: 75.00%,  total acc: 64.49%   [EVAL] batch:  418 | acc: 56.25%,  total acc: 64.47%   [EVAL] batch:  419 | acc: 87.50%,  total acc: 64.52%   [EVAL] batch:  420 | acc: 87.50%,  total acc: 64.58%   [EVAL] batch:  421 | acc: 81.25%,  total acc: 64.62%   [EVAL] batch:  422 | acc: 81.25%,  total acc: 64.66%   [EVAL] batch:  423 | acc: 87.50%,  total acc: 64.71%   [EVAL] batch:  424 | acc: 87.50%,  total acc: 64.76%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 64.85%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 64.93%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 65.01%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 65.09%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 65.17%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 65.26%   [EVAL] batch:  431 | acc: 100.00%,  total acc: 65.34%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 65.39%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 65.45%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 65.53%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 65.58%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 65.66%   [EVAL] batch:  437 | acc: 100.00%,  total acc: 65.74%   [EVAL] batch:  438 | acc: 93.75%,  total acc: 65.80%   [EVAL] batch:  439 | acc: 100.00%,  total acc: 65.88%   [EVAL] batch:  440 | acc: 100.00%,  total acc: 65.96%   [EVAL] batch:  441 | acc: 81.25%,  total acc: 65.99%   [EVAL] batch:  442 | acc: 100.00%,  total acc: 66.07%   [EVAL] batch:  443 | acc: 100.00%,  total acc: 66.15%   [EVAL] batch:  444 | acc: 93.75%,  total acc: 66.21%   [EVAL] batch:  445 | acc: 93.75%,  total acc: 66.27%   [EVAL] batch:  446 | acc: 87.50%,  total acc: 66.32%   [EVAL] batch:  447 | acc: 93.75%,  total acc: 66.38%   [EVAL] batch:  448 | acc: 93.75%,  total acc: 66.44%   [EVAL] batch:  449 | acc: 93.75%,  total acc: 66.50%   [EVAL] batch:  450 | acc: 87.50%,  total acc: 66.55%   [EVAL] batch:  451 | acc: 87.50%,  total acc: 66.59%   [EVAL] batch:  452 | acc: 87.50%,  total acc: 66.64%   [EVAL] batch:  453 | acc: 68.75%,  total acc: 66.64%   [EVAL] batch:  454 | acc: 81.25%,  total acc: 66.68%   [EVAL] batch:  455 | acc: 75.00%,  total acc: 66.69%   [EVAL] batch:  456 | acc: 75.00%,  total acc: 66.71%   [EVAL] batch:  457 | acc: 68.75%,  total acc: 66.72%   [EVAL] batch:  458 | acc: 62.50%,  total acc: 66.71%   [EVAL] batch:  459 | acc: 56.25%,  total acc: 66.68%   [EVAL] batch:  460 | acc: 75.00%,  total acc: 66.70%   [EVAL] batch:  461 | acc: 37.50%,  total acc: 66.64%   [EVAL] batch:  462 | acc: 75.00%,  total acc: 66.66%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 66.73%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 66.80%   [EVAL] batch:  465 | acc: 100.00%,  total acc: 66.87%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 66.94%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 67.01%   [EVAL] batch:  468 | acc: 93.75%,  total acc: 67.07%   [EVAL] batch:  469 | acc: 56.25%,  total acc: 67.05%   [EVAL] batch:  470 | acc: 87.50%,  total acc: 67.09%   [EVAL] batch:  471 | acc: 75.00%,  total acc: 67.11%   [EVAL] batch:  472 | acc: 56.25%,  total acc: 67.09%   [EVAL] batch:  473 | acc: 75.00%,  total acc: 67.10%   [EVAL] batch:  474 | acc: 37.50%,  total acc: 67.04%   [EVAL] batch:  475 | acc: 100.00%,  total acc: 67.11%   [EVAL] batch:  476 | acc: 100.00%,  total acc: 67.18%   [EVAL] batch:  477 | acc: 100.00%,  total acc: 67.25%   [EVAL] batch:  478 | acc: 100.00%,  total acc: 67.31%   [EVAL] batch:  479 | acc: 100.00%,  total acc: 67.38%   [EVAL] batch:  480 | acc: 93.75%,  total acc: 67.44%   [EVAL] batch:  481 | acc: 100.00%,  total acc: 67.51%   [EVAL] batch:  482 | acc: 100.00%,  total acc: 67.57%   [EVAL] batch:  483 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:  484 | acc: 100.00%,  total acc: 67.71%   [EVAL] batch:  485 | acc: 100.00%,  total acc: 67.77%   [EVAL] batch:  486 | acc: 100.00%,  total acc: 67.84%   [EVAL] batch:  487 | acc: 93.75%,  total acc: 67.89%   [EVAL] batch:  488 | acc: 87.50%,  total acc: 67.93%   [EVAL] batch:  489 | acc: 93.75%,  total acc: 67.98%   [EVAL] batch:  490 | acc: 100.00%,  total acc: 68.05%   [EVAL] batch:  491 | acc: 93.75%,  total acc: 68.10%   [EVAL] batch:  492 | acc: 93.75%,  total acc: 68.15%   [EVAL] batch:  493 | acc: 87.50%,  total acc: 68.19%   [EVAL] batch:  494 | acc: 93.75%,  total acc: 68.24%   [EVAL] batch:  495 | acc: 87.50%,  total acc: 68.28%   [EVAL] batch:  496 | acc: 93.75%,  total acc: 68.34%   [EVAL] batch:  497 | acc: 100.00%,  total acc: 68.40%   [EVAL] batch:  498 | acc: 93.75%,  total acc: 68.45%   [EVAL] batch:  499 | acc: 93.75%,  total acc: 68.50%   
cur_acc:  ['0.9514', '0.8065', '0.7708', '0.7758', '0.6310', '0.7004', '0.7460', '0.8740']
his_acc:  ['0.9514', '0.8765', '0.8205', '0.7558', '0.7057', '0.6927', '0.6694', '0.6850']
----------END
his_acc mean:  [0.9506 0.8602 0.8095 0.774  0.7516 0.72   0.6966 0.6766]
