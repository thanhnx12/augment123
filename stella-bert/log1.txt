#############params############
cuda:0
Task=Tacred, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=0, gen_num=0
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 12.6868801CurrentTrain: epoch  0, batch     1 | loss: 12.3284225CurrentTrain: epoch  0, batch     2 | loss: 12.6399384CurrentTrain: epoch  0, batch     3 | loss: 12.3410149CurrentTrain: epoch  0, batch     4 | loss: 11.9628611CurrentTrain: epoch  0, batch     5 | loss: 12.0147495CurrentTrain: epoch  0, batch     6 | loss: 11.7886543CurrentTrain: epoch  0, batch     7 | loss: 11.7825146CurrentTrain: epoch  0, batch     8 | loss: 11.8033218CurrentTrain: epoch  0, batch     9 | loss: 11.8507462CurrentTrain: epoch  0, batch    10 | loss: 11.7416553CurrentTrain: epoch  0, batch    11 | loss: 12.1967278CurrentTrain: epoch  0, batch    12 | loss: 11.1010151CurrentTrain: epoch  0, batch    13 | loss: 12.1381187CurrentTrain: epoch  0, batch    14 | loss: 11.0833435CurrentTrain: epoch  0, batch    15 | loss: 11.1755600CurrentTrain: epoch  0, batch    16 | loss: 11.4854212CurrentTrain: epoch  0, batch    17 | loss: 11.8237228CurrentTrain: epoch  0, batch    18 | loss: 11.7619848CurrentTrain: epoch  0, batch    19 | loss: 12.8745337CurrentTrain: epoch  0, batch    20 | loss: 11.4434090CurrentTrain: epoch  0, batch    21 | loss: 11.4850512CurrentTrain: epoch  0, batch    22 | loss: 11.1343403CurrentTrain: epoch  0, batch    23 | loss: 11.5730476CurrentTrain: epoch  0, batch    24 | loss: 11.1324615CurrentTrain: epoch  0, batch    25 | loss: 11.3984261CurrentTrain: epoch  0, batch    26 | loss: 10.8826399CurrentTrain: epoch  0, batch    27 | loss: 10.8928537CurrentTrain: epoch  0, batch    28 | loss: 10.8422508CurrentTrain: epoch  0, batch    29 | loss: 10.9917717CurrentTrain: epoch  0, batch    30 | loss: 10.8790417CurrentTrain: epoch  0, batch    31 | loss: 11.0727053CurrentTrain: epoch  0, batch    32 | loss: 10.8625965CurrentTrain: epoch  0, batch    33 | loss: 10.7673225CurrentTrain: epoch  0, batch    34 | loss: 11.0082016CurrentTrain: epoch  0, batch    35 | loss: 10.6078510CurrentTrain: epoch  0, batch    36 | loss: 10.6632729CurrentTrain: epoch  0, batch    37 | loss: 11.0432281CurrentTrain: epoch  1, batch     0 | loss: 11.2061481CurrentTrain: epoch  1, batch     1 | loss: 11.3327770CurrentTrain: epoch  1, batch     2 | loss: 10.9771757CurrentTrain: epoch  1, batch     3 | loss: 11.1972885CurrentTrain: epoch  1, batch     4 | loss: 11.0947857CurrentTrain: epoch  1, batch     5 | loss: 11.4491463CurrentTrain: epoch  1, batch     6 | loss: 11.9590139CurrentTrain: epoch  1, batch     7 | loss: 12.4418039CurrentTrain: epoch  1, batch     8 | loss: 12.3661976CurrentTrain: epoch  1, batch     9 | loss: 11.8485050CurrentTrain: epoch  1, batch    10 | loss: 11.7084904CurrentTrain: epoch  1, batch    11 | loss: 11.7668896CurrentTrain: epoch  1, batch    12 | loss: 11.3768997CurrentTrain: epoch  1, batch    13 | loss: 10.9297066CurrentTrain: epoch  1, batch    14 | loss: 10.8229523CurrentTrain: epoch  1, batch    15 | loss: 10.7143135CurrentTrain: epoch  1, batch    16 | loss: 10.8570843CurrentTrain: epoch  1, batch    17 | loss: 10.8320961CurrentTrain: epoch  1, batch    18 | loss: 11.0990000CurrentTrain: epoch  1, batch    19 | loss: 10.9951649CurrentTrain: epoch  1, batch    20 | loss: 11.1774559CurrentTrain: epoch  1, batch    21 | loss: 11.1780424CurrentTrain: epoch  1, batch    22 | loss: 10.9721422CurrentTrain: epoch  1, batch    23 | loss: 10.7853947CurrentTrain: epoch  1, batch    24 | loss: 11.3535099CurrentTrain: epoch  1, batch    25 | loss: 10.8444481CurrentTrain: epoch  1, batch    26 | loss: 11.1756725CurrentTrain: epoch  1, batch    27 | loss: 11.5021687CurrentTrain: epoch  1, batch    28 | loss: 11.2823172CurrentTrain: epoch  1, batch    29 | loss: 10.8193979CurrentTrain: epoch  1, batch    30 | loss: 10.6825466CurrentTrain: epoch  1, batch    31 | loss: 11.3603287CurrentTrain: epoch  1, batch    32 | loss: 10.8956108CurrentTrain: epoch  1, batch    33 | loss: 10.8615789CurrentTrain: epoch  1, batch    34 | loss: 10.9772072CurrentTrain: epoch  1, batch    35 | loss: 11.0621395CurrentTrain: epoch  1, batch    36 | loss: 11.0985126CurrentTrain: epoch  1, batch    37 | loss: 11.2966442CurrentTrain: epoch  2, batch     0 | loss: 11.1383028CurrentTrain: epoch  2, batch     1 | loss: 10.8566570CurrentTrain: epoch  2, batch     2 | loss: 11.0649281CurrentTrain: epoch  2, batch     3 | loss: 10.7288132CurrentTrain: epoch  2, batch     4 | loss: 10.7897520CurrentTrain: epoch  2, batch     5 | loss: 10.8266697CurrentTrain: epoch  2, batch     6 | loss: 11.2260761CurrentTrain: epoch  2, batch     7 | loss: 11.2158966CurrentTrain: epoch  2, batch     8 | loss: 11.5593605CurrentTrain: epoch  2, batch     9 | loss: 11.6950808CurrentTrain: epoch  2, batch    10 | loss: 11.4477119CurrentTrain: epoch  2, batch    11 | loss: 11.8348637CurrentTrain: epoch  2, batch    12 | loss: 10.6695242CurrentTrain: epoch  2, batch    13 | loss: 11.8621788CurrentTrain: epoch  2, batch    14 | loss: 11.0416670CurrentTrain: epoch  2, batch    15 | loss: 11.2487469CurrentTrain: epoch  2, batch    16 | loss: 10.8613110CurrentTrain: epoch  2, batch    17 | loss: 10.7760019CurrentTrain: epoch  2, batch    18 | loss: 11.0110588CurrentTrain: epoch  2, batch    19 | loss: 10.7249441CurrentTrain: epoch  2, batch    20 | loss: 10.4169521CurrentTrain: epoch  2, batch    21 | loss: 10.7793884CurrentTrain: epoch  2, batch    22 | loss: 10.7806387CurrentTrain: epoch  2, batch    23 | loss: 10.5851612CurrentTrain: epoch  2, batch    24 | loss: 10.6288586CurrentTrain: epoch  2, batch    25 | loss: 10.4982109CurrentTrain: epoch  2, batch    26 | loss: 10.6930618CurrentTrain: epoch  2, batch    27 | loss: 10.5055599CurrentTrain: epoch  2, batch    28 | loss: 10.8598213CurrentTrain: epoch  2, batch    29 | loss: 10.5557594CurrentTrain: epoch  2, batch    30 | loss: 10.3835068CurrentTrain: epoch  2, batch    31 | loss: 10.4496737CurrentTrain: epoch  2, batch    32 | loss: 10.8686829CurrentTrain: epoch  2, batch    33 | loss: 10.5534792CurrentTrain: epoch  2, batch    34 | loss: 10.3607979CurrentTrain: epoch  2, batch    35 | loss: 10.3919802CurrentTrain: epoch  2, batch    36 | loss: 10.5959816CurrentTrain: epoch  2, batch    37 | loss: 10.1590061CurrentTrain: epoch  3, batch     0 | loss: 10.5922813CurrentTrain: epoch  3, batch     1 | loss: 10.4672527CurrentTrain: epoch  3, batch     2 | loss: 10.4869595CurrentTrain: epoch  3, batch     3 | loss: 10.2172651CurrentTrain: epoch  3, batch     4 | loss: 10.0863438CurrentTrain: epoch  3, batch     5 | loss: 10.0720015CurrentTrain: epoch  3, batch     6 | loss: 10.0305481CurrentTrain: epoch  3, batch     7 | loss: 10.3600578CurrentTrain: epoch  3, batch     8 | loss: 10.3164921CurrentTrain: epoch  3, batch     9 | loss: 10.4717216CurrentTrain: epoch  3, batch    10 | loss: 10.5016270CurrentTrain: epoch  3, batch    11 | loss: 10.5259514CurrentTrain: epoch  3, batch    12 | loss: 10.2256212CurrentTrain: epoch  3, batch    13 | loss: 10.3140507CurrentTrain: epoch  3, batch    14 | loss: 10.2736044CurrentTrain: epoch  3, batch    15 | loss: 10.1325197CurrentTrain: epoch  3, batch    16 | loss: 9.9732876CurrentTrain: epoch  3, batch    17 | loss: 9.8402843CurrentTrain: epoch  3, batch    18 | loss: 10.0414848CurrentTrain: epoch  3, batch    19 | loss: 9.9164553CurrentTrain: epoch  3, batch    20 | loss: 9.8690968CurrentTrain: epoch  3, batch    21 | loss: 10.1082230CurrentTrain: epoch  3, batch    22 | loss: 9.3024483CurrentTrain: epoch  3, batch    23 | loss: 9.3036060CurrentTrain: epoch  3, batch    24 | loss: 9.7673626CurrentTrain: epoch  3, batch    25 | loss: 9.8505917CurrentTrain: epoch  3, batch    26 | loss: 9.4684162CurrentTrain: epoch  3, batch    27 | loss: 9.6808205CurrentTrain: epoch  3, batch    28 | loss: 9.6137438CurrentTrain: epoch  3, batch    29 | loss: 9.5883961CurrentTrain: epoch  3, batch    30 | loss: 9.3264427CurrentTrain: epoch  3, batch    31 | loss: 9.5966444CurrentTrain: epoch  3, batch    32 | loss: 9.6541700CurrentTrain: epoch  3, batch    33 | loss: 9.3753052CurrentTrain: epoch  3, batch    34 | loss: 9.1909094CurrentTrain: epoch  3, batch    35 | loss: 9.5045938CurrentTrain: epoch  3, batch    36 | loss: 9.4409847CurrentTrain: epoch  3, batch    37 | loss: 10.3006859CurrentTrain: epoch  4, batch     0 | loss: 9.3144894CurrentTrain: epoch  4, batch     1 | loss: 8.7705536CurrentTrain: epoch  4, batch     2 | loss: 9.1952963CurrentTrain: epoch  4, batch     3 | loss: 9.4308100CurrentTrain: epoch  4, batch     4 | loss: 9.7217207CurrentTrain: epoch  4, batch     5 | loss: 8.9840698CurrentTrain: epoch  4, batch     6 | loss: 9.4996586CurrentTrain: epoch  4, batch     7 | loss: 8.9564180CurrentTrain: epoch  4, batch     8 | loss: 8.8554134CurrentTrain: epoch  4, batch     9 | loss: 8.8633785CurrentTrain: epoch  4, batch    10 | loss: 8.6825619CurrentTrain: epoch  4, batch    11 | loss: 9.0315228CurrentTrain: epoch  4, batch    12 | loss: 8.3481922CurrentTrain: epoch  4, batch    13 | loss: 9.2005329CurrentTrain: epoch  4, batch    14 | loss: 8.4954729CurrentTrain: epoch  4, batch    15 | loss: 8.1371012CurrentTrain: epoch  4, batch    16 | loss: 8.2220678CurrentTrain: epoch  4, batch    17 | loss: 8.8878546CurrentTrain: epoch  4, batch    18 | loss: 8.9639053CurrentTrain: epoch  4, batch    19 | loss: 8.8683662CurrentTrain: epoch  4, batch    20 | loss: 8.1983700CurrentTrain: epoch  4, batch    21 | loss: 8.9007893CurrentTrain: epoch  4, batch    22 | loss: 8.3367205CurrentTrain: epoch  4, batch    23 | loss: 8.2499447CurrentTrain: epoch  4, batch    24 | loss: 8.3166103CurrentTrain: epoch  4, batch    25 | loss: 7.8662987CurrentTrain: epoch  4, batch    26 | loss: 8.1415005CurrentTrain: epoch  4, batch    27 | loss: 8.2334528CurrentTrain: epoch  4, batch    28 | loss: 7.9557867CurrentTrain: epoch  4, batch    29 | loss: 7.7910280CurrentTrain: epoch  4, batch    30 | loss: 8.5292320CurrentTrain: epoch  4, batch    31 | loss: 7.9335403CurrentTrain: epoch  4, batch    32 | loss: 7.5240316CurrentTrain: epoch  4, batch    33 | loss: 8.0295916CurrentTrain: epoch  4, batch    34 | loss: 7.5478678CurrentTrain: epoch  4, batch    35 | loss: 7.6103191CurrentTrain: epoch  4, batch    36 | loss: 7.5547981CurrentTrain: epoch  4, batch    37 | loss: 7.6955929
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 14.7368879CurrentTrain: epoch  0, batch     1 | loss: 14.5718737CurrentTrain: epoch  0, batch     2 | loss: 14.5481615CurrentTrain: epoch  0, batch     3 | loss: 14.4124489CurrentTrain: epoch  0, batch     4 | loss: 14.3723211CurrentTrain: epoch  0, batch     5 | loss: 14.1291981CurrentTrain: epoch  0, batch     6 | loss: 14.3631191CurrentTrain: epoch  0, batch     7 | loss: 13.8891077CurrentTrain: epoch  0, batch     8 | loss: 14.0803299CurrentTrain: epoch  0, batch     9 | loss: 13.5279064CurrentTrain: epoch  0, batch    10 | loss: 13.2762499CurrentTrain: epoch  0, batch    11 | loss: 13.8742352CurrentTrain: epoch  0, batch    12 | loss: 13.2730770CurrentTrain: epoch  0, batch    13 | loss: 13.9560127CurrentTrain: epoch  0, batch    14 | loss: 13.8578711CurrentTrain: epoch  0, batch    15 | loss: 12.6809797CurrentTrain: epoch  0, batch    16 | loss: 13.1466331CurrentTrain: epoch  0, batch    17 | loss: 13.0149364CurrentTrain: epoch  0, batch    18 | loss: 13.3541508CurrentTrain: epoch  0, batch    19 | loss: 12.1914711CurrentTrain: epoch  0, batch    20 | loss: 12.5876350CurrentTrain: epoch  0, batch    21 | loss: 12.7499475CurrentTrain: epoch  0, batch    22 | loss: 12.9584923CurrentTrain: epoch  0, batch    23 | loss: 11.6788902CurrentTrain: epoch  0, batch    24 | loss: 12.7888384CurrentTrain: epoch  0, batch    25 | loss: 13.1327085CurrentTrain: epoch  0, batch    26 | loss: 11.9577761CurrentTrain: epoch  0, batch    27 | loss: 11.7177820CurrentTrain: epoch  0, batch    28 | loss: 11.8641710CurrentTrain: epoch  0, batch    29 | loss: 12.2843046CurrentTrain: epoch  0, batch    30 | loss: 12.2543354CurrentTrain: epoch  0, batch    31 | loss: 12.0732021CurrentTrain: epoch  0, batch    32 | loss: 12.2850170CurrentTrain: epoch  0, batch    33 | loss: 10.7272301CurrentTrain: epoch  0, batch    34 | loss: 11.6805420CurrentTrain: epoch  0, batch    35 | loss: 11.4758043CurrentTrain: epoch  0, batch    36 | loss: 11.6230202CurrentTrain: epoch  0, batch    37 | loss: 11.4202404CurrentTrain: epoch  1, batch     0 | loss: 11.4286194CurrentTrain: epoch  1, batch     1 | loss: 11.9896049CurrentTrain: epoch  1, batch     2 | loss: 10.9353848CurrentTrain: epoch  1, batch     3 | loss: 11.6370907CurrentTrain: epoch  1, batch     4 | loss: 10.8650532CurrentTrain: epoch  1, batch     5 | loss: 11.3397903CurrentTrain: epoch  1, batch     6 | loss: 11.1385765CurrentTrain: epoch  1, batch     7 | loss: 10.1250801CurrentTrain: epoch  1, batch     8 | loss: 10.6873093CurrentTrain: epoch  1, batch     9 | loss: 10.9849920CurrentTrain: epoch  1, batch    10 | loss: 10.8212147CurrentTrain: epoch  1, batch    11 | loss: 10.5993233CurrentTrain: epoch  1, batch    12 | loss: 11.2071323CurrentTrain: epoch  1, batch    13 | loss: 10.4669199CurrentTrain: epoch  1, batch    14 | loss: 9.3033476CurrentTrain: epoch  1, batch    15 | loss: 10.1358175CurrentTrain: epoch  1, batch    16 | loss: 10.1803274CurrentTrain: epoch  1, batch    17 | loss: 10.2714767CurrentTrain: epoch  1, batch    18 | loss: 10.2592049CurrentTrain: epoch  1, batch    19 | loss: 9.9214602CurrentTrain: epoch  1, batch    20 | loss: 10.0782862CurrentTrain: epoch  1, batch    21 | loss: 10.2861624CurrentTrain: epoch  1, batch    22 | loss: 10.7330284CurrentTrain: epoch  1, batch    23 | loss: 9.4009352CurrentTrain: epoch  1, batch    24 | loss: 10.4694386CurrentTrain: epoch  1, batch    25 | loss: 10.6004705CurrentTrain: epoch  1, batch    26 | loss: 9.9940929CurrentTrain: epoch  1, batch    27 | loss: 9.5555058CurrentTrain: epoch  1, batch    28 | loss: 10.3824444CurrentTrain: epoch  1, batch    29 | loss: 8.3982611CurrentTrain: epoch  1, batch    30 | loss: 9.2830801CurrentTrain: epoch  1, batch    31 | loss: 9.3337822CurrentTrain: epoch  1, batch    32 | loss: 10.0476532CurrentTrain: epoch  1, batch    33 | loss: 9.3612251CurrentTrain: epoch  1, batch    34 | loss: 8.7647886CurrentTrain: epoch  1, batch    35 | loss: 9.0175781CurrentTrain: epoch  1, batch    36 | loss: 8.0457125CurrentTrain: epoch  1, batch    37 | loss: 8.0814095CurrentTrain: epoch  2, batch     0 | loss: 8.9973841CurrentTrain: epoch  2, batch     1 | loss: 9.3538752CurrentTrain: epoch  2, batch     2 | loss: 9.5737839CurrentTrain: epoch  2, batch     3 | loss: 8.7892580CurrentTrain: epoch  2, batch     4 | loss: 9.6196594CurrentTrain: epoch  2, batch     5 | loss: 9.0085897CurrentTrain: epoch  2, batch     6 | loss: 7.9268456CurrentTrain: epoch  2, batch     7 | loss: 8.3396540CurrentTrain: epoch  2, batch     8 | loss: 8.8899307CurrentTrain: epoch  2, batch     9 | loss: 8.3130112CurrentTrain: epoch  2, batch    10 | loss: 8.6858854CurrentTrain: epoch  2, batch    11 | loss: 8.0738831CurrentTrain: epoch  2, batch    12 | loss: 7.9842944CurrentTrain: epoch  2, batch    13 | loss: 8.1623964CurrentTrain: epoch  2, batch    14 | loss: 8.6884727CurrentTrain: epoch  2, batch    15 | loss: 10.3043594CurrentTrain: epoch  2, batch    16 | loss: 7.8704963CurrentTrain: epoch  2, batch    17 | loss: 10.2263269CurrentTrain: epoch  2, batch    18 | loss: 8.0362988CurrentTrain: epoch  2, batch    19 | loss: 8.6025801CurrentTrain: epoch  2, batch    20 | loss: 9.0551777CurrentTrain: epoch  2, batch    21 | loss: 7.3048744CurrentTrain: epoch  2, batch    22 | loss: 9.4473934CurrentTrain: epoch  2, batch    23 | loss: 8.8557701CurrentTrain: epoch  2, batch    24 | loss: 8.4075909CurrentTrain: epoch  2, batch    25 | loss: 8.9487762CurrentTrain: epoch  2, batch    26 | loss: 9.0867739CurrentTrain: epoch  2, batch    27 | loss: 9.8627691CurrentTrain: epoch  2, batch    28 | loss: 9.3346071CurrentTrain: epoch  2, batch    29 | loss: 8.1673069CurrentTrain: epoch  2, batch    30 | loss: 8.6091280CurrentTrain: epoch  2, batch    31 | loss: 7.2037086CurrentTrain: epoch  2, batch    32 | loss: 7.4245901CurrentTrain: epoch  2, batch    33 | loss: 8.5648413CurrentTrain: epoch  2, batch    34 | loss: 7.9605246CurrentTrain: epoch  2, batch    35 | loss: 6.5667205CurrentTrain: epoch  2, batch    36 | loss: 8.0451307CurrentTrain: epoch  2, batch    37 | loss: 9.0053949CurrentTrain: epoch  3, batch     0 | loss: 8.6553583CurrentTrain: epoch  3, batch     1 | loss: 8.0441141CurrentTrain: epoch  3, batch     2 | loss: 6.1320481CurrentTrain: epoch  3, batch     3 | loss: 9.5157862CurrentTrain: epoch  3, batch     4 | loss: 8.1358776CurrentTrain: epoch  3, batch     5 | loss: 6.9704313CurrentTrain: epoch  3, batch     6 | loss: 7.4839015CurrentTrain: epoch  3, batch     7 | loss: 6.8665214CurrentTrain: epoch  3, batch     8 | loss: 6.7277985CurrentTrain: epoch  3, batch     9 | loss: 7.1991806CurrentTrain: epoch  3, batch    10 | loss: 6.7761645CurrentTrain: epoch  3, batch    11 | loss: 8.3156691CurrentTrain: epoch  3, batch    12 | loss: 8.9169483CurrentTrain: epoch  3, batch    13 | loss: 9.2786951CurrentTrain: epoch  3, batch    14 | loss: 7.7562819CurrentTrain: epoch  3, batch    15 | loss: 10.0021534CurrentTrain: epoch  3, batch    16 | loss: 8.5499172CurrentTrain: epoch  3, batch    17 | loss: 7.5265727CurrentTrain: epoch  3, batch    18 | loss: 7.7893529CurrentTrain: epoch  3, batch    19 | loss: 7.8707519CurrentTrain: epoch  3, batch    20 | loss: 7.9250202CurrentTrain: epoch  3, batch    21 | loss: 7.4356723CurrentTrain: epoch  3, batch    22 | loss: 7.2223701CurrentTrain: epoch  3, batch    23 | loss: 8.2812748CurrentTrain: epoch  3, batch    24 | loss: 7.2106862CurrentTrain: epoch  3, batch    25 | loss: 8.9521074CurrentTrain: epoch  3, batch    26 | loss: 7.3870554CurrentTrain: epoch  3, batch    27 | loss: 7.1396141CurrentTrain: epoch  3, batch    28 | loss: 7.0856028CurrentTrain: epoch  3, batch    29 | loss: 7.4771857CurrentTrain: epoch  3, batch    30 | loss: 7.2592387CurrentTrain: epoch  3, batch    31 | loss: 7.0790200CurrentTrain: epoch  3, batch    32 | loss: 9.1884003CurrentTrain: epoch  3, batch    33 | loss: 7.8228283CurrentTrain: epoch  3, batch    34 | loss: 6.6752295CurrentTrain: epoch  3, batch    35 | loss: 8.1399078CurrentTrain: epoch  3, batch    36 | loss: 7.8400517CurrentTrain: epoch  3, batch    37 | loss: 6.3437552CurrentTrain: epoch  4, batch     0 | loss: 7.5600796CurrentTrain: epoch  4, batch     1 | loss: 6.9925637CurrentTrain: epoch  4, batch     2 | loss: 6.9798775CurrentTrain: epoch  4, batch     3 | loss: 6.6076703CurrentTrain: epoch  4, batch     4 | loss: 6.9449711CurrentTrain: epoch  4, batch     5 | loss: 7.2612000CurrentTrain: epoch  4, batch     6 | loss: 6.9826388CurrentTrain: epoch  4, batch     7 | loss: 7.8340034CurrentTrain: epoch  4, batch     8 | loss: 7.6103339CurrentTrain: epoch  4, batch     9 | loss: 7.2786927CurrentTrain: epoch  4, batch    10 | loss: 6.8598151CurrentTrain: epoch  4, batch    11 | loss: 7.2162209CurrentTrain: epoch  4, batch    12 | loss: 7.7457371CurrentTrain: epoch  4, batch    13 | loss: 7.5039663CurrentTrain: epoch  4, batch    14 | loss: 8.0892830CurrentTrain: epoch  4, batch    15 | loss: 6.0442944CurrentTrain: epoch  4, batch    16 | loss: 7.0662794CurrentTrain: epoch  4, batch    17 | loss: 8.0630388CurrentTrain: epoch  4, batch    18 | loss: 6.7049437CurrentTrain: epoch  4, batch    19 | loss: 7.8579698CurrentTrain: epoch  4, batch    20 | loss: 7.6975899CurrentTrain: epoch  4, batch    21 | loss: 7.3229561CurrentTrain: epoch  4, batch    22 | loss: 7.4653897CurrentTrain: epoch  4, batch    23 | loss: 7.6777229CurrentTrain: epoch  4, batch    24 | loss: 6.5787554CurrentTrain: epoch  4, batch    25 | loss: 8.8257151CurrentTrain: epoch  4, batch    26 | loss: 9.4862862CurrentTrain: epoch  4, batch    27 | loss: 7.0184336CurrentTrain: epoch  4, batch    28 | loss: 6.3508620CurrentTrain: epoch  4, batch    29 | loss: 7.4616318CurrentTrain: epoch  4, batch    30 | loss: 8.8952188CurrentTrain: epoch  4, batch    31 | loss: 7.2372584CurrentTrain: epoch  4, batch    32 | loss: 6.8544912CurrentTrain: epoch  4, batch    33 | loss: 6.6655679CurrentTrain: epoch  4, batch    34 | loss: 6.4435501CurrentTrain: epoch  4, batch    35 | loss: 7.7667971CurrentTrain: epoch  4, batch    36 | loss: 8.4802885CurrentTrain: epoch  4, batch    37 | loss: 6.2292271CurrentTrain: epoch  5, batch     0 | loss: 6.8649125CurrentTrain: epoch  5, batch     1 | loss: 6.2129169CurrentTrain: epoch  5, batch     2 | loss: 6.6287994CurrentTrain: epoch  5, batch     3 | loss: 6.6629558CurrentTrain: epoch  5, batch     4 | loss: 7.0602598CurrentTrain: epoch  5, batch     5 | loss: 6.5626616CurrentTrain: epoch  5, batch     6 | loss: 7.3130684CurrentTrain: epoch  5, batch     7 | loss: 7.2620625CurrentTrain: epoch  5, batch     8 | loss: 6.1518917CurrentTrain: epoch  5, batch     9 | loss: 7.7717376CurrentTrain: epoch  5, batch    10 | loss: 7.7223744CurrentTrain: epoch  5, batch    11 | loss: 6.4108944CurrentTrain: epoch  5, batch    12 | loss: 6.3790355CurrentTrain: epoch  5, batch    13 | loss: 6.5297313CurrentTrain: epoch  5, batch    14 | loss: 8.6464996CurrentTrain: epoch  5, batch    15 | loss: 7.0910525CurrentTrain: epoch  5, batch    16 | loss: 6.4052038CurrentTrain: epoch  5, batch    17 | loss: 7.5722837CurrentTrain: epoch  5, batch    18 | loss: 6.0284681CurrentTrain: epoch  5, batch    19 | loss: 7.0281501CurrentTrain: epoch  5, batch    20 | loss: 6.3378782CurrentTrain: epoch  5, batch    21 | loss: 7.5089717CurrentTrain: epoch  5, batch    22 | loss: 7.4212093CurrentTrain: epoch  5, batch    23 | loss: 6.9267821CurrentTrain: epoch  5, batch    24 | loss: 7.4695110CurrentTrain: epoch  5, batch    25 | loss: 6.3110499CurrentTrain: epoch  5, batch    26 | loss: 6.1447825CurrentTrain: epoch  5, batch    27 | loss: 6.0402489CurrentTrain: epoch  5, batch    28 | loss: 6.1762800CurrentTrain: epoch  5, batch    29 | loss: 6.6062598CurrentTrain: epoch  5, batch    30 | loss: 8.2230453CurrentTrain: epoch  5, batch    31 | loss: 6.5164123CurrentTrain: epoch  5, batch    32 | loss: 6.8943090CurrentTrain: epoch  5, batch    33 | loss: 7.1086845CurrentTrain: epoch  5, batch    34 | loss: 5.8374681CurrentTrain: epoch  5, batch    35 | loss: 6.0128388CurrentTrain: epoch  5, batch    36 | loss: 7.0114312CurrentTrain: epoch  5, batch    37 | loss: 7.3745303CurrentTrain: epoch  6, batch     0 | loss: 5.9846835CurrentTrain: epoch  6, batch     1 | loss: 6.1860232CurrentTrain: epoch  6, batch     2 | loss: 5.0556378CurrentTrain: epoch  6, batch     3 | loss: 6.2915425CurrentTrain: epoch  6, batch     4 | loss: 6.0269957CurrentTrain: epoch  6, batch     5 | loss: 6.6619043CurrentTrain: epoch  6, batch     6 | loss: 6.5827804CurrentTrain: epoch  6, batch     7 | loss: 7.4727125CurrentTrain: epoch  6, batch     8 | loss: 5.9310813CurrentTrain: epoch  6, batch     9 | loss: 6.7941642CurrentTrain: epoch  6, batch    10 | loss: 5.8739696CurrentTrain: epoch  6, batch    11 | loss: 7.3668857CurrentTrain: epoch  6, batch    12 | loss: 7.0493865CurrentTrain: epoch  6, batch    13 | loss: 6.5927114CurrentTrain: epoch  6, batch    14 | loss: 6.2499151CurrentTrain: epoch  6, batch    15 | loss: 6.2816310CurrentTrain: epoch  6, batch    16 | loss: 6.1978936CurrentTrain: epoch  6, batch    17 | loss: 5.9071145CurrentTrain: epoch  6, batch    18 | loss: 5.4466572CurrentTrain: epoch  6, batch    19 | loss: 5.6079164CurrentTrain: epoch  6, batch    20 | loss: 5.7139120CurrentTrain: epoch  6, batch    21 | loss: 5.7871566CurrentTrain: epoch  6, batch    22 | loss: 5.9138641CurrentTrain: epoch  6, batch    23 | loss: 6.1625652CurrentTrain: epoch  6, batch    24 | loss: 6.2700505CurrentTrain: epoch  6, batch    25 | loss: 6.4216366CurrentTrain: epoch  6, batch    26 | loss: 6.5156021CurrentTrain: epoch  6, batch    27 | loss: 5.6221552CurrentTrain: epoch  6, batch    28 | loss: 6.3036232CurrentTrain: epoch  6, batch    29 | loss: 7.6068611CurrentTrain: epoch  6, batch    30 | loss: 8.0842743CurrentTrain: epoch  6, batch    31 | loss: 6.2151814CurrentTrain: epoch  6, batch    32 | loss: 6.4778028CurrentTrain: epoch  6, batch    33 | loss: 6.3633394CurrentTrain: epoch  6, batch    34 | loss: 7.4161773CurrentTrain: epoch  6, batch    35 | loss: 5.7027731CurrentTrain: epoch  6, batch    36 | loss: 7.3916450CurrentTrain: epoch  6, batch    37 | loss: 5.7245102CurrentTrain: epoch  7, batch     0 | loss: 5.6943789CurrentTrain: epoch  7, batch     1 | loss: 5.6511202CurrentTrain: epoch  7, batch     2 | loss: 6.0944595CurrentTrain: epoch  7, batch     3 | loss: 5.2941952CurrentTrain: epoch  7, batch     4 | loss: 7.0318913CurrentTrain: epoch  7, batch     5 | loss: 7.1968989CurrentTrain: epoch  7, batch     6 | loss: 5.5963039CurrentTrain: epoch  7, batch     7 | loss: 6.4429345CurrentTrain: epoch  7, batch     8 | loss: 6.0202012CurrentTrain: epoch  7, batch     9 | loss: 6.1608343CurrentTrain: epoch  7, batch    10 | loss: 5.6347799CurrentTrain: epoch  7, batch    11 | loss: 6.4533401CurrentTrain: epoch  7, batch    12 | loss: 6.2351694CurrentTrain: epoch  7, batch    13 | loss: 7.4985232CurrentTrain: epoch  7, batch    14 | loss: 5.9149580CurrentTrain: epoch  7, batch    15 | loss: 5.9170141CurrentTrain: epoch  7, batch    16 | loss: 5.8156571CurrentTrain: epoch  7, batch    17 | loss: 5.8104849CurrentTrain: epoch  7, batch    18 | loss: 6.0349932CurrentTrain: epoch  7, batch    19 | loss: 5.7987561CurrentTrain: epoch  7, batch    20 | loss: 5.4695516CurrentTrain: epoch  7, batch    21 | loss: 5.6538534CurrentTrain: epoch  7, batch    22 | loss: 6.9389958CurrentTrain: epoch  7, batch    23 | loss: 6.1427407CurrentTrain: epoch  7, batch    24 | loss: 6.2071075CurrentTrain: epoch  7, batch    25 | loss: 5.9988861CurrentTrain: epoch  7, batch    26 | loss: 5.7399325CurrentTrain: epoch  7, batch    27 | loss: 6.8103900CurrentTrain: epoch  7, batch    28 | loss: 5.8239989CurrentTrain: epoch  7, batch    29 | loss: 5.6639695CurrentTrain: epoch  7, batch    30 | loss: 5.9326482CurrentTrain: epoch  7, batch    31 | loss: 5.4832916CurrentTrain: epoch  7, batch    32 | loss: 5.1217675CurrentTrain: epoch  7, batch    33 | loss: 5.5196280CurrentTrain: epoch  7, batch    34 | loss: 6.6571603CurrentTrain: epoch  7, batch    35 | loss: 6.0554109CurrentTrain: epoch  7, batch    36 | loss: 5.4343815CurrentTrain: epoch  7, batch    37 | loss: 6.8397808CurrentTrain: epoch  8, batch     0 | loss: 6.0549173CurrentTrain: epoch  8, batch     1 | loss: 6.6178532CurrentTrain: epoch  8, batch     2 | loss: 6.2113314CurrentTrain: epoch  8, batch     3 | loss: 5.3406949CurrentTrain: epoch  8, batch     4 | loss: 5.6052427CurrentTrain: epoch  8, batch     5 | loss: 5.9546552CurrentTrain: epoch  8, batch     6 | loss: 5.7188601CurrentTrain: epoch  8, batch     7 | loss: 5.3266916CurrentTrain: epoch  8, batch     8 | loss: 5.8797407CurrentTrain: epoch  8, batch     9 | loss: 6.8064213CurrentTrain: epoch  8, batch    10 | loss: 5.8408833CurrentTrain: epoch  8, batch    11 | loss: 5.1877761CurrentTrain: epoch  8, batch    12 | loss: 5.9465265CurrentTrain: epoch  8, batch    13 | loss: 5.4018884CurrentTrain: epoch  8, batch    14 | loss: 6.1759582CurrentTrain: epoch  8, batch    15 | loss: 6.9051142CurrentTrain: epoch  8, batch    16 | loss: 5.4092760CurrentTrain: epoch  8, batch    17 | loss: 5.3889103CurrentTrain: epoch  8, batch    18 | loss: 5.3243971CurrentTrain: epoch  8, batch    19 | loss: 5.5403752CurrentTrain: epoch  8, batch    20 | loss: 5.2949629CurrentTrain: epoch  8, batch    21 | loss: 6.4271264CurrentTrain: epoch  8, batch    22 | loss: 5.2046738CurrentTrain: epoch  8, batch    23 | loss: 5.1203690CurrentTrain: epoch  8, batch    24 | loss: 5.1759882CurrentTrain: epoch  8, batch    25 | loss: 5.2822289CurrentTrain: epoch  8, batch    26 | loss: 5.9839611CurrentTrain: epoch  8, batch    27 | loss: 5.5355749CurrentTrain: epoch  8, batch    28 | loss: 5.9438515CurrentTrain: epoch  8, batch    29 | loss: 5.3238769CurrentTrain: epoch  8, batch    30 | loss: 6.1910167CurrentTrain: epoch  8, batch    31 | loss: 5.2045913CurrentTrain: epoch  8, batch    32 | loss: 6.6887441CurrentTrain: epoch  8, batch    33 | loss: 5.2018414CurrentTrain: epoch  8, batch    34 | loss: 5.0919638CurrentTrain: epoch  8, batch    35 | loss: 6.5701356CurrentTrain: epoch  8, batch    36 | loss: 5.6705675CurrentTrain: epoch  8, batch    37 | loss: 4.9184775CurrentTrain: epoch  9, batch     0 | loss: 6.3265409CurrentTrain: epoch  9, batch     1 | loss: 5.7617164CurrentTrain: epoch  9, batch     2 | loss: 5.7760162CurrentTrain: epoch  9, batch     3 | loss: 5.2301950CurrentTrain: epoch  9, batch     4 | loss: 5.0339608CurrentTrain: epoch  9, batch     5 | loss: 6.0537610CurrentTrain: epoch  9, batch     6 | loss: 5.3300796CurrentTrain: epoch  9, batch     7 | loss: 5.3219833CurrentTrain: epoch  9, batch     8 | loss: 5.6201916CurrentTrain: epoch  9, batch     9 | loss: 6.4450779CurrentTrain: epoch  9, batch    10 | loss: 5.1020966CurrentTrain: epoch  9, batch    11 | loss: 5.1921687CurrentTrain: epoch  9, batch    12 | loss: 4.9064031CurrentTrain: epoch  9, batch    13 | loss: 5.2476597CurrentTrain: epoch  9, batch    14 | loss: 5.5052514CurrentTrain: epoch  9, batch    15 | loss: 5.3996291CurrentTrain: epoch  9, batch    16 | loss: 5.4799304CurrentTrain: epoch  9, batch    17 | loss: 5.1393728CurrentTrain: epoch  9, batch    18 | loss: 5.1445136CurrentTrain: epoch  9, batch    19 | loss: 5.3853664CurrentTrain: epoch  9, batch    20 | loss: 5.3035192CurrentTrain: epoch  9, batch    21 | loss: 5.1977396CurrentTrain: epoch  9, batch    22 | loss: 5.2289805CurrentTrain: epoch  9, batch    23 | loss: 4.8411565CurrentTrain: epoch  9, batch    24 | loss: 5.1403975CurrentTrain: epoch  9, batch    25 | loss: 4.9355168CurrentTrain: epoch  9, batch    26 | loss: 5.7906847CurrentTrain: epoch  9, batch    27 | loss: 5.5949183CurrentTrain: epoch  9, batch    28 | loss: 5.3969355CurrentTrain: epoch  9, batch    29 | loss: 5.1186585CurrentTrain: epoch  9, batch    30 | loss: 4.7892852CurrentTrain: epoch  9, batch    31 | loss: 4.8439398CurrentTrain: epoch  9, batch    32 | loss: 5.4042444CurrentTrain: epoch  9, batch    33 | loss: 5.2724247CurrentTrain: epoch  9, batch    34 | loss: 5.0666819CurrentTrain: epoch  9, batch    35 | loss: 5.3477211CurrentTrain: epoch  9, batch    36 | loss: 5.3000684CurrentTrain: epoch  9, batch    37 | loss: 5.1391668
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 93.06%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 94.32%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 94.27%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 94.23%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 89.06%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 88.24%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 86.81%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 85.53%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 85.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 86.31%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 88.02%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 88.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 88.94%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 89.12%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 89.51%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 89.87%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 89.79%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 89.92%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 90.23%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 88.64%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 93.06%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 94.32%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 94.27%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 94.23%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 89.06%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 88.24%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 86.81%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 85.53%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 85.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 86.31%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 88.02%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 88.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 88.94%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 89.12%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 89.51%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 89.87%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 89.79%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 89.92%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 90.23%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 88.64%   
cur_acc_llm:  [0.8863636363636364]
his_acc_llm:  [0.8863636363636364]
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 82.89%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.33%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.02%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.27%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.15%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.12%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.74%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 82.89%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.33%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.02%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.27%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.15%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.12%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.74%   
cur_acc:  ['0.8674']
his_acc:  ['0.8674']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 7.0876293CurrentTrain: epoch  0, batch     1 | loss: 7.0959210CurrentTrain: epoch  1, batch     0 | loss: 6.3443155CurrentTrain: epoch  1, batch     1 | loss: 6.2421918CurrentTrain: epoch  2, batch     0 | loss: 5.6297312CurrentTrain: epoch  2, batch     1 | loss: 4.8565984CurrentTrain: epoch  3, batch     0 | loss: 5.3429675CurrentTrain: epoch  3, batch     1 | loss: 3.9115038CurrentTrain: epoch  4, batch     0 | loss: 4.3078771CurrentTrain: epoch  4, batch     1 | loss: 4.4000015
Mixup data size:  60
MixupTrain:  epoch  0, batch     1 | loss: 16.7204626MixupTrain:  epoch  0, batch     2 | loss: 18.1068451
MemoryTrain:  epoch  0, batch     0 | loss: 1.6892848MemoryTrain:  epoch  1, batch     0 | loss: 1.9596069MemoryTrain:  epoch  2, batch     0 | loss: 1.5302169MemoryTrain:  epoch  3, batch     0 | loss: 0.6775371MemoryTrain:  epoch  4, batch     0 | loss: 0.2409425
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 9.1194487CurrentTrain: epoch  0, batch     1 | loss: 8.3622751CurrentTrain: epoch  1, batch     0 | loss: 8.4338751CurrentTrain: epoch  1, batch     1 | loss: 6.8270898CurrentTrain: epoch  2, batch     0 | loss: 7.7461867CurrentTrain: epoch  2, batch     1 | loss: 6.3901505CurrentTrain: epoch  3, batch     0 | loss: 6.7817869CurrentTrain: epoch  3, batch     1 | loss: 6.6271353CurrentTrain: epoch  4, batch     0 | loss: 7.0960317CurrentTrain: epoch  4, batch     1 | loss: 5.2938986CurrentTrain: epoch  5, batch     0 | loss: 6.1793423CurrentTrain: epoch  5, batch     1 | loss: 5.9719582CurrentTrain: epoch  6, batch     0 | loss: 5.7591000CurrentTrain: epoch  6, batch     1 | loss: 4.5442257CurrentTrain: epoch  7, batch     0 | loss: 4.7679000CurrentTrain: epoch  7, batch     1 | loss: 5.4883280CurrentTrain: epoch  8, batch     0 | loss: 5.1311874CurrentTrain: epoch  8, batch     1 | loss: 4.2589989CurrentTrain: epoch  9, batch     0 | loss: 4.7376399CurrentTrain: epoch  9, batch     1 | loss: 3.7943711
Mixup data size:  60
MixupTrain:  epoch  0, batch     1 | loss: 6.8159449MixupTrain:  epoch  0, batch     2 | loss: 7.1051096
MemoryTrain:  epoch  0, batch     0 | loss: 3.0694485MemoryTrain:  epoch  1, batch     0 | loss: 2.6848230MemoryTrain:  epoch  2, batch     0 | loss: 1.5259112MemoryTrain:  epoch  3, batch     0 | loss: 1.2377322MemoryTrain:  epoch  4, batch     0 | loss: 0.8256449MemoryTrain:  epoch  5, batch     0 | loss: 0.7418844MemoryTrain:  epoch  6, batch     0 | loss: 0.7894905MemoryTrain:  epoch  7, batch     0 | loss: 0.6412662MemoryTrain:  epoch  8, batch     0 | loss: 0.7166207MemoryTrain:  epoch  9, batch     0 | loss: 0.5894949
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 77.78%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 77.84%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 76.04%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 70.54%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 90.62%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 89.58%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 87.50%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 86.76%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 85.42%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 84.21%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 85.12%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.80%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 86.41%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.98%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.98%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 88.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.62%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 89.01%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.96%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 89.11%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.45%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 88.42%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 87.85%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 87.67%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 87.66%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 87.66%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 87.19%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 86.89%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 87.05%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 86.77%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 86.22%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 85.83%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 84.51%   [EVAL] batch:   46 | acc: 0.00%,  total acc: 82.71%   
cur_acc_llm:  [0.8863636363636364, 0.7053571428571429]
his_acc_llm:  [0.8863636363636364, 0.8271276595744681]
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 81.25%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 86.06%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 84.82%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 84.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 82.42%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 81.99%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 80.90%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 80.92%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 82.95%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.70%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.58%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.88%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.38%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.85%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 86.88%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 86.90%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.11%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 87.31%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 86.58%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 86.79%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 86.81%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 86.66%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.01%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 87.18%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 87.03%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.35%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 87.65%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 87.65%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 87.64%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 86.14%   [EVAL] batch:   46 | acc: 0.00%,  total acc: 84.31%   
cur_acc:  ['0.8674', '0.8125']
his_acc:  ['0.8674', '0.8431']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 6.1172347CurrentTrain: epoch  0, batch     1 | loss: 7.1112008CurrentTrain: epoch  1, batch     0 | loss: 5.7283459CurrentTrain: epoch  1, batch     1 | loss: 4.9379792CurrentTrain: epoch  2, batch     0 | loss: 4.2366161CurrentTrain: epoch  2, batch     1 | loss: 4.9317250CurrentTrain: epoch  3, batch     0 | loss: 5.0143967CurrentTrain: epoch  3, batch     1 | loss: 2.6359727CurrentTrain: epoch  4, batch     0 | loss: 4.2897787CurrentTrain: epoch  4, batch     1 | loss: 3.8555238
Mixup data size:  69
MixupTrain:  epoch  0, batch     0 | loss: 12.9666772MixupTrain:  epoch  0, batch     1 | loss: 10.6374535MixupTrain:  epoch  0, batch     2 | loss: 6.7020643MixupTrain:  epoch  0, batch     3 | loss: 8.9969760MixupTrain:  epoch  0, batch     4 | loss: 9.1156976
MemoryTrain:  epoch  0, batch     0 | loss: 10.6156778MemoryTrain:  epoch  1, batch     0 | loss: 9.8476810MemoryTrain:  epoch  2, batch     0 | loss: 8.3022766MemoryTrain:  epoch  3, batch     0 | loss: 4.4535604MemoryTrain:  epoch  4, batch     0 | loss: 3.1290326
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 7.5920205CurrentTrain: epoch  0, batch     1 | loss: 6.8239136CurrentTrain: epoch  1, batch     0 | loss: 7.4884562CurrentTrain: epoch  1, batch     1 | loss: 4.9073691CurrentTrain: epoch  2, batch     0 | loss: 6.2382579CurrentTrain: epoch  2, batch     1 | loss: 4.2084613CurrentTrain: epoch  3, batch     0 | loss: 5.8410983CurrentTrain: epoch  3, batch     1 | loss: 3.8063362CurrentTrain: epoch  4, batch     0 | loss: 4.3020763CurrentTrain: epoch  4, batch     1 | loss: 4.8223009CurrentTrain: epoch  5, batch     0 | loss: 3.7065496CurrentTrain: epoch  5, batch     1 | loss: 4.9869361CurrentTrain: epoch  6, batch     0 | loss: 3.7589672CurrentTrain: epoch  6, batch     1 | loss: 3.7528841CurrentTrain: epoch  7, batch     0 | loss: 3.6318989CurrentTrain: epoch  7, batch     1 | loss: 3.3545887CurrentTrain: epoch  8, batch     0 | loss: 2.6774325CurrentTrain: epoch  8, batch     1 | loss: 4.2061992CurrentTrain: epoch  9, batch     0 | loss: 4.1060605CurrentTrain: epoch  9, batch     1 | loss: 2.2044871
Mixup data size:  71
MixupTrain:  epoch  0, batch     4 | loss: 5.1734409
MemoryTrain:  epoch  0, batch     0 | loss: 2.8327885MemoryTrain:  epoch  1, batch     0 | loss: 2.7715178MemoryTrain:  epoch  2, batch     0 | loss: 2.5732744MemoryTrain:  epoch  3, batch     0 | loss: 2.0955167MemoryTrain:  epoch  4, batch     0 | loss: 1.6994292MemoryTrain:  epoch  5, batch     0 | loss: 1.4988770MemoryTrain:  epoch  6, batch     0 | loss: 1.5193602MemoryTrain:  epoch  7, batch     0 | loss: 1.2711174MemoryTrain:  epoch  8, batch     0 | loss: 1.0785232MemoryTrain:  epoch  9, batch     0 | loss: 1.2421927
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 51.56%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 45.83%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 42.86%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 39.06%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 8.04%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 15.62%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 22.22%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 27.50%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 30.68%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 34.38%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 35.58%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 35.71%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 37.08%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 37.89%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 38.97%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 40.28%   [EVAL] batch:   18 | acc: 43.75%,  total acc: 40.46%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 42.19%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 43.75%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 45.17%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 45.92%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 46.88%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 48.50%   [EVAL] batch:   25 | acc: 81.25%,  total acc: 49.76%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 51.39%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 53.12%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 54.74%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 56.04%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 57.26%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 58.59%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 59.47%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 58.64%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 58.57%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 57.47%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 56.25%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 55.92%   [EVAL] batch:   38 | acc: 43.75%,  total acc: 55.61%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 55.62%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 56.10%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 56.85%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 57.12%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 56.96%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 56.94%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 55.84%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 56.38%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 57.29%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 56.38%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 55.50%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 55.15%   [EVAL] batch:   51 | acc: 25.00%,  total acc: 54.57%   [EVAL] batch:   52 | acc: 31.25%,  total acc: 54.13%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 53.36%   
cur_acc_llm:  [0.8863636363636364, 0.7053571428571429, 0.390625]
his_acc_llm:  [0.8863636363636364, 0.8271276595744681, 0.5335648148148148]
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 63.28%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 57.29%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 54.46%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 54.69%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 55.00%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 54.55%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 53.65%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 52.40%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 52.23%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 53.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 53.91%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 55.15%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 55.56%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 56.58%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 58.13%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 60.12%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 61.93%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 63.59%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 65.10%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 66.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 67.79%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 69.87%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 70.91%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 71.67%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 72.38%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 73.05%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 73.86%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 73.53%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 73.21%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 72.22%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 71.45%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 71.55%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 71.31%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 71.41%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 71.80%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 72.02%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 72.24%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 72.30%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 72.78%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 71.60%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 71.54%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 72.14%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 71.94%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 71.57%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 71.51%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 71.34%   [EVAL] batch:   53 | acc: 18.75%,  total acc: 70.37%   
cur_acc:  ['0.8674', '0.8125', '0.6328']
his_acc:  ['0.8674', '0.8431', '0.7037']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 5.6124358CurrentTrain: epoch  0, batch     1 | loss: 5.8926134CurrentTrain: epoch  1, batch     0 | loss: 4.8353796CurrentTrain: epoch  1, batch     1 | loss: 5.0278134CurrentTrain: epoch  2, batch     0 | loss: 4.1070423CurrentTrain: epoch  2, batch     1 | loss: 2.5176663CurrentTrain: epoch  3, batch     0 | loss: 2.7492502CurrentTrain: epoch  3, batch     1 | loss: 2.4742665CurrentTrain: epoch  4, batch     0 | loss: 2.4989557CurrentTrain: epoch  4, batch     1 | loss: 2.1460266
Mixup data size:  81
MixupTrain:  epoch  0, batch     0 | loss: 9.3393091MixupTrain:  epoch  0, batch     1 | loss: 10.8845574MixupTrain:  epoch  0, batch     5 | loss: 5.5019350
MemoryTrain:  epoch  0, batch     0 | loss: 3.4262590MemoryTrain:  epoch  0, batch     1 | loss: 2.5636897MemoryTrain:  epoch  1, batch     0 | loss: 3.3168106MemoryTrain:  epoch  1, batch     1 | loss: 2.9135616MemoryTrain:  epoch  2, batch     0 | loss: 1.3410164MemoryTrain:  epoch  2, batch     1 | loss: 1.1573308MemoryTrain:  epoch  3, batch     0 | loss: 0.5108143MemoryTrain:  epoch  3, batch     1 | loss: 0.3255774MemoryTrain:  epoch  4, batch     0 | loss: 0.1960970MemoryTrain:  epoch  4, batch     1 | loss: 0.0594297
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 6.8691592CurrentTrain: epoch  0, batch     1 | loss: 6.0230765CurrentTrain: epoch  1, batch     0 | loss: 5.5309067CurrentTrain: epoch  1, batch     1 | loss: 5.0258818CurrentTrain: epoch  2, batch     0 | loss: 4.2640128CurrentTrain: epoch  2, batch     1 | loss: 4.6392212CurrentTrain: epoch  3, batch     0 | loss: 4.0321498CurrentTrain: epoch  3, batch     1 | loss: 3.4441955CurrentTrain: epoch  4, batch     0 | loss: 3.9529567CurrentTrain: epoch  4, batch     1 | loss: 2.9111025CurrentTrain: epoch  5, batch     0 | loss: 3.2304440CurrentTrain: epoch  5, batch     1 | loss: 3.3800216CurrentTrain: epoch  6, batch     0 | loss: 3.2386963CurrentTrain: epoch  6, batch     1 | loss: 2.8045821CurrentTrain: epoch  7, batch     0 | loss: 2.8869514CurrentTrain: epoch  7, batch     1 | loss: 2.5974927CurrentTrain: epoch  8, batch     0 | loss: 2.9129229CurrentTrain: epoch  8, batch     1 | loss: 2.4511242CurrentTrain: epoch  9, batch     0 | loss: 2.3989098CurrentTrain: epoch  9, batch     1 | loss: 2.2840571
Mixup data size:  81
MixupTrain:  epoch  0, batch     0 | loss: 4.5470261MixupTrain:  epoch  0, batch     5 | loss: 2.8322319
MemoryTrain:  epoch  0, batch     0 | loss: 2.2339149MemoryTrain:  epoch  0, batch     1 | loss: 0.5750201MemoryTrain:  epoch  1, batch     0 | loss: 2.5238528MemoryTrain:  epoch  1, batch     1 | loss: 1.0927238MemoryTrain:  epoch  2, batch     0 | loss: 2.2526758MemoryTrain:  epoch  2, batch     1 | loss: 0.3241411MemoryTrain:  epoch  3, batch     0 | loss: 1.4881458MemoryTrain:  epoch  3, batch     1 | loss: 0.6075973MemoryTrain:  epoch  4, batch     0 | loss: 1.0943215MemoryTrain:  epoch  4, batch     1 | loss: 0.7756404MemoryTrain:  epoch  5, batch     0 | loss: 1.0502549MemoryTrain:  epoch  5, batch     1 | loss: 0.6882633MemoryTrain:  epoch  6, batch     0 | loss: 0.9961216MemoryTrain:  epoch  6, batch     1 | loss: 0.3174525MemoryTrain:  epoch  7, batch     0 | loss: 0.7486614MemoryTrain:  epoch  7, batch     1 | loss: 0.2917606MemoryTrain:  epoch  8, batch     0 | loss: 0.7147801MemoryTrain:  epoch  8, batch     1 | loss: 0.4264173MemoryTrain:  epoch  9, batch     0 | loss: 0.5631857MemoryTrain:  epoch  9, batch     1 | loss: 0.3954374
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 95.54%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 81.77%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 78.37%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 1.04%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 6.25%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 11.72%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 19.44%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 21.88%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 26.14%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 29.17%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 31.73%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 33.04%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 35.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 37.11%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 39.34%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 40.62%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 41.78%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 44.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 46.73%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 49.15%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 51.36%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 53.39%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 55.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 56.97%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 58.33%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 59.60%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 60.99%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 61.67%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 62.70%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 63.87%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 64.77%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 63.05%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 61.25%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 60.07%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 58.45%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 57.07%   [EVAL] batch:   38 | acc: 0.00%,  total acc: 55.61%   [EVAL] batch:   39 | acc: 18.75%,  total acc: 54.69%   [EVAL] batch:   40 | acc: 12.50%,  total acc: 53.66%   [EVAL] batch:   41 | acc: 25.00%,  total acc: 52.98%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 51.74%   [EVAL] batch:   43 | acc: 12.50%,  total acc: 50.85%   [EVAL] batch:   44 | acc: 6.25%,  total acc: 49.86%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 48.78%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 49.47%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 50.52%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 50.00%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 49.88%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 50.12%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 50.12%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 50.69%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 51.59%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 52.23%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 52.96%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 53.77%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 54.56%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 55.21%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 55.53%   [EVAL] batch:   61 | acc: 37.50%,  total acc: 55.24%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 55.75%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 55.96%   [EVAL] batch:   64 | acc: 50.00%,  total acc: 55.87%   [EVAL] batch:   65 | acc: 50.00%,  total acc: 55.78%   
cur_acc_llm:  [0.8863636363636364, 0.7053571428571429, 0.390625, 0.7836538461538461]
his_acc_llm:  [0.8863636363636364, 0.8271276595744681, 0.5335648148148148, 0.5577651515151515]
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 67.86%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 66.41%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 62.50%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 64.38%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 65.91%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 68.23%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 66.83%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 54.69%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 54.17%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 50.89%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 49.22%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 50.69%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 49.38%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 48.30%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 47.92%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 46.63%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 47.32%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 49.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 49.61%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 51.10%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 51.74%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 53.29%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 55.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 57.14%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 59.09%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 60.87%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 64.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 65.38%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 66.44%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 67.63%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 68.32%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 68.96%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 69.35%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 70.12%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 70.64%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 69.49%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 68.57%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 67.01%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 65.37%   [EVAL] batch:   37 | acc: 37.50%,  total acc: 64.64%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 63.78%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 63.91%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 64.63%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 65.18%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 65.70%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 66.05%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 66.53%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 65.49%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 65.56%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 66.28%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 66.20%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 66.25%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 66.18%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 66.23%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 66.39%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 67.01%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 67.50%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 67.97%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 68.42%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 67.78%   [EVAL] batch:   58 | acc: 25.00%,  total acc: 67.06%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 66.67%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 66.50%   [EVAL] batch:   61 | acc: 37.50%,  total acc: 66.03%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 66.17%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 66.31%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 66.73%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 66.76%   
cur_acc:  ['0.8674', '0.8125', '0.6328', '0.6683']
his_acc:  ['0.8674', '0.8431', '0.7037', '0.6676']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 7.8970380CurrentTrain: epoch  0, batch     1 | loss: 7.8480158CurrentTrain: epoch  1, batch     0 | loss: 6.2872691CurrentTrain: epoch  1, batch     1 | loss: 6.3431382CurrentTrain: epoch  2, batch     0 | loss: 5.9826684CurrentTrain: epoch  2, batch     1 | loss: 4.0515046CurrentTrain: epoch  3, batch     0 | loss: 4.7276373CurrentTrain: epoch  3, batch     1 | loss: 4.9066339CurrentTrain: epoch  4, batch     0 | loss: 4.1894498CurrentTrain: epoch  4, batch     1 | loss: 4.5209579
Mixup data size:  90

MemoryTrain:  epoch  0, batch     0 | loss: 1.4366146MemoryTrain:  epoch  0, batch     1 | loss: 0.7929394MemoryTrain:  epoch  1, batch     0 | loss: 1.1917491MemoryTrain:  epoch  1, batch     1 | loss: 0.6873507MemoryTrain:  epoch  2, batch     0 | loss: 0.6094991MemoryTrain:  epoch  2, batch     1 | loss: 0.4840831MemoryTrain:  epoch  3, batch     0 | loss: 0.3973256MemoryTrain:  epoch  3, batch     1 | loss: 0.0927953MemoryTrain:  epoch  4, batch     0 | loss: 0.1834859MemoryTrain:  epoch  4, batch     1 | loss: 0.6330522
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 8.7399368CurrentTrain: epoch  0, batch     1 | loss: 9.9880142CurrentTrain: epoch  1, batch     0 | loss: 7.5634041CurrentTrain: epoch  1, batch     1 | loss: 9.0820093CurrentTrain: epoch  2, batch     0 | loss: 7.3151350CurrentTrain: epoch  2, batch     1 | loss: 7.3332744CurrentTrain: epoch  3, batch     0 | loss: 7.5531282CurrentTrain: epoch  3, batch     1 | loss: 5.8862500CurrentTrain: epoch  4, batch     0 | loss: 6.7370043CurrentTrain: epoch  4, batch     1 | loss: 6.2125425CurrentTrain: epoch  5, batch     0 | loss: 6.0513520CurrentTrain: epoch  5, batch     1 | loss: 6.1868653CurrentTrain: epoch  6, batch     0 | loss: 5.4731827CurrentTrain: epoch  6, batch     1 | loss: 5.7708259CurrentTrain: epoch  7, batch     0 | loss: 6.3053803CurrentTrain: epoch  7, batch     1 | loss: 3.7218928CurrentTrain: epoch  8, batch     0 | loss: 5.1398697CurrentTrain: epoch  8, batch     1 | loss: 4.9312105CurrentTrain: epoch  9, batch     0 | loss: 5.1711001CurrentTrain: epoch  9, batch     1 | loss: 3.7834523
Mixup data size:  89

MemoryTrain:  epoch  0, batch     0 | loss: 1.8276787MemoryTrain:  epoch  0, batch     1 | loss: 0.8863772MemoryTrain:  epoch  1, batch     0 | loss: 1.8468351MemoryTrain:  epoch  1, batch     1 | loss: 1.8903008MemoryTrain:  epoch  2, batch     0 | loss: 1.4227686MemoryTrain:  epoch  2, batch     1 | loss: 1.5927062MemoryTrain:  epoch  3, batch     0 | loss: 1.1535640MemoryTrain:  epoch  3, batch     1 | loss: 0.6155360MemoryTrain:  epoch  4, batch     0 | loss: 0.9969596MemoryTrain:  epoch  4, batch     1 | loss: 0.5732880MemoryTrain:  epoch  5, batch     0 | loss: 1.1829140MemoryTrain:  epoch  5, batch     1 | loss: 0.7097667MemoryTrain:  epoch  6, batch     0 | loss: 0.7634214MemoryTrain:  epoch  6, batch     1 | loss: 0.5722761MemoryTrain:  epoch  7, batch     0 | loss: 0.7012692MemoryTrain:  epoch  7, batch     1 | loss: 0.5915073MemoryTrain:  epoch  8, batch     0 | loss: 0.6633770MemoryTrain:  epoch  8, batch     1 | loss: 0.6725256MemoryTrain:  epoch  9, batch     0 | loss: 0.5830548MemoryTrain:  epoch  9, batch     1 | loss: 0.4743795
