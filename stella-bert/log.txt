#############params############
cuda:0
Task=Tacred, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=0, gen_num=0
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 13.2422485CurrentTrain: epoch  0, batch     1 | loss: 12.8773499CurrentTrain: epoch  0, batch     2 | loss: 12.3324814CurrentTrain: epoch  0, batch     3 | loss: 11.5798321CurrentTrain: epoch  0, batch     4 | loss: 11.0859909CurrentTrain: epoch  0, batch     5 | loss: 11.3850060CurrentTrain: epoch  0, batch     6 | loss: 11.5133390CurrentTrain: epoch  0, batch     7 | loss: 11.6831436CurrentTrain: epoch  0, batch     8 | loss: 11.1954775CurrentTrain: epoch  0, batch     9 | loss: 11.3733501CurrentTrain: epoch  0, batch    10 | loss: 11.7615948CurrentTrain: epoch  0, batch    11 | loss: 11.3967009CurrentTrain: epoch  0, batch    12 | loss: 11.2709274CurrentTrain: epoch  0, batch    13 | loss: 11.7042751CurrentTrain: epoch  0, batch    14 | loss: 11.2179909CurrentTrain: epoch  0, batch    15 | loss: 11.3064766CurrentTrain: epoch  0, batch    16 | loss: 11.3448582CurrentTrain: epoch  0, batch    17 | loss: 11.1533632CurrentTrain: epoch  0, batch    18 | loss: 11.3917084CurrentTrain: epoch  0, batch    19 | loss: 12.3134670CurrentTrain: epoch  0, batch    20 | loss: 11.0819626CurrentTrain: epoch  0, batch    21 | loss: 11.6122837CurrentTrain: epoch  0, batch    22 | loss: 11.5490360CurrentTrain: epoch  0, batch    23 | loss: 11.6694937CurrentTrain: epoch  0, batch    24 | loss: 11.5315170CurrentTrain: epoch  0, batch    25 | loss: 11.5239668CurrentTrain: epoch  0, batch    26 | loss: 11.0377989CurrentTrain: epoch  0, batch    27 | loss: 11.0405359CurrentTrain: epoch  0, batch    28 | loss: 11.0549030CurrentTrain: epoch  0, batch    29 | loss: 11.1313076CurrentTrain: epoch  0, batch    30 | loss: 11.1405544CurrentTrain: epoch  0, batch    31 | loss: 11.4460640CurrentTrain: epoch  0, batch    32 | loss: 10.9830093CurrentTrain: epoch  0, batch    33 | loss: 11.1092949CurrentTrain: epoch  0, batch    34 | loss: 11.2995968CurrentTrain: epoch  0, batch    35 | loss: 10.7293072CurrentTrain: epoch  0, batch    36 | loss: 10.7863111CurrentTrain: epoch  0, batch    37 | loss: 11.3436775CurrentTrain: epoch  1, batch     0 | loss: 11.1422176CurrentTrain: epoch  1, batch     1 | loss: 11.4183826CurrentTrain: epoch  1, batch     2 | loss: 10.9273186CurrentTrain: epoch  1, batch     3 | loss: 10.9975882CurrentTrain: epoch  1, batch     4 | loss: 10.9157715CurrentTrain: epoch  1, batch     5 | loss: 10.9990215CurrentTrain: epoch  1, batch     6 | loss: 11.8105698CurrentTrain: epoch  1, batch     7 | loss: 11.8550072CurrentTrain: epoch  1, batch     8 | loss: 11.6610994CurrentTrain: epoch  1, batch     9 | loss: 10.9908752CurrentTrain: epoch  1, batch    10 | loss: 12.0511856CurrentTrain: epoch  1, batch    11 | loss: 11.6434994CurrentTrain: epoch  1, batch    12 | loss: 11.1213732CurrentTrain: epoch  1, batch    13 | loss: 10.8475218CurrentTrain: epoch  1, batch    14 | loss: 10.8846226CurrentTrain: epoch  1, batch    15 | loss: 10.7311525CurrentTrain: epoch  1, batch    16 | loss: 10.8749428CurrentTrain: epoch  1, batch    17 | loss: 10.6742382CurrentTrain: epoch  1, batch    18 | loss: 11.1908035CurrentTrain: epoch  1, batch    19 | loss: 10.7079811CurrentTrain: epoch  1, batch    20 | loss: 11.0646191CurrentTrain: epoch  1, batch    21 | loss: 10.7566977CurrentTrain: epoch  1, batch    22 | loss: 10.7863731CurrentTrain: epoch  1, batch    23 | loss: 10.7009993CurrentTrain: epoch  1, batch    24 | loss: 11.2990589CurrentTrain: epoch  1, batch    25 | loss: 10.8041258CurrentTrain: epoch  1, batch    26 | loss: 11.4333172CurrentTrain: epoch  1, batch    27 | loss: 11.3376474CurrentTrain: epoch  1, batch    28 | loss: 11.1437206CurrentTrain: epoch  1, batch    29 | loss: 10.9827089CurrentTrain: epoch  1, batch    30 | loss: 10.8757420CurrentTrain: epoch  1, batch    31 | loss: 11.4495716CurrentTrain: epoch  1, batch    32 | loss: 10.8792458CurrentTrain: epoch  1, batch    33 | loss: 10.9015007CurrentTrain: epoch  1, batch    34 | loss: 10.7688770CurrentTrain: epoch  1, batch    35 | loss: 10.7223625CurrentTrain: epoch  1, batch    36 | loss: 10.7855721CurrentTrain: epoch  1, batch    37 | loss: 10.7454739CurrentTrain: epoch  2, batch     0 | loss: 10.9949417CurrentTrain: epoch  2, batch     1 | loss: 10.8723049CurrentTrain: epoch  2, batch     2 | loss: 10.9422646CurrentTrain: epoch  2, batch     3 | loss: 10.8059025CurrentTrain: epoch  2, batch     4 | loss: 10.7950268CurrentTrain: epoch  2, batch     5 | loss: 10.7404928CurrentTrain: epoch  2, batch     6 | loss: 11.2074318CurrentTrain: epoch  2, batch     7 | loss: 11.0733948CurrentTrain: epoch  2, batch     8 | loss: 11.2206430CurrentTrain: epoch  2, batch     9 | loss: 11.5965233CurrentTrain: epoch  2, batch    10 | loss: 12.1901178CurrentTrain: epoch  2, batch    11 | loss: 12.5891323CurrentTrain: epoch  2, batch    12 | loss: 11.3734550CurrentTrain: epoch  2, batch    13 | loss: 11.0668545CurrentTrain: epoch  2, batch    14 | loss: 10.7935371CurrentTrain: epoch  2, batch    15 | loss: 11.0216846CurrentTrain: epoch  2, batch    16 | loss: 11.2152882CurrentTrain: epoch  2, batch    17 | loss: 10.7655039CurrentTrain: epoch  2, batch    18 | loss: 10.7538719CurrentTrain: epoch  2, batch    19 | loss: 10.8479681CurrentTrain: epoch  2, batch    20 | loss: 10.9483833CurrentTrain: epoch  2, batch    21 | loss: 11.0967140CurrentTrain: epoch  2, batch    22 | loss: 11.2874756CurrentTrain: epoch  2, batch    23 | loss: 11.3711290CurrentTrain: epoch  2, batch    24 | loss: 11.5276585CurrentTrain: epoch  2, batch    25 | loss: 11.0890188CurrentTrain: epoch  2, batch    26 | loss: 10.7706757CurrentTrain: epoch  2, batch    27 | loss: 10.9266891CurrentTrain: epoch  2, batch    28 | loss: 11.2804565CurrentTrain: epoch  2, batch    29 | loss: 10.8367815CurrentTrain: epoch  2, batch    30 | loss: 10.9346762CurrentTrain: epoch  2, batch    31 | loss: 10.8531551CurrentTrain: epoch  2, batch    32 | loss: 10.7850780CurrentTrain: epoch  2, batch    33 | loss: 10.8844748CurrentTrain: epoch  2, batch    34 | loss: 10.8930235CurrentTrain: epoch  2, batch    35 | loss: 11.0549603CurrentTrain: epoch  2, batch    36 | loss: 10.7932730CurrentTrain: epoch  2, batch    37 | loss: 10.6600876CurrentTrain: epoch  3, batch     0 | loss: 10.7489128CurrentTrain: epoch  3, batch     1 | loss: 10.8975163CurrentTrain: epoch  3, batch     2 | loss: 11.0130692CurrentTrain: epoch  3, batch     3 | loss: 10.8634892CurrentTrain: epoch  3, batch     4 | loss: 10.9508810CurrentTrain: epoch  3, batch     5 | loss: 10.8532410CurrentTrain: epoch  3, batch     6 | loss: 11.0109797CurrentTrain: epoch  3, batch     7 | loss: 10.9136305CurrentTrain: epoch  3, batch     8 | loss: 10.9838820CurrentTrain: epoch  3, batch     9 | loss: 11.4706059CurrentTrain: epoch  3, batch    10 | loss: 11.0903530CurrentTrain: epoch  3, batch    11 | loss: 11.6432238CurrentTrain: epoch  3, batch    12 | loss: 13.1709900CurrentTrain: epoch  3, batch    13 | loss: 11.1500015CurrentTrain: epoch  3, batch    14 | loss: 10.8361149CurrentTrain: epoch  3, batch    15 | loss: 10.6495199CurrentTrain: epoch  3, batch    16 | loss: 11.3425159CurrentTrain: epoch  3, batch    17 | loss: 10.7240486CurrentTrain: epoch  3, batch    18 | loss: 10.7446222CurrentTrain: epoch  3, batch    19 | loss: 11.3322086CurrentTrain: epoch  3, batch    20 | loss: 10.6933022CurrentTrain: epoch  3, batch    21 | loss: 10.7792473CurrentTrain: epoch  3, batch    22 | loss: 11.2703533CurrentTrain: epoch  3, batch    23 | loss: 10.7421274CurrentTrain: epoch  3, batch    24 | loss: 10.9351158CurrentTrain: epoch  3, batch    25 | loss: 10.8500729CurrentTrain: epoch  3, batch    26 | loss: 10.8449173CurrentTrain: epoch  3, batch    27 | loss: 10.9114246CurrentTrain: epoch  3, batch    28 | loss: 11.0628843CurrentTrain: epoch  3, batch    29 | loss: 10.5888195CurrentTrain: epoch  3, batch    30 | loss: 10.6282978CurrentTrain: epoch  3, batch    31 | loss: 10.6453094CurrentTrain: epoch  3, batch    32 | loss: 10.8205976CurrentTrain: epoch  3, batch    33 | loss: 10.5248899CurrentTrain: epoch  3, batch    34 | loss: 10.7047138CurrentTrain: epoch  3, batch    35 | loss: 10.5311136CurrentTrain: epoch  3, batch    36 | loss: 10.9497099CurrentTrain: epoch  3, batch    37 | loss: 10.4013186CurrentTrain: epoch  4, batch     0 | loss: 10.7502050CurrentTrain: epoch  4, batch     1 | loss: 10.7261467CurrentTrain: epoch  4, batch     2 | loss: 10.8580685CurrentTrain: epoch  4, batch     3 | loss: 10.9259405CurrentTrain: epoch  4, batch     4 | loss: 11.0017071CurrentTrain: epoch  4, batch     5 | loss: 10.7382517CurrentTrain: epoch  4, batch     6 | loss: 10.6508751CurrentTrain: epoch  4, batch     7 | loss: 10.3719692CurrentTrain: epoch  4, batch     8 | loss: 10.6046391CurrentTrain: epoch  4, batch     9 | loss: 10.3318510CurrentTrain: epoch  4, batch    10 | loss: 10.3226871CurrentTrain: epoch  4, batch    11 | loss: 10.4809599CurrentTrain: epoch  4, batch    12 | loss: 10.3734903CurrentTrain: epoch  4, batch    13 | loss: 10.4138527CurrentTrain: epoch  4, batch    14 | loss: 10.2819080CurrentTrain: epoch  4, batch    15 | loss: 10.2898216CurrentTrain: epoch  4, batch    16 | loss: 10.2210426CurrentTrain: epoch  4, batch    17 | loss: 10.4163275CurrentTrain: epoch  4, batch    18 | loss: 10.3296413CurrentTrain: epoch  4, batch    19 | loss: 10.3271275CurrentTrain: epoch  4, batch    20 | loss: 10.2814980CurrentTrain: epoch  4, batch    21 | loss: 10.2295074CurrentTrain: epoch  4, batch    22 | loss: 10.1946115CurrentTrain: epoch  4, batch    23 | loss: 10.1881123CurrentTrain: epoch  4, batch    24 | loss: 10.1406422CurrentTrain: epoch  4, batch    25 | loss: 10.1716261CurrentTrain: epoch  4, batch    26 | loss: 10.0501900CurrentTrain: epoch  4, batch    27 | loss: 10.1318779CurrentTrain: epoch  4, batch    28 | loss: 10.0947599CurrentTrain: epoch  4, batch    29 | loss: 10.0602770CurrentTrain: epoch  4, batch    30 | loss: 10.0526752CurrentTrain: epoch  4, batch    31 | loss: 9.8642521CurrentTrain: epoch  4, batch    32 | loss: 9.6277809CurrentTrain: epoch  4, batch    33 | loss: 10.0472822CurrentTrain: epoch  4, batch    34 | loss: 9.9840946CurrentTrain: epoch  4, batch    35 | loss: 9.6873398CurrentTrain: epoch  4, batch    36 | loss: 9.8649702CurrentTrain: epoch  4, batch    37 | loss: 9.5152426CurrentTrain: epoch  5, batch     0 | loss: 9.6951504CurrentTrain: epoch  5, batch     1 | loss: 9.7130795CurrentTrain: epoch  5, batch     2 | loss: 9.6738138CurrentTrain: epoch  5, batch     3 | loss: 9.5049686CurrentTrain: epoch  5, batch     4 | loss: 9.8393898CurrentTrain: epoch  5, batch     5 | loss: 9.7002773CurrentTrain: epoch  5, batch     6 | loss: 9.6599131CurrentTrain: epoch  5, batch     7 | loss: 9.4302168CurrentTrain: epoch  5, batch     8 | loss: 9.5120726CurrentTrain: epoch  5, batch     9 | loss: 9.1669521CurrentTrain: epoch  5, batch    10 | loss: 9.1893015CurrentTrain: epoch  5, batch    11 | loss: 9.2281370CurrentTrain: epoch  5, batch    12 | loss: 9.2591677CurrentTrain: epoch  5, batch    13 | loss: 8.9800758CurrentTrain: epoch  5, batch    14 | loss: 9.1364002CurrentTrain: epoch  5, batch    15 | loss: 9.0417385CurrentTrain: epoch  5, batch    16 | loss: 8.6182842CurrentTrain: epoch  5, batch    17 | loss: 9.0932446CurrentTrain: epoch  5, batch    18 | loss: 9.3527527CurrentTrain: epoch  5, batch    19 | loss: 9.2831268CurrentTrain: epoch  5, batch    20 | loss: 9.0116558CurrentTrain: epoch  5, batch    21 | loss: 8.9235668CurrentTrain: epoch  5, batch    22 | loss: 8.5722036CurrentTrain: epoch  5, batch    23 | loss: 8.6137161CurrentTrain: epoch  5, batch    24 | loss: 8.4495735CurrentTrain: epoch  5, batch    25 | loss: 8.3696556CurrentTrain: epoch  5, batch    26 | loss: 8.4001713CurrentTrain: epoch  5, batch    27 | loss: 8.8210945CurrentTrain: epoch  5, batch    28 | loss: 8.5752811CurrentTrain: epoch  5, batch    29 | loss: 8.2935743CurrentTrain: epoch  5, batch    30 | loss: 8.0293970CurrentTrain: epoch  5, batch    31 | loss: 8.1658649CurrentTrain: epoch  5, batch    32 | loss: 8.2121162CurrentTrain: epoch  5, batch    33 | loss: 8.1568623CurrentTrain: epoch  5, batch    34 | loss: 7.9187784CurrentTrain: epoch  5, batch    35 | loss: 7.5339737CurrentTrain: epoch  5, batch    36 | loss: 8.0159245CurrentTrain: epoch  5, batch    37 | loss: 7.2938623CurrentTrain: epoch  6, batch     0 | loss: 8.1949444CurrentTrain: epoch  6, batch     1 | loss: 6.9931002CurrentTrain: epoch  6, batch     2 | loss: 7.7449598CurrentTrain: epoch  6, batch     3 | loss: 8.7644215CurrentTrain: epoch  6, batch     4 | loss: 7.5633955CurrentTrain: epoch  6, batch     5 | loss: 7.8298893CurrentTrain: epoch  6, batch     6 | loss: 7.8342075CurrentTrain: epoch  6, batch     7 | loss: 7.4609385CurrentTrain: epoch  6, batch     8 | loss: 7.7434921CurrentTrain: epoch  6, batch     9 | loss: 7.4714785CurrentTrain: epoch  6, batch    10 | loss: 7.5970941CurrentTrain: epoch  6, batch    11 | loss: 7.6575661CurrentTrain: epoch  6, batch    12 | loss: 6.7546768CurrentTrain: epoch  6, batch    13 | loss: 6.8614788CurrentTrain: epoch  6, batch    14 | loss: 7.0028358CurrentTrain: epoch  6, batch    15 | loss: 7.3564396CurrentTrain: epoch  6, batch    16 | loss: 7.2222142CurrentTrain: epoch  6, batch    17 | loss: 8.1191816CurrentTrain: epoch  6, batch    18 | loss: 6.7897406CurrentTrain: epoch  6, batch    19 | loss: 6.6103520CurrentTrain: epoch  6, batch    20 | loss: 7.5887260CurrentTrain: epoch  6, batch    21 | loss: 7.6863337CurrentTrain: epoch  6, batch    22 | loss: 7.7226620CurrentTrain: epoch  6, batch    23 | loss: 7.1952438CurrentTrain: epoch  6, batch    24 | loss: 8.0676336CurrentTrain: epoch  6, batch    25 | loss: 7.1079054CurrentTrain: epoch  6, batch    26 | loss: 7.2074547CurrentTrain: epoch  6, batch    27 | loss: 7.3890719CurrentTrain: epoch  6, batch    28 | loss: 7.2220631CurrentTrain: epoch  6, batch    29 | loss: 6.7764606CurrentTrain: epoch  6, batch    30 | loss: 7.4543953CurrentTrain: epoch  6, batch    31 | loss: 7.5146675CurrentTrain: epoch  6, batch    32 | loss: 7.4004660CurrentTrain: epoch  6, batch    33 | loss: 7.2853212CurrentTrain: epoch  6, batch    34 | loss: 7.2107239CurrentTrain: epoch  6, batch    35 | loss: 6.9508748CurrentTrain: epoch  6, batch    36 | loss: 7.7896786CurrentTrain: epoch  6, batch    37 | loss: 7.4343438CurrentTrain: epoch  7, batch     0 | loss: 6.6536560CurrentTrain: epoch  7, batch     1 | loss: 7.4918284CurrentTrain: epoch  7, batch     2 | loss: 7.0869036CurrentTrain: epoch  7, batch     3 | loss: 5.8323545CurrentTrain: epoch  7, batch     4 | loss: 6.5006208CurrentTrain: epoch  7, batch     5 | loss: 7.0435209CurrentTrain: epoch  7, batch     6 | loss: 6.2485743CurrentTrain: epoch  7, batch     7 | loss: 6.6238780CurrentTrain: epoch  7, batch     8 | loss: 7.4937901CurrentTrain: epoch  7, batch     9 | loss: 6.3197441CurrentTrain: epoch  7, batch    10 | loss: 6.1727028CurrentTrain: epoch  7, batch    11 | loss: 6.1385322CurrentTrain: epoch  7, batch    12 | loss: 8.0399694CurrentTrain: epoch  7, batch    13 | loss: 7.5008411CurrentTrain: epoch  7, batch    14 | loss: 7.5745764CurrentTrain: epoch  7, batch    15 | loss: 6.2093501CurrentTrain: epoch  7, batch    16 | loss: 7.1981125CurrentTrain: epoch  7, batch    17 | loss: 5.8749132CurrentTrain: epoch  7, batch    18 | loss: 6.8577924CurrentTrain: epoch  7, batch    19 | loss: 7.0853915CurrentTrain: epoch  7, batch    20 | loss: 6.6996536CurrentTrain: epoch  7, batch    21 | loss: 5.7364440CurrentTrain: epoch  7, batch    22 | loss: 6.5300636CurrentTrain: epoch  7, batch    23 | loss: 6.5473566CurrentTrain: epoch  7, batch    24 | loss: 7.0934629CurrentTrain: epoch  7, batch    25 | loss: 6.6593056CurrentTrain: epoch  7, batch    26 | loss: 7.9891014CurrentTrain: epoch  7, batch    27 | loss: 6.7468939CurrentTrain: epoch  7, batch    28 | loss: 5.8475127CurrentTrain: epoch  7, batch    29 | loss: 6.5621777CurrentTrain: epoch  7, batch    30 | loss: 6.4317551CurrentTrain: epoch  7, batch    31 | loss: 6.4904184CurrentTrain: epoch  7, batch    32 | loss: 6.3665657CurrentTrain: epoch  7, batch    33 | loss: 6.0353575CurrentTrain: epoch  7, batch    34 | loss: 7.0637321CurrentTrain: epoch  7, batch    35 | loss: 6.3753333CurrentTrain: epoch  7, batch    36 | loss: 5.9925690CurrentTrain: epoch  7, batch    37 | loss: 6.1695786CurrentTrain: epoch  8, batch     0 | loss: 6.7209053CurrentTrain: epoch  8, batch     1 | loss: 5.7160974CurrentTrain: epoch  8, batch     2 | loss: 6.0544348CurrentTrain: epoch  8, batch     3 | loss: 6.0010052CurrentTrain: epoch  8, batch     4 | loss: 5.7808819CurrentTrain: epoch  8, batch     5 | loss: 6.4907761CurrentTrain: epoch  8, batch     6 | loss: 6.1024570CurrentTrain: epoch  8, batch     7 | loss: 6.2282906CurrentTrain: epoch  8, batch     8 | loss: 5.9747362CurrentTrain: epoch  8, batch     9 | loss: 5.7694621CurrentTrain: epoch  8, batch    10 | loss: 6.0726252CurrentTrain: epoch  8, batch    11 | loss: 6.0129910CurrentTrain: epoch  8, batch    12 | loss: 7.1484127CurrentTrain: epoch  8, batch    13 | loss: 6.6265936CurrentTrain: epoch  8, batch    14 | loss: 6.6625967CurrentTrain: epoch  8, batch    15 | loss: 6.5150023CurrentTrain: epoch  8, batch    16 | loss: 5.9768133CurrentTrain: epoch  8, batch    17 | loss: 6.2257700CurrentTrain: epoch  8, batch    18 | loss: 7.4355850CurrentTrain: epoch  8, batch    19 | loss: 5.6329298CurrentTrain: epoch  8, batch    20 | loss: 6.5601425CurrentTrain: epoch  8, batch    21 | loss: 6.0978422CurrentTrain: epoch  8, batch    22 | loss: 5.8099146CurrentTrain: epoch  8, batch    23 | loss: 6.4833798CurrentTrain: epoch  8, batch    24 | loss: 5.5741148CurrentTrain: epoch  8, batch    25 | loss: 6.6152143CurrentTrain: epoch  8, batch    26 | loss: 5.6062212CurrentTrain: epoch  8, batch    27 | loss: 6.1426735CurrentTrain: epoch  8, batch    28 | loss: 6.7200365CurrentTrain: epoch  8, batch    29 | loss: 5.5100861CurrentTrain: epoch  8, batch    30 | loss: 5.7643905CurrentTrain: epoch  8, batch    31 | loss: 5.4898162CurrentTrain: epoch  8, batch    32 | loss: 6.9700885CurrentTrain: epoch  8, batch    33 | loss: 7.0526357CurrentTrain: epoch  8, batch    34 | loss: 6.4452477CurrentTrain: epoch  8, batch    35 | loss: 5.9344931CurrentTrain: epoch  8, batch    36 | loss: 6.0134668CurrentTrain: epoch  8, batch    37 | loss: 5.7305427CurrentTrain: epoch  9, batch     0 | loss: 6.3677807CurrentTrain: epoch  9, batch     1 | loss: 6.3569164CurrentTrain: epoch  9, batch     2 | loss: 6.2328577CurrentTrain: epoch  9, batch     3 | loss: 5.7589397CurrentTrain: epoch  9, batch     4 | loss: 6.5843210CurrentTrain: epoch  9, batch     5 | loss: 6.3890486CurrentTrain: epoch  9, batch     6 | loss: 6.1715236CurrentTrain: epoch  9, batch     7 | loss: 5.7701535CurrentTrain: epoch  9, batch     8 | loss: 6.5628815CurrentTrain: epoch  9, batch     9 | loss: 6.6639118CurrentTrain: epoch  9, batch    10 | loss: 6.0715823CurrentTrain: epoch  9, batch    11 | loss: 6.6056852CurrentTrain: epoch  9, batch    12 | loss: 5.5449438CurrentTrain: epoch  9, batch    13 | loss: 5.7816362CurrentTrain: epoch  9, batch    14 | loss: 5.9237642CurrentTrain: epoch  9, batch    15 | loss: 6.0757542CurrentTrain: epoch  9, batch    16 | loss: 5.9367385CurrentTrain: epoch  9, batch    17 | loss: 5.6812763CurrentTrain: epoch  9, batch    18 | loss: 5.5194273CurrentTrain: epoch  9, batch    19 | loss: 5.6696835CurrentTrain: epoch  9, batch    20 | loss: 5.9672294CurrentTrain: epoch  9, batch    21 | loss: 5.3698988CurrentTrain: epoch  9, batch    22 | loss: 5.4240150CurrentTrain: epoch  9, batch    23 | loss: 6.2200098CurrentTrain: epoch  9, batch    24 | loss: 6.2952228CurrentTrain: epoch  9, batch    25 | loss: 5.9836388CurrentTrain: epoch  9, batch    26 | loss: 6.1104550CurrentTrain: epoch  9, batch    27 | loss: 6.2982154CurrentTrain: epoch  9, batch    28 | loss: 5.6350937CurrentTrain: epoch  9, batch    29 | loss: 5.5520639CurrentTrain: epoch  9, batch    30 | loss: 5.6367121CurrentTrain: epoch  9, batch    31 | loss: 5.2407722CurrentTrain: epoch  9, batch    32 | loss: 6.3038793CurrentTrain: epoch  9, batch    33 | loss: 5.6977825CurrentTrain: epoch  9, batch    34 | loss: 5.8714609CurrentTrain: epoch  9, batch    35 | loss: 5.8555527CurrentTrain: epoch  9, batch    36 | loss: 5.3699722CurrentTrain: epoch  9, batch    37 | loss: 5.2995229
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 14.9595928CurrentTrain: epoch  0, batch     1 | loss: 14.0982342CurrentTrain: epoch  0, batch     2 | loss: 13.8081760CurrentTrain: epoch  0, batch     3 | loss: 14.0905418CurrentTrain: epoch  0, batch     4 | loss: 14.4930611CurrentTrain: epoch  0, batch     5 | loss: 14.6382847CurrentTrain: epoch  0, batch     6 | loss: 13.9536247CurrentTrain: epoch  0, batch     7 | loss: 14.0490847CurrentTrain: epoch  0, batch     8 | loss: 13.2168350CurrentTrain: epoch  0, batch     9 | loss: 14.3021908CurrentTrain: epoch  0, batch    10 | loss: 14.1167088CurrentTrain: epoch  0, batch    11 | loss: 13.4180393CurrentTrain: epoch  0, batch    12 | loss: 13.5131416CurrentTrain: epoch  0, batch    13 | loss: 13.0029678CurrentTrain: epoch  0, batch    14 | loss: 13.8142757CurrentTrain: epoch  0, batch    15 | loss: 13.3791180CurrentTrain: epoch  0, batch    16 | loss: 13.2017832CurrentTrain: epoch  0, batch    17 | loss: 13.7005796CurrentTrain: epoch  0, batch    18 | loss: 12.3532848CurrentTrain: epoch  0, batch    19 | loss: 13.4273310CurrentTrain: epoch  0, batch    20 | loss: 12.1328745CurrentTrain: epoch  0, batch    21 | loss: 13.2047520CurrentTrain: epoch  0, batch    22 | loss: 12.8094931CurrentTrain: epoch  0, batch    23 | loss: 12.9622526CurrentTrain: epoch  0, batch    24 | loss: 13.1095161CurrentTrain: epoch  0, batch    25 | loss: 12.4011106CurrentTrain: epoch  0, batch    26 | loss: 11.8890810CurrentTrain: epoch  0, batch    27 | loss: 11.5094337CurrentTrain: epoch  0, batch    28 | loss: 11.9468298CurrentTrain: epoch  0, batch    29 | loss: 11.7302113CurrentTrain: epoch  0, batch    30 | loss: 12.9821177CurrentTrain: epoch  0, batch    31 | loss: 12.8181553CurrentTrain: epoch  0, batch    32 | loss: 11.6547213CurrentTrain: epoch  0, batch    33 | loss: 11.8787823CurrentTrain: epoch  0, batch    34 | loss: 11.0018177CurrentTrain: epoch  0, batch    35 | loss: 10.9080772CurrentTrain: epoch  0, batch    36 | loss: 12.3523149CurrentTrain: epoch  0, batch    37 | loss: 10.9068823CurrentTrain: epoch  1, batch     0 | loss: 10.9679337CurrentTrain: epoch  1, batch     1 | loss: 11.5081463CurrentTrain: epoch  1, batch     2 | loss: 9.5159721CurrentTrain: epoch  1, batch     3 | loss: 10.7851105CurrentTrain: epoch  1, batch     4 | loss: 10.3378410CurrentTrain: epoch  1, batch     5 | loss: 10.5567312CurrentTrain: epoch  1, batch     6 | loss: 11.2714539CurrentTrain: epoch  1, batch     7 | loss: 11.3310518CurrentTrain: epoch  1, batch     8 | loss: 9.9209337CurrentTrain: epoch  1, batch     9 | loss: 11.0445747CurrentTrain: epoch  1, batch    10 | loss: 10.3060064CurrentTrain: epoch  1, batch    11 | loss: 11.1076736CurrentTrain: epoch  1, batch    12 | loss: 10.9865255CurrentTrain: epoch  1, batch    13 | loss: 10.3375063CurrentTrain: epoch  1, batch    14 | loss: 10.8974447CurrentTrain: epoch  1, batch    15 | loss: 10.2622805CurrentTrain: epoch  1, batch    16 | loss: 10.4239178CurrentTrain: epoch  1, batch    17 | loss: 9.9329634CurrentTrain: epoch  1, batch    18 | loss: 9.8125849CurrentTrain: epoch  1, batch    19 | loss: 8.5878925CurrentTrain: epoch  1, batch    20 | loss: 9.2436047CurrentTrain: epoch  1, batch    21 | loss: 8.7301559CurrentTrain: epoch  1, batch    22 | loss: 9.6982555CurrentTrain: epoch  1, batch    23 | loss: 10.2472467CurrentTrain: epoch  1, batch    24 | loss: 9.9856157CurrentTrain: epoch  1, batch    25 | loss: 9.6049461CurrentTrain: epoch  1, batch    26 | loss: 10.4781485CurrentTrain: epoch  1, batch    27 | loss: 8.4842644CurrentTrain: epoch  1, batch    28 | loss: 10.1658268CurrentTrain: epoch  1, batch    29 | loss: 10.7257671CurrentTrain: epoch  1, batch    30 | loss: 11.5004940CurrentTrain: epoch  1, batch    31 | loss: 9.7462788CurrentTrain: epoch  1, batch    32 | loss: 9.1821375CurrentTrain: epoch  1, batch    33 | loss: 9.7083035CurrentTrain: epoch  1, batch    34 | loss: 11.0745516CurrentTrain: epoch  1, batch    35 | loss: 8.9510183CurrentTrain: epoch  1, batch    36 | loss: 10.9841433CurrentTrain: epoch  1, batch    37 | loss: 7.4164252CurrentTrain: epoch  2, batch     0 | loss: 8.3740749CurrentTrain: epoch  2, batch     1 | loss: 8.4491796CurrentTrain: epoch  2, batch     2 | loss: 9.3439045CurrentTrain: epoch  2, batch     3 | loss: 8.3802938CurrentTrain: epoch  2, batch     4 | loss: 10.8007622CurrentTrain: epoch  2, batch     5 | loss: 10.0364132CurrentTrain: epoch  2, batch     6 | loss: 8.0491667CurrentTrain: epoch  2, batch     7 | loss: 9.0764093CurrentTrain: epoch  2, batch     8 | loss: 9.1537952CurrentTrain: epoch  2, batch     9 | loss: 8.0941267CurrentTrain: epoch  2, batch    10 | loss: 8.7080584CurrentTrain: epoch  2, batch    11 | loss: 8.8433666CurrentTrain: epoch  2, batch    12 | loss: 8.3713245CurrentTrain: epoch  2, batch    13 | loss: 9.9510660CurrentTrain: epoch  2, batch    14 | loss: 8.3875809CurrentTrain: epoch  2, batch    15 | loss: 9.1511030CurrentTrain: epoch  2, batch    16 | loss: 8.2696524CurrentTrain: epoch  2, batch    17 | loss: 9.1213474CurrentTrain: epoch  2, batch    18 | loss: 7.8844810CurrentTrain: epoch  2, batch    19 | loss: 8.9008169CurrentTrain: epoch  2, batch    20 | loss: 7.8021264CurrentTrain: epoch  2, batch    21 | loss: 7.8373294CurrentTrain: epoch  2, batch    22 | loss: 9.5220156CurrentTrain: epoch  2, batch    23 | loss: 8.4545155CurrentTrain: epoch  2, batch    24 | loss: 8.2381439CurrentTrain: epoch  2, batch    25 | loss: 7.9004865CurrentTrain: epoch  2, batch    26 | loss: 7.8095436CurrentTrain: epoch  2, batch    27 | loss: 9.4102392CurrentTrain: epoch  2, batch    28 | loss: 9.1616526CurrentTrain: epoch  2, batch    29 | loss: 7.9647479CurrentTrain: epoch  2, batch    30 | loss: 8.7500429CurrentTrain: epoch  2, batch    31 | loss: 7.2200713CurrentTrain: epoch  2, batch    32 | loss: 6.7891769CurrentTrain: epoch  2, batch    33 | loss: 7.3506002CurrentTrain: epoch  2, batch    34 | loss: 8.9319296CurrentTrain: epoch  2, batch    35 | loss: 8.5328789CurrentTrain: epoch  2, batch    36 | loss: 9.4791212CurrentTrain: epoch  2, batch    37 | loss: 8.6115770CurrentTrain: epoch  3, batch     0 | loss: 8.7581539CurrentTrain: epoch  3, batch     1 | loss: 8.4893970CurrentTrain: epoch  3, batch     2 | loss: 8.5675879CurrentTrain: epoch  3, batch     3 | loss: 6.7314353CurrentTrain: epoch  3, batch     4 | loss: 7.9375596CurrentTrain: epoch  3, batch     5 | loss: 7.7929010CurrentTrain: epoch  3, batch     6 | loss: 7.4823966CurrentTrain: epoch  3, batch     7 | loss: 6.6501818CurrentTrain: epoch  3, batch     8 | loss: 8.4865208CurrentTrain: epoch  3, batch     9 | loss: 8.1728191CurrentTrain: epoch  3, batch    10 | loss: 8.5151157CurrentTrain: epoch  3, batch    11 | loss: 6.5781841CurrentTrain: epoch  3, batch    12 | loss: 8.2453499CurrentTrain: epoch  3, batch    13 | loss: 7.1218877CurrentTrain: epoch  3, batch    14 | loss: 8.4512272CurrentTrain: epoch  3, batch    15 | loss: 9.3424683CurrentTrain: epoch  3, batch    16 | loss: 7.3630829CurrentTrain: epoch  3, batch    17 | loss: 6.9938092CurrentTrain: epoch  3, batch    18 | loss: 7.5905328CurrentTrain: epoch  3, batch    19 | loss: 7.2194991CurrentTrain: epoch  3, batch    20 | loss: 7.1444840CurrentTrain: epoch  3, batch    21 | loss: 7.8769975CurrentTrain: epoch  3, batch    22 | loss: 6.6785979CurrentTrain: epoch  3, batch    23 | loss: 6.8392568CurrentTrain: epoch  3, batch    24 | loss: 6.9959149CurrentTrain: epoch  3, batch    25 | loss: 6.6561761CurrentTrain: epoch  3, batch    26 | loss: 9.1724911CurrentTrain: epoch  3, batch    27 | loss: 7.4654484CurrentTrain: epoch  3, batch    28 | loss: 8.2520485CurrentTrain: epoch  3, batch    29 | loss: 6.6602821CurrentTrain: epoch  3, batch    30 | loss: 8.6201153CurrentTrain: epoch  3, batch    31 | loss: 6.8815241CurrentTrain: epoch  3, batch    32 | loss: 8.7723560CurrentTrain: epoch  3, batch    33 | loss: 6.8332896CurrentTrain: epoch  3, batch    34 | loss: 7.0076394CurrentTrain: epoch  3, batch    35 | loss: 7.8981085CurrentTrain: epoch  3, batch    36 | loss: 7.2400723CurrentTrain: epoch  3, batch    37 | loss: 6.4759059CurrentTrain: epoch  4, batch     0 | loss: 7.7868395CurrentTrain: epoch  4, batch     1 | loss: 7.8961749CurrentTrain: epoch  4, batch     2 | loss: 7.6443105CurrentTrain: epoch  4, batch     3 | loss: 6.2256966CurrentTrain: epoch  4, batch     4 | loss: 6.4257717CurrentTrain: epoch  4, batch     5 | loss: 8.1179953CurrentTrain: epoch  4, batch     6 | loss: 6.4774580CurrentTrain: epoch  4, batch     7 | loss: 6.8070521CurrentTrain: epoch  4, batch     8 | loss: 7.6868715CurrentTrain: epoch  4, batch     9 | loss: 8.6705666CurrentTrain: epoch  4, batch    10 | loss: 6.4537673CurrentTrain: epoch  4, batch    11 | loss: 7.6421857CurrentTrain: epoch  4, batch    12 | loss: 6.0898814CurrentTrain: epoch  4, batch    13 | loss: 6.6728773CurrentTrain: epoch  4, batch    14 | loss: 7.5863175CurrentTrain: epoch  4, batch    15 | loss: 7.1629009CurrentTrain: epoch  4, batch    16 | loss: 7.2871184CurrentTrain: epoch  4, batch    17 | loss: 6.7383628CurrentTrain: epoch  4, batch    18 | loss: 6.3482528CurrentTrain: epoch  4, batch    19 | loss: 7.2921724CurrentTrain: epoch  4, batch    20 | loss: 6.9440279CurrentTrain: epoch  4, batch    21 | loss: 7.5570087CurrentTrain: epoch  4, batch    22 | loss: 6.8637648CurrentTrain: epoch  4, batch    23 | loss: 6.0727496CurrentTrain: epoch  4, batch    24 | loss: 6.5588326CurrentTrain: epoch  4, batch    25 | loss: 6.1331429CurrentTrain: epoch  4, batch    26 | loss: 7.4638200CurrentTrain: epoch  4, batch    27 | loss: 7.7034302CurrentTrain: epoch  4, batch    28 | loss: 7.1399374CurrentTrain: epoch  4, batch    29 | loss: 7.1412764CurrentTrain: epoch  4, batch    30 | loss: 6.1521769CurrentTrain: epoch  4, batch    31 | loss: 6.0565753CurrentTrain: epoch  4, batch    32 | loss: 6.5439582CurrentTrain: epoch  4, batch    33 | loss: 7.6442847CurrentTrain: epoch  4, batch    34 | loss: 7.3000121CurrentTrain: epoch  4, batch    35 | loss: 8.5131254CurrentTrain: epoch  4, batch    36 | loss: 7.2582941CurrentTrain: epoch  4, batch    37 | loss: 8.0586281CurrentTrain: epoch  5, batch     0 | loss: 6.7613697CurrentTrain: epoch  5, batch     1 | loss: 6.4342895CurrentTrain: epoch  5, batch     2 | loss: 5.9581137CurrentTrain: epoch  5, batch     3 | loss: 6.4913163CurrentTrain: epoch  5, batch     4 | loss: 7.7180462CurrentTrain: epoch  5, batch     5 | loss: 7.3235149CurrentTrain: epoch  5, batch     6 | loss: 6.2328248CurrentTrain: epoch  5, batch     7 | loss: 5.9212284CurrentTrain: epoch  5, batch     8 | loss: 5.4888020CurrentTrain: epoch  5, batch     9 | loss: 6.5830812CurrentTrain: epoch  5, batch    10 | loss: 7.9193988CurrentTrain: epoch  5, batch    11 | loss: 6.4615083CurrentTrain: epoch  5, batch    12 | loss: 7.2001295CurrentTrain: epoch  5, batch    13 | loss: 6.5210600CurrentTrain: epoch  5, batch    14 | loss: 6.3900638CurrentTrain: epoch  5, batch    15 | loss: 6.2617106CurrentTrain: epoch  5, batch    16 | loss: 5.9366846CurrentTrain: epoch  5, batch    17 | loss: 6.5676904CurrentTrain: epoch  5, batch    18 | loss: 6.5813818CurrentTrain: epoch  5, batch    19 | loss: 7.4018517CurrentTrain: epoch  5, batch    20 | loss: 5.9584737CurrentTrain: epoch  5, batch    21 | loss: 6.3437362CurrentTrain: epoch  5, batch    22 | loss: 7.1899600CurrentTrain: epoch  5, batch    23 | loss: 7.4347768CurrentTrain: epoch  5, batch    24 | loss: 6.2635198CurrentTrain: epoch  5, batch    25 | loss: 6.9983163CurrentTrain: epoch  5, batch    26 | loss: 6.1503749CurrentTrain: epoch  5, batch    27 | loss: 6.1106873CurrentTrain: epoch  5, batch    28 | loss: 7.2875395CurrentTrain: epoch  5, batch    29 | loss: 6.6338320CurrentTrain: epoch  5, batch    30 | loss: 5.5390987CurrentTrain: epoch  5, batch    31 | loss: 6.9481034CurrentTrain: epoch  5, batch    32 | loss: 7.3438854CurrentTrain: epoch  5, batch    33 | loss: 5.1638589CurrentTrain: epoch  5, batch    34 | loss: 5.8426905CurrentTrain: epoch  5, batch    35 | loss: 6.5853505CurrentTrain: epoch  5, batch    36 | loss: 5.9638114CurrentTrain: epoch  5, batch    37 | loss: 5.1859603CurrentTrain: epoch  6, batch     0 | loss: 5.8045301CurrentTrain: epoch  6, batch     1 | loss: 5.7455554CurrentTrain: epoch  6, batch     2 | loss: 5.7363129CurrentTrain: epoch  6, batch     3 | loss: 5.6371946CurrentTrain: epoch  6, batch     4 | loss: 5.4137979CurrentTrain: epoch  6, batch     5 | loss: 5.8666172CurrentTrain: epoch  6, batch     6 | loss: 6.6207857CurrentTrain: epoch  6, batch     7 | loss: 5.4422116CurrentTrain: epoch  6, batch     8 | loss: 5.5402331CurrentTrain: epoch  6, batch     9 | loss: 6.1911545CurrentTrain: epoch  6, batch    10 | loss: 5.7927113CurrentTrain: epoch  6, batch    11 | loss: 5.2750769CurrentTrain: epoch  6, batch    12 | loss: 6.1648254CurrentTrain: epoch  6, batch    13 | loss: 5.4636493CurrentTrain: epoch  6, batch    14 | loss: 6.4366155CurrentTrain: epoch  6, batch    15 | loss: 6.4460835CurrentTrain: epoch  6, batch    16 | loss: 7.0741282CurrentTrain: epoch  6, batch    17 | loss: 6.5198088CurrentTrain: epoch  6, batch    18 | loss: 5.6065874CurrentTrain: epoch  6, batch    19 | loss: 5.9625735CurrentTrain: epoch  6, batch    20 | loss: 6.4680591CurrentTrain: epoch  6, batch    21 | loss: 6.4663024CurrentTrain: epoch  6, batch    22 | loss: 5.8108091CurrentTrain: epoch  6, batch    23 | loss: 5.8420873CurrentTrain: epoch  6, batch    24 | loss: 5.9079666CurrentTrain: epoch  6, batch    25 | loss: 5.8954325CurrentTrain: epoch  6, batch    26 | loss: 6.1188812CurrentTrain: epoch  6, batch    27 | loss: 5.5367036CurrentTrain: epoch  6, batch    28 | loss: 5.3077912CurrentTrain: epoch  6, batch    29 | loss: 5.5342655CurrentTrain: epoch  6, batch    30 | loss: 6.3738403CurrentTrain: epoch  6, batch    31 | loss: 5.4428205CurrentTrain: epoch  6, batch    32 | loss: 5.2638063CurrentTrain: epoch  6, batch    33 | loss: 5.3670745CurrentTrain: epoch  6, batch    34 | loss: 5.5088892CurrentTrain: epoch  6, batch    35 | loss: 6.4959030CurrentTrain: epoch  6, batch    36 | loss: 6.5644789CurrentTrain: epoch  6, batch    37 | loss: 6.5436897CurrentTrain: epoch  7, batch     0 | loss: 6.2681594CurrentTrain: epoch  7, batch     1 | loss: 5.1075053CurrentTrain: epoch  7, batch     2 | loss: 6.8938146CurrentTrain: epoch  7, batch     3 | loss: 5.2961898CurrentTrain: epoch  7, batch     4 | loss: 6.3552718CurrentTrain: epoch  7, batch     5 | loss: 5.5871687CurrentTrain: epoch  7, batch     6 | loss: 5.8418174CurrentTrain: epoch  7, batch     7 | loss: 6.3703313CurrentTrain: epoch  7, batch     8 | loss: 5.7382889CurrentTrain: epoch  7, batch     9 | loss: 5.6177640CurrentTrain: epoch  7, batch    10 | loss: 5.6878891CurrentTrain: epoch  7, batch    11 | loss: 5.3563557CurrentTrain: epoch  7, batch    12 | loss: 5.6263661CurrentTrain: epoch  7, batch    13 | loss: 5.8152757CurrentTrain: epoch  7, batch    14 | loss: 6.1620421CurrentTrain: epoch  7, batch    15 | loss: 5.5324955CurrentTrain: epoch  7, batch    16 | loss: 5.8871598CurrentTrain: epoch  7, batch    17 | loss: 5.5423484CurrentTrain: epoch  7, batch    18 | loss: 6.2774777CurrentTrain: epoch  7, batch    19 | loss: 5.6616325CurrentTrain: epoch  7, batch    20 | loss: 5.5943713CurrentTrain: epoch  7, batch    21 | loss: 5.5923638CurrentTrain: epoch  7, batch    22 | loss: 5.4838395CurrentTrain: epoch  7, batch    23 | loss: 6.2171235CurrentTrain: epoch  7, batch    24 | loss: 5.5002079CurrentTrain: epoch  7, batch    25 | loss: 5.4838471CurrentTrain: epoch  7, batch    26 | loss: 5.2851448CurrentTrain: epoch  7, batch    27 | loss: 6.7392592CurrentTrain: epoch  7, batch    28 | loss: 5.2158246CurrentTrain: epoch  7, batch    29 | loss: 5.4201002CurrentTrain: epoch  7, batch    30 | loss: 5.2841010CurrentTrain: epoch  7, batch    31 | loss: 6.3558121CurrentTrain: epoch  7, batch    32 | loss: 5.3866806CurrentTrain: epoch  7, batch    33 | loss: 5.4177003CurrentTrain: epoch  7, batch    34 | loss: 5.3117547CurrentTrain: epoch  7, batch    35 | loss: 5.3936677CurrentTrain: epoch  7, batch    36 | loss: 5.3194785CurrentTrain: epoch  7, batch    37 | loss: 5.1809731CurrentTrain: epoch  8, batch     0 | loss: 5.1590657CurrentTrain: epoch  8, batch     1 | loss: 5.4155188CurrentTrain: epoch  8, batch     2 | loss: 5.5106564CurrentTrain: epoch  8, batch     3 | loss: 5.7081356CurrentTrain: epoch  8, batch     4 | loss: 5.1594305CurrentTrain: epoch  8, batch     5 | loss: 6.0475879CurrentTrain: epoch  8, batch     6 | loss: 5.9563332CurrentTrain: epoch  8, batch     7 | loss: 5.3987322CurrentTrain: epoch  8, batch     8 | loss: 5.2476816CurrentTrain: epoch  8, batch     9 | loss: 5.1299496CurrentTrain: epoch  8, batch    10 | loss: 5.5231385CurrentTrain: epoch  8, batch    11 | loss: 5.1971021CurrentTrain: epoch  8, batch    12 | loss: 5.1912484CurrentTrain: epoch  8, batch    13 | loss: 5.5130897CurrentTrain: epoch  8, batch    14 | loss: 5.1765604CurrentTrain: epoch  8, batch    15 | loss: 6.0966640CurrentTrain: epoch  8, batch    16 | loss: 5.1869550CurrentTrain: epoch  8, batch    17 | loss: 5.4539938CurrentTrain: epoch  8, batch    18 | loss: 5.4807615CurrentTrain: epoch  8, batch    19 | loss: 5.4435501CurrentTrain: epoch  8, batch    20 | loss: 5.4748759CurrentTrain: epoch  8, batch    21 | loss: 5.1856132CurrentTrain: epoch  8, batch    22 | loss: 6.0933719CurrentTrain: epoch  8, batch    23 | loss: 5.4135418CurrentTrain: epoch  8, batch    24 | loss: 5.1279349CurrentTrain: epoch  8, batch    25 | loss: 5.1877217CurrentTrain: epoch  8, batch    26 | loss: 4.9682064CurrentTrain: epoch  8, batch    27 | loss: 6.3536534CurrentTrain: epoch  8, batch    28 | loss: 5.1031213CurrentTrain: epoch  8, batch    29 | loss: 5.0391469CurrentTrain: epoch  8, batch    30 | loss: 5.2937336CurrentTrain: epoch  8, batch    31 | loss: 5.3410821CurrentTrain: epoch  8, batch    32 | loss: 5.3572893CurrentTrain: epoch  8, batch    33 | loss: 5.4632287CurrentTrain: epoch  8, batch    34 | loss: 4.9796476CurrentTrain: epoch  8, batch    35 | loss: 5.5873537CurrentTrain: epoch  8, batch    36 | loss: 5.1429305CurrentTrain: epoch  8, batch    37 | loss: 4.9647980CurrentTrain: epoch  9, batch     0 | loss: 4.9875908CurrentTrain: epoch  9, batch     1 | loss: 5.3139729CurrentTrain: epoch  9, batch     2 | loss: 5.0126600CurrentTrain: epoch  9, batch     3 | loss: 5.5776868CurrentTrain: epoch  9, batch     4 | loss: 5.5200062CurrentTrain: epoch  9, batch     5 | loss: 5.4151654CurrentTrain: epoch  9, batch     6 | loss: 5.5828552CurrentTrain: epoch  9, batch     7 | loss: 5.3389797CurrentTrain: epoch  9, batch     8 | loss: 5.2272892CurrentTrain: epoch  9, batch     9 | loss: 5.0207143CurrentTrain: epoch  9, batch    10 | loss: 5.2736197CurrentTrain: epoch  9, batch    11 | loss: 5.2384272CurrentTrain: epoch  9, batch    12 | loss: 5.0940995CurrentTrain: epoch  9, batch    13 | loss: 4.9074073CurrentTrain: epoch  9, batch    14 | loss: 5.0987902CurrentTrain: epoch  9, batch    15 | loss: 5.0728951CurrentTrain: epoch  9, batch    16 | loss: 5.1613550CurrentTrain: epoch  9, batch    17 | loss: 5.2562966CurrentTrain: epoch  9, batch    18 | loss: 4.9851522CurrentTrain: epoch  9, batch    19 | loss: 4.8506789CurrentTrain: epoch  9, batch    20 | loss: 5.8739510CurrentTrain: epoch  9, batch    21 | loss: 5.0330305CurrentTrain: epoch  9, batch    22 | loss: 5.1182971CurrentTrain: epoch  9, batch    23 | loss: 4.8261328CurrentTrain: epoch  9, batch    24 | loss: 5.2216249CurrentTrain: epoch  9, batch    25 | loss: 5.8519373CurrentTrain: epoch  9, batch    26 | loss: 5.7964883CurrentTrain: epoch  9, batch    27 | loss: 4.8811564CurrentTrain: epoch  9, batch    28 | loss: 5.1287746CurrentTrain: epoch  9, batch    29 | loss: 4.9105186CurrentTrain: epoch  9, batch    30 | loss: 5.8362513CurrentTrain: epoch  9, batch    31 | loss: 5.0459433CurrentTrain: epoch  9, batch    32 | loss: 4.8688617CurrentTrain: epoch  9, batch    33 | loss: 5.9432235CurrentTrain: epoch  9, batch    34 | loss: 5.0861144CurrentTrain: epoch  9, batch    35 | loss: 5.5264502CurrentTrain: epoch  9, batch    36 | loss: 5.4557295CurrentTrain: epoch  9, batch    37 | loss: 4.9870057
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 90.62%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 89.58%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 87.50%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 86.76%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 85.76%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 84.87%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 84.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 86.08%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 86.68%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 87.24%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 88.22%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 88.43%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.84%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 89.01%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 89.31%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.65%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 88.07%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 90.62%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 89.58%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 87.50%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 86.76%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 85.76%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 84.87%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 84.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 86.08%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 86.68%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 87.24%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 88.22%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 88.43%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.84%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 89.01%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 89.31%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.65%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 88.07%   
cur_acc_llm:  [0.8806818181818182]
his_acc_llm:  [0.8806818181818182]
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.38%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.78%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.28%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 87.90%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.55%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.38%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.78%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.28%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 87.90%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.55%   
cur_acc:  ['0.8655']
his_acc:  ['0.8655']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 6.3425827CurrentTrain: epoch  0, batch     1 | loss: 6.2066483CurrentTrain: epoch  1, batch     0 | loss: 5.5020986CurrentTrain: epoch  1, batch     1 | loss: 5.5710354CurrentTrain: epoch  2, batch     0 | loss: 4.8375096CurrentTrain: epoch  2, batch     1 | loss: 4.3201113CurrentTrain: epoch  3, batch     0 | loss: 4.0274405CurrentTrain: epoch  3, batch     1 | loss: 3.5601137CurrentTrain: epoch  4, batch     0 | loss: 3.8024940CurrentTrain: epoch  4, batch     1 | loss: 3.0481524CurrentTrain: epoch  5, batch     0 | loss: 2.9670660CurrentTrain: epoch  5, batch     1 | loss: 3.0697587CurrentTrain: epoch  6, batch     0 | loss: 2.8721147CurrentTrain: epoch  6, batch     1 | loss: 2.6564929CurrentTrain: epoch  7, batch     0 | loss: 2.2181883CurrentTrain: epoch  7, batch     1 | loss: 2.5480373CurrentTrain: epoch  8, batch     0 | loss: 2.2676120CurrentTrain: epoch  8, batch     1 | loss: 2.2555106CurrentTrain: epoch  9, batch     0 | loss: 2.2198422CurrentTrain: epoch  9, batch     1 | loss: 1.9563895
Mixup data size:  60

MemoryTrain:  epoch  0, batch     0 | loss: 1.8605785MemoryTrain:  epoch  1, batch     0 | loss: 2.1781499MemoryTrain:  epoch  2, batch     0 | loss: 0.8476768MemoryTrain:  epoch  3, batch     0 | loss: 0.4333951MemoryTrain:  epoch  4, batch     0 | loss: 0.3193691MemoryTrain:  epoch  5, batch     0 | loss: 0.1132618MemoryTrain:  epoch  6, batch     0 | loss: 0.0674421MemoryTrain:  epoch  7, batch     0 | loss: 0.0427915MemoryTrain:  epoch  8, batch     0 | loss: 0.0254785MemoryTrain:  epoch  9, batch     0 | loss: 0.0162676
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 9.6543837CurrentTrain: epoch  0, batch     1 | loss: 8.0155869CurrentTrain: epoch  1, batch     0 | loss: 8.7005749CurrentTrain: epoch  1, batch     1 | loss: 7.9767685CurrentTrain: epoch  2, batch     0 | loss: 7.8820276CurrentTrain: epoch  2, batch     1 | loss: 7.8267908CurrentTrain: epoch  3, batch     0 | loss: 7.3360023CurrentTrain: epoch  3, batch     1 | loss: 6.0152373CurrentTrain: epoch  4, batch     0 | loss: 6.8971853CurrentTrain: epoch  4, batch     1 | loss: 5.3665295CurrentTrain: epoch  5, batch     0 | loss: 6.7887502CurrentTrain: epoch  5, batch     1 | loss: 4.9875441CurrentTrain: epoch  6, batch     0 | loss: 5.4617624CurrentTrain: epoch  6, batch     1 | loss: 5.7763386CurrentTrain: epoch  7, batch     0 | loss: 5.2323446CurrentTrain: epoch  7, batch     1 | loss: 5.0435815CurrentTrain: epoch  8, batch     0 | loss: 4.5331497CurrentTrain: epoch  8, batch     1 | loss: 4.1709924CurrentTrain: epoch  9, batch     0 | loss: 4.1766968CurrentTrain: epoch  9, batch     1 | loss: 3.7969222
Mixup data size:  59
MixupTrain:  epoch  0, batch     2 | loss: 6.1582012
MemoryTrain:  epoch  0, batch     0 | loss: 1.3367944MemoryTrain:  epoch  1, batch     0 | loss: 1.3516474MemoryTrain:  epoch  2, batch     0 | loss: 1.3108916MemoryTrain:  epoch  3, batch     0 | loss: 1.3199563MemoryTrain:  epoch  4, batch     0 | loss: 0.9539142MemoryTrain:  epoch  5, batch     0 | loss: 0.5765671MemoryTrain:  epoch  6, batch     0 | loss: 0.4747962MemoryTrain:  epoch  7, batch     0 | loss: 0.5113429MemoryTrain:  epoch  8, batch     0 | loss: 0.3985747MemoryTrain:  epoch  9, batch     0 | loss: 0.3489978
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 76.88%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 73.44%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 73.56%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 70.98%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 84.82%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 84.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 82.42%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 81.99%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 80.92%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 82.95%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 83.42%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.11%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.34%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.65%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.16%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 86.42%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 86.49%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 86.93%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 86.40%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 86.43%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 86.11%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 85.64%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 85.86%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 86.06%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 85.78%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 85.21%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 84.82%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 84.16%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 83.52%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 82.92%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 82.61%   [EVAL] batch:   46 | acc: 6.25%,  total acc: 80.98%   
cur_acc_llm:  [0.8806818181818182, 0.7098214285714286]
his_acc_llm:  [0.8806818181818182, 0.8098404255319149]
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 82.14%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 63.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 75.69%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 77.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 80.77%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 79.91%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 79.58%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 78.12%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 77.94%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 77.08%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 76.64%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 77.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 78.27%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 79.26%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 80.16%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 80.99%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 81.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.45%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 82.87%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.48%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.05%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 84.17%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 84.48%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 84.77%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 84.85%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 84.90%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 84.97%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 85.20%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.58%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 85.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.67%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 85.86%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 85.76%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 85.65%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 85.97%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 84.78%   [EVAL] batch:   46 | acc: 0.00%,  total acc: 82.98%   
cur_acc:  ['0.8655', '0.8214']
his_acc:  ['0.8655', '0.8298']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 6.9742603CurrentTrain: epoch  0, batch     1 | loss: 5.3054175CurrentTrain: epoch  1, batch     0 | loss: 6.1777730CurrentTrain: epoch  1, batch     1 | loss: 4.5213223CurrentTrain: epoch  2, batch     0 | loss: 5.6458850CurrentTrain: epoch  2, batch     1 | loss: 3.9271677CurrentTrain: epoch  3, batch     0 | loss: 4.9069428CurrentTrain: epoch  3, batch     1 | loss: 3.6325409CurrentTrain: epoch  4, batch     0 | loss: 3.9793656CurrentTrain: epoch  4, batch     1 | loss: 4.6885896CurrentTrain: epoch  5, batch     0 | loss: 3.6039884CurrentTrain: epoch  5, batch     1 | loss: 4.0870671CurrentTrain: epoch  6, batch     0 | loss: 3.3367712CurrentTrain: epoch  6, batch     1 | loss: 3.9414318CurrentTrain: epoch  7, batch     0 | loss: 3.2606914CurrentTrain: epoch  7, batch     1 | loss: 3.8153746CurrentTrain: epoch  8, batch     0 | loss: 2.6496685CurrentTrain: epoch  8, batch     1 | loss: 4.0548420CurrentTrain: epoch  9, batch     0 | loss: 3.3588934CurrentTrain: epoch  9, batch     1 | loss: 2.2069740
Mixup data size:  71
MixupTrain:  epoch  0, batch     0 | loss: 7.4202430MixupTrain:  epoch  0, batch     4 | loss: 4.1662306
MemoryTrain:  epoch  0, batch     0 | loss: 1.5300980MemoryTrain:  epoch  1, batch     0 | loss: 2.5725961MemoryTrain:  epoch  2, batch     0 | loss: 2.0976055MemoryTrain:  epoch  3, batch     0 | loss: 1.3109176MemoryTrain:  epoch  4, batch     0 | loss: 0.7856435MemoryTrain:  epoch  5, batch     0 | loss: 0.5782270MemoryTrain:  epoch  6, batch     0 | loss: 0.5656086MemoryTrain:  epoch  7, batch     0 | loss: 0.2340304MemoryTrain:  epoch  8, batch     0 | loss: 0.3054281MemoryTrain:  epoch  9, batch     0 | loss: 0.1282266
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 8.8808975CurrentTrain: epoch  0, batch     1 | loss: 7.0293241CurrentTrain: epoch  1, batch     0 | loss: 7.5671735CurrentTrain: epoch  1, batch     1 | loss: 6.2819366CurrentTrain: epoch  2, batch     0 | loss: 6.8217745CurrentTrain: epoch  2, batch     1 | loss: 4.9378963CurrentTrain: epoch  3, batch     0 | loss: 6.0049477CurrentTrain: epoch  3, batch     1 | loss: 5.0613317CurrentTrain: epoch  4, batch     0 | loss: 4.7529569CurrentTrain: epoch  4, batch     1 | loss: 6.4066467CurrentTrain: epoch  5, batch     0 | loss: 5.5916758CurrentTrain: epoch  5, batch     1 | loss: 3.8584733CurrentTrain: epoch  6, batch     0 | loss: 4.9354129CurrentTrain: epoch  6, batch     1 | loss: 3.4717188CurrentTrain: epoch  7, batch     0 | loss: 3.5981708CurrentTrain: epoch  7, batch     1 | loss: 4.5381122CurrentTrain: epoch  8, batch     0 | loss: 4.1896901CurrentTrain: epoch  8, batch     1 | loss: 3.0324848CurrentTrain: epoch  9, batch     0 | loss: 4.1443892CurrentTrain: epoch  9, batch     1 | loss: 2.2878399
Mixup data size:  71

MemoryTrain:  epoch  0, batch     0 | loss: 2.8678551MemoryTrain:  epoch  1, batch     0 | loss: 3.5000315MemoryTrain:  epoch  2, batch     0 | loss: 2.5890856MemoryTrain:  epoch  3, batch     0 | loss: 1.7712009MemoryTrain:  epoch  4, batch     0 | loss: 2.1197412MemoryTrain:  epoch  5, batch     0 | loss: 1.7026982MemoryTrain:  epoch  6, batch     0 | loss: 1.5344298MemoryTrain:  epoch  7, batch     0 | loss: 1.4561611MemoryTrain:  epoch  8, batch     0 | loss: 1.4507701MemoryTrain:  epoch  9, batch     0 | loss: 1.2662106
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 51.56%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 42.50%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 36.46%   [EVAL] batch:    6 | acc: 12.50%,  total acc: 33.04%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 28.91%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 54.17%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 53.12%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 51.25%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 51.04%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 55.36%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 59.38%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 63.19%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 67.05%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 69.27%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 69.71%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 67.86%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 68.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 67.58%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 68.01%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 68.06%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 68.42%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 69.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 71.13%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 72.44%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 73.64%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 74.74%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 75.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.68%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 77.31%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.66%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 78.96%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 79.44%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 79.88%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 79.73%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 79.23%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 79.29%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 79.34%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 79.90%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 80.43%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 80.45%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 79.69%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 78.96%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 78.87%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 78.20%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 77.70%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 77.08%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 76.63%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 76.46%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 76.95%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 75.64%   [EVAL] batch:   49 | acc: 25.00%,  total acc: 74.62%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 73.28%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 72.00%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 70.87%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 69.56%   
cur_acc_llm:  [0.8806818181818182, 0.7098214285714286, 0.2890625]
his_acc_llm:  [0.8806818181818182, 0.8098404255319149, 0.6956018518518519]
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 55.00%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 51.04%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 51.79%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 46.09%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 28.12%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 31.25%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 34.38%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 37.50%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 42.19%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 45.14%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 48.12%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 49.43%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 51.44%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 52.68%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 54.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 54.30%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 55.51%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 55.90%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 57.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 59.52%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 61.36%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 63.04%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 66.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 67.31%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 68.29%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 69.42%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 70.47%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 71.04%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 71.57%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 72.27%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 72.73%   [EVAL] batch:   33 | acc: 37.50%,  total acc: 71.69%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 71.61%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 71.18%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 70.44%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 70.72%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 70.51%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 70.62%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 71.34%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 71.58%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 71.95%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 72.16%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 72.78%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 71.74%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 71.68%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 72.14%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 70.92%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 70.62%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 69.98%   [EVAL] batch:   51 | acc: 31.25%,  total acc: 69.23%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 68.99%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 67.94%   
cur_acc:  ['0.8655', '0.8214', '0.4609']
his_acc:  ['0.8655', '0.8298', '0.6794']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 5.1677885CurrentTrain: epoch  0, batch     1 | loss: 5.2116370CurrentTrain: epoch  1, batch     0 | loss: 3.9162714CurrentTrain: epoch  1, batch     1 | loss: 3.5496724CurrentTrain: epoch  2, batch     0 | loss: 3.2432919CurrentTrain: epoch  2, batch     1 | loss: 2.9794998CurrentTrain: epoch  3, batch     0 | loss: 2.7108655CurrentTrain: epoch  3, batch     1 | loss: 2.9847276CurrentTrain: epoch  4, batch     0 | loss: 2.4214520CurrentTrain: epoch  4, batch     1 | loss: 2.1892300CurrentTrain: epoch  5, batch     0 | loss: 2.1608064CurrentTrain: epoch  5, batch     1 | loss: 1.9419972CurrentTrain: epoch  6, batch     0 | loss: 1.9674500CurrentTrain: epoch  6, batch     1 | loss: 1.8033926CurrentTrain: epoch  7, batch     0 | loss: 1.9368548CurrentTrain: epoch  7, batch     1 | loss: 1.8025793CurrentTrain: epoch  8, batch     0 | loss: 1.7817756CurrentTrain: epoch  8, batch     1 | loss: 1.8529726CurrentTrain: epoch  9, batch     0 | loss: 1.8048913CurrentTrain: epoch  9, batch     1 | loss: 1.7514588
Mixup data size:  81
MixupTrain:  epoch  0, batch     0 | loss: 4.8752302MixupTrain:  epoch  0, batch     1 | loss: 4.2246096MixupTrain:  epoch  0, batch     2 | loss: 3.7867971MixupTrain:  epoch  0, batch     3 | loss: 3.9288702MixupTrain:  epoch  0, batch     4 | loss: 4.1889447MixupTrain:  epoch  0, batch     5 | loss: 2.5103445
MemoryTrain:  epoch  0, batch     0 | loss: 0.3050551MemoryTrain:  epoch  0, batch     1 | loss: 0.0752062MemoryTrain:  epoch  1, batch     0 | loss: 1.3500402MemoryTrain:  epoch  1, batch     1 | loss: 0.8243797MemoryTrain:  epoch  2, batch     0 | loss: 0.5195096MemoryTrain:  epoch  2, batch     1 | loss: 0.6045041MemoryTrain:  epoch  3, batch     0 | loss: 0.5958447MemoryTrain:  epoch  3, batch     1 | loss: 0.0705399MemoryTrain:  epoch  4, batch     0 | loss: 0.4159696MemoryTrain:  epoch  4, batch     1 | loss: 0.0328400MemoryTrain:  epoch  5, batch     0 | loss: 0.1392829MemoryTrain:  epoch  5, batch     1 | loss: 0.0656687MemoryTrain:  epoch  6, batch     0 | loss: 0.0506430MemoryTrain:  epoch  6, batch     1 | loss: 0.1219071MemoryTrain:  epoch  7, batch     0 | loss: 0.0452855MemoryTrain:  epoch  7, batch     1 | loss: 0.0385780MemoryTrain:  epoch  8, batch     0 | loss: 0.0536256MemoryTrain:  epoch  8, batch     1 | loss: 0.0367151MemoryTrain:  epoch  9, batch     0 | loss: 0.1397545MemoryTrain:  epoch  9, batch     1 | loss: 0.0422372
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 5.9768496CurrentTrain: epoch  0, batch     1 | loss: 5.7317581CurrentTrain: epoch  1, batch     0 | loss: 4.8261743CurrentTrain: epoch  1, batch     1 | loss: 4.1374593CurrentTrain: epoch  2, batch     0 | loss: 4.0146194CurrentTrain: epoch  2, batch     1 | loss: 3.0641880CurrentTrain: epoch  3, batch     0 | loss: 3.5217733CurrentTrain: epoch  3, batch     1 | loss: 3.1708710CurrentTrain: epoch  4, batch     0 | loss: 4.3791275CurrentTrain: epoch  4, batch     1 | loss: 3.8928463CurrentTrain: epoch  5, batch     0 | loss: 3.6047101CurrentTrain: epoch  5, batch     1 | loss: 2.5885208CurrentTrain: epoch  6, batch     0 | loss: 2.6725392CurrentTrain: epoch  6, batch     1 | loss: 2.6184812CurrentTrain: epoch  7, batch     0 | loss: 2.4973664CurrentTrain: epoch  7, batch     1 | loss: 2.2724905CurrentTrain: epoch  8, batch     0 | loss: 2.2070599CurrentTrain: epoch  8, batch     1 | loss: 2.2429256CurrentTrain: epoch  9, batch     0 | loss: 2.1527064CurrentTrain: epoch  9, batch     1 | loss: 2.3289928
Mixup data size:  81
MixupTrain:  epoch  0, batch     0 | loss: 5.6320340MixupTrain:  epoch  0, batch     1 | loss: 4.1942063MixupTrain:  epoch  0, batch     2 | loss: 5.3107743MixupTrain:  epoch  0, batch     3 | loss: 4.8610428MixupTrain:  epoch  0, batch     4 | loss: 4.3369657MixupTrain:  epoch  0, batch     5 | loss: 2.2008540
MemoryTrain:  epoch  0, batch     0 | loss: 2.7345679MemoryTrain:  epoch  0, batch     1 | loss: 0.2519413MemoryTrain:  epoch  1, batch     0 | loss: 2.2730300MemoryTrain:  epoch  1, batch     1 | loss: 1.9869287MemoryTrain:  epoch  2, batch     0 | loss: 1.8546209MemoryTrain:  epoch  2, batch     1 | loss: 1.6778947MemoryTrain:  epoch  3, batch     0 | loss: 1.5976124MemoryTrain:  epoch  3, batch     1 | loss: 0.9964353MemoryTrain:  epoch  4, batch     0 | loss: 1.0905535MemoryTrain:  epoch  4, batch     1 | loss: 0.5886090MemoryTrain:  epoch  5, batch     0 | loss: 0.9154746MemoryTrain:  epoch  5, batch     1 | loss: 0.7581808MemoryTrain:  epoch  6, batch     0 | loss: 1.0632648MemoryTrain:  epoch  6, batch     1 | loss: 0.8235505MemoryTrain:  epoch  7, batch     0 | loss: 1.1211467MemoryTrain:  epoch  7, batch     1 | loss: 0.1647410MemoryTrain:  epoch  8, batch     0 | loss: 0.8023314MemoryTrain:  epoch  8, batch     1 | loss: 0.4230891MemoryTrain:  epoch  9, batch     0 | loss: 0.7621159MemoryTrain:  epoch  9, batch     1 | loss: 0.2221818
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 96.43%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 92.19%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 81.25%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 77.78%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 76.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 76.70%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 75.96%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 74.11%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 74.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 73.05%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 73.16%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 73.03%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 74.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 75.30%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 76.42%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 77.17%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 79.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.81%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 77.31%   [EVAL] batch:   27 | acc: 12.50%,  total acc: 75.00%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 72.63%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 70.42%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 68.35%   [EVAL] batch:   31 | acc: 12.50%,  total acc: 66.60%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 65.72%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 65.18%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 65.10%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 64.53%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 65.13%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 65.06%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 65.00%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 64.94%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 65.33%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 64.97%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 65.06%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 64.86%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 65.22%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 65.16%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 65.89%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 64.80%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 63.50%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 62.25%   [EVAL] batch:   51 | acc: 0.00%,  total acc: 61.06%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 59.91%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 60.30%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 60.91%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 61.38%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 61.95%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 62.61%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 63.24%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 63.85%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 64.14%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 64.01%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 63.99%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 63.87%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 64.13%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 64.11%   
cur_acc_llm:  [0.8806818181818182, 0.7098214285714286, 0.2890625, 0.8125]
his_acc_llm:  [0.8806818181818182, 0.8098404255319149, 0.6956018518518519, 0.6410984848484849]
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 66.96%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 67.19%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 64.58%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 66.25%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 66.48%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 68.23%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 66.35%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 26.56%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 30.00%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 37.50%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 42.97%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 46.53%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 48.75%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 50.57%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 51.56%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 52.40%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 53.57%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 55.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 55.08%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 56.25%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 56.60%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 56.91%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 58.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 60.42%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 62.22%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 63.86%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 65.36%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 66.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 68.03%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 68.98%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 70.09%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 71.12%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 71.46%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 71.77%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 72.27%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 72.73%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 71.32%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 69.64%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 68.06%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 66.22%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 65.30%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 63.94%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 64.22%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 65.09%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 66.28%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 66.62%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 67.22%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 66.17%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 66.36%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 67.06%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 65.94%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 65.62%   [EVAL] batch:   50 | acc: 31.25%,  total acc: 64.95%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 64.06%   [EVAL] batch:   52 | acc: 31.25%,  total acc: 63.44%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 63.89%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 64.43%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 64.84%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 65.24%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 64.55%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 64.09%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 63.85%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 63.83%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 63.71%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 63.89%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 64.06%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 64.23%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 64.20%   
cur_acc:  ['0.8655', '0.8214', '0.4609', '0.6635']
his_acc:  ['0.8655', '0.8298', '0.6794', '0.6420']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 8.4248648CurrentTrain: epoch  0, batch     1 | loss: 7.7406836CurrentTrain: epoch  1, batch     0 | loss: 6.8818789CurrentTrain: epoch  1, batch     1 | loss: 6.6614900CurrentTrain: epoch  2, batch     0 | loss: 6.3113146CurrentTrain: epoch  2, batch     1 | loss: 5.6879325CurrentTrain: epoch  3, batch     0 | loss: 5.8051085CurrentTrain: epoch  3, batch     1 | loss: 5.6641421CurrentTrain: epoch  4, batch     0 | loss: 5.6690893CurrentTrain: epoch  4, batch     1 | loss: 4.7099280CurrentTrain: epoch  5, batch     0 | loss: 5.5195236CurrentTrain: epoch  5, batch     1 | loss: 3.9789128CurrentTrain: epoch  6, batch     0 | loss: 4.5997715CurrentTrain: epoch  6, batch     1 | loss: 4.7105780CurrentTrain: epoch  7, batch     0 | loss: 4.6573100CurrentTrain: epoch  7, batch     1 | loss: 4.1143117CurrentTrain: epoch  8, batch     0 | loss: 4.2828879CurrentTrain: epoch  8, batch     1 | loss: 4.1504226CurrentTrain: epoch  9, batch     0 | loss: 3.6271663CurrentTrain: epoch  9, batch     1 | loss: 4.2633305
Mixup data size:  90
MixupTrain:  epoch  0, batch     0 | loss: 3.1222841MixupTrain:  epoch  0, batch     1 | loss: 3.1815878MixupTrain:  epoch  0, batch     2 | loss: 3.1388711MixupTrain:  epoch  0, batch     3 | loss: 2.9415574MixupTrain:  epoch  0, batch     4 | loss: 3.9002055MixupTrain:  epoch  0, batch     5 | loss: 3.3417603
MemoryTrain:  epoch  0, batch     0 | loss: 0.3319347MemoryTrain:  epoch  0, batch     1 | loss: 0.5744327MemoryTrain:  epoch  1, batch     0 | loss: 0.7639312MemoryTrain:  epoch  1, batch     1 | loss: 1.4402667MemoryTrain:  epoch  2, batch     0 | loss: 0.7690811MemoryTrain:  epoch  2, batch     1 | loss: 0.4096700MemoryTrain:  epoch  3, batch     0 | loss: 0.3020598MemoryTrain:  epoch  3, batch     1 | loss: 0.1522112MemoryTrain:  epoch  4, batch     0 | loss: 0.1890207MemoryTrain:  epoch  4, batch     1 | loss: 0.1083221MemoryTrain:  epoch  5, batch     0 | loss: 0.0863167MemoryTrain:  epoch  5, batch     1 | loss: 0.1519960MemoryTrain:  epoch  6, batch     0 | loss: 0.1491778MemoryTrain:  epoch  6, batch     1 | loss: 0.0302598MemoryTrain:  epoch  7, batch     0 | loss: 0.0484876MemoryTrain:  epoch  7, batch     1 | loss: 0.0492732MemoryTrain:  epoch  8, batch     0 | loss: 0.0472684MemoryTrain:  epoch  8, batch     1 | loss: 0.0335188MemoryTrain:  epoch  9, batch     0 | loss: 0.0380655MemoryTrain:  epoch  9, batch     1 | loss: 0.0542459
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 9.3432274CurrentTrain: epoch  0, batch     1 | loss: 9.5411940CurrentTrain: epoch  1, batch     0 | loss: 8.3398905CurrentTrain: epoch  1, batch     1 | loss: 8.3830662CurrentTrain: epoch  2, batch     0 | loss: 8.2720127CurrentTrain: epoch  2, batch     1 | loss: 6.4110231CurrentTrain: epoch  3, batch     0 | loss: 7.7366691CurrentTrain: epoch  3, batch     1 | loss: 6.2119298CurrentTrain: epoch  4, batch     0 | loss: 7.2811451CurrentTrain: epoch  4, batch     1 | loss: 5.4961414CurrentTrain: epoch  5, batch     0 | loss: 6.6249166CurrentTrain: epoch  5, batch     1 | loss: 4.9518065CurrentTrain: epoch  6, batch     0 | loss: 6.3912144CurrentTrain: epoch  6, batch     1 | loss: 5.0517840CurrentTrain: epoch  7, batch     0 | loss: 6.2394142CurrentTrain: epoch  7, batch     1 | loss: 4.5456181CurrentTrain: epoch  8, batch     0 | loss: 5.4332199CurrentTrain: epoch  8, batch     1 | loss: 4.2623935CurrentTrain: epoch  9, batch     0 | loss: 4.8990231CurrentTrain: epoch  9, batch     1 | loss: 4.9918151
Mixup data size:  91

MemoryTrain:  epoch  0, batch     0 | loss: 1.7346166MemoryTrain:  epoch  0, batch     1 | loss: 1.1335937MemoryTrain:  epoch  1, batch     0 | loss: 2.2073357MemoryTrain:  epoch  1, batch     1 | loss: 1.4172572MemoryTrain:  epoch  2, batch     0 | loss: 1.6751691MemoryTrain:  epoch  2, batch     1 | loss: 1.3856233MemoryTrain:  epoch  3, batch     0 | loss: 1.0740390MemoryTrain:  epoch  3, batch     1 | loss: 1.4979368MemoryTrain:  epoch  4, batch     0 | loss: 1.1952729MemoryTrain:  epoch  4, batch     1 | loss: 0.9247144MemoryTrain:  epoch  5, batch     0 | loss: 1.1866655MemoryTrain:  epoch  5, batch     1 | loss: 0.4724662MemoryTrain:  epoch  6, batch     0 | loss: 1.1331755MemoryTrain:  epoch  6, batch     1 | loss: 0.6109232MemoryTrain:  epoch  7, batch     0 | loss: 0.7495079MemoryTrain:  epoch  7, batch     1 | loss: 0.8044910MemoryTrain:  epoch  8, batch     0 | loss: 0.9218202MemoryTrain:  epoch  8, batch     1 | loss: 0.4164439MemoryTrain:  epoch  9, batch     0 | loss: 0.8940789MemoryTrain:  epoch  9, batch     1 | loss: 0.4023985
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 45.83%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 61.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 66.96%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 64.06%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 63.19%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 60.62%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 60.23%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 59.90%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 59.62%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 62.05%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 64.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 68.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 70.39%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 71.56%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 72.02%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 70.74%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 67.86%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 70.45%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 68.27%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 66.07%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 66.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 65.81%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 65.97%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 66.12%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 67.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 69.05%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 70.45%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 74.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 72.69%   [EVAL] batch:   27 | acc: 12.50%,  total acc: 70.54%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 68.32%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 66.25%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 64.31%   [EVAL] batch:   31 | acc: 12.50%,  total acc: 62.70%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 61.93%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 61.03%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 59.29%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 57.64%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 56.08%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 54.61%   [EVAL] batch:   38 | acc: 0.00%,  total acc: 53.21%   [EVAL] batch:   39 | acc: 6.25%,  total acc: 52.03%   [EVAL] batch:   40 | acc: 12.50%,  total acc: 51.07%   [EVAL] batch:   41 | acc: 0.00%,  total acc: 49.85%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 48.69%   [EVAL] batch:   43 | acc: 6.25%,  total acc: 47.73%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 46.67%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 46.47%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 47.07%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 48.18%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 47.58%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 46.75%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 45.83%   [EVAL] batch:   51 | acc: 12.50%,  total acc: 45.19%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 44.34%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 45.02%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 46.02%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 46.76%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 47.59%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 48.06%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 48.94%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 49.48%   [EVAL] batch:   60 | acc: 25.00%,  total acc: 49.08%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 48.29%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 47.52%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 46.78%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 47.21%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 47.54%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 47.48%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 47.52%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 47.64%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 48.12%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 48.77%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 49.31%   [EVAL] batch:   72 | acc: 62.50%,  total acc: 49.49%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 49.41%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 49.50%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 49.34%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 49.51%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 49.52%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 49.76%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 50.31%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 50.85%   [EVAL] batch:   81 | acc: 100.00%,  total acc: 51.45%   [EVAL] batch:   82 | acc: 93.75%,  total acc: 51.96%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 52.53%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 52.87%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 53.27%   [EVAL] batch:   86 | acc: 87.50%,  total acc: 53.66%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 53.27%   
cur_acc_llm:  [0.8806818181818182, 0.7098214285714286, 0.2890625, 0.8125, 0.7073863636363636]
his_acc_llm:  [0.8806818181818182, 0.8098404255319149, 0.6956018518518519, 0.6410984848484849, 0.5326704545454546]
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 28.12%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 22.50%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 18.75%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 19.64%   [EVAL] batch:    7 | acc: 31.25%,  total acc: 21.09%   [EVAL] batch:    8 | acc: 25.00%,  total acc: 21.53%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 22.50%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 23.86%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 24.48%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 25.96%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 29.91%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 32.08%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 34.77%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 36.40%   [EVAL] batch:   17 | acc: 37.50%,  total acc: 36.46%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 37.50%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 39.69%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 41.96%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 42.05%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 33.75%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 35.42%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 38.39%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 41.41%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 43.75%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 45.00%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 46.02%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 47.40%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 47.12%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 48.21%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 50.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 50.39%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 51.84%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 52.43%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 52.96%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 54.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 56.85%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 58.81%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 60.60%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 62.24%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 63.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 65.14%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 66.20%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 67.41%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 68.53%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 69.17%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 69.56%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 70.51%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 71.02%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 69.30%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 67.32%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 65.45%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 63.68%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 62.01%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 60.58%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 60.94%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 61.74%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 62.05%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 62.79%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 63.07%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 63.61%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 62.77%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 62.90%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 63.67%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 62.76%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 62.62%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 62.25%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 62.02%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 61.91%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 62.95%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 63.39%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 63.71%   [EVAL] batch:   57 | acc: 12.50%,  total acc: 62.82%   [EVAL] batch:   58 | acc: 18.75%,  total acc: 62.08%   [EVAL] batch:   59 | acc: 0.00%,  total acc: 61.04%   [EVAL] batch:   60 | acc: 25.00%,  total acc: 60.45%   [EVAL] batch:   61 | acc: 6.25%,  total acc: 59.58%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 58.83%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 58.30%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 58.37%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 58.52%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 58.21%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 58.00%   [EVAL] batch:   68 | acc: 18.75%,  total acc: 57.43%   [EVAL] batch:   69 | acc: 0.00%,  total acc: 56.61%   [EVAL] batch:   70 | acc: 0.00%,  total acc: 55.81%   [EVAL] batch:   71 | acc: 6.25%,  total acc: 55.12%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 54.71%   [EVAL] batch:   73 | acc: 25.00%,  total acc: 54.31%   [EVAL] batch:   74 | acc: 37.50%,  total acc: 54.08%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 53.78%   [EVAL] batch:   76 | acc: 31.25%,  total acc: 53.49%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 53.29%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 53.24%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 53.36%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 53.63%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 53.81%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 53.84%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 53.87%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 53.90%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 54.29%   [EVAL] batch:   86 | acc: 87.50%,  total acc: 54.67%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 54.26%   
cur_acc:  ['0.8655', '0.8214', '0.4609', '0.6635', '0.4205']
his_acc:  ['0.8655', '0.8298', '0.6794', '0.6420', '0.5426']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 4.1713810CurrentTrain: epoch  0, batch     1 | loss: 5.4526849CurrentTrain: epoch  1, batch     0 | loss: 3.3754678CurrentTrain: epoch  1, batch     1 | loss: 2.9479077CurrentTrain: epoch  2, batch     0 | loss: 2.7693682CurrentTrain: epoch  2, batch     1 | loss: 2.3906505CurrentTrain: epoch  3, batch     0 | loss: 2.4155092CurrentTrain: epoch  3, batch     1 | loss: 2.0619347CurrentTrain: epoch  4, batch     0 | loss: 2.0562680CurrentTrain: epoch  4, batch     1 | loss: 2.0531795CurrentTrain: epoch  5, batch     0 | loss: 1.8957530CurrentTrain: epoch  5, batch     1 | loss: 2.0502608CurrentTrain: epoch  6, batch     0 | loss: 1.9610001CurrentTrain: epoch  6, batch     1 | loss: 1.8438261CurrentTrain: epoch  7, batch     0 | loss: 1.8051625CurrentTrain: epoch  7, batch     1 | loss: 1.8051815CurrentTrain: epoch  8, batch     0 | loss: 1.7856367CurrentTrain: epoch  8, batch     1 | loss: 1.7602296CurrentTrain: epoch  9, batch     0 | loss: 1.7350123CurrentTrain: epoch  9, batch     1 | loss: 1.7241700
Mixup data size:  100
MixupTrain:  epoch  0, batch     0 | loss: 2.9652132MixupTrain:  epoch  0, batch     1 | loss: 3.4948923MixupTrain:  epoch  0, batch     2 | loss: 3.2063779MixupTrain:  epoch  0, batch     3 | loss: 3.2268807MixupTrain:  epoch  0, batch     4 | loss: 2.3652590MixupTrain:  epoch  0, batch     5 | loss: 2.9428036MixupTrain:  epoch  0, batch     6 | loss: 2.1758804
MemoryTrain:  epoch  0, batch     0 | loss: 0.1987338MemoryTrain:  epoch  0, batch     1 | loss: 0.9617131MemoryTrain:  epoch  1, batch     0 | loss: 1.3121943MemoryTrain:  epoch  1, batch     1 | loss: 0.9382725MemoryTrain:  epoch  2, batch     0 | loss: 0.5230447MemoryTrain:  epoch  2, batch     1 | loss: 0.7852182MemoryTrain:  epoch  3, batch     0 | loss: 0.1046806MemoryTrain:  epoch  3, batch     1 | loss: 0.4892442MemoryTrain:  epoch  4, batch     0 | loss: 0.2270418MemoryTrain:  epoch  4, batch     1 | loss: 0.0801158MemoryTrain:  epoch  5, batch     0 | loss: 0.1484072MemoryTrain:  epoch  5, batch     1 | loss: 0.0482744MemoryTrain:  epoch  6, batch     0 | loss: 0.0775256MemoryTrain:  epoch  6, batch     1 | loss: 0.0350404MemoryTrain:  epoch  7, batch     0 | loss: 0.0468195MemoryTrain:  epoch  7, batch     1 | loss: 0.0280511MemoryTrain:  epoch  8, batch     0 | loss: 0.0188538MemoryTrain:  epoch  8, batch     1 | loss: 0.0335711MemoryTrain:  epoch  9, batch     0 | loss: 0.0227961MemoryTrain:  epoch  9, batch     1 | loss: 0.0305068
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 6.5878782CurrentTrain: epoch  0, batch     1 | loss: 6.3792100CurrentTrain: epoch  1, batch     0 | loss: 5.1040406CurrentTrain: epoch  1, batch     1 | loss: 4.5624499CurrentTrain: epoch  2, batch     0 | loss: 4.2700052CurrentTrain: epoch  2, batch     1 | loss: 3.9169881CurrentTrain: epoch  3, batch     0 | loss: 3.6151240CurrentTrain: epoch  3, batch     1 | loss: 3.9288409CurrentTrain: epoch  4, batch     0 | loss: 3.5383468CurrentTrain: epoch  4, batch     1 | loss: 2.7587318CurrentTrain: epoch  5, batch     0 | loss: 3.1596723CurrentTrain: epoch  5, batch     1 | loss: 2.5488844CurrentTrain: epoch  6, batch     0 | loss: 2.9640093CurrentTrain: epoch  6, batch     1 | loss: 2.5722327CurrentTrain: epoch  7, batch     0 | loss: 2.9602485CurrentTrain: epoch  7, batch     1 | loss: 2.2238326CurrentTrain: epoch  8, batch     0 | loss: 2.5555029CurrentTrain: epoch  8, batch     1 | loss: 2.4743454CurrentTrain: epoch  9, batch     0 | loss: 2.3476925CurrentTrain: epoch  9, batch     1 | loss: 2.1690567
Mixup data size:  101

MemoryTrain:  epoch  0, batch     0 | loss: 1.9362171MemoryTrain:  epoch  0, batch     1 | loss: 1.8354163MemoryTrain:  epoch  1, batch     0 | loss: 2.2727423MemoryTrain:  epoch  1, batch     1 | loss: 1.7609103MemoryTrain:  epoch  2, batch     0 | loss: 1.5991166MemoryTrain:  epoch  2, batch     1 | loss: 1.3213463MemoryTrain:  epoch  3, batch     0 | loss: 0.9045589MemoryTrain:  epoch  3, batch     1 | loss: 1.8403399MemoryTrain:  epoch  4, batch     0 | loss: 1.2889228MemoryTrain:  epoch  4, batch     1 | loss: 0.9286938MemoryTrain:  epoch  5, batch     0 | loss: 0.9951205MemoryTrain:  epoch  5, batch     1 | loss: 1.0325371MemoryTrain:  epoch  6, batch     0 | loss: 0.9123814MemoryTrain:  epoch  6, batch     1 | loss: 0.7700402MemoryTrain:  epoch  7, batch     0 | loss: 0.5101319MemoryTrain:  epoch  7, batch     1 | loss: 1.1048851MemoryTrain:  epoch  8, batch     0 | loss: 0.6087480MemoryTrain:  epoch  8, batch     1 | loss: 0.9306144MemoryTrain:  epoch  9, batch     0 | loss: 0.6846904MemoryTrain:  epoch  9, batch     1 | loss: 0.3891707
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 97.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 98.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 95.14%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 85.27%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 42.19%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 42.50%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 41.67%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 46.43%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 52.34%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 57.64%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 60.62%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 61.93%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 64.06%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 62.02%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 59.38%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 59.17%   [EVAL] batch:   15 | acc: 43.75%,  total acc: 58.20%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 58.09%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 57.99%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 57.57%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 58.13%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 59.82%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 61.65%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 63.04%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 66.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 67.31%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 65.28%   [EVAL] batch:   27 | acc: 12.50%,  total acc: 63.39%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 61.42%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 59.58%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 57.86%   [EVAL] batch:   31 | acc: 12.50%,  total acc: 56.45%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 55.87%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 55.51%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 54.11%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 52.60%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 51.18%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 49.84%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 48.72%   [EVAL] batch:   39 | acc: 12.50%,  total acc: 47.81%   [EVAL] batch:   40 | acc: 6.25%,  total acc: 46.80%   [EVAL] batch:   41 | acc: 6.25%,  total acc: 45.83%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 45.06%   [EVAL] batch:   43 | acc: 25.00%,  total acc: 44.60%   [EVAL] batch:   44 | acc: 12.50%,  total acc: 43.89%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 44.02%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 44.81%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 45.96%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 45.28%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 44.50%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 43.63%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 42.91%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 42.10%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 42.71%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 43.64%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 44.42%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 45.07%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 45.58%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 46.50%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 47.29%   [EVAL] batch:   60 | acc: 25.00%,  total acc: 46.93%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 46.17%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 45.44%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 44.73%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 44.62%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 44.13%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 43.56%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 43.20%   [EVAL] batch:   68 | acc: 0.00%,  total acc: 42.57%   [EVAL] batch:   69 | acc: 6.25%,  total acc: 42.05%   [EVAL] batch:   70 | acc: 18.75%,  total acc: 41.73%   [EVAL] batch:   71 | acc: 12.50%,  total acc: 41.32%   [EVAL] batch:   72 | acc: 37.50%,  total acc: 41.27%   [EVAL] batch:   73 | acc: 37.50%,  total acc: 41.22%   [EVAL] batch:   74 | acc: 37.50%,  total acc: 41.17%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 41.12%   [EVAL] batch:   76 | acc: 43.75%,  total acc: 41.15%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 41.11%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 41.46%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 42.19%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 42.90%   [EVAL] batch:   81 | acc: 100.00%,  total acc: 43.60%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 44.05%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 44.72%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 45.00%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 45.42%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 45.62%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 46.16%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 46.70%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 47.22%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 47.80%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 48.37%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 48.92%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 49.47%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 50.00%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 50.33%   [EVAL] batch:   96 | acc: 37.50%,  total acc: 50.19%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 50.32%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 50.82%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 51.06%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 51.24%   
cur_acc_llm:  [0.8806818181818182, 0.7098214285714286, 0.2890625, 0.8125, 0.7073863636363636, 0.8526785714285714]
his_acc_llm:  [0.8806818181818182, 0.8098404255319149, 0.6956018518518519, 0.6410984848484849, 0.5326704545454546, 0.5123762376237624]
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 93.06%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 81.77%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 80.77%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 78.57%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 35.42%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 35.94%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 37.50%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 38.54%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 41.07%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 42.97%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 45.14%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 46.25%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 47.16%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 48.44%   [EVAL] batch:   12 | acc: 25.00%,  total acc: 46.63%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 46.43%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 47.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 48.44%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 50.00%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 50.69%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 51.32%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 52.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 55.06%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 57.10%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 58.97%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 60.68%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 62.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 63.70%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 64.81%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 66.07%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 67.24%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 67.92%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 68.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 69.70%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 68.01%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 66.07%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 64.24%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 62.50%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 60.86%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 59.46%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 59.84%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 60.52%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 60.86%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 61.63%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 61.79%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 62.08%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 61.28%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 61.44%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 62.24%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 61.22%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 61.25%   [EVAL] batch:   50 | acc: 50.00%,  total acc: 61.03%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 60.70%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 60.61%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 61.00%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 61.48%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 61.83%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 62.17%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 61.85%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 61.33%   [EVAL] batch:   59 | acc: 31.25%,  total acc: 60.83%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 60.35%   [EVAL] batch:   61 | acc: 6.25%,  total acc: 59.48%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 58.73%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 58.20%   [EVAL] batch:   64 | acc: 43.75%,  total acc: 57.98%   [EVAL] batch:   65 | acc: 25.00%,  total acc: 57.48%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 56.72%   [EVAL] batch:   67 | acc: 6.25%,  total acc: 55.97%   [EVAL] batch:   68 | acc: 6.25%,  total acc: 55.25%   [EVAL] batch:   69 | acc: 0.00%,  total acc: 54.46%   [EVAL] batch:   70 | acc: 0.00%,  total acc: 53.70%   [EVAL] batch:   71 | acc: 12.50%,  total acc: 53.12%   [EVAL] batch:   72 | acc: 12.50%,  total acc: 52.57%   [EVAL] batch:   73 | acc: 25.00%,  total acc: 52.20%   [EVAL] batch:   74 | acc: 6.25%,  total acc: 51.58%   [EVAL] batch:   75 | acc: 25.00%,  total acc: 51.23%   [EVAL] batch:   76 | acc: 12.50%,  total acc: 50.73%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 50.56%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 50.32%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 50.39%   [EVAL] batch:   80 | acc: 43.75%,  total acc: 50.31%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 50.23%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 50.08%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 50.07%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 50.15%   [EVAL] batch:   85 | acc: 93.75%,  total acc: 50.65%   [EVAL] batch:   86 | acc: 87.50%,  total acc: 51.08%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 51.56%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 51.83%   [EVAL] batch:   89 | acc: 81.25%,  total acc: 52.15%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 52.68%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 53.19%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 53.70%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 54.19%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 54.67%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 55.14%   [EVAL] batch:   96 | acc: 31.25%,  total acc: 54.90%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 54.78%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 54.86%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 55.06%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 55.07%   
cur_acc:  ['0.8655', '0.8214', '0.4609', '0.6635', '0.4205', '0.7857']
his_acc:  ['0.8655', '0.8298', '0.6794', '0.6420', '0.5426', '0.5507']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 6.2396669CurrentTrain: epoch  0, batch     1 | loss: 7.2939191CurrentTrain: epoch  1, batch     0 | loss: 5.2802863CurrentTrain: epoch  1, batch     1 | loss: 4.6243081CurrentTrain: epoch  2, batch     0 | loss: 4.9591613CurrentTrain: epoch  2, batch     1 | loss: 3.6103063CurrentTrain: epoch  3, batch     0 | loss: 3.9572101CurrentTrain: epoch  3, batch     1 | loss: 4.6456513CurrentTrain: epoch  4, batch     0 | loss: 4.5527668CurrentTrain: epoch  4, batch     1 | loss: 3.0068746CurrentTrain: epoch  5, batch     0 | loss: 3.6958363CurrentTrain: epoch  5, batch     1 | loss: 3.5884607CurrentTrain: epoch  6, batch     0 | loss: 3.4158325CurrentTrain: epoch  6, batch     1 | loss: 3.1506877CurrentTrain: epoch  7, batch     0 | loss: 2.9528306CurrentTrain: epoch  7, batch     1 | loss: 3.2659874CurrentTrain: epoch  8, batch     0 | loss: 3.2907281CurrentTrain: epoch  8, batch     1 | loss: 2.2533205CurrentTrain: epoch  9, batch     0 | loss: 2.7399786CurrentTrain: epoch  9, batch     1 | loss: 2.7723804
Mixup data size:  110
MixupTrain:  epoch  0, batch     0 | loss: 1.9165230MixupTrain:  epoch  0, batch     1 | loss: 2.4403330MixupTrain:  epoch  0, batch     2 | loss: 2.5331553MixupTrain:  epoch  0, batch     3 | loss: 2.7252516MixupTrain:  epoch  0, batch     4 | loss: 2.7029346MixupTrain:  epoch  0, batch     5 | loss: 2.0669017MixupTrain:  epoch  0, batch     6 | loss: 1.8236178
MemoryTrain:  epoch  0, batch     0 | loss: 0.0255156MemoryTrain:  epoch  0, batch     1 | loss: 0.0512781MemoryTrain:  epoch  0, batch     2 | loss: 0.0059357MemoryTrain:  epoch  1, batch     0 | loss: 0.0619624MemoryTrain:  epoch  1, batch     1 | loss: 0.2926871MemoryTrain:  epoch  1, batch     2 | loss: 1.9742393MemoryTrain:  epoch  2, batch     0 | loss: 0.1498819MemoryTrain:  epoch  2, batch     1 | loss: 0.0922871MemoryTrain:  epoch  2, batch     2 | loss: 0.4482821MemoryTrain:  epoch  3, batch     0 | loss: 0.0552326MemoryTrain:  epoch  3, batch     1 | loss: 0.0658080MemoryTrain:  epoch  3, batch     2 | loss: 0.0560233MemoryTrain:  epoch  4, batch     0 | loss: 0.1262773MemoryTrain:  epoch  4, batch     1 | loss: 0.0527716MemoryTrain:  epoch  4, batch     2 | loss: 0.0272370MemoryTrain:  epoch  5, batch     0 | loss: 0.0787070MemoryTrain:  epoch  5, batch     1 | loss: 0.0958373MemoryTrain:  epoch  5, batch     2 | loss: 0.0178565MemoryTrain:  epoch  6, batch     0 | loss: 0.0417419MemoryTrain:  epoch  6, batch     1 | loss: 0.0393029MemoryTrain:  epoch  6, batch     2 | loss: 0.1415074MemoryTrain:  epoch  7, batch     0 | loss: 0.1038223MemoryTrain:  epoch  7, batch     1 | loss: 0.0165188MemoryTrain:  epoch  7, batch     2 | loss: 0.0149546MemoryTrain:  epoch  8, batch     0 | loss: 0.0800298MemoryTrain:  epoch  8, batch     1 | loss: 0.0381141MemoryTrain:  epoch  8, batch     2 | loss: 0.0157648MemoryTrain:  epoch  9, batch     0 | loss: 0.0229893MemoryTrain:  epoch  9, batch     1 | loss: 0.0225985MemoryTrain:  epoch  9, batch     2 | loss: 0.0224531
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 9.6183090CurrentTrain: epoch  0, batch     1 | loss: 6.6566439CurrentTrain: epoch  1, batch     0 | loss: 6.6609354CurrentTrain: epoch  1, batch     1 | loss: 7.3710618CurrentTrain: epoch  2, batch     0 | loss: 6.5656419CurrentTrain: epoch  2, batch     1 | loss: 6.0355592CurrentTrain: epoch  3, batch     0 | loss: 6.8048487CurrentTrain: epoch  3, batch     1 | loss: 4.1857777CurrentTrain: epoch  4, batch     0 | loss: 5.0261369CurrentTrain: epoch  4, batch     1 | loss: 5.7722607CurrentTrain: epoch  5, batch     0 | loss: 5.2387424CurrentTrain: epoch  5, batch     1 | loss: 5.7359567CurrentTrain: epoch  6, batch     0 | loss: 4.3380594CurrentTrain: epoch  6, batch     1 | loss: 6.2979870CurrentTrain: epoch  7, batch     0 | loss: 5.5922413CurrentTrain: epoch  7, batch     1 | loss: 2.7984633CurrentTrain: epoch  8, batch     0 | loss: 3.7057595CurrentTrain: epoch  8, batch     1 | loss: 5.5854998CurrentTrain: epoch  9, batch     0 | loss: 3.5903757CurrentTrain: epoch  9, batch     1 | loss: 5.3969088
Mixup data size:  110
MixupTrain:  epoch  0, batch     3 | loss: 4.8775817MixupTrain:  epoch  0, batch     4 | loss: 4.6079675MixupTrain:  epoch  0, batch     5 | loss: 4.0339072MixupTrain:  epoch  0, batch     6 | loss: 4.0783408
MemoryTrain:  epoch  0, batch     0 | loss: 0.8852614MemoryTrain:  epoch  0, batch     1 | loss: 0.6062073MemoryTrain:  epoch  0, batch     2 | loss: 0.1556573MemoryTrain:  epoch  1, batch     0 | loss: 1.3171643MemoryTrain:  epoch  1, batch     1 | loss: 0.8948716MemoryTrain:  epoch  1, batch     2 | loss: 0.2539564MemoryTrain:  epoch  2, batch     0 | loss: 0.9671198MemoryTrain:  epoch  2, batch     1 | loss: 0.6494371MemoryTrain:  epoch  2, batch     2 | loss: 0.2870124MemoryTrain:  epoch  3, batch     0 | loss: 0.6846609MemoryTrain:  epoch  3, batch     1 | loss: 0.7686515MemoryTrain:  epoch  3, batch     2 | loss: 0.2676476MemoryTrain:  epoch  4, batch     0 | loss: 0.7918008MemoryTrain:  epoch  4, batch     1 | loss: 0.6591681MemoryTrain:  epoch  4, batch     2 | loss: 0.3810486MemoryTrain:  epoch  5, batch     0 | loss: 0.7252153MemoryTrain:  epoch  5, batch     1 | loss: 0.5373840MemoryTrain:  epoch  5, batch     2 | loss: 0.0839112MemoryTrain:  epoch  6, batch     0 | loss: 0.5899735MemoryTrain:  epoch  6, batch     1 | loss: 0.4844867MemoryTrain:  epoch  6, batch     2 | loss: 0.1212685MemoryTrain:  epoch  7, batch     0 | loss: 0.5659860MemoryTrain:  epoch  7, batch     1 | loss: 0.3394169MemoryTrain:  epoch  7, batch     2 | loss: 0.1063958MemoryTrain:  epoch  8, batch     0 | loss: 0.3584617MemoryTrain:  epoch  8, batch     1 | loss: 0.4376305MemoryTrain:  epoch  8, batch     2 | loss: 0.0961285MemoryTrain:  epoch  9, batch     0 | loss: 0.5327484MemoryTrain:  epoch  9, batch     1 | loss: 0.4126075MemoryTrain:  epoch  9, batch     2 | loss: 0.0528262
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 85.94%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 86.54%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 85.71%   [EVAL] batch:   14 | acc: 37.50%,  total acc: 82.50%   
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 9.38%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 10.42%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 10.94%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 11.25%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 9.38%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 16.96%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 26.56%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 34.03%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 39.38%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 43.18%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 46.88%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 46.15%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 44.64%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 45.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 46.48%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 47.06%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 47.92%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 48.68%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 50.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 52.68%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 54.83%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 56.79%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 58.59%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 60.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 61.78%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 59.95%   [EVAL] batch:   27 | acc: 12.50%,  total acc: 58.26%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 56.47%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 55.00%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 53.43%   [EVAL] batch:   31 | acc: 12.50%,  total acc: 52.15%   [EVAL] batch:   32 | acc: 31.25%,  total acc: 51.52%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 50.00%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 48.75%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 47.40%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 46.28%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 45.07%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 44.07%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 44.38%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 44.21%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 44.64%   [EVAL] batch:   42 | acc: 31.25%,  total acc: 44.33%   [EVAL] batch:   43 | acc: 43.75%,  total acc: 44.32%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 44.17%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 43.48%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 43.88%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 45.05%   [EVAL] batch:   48 | acc: 0.00%,  total acc: 44.13%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 43.62%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 42.89%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 42.19%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 41.39%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 42.13%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 43.07%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 43.86%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 44.52%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 44.94%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 45.76%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 46.25%   [EVAL] batch:   60 | acc: 25.00%,  total acc: 45.90%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 45.16%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 44.44%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 43.75%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 43.17%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 42.52%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 41.98%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 41.54%   [EVAL] batch:   68 | acc: 0.00%,  total acc: 40.94%   [EVAL] batch:   69 | acc: 0.00%,  total acc: 40.36%   [EVAL] batch:   70 | acc: 6.25%,  total acc: 39.88%   [EVAL] batch:   71 | acc: 0.00%,  total acc: 39.32%   [EVAL] batch:   72 | acc: 18.75%,  total acc: 39.04%   [EVAL] batch:   73 | acc: 12.50%,  total acc: 38.68%   [EVAL] batch:   74 | acc: 25.00%,  total acc: 38.50%   [EVAL] batch:   75 | acc: 18.75%,  total acc: 38.24%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 38.23%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 38.22%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 38.45%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 38.98%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 39.74%   [EVAL] batch:   81 | acc: 100.00%,  total acc: 40.47%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 41.04%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 41.59%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 41.47%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 41.50%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 41.45%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 42.05%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 42.56%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 43.12%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 43.75%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 44.36%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 44.96%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 45.48%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 46.05%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 46.42%   [EVAL] batch:   96 | acc: 37.50%,  total acc: 46.33%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 46.49%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 46.97%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 47.19%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 47.65%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 48.04%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 48.42%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 48.80%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 49.23%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 49.71%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 50.12%   [EVAL] batch:  107 | acc: 93.75%,  total acc: 50.52%   [EVAL] batch:  108 | acc: 81.25%,  total acc: 50.80%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 51.14%   [EVAL] batch:  110 | acc: 75.00%,  total acc: 51.35%   [EVAL] batch:  111 | acc: 68.75%,  total acc: 51.51%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 51.71%   [EVAL] batch:  113 | acc: 93.75%,  total acc: 52.08%   [EVAL] batch:  114 | acc: 75.00%,  total acc: 52.28%   [EVAL] batch:  115 | acc: 31.25%,  total acc: 52.10%   
cur_acc_llm:  [0.8806818181818182, 0.7098214285714286, 0.2890625, 0.8125, 0.7073863636363636, 0.8526785714285714, 0.825]
his_acc_llm:  [0.8806818181818182, 0.8098404255319149, 0.6956018518518519, 0.6410984848484849, 0.5326704545454546, 0.5123762376237624, 0.5210129310344828]
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 25.00%,  total acc: 82.39%   [EVAL] batch:   11 | acc: 6.25%,  total acc: 76.04%   [EVAL] batch:   12 | acc: 6.25%,  total acc: 70.67%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 66.96%   [EVAL] batch:   14 | acc: 0.00%,  total acc: 62.50%   
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 18.75%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 14.06%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 16.25%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 16.67%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 25.00%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 32.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 40.28%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 44.38%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 47.16%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 49.48%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 49.52%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 49.11%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 50.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 51.17%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 52.57%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 53.12%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 53.62%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 55.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 57.14%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 59.09%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 60.87%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 64.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 65.38%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 66.44%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 67.63%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 68.53%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 69.17%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 69.35%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 69.92%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 70.08%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 68.57%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 66.96%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 65.10%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 63.34%   [EVAL] batch:   37 | acc: 18.75%,  total acc: 62.17%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 61.06%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 61.41%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 62.20%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 63.23%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 63.35%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 63.75%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 62.91%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 62.77%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 63.54%   [EVAL] batch:   48 | acc: 6.25%,  total acc: 62.37%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 61.75%   [EVAL] batch:   50 | acc: 25.00%,  total acc: 61.03%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 60.22%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 59.32%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 59.61%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 60.23%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 60.71%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 60.96%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 60.78%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 60.28%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 60.00%   [EVAL] batch:   60 | acc: 12.50%,  total acc: 59.22%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 58.27%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 57.44%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 56.74%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 56.15%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 55.59%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 54.76%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 53.95%   [EVAL] batch:   68 | acc: 6.25%,  total acc: 53.26%   [EVAL] batch:   69 | acc: 0.00%,  total acc: 52.50%   [EVAL] batch:   70 | acc: 0.00%,  total acc: 51.76%   [EVAL] batch:   71 | acc: 0.00%,  total acc: 51.04%   [EVAL] batch:   72 | acc: 12.50%,  total acc: 50.51%   [EVAL] batch:   73 | acc: 18.75%,  total acc: 50.08%   [EVAL] batch:   74 | acc: 6.25%,  total acc: 49.50%   [EVAL] batch:   75 | acc: 18.75%,  total acc: 49.10%   [EVAL] batch:   76 | acc: 0.00%,  total acc: 48.46%   [EVAL] batch:   77 | acc: 31.25%,  total acc: 48.24%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 48.02%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 48.12%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 48.15%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 48.02%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 47.89%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 47.69%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 47.79%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 48.04%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 48.28%   [EVAL] batch:   87 | acc: 100.00%,  total acc: 48.86%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 49.16%   [EVAL] batch:   89 | acc: 81.25%,  total acc: 49.51%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 50.07%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 50.61%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 51.14%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 51.66%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 52.17%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 52.54%   [EVAL] batch:   96 | acc: 37.50%,  total acc: 52.38%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 52.49%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 52.59%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 52.69%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 52.72%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 52.94%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 53.34%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 53.61%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 53.99%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 54.30%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 54.67%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 55.09%   [EVAL] batch:  108 | acc: 93.75%,  total acc: 55.45%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 55.80%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 55.86%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 55.58%   [EVAL] batch:  112 | acc: 6.25%,  total acc: 55.14%   [EVAL] batch:  113 | acc: 6.25%,  total acc: 54.71%   [EVAL] batch:  114 | acc: 18.75%,  total acc: 54.40%   [EVAL] batch:  115 | acc: 0.00%,  total acc: 53.93%   
cur_acc:  ['0.8655', '0.8214', '0.4609', '0.6635', '0.4205', '0.7857', '0.6250']
his_acc:  ['0.8655', '0.8298', '0.6794', '0.6420', '0.5426', '0.5507', '0.5393']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 6.3044329CurrentTrain: epoch  0, batch     1 | loss: 5.2911000CurrentTrain: epoch  1, batch     0 | loss: 4.1906805CurrentTrain: epoch  1, batch     1 | loss: 3.8557045CurrentTrain: epoch  2, batch     0 | loss: 3.4757340CurrentTrain: epoch  2, batch     1 | loss: 2.2219687CurrentTrain: epoch  3, batch     0 | loss: 2.5381122CurrentTrain: epoch  3, batch     1 | loss: 2.3443148CurrentTrain: epoch  4, batch     0 | loss: 2.3174410CurrentTrain: epoch  4, batch     1 | loss: 1.8994029CurrentTrain: epoch  5, batch     0 | loss: 1.8129711CurrentTrain: epoch  5, batch     1 | loss: 2.1514299CurrentTrain: epoch  6, batch     0 | loss: 1.8435729CurrentTrain: epoch  6, batch     1 | loss: 1.8614227CurrentTrain: epoch  7, batch     0 | loss: 1.9055667CurrentTrain: epoch  7, batch     1 | loss: 1.7537400CurrentTrain: epoch  8, batch     0 | loss: 1.8348808CurrentTrain: epoch  8, batch     1 | loss: 1.9278014CurrentTrain: epoch  9, batch     0 | loss: 1.8916999CurrentTrain: epoch  9, batch     1 | loss: 1.8270272
Mixup data size:  119
MixupTrain:  epoch  0, batch     0 | loss: 1.8353604MixupTrain:  epoch  0, batch     1 | loss: 1.9925183MixupTrain:  epoch  0, batch     4 | loss: 2.0946194MixupTrain:  epoch  0, batch     5 | loss: 2.1013927MixupTrain:  epoch  0, batch     7 | loss: 2.1659990
MemoryTrain:  epoch  0, batch     0 | loss: 0.4998463MemoryTrain:  epoch  0, batch     1 | loss: 0.0933674MemoryTrain:  epoch  0, batch     2 | loss: 0.6330726MemoryTrain:  epoch  1, batch     0 | loss: 1.1536931MemoryTrain:  epoch  1, batch     1 | loss: 0.5465457MemoryTrain:  epoch  1, batch     2 | loss: 0.1755980MemoryTrain:  epoch  2, batch     0 | loss: 0.2689170MemoryTrain:  epoch  2, batch     1 | loss: 0.2473851MemoryTrain:  epoch  2, batch     2 | loss: 0.3012697MemoryTrain:  epoch  3, batch     0 | loss: 0.0753442MemoryTrain:  epoch  3, batch     1 | loss: 0.1008938MemoryTrain:  epoch  3, batch     2 | loss: 0.3250729MemoryTrain:  epoch  4, batch     0 | loss: 0.0641200MemoryTrain:  epoch  4, batch     1 | loss: 0.0994163MemoryTrain:  epoch  4, batch     2 | loss: 0.0426949MemoryTrain:  epoch  5, batch     0 | loss: 0.0304003MemoryTrain:  epoch  5, batch     1 | loss: 0.0717908MemoryTrain:  epoch  5, batch     2 | loss: 0.0461444MemoryTrain:  epoch  6, batch     0 | loss: 0.0193849MemoryTrain:  epoch  6, batch     1 | loss: 0.0195799MemoryTrain:  epoch  6, batch     2 | loss: 0.0318538MemoryTrain:  epoch  7, batch     0 | loss: 0.0189973MemoryTrain:  epoch  7, batch     1 | loss: 0.0147528MemoryTrain:  epoch  7, batch     2 | loss: 0.0248853MemoryTrain:  epoch  8, batch     0 | loss: 0.0210115MemoryTrain:  epoch  8, batch     1 | loss: 0.0268812MemoryTrain:  epoch  8, batch     2 | loss: 0.0211122MemoryTrain:  epoch  9, batch     0 | loss: 0.0171229MemoryTrain:  epoch  9, batch     1 | loss: 0.0179409MemoryTrain:  epoch  9, batch     2 | loss: 0.0170874
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 7.5315166CurrentTrain: epoch  0, batch     1 | loss: 7.4438109CurrentTrain: epoch  1, batch     0 | loss: 6.9290056CurrentTrain: epoch  1, batch     1 | loss: 5.8126268CurrentTrain: epoch  2, batch     0 | loss: 5.6951232CurrentTrain: epoch  2, batch     1 | loss: 4.3907428CurrentTrain: epoch  3, batch     0 | loss: 5.0637321CurrentTrain: epoch  3, batch     1 | loss: 3.7268970CurrentTrain: epoch  4, batch     0 | loss: 4.1107798CurrentTrain: epoch  4, batch     1 | loss: 3.9299693CurrentTrain: epoch  5, batch     0 | loss: 3.5018868CurrentTrain: epoch  5, batch     1 | loss: 3.6021650CurrentTrain: epoch  6, batch     0 | loss: 3.2614017CurrentTrain: epoch  6, batch     1 | loss: 2.9754705CurrentTrain: epoch  7, batch     0 | loss: 3.1642587CurrentTrain: epoch  7, batch     1 | loss: 2.6496029CurrentTrain: epoch  8, batch     0 | loss: 2.6662529CurrentTrain: epoch  8, batch     1 | loss: 2.4226739CurrentTrain: epoch  9, batch     0 | loss: 2.5790198CurrentTrain: epoch  9, batch     1 | loss: 2.3782954
Mixup data size:  120
MixupTrain:  epoch  0, batch     0 | loss: 3.3355861
MemoryTrain:  epoch  0, batch     0 | loss: 1.2011708MemoryTrain:  epoch  0, batch     1 | loss: 0.7094767MemoryTrain:  epoch  0, batch     2 | loss: 0.4580284MemoryTrain:  epoch  1, batch     0 | loss: 1.1664686MemoryTrain:  epoch  1, batch     1 | loss: 1.8864118MemoryTrain:  epoch  1, batch     2 | loss: 0.8436463MemoryTrain:  epoch  2, batch     0 | loss: 1.3314734MemoryTrain:  epoch  2, batch     1 | loss: 0.7215292MemoryTrain:  epoch  2, batch     2 | loss: 1.0348997MemoryTrain:  epoch  3, batch     0 | loss: 0.8591266MemoryTrain:  epoch  3, batch     1 | loss: 0.7310405MemoryTrain:  epoch  3, batch     2 | loss: 0.4178803MemoryTrain:  epoch  4, batch     0 | loss: 0.7390867MemoryTrain:  epoch  4, batch     1 | loss: 0.6573201MemoryTrain:  epoch  4, batch     2 | loss: 0.3089884MemoryTrain:  epoch  5, batch     0 | loss: 0.5883973MemoryTrain:  epoch  5, batch     1 | loss: 0.3335230MemoryTrain:  epoch  5, batch     2 | loss: 0.2817618MemoryTrain:  epoch  6, batch     0 | loss: 0.5788532MemoryTrain:  epoch  6, batch     1 | loss: 0.5478473MemoryTrain:  epoch  6, batch     2 | loss: 0.2893569MemoryTrain:  epoch  7, batch     0 | loss: 0.3781477MemoryTrain:  epoch  7, batch     1 | loss: 0.6099077MemoryTrain:  epoch  7, batch     2 | loss: 0.2711073MemoryTrain:  epoch  8, batch     0 | loss: 0.3601502MemoryTrain:  epoch  8, batch     1 | loss: 0.5291429MemoryTrain:  epoch  8, batch     2 | loss: 0.4859083MemoryTrain:  epoch  9, batch     0 | loss: 0.3471872MemoryTrain:  epoch  9, batch     1 | loss: 0.5749147MemoryTrain:  epoch  9, batch     2 | loss: 0.1873795
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 92.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.01%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 89.24%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 10.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 21.09%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 28.47%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 35.00%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 39.20%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 43.23%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 40.62%   [EVAL] batch:   14 | acc: 6.25%,  total acc: 38.33%   [EVAL] batch:   15 | acc: 0.00%,  total acc: 35.94%   [EVAL] batch:   16 | acc: 0.00%,  total acc: 33.82%   [EVAL] batch:   17 | acc: 0.00%,  total acc: 31.94%   [EVAL] batch:   18 | acc: 0.00%,  total acc: 30.26%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 30.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 33.33%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 36.36%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 38.86%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 41.15%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 43.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 45.67%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 44.44%   [EVAL] batch:   27 | acc: 12.50%,  total acc: 43.30%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 42.03%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 40.83%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 39.72%   [EVAL] batch:   31 | acc: 12.50%,  total acc: 38.87%   [EVAL] batch:   32 | acc: 31.25%,  total acc: 38.64%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 37.87%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 36.96%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 35.94%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 35.14%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 34.21%   [EVAL] batch:   38 | acc: 0.00%,  total acc: 33.33%   [EVAL] batch:   39 | acc: 6.25%,  total acc: 32.66%   [EVAL] batch:   40 | acc: 0.00%,  total acc: 31.86%   [EVAL] batch:   41 | acc: 0.00%,  total acc: 31.10%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 30.38%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 29.69%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 29.03%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 28.40%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 28.99%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 30.47%   [EVAL] batch:   48 | acc: 0.00%,  total acc: 29.85%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 29.50%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 28.92%   [EVAL] batch:   51 | acc: 12.50%,  total acc: 28.61%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 28.30%   [EVAL] batch:   53 | acc: 18.75%,  total acc: 28.12%   [EVAL] batch:   54 | acc: 37.50%,  total acc: 28.30%   [EVAL] batch:   55 | acc: 6.25%,  total acc: 27.90%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 28.40%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 29.20%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 30.40%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 31.15%   [EVAL] batch:   60 | acc: 25.00%,  total acc: 31.05%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 30.54%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 30.06%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 29.59%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 29.13%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 28.69%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 28.36%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 27.94%   [EVAL] batch:   68 | acc: 0.00%,  total acc: 27.54%   [EVAL] batch:   69 | acc: 0.00%,  total acc: 27.14%   [EVAL] batch:   70 | acc: 12.50%,  total acc: 26.94%   [EVAL] batch:   71 | acc: 0.00%,  total acc: 26.56%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 26.54%   [EVAL] batch:   73 | acc: 25.00%,  total acc: 26.52%   [EVAL] batch:   74 | acc: 37.50%,  total acc: 26.67%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 26.73%   [EVAL] batch:   76 | acc: 31.25%,  total acc: 26.79%   [EVAL] batch:   77 | acc: 43.75%,  total acc: 27.00%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 27.29%   [EVAL] batch:   79 | acc: 25.00%,  total acc: 27.27%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 27.78%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 28.28%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 28.31%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 28.35%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 28.09%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 27.91%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 27.73%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 28.34%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 29.07%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 29.79%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 30.56%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 31.32%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 32.06%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 32.71%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 33.42%   [EVAL] batch:   95 | acc: 68.75%,  total acc: 33.79%   [EVAL] batch:   96 | acc: 12.50%,  total acc: 33.57%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 33.55%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 34.09%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 34.44%   [EVAL] batch:  100 | acc: 75.00%,  total acc: 34.84%   [EVAL] batch:  101 | acc: 43.75%,  total acc: 34.93%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 35.13%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 35.34%   [EVAL] batch:  104 | acc: 81.25%,  total acc: 35.77%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 35.97%   [EVAL] batch:  106 | acc: 81.25%,  total acc: 36.39%   [EVAL] batch:  107 | acc: 93.75%,  total acc: 36.92%   [EVAL] batch:  108 | acc: 81.25%,  total acc: 37.33%   [EVAL] batch:  109 | acc: 81.25%,  total acc: 37.73%   [EVAL] batch:  110 | acc: 81.25%,  total acc: 38.12%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 38.00%   [EVAL] batch:  112 | acc: 18.75%,  total acc: 37.83%   [EVAL] batch:  113 | acc: 31.25%,  total acc: 37.77%   [EVAL] batch:  114 | acc: 12.50%,  total acc: 37.55%   [EVAL] batch:  115 | acc: 68.75%,  total acc: 37.82%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 38.30%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 38.72%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 39.13%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 39.48%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 39.82%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 40.32%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 40.80%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 41.23%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 41.55%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 41.87%   [EVAL] batch:  126 | acc: 93.75%,  total acc: 42.27%   [EVAL] batch:  127 | acc: 100.00%,  total acc: 42.72%   [EVAL] batch:  128 | acc: 100.00%,  total acc: 43.17%   [EVAL] batch:  129 | acc: 100.00%,  total acc: 43.61%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 44.04%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 44.46%   [EVAL] batch:  132 | acc: 62.50%,  total acc: 44.60%   
cur_acc_llm:  [0.8806818181818182, 0.7098214285714286, 0.2890625, 0.8125, 0.7073863636363636, 0.8526785714285714, 0.825, 0.8923611111111112]
his_acc_llm:  [0.8806818181818182, 0.8098404255319149, 0.6956018518518519, 0.6410984848484849, 0.5326704545454546, 0.5123762376237624, 0.5210129310344828, 0.44595864661654133]
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 86.36%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.18%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 87.50%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 6.25%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 8.33%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 6.25%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 8.75%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 9.38%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 19.64%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 28.91%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 36.81%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 41.25%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 44.32%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 46.88%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 47.12%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 47.32%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 48.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 49.22%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 50.74%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 51.39%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 51.97%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 53.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 55.36%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 57.39%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 59.24%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 60.94%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 63.94%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 65.05%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 66.29%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 67.24%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 67.92%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 67.94%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 68.95%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 69.13%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 67.10%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 65.18%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 63.37%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 61.66%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 60.03%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 58.65%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 58.59%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 58.23%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 58.18%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 57.99%   [EVAL] batch:   43 | acc: 43.75%,  total acc: 57.67%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 57.78%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 56.93%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 56.78%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 57.68%   [EVAL] batch:   48 | acc: 6.25%,  total acc: 56.63%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 56.12%   [EVAL] batch:   50 | acc: 18.75%,  total acc: 55.39%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 54.69%   [EVAL] batch:   52 | acc: 18.75%,  total acc: 54.01%   [EVAL] batch:   53 | acc: 37.50%,  total acc: 53.70%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 53.75%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 54.13%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 54.50%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 54.42%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 54.03%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 53.85%   [EVAL] batch:   60 | acc: 12.50%,  total acc: 53.18%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 52.32%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 51.49%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 50.68%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 49.90%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 49.15%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 48.41%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 47.70%   [EVAL] batch:   68 | acc: 6.25%,  total acc: 47.10%   [EVAL] batch:   69 | acc: 0.00%,  total acc: 46.43%   [EVAL] batch:   70 | acc: 0.00%,  total acc: 45.77%   [EVAL] batch:   71 | acc: 0.00%,  total acc: 45.14%   [EVAL] batch:   72 | acc: 12.50%,  total acc: 44.69%   [EVAL] batch:   73 | acc: 25.00%,  total acc: 44.43%   [EVAL] batch:   74 | acc: 6.25%,  total acc: 43.92%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 43.75%   [EVAL] batch:   76 | acc: 18.75%,  total acc: 43.43%   [EVAL] batch:   77 | acc: 31.25%,  total acc: 43.27%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 43.12%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 43.28%   [EVAL] batch:   80 | acc: 43.75%,  total acc: 43.29%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 43.22%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 43.07%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 42.93%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 42.57%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 42.37%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 42.10%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 42.61%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 42.98%   [EVAL] batch:   89 | acc: 87.50%,  total acc: 43.47%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 44.09%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 44.70%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 45.30%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 45.88%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 46.45%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 46.81%   [EVAL] batch:   96 | acc: 25.00%,  total acc: 46.59%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 46.68%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 46.78%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 46.81%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 46.84%   [EVAL] batch:  101 | acc: 25.00%,  total acc: 46.63%   [EVAL] batch:  102 | acc: 31.25%,  total acc: 46.48%   [EVAL] batch:  103 | acc: 12.50%,  total acc: 46.15%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 46.25%   [EVAL] batch:  105 | acc: 43.75%,  total acc: 46.23%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 46.67%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 47.16%   [EVAL] batch:  108 | acc: 93.75%,  total acc: 47.59%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 48.01%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 48.09%   [EVAL] batch:  111 | acc: 6.25%,  total acc: 47.71%   [EVAL] batch:  112 | acc: 0.00%,  total acc: 47.29%   [EVAL] batch:  113 | acc: 0.00%,  total acc: 46.88%   [EVAL] batch:  114 | acc: 6.25%,  total acc: 46.52%   [EVAL] batch:  115 | acc: 56.25%,  total acc: 46.61%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 47.01%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 47.35%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 47.69%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 48.02%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 48.24%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 48.51%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 48.93%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 49.19%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 49.45%   [EVAL] batch:  125 | acc: 87.50%,  total acc: 49.75%   [EVAL] batch:  126 | acc: 93.75%,  total acc: 50.10%   [EVAL] batch:  127 | acc: 100.00%,  total acc: 50.49%   [EVAL] batch:  128 | acc: 100.00%,  total acc: 50.87%   [EVAL] batch:  129 | acc: 100.00%,  total acc: 51.25%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 51.62%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 51.99%   [EVAL] batch:  132 | acc: 62.50%,  total acc: 52.07%   
cur_acc:  ['0.8655', '0.8214', '0.4609', '0.6635', '0.4205', '0.7857', '0.6250', '0.8750']
his_acc:  ['0.8655', '0.8298', '0.6794', '0.6420', '0.5426', '0.5507', '0.5393', '0.5207']
--------Round  1
seed:  200
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 13.2453012CurrentTrain: epoch  0, batch     1 | loss: 12.9866905CurrentTrain: epoch  0, batch     2 | loss: 12.3517361CurrentTrain: epoch  0, batch     3 | loss: 11.7243290CurrentTrain: epoch  0, batch     4 | loss: 11.2544041CurrentTrain: epoch  0, batch     5 | loss: 10.9458675CurrentTrain: epoch  0, batch     6 | loss: 11.1670885CurrentTrain: epoch  0, batch     7 | loss: 11.9722795CurrentTrain: epoch  0, batch     8 | loss: 11.2391844CurrentTrain: epoch  0, batch     9 | loss: 11.2980576CurrentTrain: epoch  0, batch    10 | loss: 11.3575726CurrentTrain: epoch  0, batch    11 | loss: 11.4537048CurrentTrain: epoch  0, batch    12 | loss: 11.5112877CurrentTrain: epoch  0, batch    13 | loss: 11.0116386CurrentTrain: epoch  0, batch    14 | loss: 10.9241457CurrentTrain: epoch  0, batch    15 | loss: 11.1395588CurrentTrain: epoch  0, batch    16 | loss: 11.3875751CurrentTrain: epoch  0, batch    17 | loss: 11.0507698CurrentTrain: epoch  0, batch    18 | loss: 11.0159521CurrentTrain: epoch  0, batch    19 | loss: 11.5135612CurrentTrain: epoch  0, batch    20 | loss: 11.5477581CurrentTrain: epoch  0, batch    21 | loss: 11.5471010CurrentTrain: epoch  0, batch    22 | loss: 10.9505911CurrentTrain: epoch  0, batch    23 | loss: 11.6489868CurrentTrain: epoch  0, batch    24 | loss: 12.1196413CurrentTrain: epoch  0, batch    25 | loss: 11.0417013CurrentTrain: epoch  0, batch    26 | loss: 11.3273335CurrentTrain: epoch  0, batch    27 | loss: 11.3408279CurrentTrain: epoch  0, batch    28 | loss: 11.0964375CurrentTrain: epoch  0, batch    29 | loss: 11.5702934CurrentTrain: epoch  0, batch    30 | loss: 11.1517735CurrentTrain: epoch  0, batch    31 | loss: 10.8068600CurrentTrain: epoch  0, batch    32 | loss: 11.5333261CurrentTrain: epoch  0, batch    33 | loss: 11.4031525CurrentTrain: epoch  0, batch    34 | loss: 11.0061836CurrentTrain: epoch  0, batch    35 | loss: 11.2496166CurrentTrain: epoch  0, batch    36 | loss: 10.7150021CurrentTrain: epoch  0, batch    37 | loss: 10.5397205CurrentTrain: epoch  1, batch     0 | loss: 10.9286795CurrentTrain: epoch  1, batch     1 | loss: 11.2168407CurrentTrain: epoch  1, batch     2 | loss: 10.9008369CurrentTrain: epoch  1, batch     3 | loss: 11.0321703CurrentTrain: epoch  1, batch     4 | loss: 12.3438139CurrentTrain: epoch  1, batch     5 | loss: 13.5414047CurrentTrain: epoch  1, batch     6 | loss: 12.3135166CurrentTrain: epoch  1, batch     7 | loss: 11.3533611CurrentTrain: epoch  1, batch     8 | loss: 10.7974558CurrentTrain: epoch  1, batch     9 | loss: 10.7519026CurrentTrain: epoch  1, batch    10 | loss: 11.0297966CurrentTrain: epoch  1, batch    11 | loss: 11.3984509CurrentTrain: epoch  1, batch    12 | loss: 11.2365503CurrentTrain: epoch  1, batch    13 | loss: 10.9832249CurrentTrain: epoch  1, batch    14 | loss: 10.7761507CurrentTrain: epoch  1, batch    15 | loss: 10.8844671CurrentTrain: epoch  1, batch    16 | loss: 11.4787140CurrentTrain: epoch  1, batch    17 | loss: 11.0208759CurrentTrain: epoch  1, batch    18 | loss: 11.0415792CurrentTrain: epoch  1, batch    19 | loss: 10.8201618CurrentTrain: epoch  1, batch    20 | loss: 10.8286018CurrentTrain: epoch  1, batch    21 | loss: 10.7514572CurrentTrain: epoch  1, batch    22 | loss: 10.6995258CurrentTrain: epoch  1, batch    23 | loss: 10.7545929CurrentTrain: epoch  1, batch    24 | loss: 10.7065287CurrentTrain: epoch  1, batch    25 | loss: 11.0629559CurrentTrain: epoch  1, batch    26 | loss: 10.7274551CurrentTrain: epoch  1, batch    27 | loss: 10.8270760CurrentTrain: epoch  1, batch    28 | loss: 10.9219933CurrentTrain: epoch  1, batch    29 | loss: 10.6881428CurrentTrain: epoch  1, batch    30 | loss: 11.1012135CurrentTrain: epoch  1, batch    31 | loss: 10.5011206CurrentTrain: epoch  1, batch    32 | loss: 10.6504622CurrentTrain: epoch  1, batch    33 | loss: 11.1594582CurrentTrain: epoch  1, batch    34 | loss: 10.6349297CurrentTrain: epoch  1, batch    35 | loss: 10.4853764CurrentTrain: epoch  1, batch    36 | loss: 10.3011637CurrentTrain: epoch  1, batch    37 | loss: 10.4277697CurrentTrain: epoch  2, batch     0 | loss: 10.3606396CurrentTrain: epoch  2, batch     1 | loss: 10.4426079CurrentTrain: epoch  2, batch     2 | loss: 10.3003674CurrentTrain: epoch  2, batch     3 | loss: 10.6706963CurrentTrain: epoch  2, batch     4 | loss: 10.5924072CurrentTrain: epoch  2, batch     5 | loss: 10.6080980CurrentTrain: epoch  2, batch     6 | loss: 10.7281170CurrentTrain: epoch  2, batch     7 | loss: 10.3539143CurrentTrain: epoch  2, batch     8 | loss: 10.3013220CurrentTrain: epoch  2, batch     9 | loss: 10.1939468CurrentTrain: epoch  2, batch    10 | loss: 10.3015079CurrentTrain: epoch  2, batch    11 | loss: 10.2276993CurrentTrain: epoch  2, batch    12 | loss: 10.0644283CurrentTrain: epoch  2, batch    13 | loss: 10.2764530CurrentTrain: epoch  2, batch    14 | loss: 10.1674080CurrentTrain: epoch  2, batch    15 | loss: 10.1320992CurrentTrain: epoch  2, batch    16 | loss: 10.1279821CurrentTrain: epoch  2, batch    17 | loss: 10.0790043CurrentTrain: epoch  2, batch    18 | loss: 10.1636372CurrentTrain: epoch  2, batch    19 | loss: 10.1083565CurrentTrain: epoch  2, batch    20 | loss: 10.0311337CurrentTrain: epoch  2, batch    21 | loss: 9.9453621CurrentTrain: epoch  2, batch    22 | loss: 10.2077160CurrentTrain: epoch  2, batch    23 | loss: 10.2348499CurrentTrain: epoch  2, batch    24 | loss: 9.7638226CurrentTrain: epoch  2, batch    25 | loss: 10.0414219CurrentTrain: epoch  2, batch    26 | loss: 9.4442253CurrentTrain: epoch  2, batch    27 | loss: 9.5633831CurrentTrain: epoch  2, batch    28 | loss: 9.5630951CurrentTrain: epoch  2, batch    29 | loss: 9.5383720CurrentTrain: epoch  2, batch    30 | loss: 9.7997780CurrentTrain: epoch  2, batch    31 | loss: 9.4372511CurrentTrain: epoch  2, batch    32 | loss: 9.6221838CurrentTrain: epoch  2, batch    33 | loss: 10.2514610CurrentTrain: epoch  2, batch    34 | loss: 9.2292852CurrentTrain: epoch  2, batch    35 | loss: 9.4503946CurrentTrain: epoch  2, batch    36 | loss: 9.3184156CurrentTrain: epoch  2, batch    37 | loss: 9.4249535CurrentTrain: epoch  3, batch     0 | loss: 8.9930372CurrentTrain: epoch  3, batch     1 | loss: 9.2240448CurrentTrain: epoch  3, batch     2 | loss: 8.8720684CurrentTrain: epoch  3, batch     3 | loss: 8.6940241CurrentTrain: epoch  3, batch     4 | loss: 8.7950611CurrentTrain: epoch  3, batch     5 | loss: 9.0004921CurrentTrain: epoch  3, batch     6 | loss: 8.6755228CurrentTrain: epoch  3, batch     7 | loss: 8.6470356CurrentTrain: epoch  3, batch     8 | loss: 8.1559162CurrentTrain: epoch  3, batch     9 | loss: 8.5522175CurrentTrain: epoch  3, batch    10 | loss: 8.9309931CurrentTrain: epoch  3, batch    11 | loss: 8.5663671CurrentTrain: epoch  3, batch    12 | loss: 8.4132042CurrentTrain: epoch  3, batch    13 | loss: 9.2913551CurrentTrain: epoch  3, batch    14 | loss: 8.4492874CurrentTrain: epoch  3, batch    15 | loss: 8.6686802CurrentTrain: epoch  3, batch    16 | loss: 8.1876650CurrentTrain: epoch  3, batch    17 | loss: 8.3091030CurrentTrain: epoch  3, batch    18 | loss: 8.9131060CurrentTrain: epoch  3, batch    19 | loss: 8.2491999CurrentTrain: epoch  3, batch    20 | loss: 8.2754936CurrentTrain: epoch  3, batch    21 | loss: 8.6853981CurrentTrain: epoch  3, batch    22 | loss: 7.6982508CurrentTrain: epoch  3, batch    23 | loss: 8.0001688CurrentTrain: epoch  3, batch    24 | loss: 8.4994392CurrentTrain: epoch  3, batch    25 | loss: 8.0360193CurrentTrain: epoch  3, batch    26 | loss: 7.6144209CurrentTrain: epoch  3, batch    27 | loss: 8.1415195CurrentTrain: epoch  3, batch    28 | loss: 7.7315540CurrentTrain: epoch  3, batch    29 | loss: 8.3129101CurrentTrain: epoch  3, batch    30 | loss: 7.8017368CurrentTrain: epoch  3, batch    31 | loss: 7.3030143CurrentTrain: epoch  3, batch    32 | loss: 6.8559875CurrentTrain: epoch  3, batch    33 | loss: 7.3517499CurrentTrain: epoch  3, batch    34 | loss: 7.2567229CurrentTrain: epoch  3, batch    35 | loss: 7.0644879CurrentTrain: epoch  3, batch    36 | loss: 7.0444946CurrentTrain: epoch  3, batch    37 | loss: 6.6064348CurrentTrain: epoch  4, batch     0 | loss: 7.5729017CurrentTrain: epoch  4, batch     1 | loss: 6.9168062CurrentTrain: epoch  4, batch     2 | loss: 6.7893057CurrentTrain: epoch  4, batch     3 | loss: 6.3140793CurrentTrain: epoch  4, batch     4 | loss: 6.5516000CurrentTrain: epoch  4, batch     5 | loss: 7.1776981CurrentTrain: epoch  4, batch     6 | loss: 7.3155971CurrentTrain: epoch  4, batch     7 | loss: 6.5165024CurrentTrain: epoch  4, batch     8 | loss: 6.9470959CurrentTrain: epoch  4, batch     9 | loss: 6.5618124CurrentTrain: epoch  4, batch    10 | loss: 6.7472858CurrentTrain: epoch  4, batch    11 | loss: 6.9788074CurrentTrain: epoch  4, batch    12 | loss: 7.5912948CurrentTrain: epoch  4, batch    13 | loss: 7.3824096CurrentTrain: epoch  4, batch    14 | loss: 6.9794741CurrentTrain: epoch  4, batch    15 | loss: 7.9449959CurrentTrain: epoch  4, batch    16 | loss: 7.3467999CurrentTrain: epoch  4, batch    17 | loss: 6.7898240CurrentTrain: epoch  4, batch    18 | loss: 6.9034362CurrentTrain: epoch  4, batch    19 | loss: 6.6473908CurrentTrain: epoch  4, batch    20 | loss: 7.6819763CurrentTrain: epoch  4, batch    21 | loss: 6.9720221CurrentTrain: epoch  4, batch    22 | loss: 7.1206498CurrentTrain: epoch  4, batch    23 | loss: 8.8720741CurrentTrain: epoch  4, batch    24 | loss: 7.1535506CurrentTrain: epoch  4, batch    25 | loss: 7.3878965CurrentTrain: epoch  4, batch    26 | loss: 7.0378122CurrentTrain: epoch  4, batch    27 | loss: 7.2308712CurrentTrain: epoch  4, batch    28 | loss: 7.3583450CurrentTrain: epoch  4, batch    29 | loss: 6.3901634CurrentTrain: epoch  4, batch    30 | loss: 7.2274294CurrentTrain: epoch  4, batch    31 | loss: 7.5575838CurrentTrain: epoch  4, batch    32 | loss: 6.6312203CurrentTrain: epoch  4, batch    33 | loss: 6.7901773CurrentTrain: epoch  4, batch    34 | loss: 6.7788105CurrentTrain: epoch  4, batch    35 | loss: 7.3048749CurrentTrain: epoch  4, batch    36 | loss: 6.7600703CurrentTrain: epoch  4, batch    37 | loss: 7.4049950CurrentTrain: epoch  5, batch     0 | loss: 7.2722259CurrentTrain: epoch  5, batch     1 | loss: 7.0266862CurrentTrain: epoch  5, batch     2 | loss: 6.4122925CurrentTrain: epoch  5, batch     3 | loss: 5.9102774CurrentTrain: epoch  5, batch     4 | loss: 5.9422588CurrentTrain: epoch  5, batch     5 | loss: 6.7597132CurrentTrain: epoch  5, batch     6 | loss: 5.9941273CurrentTrain: epoch  5, batch     7 | loss: 6.3951998CurrentTrain: epoch  5, batch     8 | loss: 6.8178911CurrentTrain: epoch  5, batch     9 | loss: 6.1338921CurrentTrain: epoch  5, batch    10 | loss: 7.0311108CurrentTrain: epoch  5, batch    11 | loss: 6.1850319CurrentTrain: epoch  5, batch    12 | loss: 6.5736008CurrentTrain: epoch  5, batch    13 | loss: 6.5734715CurrentTrain: epoch  5, batch    14 | loss: 7.1844320CurrentTrain: epoch  5, batch    15 | loss: 6.5955582CurrentTrain: epoch  5, batch    16 | loss: 7.4437003CurrentTrain: epoch  5, batch    17 | loss: 6.9593372CurrentTrain: epoch  5, batch    18 | loss: 7.4506273CurrentTrain: epoch  5, batch    19 | loss: 7.0475554CurrentTrain: epoch  5, batch    20 | loss: 6.8030157CurrentTrain: epoch  5, batch    21 | loss: 6.2759066CurrentTrain: epoch  5, batch    22 | loss: 7.6169529CurrentTrain: epoch  5, batch    23 | loss: 6.2860951CurrentTrain: epoch  5, batch    24 | loss: 6.1386671CurrentTrain: epoch  5, batch    25 | loss: 7.2568526CurrentTrain: epoch  5, batch    26 | loss: 6.7255516CurrentTrain: epoch  5, batch    27 | loss: 5.7599449CurrentTrain: epoch  5, batch    28 | loss: 7.5792322CurrentTrain: epoch  5, batch    29 | loss: 7.1760826CurrentTrain: epoch  5, batch    30 | loss: 6.0913768CurrentTrain: epoch  5, batch    31 | loss: 5.3734818CurrentTrain: epoch  5, batch    32 | loss: 6.1642027CurrentTrain: epoch  5, batch    33 | loss: 6.4838166CurrentTrain: epoch  5, batch    34 | loss: 6.6896539CurrentTrain: epoch  5, batch    35 | loss: 5.7341633CurrentTrain: epoch  5, batch    36 | loss: 7.1155953CurrentTrain: epoch  5, batch    37 | loss: 6.7916784CurrentTrain: epoch  6, batch     0 | loss: 6.9287596CurrentTrain: epoch  6, batch     1 | loss: 6.5003901CurrentTrain: epoch  6, batch     2 | loss: 7.6410809CurrentTrain: epoch  6, batch     3 | loss: 6.0036721CurrentTrain: epoch  6, batch     4 | loss: 5.2767310CurrentTrain: epoch  6, batch     5 | loss: 5.3310747CurrentTrain: epoch  6, batch     6 | loss: 6.4217348CurrentTrain: epoch  6, batch     7 | loss: 6.5288053CurrentTrain: epoch  6, batch     8 | loss: 6.4059305CurrentTrain: epoch  6, batch     9 | loss: 6.2965097CurrentTrain: epoch  6, batch    10 | loss: 6.1748419CurrentTrain: epoch  6, batch    11 | loss: 5.5709462CurrentTrain: epoch  6, batch    12 | loss: 5.9506311CurrentTrain: epoch  6, batch    13 | loss: 6.3411059CurrentTrain: epoch  6, batch    14 | loss: 7.9147668CurrentTrain: epoch  6, batch    15 | loss: 6.8510609CurrentTrain: epoch  6, batch    16 | loss: 6.1773229CurrentTrain: epoch  6, batch    17 | loss: 7.0218077CurrentTrain: epoch  6, batch    18 | loss: 6.5866756CurrentTrain: epoch  6, batch    19 | loss: 6.0785108CurrentTrain: epoch  6, batch    20 | loss: 6.5279446CurrentTrain: epoch  6, batch    21 | loss: 6.5450501CurrentTrain: epoch  6, batch    22 | loss: 6.4486628CurrentTrain: epoch  6, batch    23 | loss: 6.1232142CurrentTrain: epoch  6, batch    24 | loss: 5.9147472CurrentTrain: epoch  6, batch    25 | loss: 6.7821407CurrentTrain: epoch  6, batch    26 | loss: 7.3286037CurrentTrain: epoch  6, batch    27 | loss: 5.6682272CurrentTrain: epoch  6, batch    28 | loss: 5.4629889CurrentTrain: epoch  6, batch    29 | loss: 6.0505319CurrentTrain: epoch  6, batch    30 | loss: 5.9730196CurrentTrain: epoch  6, batch    31 | loss: 6.0408096CurrentTrain: epoch  6, batch    32 | loss: 6.1881685CurrentTrain: epoch  6, batch    33 | loss: 5.8224387CurrentTrain: epoch  6, batch    34 | loss: 6.6750588CurrentTrain: epoch  6, batch    35 | loss: 5.6227584CurrentTrain: epoch  6, batch    36 | loss: 6.6325932CurrentTrain: epoch  6, batch    37 | loss: 5.2218823CurrentTrain: epoch  7, batch     0 | loss: 6.6973276CurrentTrain: epoch  7, batch     1 | loss: 5.7173381CurrentTrain: epoch  7, batch     2 | loss: 5.6333294CurrentTrain: epoch  7, batch     3 | loss: 5.7926073CurrentTrain: epoch  7, batch     4 | loss: 5.7601962CurrentTrain: epoch  7, batch     5 | loss: 5.6665659CurrentTrain: epoch  7, batch     6 | loss: 5.7989283CurrentTrain: epoch  7, batch     7 | loss: 6.4724312CurrentTrain: epoch  7, batch     8 | loss: 6.4650517CurrentTrain: epoch  7, batch     9 | loss: 5.5795507CurrentTrain: epoch  7, batch    10 | loss: 5.7979903CurrentTrain: epoch  7, batch    11 | loss: 5.7341642CurrentTrain: epoch  7, batch    12 | loss: 5.9440789CurrentTrain: epoch  7, batch    13 | loss: 5.6331005CurrentTrain: epoch  7, batch    14 | loss: 5.8750701CurrentTrain: epoch  7, batch    15 | loss: 5.6791363CurrentTrain: epoch  7, batch    16 | loss: 5.9606333CurrentTrain: epoch  7, batch    17 | loss: 6.7348833CurrentTrain: epoch  7, batch    18 | loss: 6.2145796CurrentTrain: epoch  7, batch    19 | loss: 6.0567212CurrentTrain: epoch  7, batch    20 | loss: 5.9416552CurrentTrain: epoch  7, batch    21 | loss: 6.4615841CurrentTrain: epoch  7, batch    22 | loss: 5.9344521CurrentTrain: epoch  7, batch    23 | loss: 5.2720327CurrentTrain: epoch  7, batch    24 | loss: 7.1778488CurrentTrain: epoch  7, batch    25 | loss: 5.2100711CurrentTrain: epoch  7, batch    26 | loss: 5.4743237CurrentTrain: epoch  7, batch    27 | loss: 5.5801883CurrentTrain: epoch  7, batch    28 | loss: 5.8374815CurrentTrain: epoch  7, batch    29 | loss: 5.5271683CurrentTrain: epoch  7, batch    30 | loss: 6.0469627CurrentTrain: epoch  7, batch    31 | loss: 6.5264926CurrentTrain: epoch  7, batch    32 | loss: 5.6321440CurrentTrain: epoch  7, batch    33 | loss: 5.1584005CurrentTrain: epoch  7, batch    34 | loss: 6.9225435CurrentTrain: epoch  7, batch    35 | loss: 5.7265315CurrentTrain: epoch  7, batch    36 | loss: 5.3821917CurrentTrain: epoch  7, batch    37 | loss: 5.4858828CurrentTrain: epoch  8, batch     0 | loss: 5.5124140CurrentTrain: epoch  8, batch     1 | loss: 5.7981949CurrentTrain: epoch  8, batch     2 | loss: 5.8728428CurrentTrain: epoch  8, batch     3 | loss: 5.0531650CurrentTrain: epoch  8, batch     4 | loss: 5.0036383CurrentTrain: epoch  8, batch     5 | loss: 5.5169954CurrentTrain: epoch  8, batch     6 | loss: 5.8399401CurrentTrain: epoch  8, batch     7 | loss: 6.0477228CurrentTrain: epoch  8, batch     8 | loss: 5.1210957CurrentTrain: epoch  8, batch     9 | loss: 4.9751444CurrentTrain: epoch  8, batch    10 | loss: 6.0859375CurrentTrain: epoch  8, batch    11 | loss: 5.5533495CurrentTrain: epoch  8, batch    12 | loss: 5.6800547CurrentTrain: epoch  8, batch    13 | loss: 5.4711165CurrentTrain: epoch  8, batch    14 | loss: 5.3909869CurrentTrain: epoch  8, batch    15 | loss: 5.3395948CurrentTrain: epoch  8, batch    16 | loss: 5.5569253CurrentTrain: epoch  8, batch    17 | loss: 5.3273954CurrentTrain: epoch  8, batch    18 | loss: 5.1103020CurrentTrain: epoch  8, batch    19 | loss: 5.5274248CurrentTrain: epoch  8, batch    20 | loss: 5.1219668CurrentTrain: epoch  8, batch    21 | loss: 5.3676157CurrentTrain: epoch  8, batch    22 | loss: 5.0913115CurrentTrain: epoch  8, batch    23 | loss: 5.0564127CurrentTrain: epoch  8, batch    24 | loss: 5.3883691CurrentTrain: epoch  8, batch    25 | loss: 5.6673040CurrentTrain: epoch  8, batch    26 | loss: 5.8035073CurrentTrain: epoch  8, batch    27 | loss: 5.1390643CurrentTrain: epoch  8, batch    28 | loss: 5.9990654CurrentTrain: epoch  8, batch    29 | loss: 5.2339249CurrentTrain: epoch  8, batch    30 | loss: 5.3424935CurrentTrain: epoch  8, batch    31 | loss: 5.1549091CurrentTrain: epoch  8, batch    32 | loss: 4.9946742CurrentTrain: epoch  8, batch    33 | loss: 5.2629371CurrentTrain: epoch  8, batch    34 | loss: 5.9778018CurrentTrain: epoch  8, batch    35 | loss: 6.0917320CurrentTrain: epoch  8, batch    36 | loss: 6.1475344CurrentTrain: epoch  8, batch    37 | loss: 5.4703093CurrentTrain: epoch  9, batch     0 | loss: 5.6049838CurrentTrain: epoch  9, batch     1 | loss: 5.9967422CurrentTrain: epoch  9, batch     2 | loss: 5.3838062CurrentTrain: epoch  9, batch     3 | loss: 5.0123591CurrentTrain: epoch  9, batch     4 | loss: 4.8927708CurrentTrain: epoch  9, batch     5 | loss: 5.0161619CurrentTrain: epoch  9, batch     6 | loss: 5.0573416CurrentTrain: epoch  9, batch     7 | loss: 5.0773525CurrentTrain: epoch  9, batch     8 | loss: 4.9488869CurrentTrain: epoch  9, batch     9 | loss: 5.5534701CurrentTrain: epoch  9, batch    10 | loss: 5.2624254CurrentTrain: epoch  9, batch    11 | loss: 5.5551901CurrentTrain: epoch  9, batch    12 | loss: 5.7246728CurrentTrain: epoch  9, batch    13 | loss: 5.4553347CurrentTrain: epoch  9, batch    14 | loss: 4.9395056CurrentTrain: epoch  9, batch    15 | loss: 5.0764656CurrentTrain: epoch  9, batch    16 | loss: 5.3017716CurrentTrain: epoch  9, batch    17 | loss: 4.9240031CurrentTrain: epoch  9, batch    18 | loss: 5.2425666CurrentTrain: epoch  9, batch    19 | loss: 5.2814550CurrentTrain: epoch  9, batch    20 | loss: 4.9193215CurrentTrain: epoch  9, batch    21 | loss: 4.9538898CurrentTrain: epoch  9, batch    22 | loss: 5.0279427CurrentTrain: epoch  9, batch    23 | loss: 5.5217066CurrentTrain: epoch  9, batch    24 | loss: 5.3435245CurrentTrain: epoch  9, batch    25 | loss: 4.8798480CurrentTrain: epoch  9, batch    26 | loss: 4.9209428CurrentTrain: epoch  9, batch    27 | loss: 5.0230436CurrentTrain: epoch  9, batch    28 | loss: 4.8688736CurrentTrain: epoch  9, batch    29 | loss: 5.0045824CurrentTrain: epoch  9, batch    30 | loss: 5.0487471CurrentTrain: epoch  9, batch    31 | loss: 6.3810101CurrentTrain: epoch  9, batch    32 | loss: 5.1306925CurrentTrain: epoch  9, batch    33 | loss: 4.8963718CurrentTrain: epoch  9, batch    34 | loss: 5.0597820CurrentTrain: epoch  9, batch    35 | loss: 5.9890852CurrentTrain: epoch  9, batch    36 | loss: 4.9518414CurrentTrain: epoch  9, batch    37 | loss: 4.9859743
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 14.7612514CurrentTrain: epoch  0, batch     1 | loss: 14.5005817CurrentTrain: epoch  0, batch     2 | loss: 14.9583931CurrentTrain: epoch  0, batch     3 | loss: 14.4206686CurrentTrain: epoch  0, batch     4 | loss: 13.9571514CurrentTrain: epoch  0, batch     5 | loss: 14.8425961CurrentTrain: epoch  0, batch     6 | loss: 14.3055534CurrentTrain: epoch  0, batch     7 | loss: 13.4817505CurrentTrain: epoch  0, batch     8 | loss: 14.0736303CurrentTrain: epoch  0, batch     9 | loss: 14.4703903CurrentTrain: epoch  0, batch    10 | loss: 12.9645939CurrentTrain: epoch  0, batch    11 | loss: 13.7130823CurrentTrain: epoch  0, batch    12 | loss: 12.8997707CurrentTrain: epoch  0, batch    13 | loss: 13.0214911CurrentTrain: epoch  0, batch    14 | loss: 13.6096497CurrentTrain: epoch  0, batch    15 | loss: 13.3254366CurrentTrain: epoch  0, batch    16 | loss: 12.8801088CurrentTrain: epoch  0, batch    17 | loss: 13.3722553CurrentTrain: epoch  0, batch    18 | loss: 13.0009127CurrentTrain: epoch  0, batch    19 | loss: 12.3130426CurrentTrain: epoch  0, batch    20 | loss: 12.3333073CurrentTrain: epoch  0, batch    21 | loss: 12.5128584CurrentTrain: epoch  0, batch    22 | loss: 12.6588202CurrentTrain: epoch  0, batch    23 | loss: 12.4199572CurrentTrain: epoch  0, batch    24 | loss: 12.4336452CurrentTrain: epoch  0, batch    25 | loss: 13.0657024CurrentTrain: epoch  0, batch    26 | loss: 11.4751892CurrentTrain: epoch  0, batch    27 | loss: 11.8902855CurrentTrain: epoch  0, batch    28 | loss: 11.8373318CurrentTrain: epoch  0, batch    29 | loss: 10.9691114CurrentTrain: epoch  0, batch    30 | loss: 12.4189873CurrentTrain: epoch  0, batch    31 | loss: 11.7000904CurrentTrain: epoch  0, batch    32 | loss: 11.8166676CurrentTrain: epoch  0, batch    33 | loss: 11.9916344CurrentTrain: epoch  0, batch    34 | loss: 11.8825254CurrentTrain: epoch  0, batch    35 | loss: 10.5627317CurrentTrain: epoch  0, batch    36 | loss: 11.6873808CurrentTrain: epoch  0, batch    37 | loss: 10.3393364CurrentTrain: epoch  1, batch     0 | loss: 11.4997787CurrentTrain: epoch  1, batch     1 | loss: 10.8973379CurrentTrain: epoch  1, batch     2 | loss: 10.6172104CurrentTrain: epoch  1, batch     3 | loss: 10.3209209CurrentTrain: epoch  1, batch     4 | loss: 11.0222111CurrentTrain: epoch  1, batch     5 | loss: 10.8443871CurrentTrain: epoch  1, batch     6 | loss: 10.3746662CurrentTrain: epoch  1, batch     7 | loss: 11.8216486CurrentTrain: epoch  1, batch     8 | loss: 11.6812620CurrentTrain: epoch  1, batch     9 | loss: 9.9296055CurrentTrain: epoch  1, batch    10 | loss: 10.6780252CurrentTrain: epoch  1, batch    11 | loss: 10.0265818CurrentTrain: epoch  1, batch    12 | loss: 9.4239693CurrentTrain: epoch  1, batch    13 | loss: 9.5733528CurrentTrain: epoch  1, batch    14 | loss: 11.8062916CurrentTrain: epoch  1, batch    15 | loss: 9.9181652CurrentTrain: epoch  1, batch    16 | loss: 11.4056730CurrentTrain: epoch  1, batch    17 | loss: 11.3602638CurrentTrain: epoch  1, batch    18 | loss: 10.2472649CurrentTrain: epoch  1, batch    19 | loss: 10.1753302CurrentTrain: epoch  1, batch    20 | loss: 9.6858540CurrentTrain: epoch  1, batch    21 | loss: 10.1335268CurrentTrain: epoch  1, batch    22 | loss: 9.6916189CurrentTrain: epoch  1, batch    23 | loss: 10.3055706CurrentTrain: epoch  1, batch    24 | loss: 9.8982983CurrentTrain: epoch  1, batch    25 | loss: 9.1292372CurrentTrain: epoch  1, batch    26 | loss: 9.6028185CurrentTrain: epoch  1, batch    27 | loss: 10.4370842CurrentTrain: epoch  1, batch    28 | loss: 9.5675583CurrentTrain: epoch  1, batch    29 | loss: 8.4547167CurrentTrain: epoch  1, batch    30 | loss: 9.3861856CurrentTrain: epoch  1, batch    31 | loss: 8.2506018CurrentTrain: epoch  1, batch    32 | loss: 9.1737957CurrentTrain: epoch  1, batch    33 | loss: 8.9918747CurrentTrain: epoch  1, batch    34 | loss: 9.5955315CurrentTrain: epoch  1, batch    35 | loss: 9.7910004CurrentTrain: epoch  1, batch    36 | loss: 8.6591806CurrentTrain: epoch  1, batch    37 | loss: 8.9075089CurrentTrain: epoch  2, batch     0 | loss: 9.7584162CurrentTrain: epoch  2, batch     1 | loss: 8.0489349CurrentTrain: epoch  2, batch     2 | loss: 9.2441559CurrentTrain: epoch  2, batch     3 | loss: 9.0089893CurrentTrain: epoch  2, batch     4 | loss: 10.7349415CurrentTrain: epoch  2, batch     5 | loss: 9.0441427CurrentTrain: epoch  2, batch     6 | loss: 8.6068220CurrentTrain: epoch  2, batch     7 | loss: 7.3019824CurrentTrain: epoch  2, batch     8 | loss: 8.7972813CurrentTrain: epoch  2, batch     9 | loss: 8.5099554CurrentTrain: epoch  2, batch    10 | loss: 7.6347446CurrentTrain: epoch  2, batch    11 | loss: 7.7759871CurrentTrain: epoch  2, batch    12 | loss: 8.3273478CurrentTrain: epoch  2, batch    13 | loss: 8.9034882CurrentTrain: epoch  2, batch    14 | loss: 8.6671562CurrentTrain: epoch  2, batch    15 | loss: 9.0883665CurrentTrain: epoch  2, batch    16 | loss: 8.5090504CurrentTrain: epoch  2, batch    17 | loss: 8.6750612CurrentTrain: epoch  2, batch    18 | loss: 6.4500761CurrentTrain: epoch  2, batch    19 | loss: 8.2141733CurrentTrain: epoch  2, batch    20 | loss: 8.3524809CurrentTrain: epoch  2, batch    21 | loss: 7.8546934CurrentTrain: epoch  2, batch    22 | loss: 7.6804242CurrentTrain: epoch  2, batch    23 | loss: 8.2774076CurrentTrain: epoch  2, batch    24 | loss: 7.0997224CurrentTrain: epoch  2, batch    25 | loss: 7.8479428CurrentTrain: epoch  2, batch    26 | loss: 8.8849020CurrentTrain: epoch  2, batch    27 | loss: 7.2693701CurrentTrain: epoch  2, batch    28 | loss: 7.6826434CurrentTrain: epoch  2, batch    29 | loss: 8.2239037CurrentTrain: epoch  2, batch    30 | loss: 8.9086828CurrentTrain: epoch  2, batch    31 | loss: 7.8681970CurrentTrain: epoch  2, batch    32 | loss: 7.8527856CurrentTrain: epoch  2, batch    33 | loss: 8.5046329CurrentTrain: epoch  2, batch    34 | loss: 7.7775865CurrentTrain: epoch  2, batch    35 | loss: 10.1981659CurrentTrain: epoch  2, batch    36 | loss: 8.7408590CurrentTrain: epoch  2, batch    37 | loss: 7.0946321CurrentTrain: epoch  3, batch     0 | loss: 7.4690251CurrentTrain: epoch  3, batch     1 | loss: 7.8159709CurrentTrain: epoch  3, batch     2 | loss: 7.9895983CurrentTrain: epoch  3, batch     3 | loss: 7.4295321CurrentTrain: epoch  3, batch     4 | loss: 6.8689036CurrentTrain: epoch  3, batch     5 | loss: 8.2962112CurrentTrain: epoch  3, batch     6 | loss: 7.7667794CurrentTrain: epoch  3, batch     7 | loss: 8.0314503CurrentTrain: epoch  3, batch     8 | loss: 7.3439054CurrentTrain: epoch  3, batch     9 | loss: 7.9762135CurrentTrain: epoch  3, batch    10 | loss: 7.3833795CurrentTrain: epoch  3, batch    11 | loss: 7.2086577CurrentTrain: epoch  3, batch    12 | loss: 7.4203882CurrentTrain: epoch  3, batch    13 | loss: 7.7284169CurrentTrain: epoch  3, batch    14 | loss: 6.0812736CurrentTrain: epoch  3, batch    15 | loss: 5.7725477CurrentTrain: epoch  3, batch    16 | loss: 7.6730990CurrentTrain: epoch  3, batch    17 | loss: 7.5840735CurrentTrain: epoch  3, batch    18 | loss: 6.6934500CurrentTrain: epoch  3, batch    19 | loss: 7.6024952CurrentTrain: epoch  3, batch    20 | loss: 7.2572608CurrentTrain: epoch  3, batch    21 | loss: 7.7007351CurrentTrain: epoch  3, batch    22 | loss: 6.9834185CurrentTrain: epoch  3, batch    23 | loss: 7.0829611CurrentTrain: epoch  3, batch    24 | loss: 7.9395566CurrentTrain: epoch  3, batch    25 | loss: 7.3149238CurrentTrain: epoch  3, batch    26 | loss: 6.9149189CurrentTrain: epoch  3, batch    27 | loss: 7.3034592CurrentTrain: epoch  3, batch    28 | loss: 8.5770569CurrentTrain: epoch  3, batch    29 | loss: 8.4190178CurrentTrain: epoch  3, batch    30 | loss: 8.0223293CurrentTrain: epoch  3, batch    31 | loss: 7.3728547CurrentTrain: epoch  3, batch    32 | loss: 8.5444403CurrentTrain: epoch  3, batch    33 | loss: 7.1595588CurrentTrain: epoch  3, batch    34 | loss: 7.0868921CurrentTrain: epoch  3, batch    35 | loss: 6.7332382CurrentTrain: epoch  3, batch    36 | loss: 7.2233310CurrentTrain: epoch  3, batch    37 | loss: 8.6626234CurrentTrain: epoch  4, batch     0 | loss: 7.1529865CurrentTrain: epoch  4, batch     1 | loss: 6.9880419CurrentTrain: epoch  4, batch     2 | loss: 7.4749212CurrentTrain: epoch  4, batch     3 | loss: 5.8002901CurrentTrain: epoch  4, batch     4 | loss: 7.2471271CurrentTrain: epoch  4, batch     5 | loss: 7.1161084CurrentTrain: epoch  4, batch     6 | loss: 7.0395074CurrentTrain: epoch  4, batch     7 | loss: 6.2098813CurrentTrain: epoch  4, batch     8 | loss: 6.2548738CurrentTrain: epoch  4, batch     9 | loss: 6.3933244CurrentTrain: epoch  4, batch    10 | loss: 6.1642542CurrentTrain: epoch  4, batch    11 | loss: 6.3622961CurrentTrain: epoch  4, batch    12 | loss: 5.4321432CurrentTrain: epoch  4, batch    13 | loss: 6.9015713CurrentTrain: epoch  4, batch    14 | loss: 6.5690713CurrentTrain: epoch  4, batch    15 | loss: 7.2313342CurrentTrain: epoch  4, batch    16 | loss: 6.3399911CurrentTrain: epoch  4, batch    17 | loss: 6.0023570CurrentTrain: epoch  4, batch    18 | loss: 5.7897043CurrentTrain: epoch  4, batch    19 | loss: 7.1549821CurrentTrain: epoch  4, batch    20 | loss: 6.9319758CurrentTrain: epoch  4, batch    21 | loss: 7.4247766CurrentTrain: epoch  4, batch    22 | loss: 6.5436492CurrentTrain: epoch  4, batch    23 | loss: 6.4608779CurrentTrain: epoch  4, batch    24 | loss: 7.6856623CurrentTrain: epoch  4, batch    25 | loss: 5.5839009CurrentTrain: epoch  4, batch    26 | loss: 6.4226294CurrentTrain: epoch  4, batch    27 | loss: 6.5656309CurrentTrain: epoch  4, batch    28 | loss: 7.0857773CurrentTrain: epoch  4, batch    29 | loss: 6.7751222CurrentTrain: epoch  4, batch    30 | loss: 6.8605251CurrentTrain: epoch  4, batch    31 | loss: 7.0031862CurrentTrain: epoch  4, batch    32 | loss: 5.6998782CurrentTrain: epoch  4, batch    33 | loss: 7.0610542CurrentTrain: epoch  4, batch    34 | loss: 6.7062421CurrentTrain: epoch  4, batch    35 | loss: 7.4280076CurrentTrain: epoch  4, batch    36 | loss: 6.6511445CurrentTrain: epoch  4, batch    37 | loss: 6.8998990CurrentTrain: epoch  5, batch     0 | loss: 6.4520860CurrentTrain: epoch  5, batch     1 | loss: 6.4790168CurrentTrain: epoch  5, batch     2 | loss: 6.6785874CurrentTrain: epoch  5, batch     3 | loss: 5.7891731CurrentTrain: epoch  5, batch     4 | loss: 7.0713992CurrentTrain: epoch  5, batch     5 | loss: 7.3005252CurrentTrain: epoch  5, batch     6 | loss: 6.7586861CurrentTrain: epoch  5, batch     7 | loss: 7.0638299CurrentTrain: epoch  5, batch     8 | loss: 6.1335444CurrentTrain: epoch  5, batch     9 | loss: 6.0180755CurrentTrain: epoch  5, batch    10 | loss: 5.8788881CurrentTrain: epoch  5, batch    11 | loss: 6.4581809CurrentTrain: epoch  5, batch    12 | loss: 7.5112915CurrentTrain: epoch  5, batch    13 | loss: 5.9731846CurrentTrain: epoch  5, batch    14 | loss: 6.2050858CurrentTrain: epoch  5, batch    15 | loss: 6.9694843CurrentTrain: epoch  5, batch    16 | loss: 6.3297133CurrentTrain: epoch  5, batch    17 | loss: 5.9405751CurrentTrain: epoch  5, batch    18 | loss: 6.4195108CurrentTrain: epoch  5, batch    19 | loss: 6.2737355CurrentTrain: epoch  5, batch    20 | loss: 6.8356757CurrentTrain: epoch  5, batch    21 | loss: 5.8630133CurrentTrain: epoch  5, batch    22 | loss: 5.8507247CurrentTrain: epoch  5, batch    23 | loss: 6.1253796CurrentTrain: epoch  5, batch    24 | loss: 6.1035852CurrentTrain: epoch  5, batch    25 | loss: 5.8384767CurrentTrain: epoch  5, batch    26 | loss: 5.3635249CurrentTrain: epoch  5, batch    27 | loss: 5.8325028CurrentTrain: epoch  5, batch    28 | loss: 7.6585197CurrentTrain: epoch  5, batch    29 | loss: 6.5535340CurrentTrain: epoch  5, batch    30 | loss: 5.8390355CurrentTrain: epoch  5, batch    31 | loss: 5.9250937CurrentTrain: epoch  5, batch    32 | loss: 5.6952252CurrentTrain: epoch  5, batch    33 | loss: 6.0082383CurrentTrain: epoch  5, batch    34 | loss: 6.2588787CurrentTrain: epoch  5, batch    35 | loss: 5.3060098CurrentTrain: epoch  5, batch    36 | loss: 5.8371134CurrentTrain: epoch  5, batch    37 | loss: 6.2310367CurrentTrain: epoch  6, batch     0 | loss: 5.6001663CurrentTrain: epoch  6, batch     1 | loss: 6.3154144CurrentTrain: epoch  6, batch     2 | loss: 5.5272956CurrentTrain: epoch  6, batch     3 | loss: 6.2781610CurrentTrain: epoch  6, batch     4 | loss: 5.5668201CurrentTrain: epoch  6, batch     5 | loss: 6.1848512CurrentTrain: epoch  6, batch     6 | loss: 6.1142597CurrentTrain: epoch  6, batch     7 | loss: 6.1092334CurrentTrain: epoch  6, batch     8 | loss: 5.4377337CurrentTrain: epoch  6, batch     9 | loss: 5.9195037CurrentTrain: epoch  6, batch    10 | loss: 6.1378608CurrentTrain: epoch  6, batch    11 | loss: 6.5353551CurrentTrain: epoch  6, batch    12 | loss: 5.2215948CurrentTrain: epoch  6, batch    13 | loss: 5.4479527CurrentTrain: epoch  6, batch    14 | loss: 5.4189539CurrentTrain: epoch  6, batch    15 | loss: 5.9420757CurrentTrain: epoch  6, batch    16 | loss: 5.9965887CurrentTrain: epoch  6, batch    17 | loss: 6.6125660CurrentTrain: epoch  6, batch    18 | loss: 5.6502390CurrentTrain: epoch  6, batch    19 | loss: 5.3605452CurrentTrain: epoch  6, batch    20 | loss: 5.9681768CurrentTrain: epoch  6, batch    21 | loss: 5.4601722CurrentTrain: epoch  6, batch    22 | loss: 6.4092197CurrentTrain: epoch  6, batch    23 | loss: 5.4496384CurrentTrain: epoch  6, batch    24 | loss: 5.8119302CurrentTrain: epoch  6, batch    25 | loss: 5.8279114CurrentTrain: epoch  6, batch    26 | loss: 5.2719378CurrentTrain: epoch  6, batch    27 | loss: 5.5668097CurrentTrain: epoch  6, batch    28 | loss: 6.0925951CurrentTrain: epoch  6, batch    29 | loss: 5.9930692CurrentTrain: epoch  6, batch    30 | loss: 5.9395795CurrentTrain: epoch  6, batch    31 | loss: 4.9639492CurrentTrain: epoch  6, batch    32 | loss: 5.4026003CurrentTrain: epoch  6, batch    33 | loss: 6.3478208CurrentTrain: epoch  6, batch    34 | loss: 5.0627117CurrentTrain: epoch  6, batch    35 | loss: 5.0744390CurrentTrain: epoch  6, batch    36 | loss: 5.2369676CurrentTrain: epoch  6, batch    37 | loss: 5.9830775CurrentTrain: epoch  7, batch     0 | loss: 5.0835824CurrentTrain: epoch  7, batch     1 | loss: 5.4106760CurrentTrain: epoch  7, batch     2 | loss: 5.7667546CurrentTrain: epoch  7, batch     3 | loss: 5.2619352CurrentTrain: epoch  7, batch     4 | loss: 5.2052712CurrentTrain: epoch  7, batch     5 | loss: 5.0464692CurrentTrain: epoch  7, batch     6 | loss: 5.3951793CurrentTrain: epoch  7, batch     7 | loss: 5.5694599CurrentTrain: epoch  7, batch     8 | loss: 5.3960176CurrentTrain: epoch  7, batch     9 | loss: 5.1179533CurrentTrain: epoch  7, batch    10 | loss: 5.8667984CurrentTrain: epoch  7, batch    11 | loss: 5.2518573CurrentTrain: epoch  7, batch    12 | loss: 5.6763949CurrentTrain: epoch  7, batch    13 | loss: 5.2510395CurrentTrain: epoch  7, batch    14 | loss: 5.1568542CurrentTrain: epoch  7, batch    15 | loss: 5.5988750CurrentTrain: epoch  7, batch    16 | loss: 5.2888036CurrentTrain: epoch  7, batch    17 | loss: 5.3037758CurrentTrain: epoch  7, batch    18 | loss: 4.9752922CurrentTrain: epoch  7, batch    19 | loss: 5.8889132CurrentTrain: epoch  7, batch    20 | loss: 5.1519194CurrentTrain: epoch  7, batch    21 | loss: 5.3816900CurrentTrain: epoch  7, batch    22 | loss: 5.1440363CurrentTrain: epoch  7, batch    23 | loss: 5.0569630CurrentTrain: epoch  7, batch    24 | loss: 5.8666902CurrentTrain: epoch  7, batch    25 | loss: 5.5451984CurrentTrain: epoch  7, batch    26 | loss: 5.1625495CurrentTrain: epoch  7, batch    27 | loss: 5.1293521CurrentTrain: epoch  7, batch    28 | loss: 5.1658130CurrentTrain: epoch  7, batch    29 | loss: 4.9176693CurrentTrain: epoch  7, batch    30 | loss: 5.0608010CurrentTrain: epoch  7, batch    31 | loss: 4.9480104CurrentTrain: epoch  7, batch    32 | loss: 5.1538858CurrentTrain: epoch  7, batch    33 | loss: 5.5007920CurrentTrain: epoch  7, batch    34 | loss: 5.0157490CurrentTrain: epoch  7, batch    35 | loss: 4.8549180CurrentTrain: epoch  7, batch    36 | loss: 5.1693587CurrentTrain: epoch  7, batch    37 | loss: 4.9757462CurrentTrain: epoch  8, batch     0 | loss: 5.0206866CurrentTrain: epoch  8, batch     1 | loss: 4.9528675CurrentTrain: epoch  8, batch     2 | loss: 4.9715519CurrentTrain: epoch  8, batch     3 | loss: 5.0007634CurrentTrain: epoch  8, batch     4 | loss: 4.9705429CurrentTrain: epoch  8, batch     5 | loss: 4.8914666CurrentTrain: epoch  8, batch     6 | loss: 5.0453472CurrentTrain: epoch  8, batch     7 | loss: 4.9321837CurrentTrain: epoch  8, batch     8 | loss: 5.1815004CurrentTrain: epoch  8, batch     9 | loss: 5.0103688CurrentTrain: epoch  8, batch    10 | loss: 4.9644518CurrentTrain: epoch  8, batch    11 | loss: 4.9692497CurrentTrain: epoch  8, batch    12 | loss: 5.1119413CurrentTrain: epoch  8, batch    13 | loss: 5.2105088CurrentTrain: epoch  8, batch    14 | loss: 4.9598188CurrentTrain: epoch  8, batch    15 | loss: 4.9913802CurrentTrain: epoch  8, batch    16 | loss: 5.0506682CurrentTrain: epoch  8, batch    17 | loss: 4.9207134CurrentTrain: epoch  8, batch    18 | loss: 5.1806736CurrentTrain: epoch  8, batch    19 | loss: 4.8521008CurrentTrain: epoch  8, batch    20 | loss: 4.8654804CurrentTrain: epoch  8, batch    21 | loss: 6.0268297CurrentTrain: epoch  8, batch    22 | loss: 5.2875395CurrentTrain: epoch  8, batch    23 | loss: 5.3220687CurrentTrain: epoch  8, batch    24 | loss: 5.4029870CurrentTrain: epoch  8, batch    25 | loss: 4.8664689CurrentTrain: epoch  8, batch    26 | loss: 4.8810058CurrentTrain: epoch  8, batch    27 | loss: 5.3758378CurrentTrain: epoch  8, batch    28 | loss: 5.6232252CurrentTrain: epoch  8, batch    29 | loss: 4.7678986CurrentTrain: epoch  8, batch    30 | loss: 4.9697542CurrentTrain: epoch  8, batch    31 | loss: 5.0346718CurrentTrain: epoch  8, batch    32 | loss: 5.5204196CurrentTrain: epoch  8, batch    33 | loss: 5.7581558CurrentTrain: epoch  8, batch    34 | loss: 4.8648257CurrentTrain: epoch  8, batch    35 | loss: 4.9831657CurrentTrain: epoch  8, batch    36 | loss: 5.4307127CurrentTrain: epoch  8, batch    37 | loss: 5.0153584CurrentTrain: epoch  9, batch     0 | loss: 4.8904715CurrentTrain: epoch  9, batch     1 | loss: 5.3288522CurrentTrain: epoch  9, batch     2 | loss: 5.0503616CurrentTrain: epoch  9, batch     3 | loss: 5.1669216CurrentTrain: epoch  9, batch     4 | loss: 5.3053436CurrentTrain: epoch  9, batch     5 | loss: 4.9144850CurrentTrain: epoch  9, batch     6 | loss: 4.8299828CurrentTrain: epoch  9, batch     7 | loss: 4.8898425CurrentTrain: epoch  9, batch     8 | loss: 5.0092888CurrentTrain: epoch  9, batch     9 | loss: 5.1035509CurrentTrain: epoch  9, batch    10 | loss: 5.0284591CurrentTrain: epoch  9, batch    11 | loss: 4.9180737CurrentTrain: epoch  9, batch    12 | loss: 4.7758479CurrentTrain: epoch  9, batch    13 | loss: 4.9468312CurrentTrain: epoch  9, batch    14 | loss: 4.9730434CurrentTrain: epoch  9, batch    15 | loss: 5.0479779CurrentTrain: epoch  9, batch    16 | loss: 4.7633924CurrentTrain: epoch  9, batch    17 | loss: 4.7964010CurrentTrain: epoch  9, batch    18 | loss: 4.9165864CurrentTrain: epoch  9, batch    19 | loss: 5.3731027CurrentTrain: epoch  9, batch    20 | loss: 4.8226490CurrentTrain: epoch  9, batch    21 | loss: 5.5929179CurrentTrain: epoch  9, batch    22 | loss: 5.0714540CurrentTrain: epoch  9, batch    23 | loss: 5.0527444CurrentTrain: epoch  9, batch    24 | loss: 5.3007569CurrentTrain: epoch  9, batch    25 | loss: 5.3163304CurrentTrain: epoch  9, batch    26 | loss: 5.1726036CurrentTrain: epoch  9, batch    27 | loss: 4.8972831CurrentTrain: epoch  9, batch    28 | loss: 4.9556198CurrentTrain: epoch  9, batch    29 | loss: 4.9418368CurrentTrain: epoch  9, batch    30 | loss: 5.2459655CurrentTrain: epoch  9, batch    31 | loss: 5.0274472CurrentTrain: epoch  9, batch    32 | loss: 4.8662267CurrentTrain: epoch  9, batch    33 | loss: 4.9185052CurrentTrain: epoch  9, batch    34 | loss: 4.9654021CurrentTrain: epoch  9, batch    35 | loss: 4.9198112CurrentTrain: epoch  9, batch    36 | loss: 4.8629098CurrentTrain: epoch  9, batch    37 | loss: 4.7159204
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 91.96%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 94.32%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 94.27%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 94.23%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 90.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 87.89%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 86.76%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 85.76%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 84.87%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 85.12%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.80%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 86.41%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.98%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.98%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 88.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.62%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 88.58%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 88.71%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 87.50%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 91.96%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 94.32%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 94.27%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 94.23%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 90.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 87.89%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 86.76%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 85.76%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 84.87%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 85.12%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.80%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 86.41%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.98%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.98%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 88.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.62%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 88.58%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 88.71%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 87.50%   
cur_acc_llm:  [0.875]
his_acc_llm:  [0.875]
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 84.17%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 83.59%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.09%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 82.29%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.78%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.28%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 87.70%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.89%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.36%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 84.17%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 83.59%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.09%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 82.29%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.78%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.28%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 87.70%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.89%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.36%   
cur_acc:  ['0.8636']
his_acc:  ['0.8636']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 5.3207197CurrentTrain: epoch  0, batch     1 | loss: 6.2082167CurrentTrain: epoch  1, batch     0 | loss: 4.9012117CurrentTrain: epoch  1, batch     1 | loss: 3.9824746CurrentTrain: epoch  2, batch     0 | loss: 4.3466244CurrentTrain: epoch  2, batch     1 | loss: 3.0896714CurrentTrain: epoch  3, batch     0 | loss: 3.2097836CurrentTrain: epoch  3, batch     1 | loss: 2.9222252CurrentTrain: epoch  4, batch     0 | loss: 2.8557153CurrentTrain: epoch  4, batch     1 | loss: 2.7992735CurrentTrain: epoch  5, batch     0 | loss: 2.7414789CurrentTrain: epoch  5, batch     1 | loss: 2.0133739CurrentTrain: epoch  6, batch     0 | loss: 2.2765012CurrentTrain: epoch  6, batch     1 | loss: 2.1156781CurrentTrain: epoch  7, batch     0 | loss: 2.0408046CurrentTrain: epoch  7, batch     1 | loss: 2.0322270CurrentTrain: epoch  8, batch     0 | loss: 1.8659294CurrentTrain: epoch  8, batch     1 | loss: 1.8874931CurrentTrain: epoch  9, batch     0 | loss: 1.8350155CurrentTrain: epoch  9, batch     1 | loss: 1.7633625
Mixup data size:  60
MixupTrain:  epoch  0, batch     2 | loss: 9.4390495
MemoryTrain:  epoch  0, batch     0 | loss: 2.9707565MemoryTrain:  epoch  1, batch     0 | loss: 2.5332596MemoryTrain:  epoch  2, batch     0 | loss: 1.4568973MemoryTrain:  epoch  3, batch     0 | loss: 0.9224984MemoryTrain:  epoch  4, batch     0 | loss: 0.5189950MemoryTrain:  epoch  5, batch     0 | loss: 0.1617035MemoryTrain:  epoch  6, batch     0 | loss: 0.1298584MemoryTrain:  epoch  7, batch     0 | loss: 0.0397939MemoryTrain:  epoch  8, batch     0 | loss: 0.0137278MemoryTrain:  epoch  9, batch     0 | loss: 0.0115661
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 9.0230436CurrentTrain: epoch  0, batch     1 | loss: 8.9970350CurrentTrain: epoch  1, batch     0 | loss: 9.2753906CurrentTrain: epoch  1, batch     1 | loss: 7.2483239CurrentTrain: epoch  2, batch     0 | loss: 8.1560984CurrentTrain: epoch  2, batch     1 | loss: 8.1604900CurrentTrain: epoch  3, batch     0 | loss: 7.6138926CurrentTrain: epoch  3, batch     1 | loss: 6.7480941CurrentTrain: epoch  4, batch     0 | loss: 6.4404478CurrentTrain: epoch  4, batch     1 | loss: 6.9481468CurrentTrain: epoch  5, batch     0 | loss: 7.6338921CurrentTrain: epoch  5, batch     1 | loss: 4.8551130CurrentTrain: epoch  6, batch     0 | loss: 6.2203217CurrentTrain: epoch  6, batch     1 | loss: 5.1714382CurrentTrain: epoch  7, batch     0 | loss: 5.3326669CurrentTrain: epoch  7, batch     1 | loss: 5.3637576CurrentTrain: epoch  8, batch     0 | loss: 5.0471630CurrentTrain: epoch  8, batch     1 | loss: 4.9449205CurrentTrain: epoch  9, batch     0 | loss: 4.4520445CurrentTrain: epoch  9, batch     1 | loss: 4.7607622
Mixup data size:  60
MixupTrain:  epoch  0, batch     0 | loss: 8.0858130MixupTrain:  epoch  0, batch     1 | loss: 6.8296905MixupTrain:  epoch  0, batch     2 | loss: 6.1248705MixupTrain:  epoch  0, batch     3 | loss: 6.8656631
MemoryTrain:  epoch  0, batch     0 | loss: 1.3885627MemoryTrain:  epoch  1, batch     0 | loss: 1.7490547MemoryTrain:  epoch  2, batch     0 | loss: 1.0358189MemoryTrain:  epoch  3, batch     0 | loss: 1.5449491MemoryTrain:  epoch  4, batch     0 | loss: 0.9571747MemoryTrain:  epoch  5, batch     0 | loss: 0.9334997MemoryTrain:  epoch  6, batch     0 | loss: 0.6864938MemoryTrain:  epoch  7, batch     0 | loss: 0.4904855MemoryTrain:  epoch  8, batch     0 | loss: 0.4543341MemoryTrain:  epoch  9, batch     0 | loss: 0.4425364
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 81.25%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 78.85%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 76.79%   [EVAL] batch:   14 | acc: 6.25%,  total acc: 72.08%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 85.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 83.20%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 82.35%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 81.60%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 80.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 81.85%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 82.67%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.42%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.11%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.34%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.65%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.16%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 86.42%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 86.69%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.11%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 87.12%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 86.76%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 87.14%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 87.15%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 86.66%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.01%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.34%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.66%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 87.80%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 88.10%   [EVAL] batch:   42 | acc: 37.50%,  total acc: 86.92%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 86.08%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 85.14%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 84.51%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 83.11%   
cur_acc_llm:  [0.875, 0.7208333333333333]
his_acc_llm:  [0.875, 0.8311170212765957]
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 15.62%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 12.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 26.04%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 36.61%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 43.75%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 49.31%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 51.88%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 53.98%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 56.25%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 58.65%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 61.16%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 60.00%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 86.16%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.29%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 81.91%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.04%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.81%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.51%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.30%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.57%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.05%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 87.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 86.93%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 84.74%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 82.68%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 80.90%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 79.05%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 78.45%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 79.01%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.53%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 79.73%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 80.21%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 79.65%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 79.86%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 80.03%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 80.05%   
cur_acc:  ['0.8636', '0.6000']
his_acc:  ['0.8636', '0.8005']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 5.4340758CurrentTrain: epoch  0, batch     1 | loss: 5.9842811CurrentTrain: epoch  1, batch     0 | loss: 4.0352306CurrentTrain: epoch  1, batch     1 | loss: 5.1112509CurrentTrain: epoch  2, batch     0 | loss: 4.3316922CurrentTrain: epoch  2, batch     1 | loss: 3.0113423CurrentTrain: epoch  3, batch     0 | loss: 3.3877039CurrentTrain: epoch  3, batch     1 | loss: 3.2137253CurrentTrain: epoch  4, batch     0 | loss: 2.8251076CurrentTrain: epoch  4, batch     1 | loss: 2.9774384CurrentTrain: epoch  5, batch     0 | loss: 2.3913591CurrentTrain: epoch  5, batch     1 | loss: 2.5231395CurrentTrain: epoch  6, batch     0 | loss: 2.4779289CurrentTrain: epoch  6, batch     1 | loss: 2.5474637CurrentTrain: epoch  7, batch     0 | loss: 2.1982565CurrentTrain: epoch  7, batch     1 | loss: 2.3410199CurrentTrain: epoch  8, batch     0 | loss: 2.1669629CurrentTrain: epoch  8, batch     1 | loss: 1.8793695CurrentTrain: epoch  9, batch     0 | loss: 1.9011426CurrentTrain: epoch  9, batch     1 | loss: 1.9150997
Mixup data size:  70
MixupTrain:  epoch  0, batch     3 | loss: 5.4643698
MemoryTrain:  epoch  0, batch     0 | loss: 2.8180909MemoryTrain:  epoch  1, batch     0 | loss: 2.6479561MemoryTrain:  epoch  2, batch     0 | loss: 1.7070938MemoryTrain:  epoch  3, batch     0 | loss: 1.5644205MemoryTrain:  epoch  4, batch     0 | loss: 1.1281579MemoryTrain:  epoch  5, batch     0 | loss: 0.4539637MemoryTrain:  epoch  6, batch     0 | loss: 0.2638457MemoryTrain:  epoch  7, batch     0 | loss: 0.2452898MemoryTrain:  epoch  8, batch     0 | loss: 0.1701510MemoryTrain:  epoch  9, batch     0 | loss: 0.0864063
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 9.2773104CurrentTrain: epoch  0, batch     1 | loss: 9.0439196CurrentTrain: epoch  1, batch     0 | loss: 8.3485298CurrentTrain: epoch  1, batch     1 | loss: 7.1961222CurrentTrain: epoch  2, batch     0 | loss: 7.3226333CurrentTrain: epoch  2, batch     1 | loss: 5.7460928CurrentTrain: epoch  3, batch     0 | loss: 6.7621984CurrentTrain: epoch  3, batch     1 | loss: 4.5043778CurrentTrain: epoch  4, batch     0 | loss: 5.7639170CurrentTrain: epoch  4, batch     1 | loss: 4.5861650CurrentTrain: epoch  5, batch     0 | loss: 5.7168989CurrentTrain: epoch  5, batch     1 | loss: 4.2118363CurrentTrain: epoch  6, batch     0 | loss: 4.7735910CurrentTrain: epoch  6, batch     1 | loss: 4.9712663CurrentTrain: epoch  7, batch     0 | loss: 4.5056496CurrentTrain: epoch  7, batch     1 | loss: 4.0553107CurrentTrain: epoch  8, batch     0 | loss: 3.9668255CurrentTrain: epoch  8, batch     1 | loss: 3.6566637CurrentTrain: epoch  9, batch     0 | loss: 3.5849376CurrentTrain: epoch  9, batch     1 | loss: 3.2555609
Mixup data size:  70
MixupTrain:  epoch  0, batch     3 | loss: 6.4619841MixupTrain:  epoch  0, batch     4 | loss: 6.2153018
MemoryTrain:  epoch  0, batch     0 | loss: 1.9471909MemoryTrain:  epoch  1, batch     0 | loss: 2.3369207MemoryTrain:  epoch  2, batch     0 | loss: 1.9009440MemoryTrain:  epoch  3, batch     0 | loss: 1.4793580MemoryTrain:  epoch  4, batch     0 | loss: 1.1729860MemoryTrain:  epoch  5, batch     0 | loss: 1.1588517MemoryTrain:  epoch  6, batch     0 | loss: 0.9897618MemoryTrain:  epoch  7, batch     0 | loss: 1.0393324MemoryTrain:  epoch  8, batch     0 | loss: 0.8411627MemoryTrain:  epoch  9, batch     0 | loss: 0.7414823
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 91.41%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 88.84%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 43.75%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 42.19%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 48.75%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 48.96%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 53.57%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 59.38%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 63.19%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 66.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 69.89%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 73.08%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 71.43%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 70.00%   [EVAL] batch:   15 | acc: 43.75%,  total acc: 68.36%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 66.91%   [EVAL] batch:   17 | acc: 37.50%,  total acc: 65.28%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 64.47%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 64.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 66.07%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 67.61%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 70.05%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 71.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.12%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 73.88%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 74.78%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 75.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 76.01%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 76.76%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 75.95%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 73.90%   [EVAL] batch:   34 | acc: 18.75%,  total acc: 72.32%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 70.31%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 68.41%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 68.26%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 69.22%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 68.90%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 69.64%   [EVAL] batch:   42 | acc: 37.50%,  total acc: 68.90%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 67.33%   [EVAL] batch:   44 | acc: 12.50%,  total acc: 66.11%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 64.81%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 63.70%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 64.32%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 64.92%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 65.93%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 66.35%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 66.86%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 67.36%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 67.84%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 68.19%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 68.64%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 68.86%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 69.39%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 69.26%   
cur_acc_llm:  [0.875, 0.7208333333333333, 0.8883928571428571]
his_acc_llm:  [0.875, 0.8311170212765957, 0.6926229508196722]
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 82.69%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 77.68%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 64.58%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 67.86%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 76.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 78.98%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 80.29%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 79.46%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 77.73%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 77.57%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 76.74%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 76.64%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 77.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 78.27%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 79.26%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 80.16%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 80.99%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 81.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.45%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 82.87%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.48%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.05%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 84.68%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 84.96%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 83.52%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 81.07%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 78.93%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 77.08%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 75.17%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 74.34%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 73.88%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 74.06%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 73.63%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 74.26%   [EVAL] batch:   42 | acc: 6.25%,  total acc: 72.67%   [EVAL] batch:   43 | acc: 12.50%,  total acc: 71.31%   [EVAL] batch:   44 | acc: 12.50%,  total acc: 70.00%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 68.48%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 67.55%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 67.45%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 67.98%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 68.38%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 68.26%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 68.63%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 68.99%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 69.33%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 69.77%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 70.50%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 70.58%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 70.66%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 70.62%   [EVAL] batch:   60 | acc: 6.25%,  total acc: 69.57%   
cur_acc:  ['0.8636', '0.6000', '0.7768']
his_acc:  ['0.8636', '0.8005', '0.6957']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 4.7840858CurrentTrain: epoch  0, batch     1 | loss: 6.5760450CurrentTrain: epoch  1, batch     0 | loss: 4.3505373CurrentTrain: epoch  1, batch     1 | loss: 4.8831644CurrentTrain: epoch  2, batch     0 | loss: 3.6463850CurrentTrain: epoch  2, batch     1 | loss: 2.5644248CurrentTrain: epoch  3, batch     0 | loss: 2.4588294CurrentTrain: epoch  3, batch     1 | loss: 2.6334870CurrentTrain: epoch  4, batch     0 | loss: 2.0328922CurrentTrain: epoch  4, batch     1 | loss: 2.5389664CurrentTrain: epoch  5, batch     0 | loss: 1.9692260CurrentTrain: epoch  5, batch     1 | loss: 1.8895473CurrentTrain: epoch  6, batch     0 | loss: 1.9507222CurrentTrain: epoch  6, batch     1 | loss: 1.8617761CurrentTrain: epoch  7, batch     0 | loss: 1.9396763CurrentTrain: epoch  7, batch     1 | loss: 1.8484198CurrentTrain: epoch  8, batch     0 | loss: 1.8714945CurrentTrain: epoch  8, batch     1 | loss: 1.7800965CurrentTrain: epoch  9, batch     0 | loss: 1.8528771CurrentTrain: epoch  9, batch     1 | loss: 1.7285391
Mixup data size:  81
MixupTrain:  epoch  0, batch     3 | loss: 8.3193556MixupTrain:  epoch  0, batch     4 | loss: 5.6901374MixupTrain:  epoch  0, batch     5 | loss: 4.6833224
MemoryTrain:  epoch  0, batch     0 | loss: 1.5623825MemoryTrain:  epoch  0, batch     1 | loss: 1.3418109MemoryTrain:  epoch  1, batch     0 | loss: 2.1380711MemoryTrain:  epoch  1, batch     1 | loss: 0.7444226MemoryTrain:  epoch  2, batch     0 | loss: 1.0869634MemoryTrain:  epoch  2, batch     1 | loss: 1.9973465MemoryTrain:  epoch  3, batch     0 | loss: 0.5889592MemoryTrain:  epoch  3, batch     1 | loss: 0.4506241MemoryTrain:  epoch  4, batch     0 | loss: 0.2030160MemoryTrain:  epoch  4, batch     1 | loss: 0.4695735MemoryTrain:  epoch  5, batch     0 | loss: 0.2371582MemoryTrain:  epoch  5, batch     1 | loss: 0.1642871MemoryTrain:  epoch  6, batch     0 | loss: 0.1090426MemoryTrain:  epoch  6, batch     1 | loss: 0.0760813MemoryTrain:  epoch  7, batch     0 | loss: 0.0757471MemoryTrain:  epoch  7, batch     1 | loss: 0.0656642MemoryTrain:  epoch  8, batch     0 | loss: 0.0745088MemoryTrain:  epoch  8, batch     1 | loss: 0.0428753MemoryTrain:  epoch  9, batch     0 | loss: 0.0349117MemoryTrain:  epoch  9, batch     1 | loss: 0.0739322
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 7.8253093CurrentTrain: epoch  0, batch     1 | loss: 8.1273851CurrentTrain: epoch  1, batch     0 | loss: 7.0812101CurrentTrain: epoch  1, batch     1 | loss: 6.1003294CurrentTrain: epoch  2, batch     0 | loss: 6.2300320CurrentTrain: epoch  2, batch     1 | loss: 5.6021414CurrentTrain: epoch  3, batch     0 | loss: 5.5543871CurrentTrain: epoch  3, batch     1 | loss: 6.0491590CurrentTrain: epoch  4, batch     0 | loss: 4.9236660CurrentTrain: epoch  4, batch     1 | loss: 4.3676019CurrentTrain: epoch  5, batch     0 | loss: 4.4372182CurrentTrain: epoch  5, batch     1 | loss: 3.6054289CurrentTrain: epoch  6, batch     0 | loss: 4.1183367CurrentTrain: epoch  6, batch     1 | loss: 3.4972088CurrentTrain: epoch  7, batch     0 | loss: 3.8909547CurrentTrain: epoch  7, batch     1 | loss: 3.7293813CurrentTrain: epoch  8, batch     0 | loss: 3.3988323CurrentTrain: epoch  8, batch     1 | loss: 3.3163040CurrentTrain: epoch  9, batch     0 | loss: 2.9538393CurrentTrain: epoch  9, batch     1 | loss: 3.2940204
Mixup data size:  79
MixupTrain:  epoch  0, batch     1 | loss: 7.3314091MixupTrain:  epoch  0, batch     2 | loss: 5.8759847MixupTrain:  epoch  0, batch     3 | loss: 5.1780314
MemoryTrain:  epoch  0, batch     0 | loss: 2.2281227MemoryTrain:  epoch  0, batch     1 | loss: 0.9396006MemoryTrain:  epoch  1, batch     0 | loss: 2.4824023MemoryTrain:  epoch  1, batch     1 | loss: 0.7651232MemoryTrain:  epoch  2, batch     0 | loss: 1.4374042MemoryTrain:  epoch  2, batch     1 | loss: 1.3735137MemoryTrain:  epoch  3, batch     0 | loss: 1.3222294MemoryTrain:  epoch  3, batch     1 | loss: 1.3146831MemoryTrain:  epoch  4, batch     0 | loss: 1.3660814MemoryTrain:  epoch  4, batch     1 | loss: 1.0300306MemoryTrain:  epoch  5, batch     0 | loss: 1.1903615MemoryTrain:  epoch  5, batch     1 | loss: 0.4370040MemoryTrain:  epoch  6, batch     0 | loss: 0.9230185MemoryTrain:  epoch  6, batch     1 | loss: 0.6882385MemoryTrain:  epoch  7, batch     0 | loss: 0.9954748MemoryTrain:  epoch  7, batch     1 | loss: 0.7622600MemoryTrain:  epoch  8, batch     0 | loss: 0.9123054MemoryTrain:  epoch  8, batch     1 | loss: 0.2231656MemoryTrain:  epoch  9, batch     0 | loss: 0.6028998MemoryTrain:  epoch  9, batch     1 | loss: 0.2610333
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 91.96%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 92.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.01%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 89.24%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 39.58%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 37.50%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 42.50%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 42.71%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 47.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 53.91%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 58.33%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 65.91%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 68.23%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 67.31%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 64.29%   [EVAL] batch:   14 | acc: 0.00%,  total acc: 60.00%   [EVAL] batch:   15 | acc: 0.00%,  total acc: 56.25%   [EVAL] batch:   16 | acc: 0.00%,  total acc: 52.94%   [EVAL] batch:   17 | acc: 0.00%,  total acc: 50.00%   [EVAL] batch:   18 | acc: 0.00%,  total acc: 47.37%   [EVAL] batch:   19 | acc: 18.75%,  total acc: 45.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 48.51%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 50.85%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 52.72%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 54.69%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 56.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 58.17%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 59.49%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 60.94%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 62.28%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 63.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 64.31%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 65.43%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 65.15%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 63.79%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 63.04%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 61.81%   [EVAL] batch:   36 | acc: 25.00%,  total acc: 60.81%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 61.18%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 62.02%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 62.66%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 62.80%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 63.69%   [EVAL] batch:   42 | acc: 18.75%,  total acc: 62.65%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 61.22%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 59.86%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 58.70%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 57.71%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 58.07%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 57.91%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 57.50%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 57.23%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 56.97%   [EVAL] batch:   52 | acc: 18.75%,  total acc: 56.25%   [EVAL] batch:   53 | acc: 31.25%,  total acc: 55.79%   [EVAL] batch:   54 | acc: 25.00%,  total acc: 55.23%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 55.02%   [EVAL] batch:   56 | acc: 18.75%,  total acc: 54.39%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 53.88%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 53.60%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 53.44%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 53.69%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 54.33%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 55.06%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 55.66%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 56.15%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 56.63%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 57.00%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 57.63%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 57.97%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 58.30%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 58.80%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 59.20%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 59.76%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 60.22%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 60.75%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 61.27%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 61.77%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 61.94%   
cur_acc_llm:  [0.875, 0.7208333333333333, 0.8883928571428571, 0.8923611111111112]
his_acc_llm:  [0.875, 0.8311170212765957, 0.6926229508196722, 0.6193910256410257]
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 83.52%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 86.06%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 88.67%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 89.34%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 85.76%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 48.44%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 53.12%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 57.14%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 69.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 72.16%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 73.44%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 74.04%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 73.66%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 73.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 72.66%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 72.79%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 72.22%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 72.37%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 73.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 74.40%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 75.57%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 76.63%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 77.60%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 78.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.33%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 79.86%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.58%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 81.67%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 82.06%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 82.42%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 81.06%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 78.68%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 76.43%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 74.48%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 72.47%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 71.71%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 71.15%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 71.41%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 71.19%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   42 | acc: 6.25%,  total acc: 70.35%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 68.75%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 67.22%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 65.76%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 64.76%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 64.45%   [EVAL] batch:   48 | acc: 6.25%,  total acc: 63.27%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 62.12%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 60.91%   [EVAL] batch:   51 | acc: 12.50%,  total acc: 59.98%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 58.84%   [EVAL] batch:   53 | acc: 18.75%,  total acc: 58.10%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 58.41%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 58.48%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 58.88%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 58.94%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 59.00%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 58.85%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 58.71%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 59.17%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 59.72%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 60.25%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 60.67%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 61.08%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 61.29%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 61.67%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 61.59%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 61.79%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 62.15%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 63.01%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 63.51%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 64.00%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 64.47%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 64.94%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 65.06%   
cur_acc:  ['0.8636', '0.6000', '0.7768', '0.8576']
his_acc:  ['0.8636', '0.8005', '0.6957', '0.6506']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 8.2292976CurrentTrain: epoch  0, batch     1 | loss: 7.1020756CurrentTrain: epoch  1, batch     0 | loss: 6.7194281CurrentTrain: epoch  1, batch     1 | loss: 7.2245975CurrentTrain: epoch  2, batch     0 | loss: 6.2746897CurrentTrain: epoch  2, batch     1 | loss: 5.8159041CurrentTrain: epoch  3, batch     0 | loss: 5.0911951CurrentTrain: epoch  3, batch     1 | loss: 5.8122349CurrentTrain: epoch  4, batch     0 | loss: 4.3061290CurrentTrain: epoch  4, batch     1 | loss: 5.8303852CurrentTrain: epoch  5, batch     0 | loss: 4.7620592CurrentTrain: epoch  5, batch     1 | loss: 3.7724526CurrentTrain: epoch  6, batch     0 | loss: 4.4431925CurrentTrain: epoch  6, batch     1 | loss: 3.1726711CurrentTrain: epoch  7, batch     0 | loss: 3.7631454CurrentTrain: epoch  7, batch     1 | loss: 3.4596841CurrentTrain: epoch  8, batch     0 | loss: 3.5077331CurrentTrain: epoch  8, batch     1 | loss: 3.0384250CurrentTrain: epoch  9, batch     0 | loss: 3.5780830CurrentTrain: epoch  9, batch     1 | loss: 2.8014095
Mixup data size:  90
MixupTrain:  epoch  0, batch     1 | loss: 4.2718302MixupTrain:  epoch  0, batch     4 | loss: 4.0176530MixupTrain:  epoch  0, batch     5 | loss: 3.8425185
MemoryTrain:  epoch  0, batch     0 | loss: 0.5765443MemoryTrain:  epoch  0, batch     1 | loss: 0.1247565MemoryTrain:  epoch  1, batch     0 | loss: 0.7473263MemoryTrain:  epoch  1, batch     1 | loss: 0.8948389MemoryTrain:  epoch  2, batch     0 | loss: 0.3670669MemoryTrain:  epoch  2, batch     1 | loss: 0.9674179MemoryTrain:  epoch  3, batch     0 | loss: 0.2542969MemoryTrain:  epoch  3, batch     1 | loss: 0.1528083MemoryTrain:  epoch  4, batch     0 | loss: 0.0896952MemoryTrain:  epoch  4, batch     1 | loss: 0.2174645MemoryTrain:  epoch  5, batch     0 | loss: 0.2512749MemoryTrain:  epoch  5, batch     1 | loss: 0.0340869MemoryTrain:  epoch  6, batch     0 | loss: 0.1627013MemoryTrain:  epoch  6, batch     1 | loss: 0.1024821MemoryTrain:  epoch  7, batch     0 | loss: 0.0445073MemoryTrain:  epoch  7, batch     1 | loss: 0.0214836MemoryTrain:  epoch  8, batch     0 | loss: 0.0230362MemoryTrain:  epoch  8, batch     1 | loss: 0.0457472MemoryTrain:  epoch  9, batch     0 | loss: 0.0186347MemoryTrain:  epoch  9, batch     1 | loss: 0.0286504
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 10.3592215CurrentTrain: epoch  0, batch     1 | loss: 9.3039045CurrentTrain: epoch  1, batch     0 | loss: 8.2983961CurrentTrain: epoch  1, batch     1 | loss: 9.2764769CurrentTrain: epoch  2, batch     0 | loss: 7.4517870CurrentTrain: epoch  2, batch     1 | loss: 8.8267851CurrentTrain: epoch  3, batch     0 | loss: 7.0307045CurrentTrain: epoch  3, batch     1 | loss: 7.8213425CurrentTrain: epoch  4, batch     0 | loss: 6.8700519CurrentTrain: epoch  4, batch     1 | loss: 5.9811416CurrentTrain: epoch  5, batch     0 | loss: 6.6351624CurrentTrain: epoch  5, batch     1 | loss: 5.9532180CurrentTrain: epoch  6, batch     0 | loss: 5.8555007CurrentTrain: epoch  6, batch     1 | loss: 5.1928420CurrentTrain: epoch  7, batch     0 | loss: 5.8604169CurrentTrain: epoch  7, batch     1 | loss: 4.7997952CurrentTrain: epoch  8, batch     0 | loss: 5.1871433CurrentTrain: epoch  8, batch     1 | loss: 5.3209143CurrentTrain: epoch  9, batch     0 | loss: 5.3831425CurrentTrain: epoch  9, batch     1 | loss: 3.6957958
Mixup data size:  91
MixupTrain:  epoch  0, batch     0 | loss: 4.9053751MixupTrain:  epoch  0, batch     1 | loss: 4.2813270MixupTrain:  epoch  0, batch     2 | loss: 4.7665088MixupTrain:  epoch  0, batch     3 | loss: 4.1271497MixupTrain:  epoch  0, batch     5 | loss: 6.0028588
MemoryTrain:  epoch  0, batch     0 | loss: 1.0566248MemoryTrain:  epoch  0, batch     1 | loss: 0.9246373MemoryTrain:  epoch  1, batch     0 | loss: 0.9152460MemoryTrain:  epoch  1, batch     1 | loss: 1.3979427MemoryTrain:  epoch  2, batch     0 | loss: 1.0067551MemoryTrain:  epoch  2, batch     1 | loss: 0.8143021MemoryTrain:  epoch  3, batch     0 | loss: 1.0431112MemoryTrain:  epoch  3, batch     1 | loss: 0.7047750MemoryTrain:  epoch  4, batch     0 | loss: 0.7728029MemoryTrain:  epoch  4, batch     1 | loss: 0.7963046MemoryTrain:  epoch  5, batch     0 | loss: 0.7193074MemoryTrain:  epoch  5, batch     1 | loss: 0.4900658MemoryTrain:  epoch  6, batch     0 | loss: 0.5801978MemoryTrain:  epoch  6, batch     1 | loss: 0.7946875MemoryTrain:  epoch  7, batch     0 | loss: 0.5700301MemoryTrain:  epoch  7, batch     1 | loss: 0.5165710MemoryTrain:  epoch  8, batch     0 | loss: 0.6460065MemoryTrain:  epoch  8, batch     1 | loss: 0.4342423MemoryTrain:  epoch  9, batch     0 | loss: 0.5816512MemoryTrain:  epoch  9, batch     1 | loss: 0.2953558
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 48.44%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 52.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 58.33%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 59.82%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 57.81%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:    9 | acc: 25.00%,  total acc: 53.12%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 52.84%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 52.60%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 51.92%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 55.36%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 58.33%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 60.55%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 62.87%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 64.93%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 66.12%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 67.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 68.18%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 60.94%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 63.75%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 63.54%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 66.07%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 75.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 77.84%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 77.40%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 73.21%   [EVAL] batch:   14 | acc: 0.00%,  total acc: 68.33%   [EVAL] batch:   15 | acc: 0.00%,  total acc: 64.06%   [EVAL] batch:   16 | acc: 0.00%,  total acc: 60.29%   [EVAL] batch:   17 | acc: 0.00%,  total acc: 56.94%   [EVAL] batch:   18 | acc: 0.00%,  total acc: 53.95%   [EVAL] batch:   19 | acc: 18.75%,  total acc: 52.19%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 54.17%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 58.15%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 59.90%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 61.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 62.98%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 64.12%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 65.40%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 66.59%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 67.29%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 68.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 68.56%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 66.91%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 66.25%   [EVAL] batch:   35 | acc: 31.25%,  total acc: 65.28%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 64.02%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 64.14%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 64.58%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 65.16%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 64.94%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 65.77%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 64.53%   [EVAL] batch:   43 | acc: 12.50%,  total acc: 63.35%   [EVAL] batch:   44 | acc: 6.25%,  total acc: 62.08%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 61.01%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 60.24%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 60.68%   [EVAL] batch:   48 | acc: 0.00%,  total acc: 59.44%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 58.25%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 57.23%   [EVAL] batch:   51 | acc: 0.00%,  total acc: 56.13%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 55.19%   [EVAL] batch:   53 | acc: 18.75%,  total acc: 54.51%   [EVAL] batch:   54 | acc: 18.75%,  total acc: 53.86%   [EVAL] batch:   55 | acc: 37.50%,  total acc: 53.57%   [EVAL] batch:   56 | acc: 6.25%,  total acc: 52.74%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 52.26%   [EVAL] batch:   58 | acc: 25.00%,  total acc: 51.80%   [EVAL] batch:   59 | acc: 25.00%,  total acc: 51.35%   [EVAL] batch:   60 | acc: 18.75%,  total acc: 50.82%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 50.30%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 49.80%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 49.51%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 49.13%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 48.96%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 48.60%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 49.36%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 49.55%   [EVAL] batch:   69 | acc: 43.75%,  total acc: 49.46%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 49.56%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 49.74%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 50.43%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 51.10%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 51.75%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 52.38%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 53.00%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 53.29%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 53.01%   [EVAL] batch:   79 | acc: 37.50%,  total acc: 52.81%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 52.85%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 53.12%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 53.46%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 53.87%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 53.90%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 53.85%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 53.66%   [EVAL] batch:   87 | acc: 37.50%,  total acc: 53.48%   [EVAL] batch:   88 | acc: 43.75%,  total acc: 53.37%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 53.33%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 53.37%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 53.87%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 54.30%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 54.79%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 55.26%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 55.73%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 56.06%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 56.51%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 56.94%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 56.56%   
cur_acc_llm:  [0.875, 0.7208333333333333, 0.8883928571428571, 0.8923611111111112, 0.6818181818181818]
his_acc_llm:  [0.875, 0.8311170212765957, 0.6926229508196722, 0.6193910256410257, 0.565625]
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 43.75%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 35.00%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 29.17%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 27.68%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 31.25%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 33.33%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 35.62%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 38.64%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 39.58%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 41.35%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 45.54%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 48.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 51.95%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 53.68%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 55.56%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 55.26%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 55.00%   [EVAL] batch:   20 | acc: 50.00%,  total acc: 54.76%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 53.69%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 46.88%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 47.50%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 54.46%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 60.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 67.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 70.45%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 71.63%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 70.54%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 69.92%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 70.22%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 69.79%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 70.07%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 70.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 72.32%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 73.58%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 74.73%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 76.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.64%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 78.24%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.02%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 79.31%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 79.58%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 79.84%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 79.36%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 77.02%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 75.00%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 73.09%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 71.28%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 70.56%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 70.03%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 70.31%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 70.12%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 69.19%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 67.61%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 66.11%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 64.67%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 63.70%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 63.41%   [EVAL] batch:   48 | acc: 0.00%,  total acc: 62.12%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 60.88%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 59.68%   [EVAL] batch:   51 | acc: 0.00%,  total acc: 58.53%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 57.43%   [EVAL] batch:   53 | acc: 31.25%,  total acc: 56.94%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 57.50%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 57.92%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 58.33%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 58.41%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 58.58%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 58.33%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 58.20%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 58.57%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 59.13%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 59.47%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 59.90%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 60.32%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 60.45%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 59.83%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 59.33%   [EVAL] batch:   69 | acc: 56.25%,  total acc: 59.29%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 59.24%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 59.46%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 60.02%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 60.56%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 61.08%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 61.60%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 62.09%   [EVAL] batch:   77 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   80 | acc: 31.25%,  total acc: 62.11%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 61.36%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 60.62%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 59.90%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 59.56%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 59.59%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 59.48%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 59.48%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 59.44%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 59.55%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 59.99%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 60.35%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 60.70%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 60.92%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 61.20%   [EVAL] batch:   96 | acc: 43.75%,  total acc: 61.02%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 60.78%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 60.86%   [EVAL] batch:   99 | acc: 12.50%,  total acc: 60.38%   
cur_acc:  ['0.8636', '0.6000', '0.7768', '0.8576', '0.5369']
his_acc:  ['0.8636', '0.8005', '0.6957', '0.6506', '0.6038']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 6.6003761CurrentTrain: epoch  0, batch     1 | loss: 5.9411230CurrentTrain: epoch  1, batch     0 | loss: 4.3136044CurrentTrain: epoch  1, batch     1 | loss: 4.9447579CurrentTrain: epoch  2, batch     0 | loss: 3.9122677CurrentTrain: epoch  2, batch     1 | loss: 3.9774868CurrentTrain: epoch  3, batch     0 | loss: 3.4376798CurrentTrain: epoch  3, batch     1 | loss: 3.6154454CurrentTrain: epoch  4, batch     0 | loss: 2.7171097CurrentTrain: epoch  4, batch     1 | loss: 3.4694710CurrentTrain: epoch  5, batch     0 | loss: 3.0343318CurrentTrain: epoch  5, batch     1 | loss: 2.2710867CurrentTrain: epoch  6, batch     0 | loss: 2.9824386CurrentTrain: epoch  6, batch     1 | loss: 2.2956393CurrentTrain: epoch  7, batch     0 | loss: 2.3255391CurrentTrain: epoch  7, batch     1 | loss: 3.2164857CurrentTrain: epoch  8, batch     0 | loss: 2.4684143CurrentTrain: epoch  8, batch     1 | loss: 2.6703513CurrentTrain: epoch  9, batch     0 | loss: 2.2588515CurrentTrain: epoch  9, batch     1 | loss: 2.7197552
Mixup data size:  101

MemoryTrain:  epoch  0, batch     0 | loss: 1.2112277MemoryTrain:  epoch  0, batch     1 | loss: 0.4960918MemoryTrain:  epoch  1, batch     0 | loss: 1.3435415MemoryTrain:  epoch  1, batch     1 | loss: 1.0077715MemoryTrain:  epoch  2, batch     0 | loss: 0.7652617MemoryTrain:  epoch  2, batch     1 | loss: 1.2944494MemoryTrain:  epoch  3, batch     0 | loss: 1.0108902MemoryTrain:  epoch  3, batch     1 | loss: 0.3362748MemoryTrain:  epoch  4, batch     0 | loss: 0.4855391MemoryTrain:  epoch  4, batch     1 | loss: 0.1373632MemoryTrain:  epoch  5, batch     0 | loss: 0.1003410MemoryTrain:  epoch  5, batch     1 | loss: 0.3954986MemoryTrain:  epoch  6, batch     0 | loss: 0.0580031MemoryTrain:  epoch  6, batch     1 | loss: 0.2547524MemoryTrain:  epoch  7, batch     0 | loss: 0.0611163MemoryTrain:  epoch  7, batch     1 | loss: 0.0947460MemoryTrain:  epoch  8, batch     0 | loss: 0.0997233MemoryTrain:  epoch  8, batch     1 | loss: 0.0340094MemoryTrain:  epoch  9, batch     0 | loss: 0.0519711MemoryTrain:  epoch  9, batch     1 | loss: 0.0640667
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 8.1029510CurrentTrain: epoch  0, batch     1 | loss: 6.9722672CurrentTrain: epoch  1, batch     0 | loss: 6.7922516CurrentTrain: epoch  1, batch     1 | loss: 5.7362390CurrentTrain: epoch  2, batch     0 | loss: 6.2301755CurrentTrain: epoch  2, batch     1 | loss: 5.6061091CurrentTrain: epoch  3, batch     0 | loss: 5.9416146CurrentTrain: epoch  3, batch     1 | loss: 4.0446410CurrentTrain: epoch  4, batch     0 | loss: 5.2187004CurrentTrain: epoch  4, batch     1 | loss: 4.1624990CurrentTrain: epoch  5, batch     0 | loss: 4.2200561CurrentTrain: epoch  5, batch     1 | loss: 4.1215363CurrentTrain: epoch  6, batch     0 | loss: 4.3034048CurrentTrain: epoch  6, batch     1 | loss: 3.2670615CurrentTrain: epoch  7, batch     0 | loss: 3.4856389CurrentTrain: epoch  7, batch     1 | loss: 3.5825212CurrentTrain: epoch  8, batch     0 | loss: 3.5100470CurrentTrain: epoch  8, batch     1 | loss: 3.6986589CurrentTrain: epoch  9, batch     0 | loss: 3.9683812CurrentTrain: epoch  9, batch     1 | loss: 2.4447403
Mixup data size:  101
MixupTrain:  epoch  0, batch     0 | loss: 3.5888703MixupTrain:  epoch  0, batch     2 | loss: 4.2827665MixupTrain:  epoch  0, batch     5 | loss: 4.0332288MixupTrain:  epoch  0, batch     6 | loss: 2.3377537
MemoryTrain:  epoch  0, batch     0 | loss: 0.9507582MemoryTrain:  epoch  0, batch     1 | loss: 1.2926040MemoryTrain:  epoch  1, batch     0 | loss: 1.3079485MemoryTrain:  epoch  1, batch     1 | loss: 1.5584078MemoryTrain:  epoch  2, batch     0 | loss: 0.9062207MemoryTrain:  epoch  2, batch     1 | loss: 1.6758028MemoryTrain:  epoch  3, batch     0 | loss: 1.4852438MemoryTrain:  epoch  3, batch     1 | loss: 0.6575332MemoryTrain:  epoch  4, batch     0 | loss: 1.1204190MemoryTrain:  epoch  4, batch     1 | loss: 0.6476201MemoryTrain:  epoch  5, batch     0 | loss: 0.6476114MemoryTrain:  epoch  5, batch     1 | loss: 0.7983689MemoryTrain:  epoch  6, batch     0 | loss: 0.6145958MemoryTrain:  epoch  6, batch     1 | loss: 0.5492511MemoryTrain:  epoch  7, batch     0 | loss: 0.7811052MemoryTrain:  epoch  7, batch     1 | loss: 0.5047520MemoryTrain:  epoch  8, batch     0 | loss: 0.6043890MemoryTrain:  epoch  8, batch     1 | loss: 0.5328598MemoryTrain:  epoch  9, batch     0 | loss: 0.4877705MemoryTrain:  epoch  9, batch     1 | loss: 0.3562629
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 53.75%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 45.83%   [EVAL] batch:    6 | acc: 12.50%,  total acc: 41.07%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 35.94%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 3.12%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 2.50%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 2.08%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 11.61%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 22.66%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 29.86%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 36.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 42.05%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 45.31%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 45.67%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 43.75%   [EVAL] batch:   14 | acc: 0.00%,  total acc: 40.83%   [EVAL] batch:   15 | acc: 0.00%,  total acc: 38.28%   [EVAL] batch:   16 | acc: 0.00%,  total acc: 36.03%   [EVAL] batch:   17 | acc: 0.00%,  total acc: 34.03%   [EVAL] batch:   18 | acc: 0.00%,  total acc: 32.24%   [EVAL] batch:   19 | acc: 18.75%,  total acc: 31.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 34.82%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 37.78%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 40.22%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 42.71%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 45.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 47.12%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 48.84%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 50.67%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 52.37%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 53.54%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 54.84%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 56.05%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 55.68%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 54.23%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 52.68%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 51.22%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 50.00%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 49.84%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 49.84%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 50.47%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 50.46%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 51.64%   [EVAL] batch:   42 | acc: 25.00%,  total acc: 51.02%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 49.86%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 48.75%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 47.69%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 47.07%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 47.27%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 46.56%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 45.75%   [EVAL] batch:   50 | acc: 12.50%,  total acc: 45.10%   [EVAL] batch:   51 | acc: 12.50%,  total acc: 44.47%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 43.75%   [EVAL] batch:   53 | acc: 31.25%,  total acc: 43.52%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 43.64%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 43.64%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 43.42%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 43.32%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 43.22%   [EVAL] batch:   59 | acc: 31.25%,  total acc: 43.02%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 43.03%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 42.84%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 43.15%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 43.55%   [EVAL] batch:   64 | acc: 56.25%,  total acc: 43.75%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 44.13%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 44.22%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 45.04%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 45.20%   [EVAL] batch:   69 | acc: 25.00%,  total acc: 44.91%   [EVAL] batch:   70 | acc: 37.50%,  total acc: 44.81%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 45.05%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 45.80%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 46.54%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 47.25%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 47.94%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 48.62%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 48.96%   [EVAL] batch:   78 | acc: 25.00%,  total acc: 48.66%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 48.67%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 48.30%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 47.94%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 47.74%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 47.54%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 47.35%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 47.46%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 47.34%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 47.37%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 47.40%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 47.43%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 47.53%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 48.10%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 48.59%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 49.07%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 49.61%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 50.07%   [EVAL] batch:   96 | acc: 75.00%,  total acc: 50.32%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 50.83%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 51.33%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 51.62%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 52.10%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 52.27%   [EVAL] batch:  102 | acc: 31.25%,  total acc: 52.06%   [EVAL] batch:  103 | acc: 0.00%,  total acc: 51.56%   [EVAL] batch:  104 | acc: 12.50%,  total acc: 51.19%   [EVAL] batch:  105 | acc: 12.50%,  total acc: 50.83%   [EVAL] batch:  106 | acc: 0.00%,  total acc: 50.35%   
cur_acc_llm:  [0.875, 0.7208333333333333, 0.8883928571428571, 0.8923611111111112, 0.6818181818181818, 0.359375]
his_acc_llm:  [0.875, 0.8311170212765957, 0.6926229508196722, 0.6193910256410257, 0.565625, 0.5035046728971962]
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 75.78%   
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 16.67%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 14.06%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 16.25%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 16.67%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 17.86%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 22.66%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 27.78%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 29.38%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 30.11%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 31.77%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 31.73%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 32.14%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 35.00%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 36.72%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 38.60%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 40.28%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 42.11%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 44.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 46.73%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 49.15%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 51.36%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 53.39%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 55.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 56.97%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 58.33%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 59.82%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 60.78%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 61.25%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 62.10%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 63.09%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 62.31%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 60.48%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 58.75%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 57.47%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 56.08%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 55.76%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 55.61%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 56.25%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 56.40%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 57.44%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 56.10%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 54.83%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 53.61%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 52.45%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 51.73%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 51.69%   [EVAL] batch:   48 | acc: 0.00%,  total acc: 50.64%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 49.62%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 48.65%   [EVAL] batch:   51 | acc: 0.00%,  total acc: 47.72%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 46.82%   [EVAL] batch:   53 | acc: 31.25%,  total acc: 46.53%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 47.05%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 47.32%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 47.81%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 47.95%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 48.31%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 48.12%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 48.05%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 48.59%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 49.11%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 49.61%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 49.90%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 50.38%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 50.65%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 50.37%   [EVAL] batch:   68 | acc: 6.25%,  total acc: 49.73%   [EVAL] batch:   69 | acc: 37.50%,  total acc: 49.55%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 49.56%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 49.83%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 50.51%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 51.18%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 51.83%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 52.47%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 53.08%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 53.37%   [EVAL] batch:   78 | acc: 12.50%,  total acc: 52.85%   [EVAL] batch:   79 | acc: 12.50%,  total acc: 52.34%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 51.77%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 51.14%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 50.60%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 50.00%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 49.78%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 49.78%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 49.64%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 49.79%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 50.00%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 50.07%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 50.27%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 50.82%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 51.28%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 51.80%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 52.17%   [EVAL] batch:   95 | acc: 75.00%,  total acc: 52.41%   [EVAL] batch:   96 | acc: 25.00%,  total acc: 52.13%   [EVAL] batch:   97 | acc: 18.75%,  total acc: 51.79%   [EVAL] batch:   98 | acc: 43.75%,  total acc: 51.70%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 52.00%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 52.48%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 52.63%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 52.97%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 53.06%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 53.39%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 53.83%   [EVAL] batch:  106 | acc: 25.00%,  total acc: 53.56%   
cur_acc:  ['0.8636', '0.6000', '0.7768', '0.8576', '0.5369', '0.7578']
his_acc:  ['0.8636', '0.8005', '0.6957', '0.6506', '0.6038', '0.5356']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 5.4401169CurrentTrain: epoch  0, batch     1 | loss: 5.1688457CurrentTrain: epoch  1, batch     0 | loss: 3.1120739CurrentTrain: epoch  1, batch     1 | loss: 3.2069125CurrentTrain: epoch  2, batch     0 | loss: 2.4150515CurrentTrain: epoch  2, batch     1 | loss: 2.2029822CurrentTrain: epoch  3, batch     0 | loss: 2.1121745CurrentTrain: epoch  3, batch     1 | loss: 2.2870569CurrentTrain: epoch  4, batch     0 | loss: 2.0919604CurrentTrain: epoch  4, batch     1 | loss: 1.9879460CurrentTrain: epoch  5, batch     0 | loss: 1.9917825CurrentTrain: epoch  5, batch     1 | loss: 1.8482862CurrentTrain: epoch  6, batch     0 | loss: 1.8054895CurrentTrain: epoch  6, batch     1 | loss: 1.7628026CurrentTrain: epoch  7, batch     0 | loss: 1.7678764CurrentTrain: epoch  7, batch     1 | loss: 1.7300339CurrentTrain: epoch  8, batch     0 | loss: 1.7762136CurrentTrain: epoch  8, batch     1 | loss: 1.7424853CurrentTrain: epoch  9, batch     0 | loss: 1.7324129CurrentTrain: epoch  9, batch     1 | loss: 1.7868758
Mixup data size:  110
MixupTrain:  epoch  0, batch     0 | loss: 3.4136709
MemoryTrain:  epoch  0, batch     0 | loss: 1.6499064MemoryTrain:  epoch  0, batch     1 | loss: 0.8221717MemoryTrain:  epoch  0, batch     2 | loss: 1.7521067MemoryTrain:  epoch  1, batch     0 | loss: 1.6402808MemoryTrain:  epoch  1, batch     1 | loss: 1.1788477MemoryTrain:  epoch  1, batch     2 | loss: 1.1203979MemoryTrain:  epoch  2, batch     0 | loss: 0.5241403MemoryTrain:  epoch  2, batch     1 | loss: 0.6457409MemoryTrain:  epoch  2, batch     2 | loss: 0.4675202MemoryTrain:  epoch  3, batch     0 | loss: 0.3615643MemoryTrain:  epoch  3, batch     1 | loss: 0.4818395MemoryTrain:  epoch  3, batch     2 | loss: 0.1004756MemoryTrain:  epoch  4, batch     0 | loss: 0.3328598MemoryTrain:  epoch  4, batch     1 | loss: 0.0633378MemoryTrain:  epoch  4, batch     2 | loss: 0.1251179MemoryTrain:  epoch  5, batch     0 | loss: 0.1312572MemoryTrain:  epoch  5, batch     1 | loss: 0.0716094MemoryTrain:  epoch  5, batch     2 | loss: 0.0176277MemoryTrain:  epoch  6, batch     0 | loss: 0.0963718MemoryTrain:  epoch  6, batch     1 | loss: 0.0965404MemoryTrain:  epoch  6, batch     2 | loss: 0.0301914MemoryTrain:  epoch  7, batch     0 | loss: 0.0785623MemoryTrain:  epoch  7, batch     1 | loss: 0.0323493MemoryTrain:  epoch  7, batch     2 | loss: 0.0826136MemoryTrain:  epoch  8, batch     0 | loss: 0.0607577MemoryTrain:  epoch  8, batch     1 | loss: 0.0971443MemoryTrain:  epoch  8, batch     2 | loss: 0.0409206MemoryTrain:  epoch  9, batch     0 | loss: 0.1124566MemoryTrain:  epoch  9, batch     1 | loss: 0.1338261MemoryTrain:  epoch  9, batch     2 | loss: 0.0305100
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 5.9672570CurrentTrain: epoch  0, batch     1 | loss: 6.4978275CurrentTrain: epoch  1, batch     0 | loss: 4.8676877CurrentTrain: epoch  1, batch     1 | loss: 4.2904444CurrentTrain: epoch  2, batch     0 | loss: 4.1711159CurrentTrain: epoch  2, batch     1 | loss: 3.0009627CurrentTrain: epoch  3, batch     0 | loss: 3.4705632CurrentTrain: epoch  3, batch     1 | loss: 3.1782804CurrentTrain: epoch  4, batch     0 | loss: 3.3144317CurrentTrain: epoch  4, batch     1 | loss: 2.7462120CurrentTrain: epoch  5, batch     0 | loss: 2.8030393CurrentTrain: epoch  5, batch     1 | loss: 2.8898294CurrentTrain: epoch  6, batch     0 | loss: 2.5730135CurrentTrain: epoch  6, batch     1 | loss: 2.3646879CurrentTrain: epoch  7, batch     0 | loss: 2.3476317CurrentTrain: epoch  7, batch     1 | loss: 2.2801614CurrentTrain: epoch  8, batch     0 | loss: 2.2412553CurrentTrain: epoch  8, batch     1 | loss: 2.2326055CurrentTrain: epoch  9, batch     0 | loss: 2.3097055CurrentTrain: epoch  9, batch     1 | loss: 2.0496056
Mixup data size:  111

MemoryTrain:  epoch  0, batch     0 | loss: 1.4968731MemoryTrain:  epoch  0, batch     1 | loss: 0.8996394MemoryTrain:  epoch  0, batch     2 | loss: 0.7331143MemoryTrain:  epoch  1, batch     0 | loss: 2.0944219MemoryTrain:  epoch  1, batch     1 | loss: 1.1452336MemoryTrain:  epoch  1, batch     2 | loss: 0.3092720MemoryTrain:  epoch  2, batch     0 | loss: 1.3703897MemoryTrain:  epoch  2, batch     1 | loss: 0.9001000MemoryTrain:  epoch  2, batch     2 | loss: 0.9723278MemoryTrain:  epoch  3, batch     0 | loss: 1.1111296MemoryTrain:  epoch  3, batch     1 | loss: 0.8617691MemoryTrain:  epoch  3, batch     2 | loss: 0.3581395MemoryTrain:  epoch  4, batch     0 | loss: 0.8755862MemoryTrain:  epoch  4, batch     1 | loss: 0.6088997MemoryTrain:  epoch  4, batch     2 | loss: 0.5401809MemoryTrain:  epoch  5, batch     0 | loss: 0.6686410MemoryTrain:  epoch  5, batch     1 | loss: 0.4787743MemoryTrain:  epoch  5, batch     2 | loss: 0.4961320MemoryTrain:  epoch  6, batch     0 | loss: 0.6739691MemoryTrain:  epoch  6, batch     1 | loss: 0.4643967MemoryTrain:  epoch  6, batch     2 | loss: 0.2667207MemoryTrain:  epoch  7, batch     0 | loss: 0.6550501MemoryTrain:  epoch  7, batch     1 | loss: 0.5192223MemoryTrain:  epoch  7, batch     2 | loss: 0.2979546MemoryTrain:  epoch  8, batch     0 | loss: 0.8501780MemoryTrain:  epoch  8, batch     1 | loss: 0.5422099MemoryTrain:  epoch  8, batch     2 | loss: 0.1596576MemoryTrain:  epoch  9, batch     0 | loss: 0.6367018MemoryTrain:  epoch  9, batch     1 | loss: 0.3887135MemoryTrain:  epoch  9, batch     2 | loss: 0.2938595
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 31.25%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 75.00%   [EVAL] batch:    9 | acc: 12.50%,  total acc: 68.75%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 66.48%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 67.31%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 1.56%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 1.25%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 1.04%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 10.71%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 21.88%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 29.17%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 34.38%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 39.77%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 44.79%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 45.67%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 43.75%   [EVAL] batch:   14 | acc: 0.00%,  total acc: 40.83%   [EVAL] batch:   15 | acc: 0.00%,  total acc: 38.28%   [EVAL] batch:   16 | acc: 0.00%,  total acc: 36.03%   [EVAL] batch:   17 | acc: 0.00%,  total acc: 34.03%   [EVAL] batch:   18 | acc: 0.00%,  total acc: 32.24%   [EVAL] batch:   19 | acc: 18.75%,  total acc: 31.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 34.82%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 37.78%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 40.22%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 42.71%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 45.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 47.12%   [EVAL] batch:   26 | acc: 62.50%,  total acc: 47.69%   [EVAL] batch:   27 | acc: 75.00%,  total acc: 48.66%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 50.00%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 50.83%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 51.61%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 51.95%   [EVAL] batch:   32 | acc: 25.00%,  total acc: 51.14%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 49.82%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 48.57%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 47.22%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 46.28%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 46.22%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 45.99%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 46.72%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 46.65%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 47.92%   [EVAL] batch:   42 | acc: 18.75%,  total acc: 47.24%   [EVAL] batch:   43 | acc: 6.25%,  total acc: 46.31%   [EVAL] batch:   44 | acc: 12.50%,  total acc: 45.56%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 44.97%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 44.81%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 44.79%   [EVAL] batch:   48 | acc: 6.25%,  total acc: 44.01%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 43.25%   [EVAL] batch:   50 | acc: 12.50%,  total acc: 42.65%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 41.95%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 41.27%   [EVAL] batch:   53 | acc: 18.75%,  total acc: 40.86%   [EVAL] batch:   54 | acc: 12.50%,  total acc: 40.34%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 40.40%   [EVAL] batch:   56 | acc: 12.50%,  total acc: 39.91%   [EVAL] batch:   57 | acc: 6.25%,  total acc: 39.33%   [EVAL] batch:   58 | acc: 12.50%,  total acc: 38.88%   [EVAL] batch:   59 | acc: 6.25%,  total acc: 38.33%   [EVAL] batch:   60 | acc: 6.25%,  total acc: 37.81%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 37.20%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 36.61%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 36.04%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 35.58%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 35.04%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 34.89%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 35.85%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 36.23%   [EVAL] batch:   69 | acc: 25.00%,  total acc: 36.07%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 36.36%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 36.72%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 37.41%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 38.26%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 39.00%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 39.64%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 40.34%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 40.71%   [EVAL] batch:   78 | acc: 12.50%,  total acc: 40.35%   [EVAL] batch:   79 | acc: 25.00%,  total acc: 40.16%   [EVAL] batch:   80 | acc: 37.50%,  total acc: 40.12%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 40.32%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 40.59%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 41.07%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 41.25%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 41.64%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 41.67%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 41.97%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 42.21%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 42.43%   [EVAL] batch:   90 | acc: 37.50%,  total acc: 42.38%   [EVAL] batch:   91 | acc: 18.75%,  total acc: 42.12%   [EVAL] batch:   92 | acc: 37.50%,  total acc: 42.07%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 41.95%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 41.91%   [EVAL] batch:   95 | acc: 18.75%,  total acc: 41.67%   [EVAL] batch:   96 | acc: 18.75%,  total acc: 41.43%   [EVAL] batch:   97 | acc: 12.50%,  total acc: 41.14%   [EVAL] batch:   98 | acc: 31.25%,  total acc: 41.04%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 41.38%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 41.96%   [EVAL] batch:  101 | acc: 31.25%,  total acc: 41.85%   [EVAL] batch:  102 | acc: 31.25%,  total acc: 41.75%   [EVAL] batch:  103 | acc: 0.00%,  total acc: 41.35%   [EVAL] batch:  104 | acc: 18.75%,  total acc: 41.13%   [EVAL] batch:  105 | acc: 12.50%,  total acc: 40.86%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 40.83%   [EVAL] batch:  107 | acc: 81.25%,  total acc: 41.20%   [EVAL] batch:  108 | acc: 81.25%,  total acc: 41.57%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 41.99%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 42.51%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 42.97%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 43.47%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 43.64%   [EVAL] batch:  114 | acc: 31.25%,  total acc: 43.53%   [EVAL] batch:  115 | acc: 12.50%,  total acc: 43.27%   [EVAL] batch:  116 | acc: 25.00%,  total acc: 43.11%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 43.43%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 43.75%   
cur_acc_llm:  [0.875, 0.7208333333333333, 0.8883928571428571, 0.8923611111111112, 0.6818181818181818, 0.359375, 0.6730769230769231]
his_acc_llm:  [0.875, 0.8311170212765957, 0.6926229508196722, 0.6193910256410257, 0.565625, 0.5035046728971962, 0.4375]
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 76.14%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 74.04%   
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 29.69%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 29.17%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 29.46%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 30.47%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 34.72%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 35.00%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 34.66%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 35.94%   [EVAL] batch:   12 | acc: 25.00%,  total acc: 35.10%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 35.27%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 37.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 39.06%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 40.81%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 42.01%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 43.42%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 45.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 48.21%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 50.57%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 52.72%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 54.69%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 56.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 58.17%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 58.80%   [EVAL] batch:   27 | acc: 81.25%,  total acc: 59.60%   [EVAL] batch:   28 | acc: 81.25%,  total acc: 60.34%   [EVAL] batch:   29 | acc: 62.50%,  total acc: 60.42%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 60.69%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 61.13%   [EVAL] batch:   32 | acc: 31.25%,  total acc: 60.23%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 58.46%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 56.79%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 55.38%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 53.89%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 53.62%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 53.69%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 54.37%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 54.57%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 55.65%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 54.36%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 53.12%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 51.94%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 50.82%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 50.13%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 49.87%   [EVAL] batch:   48 | acc: 0.00%,  total acc: 48.85%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 47.88%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 46.94%   [EVAL] batch:   51 | acc: 0.00%,  total acc: 46.03%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 45.17%   [EVAL] batch:   53 | acc: 31.25%,  total acc: 44.91%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 45.68%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 46.32%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 46.82%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 47.09%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 47.46%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 47.50%   [EVAL] batch:   60 | acc: 6.25%,  total acc: 46.82%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 46.07%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 45.73%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 45.21%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 44.81%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 44.32%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 43.75%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 43.38%   [EVAL] batch:   68 | acc: 6.25%,  total acc: 42.84%   [EVAL] batch:   69 | acc: 18.75%,  total acc: 42.50%   [EVAL] batch:   70 | acc: 37.50%,  total acc: 42.43%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 42.80%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 43.58%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 44.34%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 45.08%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 45.72%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 46.43%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 46.79%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 46.68%   [EVAL] batch:   79 | acc: 18.75%,  total acc: 46.33%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 45.83%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 45.27%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 44.80%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 44.27%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 44.12%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 44.26%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 44.25%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 44.32%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 44.59%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 44.72%   [EVAL] batch:   90 | acc: 50.00%,  total acc: 44.78%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 44.84%   [EVAL] batch:   92 | acc: 37.50%,  total acc: 44.76%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 44.68%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 44.61%   [EVAL] batch:   95 | acc: 25.00%,  total acc: 44.40%   [EVAL] batch:   96 | acc: 0.00%,  total acc: 43.94%   [EVAL] batch:   97 | acc: 6.25%,  total acc: 43.56%   [EVAL] batch:   98 | acc: 0.00%,  total acc: 43.12%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 43.31%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 43.87%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 44.18%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 44.60%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 44.77%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 45.18%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 45.64%   [EVAL] batch:  106 | acc: 81.25%,  total acc: 45.97%   [EVAL] batch:  107 | acc: 50.00%,  total acc: 46.01%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 46.27%   [EVAL] batch:  109 | acc: 75.00%,  total acc: 46.53%   [EVAL] batch:  110 | acc: 75.00%,  total acc: 46.79%   [EVAL] batch:  111 | acc: 68.75%,  total acc: 46.99%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 47.35%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 47.64%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 47.93%   [EVAL] batch:  115 | acc: 62.50%,  total acc: 48.06%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 48.45%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 48.78%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 48.95%   
cur_acc:  ['0.8636', '0.6000', '0.7768', '0.8576', '0.5369', '0.7578', '0.7404']
his_acc:  ['0.8636', '0.8005', '0.6957', '0.6506', '0.6038', '0.5356', '0.4895']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 4.3300462CurrentTrain: epoch  0, batch     1 | loss: 3.5809445CurrentTrain: epoch  1, batch     0 | loss: 2.6830547CurrentTrain: epoch  1, batch     1 | loss: 2.8639834CurrentTrain: epoch  2, batch     0 | loss: 2.1894026CurrentTrain: epoch  2, batch     1 | loss: 3.4556296CurrentTrain: epoch  3, batch     0 | loss: 2.2721217CurrentTrain: epoch  3, batch     1 | loss: 2.2314248CurrentTrain: epoch  4, batch     0 | loss: 2.0847812CurrentTrain: epoch  4, batch     1 | loss: 2.0406628CurrentTrain: epoch  5, batch     0 | loss: 2.1064022CurrentTrain: epoch  5, batch     1 | loss: 1.9573416CurrentTrain: epoch  6, batch     0 | loss: 1.9145339CurrentTrain: epoch  6, batch     1 | loss: 1.9681287CurrentTrain: epoch  7, batch     0 | loss: 1.8275919CurrentTrain: epoch  7, batch     1 | loss: 1.7843446CurrentTrain: epoch  8, batch     0 | loss: 1.7902898CurrentTrain: epoch  8, batch     1 | loss: 1.7836614CurrentTrain: epoch  9, batch     0 | loss: 1.8089511CurrentTrain: epoch  9, batch     1 | loss: 1.8172158
Mixup data size:  120

MemoryTrain:  epoch  0, batch     0 | loss: 1.0959734MemoryTrain:  epoch  0, batch     1 | loss: 1.4467576MemoryTrain:  epoch  0, batch     2 | loss: 0.3597517MemoryTrain:  epoch  1, batch     0 | loss: 1.7288382MemoryTrain:  epoch  1, batch     1 | loss: 0.7596000MemoryTrain:  epoch  1, batch     2 | loss: 1.3286877MemoryTrain:  epoch  2, batch     0 | loss: 1.0789695MemoryTrain:  epoch  2, batch     1 | loss: 0.4795158MemoryTrain:  epoch  2, batch     2 | loss: 0.1235471MemoryTrain:  epoch  3, batch     0 | loss: 0.9102455MemoryTrain:  epoch  3, batch     1 | loss: 0.3176966MemoryTrain:  epoch  3, batch     2 | loss: 0.1232081MemoryTrain:  epoch  4, batch     0 | loss: 0.0595259MemoryTrain:  epoch  4, batch     1 | loss: 0.6125891MemoryTrain:  epoch  4, batch     2 | loss: 0.0421685MemoryTrain:  epoch  5, batch     0 | loss: 0.1626135MemoryTrain:  epoch  5, batch     1 | loss: 0.2132677MemoryTrain:  epoch  5, batch     2 | loss: 0.1786723MemoryTrain:  epoch  6, batch     0 | loss: 0.3159932MemoryTrain:  epoch  6, batch     1 | loss: 0.1839921MemoryTrain:  epoch  6, batch     2 | loss: 0.0288025MemoryTrain:  epoch  7, batch     0 | loss: 0.0343021MemoryTrain:  epoch  7, batch     1 | loss: 0.1145647MemoryTrain:  epoch  7, batch     2 | loss: 0.2848573MemoryTrain:  epoch  8, batch     0 | loss: 0.0181441MemoryTrain:  epoch  8, batch     1 | loss: 0.1689721MemoryTrain:  epoch  8, batch     2 | loss: 0.0518980MemoryTrain:  epoch  9, batch     0 | loss: 0.0265618MemoryTrain:  epoch  9, batch     1 | loss: 0.1059756MemoryTrain:  epoch  9, batch     2 | loss: 0.0126464
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 5.7193279CurrentTrain: epoch  0, batch     1 | loss: 6.3848367CurrentTrain: epoch  1, batch     0 | loss: 4.5701036CurrentTrain: epoch  1, batch     1 | loss: 4.0142140CurrentTrain: epoch  2, batch     0 | loss: 3.9384038CurrentTrain: epoch  2, batch     1 | loss: 3.2668562CurrentTrain: epoch  3, batch     0 | loss: 3.5529089CurrentTrain: epoch  3, batch     1 | loss: 2.8307717CurrentTrain: epoch  4, batch     0 | loss: 3.0385211CurrentTrain: epoch  4, batch     1 | loss: 2.8979208CurrentTrain: epoch  5, batch     0 | loss: 2.7769496CurrentTrain: epoch  5, batch     1 | loss: 2.3594234CurrentTrain: epoch  6, batch     0 | loss: 2.5785985CurrentTrain: epoch  6, batch     1 | loss: 2.3666317CurrentTrain: epoch  7, batch     0 | loss: 2.3292279CurrentTrain: epoch  7, batch     1 | loss: 2.1327984CurrentTrain: epoch  8, batch     0 | loss: 2.1470089CurrentTrain: epoch  8, batch     1 | loss: 2.1735682CurrentTrain: epoch  9, batch     0 | loss: 2.0930455CurrentTrain: epoch  9, batch     1 | loss: 2.1987298
Mixup data size:  121

MemoryTrain:  epoch  0, batch     0 | loss: 1.7217743MemoryTrain:  epoch  0, batch     1 | loss: 1.0791091MemoryTrain:  epoch  0, batch     2 | loss: 0.4726129MemoryTrain:  epoch  1, batch     0 | loss: 1.5276182MemoryTrain:  epoch  1, batch     1 | loss: 1.9640832MemoryTrain:  epoch  1, batch     2 | loss: 1.0233343MemoryTrain:  epoch  2, batch     0 | loss: 1.4893944MemoryTrain:  epoch  2, batch     1 | loss: 0.5807559MemoryTrain:  epoch  2, batch     2 | loss: 1.1276956MemoryTrain:  epoch  3, batch     0 | loss: 0.9022461MemoryTrain:  epoch  3, batch     1 | loss: 0.9391271MemoryTrain:  epoch  3, batch     2 | loss: 1.0783119MemoryTrain:  epoch  4, batch     0 | loss: 0.9610356MemoryTrain:  epoch  4, batch     1 | loss: 0.7441393MemoryTrain:  epoch  4, batch     2 | loss: 0.5834303MemoryTrain:  epoch  5, batch     0 | loss: 0.6639475MemoryTrain:  epoch  5, batch     1 | loss: 0.6278117MemoryTrain:  epoch  5, batch     2 | loss: 0.4291450MemoryTrain:  epoch  6, batch     0 | loss: 0.6304249MemoryTrain:  epoch  6, batch     1 | loss: 0.6518413MemoryTrain:  epoch  6, batch     2 | loss: 0.4353194MemoryTrain:  epoch  7, batch     0 | loss: 0.3823524MemoryTrain:  epoch  7, batch     1 | loss: 0.4636359MemoryTrain:  epoch  7, batch     2 | loss: 0.3489957MemoryTrain:  epoch  8, batch     0 | loss: 0.5504247MemoryTrain:  epoch  8, batch     1 | loss: 0.3304412MemoryTrain:  epoch  8, batch     2 | loss: 0.1938570MemoryTrain:  epoch  9, batch     0 | loss: 0.4655325MemoryTrain:  epoch  9, batch     1 | loss: 0.4026227MemoryTrain:  epoch  9, batch     2 | loss: 0.2345542
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 97.22%   [EVAL] batch:    9 | acc: 25.00%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 87.50%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 9.82%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 20.31%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 27.78%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 33.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 38.64%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 43.23%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:   13 | acc: 6.25%,  total acc: 41.07%   [EVAL] batch:   14 | acc: 0.00%,  total acc: 38.33%   [EVAL] batch:   15 | acc: 0.00%,  total acc: 35.94%   [EVAL] batch:   16 | acc: 0.00%,  total acc: 33.82%   [EVAL] batch:   17 | acc: 0.00%,  total acc: 31.94%   [EVAL] batch:   18 | acc: 0.00%,  total acc: 30.26%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 30.00%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 33.04%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 35.80%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 38.04%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 40.36%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 42.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 44.95%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 46.06%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 47.54%   [EVAL] batch:   28 | acc: 81.25%,  total acc: 48.71%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 49.58%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 50.60%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 51.37%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 51.14%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 50.00%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 48.93%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 47.57%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 46.79%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 46.38%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 45.67%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 46.41%   [EVAL] batch:   40 | acc: 25.00%,  total acc: 45.88%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 47.17%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 46.37%   [EVAL] batch:   43 | acc: 6.25%,  total acc: 45.45%   [EVAL] batch:   44 | acc: 12.50%,  total acc: 44.72%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 44.16%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 44.02%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 44.01%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 43.37%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 42.50%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 41.79%   [EVAL] batch:   51 | acc: 0.00%,  total acc: 40.99%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 40.21%   [EVAL] batch:   53 | acc: 37.50%,  total acc: 40.16%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 40.57%   [EVAL] batch:   55 | acc: 56.25%,  total acc: 40.85%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 40.68%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 40.52%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 40.36%   [EVAL] batch:   59 | acc: 31.25%,  total acc: 40.21%   [EVAL] batch:   60 | acc: 6.25%,  total acc: 39.65%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 39.01%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 38.39%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 37.79%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 37.31%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 36.74%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 36.57%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 37.50%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 37.95%   [EVAL] batch:   69 | acc: 25.00%,  total acc: 37.77%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 38.20%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 38.63%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 39.38%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 40.12%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 40.83%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 41.37%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 42.05%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 42.39%   [EVAL] batch:   78 | acc: 25.00%,  total acc: 42.17%   [EVAL] batch:   79 | acc: 37.50%,  total acc: 42.11%   [EVAL] batch:   80 | acc: 12.50%,  total acc: 41.74%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 41.23%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 40.74%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 40.25%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 39.93%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 39.53%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 39.37%   [EVAL] batch:   87 | acc: 25.00%,  total acc: 39.20%   [EVAL] batch:   88 | acc: 43.75%,  total acc: 39.26%   [EVAL] batch:   89 | acc: 25.00%,  total acc: 39.10%   [EVAL] batch:   90 | acc: 18.75%,  total acc: 38.87%   [EVAL] batch:   91 | acc: 18.75%,  total acc: 38.65%   [EVAL] batch:   92 | acc: 25.00%,  total acc: 38.51%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 38.36%   [EVAL] batch:   94 | acc: 18.75%,  total acc: 38.16%   [EVAL] batch:   95 | acc: 12.50%,  total acc: 37.89%   [EVAL] batch:   96 | acc: 12.50%,  total acc: 37.63%   [EVAL] batch:   97 | acc: 12.50%,  total acc: 37.37%   [EVAL] batch:   98 | acc: 12.50%,  total acc: 37.12%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 37.44%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 38.00%   [EVAL] batch:  101 | acc: 12.50%,  total acc: 37.75%   [EVAL] batch:  102 | acc: 31.25%,  total acc: 37.68%   [EVAL] batch:  103 | acc: 18.75%,  total acc: 37.50%   [EVAL] batch:  104 | acc: 31.25%,  total acc: 37.44%   [EVAL] batch:  105 | acc: 25.00%,  total acc: 37.32%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 37.27%   [EVAL] batch:  107 | acc: 87.50%,  total acc: 37.73%   [EVAL] batch:  108 | acc: 81.25%,  total acc: 38.13%   [EVAL] batch:  109 | acc: 68.75%,  total acc: 38.41%   [EVAL] batch:  110 | acc: 93.75%,  total acc: 38.91%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 39.40%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 39.93%   [EVAL] batch:  113 | acc: 93.75%,  total acc: 40.41%   [EVAL] batch:  114 | acc: 68.75%,  total acc: 40.65%   [EVAL] batch:  115 | acc: 50.00%,  total acc: 40.73%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 41.08%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 41.26%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 41.49%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 41.98%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 42.46%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 42.93%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 43.39%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 43.85%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 44.30%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 44.74%   [EVAL] batch:  126 | acc: 100.00%,  total acc: 45.18%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 45.36%   [EVAL] batch:  128 | acc: 18.75%,  total acc: 45.16%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 45.34%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 45.75%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 46.12%   [EVAL] batch:  132 | acc: 62.50%,  total acc: 46.24%   
cur_acc_llm:  [0.875, 0.7208333333333333, 0.8883928571428571, 0.8923611111111112, 0.6818181818181818, 0.359375, 0.6730769230769231, 0.875]
his_acc_llm:  [0.875, 0.8311170212765957, 0.6926229508196722, 0.6193910256410257, 0.565625, 0.5035046728971962, 0.4375, 0.462406015037594]
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 97.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 98.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 95.83%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 81.25%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 78.12%   
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 35.42%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 31.25%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 32.50%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 32.14%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 35.16%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 38.89%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 40.00%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 39.77%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 40.62%   [EVAL] batch:   12 | acc: 18.75%,  total acc: 38.94%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 37.95%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 40.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 41.41%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 43.01%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 44.10%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 45.72%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 47.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 50.00%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 52.27%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 54.35%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 58.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 59.62%   [EVAL] batch:   26 | acc: 62.50%,  total acc: 59.72%   [EVAL] batch:   27 | acc: 81.25%,  total acc: 60.49%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 61.42%   [EVAL] batch:   29 | acc: 62.50%,  total acc: 61.46%   [EVAL] batch:   30 | acc: 62.50%,  total acc: 61.49%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 62.11%   [EVAL] batch:   32 | acc: 31.25%,  total acc: 61.17%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 59.38%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 57.68%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 56.25%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 54.90%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 54.61%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 54.65%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 55.47%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 55.64%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 56.70%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 55.38%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 54.12%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 52.92%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 51.77%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 51.06%   [EVAL] batch:   47 | acc: 31.25%,  total acc: 50.65%   [EVAL] batch:   48 | acc: 0.00%,  total acc: 49.62%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 48.62%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 47.67%   [EVAL] batch:   51 | acc: 0.00%,  total acc: 46.75%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 45.87%   [EVAL] batch:   53 | acc: 31.25%,  total acc: 45.60%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 46.02%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 46.88%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 47.48%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 47.74%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 48.20%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 48.12%   [EVAL] batch:   60 | acc: 6.25%,  total acc: 47.44%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 46.67%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 46.33%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 45.80%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 45.29%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 44.79%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 44.22%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 43.84%   [EVAL] batch:   68 | acc: 0.00%,  total acc: 43.21%   [EVAL] batch:   69 | acc: 18.75%,  total acc: 42.86%   [EVAL] batch:   70 | acc: 37.50%,  total acc: 42.78%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 43.14%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 43.92%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 44.68%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 45.42%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 46.05%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 46.75%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 47.12%   [EVAL] batch:   78 | acc: 6.25%,  total acc: 46.60%   [EVAL] batch:   79 | acc: 6.25%,  total acc: 46.09%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 45.52%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 44.97%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 44.43%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 43.97%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 43.75%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 43.68%   [EVAL] batch:   87 | acc: 31.25%,  total acc: 43.54%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 43.61%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 43.75%   [EVAL] batch:   90 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 43.82%   [EVAL] batch:   92 | acc: 31.25%,  total acc: 43.68%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 43.48%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 43.36%   [EVAL] batch:   95 | acc: 18.75%,  total acc: 43.10%   [EVAL] batch:   96 | acc: 0.00%,  total acc: 42.65%   [EVAL] batch:   97 | acc: 6.25%,  total acc: 42.28%   [EVAL] batch:   98 | acc: 0.00%,  total acc: 41.86%   [EVAL] batch:   99 | acc: 56.25%,  total acc: 42.00%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 42.57%   [EVAL] batch:  101 | acc: 43.75%,  total acc: 42.59%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 43.02%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 43.21%   [EVAL] batch:  104 | acc: 81.25%,  total acc: 43.57%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 43.99%   [EVAL] batch:  106 | acc: 81.25%,  total acc: 44.33%   [EVAL] batch:  107 | acc: 43.75%,  total acc: 44.33%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 44.61%   [EVAL] batch:  109 | acc: 75.00%,  total acc: 44.89%   [EVAL] batch:  110 | acc: 93.75%,  total acc: 45.33%   [EVAL] batch:  111 | acc: 81.25%,  total acc: 45.65%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 46.07%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 46.38%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 46.74%   [EVAL] batch:  115 | acc: 62.50%,  total acc: 46.88%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 47.28%   [EVAL] batch:  117 | acc: 68.75%,  total acc: 47.46%   [EVAL] batch:  118 | acc: 50.00%,  total acc: 47.48%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 47.92%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 48.24%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 48.67%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 49.09%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 49.50%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 49.90%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 50.30%   [EVAL] batch:  126 | acc: 100.00%,  total acc: 50.69%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 50.78%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 50.78%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 50.91%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 50.81%   [EVAL] batch:  131 | acc: 50.00%,  total acc: 50.80%   [EVAL] batch:  132 | acc: 25.00%,  total acc: 50.61%   
cur_acc:  ['0.8636', '0.6000', '0.7768', '0.8576', '0.5369', '0.7578', '0.7404', '0.7812']
his_acc:  ['0.8636', '0.8005', '0.6957', '0.6506', '0.6038', '0.5356', '0.4895', '0.5061']
--------Round  2
seed:  300
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 13.2152815CurrentTrain: epoch  0, batch     1 | loss: 12.8978548CurrentTrain: epoch  0, batch     2 | loss: 12.3794336CurrentTrain: epoch  0, batch     3 | loss: 11.3690224CurrentTrain: epoch  0, batch     4 | loss: 11.2030773CurrentTrain: epoch  0, batch     5 | loss: 11.4336548CurrentTrain: epoch  0, batch     6 | loss: 11.5284767CurrentTrain: epoch  0, batch     7 | loss: 11.6612797CurrentTrain: epoch  0, batch     8 | loss: 11.3044090CurrentTrain: epoch  0, batch     9 | loss: 11.2537613CurrentTrain: epoch  0, batch    10 | loss: 11.4847012CurrentTrain: epoch  0, batch    11 | loss: 11.3214359CurrentTrain: epoch  0, batch    12 | loss: 12.0788441CurrentTrain: epoch  0, batch    13 | loss: 11.5301676CurrentTrain: epoch  0, batch    14 | loss: 11.5133801CurrentTrain: epoch  0, batch    15 | loss: 11.4216413CurrentTrain: epoch  0, batch    16 | loss: 11.8396702CurrentTrain: epoch  0, batch    17 | loss: 11.6186991CurrentTrain: epoch  0, batch    18 | loss: 11.0013523CurrentTrain: epoch  0, batch    19 | loss: 11.0677071CurrentTrain: epoch  0, batch    20 | loss: 11.0641012CurrentTrain: epoch  0, batch    21 | loss: 11.8653641CurrentTrain: epoch  0, batch    22 | loss: 11.2140350CurrentTrain: epoch  0, batch    23 | loss: 11.1291103CurrentTrain: epoch  0, batch    24 | loss: 11.0238438CurrentTrain: epoch  0, batch    25 | loss: 11.0459661CurrentTrain: epoch  0, batch    26 | loss: 11.0648441CurrentTrain: epoch  0, batch    27 | loss: 11.7271633CurrentTrain: epoch  0, batch    28 | loss: 11.2428236CurrentTrain: epoch  0, batch    29 | loss: 10.9625168CurrentTrain: epoch  0, batch    30 | loss: 11.1511898CurrentTrain: epoch  0, batch    31 | loss: 11.7256794CurrentTrain: epoch  0, batch    32 | loss: 10.8218727CurrentTrain: epoch  0, batch    33 | loss: 11.3186045CurrentTrain: epoch  0, batch    34 | loss: 10.7986317CurrentTrain: epoch  0, batch    35 | loss: 10.9312649CurrentTrain: epoch  0, batch    36 | loss: 10.7167568CurrentTrain: epoch  0, batch    37 | loss: 10.9197445CurrentTrain: epoch  1, batch     0 | loss: 11.0320606CurrentTrain: epoch  1, batch     1 | loss: 10.9041357CurrentTrain: epoch  1, batch     2 | loss: 10.8850622CurrentTrain: epoch  1, batch     3 | loss: 10.9184542CurrentTrain: epoch  1, batch     4 | loss: 11.0329533CurrentTrain: epoch  1, batch     5 | loss: 11.7963810CurrentTrain: epoch  1, batch     6 | loss: 12.5716200CurrentTrain: epoch  1, batch     7 | loss: 12.3077793CurrentTrain: epoch  1, batch     8 | loss: 11.4529533CurrentTrain: epoch  1, batch     9 | loss: 11.1946802CurrentTrain: epoch  1, batch    10 | loss: 11.1144171CurrentTrain: epoch  1, batch    11 | loss: 11.0614738CurrentTrain: epoch  1, batch    12 | loss: 10.9576683CurrentTrain: epoch  1, batch    13 | loss: 10.7728796CurrentTrain: epoch  1, batch    14 | loss: 11.0755291CurrentTrain: epoch  1, batch    15 | loss: 10.8533173CurrentTrain: epoch  1, batch    16 | loss: 11.3137579CurrentTrain: epoch  1, batch    17 | loss: 11.0909290CurrentTrain: epoch  1, batch    18 | loss: 11.3950310CurrentTrain: epoch  1, batch    19 | loss: 11.2974262CurrentTrain: epoch  1, batch    20 | loss: 11.1881342CurrentTrain: epoch  1, batch    21 | loss: 10.8740635CurrentTrain: epoch  1, batch    22 | loss: 10.7554560CurrentTrain: epoch  1, batch    23 | loss: 10.7944202CurrentTrain: epoch  1, batch    24 | loss: 10.7698212CurrentTrain: epoch  1, batch    25 | loss: 10.6767893CurrentTrain: epoch  1, batch    26 | loss: 11.0167589CurrentTrain: epoch  1, batch    27 | loss: 10.6746597CurrentTrain: epoch  1, batch    28 | loss: 10.6467628CurrentTrain: epoch  1, batch    29 | loss: 10.9425240CurrentTrain: epoch  1, batch    30 | loss: 10.5923100CurrentTrain: epoch  1, batch    31 | loss: 10.8538399CurrentTrain: epoch  1, batch    32 | loss: 10.6321850CurrentTrain: epoch  1, batch    33 | loss: 11.3538504CurrentTrain: epoch  1, batch    34 | loss: 10.7266750CurrentTrain: epoch  1, batch    35 | loss: 10.4769392CurrentTrain: epoch  1, batch    36 | loss: 10.7098637CurrentTrain: epoch  1, batch    37 | loss: 10.4148045CurrentTrain: epoch  2, batch     0 | loss: 10.7273445CurrentTrain: epoch  2, batch     1 | loss: 10.5797997CurrentTrain: epoch  2, batch     2 | loss: 10.8005753CurrentTrain: epoch  2, batch     3 | loss: 10.5674582CurrentTrain: epoch  2, batch     4 | loss: 10.7686577CurrentTrain: epoch  2, batch     5 | loss: 10.7605534CurrentTrain: epoch  2, batch     6 | loss: 10.7000856CurrentTrain: epoch  2, batch     7 | loss: 10.6406078CurrentTrain: epoch  2, batch     8 | loss: 10.5255337CurrentTrain: epoch  2, batch     9 | loss: 10.4241629CurrentTrain: epoch  2, batch    10 | loss: 10.4860525CurrentTrain: epoch  2, batch    11 | loss: 10.2943316CurrentTrain: epoch  2, batch    12 | loss: 10.2997675CurrentTrain: epoch  2, batch    13 | loss: 10.3128777CurrentTrain: epoch  2, batch    14 | loss: 10.3922749CurrentTrain: epoch  2, batch    15 | loss: 10.3670807CurrentTrain: epoch  2, batch    16 | loss: 10.3627052CurrentTrain: epoch  2, batch    17 | loss: 10.2591734CurrentTrain: epoch  2, batch    18 | loss: 10.4171886CurrentTrain: epoch  2, batch    19 | loss: 10.1437664CurrentTrain: epoch  2, batch    20 | loss: 10.0806198CurrentTrain: epoch  2, batch    21 | loss: 10.3946810CurrentTrain: epoch  2, batch    22 | loss: 10.3392658CurrentTrain: epoch  2, batch    23 | loss: 10.1481047CurrentTrain: epoch  2, batch    24 | loss: 10.4325819CurrentTrain: epoch  2, batch    25 | loss: 10.2541952CurrentTrain: epoch  2, batch    26 | loss: 10.2351379CurrentTrain: epoch  2, batch    27 | loss: 9.9045067CurrentTrain: epoch  2, batch    28 | loss: 10.1082726CurrentTrain: epoch  2, batch    29 | loss: 10.3439198CurrentTrain: epoch  2, batch    30 | loss: 9.9951458CurrentTrain: epoch  2, batch    31 | loss: 9.8169699CurrentTrain: epoch  2, batch    32 | loss: 9.9042435CurrentTrain: epoch  2, batch    33 | loss: 10.0392742CurrentTrain: epoch  2, batch    34 | loss: 9.7964697CurrentTrain: epoch  2, batch    35 | loss: 9.8401251CurrentTrain: epoch  2, batch    36 | loss: 9.7044277CurrentTrain: epoch  2, batch    37 | loss: 9.6458483CurrentTrain: epoch  3, batch     0 | loss: 9.7824211CurrentTrain: epoch  3, batch     1 | loss: 9.8612366CurrentTrain: epoch  3, batch     2 | loss: 10.1568489CurrentTrain: epoch  3, batch     3 | loss: 9.6850605CurrentTrain: epoch  3, batch     4 | loss: 9.9135075CurrentTrain: epoch  3, batch     5 | loss: 9.4296989CurrentTrain: epoch  3, batch     6 | loss: 10.0984373CurrentTrain: epoch  3, batch     7 | loss: 9.6819363CurrentTrain: epoch  3, batch     8 | loss: 9.9361258CurrentTrain: epoch  3, batch     9 | loss: 9.6487694CurrentTrain: epoch  3, batch    10 | loss: 9.5420809CurrentTrain: epoch  3, batch    11 | loss: 9.7214556CurrentTrain: epoch  3, batch    12 | loss: 9.3645496CurrentTrain: epoch  3, batch    13 | loss: 9.6766243CurrentTrain: epoch  3, batch    14 | loss: 9.2420740CurrentTrain: epoch  3, batch    15 | loss: 9.0163832CurrentTrain: epoch  3, batch    16 | loss: 9.6207514CurrentTrain: epoch  3, batch    17 | loss: 9.6623516CurrentTrain: epoch  3, batch    18 | loss: 9.1641836CurrentTrain: epoch  3, batch    19 | loss: 9.3368549CurrentTrain: epoch  3, batch    20 | loss: 8.6826668CurrentTrain: epoch  3, batch    21 | loss: 8.7710552CurrentTrain: epoch  3, batch    22 | loss: 8.8126078CurrentTrain: epoch  3, batch    23 | loss: 8.6576366CurrentTrain: epoch  3, batch    24 | loss: 8.7292385CurrentTrain: epoch  3, batch    25 | loss: 8.5725698CurrentTrain: epoch  3, batch    26 | loss: 8.4652996CurrentTrain: epoch  3, batch    27 | loss: 8.6083393CurrentTrain: epoch  3, batch    28 | loss: 8.7157345CurrentTrain: epoch  3, batch    29 | loss: 8.9767647CurrentTrain: epoch  3, batch    30 | loss: 8.8415556CurrentTrain: epoch  3, batch    31 | loss: 8.5233116CurrentTrain: epoch  3, batch    32 | loss: 8.5514307CurrentTrain: epoch  3, batch    33 | loss: 8.7561073CurrentTrain: epoch  3, batch    34 | loss: 8.9207268CurrentTrain: epoch  3, batch    35 | loss: 7.9309254CurrentTrain: epoch  3, batch    36 | loss: 8.7652092CurrentTrain: epoch  3, batch    37 | loss: 8.7908878CurrentTrain: epoch  4, batch     0 | loss: 8.6746645CurrentTrain: epoch  4, batch     1 | loss: 8.0780001CurrentTrain: epoch  4, batch     2 | loss: 7.9519086CurrentTrain: epoch  4, batch     3 | loss: 7.8940868CurrentTrain: epoch  4, batch     4 | loss: 8.3872566CurrentTrain: epoch  4, batch     5 | loss: 8.0853910CurrentTrain: epoch  4, batch     6 | loss: 8.4654484CurrentTrain: epoch  4, batch     7 | loss: 7.7061434CurrentTrain: epoch  4, batch     8 | loss: 8.1333599CurrentTrain: epoch  4, batch     9 | loss: 8.3062277CurrentTrain: epoch  4, batch    10 | loss: 7.8380594CurrentTrain: epoch  4, batch    11 | loss: 7.9375515CurrentTrain: epoch  4, batch    12 | loss: 8.1173267CurrentTrain: epoch  4, batch    13 | loss: 7.3232183CurrentTrain: epoch  4, batch    14 | loss: 7.0471392CurrentTrain: epoch  4, batch    15 | loss: 8.2718229CurrentTrain: epoch  4, batch    16 | loss: 7.9496031CurrentTrain: epoch  4, batch    17 | loss: 8.1499147CurrentTrain: epoch  4, batch    18 | loss: 7.5021272CurrentTrain: epoch  4, batch    19 | loss: 7.7304440CurrentTrain: epoch  4, batch    20 | loss: 7.4535842CurrentTrain: epoch  4, batch    21 | loss: 7.3042164CurrentTrain: epoch  4, batch    22 | loss: 7.4340782CurrentTrain: epoch  4, batch    23 | loss: 7.1700087CurrentTrain: epoch  4, batch    24 | loss: 7.1832924CurrentTrain: epoch  4, batch    25 | loss: 6.9280872CurrentTrain: epoch  4, batch    26 | loss: 7.9697132CurrentTrain: epoch  4, batch    27 | loss: 7.3286471CurrentTrain: epoch  4, batch    28 | loss: 7.3281188CurrentTrain: epoch  4, batch    29 | loss: 6.8548245CurrentTrain: epoch  4, batch    30 | loss: 6.8583889CurrentTrain: epoch  4, batch    31 | loss: 7.8436441CurrentTrain: epoch  4, batch    32 | loss: 6.8336201CurrentTrain: epoch  4, batch    33 | loss: 7.1680431CurrentTrain: epoch  4, batch    34 | loss: 6.9593539CurrentTrain: epoch  4, batch    35 | loss: 6.5378604CurrentTrain: epoch  4, batch    36 | loss: 7.1269693CurrentTrain: epoch  4, batch    37 | loss: 8.7332563CurrentTrain: epoch  5, batch     0 | loss: 6.8535128CurrentTrain: epoch  5, batch     1 | loss: 8.3800373CurrentTrain: epoch  5, batch     2 | loss: 7.6554303CurrentTrain: epoch  5, batch     3 | loss: 6.6775055CurrentTrain: epoch  5, batch     4 | loss: 7.5645514CurrentTrain: epoch  5, batch     5 | loss: 7.3680649CurrentTrain: epoch  5, batch     6 | loss: 7.2595983CurrentTrain: epoch  5, batch     7 | loss: 6.9316359CurrentTrain: epoch  5, batch     8 | loss: 8.0870800CurrentTrain: epoch  5, batch     9 | loss: 6.9851770CurrentTrain: epoch  5, batch    10 | loss: 7.1382227CurrentTrain: epoch  5, batch    11 | loss: 6.5620604CurrentTrain: epoch  5, batch    12 | loss: 6.1836486CurrentTrain: epoch  5, batch    13 | loss: 7.1297793CurrentTrain: epoch  5, batch    14 | loss: 6.4770737CurrentTrain: epoch  5, batch    15 | loss: 6.6742344CurrentTrain: epoch  5, batch    16 | loss: 6.3108001CurrentTrain: epoch  5, batch    17 | loss: 7.5293283CurrentTrain: epoch  5, batch    18 | loss: 7.5441837CurrentTrain: epoch  5, batch    19 | loss: 6.8789301CurrentTrain: epoch  5, batch    20 | loss: 7.2100286CurrentTrain: epoch  5, batch    21 | loss: 7.1915383CurrentTrain: epoch  5, batch    22 | loss: 6.1327457CurrentTrain: epoch  5, batch    23 | loss: 6.7535501CurrentTrain: epoch  5, batch    24 | loss: 6.7881217CurrentTrain: epoch  5, batch    25 | loss: 6.6934919CurrentTrain: epoch  5, batch    26 | loss: 6.6795692CurrentTrain: epoch  5, batch    27 | loss: 5.8010340CurrentTrain: epoch  5, batch    28 | loss: 6.9365668CurrentTrain: epoch  5, batch    29 | loss: 7.5095940CurrentTrain: epoch  5, batch    30 | loss: 6.8772178CurrentTrain: epoch  5, batch    31 | loss: 7.0150423CurrentTrain: epoch  5, batch    32 | loss: 6.9656296CurrentTrain: epoch  5, batch    33 | loss: 6.7250605CurrentTrain: epoch  5, batch    34 | loss: 7.3291187CurrentTrain: epoch  5, batch    35 | loss: 6.9828920CurrentTrain: epoch  5, batch    36 | loss: 7.1290874CurrentTrain: epoch  5, batch    37 | loss: 7.1142774CurrentTrain: epoch  6, batch     0 | loss: 6.4200597CurrentTrain: epoch  6, batch     1 | loss: 7.1758971CurrentTrain: epoch  6, batch     2 | loss: 7.6223202CurrentTrain: epoch  6, batch     3 | loss: 6.7403717CurrentTrain: epoch  6, batch     4 | loss: 6.8270988CurrentTrain: epoch  6, batch     5 | loss: 6.4328299CurrentTrain: epoch  6, batch     6 | loss: 6.6002479CurrentTrain: epoch  6, batch     7 | loss: 7.3255792CurrentTrain: epoch  6, batch     8 | loss: 6.4920073CurrentTrain: epoch  6, batch     9 | loss: 6.0621986CurrentTrain: epoch  6, batch    10 | loss: 6.8205328CurrentTrain: epoch  6, batch    11 | loss: 6.0553083CurrentTrain: epoch  6, batch    12 | loss: 6.3527694CurrentTrain: epoch  6, batch    13 | loss: 6.5107970CurrentTrain: epoch  6, batch    14 | loss: 6.4063025CurrentTrain: epoch  6, batch    15 | loss: 6.6917868CurrentTrain: epoch  6, batch    16 | loss: 5.7849669CurrentTrain: epoch  6, batch    17 | loss: 6.9926529CurrentTrain: epoch  6, batch    18 | loss: 5.5957198CurrentTrain: epoch  6, batch    19 | loss: 6.9660916CurrentTrain: epoch  6, batch    20 | loss: 7.0427370CurrentTrain: epoch  6, batch    21 | loss: 6.1330423CurrentTrain: epoch  6, batch    22 | loss: 6.2548294CurrentTrain: epoch  6, batch    23 | loss: 6.3563871CurrentTrain: epoch  6, batch    24 | loss: 6.1219254CurrentTrain: epoch  6, batch    25 | loss: 6.5922880CurrentTrain: epoch  6, batch    26 | loss: 6.2179351CurrentTrain: epoch  6, batch    27 | loss: 6.9087939CurrentTrain: epoch  6, batch    28 | loss: 5.7527704CurrentTrain: epoch  6, batch    29 | loss: 6.4390345CurrentTrain: epoch  6, batch    30 | loss: 7.4259987CurrentTrain: epoch  6, batch    31 | loss: 5.7414274CurrentTrain: epoch  6, batch    32 | loss: 6.8838243CurrentTrain: epoch  6, batch    33 | loss: 6.3044796CurrentTrain: epoch  6, batch    34 | loss: 6.0317163CurrentTrain: epoch  6, batch    35 | loss: 6.4649482CurrentTrain: epoch  6, batch    36 | loss: 5.8210034CurrentTrain: epoch  6, batch    37 | loss: 6.5545664CurrentTrain: epoch  7, batch     0 | loss: 5.8191013CurrentTrain: epoch  7, batch     1 | loss: 5.9526796CurrentTrain: epoch  7, batch     2 | loss: 5.6893187CurrentTrain: epoch  7, batch     3 | loss: 6.4299879CurrentTrain: epoch  7, batch     4 | loss: 6.3465323CurrentTrain: epoch  7, batch     5 | loss: 6.6444535CurrentTrain: epoch  7, batch     6 | loss: 5.7467780CurrentTrain: epoch  7, batch     7 | loss: 6.0533400CurrentTrain: epoch  7, batch     8 | loss: 6.7694364CurrentTrain: epoch  7, batch     9 | loss: 5.6858444CurrentTrain: epoch  7, batch    10 | loss: 5.5148382CurrentTrain: epoch  7, batch    11 | loss: 6.0767417CurrentTrain: epoch  7, batch    12 | loss: 5.9008646CurrentTrain: epoch  7, batch    13 | loss: 6.6495051CurrentTrain: epoch  7, batch    14 | loss: 5.8531146CurrentTrain: epoch  7, batch    15 | loss: 4.8153858CurrentTrain: epoch  7, batch    16 | loss: 6.0066280CurrentTrain: epoch  7, batch    17 | loss: 5.4838905CurrentTrain: epoch  7, batch    18 | loss: 6.3286772CurrentTrain: epoch  7, batch    19 | loss: 6.8425493CurrentTrain: epoch  7, batch    20 | loss: 6.0295439CurrentTrain: epoch  7, batch    21 | loss: 5.5503836CurrentTrain: epoch  7, batch    22 | loss: 5.8232937CurrentTrain: epoch  7, batch    23 | loss: 6.9569836CurrentTrain: epoch  7, batch    24 | loss: 5.9316816CurrentTrain: epoch  7, batch    25 | loss: 6.5703926CurrentTrain: epoch  7, batch    26 | loss: 6.6093426CurrentTrain: epoch  7, batch    27 | loss: 5.8604193CurrentTrain: epoch  7, batch    28 | loss: 5.5658045CurrentTrain: epoch  7, batch    29 | loss: 6.3122792CurrentTrain: epoch  7, batch    30 | loss: 6.7985897CurrentTrain: epoch  7, batch    31 | loss: 5.7945709CurrentTrain: epoch  7, batch    32 | loss: 5.7487841CurrentTrain: epoch  7, batch    33 | loss: 6.1306372CurrentTrain: epoch  7, batch    34 | loss: 6.5833344CurrentTrain: epoch  7, batch    35 | loss: 5.5838757CurrentTrain: epoch  7, batch    36 | loss: 6.2977381CurrentTrain: epoch  7, batch    37 | loss: 6.4507170CurrentTrain: epoch  8, batch     0 | loss: 5.8410072CurrentTrain: epoch  8, batch     1 | loss: 5.4651060CurrentTrain: epoch  8, batch     2 | loss: 6.4216461CurrentTrain: epoch  8, batch     3 | loss: 5.9860435CurrentTrain: epoch  8, batch     4 | loss: 6.0800786CurrentTrain: epoch  8, batch     5 | loss: 5.8276439CurrentTrain: epoch  8, batch     6 | loss: 6.2115645CurrentTrain: epoch  8, batch     7 | loss: 5.8436608CurrentTrain: epoch  8, batch     8 | loss: 5.8006411CurrentTrain: epoch  8, batch     9 | loss: 5.5694356CurrentTrain: epoch  8, batch    10 | loss: 6.1548467CurrentTrain: epoch  8, batch    11 | loss: 5.7318058CurrentTrain: epoch  8, batch    12 | loss: 5.5506401CurrentTrain: epoch  8, batch    13 | loss: 5.3806024CurrentTrain: epoch  8, batch    14 | loss: 5.7752285CurrentTrain: epoch  8, batch    15 | loss: 5.3747263CurrentTrain: epoch  8, batch    16 | loss: 5.7807050CurrentTrain: epoch  8, batch    17 | loss: 6.0084677CurrentTrain: epoch  8, batch    18 | loss: 6.1346359CurrentTrain: epoch  8, batch    19 | loss: 5.3669043CurrentTrain: epoch  8, batch    20 | loss: 5.6405478CurrentTrain: epoch  8, batch    21 | loss: 5.2761784CurrentTrain: epoch  8, batch    22 | loss: 5.2649660CurrentTrain: epoch  8, batch    23 | loss: 5.2393103CurrentTrain: epoch  8, batch    24 | loss: 5.6922207CurrentTrain: epoch  8, batch    25 | loss: 5.4697447CurrentTrain: epoch  8, batch    26 | loss: 5.3388062CurrentTrain: epoch  8, batch    27 | loss: 5.8822088CurrentTrain: epoch  8, batch    28 | loss: 5.3072529CurrentTrain: epoch  8, batch    29 | loss: 5.8543372CurrentTrain: epoch  8, batch    30 | loss: 5.6853061CurrentTrain: epoch  8, batch    31 | loss: 6.0586667CurrentTrain: epoch  8, batch    32 | loss: 5.5757790CurrentTrain: epoch  8, batch    33 | loss: 6.0742435CurrentTrain: epoch  8, batch    34 | loss: 4.9466944CurrentTrain: epoch  8, batch    35 | loss: 5.0883417CurrentTrain: epoch  8, batch    36 | loss: 5.9643130CurrentTrain: epoch  8, batch    37 | loss: 5.7398968CurrentTrain: epoch  9, batch     0 | loss: 5.5503235CurrentTrain: epoch  9, batch     1 | loss: 5.3882542CurrentTrain: epoch  9, batch     2 | loss: 5.2670007CurrentTrain: epoch  9, batch     3 | loss: 5.4027820CurrentTrain: epoch  9, batch     4 | loss: 5.5815954CurrentTrain: epoch  9, batch     5 | loss: 5.4125586CurrentTrain: epoch  9, batch     6 | loss: 6.1470137CurrentTrain: epoch  9, batch     7 | loss: 5.3163490CurrentTrain: epoch  9, batch     8 | loss: 5.2270231CurrentTrain: epoch  9, batch     9 | loss: 5.6284294CurrentTrain: epoch  9, batch    10 | loss: 5.5712481CurrentTrain: epoch  9, batch    11 | loss: 5.8004932CurrentTrain: epoch  9, batch    12 | loss: 5.1740222CurrentTrain: epoch  9, batch    13 | loss: 5.2588501CurrentTrain: epoch  9, batch    14 | loss: 4.9741602CurrentTrain: epoch  9, batch    15 | loss: 5.1013803CurrentTrain: epoch  9, batch    16 | loss: 5.7982969CurrentTrain: epoch  9, batch    17 | loss: 5.2646065CurrentTrain: epoch  9, batch    18 | loss: 5.4190187CurrentTrain: epoch  9, batch    19 | loss: 5.3593540CurrentTrain: epoch  9, batch    20 | loss: 5.8169694CurrentTrain: epoch  9, batch    21 | loss: 5.4789772CurrentTrain: epoch  9, batch    22 | loss: 5.1289091CurrentTrain: epoch  9, batch    23 | loss: 5.3680248CurrentTrain: epoch  9, batch    24 | loss: 4.9909463CurrentTrain: epoch  9, batch    25 | loss: 4.9456511CurrentTrain: epoch  9, batch    26 | loss: 5.2544060CurrentTrain: epoch  9, batch    27 | loss: 5.7427745CurrentTrain: epoch  9, batch    28 | loss: 5.3504171CurrentTrain: epoch  9, batch    29 | loss: 5.3670387CurrentTrain: epoch  9, batch    30 | loss: 5.1266050CurrentTrain: epoch  9, batch    31 | loss: 5.4237137CurrentTrain: epoch  9, batch    32 | loss: 5.6709099CurrentTrain: epoch  9, batch    33 | loss: 5.2050495CurrentTrain: epoch  9, batch    34 | loss: 5.1114130CurrentTrain: epoch  9, batch    35 | loss: 5.2136836CurrentTrain: epoch  9, batch    36 | loss: 5.2576933CurrentTrain: epoch  9, batch    37 | loss: 5.9975643
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 14.9705439CurrentTrain: epoch  0, batch     1 | loss: 14.6742401CurrentTrain: epoch  0, batch     2 | loss: 14.7133083CurrentTrain: epoch  0, batch     3 | loss: 14.3418818CurrentTrain: epoch  0, batch     4 | loss: 14.9109612CurrentTrain: epoch  0, batch     5 | loss: 14.0614796CurrentTrain: epoch  0, batch     6 | loss: 13.7603283CurrentTrain: epoch  0, batch     7 | loss: 13.9849854CurrentTrain: epoch  0, batch     8 | loss: 13.9129620CurrentTrain: epoch  0, batch     9 | loss: 13.8564892CurrentTrain: epoch  0, batch    10 | loss: 13.0785046CurrentTrain: epoch  0, batch    11 | loss: 13.6112957CurrentTrain: epoch  0, batch    12 | loss: 13.1083088CurrentTrain: epoch  0, batch    13 | loss: 13.3439217CurrentTrain: epoch  0, batch    14 | loss: 13.2969894CurrentTrain: epoch  0, batch    15 | loss: 13.3487577CurrentTrain: epoch  0, batch    16 | loss: 13.6730251CurrentTrain: epoch  0, batch    17 | loss: 13.8745155CurrentTrain: epoch  0, batch    18 | loss: 13.0551434CurrentTrain: epoch  0, batch    19 | loss: 12.7176476CurrentTrain: epoch  0, batch    20 | loss: 12.7175045CurrentTrain: epoch  0, batch    21 | loss: 13.1944027CurrentTrain: epoch  0, batch    22 | loss: 12.9804926CurrentTrain: epoch  0, batch    23 | loss: 12.2035770CurrentTrain: epoch  0, batch    24 | loss: 11.7971659CurrentTrain: epoch  0, batch    25 | loss: 12.9810886CurrentTrain: epoch  0, batch    26 | loss: 11.8831282CurrentTrain: epoch  0, batch    27 | loss: 11.2529306CurrentTrain: epoch  0, batch    28 | loss: 12.3900099CurrentTrain: epoch  0, batch    29 | loss: 13.1088619CurrentTrain: epoch  0, batch    30 | loss: 12.1822052CurrentTrain: epoch  0, batch    31 | loss: 12.0286617CurrentTrain: epoch  0, batch    32 | loss: 12.0013733CurrentTrain: epoch  0, batch    33 | loss: 11.6738939CurrentTrain: epoch  0, batch    34 | loss: 13.4183331CurrentTrain: epoch  0, batch    35 | loss: 12.5872765CurrentTrain: epoch  0, batch    36 | loss: 11.1364202CurrentTrain: epoch  0, batch    37 | loss: 11.7353058CurrentTrain: epoch  1, batch     0 | loss: 11.2924795CurrentTrain: epoch  1, batch     1 | loss: 11.7082958CurrentTrain: epoch  1, batch     2 | loss: 10.5186634CurrentTrain: epoch  1, batch     3 | loss: 10.2952557CurrentTrain: epoch  1, batch     4 | loss: 11.4222527CurrentTrain: epoch  1, batch     5 | loss: 11.4559803CurrentTrain: epoch  1, batch     6 | loss: 9.7087603CurrentTrain: epoch  1, batch     7 | loss: 11.1271143CurrentTrain: epoch  1, batch     8 | loss: 10.7319546CurrentTrain: epoch  1, batch     9 | loss: 11.0389338CurrentTrain: epoch  1, batch    10 | loss: 10.3773975CurrentTrain: epoch  1, batch    11 | loss: 9.5784683CurrentTrain: epoch  1, batch    12 | loss: 9.6624756CurrentTrain: epoch  1, batch    13 | loss: 10.2689734CurrentTrain: epoch  1, batch    14 | loss: 10.3258047CurrentTrain: epoch  1, batch    15 | loss: 10.4367218CurrentTrain: epoch  1, batch    16 | loss: 10.3081121CurrentTrain: epoch  1, batch    17 | loss: 10.3615227CurrentTrain: epoch  1, batch    18 | loss: 9.9293528CurrentTrain: epoch  1, batch    19 | loss: 9.5029249CurrentTrain: epoch  1, batch    20 | loss: 10.1102123CurrentTrain: epoch  1, batch    21 | loss: 10.9760437CurrentTrain: epoch  1, batch    22 | loss: 11.1053438CurrentTrain: epoch  1, batch    23 | loss: 9.7324047CurrentTrain: epoch  1, batch    24 | loss: 10.4489307CurrentTrain: epoch  1, batch    25 | loss: 9.9933033CurrentTrain: epoch  1, batch    26 | loss: 9.0183926CurrentTrain: epoch  1, batch    27 | loss: 9.6326990CurrentTrain: epoch  1, batch    28 | loss: 9.8513317CurrentTrain: epoch  1, batch    29 | loss: 9.6147461CurrentTrain: epoch  1, batch    30 | loss: 9.7117720CurrentTrain: epoch  1, batch    31 | loss: 9.0346203CurrentTrain: epoch  1, batch    32 | loss: 9.2947798CurrentTrain: epoch  1, batch    33 | loss: 10.1579781CurrentTrain: epoch  1, batch    34 | loss: 9.2963743CurrentTrain: epoch  1, batch    35 | loss: 9.8104763CurrentTrain: epoch  1, batch    36 | loss: 9.5154133CurrentTrain: epoch  1, batch    37 | loss: 9.1648474CurrentTrain: epoch  2, batch     0 | loss: 7.9546547CurrentTrain: epoch  2, batch     1 | loss: 7.7176085CurrentTrain: epoch  2, batch     2 | loss: 9.8143368CurrentTrain: epoch  2, batch     3 | loss: 8.3111143CurrentTrain: epoch  2, batch     4 | loss: 7.9831510CurrentTrain: epoch  2, batch     5 | loss: 9.0529194CurrentTrain: epoch  2, batch     6 | loss: 8.5865517CurrentTrain: epoch  2, batch     7 | loss: 9.3393154CurrentTrain: epoch  2, batch     8 | loss: 10.2741976CurrentTrain: epoch  2, batch     9 | loss: 9.5180550CurrentTrain: epoch  2, batch    10 | loss: 8.7008038CurrentTrain: epoch  2, batch    11 | loss: 8.7911406CurrentTrain: epoch  2, batch    12 | loss: 9.1287670CurrentTrain: epoch  2, batch    13 | loss: 8.1340389CurrentTrain: epoch  2, batch    14 | loss: 8.6068306CurrentTrain: epoch  2, batch    15 | loss: 7.4023967CurrentTrain: epoch  2, batch    16 | loss: 8.0730095CurrentTrain: epoch  2, batch    17 | loss: 7.6301279CurrentTrain: epoch  2, batch    18 | loss: 8.3556166CurrentTrain: epoch  2, batch    19 | loss: 10.0222797CurrentTrain: epoch  2, batch    20 | loss: 8.3789959CurrentTrain: epoch  2, batch    21 | loss: 8.7147226CurrentTrain: epoch  2, batch    22 | loss: 8.3147802CurrentTrain: epoch  2, batch    23 | loss: 7.3666744CurrentTrain: epoch  2, batch    24 | loss: 8.3038626CurrentTrain: epoch  2, batch    25 | loss: 8.0833235CurrentTrain: epoch  2, batch    26 | loss: 9.3454075CurrentTrain: epoch  2, batch    27 | loss: 7.5957193CurrentTrain: epoch  2, batch    28 | loss: 8.4258976CurrentTrain: epoch  2, batch    29 | loss: 6.7733903CurrentTrain: epoch  2, batch    30 | loss: 8.5502396CurrentTrain: epoch  2, batch    31 | loss: 7.9199467CurrentTrain: epoch  2, batch    32 | loss: 7.9897184CurrentTrain: epoch  2, batch    33 | loss: 7.4595885CurrentTrain: epoch  2, batch    34 | loss: 9.0812235CurrentTrain: epoch  2, batch    35 | loss: 8.9325409CurrentTrain: epoch  2, batch    36 | loss: 7.8576384CurrentTrain: epoch  2, batch    37 | loss: 8.2418365CurrentTrain: epoch  3, batch     0 | loss: 6.5267911CurrentTrain: epoch  3, batch     1 | loss: 7.2495189CurrentTrain: epoch  3, batch     2 | loss: 7.3895240CurrentTrain: epoch  3, batch     3 | loss: 7.3552532CurrentTrain: epoch  3, batch     4 | loss: 7.9619555CurrentTrain: epoch  3, batch     5 | loss: 6.7305155CurrentTrain: epoch  3, batch     6 | loss: 7.0453167CurrentTrain: epoch  3, batch     7 | loss: 6.5443945CurrentTrain: epoch  3, batch     8 | loss: 6.3006868CurrentTrain: epoch  3, batch     9 | loss: 8.2446136CurrentTrain: epoch  3, batch    10 | loss: 8.3825665CurrentTrain: epoch  3, batch    11 | loss: 7.2540874CurrentTrain: epoch  3, batch    12 | loss: 7.7164192CurrentTrain: epoch  3, batch    13 | loss: 6.6021185CurrentTrain: epoch  3, batch    14 | loss: 7.7824879CurrentTrain: epoch  3, batch    15 | loss: 7.5248671CurrentTrain: epoch  3, batch    16 | loss: 7.7151227CurrentTrain: epoch  3, batch    17 | loss: 9.2332544CurrentTrain: epoch  3, batch    18 | loss: 8.0445538CurrentTrain: epoch  3, batch    19 | loss: 7.4478288CurrentTrain: epoch  3, batch    20 | loss: 7.5761337CurrentTrain: epoch  3, batch    21 | loss: 6.5162096CurrentTrain: epoch  3, batch    22 | loss: 8.3046970CurrentTrain: epoch  3, batch    23 | loss: 7.8036022CurrentTrain: epoch  3, batch    24 | loss: 6.6767101CurrentTrain: epoch  3, batch    25 | loss: 8.7581196CurrentTrain: epoch  3, batch    26 | loss: 7.9231749CurrentTrain: epoch  3, batch    27 | loss: 8.8754416CurrentTrain: epoch  3, batch    28 | loss: 6.8143973CurrentTrain: epoch  3, batch    29 | loss: 7.5821505CurrentTrain: epoch  3, batch    30 | loss: 9.3535414CurrentTrain: epoch  3, batch    31 | loss: 7.8026114CurrentTrain: epoch  3, batch    32 | loss: 7.2682714CurrentTrain: epoch  3, batch    33 | loss: 6.6463065CurrentTrain: epoch  3, batch    34 | loss: 6.7614942CurrentTrain: epoch  3, batch    35 | loss: 8.9144192CurrentTrain: epoch  3, batch    36 | loss: 7.0869966CurrentTrain: epoch  3, batch    37 | loss: 9.5779018CurrentTrain: epoch  4, batch     0 | loss: 8.0362825CurrentTrain: epoch  4, batch     1 | loss: 7.5391669CurrentTrain: epoch  4, batch     2 | loss: 6.5343275CurrentTrain: epoch  4, batch     3 | loss: 6.7070751CurrentTrain: epoch  4, batch     4 | loss: 7.7809501CurrentTrain: epoch  4, batch     5 | loss: 6.8086882CurrentTrain: epoch  4, batch     6 | loss: 7.3066688CurrentTrain: epoch  4, batch     7 | loss: 7.0686197CurrentTrain: epoch  4, batch     8 | loss: 7.7457385CurrentTrain: epoch  4, batch     9 | loss: 6.1021023CurrentTrain: epoch  4, batch    10 | loss: 6.6501226CurrentTrain: epoch  4, batch    11 | loss: 6.5833974CurrentTrain: epoch  4, batch    12 | loss: 6.6483355CurrentTrain: epoch  4, batch    13 | loss: 6.0736117CurrentTrain: epoch  4, batch    14 | loss: 7.1604290CurrentTrain: epoch  4, batch    15 | loss: 6.3716025CurrentTrain: epoch  4, batch    16 | loss: 6.5120459CurrentTrain: epoch  4, batch    17 | loss: 6.2961874CurrentTrain: epoch  4, batch    18 | loss: 9.5749083CurrentTrain: epoch  4, batch    19 | loss: 7.2541556CurrentTrain: epoch  4, batch    20 | loss: 6.7031150CurrentTrain: epoch  4, batch    21 | loss: 6.4568796CurrentTrain: epoch  4, batch    22 | loss: 6.4469633CurrentTrain: epoch  4, batch    23 | loss: 5.9371333CurrentTrain: epoch  4, batch    24 | loss: 7.4377108CurrentTrain: epoch  4, batch    25 | loss: 6.0878577CurrentTrain: epoch  4, batch    26 | loss: 8.8283272CurrentTrain: epoch  4, batch    27 | loss: 7.5392108CurrentTrain: epoch  4, batch    28 | loss: 6.6627502CurrentTrain: epoch  4, batch    29 | loss: 6.2304492CurrentTrain: epoch  4, batch    30 | loss: 6.8215284CurrentTrain: epoch  4, batch    31 | loss: 7.2750053CurrentTrain: epoch  4, batch    32 | loss: 8.0607519CurrentTrain: epoch  4, batch    33 | loss: 8.4244184CurrentTrain: epoch  4, batch    34 | loss: 6.9671030CurrentTrain: epoch  4, batch    35 | loss: 7.5350780CurrentTrain: epoch  4, batch    36 | loss: 7.4794374CurrentTrain: epoch  4, batch    37 | loss: 8.7395954CurrentTrain: epoch  5, batch     0 | loss: 6.5928192CurrentTrain: epoch  5, batch     1 | loss: 6.0842113CurrentTrain: epoch  5, batch     2 | loss: 6.2739496CurrentTrain: epoch  5, batch     3 | loss: 6.2475066CurrentTrain: epoch  5, batch     4 | loss: 6.9570317CurrentTrain: epoch  5, batch     5 | loss: 6.1018157CurrentTrain: epoch  5, batch     6 | loss: 6.9202981CurrentTrain: epoch  5, batch     7 | loss: 6.9442101CurrentTrain: epoch  5, batch     8 | loss: 6.3576694CurrentTrain: epoch  5, batch     9 | loss: 5.8815284CurrentTrain: epoch  5, batch    10 | loss: 7.7999024CurrentTrain: epoch  5, batch    11 | loss: 6.3438387CurrentTrain: epoch  5, batch    12 | loss: 5.9373217CurrentTrain: epoch  5, batch    13 | loss: 7.0815167CurrentTrain: epoch  5, batch    14 | loss: 6.4591656CurrentTrain: epoch  5, batch    15 | loss: 6.3254800CurrentTrain: epoch  5, batch    16 | loss: 7.0614986CurrentTrain: epoch  5, batch    17 | loss: 5.9931736CurrentTrain: epoch  5, batch    18 | loss: 5.1856189CurrentTrain: epoch  5, batch    19 | loss: 6.0742812CurrentTrain: epoch  5, batch    20 | loss: 5.8817148CurrentTrain: epoch  5, batch    21 | loss: 6.0765915CurrentTrain: epoch  5, batch    22 | loss: 6.3980885CurrentTrain: epoch  5, batch    23 | loss: 6.6815410CurrentTrain: epoch  5, batch    24 | loss: 5.7351713CurrentTrain: epoch  5, batch    25 | loss: 5.8106661CurrentTrain: epoch  5, batch    26 | loss: 5.6367311CurrentTrain: epoch  5, batch    27 | loss: 7.0301399CurrentTrain: epoch  5, batch    28 | loss: 6.7916551CurrentTrain: epoch  5, batch    29 | loss: 6.7907753CurrentTrain: epoch  5, batch    30 | loss: 5.8661222CurrentTrain: epoch  5, batch    31 | loss: 6.2104783CurrentTrain: epoch  5, batch    32 | loss: 6.8773508CurrentTrain: epoch  5, batch    33 | loss: 6.5429683CurrentTrain: epoch  5, batch    34 | loss: 5.9322271CurrentTrain: epoch  5, batch    35 | loss: 8.1015511CurrentTrain: epoch  5, batch    36 | loss: 7.4536800CurrentTrain: epoch  5, batch    37 | loss: 6.2203226CurrentTrain: epoch  6, batch     0 | loss: 7.2879457CurrentTrain: epoch  6, batch     1 | loss: 6.0624251CurrentTrain: epoch  6, batch     2 | loss: 5.6675339CurrentTrain: epoch  6, batch     3 | loss: 7.4084568CurrentTrain: epoch  6, batch     4 | loss: 6.3442507CurrentTrain: epoch  6, batch     5 | loss: 5.3738279CurrentTrain: epoch  6, batch     6 | loss: 5.4747925CurrentTrain: epoch  6, batch     7 | loss: 5.5880466CurrentTrain: epoch  6, batch     8 | loss: 6.1178498CurrentTrain: epoch  6, batch     9 | loss: 6.5438762CurrentTrain: epoch  6, batch    10 | loss: 5.8067546CurrentTrain: epoch  6, batch    11 | loss: 5.9018707CurrentTrain: epoch  6, batch    12 | loss: 5.4679675CurrentTrain: epoch  6, batch    13 | loss: 5.2596202CurrentTrain: epoch  6, batch    14 | loss: 5.8181987CurrentTrain: epoch  6, batch    15 | loss: 6.5657635CurrentTrain: epoch  6, batch    16 | loss: 5.3967404CurrentTrain: epoch  6, batch    17 | loss: 5.8514915CurrentTrain: epoch  6, batch    18 | loss: 5.3433738CurrentTrain: epoch  6, batch    19 | loss: 6.1077986CurrentTrain: epoch  6, batch    20 | loss: 6.6998243CurrentTrain: epoch  6, batch    21 | loss: 5.4193568CurrentTrain: epoch  6, batch    22 | loss: 6.9271002CurrentTrain: epoch  6, batch    23 | loss: 5.6041417CurrentTrain: epoch  6, batch    24 | loss: 6.1802559CurrentTrain: epoch  6, batch    25 | loss: 5.2438693CurrentTrain: epoch  6, batch    26 | loss: 5.7837071CurrentTrain: epoch  6, batch    27 | loss: 5.4835892CurrentTrain: epoch  6, batch    28 | loss: 5.7740488CurrentTrain: epoch  6, batch    29 | loss: 5.8313522CurrentTrain: epoch  6, batch    30 | loss: 6.0957227CurrentTrain: epoch  6, batch    31 | loss: 6.0516782CurrentTrain: epoch  6, batch    32 | loss: 5.7699289CurrentTrain: epoch  6, batch    33 | loss: 6.0926881CurrentTrain: epoch  6, batch    34 | loss: 5.7347617CurrentTrain: epoch  6, batch    35 | loss: 5.5047350CurrentTrain: epoch  6, batch    36 | loss: 6.7765532CurrentTrain: epoch  6, batch    37 | loss: 5.5686903CurrentTrain: epoch  7, batch     0 | loss: 5.6964569CurrentTrain: epoch  7, batch     1 | loss: 5.8710427CurrentTrain: epoch  7, batch     2 | loss: 5.6507826CurrentTrain: epoch  7, batch     3 | loss: 6.2047048CurrentTrain: epoch  7, batch     4 | loss: 5.9878817CurrentTrain: epoch  7, batch     5 | loss: 5.9627914CurrentTrain: epoch  7, batch     6 | loss: 5.3897362CurrentTrain: epoch  7, batch     7 | loss: 6.2356925CurrentTrain: epoch  7, batch     8 | loss: 6.0219102CurrentTrain: epoch  7, batch     9 | loss: 5.1035604CurrentTrain: epoch  7, batch    10 | loss: 5.4251943CurrentTrain: epoch  7, batch    11 | loss: 5.2826757CurrentTrain: epoch  7, batch    12 | loss: 5.3496337CurrentTrain: epoch  7, batch    13 | loss: 5.5937104CurrentTrain: epoch  7, batch    14 | loss: 5.7862973CurrentTrain: epoch  7, batch    15 | loss: 5.1809163CurrentTrain: epoch  7, batch    16 | loss: 5.2291203CurrentTrain: epoch  7, batch    17 | loss: 5.4327145CurrentTrain: epoch  7, batch    18 | loss: 5.2845926CurrentTrain: epoch  7, batch    19 | loss: 5.3975043CurrentTrain: epoch  7, batch    20 | loss: 5.6865497CurrentTrain: epoch  7, batch    21 | loss: 5.2698035CurrentTrain: epoch  7, batch    22 | loss: 5.0967836CurrentTrain: epoch  7, batch    23 | loss: 6.2665448CurrentTrain: epoch  7, batch    24 | loss: 6.3098826CurrentTrain: epoch  7, batch    25 | loss: 5.9524283CurrentTrain: epoch  7, batch    26 | loss: 5.1113272CurrentTrain: epoch  7, batch    27 | loss: 5.3382649CurrentTrain: epoch  7, batch    28 | loss: 5.2393055CurrentTrain: epoch  7, batch    29 | loss: 5.1994309CurrentTrain: epoch  7, batch    30 | loss: 5.0820732CurrentTrain: epoch  7, batch    31 | loss: 5.1770301CurrentTrain: epoch  7, batch    32 | loss: 5.1951241CurrentTrain: epoch  7, batch    33 | loss: 5.5347462CurrentTrain: epoch  7, batch    34 | loss: 6.2134008CurrentTrain: epoch  7, batch    35 | loss: 5.2845111CurrentTrain: epoch  7, batch    36 | loss: 5.2785606CurrentTrain: epoch  7, batch    37 | loss: 5.6973414CurrentTrain: epoch  8, batch     0 | loss: 5.3579321CurrentTrain: epoch  8, batch     1 | loss: 5.6713233CurrentTrain: epoch  8, batch     2 | loss: 5.7499003CurrentTrain: epoch  8, batch     3 | loss: 5.9256706CurrentTrain: epoch  8, batch     4 | loss: 5.5272622CurrentTrain: epoch  8, batch     5 | loss: 5.0751786CurrentTrain: epoch  8, batch     6 | loss: 5.0593939CurrentTrain: epoch  8, batch     7 | loss: 5.2430553CurrentTrain: epoch  8, batch     8 | loss: 5.1644630CurrentTrain: epoch  8, batch     9 | loss: 5.6080894CurrentTrain: epoch  8, batch    10 | loss: 5.5975432CurrentTrain: epoch  8, batch    11 | loss: 6.3065934CurrentTrain: epoch  8, batch    12 | loss: 5.4318471CurrentTrain: epoch  8, batch    13 | loss: 5.4135265CurrentTrain: epoch  8, batch    14 | loss: 5.2280779CurrentTrain: epoch  8, batch    15 | loss: 5.3108792CurrentTrain: epoch  8, batch    16 | loss: 5.7865305CurrentTrain: epoch  8, batch    17 | loss: 5.6927500CurrentTrain: epoch  8, batch    18 | loss: 5.2106662CurrentTrain: epoch  8, batch    19 | loss: 5.4334569CurrentTrain: epoch  8, batch    20 | loss: 6.2065172CurrentTrain: epoch  8, batch    21 | loss: 5.3185077CurrentTrain: epoch  8, batch    22 | loss: 5.3292928CurrentTrain: epoch  8, batch    23 | loss: 5.5181322CurrentTrain: epoch  8, batch    24 | loss: 5.9858418CurrentTrain: epoch  8, batch    25 | loss: 5.4813309CurrentTrain: epoch  8, batch    26 | loss: 5.2295356CurrentTrain: epoch  8, batch    27 | loss: 5.2496781CurrentTrain: epoch  8, batch    28 | loss: 4.9717479CurrentTrain: epoch  8, batch    29 | loss: 5.6411967CurrentTrain: epoch  8, batch    30 | loss: 5.3588943CurrentTrain: epoch  8, batch    31 | loss: 5.1084909CurrentTrain: epoch  8, batch    32 | loss: 5.3283315CurrentTrain: epoch  8, batch    33 | loss: 5.1754608CurrentTrain: epoch  8, batch    34 | loss: 6.6048493CurrentTrain: epoch  8, batch    35 | loss: 5.0127997CurrentTrain: epoch  8, batch    36 | loss: 5.4375973CurrentTrain: epoch  8, batch    37 | loss: 5.1446986CurrentTrain: epoch  9, batch     0 | loss: 5.6098423CurrentTrain: epoch  9, batch     1 | loss: 5.8264689CurrentTrain: epoch  9, batch     2 | loss: 5.2624235CurrentTrain: epoch  9, batch     3 | loss: 5.1275387CurrentTrain: epoch  9, batch     4 | loss: 5.1896076CurrentTrain: epoch  9, batch     5 | loss: 5.3513422CurrentTrain: epoch  9, batch     6 | loss: 5.2805243CurrentTrain: epoch  9, batch     7 | loss: 5.2591696CurrentTrain: epoch  9, batch     8 | loss: 5.2458024CurrentTrain: epoch  9, batch     9 | loss: 5.1657267CurrentTrain: epoch  9, batch    10 | loss: 5.0759521CurrentTrain: epoch  9, batch    11 | loss: 5.2195458CurrentTrain: epoch  9, batch    12 | loss: 5.0849504CurrentTrain: epoch  9, batch    13 | loss: 5.0602703CurrentTrain: epoch  9, batch    14 | loss: 5.0337343CurrentTrain: epoch  9, batch    15 | loss: 5.0867114CurrentTrain: epoch  9, batch    16 | loss: 4.9810414CurrentTrain: epoch  9, batch    17 | loss: 5.0703359CurrentTrain: epoch  9, batch    18 | loss: 5.2005520CurrentTrain: epoch  9, batch    19 | loss: 4.8511171CurrentTrain: epoch  9, batch    20 | loss: 5.4528394CurrentTrain: epoch  9, batch    21 | loss: 5.1968489CurrentTrain: epoch  9, batch    22 | loss: 4.8311901CurrentTrain: epoch  9, batch    23 | loss: 5.2499690CurrentTrain: epoch  9, batch    24 | loss: 6.1283617CurrentTrain: epoch  9, batch    25 | loss: 5.1158695CurrentTrain: epoch  9, batch    26 | loss: 4.9753928CurrentTrain: epoch  9, batch    27 | loss: 4.8666935CurrentTrain: epoch  9, batch    28 | loss: 5.0430207CurrentTrain: epoch  9, batch    29 | loss: 4.9734602CurrentTrain: epoch  9, batch    30 | loss: 4.9662023CurrentTrain: epoch  9, batch    31 | loss: 4.8001275CurrentTrain: epoch  9, batch    32 | loss: 4.8505092CurrentTrain: epoch  9, batch    33 | loss: 4.8264413CurrentTrain: epoch  9, batch    34 | loss: 5.1488185CurrentTrain: epoch  9, batch    35 | loss: 4.8851252CurrentTrain: epoch  9, batch    36 | loss: 4.8629265CurrentTrain: epoch  9, batch    37 | loss: 5.2144337
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.38%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 82.89%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.33%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.02%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.27%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.15%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 88.31%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.67%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 87.12%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.38%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 82.89%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.33%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.02%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.27%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.15%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 88.31%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.67%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 87.12%   
cur_acc_llm:  [0.8712121212121212]
his_acc_llm:  [0.8712121212121212]
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 84.38%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.23%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.60%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.20%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.26%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.95%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 88.51%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.67%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 87.12%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 84.38%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.23%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.60%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.20%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.26%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.95%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 88.51%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.67%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 87.12%   
cur_acc:  ['0.8712']
his_acc:  ['0.8712']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 6.0049133CurrentTrain: epoch  0, batch     1 | loss: 5.1932750CurrentTrain: epoch  1, batch     0 | loss: 4.8446798CurrentTrain: epoch  1, batch     1 | loss: 4.3079429CurrentTrain: epoch  2, batch     0 | loss: 3.9206271CurrentTrain: epoch  2, batch     1 | loss: 3.2708766CurrentTrain: epoch  3, batch     0 | loss: 3.4547138CurrentTrain: epoch  3, batch     1 | loss: 2.7963223CurrentTrain: epoch  4, batch     0 | loss: 2.8588352CurrentTrain: epoch  4, batch     1 | loss: 2.2313006CurrentTrain: epoch  5, batch     0 | loss: 2.1758909CurrentTrain: epoch  5, batch     1 | loss: 2.0136044CurrentTrain: epoch  6, batch     0 | loss: 2.0245318CurrentTrain: epoch  6, batch     1 | loss: 1.7705961CurrentTrain: epoch  7, batch     0 | loss: 1.7832072CurrentTrain: epoch  7, batch     1 | loss: 1.8199340CurrentTrain: epoch  8, batch     0 | loss: 1.8037018CurrentTrain: epoch  8, batch     1 | loss: 1.7389156CurrentTrain: epoch  9, batch     0 | loss: 1.7353352CurrentTrain: epoch  9, batch     1 | loss: 1.7285717
Mixup data size:  60
MixupTrain:  epoch  0, batch     0 | loss: 7.6817770MixupTrain:  epoch  0, batch     1 | loss: 6.8836405MixupTrain:  epoch  0, batch     2 | loss: 5.6843375MixupTrain:  epoch  0, batch     3 | loss: 4.4394665
MemoryTrain:  epoch  0, batch     0 | loss: 4.1064110MemoryTrain:  epoch  1, batch     0 | loss: 4.9216089MemoryTrain:  epoch  2, batch     0 | loss: 3.9035292MemoryTrain:  epoch  3, batch     0 | loss: 3.5980749MemoryTrain:  epoch  4, batch     0 | loss: 2.4928927MemoryTrain:  epoch  5, batch     0 | loss: 1.8358272MemoryTrain:  epoch  6, batch     0 | loss: 1.5991999MemoryTrain:  epoch  7, batch     0 | loss: 1.0073284MemoryTrain:  epoch  8, batch     0 | loss: 0.4665090MemoryTrain:  epoch  9, batch     0 | loss: 0.2786434
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 8.0921946CurrentTrain: epoch  0, batch     1 | loss: 8.1043215CurrentTrain: epoch  1, batch     0 | loss: 8.0689173CurrentTrain: epoch  1, batch     1 | loss: 7.0632935CurrentTrain: epoch  2, batch     0 | loss: 6.4377618CurrentTrain: epoch  2, batch     1 | loss: 6.9871807CurrentTrain: epoch  3, batch     0 | loss: 5.8354015CurrentTrain: epoch  3, batch     1 | loss: 6.0791335CurrentTrain: epoch  4, batch     0 | loss: 6.1003814CurrentTrain: epoch  4, batch     1 | loss: 5.0958691CurrentTrain: epoch  5, batch     0 | loss: 5.2324910CurrentTrain: epoch  5, batch     1 | loss: 5.0704064CurrentTrain: epoch  6, batch     0 | loss: 4.3478637CurrentTrain: epoch  6, batch     1 | loss: 4.5539846CurrentTrain: epoch  7, batch     0 | loss: 4.8110833CurrentTrain: epoch  7, batch     1 | loss: 3.1435435CurrentTrain: epoch  8, batch     0 | loss: 4.1156387CurrentTrain: epoch  8, batch     1 | loss: 3.3991151CurrentTrain: epoch  9, batch     0 | loss: 3.8864925CurrentTrain: epoch  9, batch     1 | loss: 3.1124225
Mixup data size:  60
MixupTrain:  epoch  0, batch     0 | loss: 6.7585376MixupTrain:  epoch  0, batch     2 | loss: 7.4998824
MemoryTrain:  epoch  0, batch     0 | loss: 2.6384785MemoryTrain:  epoch  1, batch     0 | loss: 2.7769954MemoryTrain:  epoch  2, batch     0 | loss: 2.3310461MemoryTrain:  epoch  3, batch     0 | loss: 1.7829200MemoryTrain:  epoch  4, batch     0 | loss: 1.8312569MemoryTrain:  epoch  5, batch     0 | loss: 1.3071866MemoryTrain:  epoch  6, batch     0 | loss: 1.0116651MemoryTrain:  epoch  7, batch     0 | loss: 0.8258785MemoryTrain:  epoch  8, batch     0 | loss: 0.8564153MemoryTrain:  epoch  9, batch     0 | loss: 0.7044948
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 92.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 89.93%   
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 35.42%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 37.50%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 41.67%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 45.54%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 51.56%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 56.94%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 60.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 63.64%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 63.94%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 61.61%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 62.92%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 64.06%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 64.71%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 65.97%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 67.76%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 70.24%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 71.59%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 72.55%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 73.70%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 74.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.72%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 76.16%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.01%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 77.59%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 78.63%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 79.30%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 79.92%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 80.33%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 80.54%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 81.08%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 81.42%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 81.58%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 82.05%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.50%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 81.71%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 81.85%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 82.27%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 82.53%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 82.92%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 83.29%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 83.64%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 83.98%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 84.31%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 83.88%   
cur_acc_llm:  [0.8712121212121212, 0.8993055555555556]
his_acc_llm:  [0.8712121212121212, 0.83875]
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 18.75%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 20.83%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 15.62%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 20.00%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 21.88%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 28.57%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 34.38%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 38.19%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 43.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 46.59%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 51.04%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 54.81%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 58.04%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 60.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 63.28%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 65.44%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 63.19%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 57.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 59.38%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 63.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 73.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 76.14%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 78.37%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 77.68%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 75.78%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 75.74%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 74.34%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 76.19%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 77.27%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 78.26%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.77%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.92%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 82.54%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 82.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.47%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.98%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 83.14%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 80.88%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 79.29%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 77.43%   [EVAL] batch:   36 | acc: 25.00%,  total acc: 76.01%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 74.84%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 74.20%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 74.53%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 73.93%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 74.40%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 74.56%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 75.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 76.09%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 76.60%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 77.55%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 77.25%   
cur_acc:  ['0.8712', '0.6319']
his_acc:  ['0.8712', '0.7725']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 6.1011758CurrentTrain: epoch  0, batch     1 | loss: 4.8720026CurrentTrain: epoch  1, batch     0 | loss: 4.5111284CurrentTrain: epoch  1, batch     1 | loss: 4.8399863CurrentTrain: epoch  2, batch     0 | loss: 3.9282753CurrentTrain: epoch  2, batch     1 | loss: 3.8477042CurrentTrain: epoch  3, batch     0 | loss: 3.5455928CurrentTrain: epoch  3, batch     1 | loss: 3.1892831CurrentTrain: epoch  4, batch     0 | loss: 2.9571915CurrentTrain: epoch  4, batch     1 | loss: 3.7906144CurrentTrain: epoch  5, batch     0 | loss: 3.2463107CurrentTrain: epoch  5, batch     1 | loss: 2.6993611CurrentTrain: epoch  6, batch     0 | loss: 3.0099750CurrentTrain: epoch  6, batch     1 | loss: 2.4816251CurrentTrain: epoch  7, batch     0 | loss: 2.5764740CurrentTrain: epoch  7, batch     1 | loss: 2.4066653CurrentTrain: epoch  8, batch     0 | loss: 2.1080539CurrentTrain: epoch  8, batch     1 | loss: 2.3938491CurrentTrain: epoch  9, batch     0 | loss: 2.0717516CurrentTrain: epoch  9, batch     1 | loss: 2.1089678
Mixup data size:  69
MixupTrain:  epoch  0, batch     2 | loss: 4.6079578
MemoryTrain:  epoch  0, batch     0 | loss: 2.4747176MemoryTrain:  epoch  1, batch     0 | loss: 2.5187259MemoryTrain:  epoch  2, batch     0 | loss: 2.0393705MemoryTrain:  epoch  3, batch     0 | loss: 1.4922452MemoryTrain:  epoch  4, batch     0 | loss: 1.1300895MemoryTrain:  epoch  5, batch     0 | loss: 0.5673474MemoryTrain:  epoch  6, batch     0 | loss: 0.3570080MemoryTrain:  epoch  7, batch     0 | loss: 0.8597956MemoryTrain:  epoch  8, batch     0 | loss: 0.4666608MemoryTrain:  epoch  9, batch     0 | loss: 0.6391445
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 8.5413818CurrentTrain: epoch  0, batch     1 | loss: 7.8637557CurrentTrain: epoch  1, batch     0 | loss: 8.0184574CurrentTrain: epoch  1, batch     1 | loss: 6.4877076CurrentTrain: epoch  2, batch     0 | loss: 6.9469013CurrentTrain: epoch  2, batch     1 | loss: 6.7437949CurrentTrain: epoch  3, batch     0 | loss: 6.1512785CurrentTrain: epoch  3, batch     1 | loss: 6.9060097CurrentTrain: epoch  4, batch     0 | loss: 6.0578732CurrentTrain: epoch  4, batch     1 | loss: 5.1972399CurrentTrain: epoch  5, batch     0 | loss: 5.3251934CurrentTrain: epoch  5, batch     1 | loss: 4.1731987CurrentTrain: epoch  6, batch     0 | loss: 4.6472230CurrentTrain: epoch  6, batch     1 | loss: 4.6325364CurrentTrain: epoch  7, batch     0 | loss: 4.5638571CurrentTrain: epoch  7, batch     1 | loss: 4.5517178CurrentTrain: epoch  8, batch     0 | loss: 4.5701647CurrentTrain: epoch  8, batch     1 | loss: 3.5293589CurrentTrain: epoch  9, batch     0 | loss: 4.1048255CurrentTrain: epoch  9, batch     1 | loss: 3.0282128
Mixup data size:  68
MixupTrain:  epoch  0, batch     0 | loss: 8.7858994MixupTrain:  epoch  0, batch     1 | loss: 5.5701743MixupTrain:  epoch  0, batch     2 | loss: 5.5515080MixupTrain:  epoch  0, batch     3 | loss: 5.9214936MixupTrain:  epoch  0, batch     4 | loss: 3.5499832
MemoryTrain:  epoch  0, batch     0 | loss: 2.0960512MemoryTrain:  epoch  1, batch     0 | loss: 2.2133818MemoryTrain:  epoch  2, batch     0 | loss: 1.7980437MemoryTrain:  epoch  3, batch     0 | loss: 1.6376519MemoryTrain:  epoch  4, batch     0 | loss: 1.3320282MemoryTrain:  epoch  5, batch     0 | loss: 1.1436998MemoryTrain:  epoch  6, batch     0 | loss: 1.1045539MemoryTrain:  epoch  7, batch     0 | loss: 1.0794572MemoryTrain:  epoch  8, batch     0 | loss: 1.0007124MemoryTrain:  epoch  9, batch     0 | loss: 0.8855624
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 18.75%,  total acc: 73.12%   [EVAL] batch:   10 | acc: 6.25%,  total acc: 67.05%   [EVAL] batch:   11 | acc: 25.00%,  total acc: 63.54%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 60.27%   
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 39.58%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 40.62%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 37.50%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 39.58%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 40.18%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 47.66%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 52.78%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 55.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 57.95%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 59.90%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 59.13%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 56.70%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 57.50%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 57.81%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 57.72%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 57.99%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 58.88%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 59.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 61.61%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 63.35%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 64.67%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 66.15%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 67.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 69.68%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 70.76%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 71.77%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 72.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 73.39%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 75.55%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 75.89%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 76.22%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 76.18%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 76.15%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 76.28%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 76.88%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 75.91%   [EVAL] batch:   41 | acc: 25.00%,  total acc: 74.70%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 74.13%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 74.01%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 74.58%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 75.14%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 75.66%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 76.17%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 76.66%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 77.12%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 77.33%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 77.64%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 77.95%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 78.36%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 78.79%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 78.40%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 77.80%   [EVAL] batch:   58 | acc: 25.00%,  total acc: 76.91%   [EVAL] batch:   59 | acc: 12.50%,  total acc: 75.83%   [EVAL] batch:   60 | acc: 18.75%,  total acc: 74.90%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 74.09%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 73.81%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 72.85%   
cur_acc_llm:  [0.8712121212121212, 0.8993055555555556, 0.6026785714285714]
his_acc_llm:  [0.8712121212121212, 0.83875, 0.728515625]
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 75.69%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 73.30%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 71.35%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 71.63%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 66.52%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 47.92%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 42.19%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 45.83%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 50.89%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 57.03%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 61.81%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 64.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 67.61%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 69.27%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 71.15%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 70.98%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 71.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 70.59%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 70.14%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 69.74%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 70.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 73.30%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 74.46%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 75.52%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 76.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.40%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 78.01%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 79.53%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.65%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 80.49%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 78.49%   [EVAL] batch:   34 | acc: 18.75%,  total acc: 76.79%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 75.17%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 73.99%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 72.86%   [EVAL] batch:   38 | acc: 43.75%,  total acc: 72.12%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 72.50%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 71.95%   [EVAL] batch:   41 | acc: 18.75%,  total acc: 70.68%   [EVAL] batch:   42 | acc: 18.75%,  total acc: 69.48%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 69.46%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 70.79%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 71.41%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 72.01%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 72.58%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 73.12%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 73.28%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 73.32%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 73.35%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 73.61%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 73.64%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 73.55%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 73.36%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 73.28%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 73.20%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 73.23%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 72.75%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 72.68%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 72.22%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 71.09%   
cur_acc:  ['0.8712', '0.6319', '0.6652']
his_acc:  ['0.8712', '0.7725', '0.7109']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 4.3207183CurrentTrain: epoch  0, batch     1 | loss: 4.3315682CurrentTrain: epoch  1, batch     0 | loss: 3.0659609CurrentTrain: epoch  1, batch     1 | loss: 2.2676640CurrentTrain: epoch  2, batch     0 | loss: 2.2672923CurrentTrain: epoch  2, batch     1 | loss: 2.1485682CurrentTrain: epoch  3, batch     0 | loss: 2.1377008CurrentTrain: epoch  3, batch     1 | loss: 1.9098536CurrentTrain: epoch  4, batch     0 | loss: 2.0278249CurrentTrain: epoch  4, batch     1 | loss: 1.9410648CurrentTrain: epoch  5, batch     0 | loss: 1.9985824CurrentTrain: epoch  5, batch     1 | loss: 2.5591545CurrentTrain: epoch  6, batch     0 | loss: 2.1339974CurrentTrain: epoch  6, batch     1 | loss: 1.9873662CurrentTrain: epoch  7, batch     0 | loss: 1.8522456CurrentTrain: epoch  7, batch     1 | loss: 1.7906376CurrentTrain: epoch  8, batch     0 | loss: 1.7490921CurrentTrain: epoch  8, batch     1 | loss: 1.8034970CurrentTrain: epoch  9, batch     0 | loss: 1.7715017CurrentTrain: epoch  9, batch     1 | loss: 1.7328852
Mixup data size:  80

MemoryTrain:  epoch  0, batch     0 | loss: 2.6996284MemoryTrain:  epoch  0, batch     1 | loss: 1.2596558MemoryTrain:  epoch  1, batch     0 | loss: 2.8336992MemoryTrain:  epoch  1, batch     1 | loss: 3.3745143MemoryTrain:  epoch  2, batch     0 | loss: 1.7366948MemoryTrain:  epoch  2, batch     1 | loss: 1.6599683MemoryTrain:  epoch  3, batch     0 | loss: 1.6354687MemoryTrain:  epoch  3, batch     1 | loss: 0.3975129MemoryTrain:  epoch  4, batch     0 | loss: 1.2827392MemoryTrain:  epoch  4, batch     1 | loss: 0.5950295MemoryTrain:  epoch  5, batch     0 | loss: 0.6065108MemoryTrain:  epoch  5, batch     1 | loss: 0.1629296MemoryTrain:  epoch  6, batch     0 | loss: 0.4258901MemoryTrain:  epoch  6, batch     1 | loss: 0.5640056MemoryTrain:  epoch  7, batch     0 | loss: 0.4499518MemoryTrain:  epoch  7, batch     1 | loss: 0.6758881MemoryTrain:  epoch  8, batch     0 | loss: 0.4811706MemoryTrain:  epoch  8, batch     1 | loss: 0.1653616MemoryTrain:  epoch  9, batch     0 | loss: 0.3090066MemoryTrain:  epoch  9, batch     1 | loss: 0.1602977
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 6.8347273CurrentTrain: epoch  0, batch     1 | loss: 6.2736173CurrentTrain: epoch  1, batch     0 | loss: 5.0835505CurrentTrain: epoch  1, batch     1 | loss: 5.0393653CurrentTrain: epoch  2, batch     0 | loss: 4.8402643CurrentTrain: epoch  2, batch     1 | loss: 3.3827038CurrentTrain: epoch  3, batch     0 | loss: 4.1323647CurrentTrain: epoch  3, batch     1 | loss: 3.6059356CurrentTrain: epoch  4, batch     0 | loss: 3.5411892CurrentTrain: epoch  4, batch     1 | loss: 3.5440378CurrentTrain: epoch  5, batch     0 | loss: 3.4121017CurrentTrain: epoch  5, batch     1 | loss: 3.6954012CurrentTrain: epoch  6, batch     0 | loss: 2.9936724CurrentTrain: epoch  6, batch     1 | loss: 2.7365799CurrentTrain: epoch  7, batch     0 | loss: 2.8172104CurrentTrain: epoch  7, batch     1 | loss: 2.7187076CurrentTrain: epoch  8, batch     0 | loss: 2.6544843CurrentTrain: epoch  8, batch     1 | loss: 2.5123382CurrentTrain: epoch  9, batch     0 | loss: 2.4293208CurrentTrain: epoch  9, batch     1 | loss: 2.4003749
Mixup data size:  80
MixupTrain:  epoch  0, batch     0 | loss: 4.8427610MixupTrain:  epoch  0, batch     1 | loss: 4.7968920MixupTrain:  epoch  0, batch     4 | loss: 5.1531003
MemoryTrain:  epoch  0, batch     0 | loss: 1.4658608MemoryTrain:  epoch  0, batch     1 | loss: 1.0624897MemoryTrain:  epoch  1, batch     0 | loss: 2.2603602MemoryTrain:  epoch  1, batch     1 | loss: 0.5370480MemoryTrain:  epoch  2, batch     0 | loss: 1.8370998MemoryTrain:  epoch  2, batch     1 | loss: 0.9691580MemoryTrain:  epoch  3, batch     0 | loss: 1.5316200MemoryTrain:  epoch  3, batch     1 | loss: 0.4367022MemoryTrain:  epoch  4, batch     0 | loss: 1.2354976MemoryTrain:  epoch  4, batch     1 | loss: 0.1616138MemoryTrain:  epoch  5, batch     0 | loss: 1.0921739MemoryTrain:  epoch  5, batch     1 | loss: 0.2583609MemoryTrain:  epoch  6, batch     0 | loss: 1.0472035MemoryTrain:  epoch  6, batch     1 | loss: 0.2731001MemoryTrain:  epoch  7, batch     0 | loss: 0.8703845MemoryTrain:  epoch  7, batch     1 | loss: 0.4599251MemoryTrain:  epoch  8, batch     0 | loss: 0.6197898MemoryTrain:  epoch  8, batch     1 | loss: 0.4414766MemoryTrain:  epoch  9, batch     0 | loss: 0.6522143MemoryTrain:  epoch  9, batch     1 | loss: 0.1504081
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 95.83%   [EVAL] batch:    9 | acc: 25.00%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 86.36%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 86.61%   
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 48.44%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 52.50%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 53.12%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 51.79%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 57.81%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 61.11%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 61.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 63.07%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 63.54%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 61.54%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 58.93%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 59.58%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 59.77%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 60.29%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 60.07%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 61.18%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 64.29%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 65.91%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 67.12%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 68.49%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 70.91%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 71.76%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 72.77%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 73.71%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 74.38%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 76.33%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 76.65%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 76.96%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 77.26%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 77.03%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 76.97%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 77.24%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 77.66%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 76.83%   [EVAL] batch:   41 | acc: 25.00%,  total acc: 75.60%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 75.15%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 75.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 76.09%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 76.60%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 77.55%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 77.75%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 77.94%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 78.37%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 78.66%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 78.94%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 79.32%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 79.24%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 79.20%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 79.13%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 78.75%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 78.48%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 78.33%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 77.98%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 78.03%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 78.37%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 78.69%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 79.01%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 79.32%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 79.62%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 79.91%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 80.19%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 80.30%   [EVAL] batch:   72 | acc: 18.75%,  total acc: 79.45%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 78.97%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 79.25%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 79.36%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 79.63%   [EVAL] batch:   77 | acc: 6.25%,  total acc: 78.69%   
cur_acc_llm:  [0.8712121212121212, 0.8993055555555556, 0.6026785714285714, 0.8660714285714286]
his_acc_llm:  [0.8712121212121212, 0.83875, 0.728515625, 0.7868589743589743]
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 99.22%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 97.92%   [EVAL] batch:    9 | acc: 25.00%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 18.75%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 80.73%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 77.88%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 75.45%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 45.31%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 47.50%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 47.92%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 54.46%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 60.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 66.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 69.89%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 71.35%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 72.12%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 70.09%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 69.58%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 68.36%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 68.40%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 68.09%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 70.24%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 71.59%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 72.83%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 73.96%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.96%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 76.62%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.46%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 78.23%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.44%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.08%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 79.17%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 77.02%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 75.00%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 72.92%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 71.11%   [EVAL] batch:   37 | acc: 18.75%,  total acc: 69.74%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 68.91%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 69.53%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 69.05%   [EVAL] batch:   41 | acc: 12.50%,  total acc: 67.71%   [EVAL] batch:   42 | acc: 18.75%,  total acc: 66.57%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 66.62%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 67.36%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 68.07%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 70.03%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 70.62%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 70.71%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 70.91%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 70.99%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 71.41%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 71.59%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 71.54%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 71.38%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 71.55%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 71.50%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 71.56%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 71.21%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 71.17%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 70.73%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 70.70%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 71.59%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 72.01%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 72.43%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 72.83%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 73.21%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 73.50%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 73.87%   [EVAL] batch:   72 | acc: 31.25%,  total acc: 73.29%   [EVAL] batch:   73 | acc: 18.75%,  total acc: 72.55%   [EVAL] batch:   74 | acc: 37.50%,  total acc: 72.08%   [EVAL] batch:   75 | acc: 43.75%,  total acc: 71.71%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 71.59%   [EVAL] batch:   77 | acc: 0.00%,  total acc: 70.67%   
cur_acc:  ['0.8712', '0.6319', '0.6652', '0.7545']
his_acc:  ['0.8712', '0.7725', '0.7109', '0.7067']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 4.4828615CurrentTrain: epoch  0, batch     1 | loss: 5.2306275CurrentTrain: epoch  1, batch     0 | loss: 3.1221633CurrentTrain: epoch  1, batch     1 | loss: 2.6636949CurrentTrain: epoch  2, batch     0 | loss: 2.2964497CurrentTrain: epoch  2, batch     1 | loss: 3.5194180CurrentTrain: epoch  3, batch     0 | loss: 2.9838266CurrentTrain: epoch  3, batch     1 | loss: 1.8821121CurrentTrain: epoch  4, batch     0 | loss: 1.8490129CurrentTrain: epoch  4, batch     1 | loss: 1.9842894CurrentTrain: epoch  5, batch     0 | loss: 1.8756108CurrentTrain: epoch  5, batch     1 | loss: 1.8620050CurrentTrain: epoch  6, batch     0 | loss: 1.8319886CurrentTrain: epoch  6, batch     1 | loss: 1.8300182CurrentTrain: epoch  7, batch     0 | loss: 1.8082118CurrentTrain: epoch  7, batch     1 | loss: 1.7575790CurrentTrain: epoch  8, batch     0 | loss: 1.7787931CurrentTrain: epoch  8, batch     1 | loss: 1.7258303CurrentTrain: epoch  9, batch     0 | loss: 1.7327092CurrentTrain: epoch  9, batch     1 | loss: 1.7223353
Mixup data size:  92
MixupTrain:  epoch  0, batch     2 | loss: 3.8546219MixupTrain:  epoch  0, batch     4 | loss: 4.4859467
MemoryTrain:  epoch  0, batch     0 | loss: 1.6754951MemoryTrain:  epoch  0, batch     1 | loss: 1.4540323MemoryTrain:  epoch  1, batch     0 | loss: 2.1542010MemoryTrain:  epoch  1, batch     1 | loss: 1.0208253MemoryTrain:  epoch  2, batch     0 | loss: 1.4777734MemoryTrain:  epoch  2, batch     1 | loss: 0.1902606MemoryTrain:  epoch  3, batch     0 | loss: 0.5807126MemoryTrain:  epoch  3, batch     1 | loss: 0.6398260MemoryTrain:  epoch  4, batch     0 | loss: 0.3129553MemoryTrain:  epoch  4, batch     1 | loss: 0.2893905MemoryTrain:  epoch  5, batch     0 | loss: 0.5636260MemoryTrain:  epoch  5, batch     1 | loss: 0.1949725MemoryTrain:  epoch  6, batch     0 | loss: 0.2476404MemoryTrain:  epoch  6, batch     1 | loss: 0.1328496MemoryTrain:  epoch  7, batch     0 | loss: 0.1712390MemoryTrain:  epoch  7, batch     1 | loss: 0.0388271MemoryTrain:  epoch  8, batch     0 | loss: 0.0928148MemoryTrain:  epoch  8, batch     1 | loss: 0.3374266MemoryTrain:  epoch  9, batch     0 | loss: 0.0814665MemoryTrain:  epoch  9, batch     1 | loss: 0.0365283
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 5.4834547CurrentTrain: epoch  0, batch     1 | loss: 5.7878065CurrentTrain: epoch  1, batch     0 | loss: 4.4204850CurrentTrain: epoch  1, batch     1 | loss: 3.7171917CurrentTrain: epoch  2, batch     0 | loss: 3.6745594CurrentTrain: epoch  2, batch     1 | loss: 3.0496297CurrentTrain: epoch  3, batch     0 | loss: 3.0792069CurrentTrain: epoch  3, batch     1 | loss: 2.6215630CurrentTrain: epoch  4, batch     0 | loss: 3.0276899CurrentTrain: epoch  4, batch     1 | loss: 2.5423758CurrentTrain: epoch  5, batch     0 | loss: 2.8256128CurrentTrain: epoch  5, batch     1 | loss: 2.3620608CurrentTrain: epoch  6, batch     0 | loss: 2.4129102CurrentTrain: epoch  6, batch     1 | loss: 2.3645482CurrentTrain: epoch  7, batch     0 | loss: 2.3633618CurrentTrain: epoch  7, batch     1 | loss: 2.0886898CurrentTrain: epoch  8, batch     0 | loss: 2.1370225CurrentTrain: epoch  8, batch     1 | loss: 2.0664375CurrentTrain: epoch  9, batch     0 | loss: 2.0481560CurrentTrain: epoch  9, batch     1 | loss: 2.1708724
Mixup data size:  90
MixupTrain:  epoch  0, batch     2 | loss: 4.6101681MixupTrain:  epoch  0, batch     3 | loss: 4.4051836MixupTrain:  epoch  0, batch     4 | loss: 4.0346217MixupTrain:  epoch  0, batch     5 | loss: 3.6629108
MemoryTrain:  epoch  0, batch     0 | loss: 1.9799573MemoryTrain:  epoch  0, batch     1 | loss: 0.4736702MemoryTrain:  epoch  1, batch     0 | loss: 1.8439500MemoryTrain:  epoch  1, batch     1 | loss: 1.6030188MemoryTrain:  epoch  2, batch     0 | loss: 1.0449784MemoryTrain:  epoch  2, batch     1 | loss: 1.2166809MemoryTrain:  epoch  3, batch     0 | loss: 0.9504322MemoryTrain:  epoch  3, batch     1 | loss: 0.8329606MemoryTrain:  epoch  4, batch     0 | loss: 0.8806359MemoryTrain:  epoch  4, batch     1 | loss: 0.5720298MemoryTrain:  epoch  5, batch     0 | loss: 0.8580325MemoryTrain:  epoch  5, batch     1 | loss: 0.4807355MemoryTrain:  epoch  6, batch     0 | loss: 0.8415984MemoryTrain:  epoch  6, batch     1 | loss: 0.5467615MemoryTrain:  epoch  7, batch     0 | loss: 0.8045681MemoryTrain:  epoch  7, batch     1 | loss: 0.3883440MemoryTrain:  epoch  8, batch     0 | loss: 0.5915340MemoryTrain:  epoch  8, batch     1 | loss: 0.4521455MemoryTrain:  epoch  9, batch     0 | loss: 0.3401144MemoryTrain:  epoch  9, batch     1 | loss: 0.3787452
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 74.31%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 73.30%   [EVAL] batch:   11 | acc: 0.00%,  total acc: 67.19%   [EVAL] batch:   12 | acc: 0.00%,  total acc: 62.02%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 61.25%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 61.46%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 63.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 73.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 75.57%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 71.43%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 71.67%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 71.09%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 70.96%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 72.04%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 72.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 75.28%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 76.09%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 78.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.85%   [EVAL] batch:   26 | acc: 18.75%,  total acc: 76.62%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 73.88%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 71.34%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 68.96%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 66.73%   [EVAL] batch:   31 | acc: 0.00%,  total acc: 64.65%   [EVAL] batch:   32 | acc: 31.25%,  total acc: 63.64%   [EVAL] batch:   33 | acc: 37.50%,  total acc: 62.87%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 62.86%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 62.33%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 61.66%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 61.18%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 60.90%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 61.72%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 61.13%   [EVAL] batch:   41 | acc: 6.25%,  total acc: 59.82%   [EVAL] batch:   42 | acc: 25.00%,  total acc: 59.01%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 59.09%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 59.72%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 60.33%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 61.17%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 61.98%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 62.76%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 63.25%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 63.36%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 62.50%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 61.56%   [EVAL] batch:   53 | acc: 18.75%,  total acc: 60.76%   [EVAL] batch:   54 | acc: 12.50%,  total acc: 59.89%   [EVAL] batch:   55 | acc: 6.25%,  total acc: 58.93%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 58.99%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 59.16%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 59.22%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 59.06%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 58.61%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 58.67%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 58.73%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 58.89%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 59.52%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 60.13%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 60.73%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 61.31%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 61.87%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 62.41%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 62.94%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 63.37%   [EVAL] batch:   72 | acc: 31.25%,  total acc: 62.93%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 63.01%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 63.50%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 63.73%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 64.20%   [EVAL] batch:   77 | acc: 43.75%,  total acc: 63.94%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 63.92%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 63.75%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 63.89%   [EVAL] batch:   81 | acc: 100.00%,  total acc: 64.33%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 64.76%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 65.37%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 65.33%   [EVAL] batch:   86 | acc: 75.00%,  total acc: 65.45%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 65.41%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 64.68%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 63.96%   
cur_acc_llm:  [0.8712121212121212, 0.8993055555555556, 0.6026785714285714, 0.8660714285714286, 0.6201923076923077]
his_acc_llm:  [0.8712121212121212, 0.83875, 0.728515625, 0.7868589743589743, 0.6395833333333333]
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 64.58%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 61.72%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 60.42%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 61.25%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 64.58%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 63.46%   
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 52.50%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 52.08%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 57.14%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 71.59%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 72.92%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 73.08%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 70.98%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 69.92%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 70.22%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 69.79%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 69.41%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 70.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 71.43%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 72.73%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 73.91%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 76.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.92%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 77.55%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 78.35%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 79.09%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 79.38%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 79.64%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.27%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 78.98%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 76.65%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 74.46%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 72.40%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 70.44%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 68.75%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 67.79%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 68.44%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 67.99%   [EVAL] batch:   41 | acc: 18.75%,  total acc: 66.82%   [EVAL] batch:   42 | acc: 18.75%,  total acc: 65.70%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 66.39%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 66.98%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 67.69%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 68.10%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 69.38%   [EVAL] batch:   50 | acc: 50.00%,  total acc: 69.00%   [EVAL] batch:   51 | acc: 25.00%,  total acc: 68.15%   [EVAL] batch:   52 | acc: 25.00%,  total acc: 67.33%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 66.09%   [EVAL] batch:   54 | acc: 6.25%,  total acc: 65.00%   [EVAL] batch:   55 | acc: 6.25%,  total acc: 63.95%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 63.71%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 63.90%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 63.98%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 64.06%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 63.73%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 63.71%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 63.29%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 63.38%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 63.94%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 64.49%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 65.02%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 65.53%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 66.03%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 66.52%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 66.90%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 67.27%   [EVAL] batch:   72 | acc: 18.75%,  total acc: 66.61%   [EVAL] batch:   73 | acc: 18.75%,  total acc: 65.96%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 65.83%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 65.71%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 65.67%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 65.87%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 65.90%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 66.17%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 66.28%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 65.93%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 65.59%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 65.40%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 65.29%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 65.19%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 65.23%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 65.27%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 65.52%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 65.42%   
cur_acc:  ['0.8712', '0.6319', '0.6652', '0.7545', '0.6346']
his_acc:  ['0.8712', '0.7725', '0.7109', '0.7067', '0.6542']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 6.2213540CurrentTrain: epoch  0, batch     1 | loss: 5.8274617CurrentTrain: epoch  1, batch     0 | loss: 5.4436646CurrentTrain: epoch  1, batch     1 | loss: 4.6684780CurrentTrain: epoch  2, batch     0 | loss: 4.3950310CurrentTrain: epoch  2, batch     1 | loss: 4.3546786CurrentTrain: epoch  3, batch     0 | loss: 4.2129192CurrentTrain: epoch  3, batch     1 | loss: 2.8661149CurrentTrain: epoch  4, batch     0 | loss: 3.1335883CurrentTrain: epoch  4, batch     1 | loss: 3.9359720CurrentTrain: epoch  5, batch     0 | loss: 3.2767897CurrentTrain: epoch  5, batch     1 | loss: 3.2358768CurrentTrain: epoch  6, batch     0 | loss: 3.3711174CurrentTrain: epoch  6, batch     1 | loss: 2.8713973CurrentTrain: epoch  7, batch     0 | loss: 2.8499708CurrentTrain: epoch  7, batch     1 | loss: 2.9124844CurrentTrain: epoch  8, batch     0 | loss: 2.3267896CurrentTrain: epoch  8, batch     1 | loss: 2.6787000CurrentTrain: epoch  9, batch     0 | loss: 2.1766186CurrentTrain: epoch  9, batch     1 | loss: 2.6899469
Mixup data size:  101

MemoryTrain:  epoch  0, batch     0 | loss: 0.9514115MemoryTrain:  epoch  0, batch     1 | loss: 0.6666554MemoryTrain:  epoch  1, batch     0 | loss: 1.0033985MemoryTrain:  epoch  1, batch     1 | loss: 1.1357700MemoryTrain:  epoch  2, batch     0 | loss: 0.5979955MemoryTrain:  epoch  2, batch     1 | loss: 0.4046104MemoryTrain:  epoch  3, batch     0 | loss: 0.3846694MemoryTrain:  epoch  3, batch     1 | loss: 0.2484591MemoryTrain:  epoch  4, batch     0 | loss: 0.1661045MemoryTrain:  epoch  4, batch     1 | loss: 0.3418654MemoryTrain:  epoch  5, batch     0 | loss: 0.0976821MemoryTrain:  epoch  5, batch     1 | loss: 0.1343603MemoryTrain:  epoch  6, batch     0 | loss: 0.1068791MemoryTrain:  epoch  6, batch     1 | loss: 0.0757856MemoryTrain:  epoch  7, batch     0 | loss: 0.1204958MemoryTrain:  epoch  7, batch     1 | loss: 0.0867345MemoryTrain:  epoch  8, batch     0 | loss: 0.0433325MemoryTrain:  epoch  8, batch     1 | loss: 0.1562797MemoryTrain:  epoch  9, batch     0 | loss: 0.0446376MemoryTrain:  epoch  9, batch     1 | loss: 0.0443660
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 7.9946070CurrentTrain: epoch  0, batch     1 | loss: 8.3638220CurrentTrain: epoch  1, batch     0 | loss: 7.0388055CurrentTrain: epoch  1, batch     1 | loss: 6.5304766CurrentTrain: epoch  2, batch     0 | loss: 6.4956179CurrentTrain: epoch  2, batch     1 | loss: 5.6669612CurrentTrain: epoch  3, batch     0 | loss: 6.6723256CurrentTrain: epoch  3, batch     1 | loss: 5.0182834CurrentTrain: epoch  4, batch     0 | loss: 5.7155676CurrentTrain: epoch  4, batch     1 | loss: 5.7571645CurrentTrain: epoch  5, batch     0 | loss: 4.9395990CurrentTrain: epoch  5, batch     1 | loss: 4.9300971CurrentTrain: epoch  6, batch     0 | loss: 5.0692472CurrentTrain: epoch  6, batch     1 | loss: 3.8077385CurrentTrain: epoch  7, batch     0 | loss: 4.2680869CurrentTrain: epoch  7, batch     1 | loss: 4.1319461CurrentTrain: epoch  8, batch     0 | loss: 3.9282858CurrentTrain: epoch  8, batch     1 | loss: 4.5706034CurrentTrain: epoch  9, batch     0 | loss: 3.8859713CurrentTrain: epoch  9, batch     1 | loss: 3.3516731
Mixup data size:  100
MixupTrain:  epoch  0, batch     2 | loss: 4.4451621MixupTrain:  epoch  0, batch     3 | loss: 3.9660598MixupTrain:  epoch  0, batch     6 | loss: 2.6018175
MemoryTrain:  epoch  0, batch     0 | loss: 1.2111602MemoryTrain:  epoch  0, batch     1 | loss: 0.8523819MemoryTrain:  epoch  1, batch     0 | loss: 0.8872684MemoryTrain:  epoch  1, batch     1 | loss: 1.3998684MemoryTrain:  epoch  2, batch     0 | loss: 1.1657546MemoryTrain:  epoch  2, batch     1 | loss: 0.9465743MemoryTrain:  epoch  3, batch     0 | loss: 1.0942963MemoryTrain:  epoch  3, batch     1 | loss: 0.5533602MemoryTrain:  epoch  4, batch     0 | loss: 0.9480205MemoryTrain:  epoch  4, batch     1 | loss: 0.5184088MemoryTrain:  epoch  5, batch     0 | loss: 0.8448287MemoryTrain:  epoch  5, batch     1 | loss: 0.8218158MemoryTrain:  epoch  6, batch     0 | loss: 0.6461611MemoryTrain:  epoch  6, batch     1 | loss: 0.3858398MemoryTrain:  epoch  7, batch     0 | loss: 0.6066267MemoryTrain:  epoch  7, batch     1 | loss: 0.5809388MemoryTrain:  epoch  8, batch     0 | loss: 0.5814806MemoryTrain:  epoch  8, batch     1 | loss: 0.6573150MemoryTrain:  epoch  9, batch     0 | loss: 0.5004828MemoryTrain:  epoch  9, batch     1 | loss: 0.5093656
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 25.00%,  total acc: 78.98%   [EVAL] batch:   11 | acc: 18.75%,  total acc: 73.96%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 70.67%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 67.86%   [EVAL] batch:   14 | acc: 18.75%,  total acc: 64.58%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 9.38%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 12.50%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 9.38%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 8.75%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 8.33%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 17.86%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 28.12%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 36.11%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 41.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 47.16%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 51.04%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 50.96%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 49.11%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 50.42%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 51.17%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 51.84%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 52.08%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 53.62%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 55.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 57.14%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 59.09%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 60.60%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 62.24%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 63.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 65.14%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 63.19%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 60.94%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 58.84%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 56.88%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 55.04%   [EVAL] batch:   31 | acc: 0.00%,  total acc: 53.32%   [EVAL] batch:   32 | acc: 0.00%,  total acc: 51.70%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 50.37%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 49.11%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 47.92%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 46.79%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 45.56%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 45.19%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 45.62%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 45.43%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 45.68%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 46.22%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 47.16%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 47.78%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 48.64%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 49.60%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 50.65%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 51.53%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 52.00%   [EVAL] batch:   50 | acc: 25.00%,  total acc: 51.47%   [EVAL] batch:   51 | acc: 25.00%,  total acc: 50.96%   [EVAL] batch:   52 | acc: 18.75%,  total acc: 50.35%   [EVAL] batch:   53 | acc: 18.75%,  total acc: 49.77%   [EVAL] batch:   54 | acc: 25.00%,  total acc: 49.32%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 48.44%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 48.46%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 48.38%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 48.31%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 48.33%   [EVAL] batch:   60 | acc: 25.00%,  total acc: 47.95%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 48.08%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 47.82%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 48.14%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 48.85%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 49.62%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 50.37%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 51.10%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 51.81%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 52.50%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 53.17%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 53.73%   [EVAL] batch:   72 | acc: 31.25%,  total acc: 53.42%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 53.72%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 54.33%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 54.93%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 55.52%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 55.45%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 55.70%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 56.02%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 56.25%   [EVAL] batch:   81 | acc: 100.00%,  total acc: 56.78%   [EVAL] batch:   82 | acc: 93.75%,  total acc: 57.23%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 57.74%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 57.65%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 56.98%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 56.47%   [EVAL] batch:   87 | acc: 25.00%,  total acc: 56.11%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 55.48%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 55.21%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 55.43%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 55.71%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 56.12%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 56.52%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 56.84%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 57.10%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 57.47%   [EVAL] batch:   97 | acc: 75.00%,  total acc: 57.65%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 58.02%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 57.94%   [EVAL] batch:  100 | acc: 18.75%,  total acc: 57.55%   [EVAL] batch:  101 | acc: 18.75%,  total acc: 57.17%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 57.04%   [EVAL] batch:  103 | acc: 31.25%,  total acc: 56.79%   [EVAL] batch:  104 | acc: 0.00%,  total acc: 56.25%   
cur_acc_llm:  [0.8712121212121212, 0.8993055555555556, 0.6026785714285714, 0.8660714285714286, 0.6201923076923077, 0.6458333333333334]
his_acc_llm:  [0.8712121212121212, 0.83875, 0.728515625, 0.7868589743589743, 0.6395833333333333, 0.5625]
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 47.92%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 54.69%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 57.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 61.46%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 66.07%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 67.19%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 69.44%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   10 | acc: 12.50%,  total acc: 63.64%   [EVAL] batch:   11 | acc: 25.00%,  total acc: 60.42%   [EVAL] batch:   12 | acc: 18.75%,  total acc: 57.21%   [EVAL] batch:   13 | acc: 6.25%,  total acc: 53.57%   [EVAL] batch:   14 | acc: 0.00%,  total acc: 50.00%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 28.12%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 31.25%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 32.29%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 39.29%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 46.88%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 52.78%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 56.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 60.80%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 63.02%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 63.94%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 63.84%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 64.58%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 64.06%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 64.71%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 64.58%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 64.47%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 67.26%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 70.11%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 71.35%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.56%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 74.31%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.22%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 76.08%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 76.46%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 76.81%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 77.54%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 76.33%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 74.08%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 71.96%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 69.97%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 68.07%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 66.45%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 65.54%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 66.25%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 65.85%   [EVAL] batch:   41 | acc: 12.50%,  total acc: 64.58%   [EVAL] batch:   42 | acc: 18.75%,  total acc: 63.52%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 63.49%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 64.17%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 64.81%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 65.56%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 66.02%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 66.71%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 67.12%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 66.54%   [EVAL] batch:   51 | acc: 31.25%,  total acc: 65.87%   [EVAL] batch:   52 | acc: 25.00%,  total acc: 65.09%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 63.89%   [EVAL] batch:   54 | acc: 25.00%,  total acc: 63.18%   [EVAL] batch:   55 | acc: 6.25%,  total acc: 62.17%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 61.95%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 62.28%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 62.18%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 62.29%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 62.09%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 62.00%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 61.51%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 61.62%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 62.21%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 62.78%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 63.34%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 63.88%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 64.40%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 64.91%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 65.40%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 65.80%   [EVAL] batch:   72 | acc: 6.25%,  total acc: 64.98%   [EVAL] batch:   73 | acc: 12.50%,  total acc: 64.27%   [EVAL] batch:   74 | acc: 43.75%,  total acc: 64.00%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 63.90%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 63.80%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 63.94%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 64.00%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 64.22%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 64.27%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 64.10%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 63.70%   [EVAL] batch:   83 | acc: 43.75%,  total acc: 63.47%   [EVAL] batch:   84 | acc: 37.50%,  total acc: 63.16%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 63.08%   [EVAL] batch:   86 | acc: 75.00%,  total acc: 63.22%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 63.28%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 63.41%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 63.33%   [EVAL] batch:   90 | acc: 43.75%,  total acc: 63.12%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 63.04%   [EVAL] batch:   92 | acc: 50.00%,  total acc: 62.90%   [EVAL] batch:   93 | acc: 81.25%,  total acc: 63.10%   [EVAL] batch:   94 | acc: 62.50%,  total acc: 63.09%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 63.28%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 63.60%   [EVAL] batch:   97 | acc: 75.00%,  total acc: 63.71%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 64.02%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 63.69%   [EVAL] batch:  100 | acc: 6.25%,  total acc: 63.12%   [EVAL] batch:  101 | acc: 25.00%,  total acc: 62.75%   [EVAL] batch:  102 | acc: 18.75%,  total acc: 62.32%   [EVAL] batch:  103 | acc: 6.25%,  total acc: 61.78%   [EVAL] batch:  104 | acc: 0.00%,  total acc: 61.19%   
cur_acc:  ['0.8712', '0.6319', '0.6652', '0.7545', '0.6346', '0.5000']
his_acc:  ['0.8712', '0.7725', '0.7109', '0.7067', '0.6542', '0.6119']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 5.3090324CurrentTrain: epoch  0, batch     1 | loss: 7.1519532CurrentTrain: epoch  1, batch     0 | loss: 5.0051436CurrentTrain: epoch  1, batch     1 | loss: 5.3779650CurrentTrain: epoch  2, batch     0 | loss: 4.1974888CurrentTrain: epoch  2, batch     1 | loss: 3.2898533CurrentTrain: epoch  3, batch     0 | loss: 3.2557960CurrentTrain: epoch  3, batch     1 | loss: 3.7286818CurrentTrain: epoch  4, batch     0 | loss: 3.1335335CurrentTrain: epoch  4, batch     1 | loss: 3.4568782CurrentTrain: epoch  5, batch     0 | loss: 2.5865927CurrentTrain: epoch  5, batch     1 | loss: 3.5640841CurrentTrain: epoch  6, batch     0 | loss: 2.0035634CurrentTrain: epoch  6, batch     1 | loss: 4.1807151CurrentTrain: epoch  7, batch     0 | loss: 3.2283549CurrentTrain: epoch  7, batch     1 | loss: 1.9122916CurrentTrain: epoch  8, batch     0 | loss: 2.6461887CurrentTrain: epoch  8, batch     1 | loss: 2.3512681CurrentTrain: epoch  9, batch     0 | loss: 2.3853040CurrentTrain: epoch  9, batch     1 | loss: 2.3910229
Mixup data size:  110
MixupTrain:  epoch  0, batch     1 | loss: 3.1284164
MemoryTrain:  epoch  0, batch     0 | loss: 1.1888413MemoryTrain:  epoch  0, batch     1 | loss: 0.2952768MemoryTrain:  epoch  0, batch     2 | loss: 2.6914752MemoryTrain:  epoch  1, batch     0 | loss: 1.3593974MemoryTrain:  epoch  1, batch     1 | loss: 1.0661541MemoryTrain:  epoch  1, batch     2 | loss: 1.4696507MemoryTrain:  epoch  2, batch     0 | loss: 0.6329554MemoryTrain:  epoch  2, batch     1 | loss: 0.7470685MemoryTrain:  epoch  2, batch     2 | loss: 0.4083365MemoryTrain:  epoch  3, batch     0 | loss: 0.2303939MemoryTrain:  epoch  3, batch     1 | loss: 0.3889760MemoryTrain:  epoch  3, batch     2 | loss: 0.3731246MemoryTrain:  epoch  4, batch     0 | loss: 0.1553127MemoryTrain:  epoch  4, batch     1 | loss: 0.1845361MemoryTrain:  epoch  4, batch     2 | loss: 0.1409266MemoryTrain:  epoch  5, batch     0 | loss: 0.0992804MemoryTrain:  epoch  5, batch     1 | loss: 0.1883755MemoryTrain:  epoch  5, batch     2 | loss: 0.0703312MemoryTrain:  epoch  6, batch     0 | loss: 0.0999138MemoryTrain:  epoch  6, batch     1 | loss: 0.0730073MemoryTrain:  epoch  6, batch     2 | loss: 0.0733654MemoryTrain:  epoch  7, batch     0 | loss: 0.0971330MemoryTrain:  epoch  7, batch     1 | loss: 0.1128789MemoryTrain:  epoch  7, batch     2 | loss: 0.0452545MemoryTrain:  epoch  8, batch     0 | loss: 0.0783778MemoryTrain:  epoch  8, batch     1 | loss: 0.0416368MemoryTrain:  epoch  8, batch     2 | loss: 0.0376390MemoryTrain:  epoch  9, batch     0 | loss: 0.0666282MemoryTrain:  epoch  9, batch     1 | loss: 0.0275544MemoryTrain:  epoch  9, batch     2 | loss: 0.0266321
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 6.6856766CurrentTrain: epoch  0, batch     1 | loss: 8.0886307CurrentTrain: epoch  1, batch     0 | loss: 6.7324657CurrentTrain: epoch  1, batch     1 | loss: 4.5642185CurrentTrain: epoch  2, batch     0 | loss: 5.3147378CurrentTrain: epoch  2, batch     1 | loss: 4.1117573CurrentTrain: epoch  3, batch     0 | loss: 4.3951178CurrentTrain: epoch  3, batch     1 | loss: 4.0455494CurrentTrain: epoch  4, batch     0 | loss: 3.8561625CurrentTrain: epoch  4, batch     1 | loss: 3.9774177CurrentTrain: epoch  5, batch     0 | loss: 4.0169735CurrentTrain: epoch  5, batch     1 | loss: 2.7225549CurrentTrain: epoch  6, batch     0 | loss: 3.2976761CurrentTrain: epoch  6, batch     1 | loss: 3.6598792CurrentTrain: epoch  7, batch     0 | loss: 3.0586071CurrentTrain: epoch  7, batch     1 | loss: 3.4129803CurrentTrain: epoch  8, batch     0 | loss: 3.1743410CurrentTrain: epoch  8, batch     1 | loss: 2.4012790CurrentTrain: epoch  9, batch     0 | loss: 3.0795712CurrentTrain: epoch  9, batch     1 | loss: 2.3892653
Mixup data size:  111
MixupTrain:  epoch  0, batch     0 | loss: 4.2099836MixupTrain:  epoch  0, batch     2 | loss: 4.3137744MixupTrain:  epoch  0, batch     3 | loss: 3.9008114MixupTrain:  epoch  0, batch     5 | loss: 4.7848349MixupTrain:  epoch  0, batch     6 | loss: 3.4854108
MemoryTrain:  epoch  0, batch     0 | loss: 1.2155924MemoryTrain:  epoch  0, batch     1 | loss: 1.1106997MemoryTrain:  epoch  0, batch     2 | loss: 0.1863722MemoryTrain:  epoch  1, batch     0 | loss: 1.5538645MemoryTrain:  epoch  1, batch     1 | loss: 1.1756618MemoryTrain:  epoch  1, batch     2 | loss: 0.1917518MemoryTrain:  epoch  2, batch     0 | loss: 0.9438916MemoryTrain:  epoch  2, batch     1 | loss: 1.1166046MemoryTrain:  epoch  2, batch     2 | loss: 0.2636837MemoryTrain:  epoch  3, batch     0 | loss: 0.7390298MemoryTrain:  epoch  3, batch     1 | loss: 0.7739139MemoryTrain:  epoch  3, batch     2 | loss: 0.3891520MemoryTrain:  epoch  4, batch     0 | loss: 0.7938494MemoryTrain:  epoch  4, batch     1 | loss: 0.6949226MemoryTrain:  epoch  4, batch     2 | loss: 0.2383592MemoryTrain:  epoch  5, batch     0 | loss: 0.4497356MemoryTrain:  epoch  5, batch     1 | loss: 0.6034931MemoryTrain:  epoch  5, batch     2 | loss: 0.4246089MemoryTrain:  epoch  6, batch     0 | loss: 0.4312456MemoryTrain:  epoch  6, batch     1 | loss: 0.9141765MemoryTrain:  epoch  6, batch     2 | loss: 0.1939823MemoryTrain:  epoch  7, batch     0 | loss: 0.6706939MemoryTrain:  epoch  7, batch     1 | loss: 0.7889539MemoryTrain:  epoch  7, batch     2 | loss: 0.0706690MemoryTrain:  epoch  8, batch     0 | loss: 0.5641824MemoryTrain:  epoch  8, batch     1 | loss: 0.6212945MemoryTrain:  epoch  8, batch     2 | loss: 0.1600707MemoryTrain:  epoch  9, batch     0 | loss: 0.5950715MemoryTrain:  epoch  9, batch     1 | loss: 0.4241657MemoryTrain:  epoch  9, batch     2 | loss: 0.0594796
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 51.25%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 46.88%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 43.75%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 39.06%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 3.12%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 2.08%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 3.12%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 3.75%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 4.17%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 9.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 21.09%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 29.17%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 34.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 39.20%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 43.23%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 43.27%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 41.96%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 43.75%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 44.92%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 45.96%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 46.18%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 47.37%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 49.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 51.49%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 53.69%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 55.43%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 57.29%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 59.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 60.58%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 58.80%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 56.70%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 54.74%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 52.92%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 51.21%   [EVAL] batch:   31 | acc: 0.00%,  total acc: 49.61%   [EVAL] batch:   32 | acc: 0.00%,  total acc: 48.11%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 46.69%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 45.36%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 44.10%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 42.91%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 41.78%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 41.67%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 43.12%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 42.99%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 43.01%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 43.31%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 44.03%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 45.00%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 46.06%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 47.21%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 48.31%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 49.36%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 50.25%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 50.12%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 49.52%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 48.82%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 48.15%   [EVAL] batch:   54 | acc: 25.00%,  total acc: 47.73%   [EVAL] batch:   55 | acc: 6.25%,  total acc: 46.99%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 47.26%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 47.84%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 48.20%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 48.65%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 48.77%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 49.09%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 49.21%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 49.51%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 50.19%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 50.85%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 51.59%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 52.30%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 52.99%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 53.66%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 54.31%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 54.86%   [EVAL] batch:   72 | acc: 31.25%,  total acc: 54.54%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 54.48%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 55.00%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 55.59%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 56.17%   [EVAL] batch:   77 | acc: 12.50%,  total acc: 55.61%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 55.62%   [EVAL] batch:   79 | acc: 25.00%,  total acc: 55.23%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 55.25%   [EVAL] batch:   81 | acc: 93.75%,  total acc: 55.72%   [EVAL] batch:   82 | acc: 93.75%,  total acc: 56.17%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 56.70%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 56.18%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 55.52%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 54.89%   [EVAL] batch:   87 | acc: 0.00%,  total acc: 54.26%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 53.65%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 53.40%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 53.57%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 53.87%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 54.37%   [EVAL] batch:   93 | acc: 81.25%,  total acc: 54.65%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 54.87%   [EVAL] batch:   95 | acc: 62.50%,  total acc: 54.95%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 55.09%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 55.04%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 55.43%   [EVAL] batch:   99 | acc: 56.25%,  total acc: 55.44%   [EVAL] batch:  100 | acc: 12.50%,  total acc: 55.01%   [EVAL] batch:  101 | acc: 6.25%,  total acc: 54.53%   [EVAL] batch:  102 | acc: 25.00%,  total acc: 54.25%   [EVAL] batch:  103 | acc: 0.00%,  total acc: 53.73%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 53.87%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 54.30%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 54.09%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 53.88%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 53.56%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 53.35%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 53.10%   [EVAL] batch:  111 | acc: 6.25%,  total acc: 52.68%   
cur_acc_llm:  [0.8712121212121212, 0.8993055555555556, 0.6026785714285714, 0.8660714285714286, 0.6201923076923077, 0.6458333333333334, 0.390625]
his_acc_llm:  [0.8712121212121212, 0.83875, 0.728515625, 0.7868589743589743, 0.6395833333333333, 0.5625, 0.5267857142857143]
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 67.19%   
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 14.58%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 12.50%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 13.54%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 17.86%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 22.66%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 26.39%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 28.75%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 31.25%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 33.33%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 33.17%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 32.59%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 35.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 36.33%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 38.60%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 39.93%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 41.12%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 43.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 46.13%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 48.58%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 50.54%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 52.34%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 54.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 56.01%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 57.41%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 58.93%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 60.34%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 61.25%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 61.90%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 62.89%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 62.12%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 60.29%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 58.57%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 56.94%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 55.41%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 53.95%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 53.37%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 54.22%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 53.66%   [EVAL] batch:   41 | acc: 25.00%,  total acc: 52.98%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 53.05%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 53.41%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 54.31%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 55.16%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 56.12%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 56.77%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 57.65%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 58.50%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 58.09%   [EVAL] batch:   51 | acc: 12.50%,  total acc: 57.21%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 56.25%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 55.21%   [EVAL] batch:   54 | acc: 6.25%,  total acc: 54.32%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 53.35%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 53.40%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 53.88%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 53.81%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 53.65%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 53.48%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 53.33%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 53.17%   [EVAL] batch:   63 | acc: 62.50%,  total acc: 53.32%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 53.94%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 54.64%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 55.32%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 55.97%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 56.61%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 57.23%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 57.83%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 58.33%   [EVAL] batch:   72 | acc: 6.25%,  total acc: 57.62%   [EVAL] batch:   73 | acc: 12.50%,  total acc: 57.01%   [EVAL] batch:   74 | acc: 37.50%,  total acc: 56.75%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 56.83%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 56.90%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 57.21%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 57.44%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 57.81%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 57.95%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 58.16%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 58.28%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 58.63%   [EVAL] batch:   84 | acc: 37.50%,  total acc: 58.38%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 57.85%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 57.69%   [EVAL] batch:   87 | acc: 43.75%,  total acc: 57.53%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 57.72%   [EVAL] batch:   89 | acc: 37.50%,  total acc: 57.50%   [EVAL] batch:   90 | acc: 18.75%,  total acc: 57.07%   [EVAL] batch:   91 | acc: 12.50%,  total acc: 56.59%   [EVAL] batch:   92 | acc: 12.50%,  total acc: 56.12%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 55.92%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 55.66%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 55.92%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 56.31%   [EVAL] batch:   97 | acc: 81.25%,  total acc: 56.57%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 56.94%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 56.62%   [EVAL] batch:  100 | acc: 12.50%,  total acc: 56.19%   [EVAL] batch:  101 | acc: 25.00%,  total acc: 55.88%   [EVAL] batch:  102 | acc: 18.75%,  total acc: 55.52%   [EVAL] batch:  103 | acc: 12.50%,  total acc: 55.11%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 55.30%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 55.66%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 55.96%   [EVAL] batch:  107 | acc: 75.00%,  total acc: 56.13%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 56.19%   [EVAL] batch:  109 | acc: 68.75%,  total acc: 56.31%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 56.31%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 55.97%   
cur_acc:  ['0.8712', '0.6319', '0.6652', '0.7545', '0.6346', '0.5000', '0.6719']
his_acc:  ['0.8712', '0.7725', '0.7109', '0.7067', '0.6542', '0.6119', '0.5597']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 7.6523266CurrentTrain: epoch  0, batch     1 | loss: 7.8842568CurrentTrain: epoch  1, batch     0 | loss: 7.0494118CurrentTrain: epoch  1, batch     1 | loss: 5.1551657CurrentTrain: epoch  2, batch     0 | loss: 5.4007516CurrentTrain: epoch  2, batch     1 | loss: 5.2194691CurrentTrain: epoch  3, batch     0 | loss: 5.5667052CurrentTrain: epoch  3, batch     1 | loss: 4.1337018CurrentTrain: epoch  4, batch     0 | loss: 4.5068626CurrentTrain: epoch  4, batch     1 | loss: 5.1894312CurrentTrain: epoch  5, batch     0 | loss: 4.4390850CurrentTrain: epoch  5, batch     1 | loss: 4.3480396CurrentTrain: epoch  6, batch     0 | loss: 4.1790934CurrentTrain: epoch  6, batch     1 | loss: 4.0811810CurrentTrain: epoch  7, batch     0 | loss: 4.1186295CurrentTrain: epoch  7, batch     1 | loss: 3.5854390CurrentTrain: epoch  8, batch     0 | loss: 3.4835057CurrentTrain: epoch  8, batch     1 | loss: 3.2690642CurrentTrain: epoch  9, batch     0 | loss: 3.2456460CurrentTrain: epoch  9, batch     1 | loss: 2.8487673
Mixup data size:  121
MixupTrain:  epoch  0, batch     0 | loss: 3.2355063MixupTrain:  epoch  0, batch     2 | loss: 2.5857983MixupTrain:  epoch  0, batch     3 | loss: 2.9694654MixupTrain:  epoch  0, batch     5 | loss: 4.3011586MixupTrain:  epoch  0, batch     6 | loss: 2.5489691MixupTrain:  epoch  0, batch     7 | loss: 2.7082997
MemoryTrain:  epoch  0, batch     0 | loss: 0.2368940MemoryTrain:  epoch  0, batch     1 | loss: 0.0561853MemoryTrain:  epoch  0, batch     2 | loss: 0.0407005MemoryTrain:  epoch  1, batch     0 | loss: 0.6805387MemoryTrain:  epoch  1, batch     1 | loss: 0.3127853MemoryTrain:  epoch  1, batch     2 | loss: 0.5778934MemoryTrain:  epoch  2, batch     0 | loss: 0.4189156MemoryTrain:  epoch  2, batch     1 | loss: 0.1803566MemoryTrain:  epoch  2, batch     2 | loss: 0.1487688MemoryTrain:  epoch  3, batch     0 | loss: 0.1283318MemoryTrain:  epoch  3, batch     1 | loss: 0.2526393MemoryTrain:  epoch  3, batch     2 | loss: 0.0455556MemoryTrain:  epoch  4, batch     0 | loss: 0.1090218MemoryTrain:  epoch  4, batch     1 | loss: 0.0916333MemoryTrain:  epoch  4, batch     2 | loss: 0.1885569MemoryTrain:  epoch  5, batch     0 | loss: 0.0674904MemoryTrain:  epoch  5, batch     1 | loss: 0.0482929MemoryTrain:  epoch  5, batch     2 | loss: 0.1103436MemoryTrain:  epoch  6, batch     0 | loss: 0.0716331MemoryTrain:  epoch  6, batch     1 | loss: 0.0479159MemoryTrain:  epoch  6, batch     2 | loss: 0.0633656MemoryTrain:  epoch  7, batch     0 | loss: 0.0274094MemoryTrain:  epoch  7, batch     1 | loss: 0.0352919MemoryTrain:  epoch  7, batch     2 | loss: 0.0317892MemoryTrain:  epoch  8, batch     0 | loss: 0.0426025MemoryTrain:  epoch  8, batch     1 | loss: 0.0472744MemoryTrain:  epoch  8, batch     2 | loss: 0.0299019MemoryTrain:  epoch  9, batch     0 | loss: 0.0328452MemoryTrain:  epoch  9, batch     1 | loss: 0.0268407MemoryTrain:  epoch  9, batch     2 | loss: 0.0214783
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 9.7232475CurrentTrain: epoch  0, batch     1 | loss: 8.6868811CurrentTrain: epoch  1, batch     0 | loss: 8.9099360CurrentTrain: epoch  1, batch     1 | loss: 6.1088724CurrentTrain: epoch  2, batch     0 | loss: 8.1738834CurrentTrain: epoch  2, batch     1 | loss: 6.3474340CurrentTrain: epoch  3, batch     0 | loss: 7.2216520CurrentTrain: epoch  3, batch     1 | loss: 6.2759247CurrentTrain: epoch  4, batch     0 | loss: 7.5066662CurrentTrain: epoch  4, batch     1 | loss: 4.9267759CurrentTrain: epoch  5, batch     0 | loss: 7.4581513CurrentTrain: epoch  5, batch     1 | loss: 4.0279727CurrentTrain: epoch  6, batch     0 | loss: 5.5910320CurrentTrain: epoch  6, batch     1 | loss: 6.1165848CurrentTrain: epoch  7, batch     0 | loss: 5.7370348CurrentTrain: epoch  7, batch     1 | loss: 5.2859287CurrentTrain: epoch  8, batch     0 | loss: 5.2721748CurrentTrain: epoch  8, batch     1 | loss: 5.0894084CurrentTrain: epoch  9, batch     0 | loss: 5.1908674CurrentTrain: epoch  9, batch     1 | loss: 4.7014089
Mixup data size:  120
MixupTrain:  epoch  0, batch     0 | loss: 3.5363642MixupTrain:  epoch  0, batch     5 | loss: 3.6481965MixupTrain:  epoch  0, batch     6 | loss: 4.5688610MixupTrain:  epoch  0, batch     7 | loss: 3.5974541
MemoryTrain:  epoch  0, batch     0 | loss: 0.8376610MemoryTrain:  epoch  0, batch     1 | loss: 0.3938802MemoryTrain:  epoch  0, batch     2 | loss: 0.8244328MemoryTrain:  epoch  1, batch     0 | loss: 0.5285787MemoryTrain:  epoch  1, batch     1 | loss: 0.9452621MemoryTrain:  epoch  1, batch     2 | loss: 1.2469598MemoryTrain:  epoch  2, batch     0 | loss: 0.6100959MemoryTrain:  epoch  2, batch     1 | loss: 0.6406376MemoryTrain:  epoch  2, batch     2 | loss: 0.4980268MemoryTrain:  epoch  3, batch     0 | loss: 0.5118049MemoryTrain:  epoch  3, batch     1 | loss: 0.5054601MemoryTrain:  epoch  3, batch     2 | loss: 0.3550390MemoryTrain:  epoch  4, batch     0 | loss: 0.5165458MemoryTrain:  epoch  4, batch     1 | loss: 0.5364799MemoryTrain:  epoch  4, batch     2 | loss: 0.2250290MemoryTrain:  epoch  5, batch     0 | loss: 0.6636009MemoryTrain:  epoch  5, batch     1 | loss: 0.2953167MemoryTrain:  epoch  5, batch     2 | loss: 0.5174340MemoryTrain:  epoch  6, batch     0 | loss: 0.4181637MemoryTrain:  epoch  6, batch     1 | loss: 0.5995194MemoryTrain:  epoch  6, batch     2 | loss: 0.4506882MemoryTrain:  epoch  7, batch     0 | loss: 0.3472503MemoryTrain:  epoch  7, batch     1 | loss: 0.5384246MemoryTrain:  epoch  7, batch     2 | loss: 0.3597443MemoryTrain:  epoch  8, batch     0 | loss: 0.3589444MemoryTrain:  epoch  8, batch     1 | loss: 0.4989099MemoryTrain:  epoch  8, batch     2 | loss: 0.2190473MemoryTrain:  epoch  9, batch     0 | loss: 0.3274829MemoryTrain:  epoch  9, batch     1 | loss: 0.4375686MemoryTrain:  epoch  9, batch     2 | loss: 0.3169935
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 14.58%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 12.50%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 14.29%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 25.00%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 33.33%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 38.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 44.32%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 48.44%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 51.92%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 55.36%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 58.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 60.94%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 63.24%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 65.28%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 66.78%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 68.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 69.94%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 68.75%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 4.17%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 4.69%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 3.75%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 4.17%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 13.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 24.22%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 31.25%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 35.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 39.77%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 44.27%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 44.23%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 42.86%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 45.00%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 46.09%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 47.06%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 47.22%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 48.68%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 50.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 52.68%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 54.83%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 56.52%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 58.33%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 60.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 61.54%   [EVAL] batch:   26 | acc: 18.75%,  total acc: 59.95%   [EVAL] batch:   27 | acc: 12.50%,  total acc: 58.26%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 56.47%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 54.58%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 52.82%   [EVAL] batch:   31 | acc: 12.50%,  total acc: 51.56%   [EVAL] batch:   32 | acc: 0.00%,  total acc: 50.00%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 48.53%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 47.14%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 45.83%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 44.59%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 43.42%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 43.27%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 44.69%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 44.36%   [EVAL] batch:   41 | acc: 0.00%,  total acc: 43.30%   [EVAL] batch:   42 | acc: 6.25%,  total acc: 42.44%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 43.04%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 44.03%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 45.11%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 46.14%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 47.27%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 48.34%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 49.25%   [EVAL] batch:   50 | acc: 31.25%,  total acc: 48.90%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 48.08%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 47.17%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 46.30%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 45.45%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 44.64%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 44.74%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 45.26%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 45.87%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 46.25%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 46.41%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 47.08%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 47.22%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 47.56%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 48.27%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 48.96%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 49.72%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 50.46%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 51.18%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 51.88%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 52.55%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 53.12%   [EVAL] batch:   72 | acc: 0.00%,  total acc: 52.40%   [EVAL] batch:   73 | acc: 6.25%,  total acc: 51.77%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 52.25%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 52.88%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 53.49%   [EVAL] batch:   77 | acc: 12.50%,  total acc: 52.96%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 53.09%   [EVAL] batch:   79 | acc: 31.25%,  total acc: 52.81%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 52.78%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 53.12%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 53.54%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 53.87%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 53.38%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 52.76%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 52.16%   [EVAL] batch:   87 | acc: 0.00%,  total acc: 51.56%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 50.98%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 50.76%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 50.96%   [EVAL] batch:   91 | acc: 87.50%,  total acc: 51.36%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 51.81%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 52.19%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 52.37%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 52.34%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 52.45%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 52.42%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 52.84%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 52.75%   [EVAL] batch:  100 | acc: 6.25%,  total acc: 52.29%   [EVAL] batch:  101 | acc: 6.25%,  total acc: 51.84%   [EVAL] batch:  102 | acc: 6.25%,  total acc: 51.40%   [EVAL] batch:  103 | acc: 0.00%,  total acc: 50.90%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 51.07%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 51.53%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 51.46%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 51.04%   [EVAL] batch:  108 | acc: 6.25%,  total acc: 50.63%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 50.23%   [EVAL] batch:  110 | acc: 6.25%,  total acc: 49.83%   [EVAL] batch:  111 | acc: 6.25%,  total acc: 49.44%   [EVAL] batch:  112 | acc: 25.00%,  total acc: 49.23%   [EVAL] batch:  113 | acc: 12.50%,  total acc: 48.90%   [EVAL] batch:  114 | acc: 0.00%,  total acc: 48.48%   [EVAL] batch:  115 | acc: 12.50%,  total acc: 48.17%   [EVAL] batch:  116 | acc: 18.75%,  total acc: 47.92%   [EVAL] batch:  117 | acc: 18.75%,  total acc: 47.67%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 48.00%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 48.44%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 48.81%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 49.18%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 49.54%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 49.90%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 50.30%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 50.69%   [EVAL] batch:  126 | acc: 100.00%,  total acc: 51.08%   [EVAL] batch:  127 | acc: 100.00%,  total acc: 51.46%   [EVAL] batch:  128 | acc: 100.00%,  total acc: 51.84%   [EVAL] batch:  129 | acc: 100.00%,  total acc: 52.21%   [EVAL] batch:  130 | acc: 93.75%,  total acc: 52.53%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 52.89%   [EVAL] batch:  132 | acc: 62.50%,  total acc: 52.96%   
cur_acc_llm:  [0.8712121212121212, 0.8993055555555556, 0.6026785714285714, 0.8660714285714286, 0.6201923076923077, 0.6458333333333334, 0.390625, 0.6875]
his_acc_llm:  [0.8712121212121212, 0.83875, 0.728515625, 0.7868589743589743, 0.6395833333333333, 0.5625, 0.5267857142857143, 0.5296052631578947]
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 9.38%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 6.25%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 10.94%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 12.50%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 11.46%   [EVAL] batch:    6 | acc: 12.50%,  total acc: 11.61%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 16.41%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 20.14%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 23.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 28.41%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 31.77%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 34.62%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 37.50%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 39.58%   [EVAL] batch:   15 | acc: 43.75%,  total acc: 39.84%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 40.44%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 40.97%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 42.11%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 43.44%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 45.24%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 44.89%   
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 10.94%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 12.50%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 13.54%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 16.96%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 22.66%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 27.78%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 30.63%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 33.52%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 35.94%   [EVAL] batch:   12 | acc: 25.00%,  total acc: 35.10%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 34.38%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 36.25%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 37.11%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 39.34%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 40.62%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 41.45%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 43.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 46.43%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 48.86%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 50.82%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 52.60%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 54.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 57.64%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 59.15%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 60.56%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 61.25%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 61.90%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 62.70%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 61.93%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 60.11%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 58.39%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 56.77%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 55.24%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 53.78%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 53.21%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 54.22%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 53.81%   [EVAL] batch:   41 | acc: 18.75%,  total acc: 52.98%   [EVAL] batch:   42 | acc: 31.25%,  total acc: 52.47%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 52.84%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 53.75%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 54.62%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 55.59%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 57.14%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 58.00%   [EVAL] batch:   50 | acc: 31.25%,  total acc: 57.48%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 56.49%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 55.54%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 54.51%   [EVAL] batch:   54 | acc: 6.25%,  total acc: 53.64%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 52.68%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 52.63%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 53.02%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 52.97%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 53.12%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 52.97%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 52.92%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 52.68%   [EVAL] batch:   63 | acc: 62.50%,  total acc: 52.83%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 53.46%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 54.07%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 54.76%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 55.42%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 56.07%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 56.70%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 57.31%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 57.81%   [EVAL] batch:   72 | acc: 6.25%,  total acc: 57.11%   [EVAL] batch:   73 | acc: 6.25%,  total acc: 56.42%   [EVAL] batch:   74 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 56.50%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 56.74%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 57.05%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 57.20%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 57.50%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 57.64%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 57.77%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 57.91%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 58.18%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 57.72%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 57.05%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 56.47%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 56.04%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 56.32%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 56.32%   [EVAL] batch:   90 | acc: 6.25%,  total acc: 55.77%   [EVAL] batch:   91 | acc: 0.00%,  total acc: 55.16%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 54.64%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 54.26%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 54.01%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 54.30%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 54.70%   [EVAL] batch:   97 | acc: 81.25%,  total acc: 54.97%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 55.37%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 55.06%   [EVAL] batch:  100 | acc: 0.00%,  total acc: 54.52%   [EVAL] batch:  101 | acc: 6.25%,  total acc: 54.04%   [EVAL] batch:  102 | acc: 6.25%,  total acc: 53.58%   [EVAL] batch:  103 | acc: 0.00%,  total acc: 53.06%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 53.27%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 53.71%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 53.86%   [EVAL] batch:  107 | acc: 62.50%,  total acc: 53.94%   [EVAL] batch:  108 | acc: 68.75%,  total acc: 54.07%   [EVAL] batch:  109 | acc: 68.75%,  total acc: 54.20%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 54.17%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 53.85%   [EVAL] batch:  112 | acc: 18.75%,  total acc: 53.54%   [EVAL] batch:  113 | acc: 0.00%,  total acc: 53.07%   [EVAL] batch:  114 | acc: 12.50%,  total acc: 52.72%   [EVAL] batch:  115 | acc: 25.00%,  total acc: 52.48%   [EVAL] batch:  116 | acc: 12.50%,  total acc: 52.14%   [EVAL] batch:  117 | acc: 12.50%,  total acc: 51.80%   [EVAL] batch:  118 | acc: 43.75%,  total acc: 51.73%   [EVAL] batch:  119 | acc: 43.75%,  total acc: 51.67%   [EVAL] batch:  120 | acc: 56.25%,  total acc: 51.70%   [EVAL] batch:  121 | acc: 75.00%,  total acc: 51.90%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 52.08%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 52.22%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 52.40%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 52.48%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 52.46%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 52.49%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 52.42%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 52.50%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 52.58%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 52.75%   [EVAL] batch:  132 | acc: 56.25%,  total acc: 52.77%   
cur_acc:  ['0.8712', '0.6319', '0.6652', '0.7545', '0.6346', '0.5000', '0.6719', '0.4489']
his_acc:  ['0.8712', '0.7725', '0.7109', '0.7067', '0.6542', '0.6119', '0.5597', '0.5277']
--------Round  3
seed:  400
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 0 1 2 5 3 4 6]
prepared data!
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 13.2038918CurrentTrain: epoch  0, batch     1 | loss: 13.0012493CurrentTrain: epoch  0, batch     2 | loss: 12.1804352CurrentTrain: epoch  0, batch     3 | loss: 11.4469795CurrentTrain: epoch  0, batch     4 | loss: 11.3164129CurrentTrain: epoch  0, batch     5 | loss: 11.1137562CurrentTrain: epoch  0, batch     6 | loss: 11.7964096CurrentTrain: epoch  0, batch     7 | loss: 11.3161755CurrentTrain: epoch  0, batch     8 | loss: 11.5694523CurrentTrain: epoch  0, batch     9 | loss: 11.3707581CurrentTrain: epoch  0, batch    10 | loss: 11.3228512CurrentTrain: epoch  0, batch    11 | loss: 11.4026423CurrentTrain: epoch  0, batch    12 | loss: 11.2435064CurrentTrain: epoch  0, batch    13 | loss: 11.4729528CurrentTrain: epoch  0, batch    14 | loss: 11.3853436CurrentTrain: epoch  0, batch    15 | loss: 11.5864029CurrentTrain: epoch  0, batch    16 | loss: 11.6131134CurrentTrain: epoch  0, batch    17 | loss: 11.1287336CurrentTrain: epoch  0, batch    18 | loss: 11.2284985CurrentTrain: epoch  0, batch    19 | loss: 11.0996132CurrentTrain: epoch  0, batch    20 | loss: 11.1241665CurrentTrain: epoch  0, batch    21 | loss: 11.2920704CurrentTrain: epoch  0, batch    22 | loss: 11.2305861CurrentTrain: epoch  0, batch    23 | loss: 11.1882601CurrentTrain: epoch  0, batch    24 | loss: 11.2790461CurrentTrain: epoch  0, batch    25 | loss: 11.3372250CurrentTrain: epoch  0, batch    26 | loss: 10.7745438CurrentTrain: epoch  0, batch    27 | loss: 11.3601866CurrentTrain: epoch  0, batch    28 | loss: 11.2363749CurrentTrain: epoch  0, batch    29 | loss: 11.9593220CurrentTrain: epoch  0, batch    30 | loss: 11.6676750CurrentTrain: epoch  0, batch    31 | loss: 11.3335648CurrentTrain: epoch  0, batch    32 | loss: 11.0434341CurrentTrain: epoch  0, batch    33 | loss: 11.3572540CurrentTrain: epoch  0, batch    34 | loss: 11.1192217CurrentTrain: epoch  0, batch    35 | loss: 11.1320667CurrentTrain: epoch  0, batch    36 | loss: 10.8203659CurrentTrain: epoch  0, batch    37 | loss: 10.6989307CurrentTrain: epoch  1, batch     0 | loss: 10.9426184CurrentTrain: epoch  1, batch     1 | loss: 11.0140600CurrentTrain: epoch  1, batch     2 | loss: 10.9570599CurrentTrain: epoch  1, batch     3 | loss: 10.8990688CurrentTrain: epoch  1, batch     4 | loss: 10.8355694CurrentTrain: epoch  1, batch     5 | loss: 10.8401585CurrentTrain: epoch  1, batch     6 | loss: 10.7946186CurrentTrain: epoch  1, batch     7 | loss: 11.0788136CurrentTrain: epoch  1, batch     8 | loss: 11.2240486CurrentTrain: epoch  1, batch     9 | loss: 11.6256580CurrentTrain: epoch  1, batch    10 | loss: 11.3877945CurrentTrain: epoch  1, batch    11 | loss: 11.0557079CurrentTrain: epoch  1, batch    12 | loss: 11.0146379CurrentTrain: epoch  1, batch    13 | loss: 11.1812773CurrentTrain: epoch  1, batch    14 | loss: 10.6684589CurrentTrain: epoch  1, batch    15 | loss: 10.8374176CurrentTrain: epoch  1, batch    16 | loss: 10.8133316CurrentTrain: epoch  1, batch    17 | loss: 10.6516438CurrentTrain: epoch  1, batch    18 | loss: 10.7748194CurrentTrain: epoch  1, batch    19 | loss: 10.4928741CurrentTrain: epoch  1, batch    20 | loss: 10.6595678CurrentTrain: epoch  1, batch    21 | loss: 10.6355228CurrentTrain: epoch  1, batch    22 | loss: 10.7337818CurrentTrain: epoch  1, batch    23 | loss: 10.8350573CurrentTrain: epoch  1, batch    24 | loss: 11.1220322CurrentTrain: epoch  1, batch    25 | loss: 10.8677750CurrentTrain: epoch  1, batch    26 | loss: 10.6912985CurrentTrain: epoch  1, batch    27 | loss: 10.6595020CurrentTrain: epoch  1, batch    28 | loss: 10.5762730CurrentTrain: epoch  1, batch    29 | loss: 10.9841995CurrentTrain: epoch  1, batch    30 | loss: 10.5767288CurrentTrain: epoch  1, batch    31 | loss: 10.5222321CurrentTrain: epoch  1, batch    32 | loss: 10.5669184CurrentTrain: epoch  1, batch    33 | loss: 10.3737059CurrentTrain: epoch  1, batch    34 | loss: 10.2199631CurrentTrain: epoch  1, batch    35 | loss: 10.2522917CurrentTrain: epoch  1, batch    36 | loss: 10.6535988CurrentTrain: epoch  1, batch    37 | loss: 10.4045401CurrentTrain: epoch  2, batch     0 | loss: 10.5926819CurrentTrain: epoch  2, batch     1 | loss: 10.5522823CurrentTrain: epoch  2, batch     2 | loss: 10.3281384CurrentTrain: epoch  2, batch     3 | loss: 10.4585915CurrentTrain: epoch  2, batch     4 | loss: 10.2803679CurrentTrain: epoch  2, batch     5 | loss: 10.1044226CurrentTrain: epoch  2, batch     6 | loss: 10.1125898CurrentTrain: epoch  2, batch     7 | loss: 10.2416515CurrentTrain: epoch  2, batch     8 | loss: 9.9871140CurrentTrain: epoch  2, batch     9 | loss: 10.0993633CurrentTrain: epoch  2, batch    10 | loss: 9.9738321CurrentTrain: epoch  2, batch    11 | loss: 9.8598976CurrentTrain: epoch  2, batch    12 | loss: 9.9945278CurrentTrain: epoch  2, batch    13 | loss: 10.1086941CurrentTrain: epoch  2, batch    14 | loss: 9.9388313CurrentTrain: epoch  2, batch    15 | loss: 9.8337812CurrentTrain: epoch  2, batch    16 | loss: 9.5261488CurrentTrain: epoch  2, batch    17 | loss: 9.7725430CurrentTrain: epoch  2, batch    18 | loss: 9.6388073CurrentTrain: epoch  2, batch    19 | loss: 9.5037098CurrentTrain: epoch  2, batch    20 | loss: 9.9623909CurrentTrain: epoch  2, batch    21 | loss: 9.6192169CurrentTrain: epoch  2, batch    22 | loss: 9.3146734CurrentTrain: epoch  2, batch    23 | loss: 9.7421894CurrentTrain: epoch  2, batch    24 | loss: 10.2938156CurrentTrain: epoch  2, batch    25 | loss: 10.1503220CurrentTrain: epoch  2, batch    26 | loss: 9.3650436CurrentTrain: epoch  2, batch    27 | loss: 9.5284538CurrentTrain: epoch  2, batch    28 | loss: 9.5804424CurrentTrain: epoch  2, batch    29 | loss: 9.2074814CurrentTrain: epoch  2, batch    30 | loss: 9.0487986CurrentTrain: epoch  2, batch    31 | loss: 9.2916870CurrentTrain: epoch  2, batch    32 | loss: 9.3290749CurrentTrain: epoch  2, batch    33 | loss: 9.4184914CurrentTrain: epoch  2, batch    34 | loss: 9.0607290CurrentTrain: epoch  2, batch    35 | loss: 8.7415276CurrentTrain: epoch  2, batch    36 | loss: 9.1964741CurrentTrain: epoch  2, batch    37 | loss: 8.0984631CurrentTrain: epoch  3, batch     0 | loss: 8.7634735CurrentTrain: epoch  3, batch     1 | loss: 8.5392742CurrentTrain: epoch  3, batch     2 | loss: 8.4387150CurrentTrain: epoch  3, batch     3 | loss: 8.1969833CurrentTrain: epoch  3, batch     4 | loss: 8.8219252CurrentTrain: epoch  3, batch     5 | loss: 8.8303280CurrentTrain: epoch  3, batch     6 | loss: 8.4710054CurrentTrain: epoch  3, batch     7 | loss: 8.7251835CurrentTrain: epoch  3, batch     8 | loss: 8.6086216CurrentTrain: epoch  3, batch     9 | loss: 8.9033413CurrentTrain: epoch  3, batch    10 | loss: 8.9926348CurrentTrain: epoch  3, batch    11 | loss: 8.7833519CurrentTrain: epoch  3, batch    12 | loss: 8.7609520CurrentTrain: epoch  3, batch    13 | loss: 8.1260986CurrentTrain: epoch  3, batch    14 | loss: 8.5538025CurrentTrain: epoch  3, batch    15 | loss: 8.8514290CurrentTrain: epoch  3, batch    16 | loss: 7.5317769CurrentTrain: epoch  3, batch    17 | loss: 8.3651953CurrentTrain: epoch  3, batch    18 | loss: 7.8848872CurrentTrain: epoch  3, batch    19 | loss: 7.5525436CurrentTrain: epoch  3, batch    20 | loss: 7.7990646CurrentTrain: epoch  3, batch    21 | loss: 7.6564126CurrentTrain: epoch  3, batch    22 | loss: 7.4212389CurrentTrain: epoch  3, batch    23 | loss: 7.8855901CurrentTrain: epoch  3, batch    24 | loss: 7.9500942CurrentTrain: epoch  3, batch    25 | loss: 7.5206780CurrentTrain: epoch  3, batch    26 | loss: 7.7893233CurrentTrain: epoch  3, batch    27 | loss: 7.8702888CurrentTrain: epoch  3, batch    28 | loss: 8.0619345CurrentTrain: epoch  3, batch    29 | loss: 7.7077150CurrentTrain: epoch  3, batch    30 | loss: 7.3886871CurrentTrain: epoch  3, batch    31 | loss: 8.4251642CurrentTrain: epoch  3, batch    32 | loss: 7.5868087CurrentTrain: epoch  3, batch    33 | loss: 7.5882516CurrentTrain: epoch  3, batch    34 | loss: 6.9339910CurrentTrain: epoch  3, batch    35 | loss: 7.3130932CurrentTrain: epoch  3, batch    36 | loss: 8.0743551CurrentTrain: epoch  3, batch    37 | loss: 7.0664625CurrentTrain: epoch  4, batch     0 | loss: 6.6380844CurrentTrain: epoch  4, batch     1 | loss: 7.2541771CurrentTrain: epoch  4, batch     2 | loss: 9.2641106CurrentTrain: epoch  4, batch     3 | loss: 7.3518400CurrentTrain: epoch  4, batch     4 | loss: 7.6861129CurrentTrain: epoch  4, batch     5 | loss: 8.0305710CurrentTrain: epoch  4, batch     6 | loss: 7.2238226CurrentTrain: epoch  4, batch     7 | loss: 6.9953508CurrentTrain: epoch  4, batch     8 | loss: 6.6618524CurrentTrain: epoch  4, batch     9 | loss: 6.8272767CurrentTrain: epoch  4, batch    10 | loss: 7.2484307CurrentTrain: epoch  4, batch    11 | loss: 7.3559055CurrentTrain: epoch  4, batch    12 | loss: 7.3232651CurrentTrain: epoch  4, batch    13 | loss: 7.2183032CurrentTrain: epoch  4, batch    14 | loss: 7.1425533CurrentTrain: epoch  4, batch    15 | loss: 7.4733887CurrentTrain: epoch  4, batch    16 | loss: 7.4904127CurrentTrain: epoch  4, batch    17 | loss: 6.9774036CurrentTrain: epoch  4, batch    18 | loss: 7.5612555CurrentTrain: epoch  4, batch    19 | loss: 6.7826328CurrentTrain: epoch  4, batch    20 | loss: 6.7360144CurrentTrain: epoch  4, batch    21 | loss: 7.3960896CurrentTrain: epoch  4, batch    22 | loss: 7.0322952CurrentTrain: epoch  4, batch    23 | loss: 6.5397005CurrentTrain: epoch  4, batch    24 | loss: 7.0359526CurrentTrain: epoch  4, batch    25 | loss: 6.5932732CurrentTrain: epoch  4, batch    26 | loss: 7.6103048CurrentTrain: epoch  4, batch    27 | loss: 6.7949553CurrentTrain: epoch  4, batch    28 | loss: 6.6066551CurrentTrain: epoch  4, batch    29 | loss: 6.5685139CurrentTrain: epoch  4, batch    30 | loss: 6.7962294CurrentTrain: epoch  4, batch    31 | loss: 6.7943516CurrentTrain: epoch  4, batch    32 | loss: 6.7970715CurrentTrain: epoch  4, batch    33 | loss: 6.7183709CurrentTrain: epoch  4, batch    34 | loss: 7.2313976CurrentTrain: epoch  4, batch    35 | loss: 6.8534937CurrentTrain: epoch  4, batch    36 | loss: 6.1994243CurrentTrain: epoch  4, batch    37 | loss: 6.0464702CurrentTrain: epoch  5, batch     0 | loss: 6.3168073CurrentTrain: epoch  5, batch     1 | loss: 5.7673864CurrentTrain: epoch  5, batch     2 | loss: 5.6930647CurrentTrain: epoch  5, batch     3 | loss: 7.3025169CurrentTrain: epoch  5, batch     4 | loss: 7.3619976CurrentTrain: epoch  5, batch     5 | loss: 6.8611412CurrentTrain: epoch  5, batch     6 | loss: 6.2873001CurrentTrain: epoch  5, batch     7 | loss: 6.8364506CurrentTrain: epoch  5, batch     8 | loss: 6.7055807CurrentTrain: epoch  5, batch     9 | loss: 6.8886786CurrentTrain: epoch  5, batch    10 | loss: 6.7065258CurrentTrain: epoch  5, batch    11 | loss: 7.2128468CurrentTrain: epoch  5, batch    12 | loss: 5.6583242CurrentTrain: epoch  5, batch    13 | loss: 6.0550861CurrentTrain: epoch  5, batch    14 | loss: 6.6506119CurrentTrain: epoch  5, batch    15 | loss: 7.2390509CurrentTrain: epoch  5, batch    16 | loss: 6.0289636CurrentTrain: epoch  5, batch    17 | loss: 6.9670496CurrentTrain: epoch  5, batch    18 | loss: 6.4144602CurrentTrain: epoch  5, batch    19 | loss: 6.1512117CurrentTrain: epoch  5, batch    20 | loss: 6.6594677CurrentTrain: epoch  5, batch    21 | loss: 6.3009515CurrentTrain: epoch  5, batch    22 | loss: 6.2169447CurrentTrain: epoch  5, batch    23 | loss: 6.4360294CurrentTrain: epoch  5, batch    24 | loss: 6.5572286CurrentTrain: epoch  5, batch    25 | loss: 6.0863829CurrentTrain: epoch  5, batch    26 | loss: 6.3137541CurrentTrain: epoch  5, batch    27 | loss: 7.0995970CurrentTrain: epoch  5, batch    28 | loss: 6.8331852CurrentTrain: epoch  5, batch    29 | loss: 6.3394241CurrentTrain: epoch  5, batch    30 | loss: 5.8378611CurrentTrain: epoch  5, batch    31 | loss: 7.5874429CurrentTrain: epoch  5, batch    32 | loss: 5.5417142CurrentTrain: epoch  5, batch    33 | loss: 6.1071692CurrentTrain: epoch  5, batch    34 | loss: 6.5445933CurrentTrain: epoch  5, batch    35 | loss: 6.7509718CurrentTrain: epoch  5, batch    36 | loss: 6.3945923CurrentTrain: epoch  5, batch    37 | loss: 7.2535391CurrentTrain: epoch  6, batch     0 | loss: 6.3095818CurrentTrain: epoch  6, batch     1 | loss: 5.9782028CurrentTrain: epoch  6, batch     2 | loss: 6.2204156CurrentTrain: epoch  6, batch     3 | loss: 5.4703465CurrentTrain: epoch  6, batch     4 | loss: 6.2410355CurrentTrain: epoch  6, batch     5 | loss: 5.9929504CurrentTrain: epoch  6, batch     6 | loss: 5.6071777CurrentTrain: epoch  6, batch     7 | loss: 6.2583551CurrentTrain: epoch  6, batch     8 | loss: 6.6955719CurrentTrain: epoch  6, batch     9 | loss: 6.8535047CurrentTrain: epoch  6, batch    10 | loss: 5.8050203CurrentTrain: epoch  6, batch    11 | loss: 6.2029710CurrentTrain: epoch  6, batch    12 | loss: 6.3190713CurrentTrain: epoch  6, batch    13 | loss: 5.6392446CurrentTrain: epoch  6, batch    14 | loss: 6.3432016CurrentTrain: epoch  6, batch    15 | loss: 5.5204616CurrentTrain: epoch  6, batch    16 | loss: 5.4834595CurrentTrain: epoch  6, batch    17 | loss: 5.6340618CurrentTrain: epoch  6, batch    18 | loss: 5.8991318CurrentTrain: epoch  6, batch    19 | loss: 6.8890104CurrentTrain: epoch  6, batch    20 | loss: 6.0886350CurrentTrain: epoch  6, batch    21 | loss: 5.9312320CurrentTrain: epoch  6, batch    22 | loss: 5.9531012CurrentTrain: epoch  6, batch    23 | loss: 6.6452074CurrentTrain: epoch  6, batch    24 | loss: 5.5145388CurrentTrain: epoch  6, batch    25 | loss: 6.9718165CurrentTrain: epoch  6, batch    26 | loss: 6.7183013CurrentTrain: epoch  6, batch    27 | loss: 6.3720822CurrentTrain: epoch  6, batch    28 | loss: 6.7285686CurrentTrain: epoch  6, batch    29 | loss: 5.7569785CurrentTrain: epoch  6, batch    30 | loss: 5.5414734CurrentTrain: epoch  6, batch    31 | loss: 6.7894559CurrentTrain: epoch  6, batch    32 | loss: 7.0705795CurrentTrain: epoch  6, batch    33 | loss: 5.8326354CurrentTrain: epoch  6, batch    34 | loss: 6.3336926CurrentTrain: epoch  6, batch    35 | loss: 5.6500807CurrentTrain: epoch  6, batch    36 | loss: 6.0294762CurrentTrain: epoch  6, batch    37 | loss: 5.6381140CurrentTrain: epoch  7, batch     0 | loss: 6.0227833CurrentTrain: epoch  7, batch     1 | loss: 5.3038874CurrentTrain: epoch  7, batch     2 | loss: 5.7124062CurrentTrain: epoch  7, batch     3 | loss: 6.0079041CurrentTrain: epoch  7, batch     4 | loss: 5.5939388CurrentTrain: epoch  7, batch     5 | loss: 5.4710751CurrentTrain: epoch  7, batch     6 | loss: 5.5978851CurrentTrain: epoch  7, batch     7 | loss: 6.7532482CurrentTrain: epoch  7, batch     8 | loss: 5.5528831CurrentTrain: epoch  7, batch     9 | loss: 5.6948681CurrentTrain: epoch  7, batch    10 | loss: 5.2726822CurrentTrain: epoch  7, batch    11 | loss: 5.6621871CurrentTrain: epoch  7, batch    12 | loss: 5.2929740CurrentTrain: epoch  7, batch    13 | loss: 5.2242498CurrentTrain: epoch  7, batch    14 | loss: 5.5714636CurrentTrain: epoch  7, batch    15 | loss: 5.1159482CurrentTrain: epoch  7, batch    16 | loss: 6.2968392CurrentTrain: epoch  7, batch    17 | loss: 5.3422699CurrentTrain: epoch  7, batch    18 | loss: 6.1721654CurrentTrain: epoch  7, batch    19 | loss: 6.1076012CurrentTrain: epoch  7, batch    20 | loss: 5.3587074CurrentTrain: epoch  7, batch    21 | loss: 5.1251917CurrentTrain: epoch  7, batch    22 | loss: 5.0408020CurrentTrain: epoch  7, batch    23 | loss: 6.4462051CurrentTrain: epoch  7, batch    24 | loss: 6.1731119CurrentTrain: epoch  7, batch    25 | loss: 5.5540404CurrentTrain: epoch  7, batch    26 | loss: 5.2153244CurrentTrain: epoch  7, batch    27 | loss: 6.2918200CurrentTrain: epoch  7, batch    28 | loss: 6.3266430CurrentTrain: epoch  7, batch    29 | loss: 5.8750567CurrentTrain: epoch  7, batch    30 | loss: 5.9249244CurrentTrain: epoch  7, batch    31 | loss: 6.8203330CurrentTrain: epoch  7, batch    32 | loss: 5.8430672CurrentTrain: epoch  7, batch    33 | loss: 5.3519263CurrentTrain: epoch  7, batch    34 | loss: 6.1341395CurrentTrain: epoch  7, batch    35 | loss: 5.3656211CurrentTrain: epoch  7, batch    36 | loss: 6.1703701CurrentTrain: epoch  7, batch    37 | loss: 5.7364922CurrentTrain: epoch  8, batch     0 | loss: 5.4859352CurrentTrain: epoch  8, batch     1 | loss: 5.8194008CurrentTrain: epoch  8, batch     2 | loss: 5.5771379CurrentTrain: epoch  8, batch     3 | loss: 5.3644228CurrentTrain: epoch  8, batch     4 | loss: 5.5065670CurrentTrain: epoch  8, batch     5 | loss: 5.7192917CurrentTrain: epoch  8, batch     6 | loss: 5.5574255CurrentTrain: epoch  8, batch     7 | loss: 5.4108987CurrentTrain: epoch  8, batch     8 | loss: 5.5362425CurrentTrain: epoch  8, batch     9 | loss: 5.1671991CurrentTrain: epoch  8, batch    10 | loss: 5.7339878CurrentTrain: epoch  8, batch    11 | loss: 5.5385303CurrentTrain: epoch  8, batch    12 | loss: 5.0658050CurrentTrain: epoch  8, batch    13 | loss: 5.1306710CurrentTrain: epoch  8, batch    14 | loss: 5.2605982CurrentTrain: epoch  8, batch    15 | loss: 5.1647234CurrentTrain: epoch  8, batch    16 | loss: 5.3244629CurrentTrain: epoch  8, batch    17 | loss: 5.1037211CurrentTrain: epoch  8, batch    18 | loss: 5.1470208CurrentTrain: epoch  8, batch    19 | loss: 5.1922684CurrentTrain: epoch  8, batch    20 | loss: 5.1485696CurrentTrain: epoch  8, batch    21 | loss: 4.9268951CurrentTrain: epoch  8, batch    22 | loss: 5.3870921CurrentTrain: epoch  8, batch    23 | loss: 5.7505102CurrentTrain: epoch  8, batch    24 | loss: 4.9640093CurrentTrain: epoch  8, batch    25 | loss: 5.8391380CurrentTrain: epoch  8, batch    26 | loss: 5.2243609CurrentTrain: epoch  8, batch    27 | loss: 6.8320284CurrentTrain: epoch  8, batch    28 | loss: 5.5980911CurrentTrain: epoch  8, batch    29 | loss: 5.9854054CurrentTrain: epoch  8, batch    30 | loss: 4.8598742CurrentTrain: epoch  8, batch    31 | loss: 5.6486330CurrentTrain: epoch  8, batch    32 | loss: 5.5535092CurrentTrain: epoch  8, batch    33 | loss: 5.4233274CurrentTrain: epoch  8, batch    34 | loss: 4.9932013CurrentTrain: epoch  8, batch    35 | loss: 5.1957908CurrentTrain: epoch  8, batch    36 | loss: 5.0579071CurrentTrain: epoch  8, batch    37 | loss: 6.7120943CurrentTrain: epoch  9, batch     0 | loss: 5.2178879CurrentTrain: epoch  9, batch     1 | loss: 5.3535008CurrentTrain: epoch  9, batch     2 | loss: 5.6434994CurrentTrain: epoch  9, batch     3 | loss: 5.0070410CurrentTrain: epoch  9, batch     4 | loss: 6.1049929CurrentTrain: epoch  9, batch     5 | loss: 5.2583132CurrentTrain: epoch  9, batch     6 | loss: 5.1206608CurrentTrain: epoch  9, batch     7 | loss: 4.9678774CurrentTrain: epoch  9, batch     8 | loss: 5.1654387CurrentTrain: epoch  9, batch     9 | loss: 5.7712021CurrentTrain: epoch  9, batch    10 | loss: 5.3788013CurrentTrain: epoch  9, batch    11 | loss: 5.0173259CurrentTrain: epoch  9, batch    12 | loss: 5.1773057CurrentTrain: epoch  9, batch    13 | loss: 5.8456850CurrentTrain: epoch  9, batch    14 | loss: 5.3736439CurrentTrain: epoch  9, batch    15 | loss: 5.1530557CurrentTrain: epoch  9, batch    16 | loss: 5.4916868CurrentTrain: epoch  9, batch    17 | loss: 4.8803353CurrentTrain: epoch  9, batch    18 | loss: 5.0241857CurrentTrain: epoch  9, batch    19 | loss: 5.1250019CurrentTrain: epoch  9, batch    20 | loss: 5.0917225CurrentTrain: epoch  9, batch    21 | loss: 5.0521140CurrentTrain: epoch  9, batch    22 | loss: 5.2014322CurrentTrain: epoch  9, batch    23 | loss: 5.0195422CurrentTrain: epoch  9, batch    24 | loss: 5.5055962CurrentTrain: epoch  9, batch    25 | loss: 5.1939154CurrentTrain: epoch  9, batch    26 | loss: 5.0070872CurrentTrain: epoch  9, batch    27 | loss: 4.9116659CurrentTrain: epoch  9, batch    28 | loss: 5.6163902CurrentTrain: epoch  9, batch    29 | loss: 4.7394323CurrentTrain: epoch  9, batch    30 | loss: 4.9759216CurrentTrain: epoch  9, batch    31 | loss: 4.9849892CurrentTrain: epoch  9, batch    32 | loss: 5.0722485CurrentTrain: epoch  9, batch    33 | loss: 5.1837130CurrentTrain: epoch  9, batch    34 | loss: 4.9725266CurrentTrain: epoch  9, batch    35 | loss: 4.9859481CurrentTrain: epoch  9, batch    36 | loss: 5.1670613CurrentTrain: epoch  9, batch    37 | loss: 4.6834178
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 15.1334534CurrentTrain: epoch  0, batch     1 | loss: 14.2354469CurrentTrain: epoch  0, batch     2 | loss: 14.6398849CurrentTrain: epoch  0, batch     3 | loss: 14.4101372CurrentTrain: epoch  0, batch     4 | loss: 13.9319954CurrentTrain: epoch  0, batch     5 | loss: 14.6178665CurrentTrain: epoch  0, batch     6 | loss: 13.3956318CurrentTrain: epoch  0, batch     7 | loss: 14.0077972CurrentTrain: epoch  0, batch     8 | loss: 14.0363350CurrentTrain: epoch  0, batch     9 | loss: 13.5231028CurrentTrain: epoch  0, batch    10 | loss: 13.4495544CurrentTrain: epoch  0, batch    11 | loss: 13.5632010CurrentTrain: epoch  0, batch    12 | loss: 13.7617531CurrentTrain: epoch  0, batch    13 | loss: 13.3332119CurrentTrain: epoch  0, batch    14 | loss: 13.2904797CurrentTrain: epoch  0, batch    15 | loss: 13.4229002CurrentTrain: epoch  0, batch    16 | loss: 14.4028368CurrentTrain: epoch  0, batch    17 | loss: 13.8563967CurrentTrain: epoch  0, batch    18 | loss: 12.9158154CurrentTrain: epoch  0, batch    19 | loss: 12.6006279CurrentTrain: epoch  0, batch    20 | loss: 12.5576859CurrentTrain: epoch  0, batch    21 | loss: 12.9508238CurrentTrain: epoch  0, batch    22 | loss: 12.5000839CurrentTrain: epoch  0, batch    23 | loss: 12.4892883CurrentTrain: epoch  0, batch    24 | loss: 12.2693510CurrentTrain: epoch  0, batch    25 | loss: 12.4112291CurrentTrain: epoch  0, batch    26 | loss: 12.7016621CurrentTrain: epoch  0, batch    27 | loss: 12.6738167CurrentTrain: epoch  0, batch    28 | loss: 12.8739538CurrentTrain: epoch  0, batch    29 | loss: 11.7658386CurrentTrain: epoch  0, batch    30 | loss: 11.8782864CurrentTrain: epoch  0, batch    31 | loss: 11.6290255CurrentTrain: epoch  0, batch    32 | loss: 11.4625845CurrentTrain: epoch  0, batch    33 | loss: 12.8068485CurrentTrain: epoch  0, batch    34 | loss: 12.2603264CurrentTrain: epoch  0, batch    35 | loss: 11.8043251CurrentTrain: epoch  0, batch    36 | loss: 11.8438663CurrentTrain: epoch  0, batch    37 | loss: 10.6875992CurrentTrain: epoch  1, batch     0 | loss: 11.2565126CurrentTrain: epoch  1, batch     1 | loss: 11.9610014CurrentTrain: epoch  1, batch     2 | loss: 11.0282335CurrentTrain: epoch  1, batch     3 | loss: 10.5631390CurrentTrain: epoch  1, batch     4 | loss: 11.0052299CurrentTrain: epoch  1, batch     5 | loss: 10.4700508CurrentTrain: epoch  1, batch     6 | loss: 11.2258577CurrentTrain: epoch  1, batch     7 | loss: 10.9165726CurrentTrain: epoch  1, batch     8 | loss: 12.1261740CurrentTrain: epoch  1, batch     9 | loss: 10.1598587CurrentTrain: epoch  1, batch    10 | loss: 9.2328272CurrentTrain: epoch  1, batch    11 | loss: 9.9708214CurrentTrain: epoch  1, batch    12 | loss: 10.3166685CurrentTrain: epoch  1, batch    13 | loss: 10.3350277CurrentTrain: epoch  1, batch    14 | loss: 9.6781101CurrentTrain: epoch  1, batch    15 | loss: 9.8451262CurrentTrain: epoch  1, batch    16 | loss: 10.2515182CurrentTrain: epoch  1, batch    17 | loss: 10.8192348CurrentTrain: epoch  1, batch    18 | loss: 9.7617283CurrentTrain: epoch  1, batch    19 | loss: 10.8164377CurrentTrain: epoch  1, batch    20 | loss: 10.3851719CurrentTrain: epoch  1, batch    21 | loss: 9.7356501CurrentTrain: epoch  1, batch    22 | loss: 9.2956429CurrentTrain: epoch  1, batch    23 | loss: 10.0055332CurrentTrain: epoch  1, batch    24 | loss: 10.7895927CurrentTrain: epoch  1, batch    25 | loss: 10.3621244CurrentTrain: epoch  1, batch    26 | loss: 9.6146269CurrentTrain: epoch  1, batch    27 | loss: 9.5995426CurrentTrain: epoch  1, batch    28 | loss: 9.8693447CurrentTrain: epoch  1, batch    29 | loss: 10.9793596CurrentTrain: epoch  1, batch    30 | loss: 8.8530445CurrentTrain: epoch  1, batch    31 | loss: 10.0802202CurrentTrain: epoch  1, batch    32 | loss: 10.1206665CurrentTrain: epoch  1, batch    33 | loss: 10.0087471CurrentTrain: epoch  1, batch    34 | loss: 9.8208303CurrentTrain: epoch  1, batch    35 | loss: 9.0859203CurrentTrain: epoch  1, batch    36 | loss: 9.6340399CurrentTrain: epoch  1, batch    37 | loss: 8.4605541CurrentTrain: epoch  2, batch     0 | loss: 8.7790270CurrentTrain: epoch  2, batch     1 | loss: 8.5525284CurrentTrain: epoch  2, batch     2 | loss: 9.9654837CurrentTrain: epoch  2, batch     3 | loss: 8.3451071CurrentTrain: epoch  2, batch     4 | loss: 9.0028038CurrentTrain: epoch  2, batch     5 | loss: 9.2025681CurrentTrain: epoch  2, batch     6 | loss: 9.3265190CurrentTrain: epoch  2, batch     7 | loss: 9.5642633CurrentTrain: epoch  2, batch     8 | loss: 7.9808393CurrentTrain: epoch  2, batch     9 | loss: 8.0398617CurrentTrain: epoch  2, batch    10 | loss: 6.7350769CurrentTrain: epoch  2, batch    11 | loss: 7.1613636CurrentTrain: epoch  2, batch    12 | loss: 8.5480442CurrentTrain: epoch  2, batch    13 | loss: 7.8246393CurrentTrain: epoch  2, batch    14 | loss: 9.0090637CurrentTrain: epoch  2, batch    15 | loss: 9.1129103CurrentTrain: epoch  2, batch    16 | loss: 7.2276640CurrentTrain: epoch  2, batch    17 | loss: 9.9879484CurrentTrain: epoch  2, batch    18 | loss: 8.9157476CurrentTrain: epoch  2, batch    19 | loss: 8.1185665CurrentTrain: epoch  2, batch    20 | loss: 8.7581863CurrentTrain: epoch  2, batch    21 | loss: 8.0231857CurrentTrain: epoch  2, batch    22 | loss: 8.2702036CurrentTrain: epoch  2, batch    23 | loss: 8.0867252CurrentTrain: epoch  2, batch    24 | loss: 8.9409733CurrentTrain: epoch  2, batch    25 | loss: 7.2238374CurrentTrain: epoch  2, batch    26 | loss: 9.8668604CurrentTrain: epoch  2, batch    27 | loss: 8.9868298CurrentTrain: epoch  2, batch    28 | loss: 8.2750921CurrentTrain: epoch  2, batch    29 | loss: 7.3880143CurrentTrain: epoch  2, batch    30 | loss: 9.2949972CurrentTrain: epoch  2, batch    31 | loss: 9.0604582CurrentTrain: epoch  2, batch    32 | loss: 9.1489363CurrentTrain: epoch  2, batch    33 | loss: 9.3222055CurrentTrain: epoch  2, batch    34 | loss: 8.5324678CurrentTrain: epoch  2, batch    35 | loss: 7.7032881CurrentTrain: epoch  2, batch    36 | loss: 7.7569876CurrentTrain: epoch  2, batch    37 | loss: 6.5904784CurrentTrain: epoch  3, batch     0 | loss: 8.0840969CurrentTrain: epoch  3, batch     1 | loss: 8.2313967CurrentTrain: epoch  3, batch     2 | loss: 8.2400160CurrentTrain: epoch  3, batch     3 | loss: 7.2189550CurrentTrain: epoch  3, batch     4 | loss: 8.2174187CurrentTrain: epoch  3, batch     5 | loss: 7.6925220CurrentTrain: epoch  3, batch     6 | loss: 7.5639281CurrentTrain: epoch  3, batch     7 | loss: 8.7565260CurrentTrain: epoch  3, batch     8 | loss: 6.2168374CurrentTrain: epoch  3, batch     9 | loss: 8.9175606CurrentTrain: epoch  3, batch    10 | loss: 7.6663141CurrentTrain: epoch  3, batch    11 | loss: 6.6871285CurrentTrain: epoch  3, batch    12 | loss: 7.3071465CurrentTrain: epoch  3, batch    13 | loss: 7.6603837CurrentTrain: epoch  3, batch    14 | loss: 7.9673519CurrentTrain: epoch  3, batch    15 | loss: 9.0512877CurrentTrain: epoch  3, batch    16 | loss: 7.1016426CurrentTrain: epoch  3, batch    17 | loss: 8.0857220CurrentTrain: epoch  3, batch    18 | loss: 7.3868756CurrentTrain: epoch  3, batch    19 | loss: 7.6547265CurrentTrain: epoch  3, batch    20 | loss: 7.5698748CurrentTrain: epoch  3, batch    21 | loss: 6.4794230CurrentTrain: epoch  3, batch    22 | loss: 7.1424475CurrentTrain: epoch  3, batch    23 | loss: 7.1569538CurrentTrain: epoch  3, batch    24 | loss: 6.2907858CurrentTrain: epoch  3, batch    25 | loss: 7.3474779CurrentTrain: epoch  3, batch    26 | loss: 8.2912045CurrentTrain: epoch  3, batch    27 | loss: 7.7856355CurrentTrain: epoch  3, batch    28 | loss: 7.9683232CurrentTrain: epoch  3, batch    29 | loss: 7.3778710CurrentTrain: epoch  3, batch    30 | loss: 8.1636219CurrentTrain: epoch  3, batch    31 | loss: 7.5183554CurrentTrain: epoch  3, batch    32 | loss: 7.6202316CurrentTrain: epoch  3, batch    33 | loss: 6.5807395CurrentTrain: epoch  3, batch    34 | loss: 8.0615625CurrentTrain: epoch  3, batch    35 | loss: 6.7570205CurrentTrain: epoch  3, batch    36 | loss: 7.1811934CurrentTrain: epoch  3, batch    37 | loss: 8.2492523CurrentTrain: epoch  4, batch     0 | loss: 7.6522541CurrentTrain: epoch  4, batch     1 | loss: 6.3213959CurrentTrain: epoch  4, batch     2 | loss: 7.7468929CurrentTrain: epoch  4, batch     3 | loss: 7.3484421CurrentTrain: epoch  4, batch     4 | loss: 6.4006710CurrentTrain: epoch  4, batch     5 | loss: 7.2657275CurrentTrain: epoch  4, batch     6 | loss: 6.6454511CurrentTrain: epoch  4, batch     7 | loss: 6.6802382CurrentTrain: epoch  4, batch     8 | loss: 7.5963993CurrentTrain: epoch  4, batch     9 | loss: 6.5750909CurrentTrain: epoch  4, batch    10 | loss: 7.4436431CurrentTrain: epoch  4, batch    11 | loss: 6.6619234CurrentTrain: epoch  4, batch    12 | loss: 7.6095619CurrentTrain: epoch  4, batch    13 | loss: 6.7395673CurrentTrain: epoch  4, batch    14 | loss: 6.9773455CurrentTrain: epoch  4, batch    15 | loss: 8.2662134CurrentTrain: epoch  4, batch    16 | loss: 5.9852357CurrentTrain: epoch  4, batch    17 | loss: 6.9540677CurrentTrain: epoch  4, batch    18 | loss: 6.4795370CurrentTrain: epoch  4, batch    19 | loss: 7.0089397CurrentTrain: epoch  4, batch    20 | loss: 7.4126663CurrentTrain: epoch  4, batch    21 | loss: 6.1928005CurrentTrain: epoch  4, batch    22 | loss: 7.3243642CurrentTrain: epoch  4, batch    23 | loss: 6.0816355CurrentTrain: epoch  4, batch    24 | loss: 7.7670746CurrentTrain: epoch  4, batch    25 | loss: 6.6258512CurrentTrain: epoch  4, batch    26 | loss: 6.2208529CurrentTrain: epoch  4, batch    27 | loss: 8.6202374CurrentTrain: epoch  4, batch    28 | loss: 6.0882053CurrentTrain: epoch  4, batch    29 | loss: 6.1788440CurrentTrain: epoch  4, batch    30 | loss: 6.3824739CurrentTrain: epoch  4, batch    31 | loss: 7.5721631CurrentTrain: epoch  4, batch    32 | loss: 7.1719084CurrentTrain: epoch  4, batch    33 | loss: 7.6137333CurrentTrain: epoch  4, batch    34 | loss: 7.1806860CurrentTrain: epoch  4, batch    35 | loss: 6.7923827CurrentTrain: epoch  4, batch    36 | loss: 6.0710583CurrentTrain: epoch  4, batch    37 | loss: 7.9064608CurrentTrain: epoch  5, batch     0 | loss: 6.6199522CurrentTrain: epoch  5, batch     1 | loss: 6.5348763CurrentTrain: epoch  5, batch     2 | loss: 6.6322351CurrentTrain: epoch  5, batch     3 | loss: 6.5839343CurrentTrain: epoch  5, batch     4 | loss: 5.9638577CurrentTrain: epoch  5, batch     5 | loss: 6.5632882CurrentTrain: epoch  5, batch     6 | loss: 5.9908900CurrentTrain: epoch  5, batch     7 | loss: 5.8972907CurrentTrain: epoch  5, batch     8 | loss: 6.1888142CurrentTrain: epoch  5, batch     9 | loss: 5.6595197CurrentTrain: epoch  5, batch    10 | loss: 6.0353637CurrentTrain: epoch  5, batch    11 | loss: 7.2436485CurrentTrain: epoch  5, batch    12 | loss: 5.5970774CurrentTrain: epoch  5, batch    13 | loss: 7.1795645CurrentTrain: epoch  5, batch    14 | loss: 6.6942577CurrentTrain: epoch  5, batch    15 | loss: 6.7713718CurrentTrain: epoch  5, batch    16 | loss: 6.6560216CurrentTrain: epoch  5, batch    17 | loss: 6.0969405CurrentTrain: epoch  5, batch    18 | loss: 6.3702412CurrentTrain: epoch  5, batch    19 | loss: 6.0812721CurrentTrain: epoch  5, batch    20 | loss: 5.4190140CurrentTrain: epoch  5, batch    21 | loss: 5.7941141CurrentTrain: epoch  5, batch    22 | loss: 5.7845535CurrentTrain: epoch  5, batch    23 | loss: 5.9447365CurrentTrain: epoch  5, batch    24 | loss: 6.1296844CurrentTrain: epoch  5, batch    25 | loss: 5.9501715CurrentTrain: epoch  5, batch    26 | loss: 6.2983198CurrentTrain: epoch  5, batch    27 | loss: 6.1682358CurrentTrain: epoch  5, batch    28 | loss: 6.7979541CurrentTrain: epoch  5, batch    29 | loss: 7.4967194CurrentTrain: epoch  5, batch    30 | loss: 6.0171928CurrentTrain: epoch  5, batch    31 | loss: 5.7878985CurrentTrain: epoch  5, batch    32 | loss: 6.8217235CurrentTrain: epoch  5, batch    33 | loss: 6.2987461CurrentTrain: epoch  5, batch    34 | loss: 6.7745018CurrentTrain: epoch  5, batch    35 | loss: 5.7637086CurrentTrain: epoch  5, batch    36 | loss: 5.8992581CurrentTrain: epoch  5, batch    37 | loss: 6.4161401CurrentTrain: epoch  6, batch     0 | loss: 5.8149371CurrentTrain: epoch  6, batch     1 | loss: 5.8669939CurrentTrain: epoch  6, batch     2 | loss: 5.8043127CurrentTrain: epoch  6, batch     3 | loss: 5.8827133CurrentTrain: epoch  6, batch     4 | loss: 6.7534008CurrentTrain: epoch  6, batch     5 | loss: 5.4766221CurrentTrain: epoch  6, batch     6 | loss: 5.9243479CurrentTrain: epoch  6, batch     7 | loss: 5.4080057CurrentTrain: epoch  6, batch     8 | loss: 6.3823357CurrentTrain: epoch  6, batch     9 | loss: 6.1535335CurrentTrain: epoch  6, batch    10 | loss: 6.0682468CurrentTrain: epoch  6, batch    11 | loss: 5.6522889CurrentTrain: epoch  6, batch    12 | loss: 5.6972351CurrentTrain: epoch  6, batch    13 | loss: 5.9130273CurrentTrain: epoch  6, batch    14 | loss: 5.3930626CurrentTrain: epoch  6, batch    15 | loss: 5.3957415CurrentTrain: epoch  6, batch    16 | loss: 5.8358278CurrentTrain: epoch  6, batch    17 | loss: 5.4883342CurrentTrain: epoch  6, batch    18 | loss: 5.5621243CurrentTrain: epoch  6, batch    19 | loss: 5.6650381CurrentTrain: epoch  6, batch    20 | loss: 5.5951309CurrentTrain: epoch  6, batch    21 | loss: 6.0814209CurrentTrain: epoch  6, batch    22 | loss: 5.0093884CurrentTrain: epoch  6, batch    23 | loss: 5.5895529CurrentTrain: epoch  6, batch    24 | loss: 5.1503701CurrentTrain: epoch  6, batch    25 | loss: 7.4911585CurrentTrain: epoch  6, batch    26 | loss: 5.6757488CurrentTrain: epoch  6, batch    27 | loss: 5.9877810CurrentTrain: epoch  6, batch    28 | loss: 7.0048375CurrentTrain: epoch  6, batch    29 | loss: 5.2955847CurrentTrain: epoch  6, batch    30 | loss: 5.7189732CurrentTrain: epoch  6, batch    31 | loss: 5.6103072CurrentTrain: epoch  6, batch    32 | loss: 5.4853864CurrentTrain: epoch  6, batch    33 | loss: 5.3433628CurrentTrain: epoch  6, batch    34 | loss: 5.6092672CurrentTrain: epoch  6, batch    35 | loss: 7.0674677CurrentTrain: epoch  6, batch    36 | loss: 6.0962901CurrentTrain: epoch  6, batch    37 | loss: 5.4159484CurrentTrain: epoch  7, batch     0 | loss: 5.4881759CurrentTrain: epoch  7, batch     1 | loss: 5.7482448CurrentTrain: epoch  7, batch     2 | loss: 5.2258286CurrentTrain: epoch  7, batch     3 | loss: 5.7489471CurrentTrain: epoch  7, batch     4 | loss: 6.1385984CurrentTrain: epoch  7, batch     5 | loss: 5.3459821CurrentTrain: epoch  7, batch     6 | loss: 5.2793393CurrentTrain: epoch  7, batch     7 | loss: 6.3341641CurrentTrain: epoch  7, batch     8 | loss: 5.2272625CurrentTrain: epoch  7, batch     9 | loss: 5.4999957CurrentTrain: epoch  7, batch    10 | loss: 5.6575956CurrentTrain: epoch  7, batch    11 | loss: 5.5882330CurrentTrain: epoch  7, batch    12 | loss: 5.0407882CurrentTrain: epoch  7, batch    13 | loss: 5.1591272CurrentTrain: epoch  7, batch    14 | loss: 5.3131418CurrentTrain: epoch  7, batch    15 | loss: 5.1386695CurrentTrain: epoch  7, batch    16 | loss: 5.3853149CurrentTrain: epoch  7, batch    17 | loss: 5.1793599CurrentTrain: epoch  7, batch    18 | loss: 5.3290792CurrentTrain: epoch  7, batch    19 | loss: 5.2740588CurrentTrain: epoch  7, batch    20 | loss: 5.4381447CurrentTrain: epoch  7, batch    21 | loss: 5.3323321CurrentTrain: epoch  7, batch    22 | loss: 5.3221431CurrentTrain: epoch  7, batch    23 | loss: 6.9934888CurrentTrain: epoch  7, batch    24 | loss: 5.2992435CurrentTrain: epoch  7, batch    25 | loss: 5.2281747CurrentTrain: epoch  7, batch    26 | loss: 5.1955781CurrentTrain: epoch  7, batch    27 | loss: 5.1301589CurrentTrain: epoch  7, batch    28 | loss: 5.1183939CurrentTrain: epoch  7, batch    29 | loss: 5.1898046CurrentTrain: epoch  7, batch    30 | loss: 5.2521935CurrentTrain: epoch  7, batch    31 | loss: 4.9850402CurrentTrain: epoch  7, batch    32 | loss: 5.3095832CurrentTrain: epoch  7, batch    33 | loss: 5.0948009CurrentTrain: epoch  7, batch    34 | loss: 5.7096534CurrentTrain: epoch  7, batch    35 | loss: 5.4209247CurrentTrain: epoch  7, batch    36 | loss: 6.0903559CurrentTrain: epoch  7, batch    37 | loss: 5.4506421CurrentTrain: epoch  8, batch     0 | loss: 5.0274210CurrentTrain: epoch  8, batch     1 | loss: 5.1971097CurrentTrain: epoch  8, batch     2 | loss: 5.3356342CurrentTrain: epoch  8, batch     3 | loss: 5.3313074CurrentTrain: epoch  8, batch     4 | loss: 5.1206918CurrentTrain: epoch  8, batch     5 | loss: 5.2245288CurrentTrain: epoch  8, batch     6 | loss: 5.5065942CurrentTrain: epoch  8, batch     7 | loss: 5.0918407CurrentTrain: epoch  8, batch     8 | loss: 4.9978046CurrentTrain: epoch  8, batch     9 | loss: 5.2045097CurrentTrain: epoch  8, batch    10 | loss: 4.9232917CurrentTrain: epoch  8, batch    11 | loss: 5.4944024CurrentTrain: epoch  8, batch    12 | loss: 5.3884363CurrentTrain: epoch  8, batch    13 | loss: 5.0104103CurrentTrain: epoch  8, batch    14 | loss: 6.3473101CurrentTrain: epoch  8, batch    15 | loss: 5.1371212CurrentTrain: epoch  8, batch    16 | loss: 5.0765972CurrentTrain: epoch  8, batch    17 | loss: 5.2641616CurrentTrain: epoch  8, batch    18 | loss: 5.1782074CurrentTrain: epoch  8, batch    19 | loss: 5.3980808CurrentTrain: epoch  8, batch    20 | loss: 5.3029027CurrentTrain: epoch  8, batch    21 | loss: 5.3433299CurrentTrain: epoch  8, batch    22 | loss: 5.1196795CurrentTrain: epoch  8, batch    23 | loss: 4.8880854CurrentTrain: epoch  8, batch    24 | loss: 4.8930578CurrentTrain: epoch  8, batch    25 | loss: 5.0870433CurrentTrain: epoch  8, batch    26 | loss: 5.0102057CurrentTrain: epoch  8, batch    27 | loss: 5.3646383CurrentTrain: epoch  8, batch    28 | loss: 4.8697624CurrentTrain: epoch  8, batch    29 | loss: 4.9491515CurrentTrain: epoch  8, batch    30 | loss: 4.9654217CurrentTrain: epoch  8, batch    31 | loss: 4.7536960CurrentTrain: epoch  8, batch    32 | loss: 4.8667779CurrentTrain: epoch  8, batch    33 | loss: 5.1346254CurrentTrain: epoch  8, batch    34 | loss: 5.1075611CurrentTrain: epoch  8, batch    35 | loss: 5.2968912CurrentTrain: epoch  8, batch    36 | loss: 5.0319552CurrentTrain: epoch  8, batch    37 | loss: 5.1645465CurrentTrain: epoch  9, batch     0 | loss: 4.9626031CurrentTrain: epoch  9, batch     1 | loss: 5.1435113CurrentTrain: epoch  9, batch     2 | loss: 4.8668432CurrentTrain: epoch  9, batch     3 | loss: 5.2332263CurrentTrain: epoch  9, batch     4 | loss: 5.3047395CurrentTrain: epoch  9, batch     5 | loss: 4.8478003CurrentTrain: epoch  9, batch     6 | loss: 4.9402995CurrentTrain: epoch  9, batch     7 | loss: 5.1233068CurrentTrain: epoch  9, batch     8 | loss: 5.0389333CurrentTrain: epoch  9, batch     9 | loss: 5.0288377CurrentTrain: epoch  9, batch    10 | loss: 5.0371070CurrentTrain: epoch  9, batch    11 | loss: 4.8254914CurrentTrain: epoch  9, batch    12 | loss: 4.9070477CurrentTrain: epoch  9, batch    13 | loss: 4.8645959CurrentTrain: epoch  9, batch    14 | loss: 5.3551655CurrentTrain: epoch  9, batch    15 | loss: 4.9164596CurrentTrain: epoch  9, batch    16 | loss: 4.8659701CurrentTrain: epoch  9, batch    17 | loss: 4.9632864CurrentTrain: epoch  9, batch    18 | loss: 5.2324176CurrentTrain: epoch  9, batch    19 | loss: 4.8884878CurrentTrain: epoch  9, batch    20 | loss: 5.3221421CurrentTrain: epoch  9, batch    21 | loss: 5.3627038CurrentTrain: epoch  9, batch    22 | loss: 5.0166583CurrentTrain: epoch  9, batch    23 | loss: 5.0562320CurrentTrain: epoch  9, batch    24 | loss: 4.7907028CurrentTrain: epoch  9, batch    25 | loss: 5.1461883CurrentTrain: epoch  9, batch    26 | loss: 5.3968067CurrentTrain: epoch  9, batch    27 | loss: 4.8694315CurrentTrain: epoch  9, batch    28 | loss: 4.9892631CurrentTrain: epoch  9, batch    29 | loss: 4.9010134CurrentTrain: epoch  9, batch    30 | loss: 4.8280187CurrentTrain: epoch  9, batch    31 | loss: 4.9598503CurrentTrain: epoch  9, batch    32 | loss: 5.0324001CurrentTrain: epoch  9, batch    33 | loss: 5.4922791CurrentTrain: epoch  9, batch    34 | loss: 4.9496374CurrentTrain: epoch  9, batch    35 | loss: 4.9834256CurrentTrain: epoch  9, batch    36 | loss: 5.4468045CurrentTrain: epoch  9, batch    37 | loss: 4.6723943
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.36%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 86.98%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 85.42%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 84.51%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.30%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.57%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.05%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 87.07%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 87.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.70%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.17%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.36%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 86.98%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 85.42%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 84.51%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.30%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.57%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.05%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 87.07%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 87.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.70%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.17%   
cur_acc_llm:  [0.8617424242424242]
his_acc_llm:  [0.8617424242424242]
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.55%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.93%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 83.68%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.63%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.05%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.68%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.78%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.04%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.93%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.51%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.67%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 87.12%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.55%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.93%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 83.68%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.63%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.05%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.68%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.78%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.04%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.93%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.51%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.67%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 87.12%   
cur_acc:  ['0.8712']
his_acc:  ['0.8712']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 7.0535045CurrentTrain: epoch  0, batch     1 | loss: 7.0335197CurrentTrain: epoch  1, batch     0 | loss: 6.1296787CurrentTrain: epoch  1, batch     1 | loss: 4.1665487CurrentTrain: epoch  2, batch     0 | loss: 3.7980928CurrentTrain: epoch  2, batch     1 | loss: 5.4262137CurrentTrain: epoch  3, batch     0 | loss: 5.0973053CurrentTrain: epoch  3, batch     1 | loss: 4.2052827CurrentTrain: epoch  4, batch     0 | loss: 4.6224957CurrentTrain: epoch  4, batch     1 | loss: 3.2050412CurrentTrain: epoch  5, batch     0 | loss: 4.3480921CurrentTrain: epoch  5, batch     1 | loss: 2.3222649CurrentTrain: epoch  6, batch     0 | loss: 3.5640936CurrentTrain: epoch  6, batch     1 | loss: 3.2633953CurrentTrain: epoch  7, batch     0 | loss: 3.1700397CurrentTrain: epoch  7, batch     1 | loss: 2.8571618CurrentTrain: epoch  8, batch     0 | loss: 2.9725270CurrentTrain: epoch  8, batch     1 | loss: 2.6692634CurrentTrain: epoch  9, batch     0 | loss: 2.5374713CurrentTrain: epoch  9, batch     1 | loss: 3.1554461
Mixup data size:  60
MixupTrain:  epoch  0, batch     0 | loss: 7.0537467MixupTrain:  epoch  0, batch     1 | loss: 5.2341647MixupTrain:  epoch  0, batch     2 | loss: 5.8461927MixupTrain:  epoch  0, batch     3 | loss: 5.3124053
MemoryTrain:  epoch  0, batch     0 | loss: 2.0977998MemoryTrain:  epoch  1, batch     0 | loss: 1.9454263MemoryTrain:  epoch  2, batch     0 | loss: 1.2150873MemoryTrain:  epoch  3, batch     0 | loss: 0.8893413MemoryTrain:  epoch  4, batch     0 | loss: 0.5139521MemoryTrain:  epoch  5, batch     0 | loss: 0.1795800MemoryTrain:  epoch  6, batch     0 | loss: 0.0976653MemoryTrain:  epoch  7, batch     0 | loss: 0.0743260MemoryTrain:  epoch  8, batch     0 | loss: 0.0599899MemoryTrain:  epoch  9, batch     0 | loss: 0.0242789
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 7.7539892CurrentTrain: epoch  0, batch     1 | loss: 7.5501189CurrentTrain: epoch  1, batch     0 | loss: 8.1678505CurrentTrain: epoch  1, batch     1 | loss: 4.7903037CurrentTrain: epoch  2, batch     0 | loss: 6.2518287CurrentTrain: epoch  2, batch     1 | loss: 5.5020223CurrentTrain: epoch  3, batch     0 | loss: 5.9047852CurrentTrain: epoch  3, batch     1 | loss: 4.4673481CurrentTrain: epoch  4, batch     0 | loss: 4.9321585CurrentTrain: epoch  4, batch     1 | loss: 4.6036248CurrentTrain: epoch  5, batch     0 | loss: 4.8658600CurrentTrain: epoch  5, batch     1 | loss: 3.2206793CurrentTrain: epoch  6, batch     0 | loss: 4.3475828CurrentTrain: epoch  6, batch     1 | loss: 3.3081012CurrentTrain: epoch  7, batch     0 | loss: 4.0837140CurrentTrain: epoch  7, batch     1 | loss: 3.8209858CurrentTrain: epoch  8, batch     0 | loss: 3.9047556CurrentTrain: epoch  8, batch     1 | loss: 3.3522255CurrentTrain: epoch  9, batch     0 | loss: 4.2922812CurrentTrain: epoch  9, batch     1 | loss: 2.3344028
Mixup data size:  60
MixupTrain:  epoch  0, batch     1 | loss: 6.9068414MixupTrain:  epoch  0, batch     3 | loss: 5.9516949
MemoryTrain:  epoch  0, batch     0 | loss: 3.6576481MemoryTrain:  epoch  1, batch     0 | loss: 3.6769691MemoryTrain:  epoch  2, batch     0 | loss: 3.5135009MemoryTrain:  epoch  3, batch     0 | loss: 2.8273373MemoryTrain:  epoch  4, batch     0 | loss: 2.1653440MemoryTrain:  epoch  5, batch     0 | loss: 1.9734488MemoryTrain:  epoch  6, batch     0 | loss: 1.8825421MemoryTrain:  epoch  7, batch     0 | loss: 1.8029691MemoryTrain:  epoch  8, batch     0 | loss: 1.3315020MemoryTrain:  epoch  9, batch     0 | loss: 0.9717735
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 51.56%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 46.25%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 48.96%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 47.32%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 41.41%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 27.08%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 26.56%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 27.50%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 25.00%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 32.14%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 40.62%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 46.53%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 51.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 55.11%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 58.33%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 61.06%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 62.05%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 62.89%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 63.60%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 64.58%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 66.12%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 67.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 69.05%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 70.45%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 74.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 75.69%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 77.16%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 77.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 78.43%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 78.71%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 78.41%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 78.86%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 77.50%   [EVAL] batch:   35 | acc: 31.25%,  total acc: 76.22%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 75.00%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 74.18%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 73.56%   [EVAL] batch:   39 | acc: 18.75%,  total acc: 72.19%   
cur_acc_llm:  [0.8617424242424242, 0.4140625]
his_acc_llm:  [0.8617424242424242, 0.721875]
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 71.09%   
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 16.67%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 15.62%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 16.25%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 14.58%   [EVAL] batch:    6 | acc: 6.25%,  total acc: 13.39%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 12.50%   [EVAL] batch:    8 | acc: 18.75%,  total acc: 13.19%   [EVAL] batch:    9 | acc: 18.75%,  total acc: 13.75%   [EVAL] batch:   10 | acc: 12.50%,  total acc: 13.64%   [EVAL] batch:   11 | acc: 18.75%,  total acc: 14.06%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 16.35%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 20.09%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 23.75%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 26.17%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 29.04%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 30.90%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 32.89%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 35.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 38.69%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 41.48%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 44.02%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 46.35%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 48.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 50.48%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 52.08%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 53.79%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 55.39%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 56.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 58.06%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 59.38%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 60.23%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 61.21%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 60.36%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 60.76%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 61.49%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 62.17%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 63.14%   [EVAL] batch:   39 | acc: 43.75%,  total acc: 62.66%   
cur_acc:  ['0.8712', '0.7109']
his_acc:  ['0.8712', '0.6266']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 4.2675076CurrentTrain: epoch  0, batch     1 | loss: 5.1187463CurrentTrain: epoch  1, batch     0 | loss: 3.2171402CurrentTrain: epoch  1, batch     1 | loss: 3.0124686CurrentTrain: epoch  2, batch     0 | loss: 2.9929447CurrentTrain: epoch  2, batch     1 | loss: 2.3433416CurrentTrain: epoch  3, batch     0 | loss: 2.5217516CurrentTrain: epoch  3, batch     1 | loss: 2.2819169CurrentTrain: epoch  4, batch     0 | loss: 2.1890240CurrentTrain: epoch  4, batch     1 | loss: 1.9413956CurrentTrain: epoch  5, batch     0 | loss: 1.9244156CurrentTrain: epoch  5, batch     1 | loss: 1.9247556CurrentTrain: epoch  6, batch     0 | loss: 1.7948183CurrentTrain: epoch  6, batch     1 | loss: 1.7553711CurrentTrain: epoch  7, batch     0 | loss: 1.8771489CurrentTrain: epoch  7, batch     1 | loss: 1.8542207CurrentTrain: epoch  8, batch     0 | loss: 1.8514092CurrentTrain: epoch  8, batch     1 | loss: 1.6986743CurrentTrain: epoch  9, batch     0 | loss: 1.7369750CurrentTrain: epoch  9, batch     1 | loss: 1.7695429
Mixup data size:  71
MixupTrain:  epoch  0, batch     1 | loss: 3.4887273MixupTrain:  epoch  0, batch     4 | loss: 2.3648764
MemoryTrain:  epoch  0, batch     0 | loss: 2.0463901MemoryTrain:  epoch  1, batch     0 | loss: 1.9553691MemoryTrain:  epoch  2, batch     0 | loss: 1.3998936MemoryTrain:  epoch  3, batch     0 | loss: 0.9006329MemoryTrain:  epoch  4, batch     0 | loss: 0.8113738MemoryTrain:  epoch  5, batch     0 | loss: 0.5244519MemoryTrain:  epoch  6, batch     0 | loss: 0.2851271MemoryTrain:  epoch  7, batch     0 | loss: 0.1084526MemoryTrain:  epoch  8, batch     0 | loss: 0.0351883MemoryTrain:  epoch  9, batch     0 | loss: 0.0181910
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 6.5513725CurrentTrain: epoch  0, batch     1 | loss: 5.9935188CurrentTrain: epoch  1, batch     0 | loss: 5.2713556CurrentTrain: epoch  1, batch     1 | loss: 5.4922943CurrentTrain: epoch  2, batch     0 | loss: 4.5668817CurrentTrain: epoch  2, batch     1 | loss: 4.6623268CurrentTrain: epoch  3, batch     0 | loss: 4.4533291CurrentTrain: epoch  3, batch     1 | loss: 3.5933809CurrentTrain: epoch  4, batch     0 | loss: 3.9707923CurrentTrain: epoch  4, batch     1 | loss: 3.2118111CurrentTrain: epoch  5, batch     0 | loss: 3.5807781CurrentTrain: epoch  5, batch     1 | loss: 3.0325675CurrentTrain: epoch  6, batch     0 | loss: 3.5779507CurrentTrain: epoch  6, batch     1 | loss: 2.3407445CurrentTrain: epoch  7, batch     0 | loss: 2.7653539CurrentTrain: epoch  7, batch     1 | loss: 2.6200411CurrentTrain: epoch  8, batch     0 | loss: 2.7073464CurrentTrain: epoch  8, batch     1 | loss: 2.9117932CurrentTrain: epoch  9, batch     0 | loss: 2.6344161CurrentTrain: epoch  9, batch     1 | loss: 2.4130681
Mixup data size:  69

MemoryTrain:  epoch  0, batch     0 | loss: 2.9639649MemoryTrain:  epoch  1, batch     0 | loss: 3.5593777MemoryTrain:  epoch  2, batch     0 | loss: 3.4787471MemoryTrain:  epoch  3, batch     0 | loss: 2.5876334MemoryTrain:  epoch  4, batch     0 | loss: 2.0874441MemoryTrain:  epoch  5, batch     0 | loss: 1.9581460MemoryTrain:  epoch  6, batch     0 | loss: 1.9240000MemoryTrain:  epoch  7, batch     0 | loss: 1.4349232MemoryTrain:  epoch  8, batch     0 | loss: 1.5598969MemoryTrain:  epoch  9, batch     0 | loss: 1.4899983
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 97.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 98.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 97.92%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 93.12%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 91.07%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 34.38%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 33.75%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 31.25%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 38.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 46.09%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 51.39%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 59.66%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 61.98%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 61.54%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 60.27%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 60.42%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 60.94%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 61.76%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 62.15%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 63.49%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 65.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 68.18%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 72.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.08%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 73.84%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 74.78%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 75.43%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 75.83%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 76.41%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 76.95%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 76.52%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 77.02%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 75.71%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 74.31%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 72.80%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 71.71%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 70.51%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 70.16%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 70.73%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 71.28%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 71.95%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 72.59%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 73.19%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 73.78%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 74.34%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 74.87%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 74.49%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 74.50%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 74.63%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 75.12%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 75.59%   [EVAL] batch:   53 | acc: 25.00%,  total acc: 74.65%   
cur_acc_llm:  [0.8617424242424242, 0.4140625, 0.9107142857142857]
his_acc_llm:  [0.8617424242424242, 0.721875, 0.7465277777777778]
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 96.43%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 95.83%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 86.98%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 86.16%   
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 16.67%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 18.75%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 15.62%   [EVAL] batch:    6 | acc: 12.50%,  total acc: 15.18%   [EVAL] batch:    7 | acc: 25.00%,  total acc: 16.41%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 19.44%   [EVAL] batch:    9 | acc: 25.00%,  total acc: 20.00%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 21.02%   [EVAL] batch:   11 | acc: 25.00%,  total acc: 21.35%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 22.60%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 24.55%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 27.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 29.30%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 31.99%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 33.68%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 35.20%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 37.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 40.48%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 43.18%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 45.65%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 47.92%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 50.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 51.92%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 53.47%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 55.13%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 56.68%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 57.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 59.27%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 60.35%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 60.98%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 61.95%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 61.07%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 61.28%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 61.99%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 62.83%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 63.78%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 64.38%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 64.94%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 66.42%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 67.92%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 68.61%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 69.28%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 69.92%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 69.52%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 69.38%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 69.12%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 69.59%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 70.05%   [EVAL] batch:   53 | acc: 25.00%,  total acc: 69.21%   
cur_acc:  ['0.8712', '0.7109', '0.8616']
his_acc:  ['0.8712', '0.6266', '0.6921']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 5.7369919CurrentTrain: epoch  0, batch     1 | loss: 4.8650422CurrentTrain: epoch  1, batch     0 | loss: 4.9742823CurrentTrain: epoch  1, batch     1 | loss: 5.0571966CurrentTrain: epoch  2, batch     0 | loss: 3.9633477CurrentTrain: epoch  2, batch     1 | loss: 3.6320436CurrentTrain: epoch  3, batch     0 | loss: 3.1115005CurrentTrain: epoch  3, batch     1 | loss: 2.5344088CurrentTrain: epoch  4, batch     0 | loss: 2.6666727CurrentTrain: epoch  4, batch     1 | loss: 2.7018926CurrentTrain: epoch  5, batch     0 | loss: 2.3491073CurrentTrain: epoch  5, batch     1 | loss: 2.1928585CurrentTrain: epoch  6, batch     0 | loss: 2.1784298CurrentTrain: epoch  6, batch     1 | loss: 2.0128515CurrentTrain: epoch  7, batch     0 | loss: 1.9950454CurrentTrain: epoch  7, batch     1 | loss: 1.9058713CurrentTrain: epoch  8, batch     0 | loss: 1.8824291CurrentTrain: epoch  8, batch     1 | loss: 1.8337075CurrentTrain: epoch  9, batch     0 | loss: 1.9289796CurrentTrain: epoch  9, batch     1 | loss: 1.7685257
Mixup data size:  81
MixupTrain:  epoch  0, batch     1 | loss: 2.8214546MixupTrain:  epoch  0, batch     5 | loss: 1.2443914
MemoryTrain:  epoch  0, batch     0 | loss: 1.5121697MemoryTrain:  epoch  0, batch     1 | loss: 0.0296491MemoryTrain:  epoch  1, batch     0 | loss: 1.2278891MemoryTrain:  epoch  1, batch     1 | loss: 0.6600089MemoryTrain:  epoch  2, batch     0 | loss: 0.5362215MemoryTrain:  epoch  2, batch     1 | loss: 0.2138868MemoryTrain:  epoch  3, batch     0 | loss: 0.2813250MemoryTrain:  epoch  3, batch     1 | loss: 0.0256524MemoryTrain:  epoch  4, batch     0 | loss: 0.0423710MemoryTrain:  epoch  4, batch     1 | loss: 0.1045261MemoryTrain:  epoch  5, batch     0 | loss: 0.0537607MemoryTrain:  epoch  5, batch     1 | loss: 0.0119199MemoryTrain:  epoch  6, batch     0 | loss: 0.0270490MemoryTrain:  epoch  6, batch     1 | loss: 0.0904068MemoryTrain:  epoch  7, batch     0 | loss: 0.0226648MemoryTrain:  epoch  7, batch     1 | loss: 0.0594815MemoryTrain:  epoch  8, batch     0 | loss: 0.0162404MemoryTrain:  epoch  8, batch     1 | loss: 0.0213152MemoryTrain:  epoch  9, batch     0 | loss: 0.0134998MemoryTrain:  epoch  9, batch     1 | loss: 0.0049172
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 8.6318712CurrentTrain: epoch  0, batch     1 | loss: 7.7634368CurrentTrain: epoch  1, batch     0 | loss: 8.2102747CurrentTrain: epoch  1, batch     1 | loss: 5.6336136CurrentTrain: epoch  2, batch     0 | loss: 6.5176907CurrentTrain: epoch  2, batch     1 | loss: 5.9224925CurrentTrain: epoch  3, batch     0 | loss: 6.0200610CurrentTrain: epoch  3, batch     1 | loss: 5.2284164CurrentTrain: epoch  4, batch     0 | loss: 5.1200309CurrentTrain: epoch  4, batch     1 | loss: 4.9539165CurrentTrain: epoch  5, batch     0 | loss: 4.9445100CurrentTrain: epoch  5, batch     1 | loss: 4.3713732CurrentTrain: epoch  6, batch     0 | loss: 4.5749583CurrentTrain: epoch  6, batch     1 | loss: 3.8175251CurrentTrain: epoch  7, batch     0 | loss: 4.1292191CurrentTrain: epoch  7, batch     1 | loss: 3.7289181CurrentTrain: epoch  8, batch     0 | loss: 3.7001770CurrentTrain: epoch  8, batch     1 | loss: 3.2485514CurrentTrain: epoch  9, batch     0 | loss: 3.3941000CurrentTrain: epoch  9, batch     1 | loss: 3.1897101
Mixup data size:  80
MixupTrain:  epoch  0, batch     4 | loss: 5.4196753
MemoryTrain:  epoch  0, batch     0 | loss: 1.9901593MemoryTrain:  epoch  0, batch     1 | loss: 0.6199257MemoryTrain:  epoch  1, batch     0 | loss: 2.2530608MemoryTrain:  epoch  1, batch     1 | loss: 2.6877711MemoryTrain:  epoch  2, batch     0 | loss: 1.7282212MemoryTrain:  epoch  2, batch     1 | loss: 0.8121611MemoryTrain:  epoch  3, batch     0 | loss: 1.5913855MemoryTrain:  epoch  3, batch     1 | loss: 0.5831061MemoryTrain:  epoch  4, batch     0 | loss: 1.3204405MemoryTrain:  epoch  4, batch     1 | loss: 0.9014883MemoryTrain:  epoch  5, batch     0 | loss: 0.9804952MemoryTrain:  epoch  5, batch     1 | loss: 0.7563785MemoryTrain:  epoch  6, batch     0 | loss: 0.9204091MemoryTrain:  epoch  6, batch     1 | loss: 0.2566419MemoryTrain:  epoch  7, batch     0 | loss: 0.5799187MemoryTrain:  epoch  7, batch     1 | loss: 0.7696526MemoryTrain:  epoch  8, batch     0 | loss: 0.6585578MemoryTrain:  epoch  8, batch     1 | loss: 0.5076164MemoryTrain:  epoch  9, batch     0 | loss: 0.7583625MemoryTrain:  epoch  9, batch     1 | loss: 0.3136929
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 35.42%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 32.81%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 33.75%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 34.38%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 41.07%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 46.88%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 52.78%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 57.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 61.36%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 67.31%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 69.64%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 71.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 72.22%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 3.12%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 4.17%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 4.69%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 3.75%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 4.17%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 13.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 24.22%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 31.94%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 38.75%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 43.75%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 47.40%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 49.52%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 50.45%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 51.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 51.95%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 53.31%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 54.17%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 54.61%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.33%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 60.23%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 61.68%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 63.28%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 64.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 66.11%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 67.13%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 68.30%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 68.97%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 69.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 70.56%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 71.09%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 71.69%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 70.54%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 69.62%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 68.58%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 67.60%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 66.51%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 66.09%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 66.77%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 67.41%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 68.17%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 68.89%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 69.58%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 70.24%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 70.88%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 71.48%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 70.79%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 70.12%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 70.22%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 70.55%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 70.64%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 70.25%   [EVAL] batch:   54 | acc: 37.50%,  total acc: 69.66%   [EVAL] batch:   55 | acc: 31.25%,  total acc: 68.97%   [EVAL] batch:   56 | acc: 25.00%,  total acc: 68.20%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 67.56%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 67.16%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 67.08%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 67.62%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 67.94%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 68.95%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 69.42%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 69.89%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 70.34%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 70.77%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 71.20%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 71.61%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 71.30%   
cur_acc_llm:  [0.8617424242424242, 0.4140625, 0.9107142857142857, 0.7222222222222222]
his_acc_llm:  [0.8617424242424242, 0.721875, 0.7465277777777778, 0.7130281690140845]
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 12.50%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 9.38%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 11.25%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 11.46%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 19.64%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 27.34%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 31.94%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 37.50%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 41.48%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 46.35%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 50.48%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 54.02%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 57.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 59.77%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 62.13%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 60.07%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 3.12%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 4.17%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 6.25%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 8.75%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 7.29%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 14.29%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 19.53%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 26.39%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 28.75%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 31.82%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 33.85%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 34.62%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 36.61%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 39.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 40.23%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 42.28%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 43.40%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 44.41%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 46.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 49.11%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 51.42%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 53.53%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 55.47%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 57.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 58.89%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 60.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 61.61%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 62.93%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 63.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 65.12%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 66.02%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 66.10%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 66.91%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 65.89%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 65.80%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 64.86%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 65.30%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 65.38%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 65.94%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 66.46%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 67.11%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 67.88%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 68.61%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 69.31%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 69.97%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 70.61%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 71.22%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 70.66%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 70.38%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 69.85%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 69.95%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 69.69%   [EVAL] batch:   53 | acc: 37.50%,  total acc: 69.10%   [EVAL] batch:   54 | acc: 12.50%,  total acc: 68.07%   [EVAL] batch:   55 | acc: 6.25%,  total acc: 66.96%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 65.79%   [EVAL] batch:   57 | acc: 6.25%,  total acc: 64.76%   [EVAL] batch:   58 | acc: 18.75%,  total acc: 63.98%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 63.75%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 64.24%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 64.11%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 64.58%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 64.94%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 65.29%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 65.81%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 66.32%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 66.82%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 67.30%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 67.77%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 67.52%   
cur_acc:  ['0.8712', '0.7109', '0.8616', '0.6007']
his_acc:  ['0.8712', '0.6266', '0.6921', '0.6752']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 4.8047862CurrentTrain: epoch  0, batch     1 | loss: 4.2851906CurrentTrain: epoch  1, batch     0 | loss: 3.2923050CurrentTrain: epoch  1, batch     1 | loss: 2.4303505CurrentTrain: epoch  2, batch     0 | loss: 2.4039416CurrentTrain: epoch  2, batch     1 | loss: 2.4316778CurrentTrain: epoch  3, batch     0 | loss: 2.1712508CurrentTrain: epoch  3, batch     1 | loss: 1.9898376CurrentTrain: epoch  4, batch     0 | loss: 1.9772602CurrentTrain: epoch  4, batch     1 | loss: 2.0196824CurrentTrain: epoch  5, batch     0 | loss: 2.0385802CurrentTrain: epoch  5, batch     1 | loss: 1.8426849CurrentTrain: epoch  6, batch     0 | loss: 1.8660364CurrentTrain: epoch  6, batch     1 | loss: 2.0140352CurrentTrain: epoch  7, batch     0 | loss: 1.8522431CurrentTrain: epoch  7, batch     1 | loss: 1.7528992CurrentTrain: epoch  8, batch     0 | loss: 1.7482505CurrentTrain: epoch  8, batch     1 | loss: 1.7513195CurrentTrain: epoch  9, batch     0 | loss: 1.7998872CurrentTrain: epoch  9, batch     1 | loss: 1.7161757
Mixup data size:  90

MemoryTrain:  epoch  0, batch     0 | loss: 2.2319474MemoryTrain:  epoch  0, batch     1 | loss: 0.8365108MemoryTrain:  epoch  1, batch     0 | loss: 2.1449332MemoryTrain:  epoch  1, batch     1 | loss: 1.4548997MemoryTrain:  epoch  2, batch     0 | loss: 1.3767064MemoryTrain:  epoch  2, batch     1 | loss: 0.7683696MemoryTrain:  epoch  3, batch     0 | loss: 0.7768688MemoryTrain:  epoch  3, batch     1 | loss: 0.0998869MemoryTrain:  epoch  4, batch     0 | loss: 0.4730290MemoryTrain:  epoch  4, batch     1 | loss: 0.1460357MemoryTrain:  epoch  5, batch     0 | loss: 0.1994976MemoryTrain:  epoch  5, batch     1 | loss: 0.0547386MemoryTrain:  epoch  6, batch     0 | loss: 0.1845961MemoryTrain:  epoch  6, batch     1 | loss: 0.0366533MemoryTrain:  epoch  7, batch     0 | loss: 0.1777669MemoryTrain:  epoch  7, batch     1 | loss: 0.0337424MemoryTrain:  epoch  8, batch     0 | loss: 0.0627444MemoryTrain:  epoch  8, batch     1 | loss: 0.0304815MemoryTrain:  epoch  9, batch     0 | loss: 0.0377756MemoryTrain:  epoch  9, batch     1 | loss: 0.0237698
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 5.6696391CurrentTrain: epoch  0, batch     1 | loss: 6.3131809CurrentTrain: epoch  1, batch     0 | loss: 4.7077017CurrentTrain: epoch  1, batch     1 | loss: 4.3503013CurrentTrain: epoch  2, batch     0 | loss: 3.9698687CurrentTrain: epoch  2, batch     1 | loss: 3.4741812CurrentTrain: epoch  3, batch     0 | loss: 3.6468954CurrentTrain: epoch  3, batch     1 | loss: 3.0231681CurrentTrain: epoch  4, batch     0 | loss: 3.1763289CurrentTrain: epoch  4, batch     1 | loss: 2.7023723CurrentTrain: epoch  5, batch     0 | loss: 2.9393377CurrentTrain: epoch  5, batch     1 | loss: 2.6804342CurrentTrain: epoch  6, batch     0 | loss: 2.7032890CurrentTrain: epoch  6, batch     1 | loss: 2.4691117CurrentTrain: epoch  7, batch     0 | loss: 2.6052446CurrentTrain: epoch  7, batch     1 | loss: 2.2202125CurrentTrain: epoch  8, batch     0 | loss: 2.3762221CurrentTrain: epoch  8, batch     1 | loss: 2.2225189CurrentTrain: epoch  9, batch     0 | loss: 2.2536545CurrentTrain: epoch  9, batch     1 | loss: 2.1118817
Mixup data size:  91
MixupTrain:  epoch  0, batch     0 | loss: 4.4065969MixupTrain:  epoch  0, batch     1 | loss: 4.5724205MixupTrain:  epoch  0, batch     4 | loss: 4.6244342MixupTrain:  epoch  0, batch     5 | loss: 3.4717075
MemoryTrain:  epoch  0, batch     0 | loss: 1.4768639MemoryTrain:  epoch  0, batch     1 | loss: 0.4054007MemoryTrain:  epoch  1, batch     0 | loss: 2.0637677MemoryTrain:  epoch  1, batch     1 | loss: 0.7489218MemoryTrain:  epoch  2, batch     0 | loss: 1.4345806MemoryTrain:  epoch  2, batch     1 | loss: 1.1646005MemoryTrain:  epoch  3, batch     0 | loss: 1.1071136MemoryTrain:  epoch  3, batch     1 | loss: 1.0029050MemoryTrain:  epoch  4, batch     0 | loss: 0.8263106MemoryTrain:  epoch  4, batch     1 | loss: 0.9898635MemoryTrain:  epoch  5, batch     0 | loss: 0.6730615MemoryTrain:  epoch  5, batch     1 | loss: 0.8284477MemoryTrain:  epoch  6, batch     0 | loss: 0.5762678MemoryTrain:  epoch  6, batch     1 | loss: 0.7962603MemoryTrain:  epoch  7, batch     0 | loss: 0.6610879MemoryTrain:  epoch  7, batch     1 | loss: 0.4360167MemoryTrain:  epoch  8, batch     0 | loss: 0.6302702MemoryTrain:  epoch  8, batch     1 | loss: 0.4668939MemoryTrain:  epoch  9, batch     0 | loss: 0.6929137MemoryTrain:  epoch  9, batch     1 | loss: 0.4784920
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 31.25%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 0.00%,  total acc: 64.58%   [EVAL] batch:    9 | acc: 6.25%,  total acc: 58.75%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 56.25%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 58.33%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 56.73%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 1.56%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 1.25%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 1.04%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 9.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 21.09%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 29.17%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 35.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 40.34%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 44.27%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 45.19%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 44.20%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 45.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 46.48%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 48.16%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 49.31%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 51.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 54.17%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 58.15%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 59.90%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 61.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 62.98%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 64.12%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 65.40%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 66.16%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 66.88%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 67.74%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 68.36%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 68.18%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 68.93%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 67.86%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 67.01%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 66.39%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 66.12%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 66.51%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 66.72%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 67.38%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 68.01%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 69.46%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 70.79%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 71.41%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 72.01%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 71.05%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 70.88%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 71.08%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 71.63%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 72.17%   [EVAL] batch:   53 | acc: 31.25%,  total acc: 71.41%   [EVAL] batch:   54 | acc: 12.50%,  total acc: 70.34%   [EVAL] batch:   55 | acc: 37.50%,  total acc: 69.75%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 69.19%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 68.32%   [EVAL] batch:   58 | acc: 12.50%,  total acc: 67.37%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 67.08%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 67.62%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 68.04%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 68.35%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 69.23%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 69.70%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 70.15%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 70.59%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 71.01%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 71.43%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 71.39%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 71.27%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 71.23%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 71.20%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 71.33%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 71.71%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 72.08%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 72.12%   [EVAL] batch:   78 | acc: 6.25%,  total acc: 71.28%   [EVAL] batch:   79 | acc: 6.25%,  total acc: 70.47%   [EVAL] batch:   80 | acc: 12.50%,  total acc: 69.75%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 69.59%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 69.65%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 68.90%   
cur_acc_llm:  [0.8617424242424242, 0.4140625, 0.9107142857142857, 0.7222222222222222, 0.5673076923076923]
his_acc_llm:  [0.8617424242424242, 0.721875, 0.7465277777777778, 0.7130281690140845, 0.6889880952380952]
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 51.56%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 48.75%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 42.71%   [EVAL] batch:    6 | acc: 12.50%,  total acc: 38.39%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 42.19%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 45.83%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 49.38%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 53.41%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 56.77%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 56.25%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 4.17%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 4.69%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 7.50%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 6.25%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 13.39%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 17.97%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 25.00%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 27.50%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 30.68%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 32.81%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 33.65%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 35.71%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 38.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 39.45%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 41.54%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 42.71%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 43.75%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 45.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 48.51%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 50.85%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 52.99%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 54.95%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 56.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 58.41%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 59.72%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 61.16%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 62.28%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 63.33%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 64.11%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 64.84%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 64.96%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 65.81%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 64.82%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 64.76%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 64.02%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 64.47%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 64.74%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 65.16%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 65.70%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 66.37%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 67.15%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 67.90%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 68.61%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 69.29%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 69.95%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 70.57%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 69.39%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 69.12%   [EVAL] batch:   50 | acc: 31.25%,  total acc: 68.38%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 68.03%   [EVAL] batch:   52 | acc: 31.25%,  total acc: 67.33%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 66.09%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 64.89%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 63.73%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 62.61%   [EVAL] batch:   57 | acc: 6.25%,  total acc: 61.64%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 60.59%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 60.21%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 60.76%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 60.58%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 60.02%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 59.67%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 60.00%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 60.61%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 61.19%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 61.76%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 62.32%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 62.86%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 62.85%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 62.76%   [EVAL] batch:   72 | acc: 62.50%,  total acc: 62.76%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:   74 | acc: 43.75%,  total acc: 62.25%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 61.84%   [EVAL] batch:   76 | acc: 12.50%,  total acc: 61.20%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 60.90%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 61.16%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 61.33%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 61.57%   [EVAL] batch:   81 | acc: 93.75%,  total acc: 61.97%   [EVAL] batch:   82 | acc: 93.75%,  total acc: 62.35%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 61.68%   
cur_acc:  ['0.8712', '0.7109', '0.8616', '0.6007', '0.5625']
his_acc:  ['0.8712', '0.6266', '0.6921', '0.6752', '0.6168']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 6.0387964CurrentTrain: epoch  0, batch     1 | loss: 5.5581450CurrentTrain: epoch  1, batch     0 | loss: 4.7512169CurrentTrain: epoch  1, batch     1 | loss: 3.5387616CurrentTrain: epoch  2, batch     0 | loss: 3.5345807CurrentTrain: epoch  2, batch     1 | loss: 3.6508036CurrentTrain: epoch  3, batch     0 | loss: 3.0479302CurrentTrain: epoch  3, batch     1 | loss: 3.0589516CurrentTrain: epoch  4, batch     0 | loss: 2.8784664CurrentTrain: epoch  4, batch     1 | loss: 2.2518666CurrentTrain: epoch  5, batch     0 | loss: 2.3146451CurrentTrain: epoch  5, batch     1 | loss: 2.2484534CurrentTrain: epoch  6, batch     0 | loss: 1.9571764CurrentTrain: epoch  6, batch     1 | loss: 1.9927194CurrentTrain: epoch  7, batch     0 | loss: 1.9157567CurrentTrain: epoch  7, batch     1 | loss: 1.8840408CurrentTrain: epoch  8, batch     0 | loss: 1.8593203CurrentTrain: epoch  8, batch     1 | loss: 1.8747936CurrentTrain: epoch  9, batch     0 | loss: 1.7734034CurrentTrain: epoch  9, batch     1 | loss: 1.8060795
Mixup data size:  100
MixupTrain:  epoch  0, batch     0 | loss: 3.2128478MixupTrain:  epoch  0, batch     3 | loss: 2.1028776MixupTrain:  epoch  0, batch     4 | loss: 1.9979804MixupTrain:  epoch  0, batch     5 | loss: 2.0175545MixupTrain:  epoch  0, batch     6 | loss: 1.9909389
MemoryTrain:  epoch  0, batch     0 | loss: 0.8409745MemoryTrain:  epoch  0, batch     1 | loss: 0.0202202MemoryTrain:  epoch  1, batch     0 | loss: 1.0501536MemoryTrain:  epoch  1, batch     1 | loss: 0.6164577MemoryTrain:  epoch  2, batch     0 | loss: 0.2647129MemoryTrain:  epoch  2, batch     1 | loss: 0.2344105MemoryTrain:  epoch  3, batch     0 | loss: 0.1331589MemoryTrain:  epoch  3, batch     1 | loss: 0.1372891MemoryTrain:  epoch  4, batch     0 | loss: 0.1460444MemoryTrain:  epoch  4, batch     1 | loss: 0.0480324MemoryTrain:  epoch  5, batch     0 | loss: 0.0721178MemoryTrain:  epoch  5, batch     1 | loss: 0.0278536MemoryTrain:  epoch  6, batch     0 | loss: 0.0235468MemoryTrain:  epoch  6, batch     1 | loss: 0.0542033MemoryTrain:  epoch  7, batch     0 | loss: 0.0226995MemoryTrain:  epoch  7, batch     1 | loss: 0.0966361MemoryTrain:  epoch  8, batch     0 | loss: 0.0320976MemoryTrain:  epoch  8, batch     1 | loss: 0.0300669MemoryTrain:  epoch  9, batch     0 | loss: 0.0296322MemoryTrain:  epoch  9, batch     1 | loss: 0.0227697
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 7.4833727CurrentTrain: epoch  0, batch     1 | loss: 8.2405882CurrentTrain: epoch  1, batch     0 | loss: 6.9815950CurrentTrain: epoch  1, batch     1 | loss: 5.4930749CurrentTrain: epoch  2, batch     0 | loss: 5.6103597CurrentTrain: epoch  2, batch     1 | loss: 5.8040133CurrentTrain: epoch  3, batch     0 | loss: 5.8864541CurrentTrain: epoch  3, batch     1 | loss: 4.1272750CurrentTrain: epoch  4, batch     0 | loss: 4.6980333CurrentTrain: epoch  4, batch     1 | loss: 5.0547829CurrentTrain: epoch  5, batch     0 | loss: 4.7960367CurrentTrain: epoch  5, batch     1 | loss: 4.5134563CurrentTrain: epoch  6, batch     0 | loss: 4.0820518CurrentTrain: epoch  6, batch     1 | loss: 4.0930939CurrentTrain: epoch  7, batch     0 | loss: 3.5515375CurrentTrain: epoch  7, batch     1 | loss: 3.3508110CurrentTrain: epoch  8, batch     0 | loss: 3.4010231CurrentTrain: epoch  8, batch     1 | loss: 2.6214406CurrentTrain: epoch  9, batch     0 | loss: 2.9272456CurrentTrain: epoch  9, batch     1 | loss: 3.0012772
Mixup data size:  101
MixupTrain:  epoch  0, batch     2 | loss: 4.2354652MixupTrain:  epoch  0, batch     3 | loss: 4.3068230MixupTrain:  epoch  0, batch     4 | loss: 3.6373166MixupTrain:  epoch  0, batch     6 | loss: 3.0852279
MemoryTrain:  epoch  0, batch     0 | loss: 0.7849306MemoryTrain:  epoch  0, batch     1 | loss: 1.1394135MemoryTrain:  epoch  1, batch     0 | loss: 1.2883987MemoryTrain:  epoch  1, batch     1 | loss: 1.1075777MemoryTrain:  epoch  2, batch     0 | loss: 1.5102775MemoryTrain:  epoch  2, batch     1 | loss: 0.5433043MemoryTrain:  epoch  3, batch     0 | loss: 0.8989657MemoryTrain:  epoch  3, batch     1 | loss: 0.7811410MemoryTrain:  epoch  4, batch     0 | loss: 0.7161099MemoryTrain:  epoch  4, batch     1 | loss: 0.7210304MemoryTrain:  epoch  5, batch     0 | loss: 0.7480992MemoryTrain:  epoch  5, batch     1 | loss: 0.5723035MemoryTrain:  epoch  6, batch     0 | loss: 0.4975436MemoryTrain:  epoch  6, batch     1 | loss: 0.6707838MemoryTrain:  epoch  7, batch     0 | loss: 0.5149363MemoryTrain:  epoch  7, batch     1 | loss: 0.4278697MemoryTrain:  epoch  8, batch     0 | loss: 0.5315530MemoryTrain:  epoch  8, batch     1 | loss: 0.3541842MemoryTrain:  epoch  9, batch     0 | loss: 0.6491327MemoryTrain:  epoch  9, batch     1 | loss: 0.4003448
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 86.98%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 86.16%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 1.56%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 1.25%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 1.04%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 10.71%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 21.88%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 29.86%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 36.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 42.05%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 46.35%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 46.63%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 45.09%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 46.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 47.27%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 48.90%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 50.00%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 51.32%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 52.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 55.06%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 57.10%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 58.70%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 60.42%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 62.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 63.46%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 64.58%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 65.85%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 66.59%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 67.29%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 68.15%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 68.56%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 69.30%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 68.21%   [EVAL] batch:   35 | acc: 31.25%,  total acc: 67.19%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 65.88%   [EVAL] batch:   37 | acc: 25.00%,  total acc: 64.80%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 63.94%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 63.59%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 64.33%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 65.03%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 65.84%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 66.62%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 67.36%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 68.07%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:   48 | acc: 6.25%,  total acc: 68.11%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 66.75%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 66.95%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 66.86%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 66.44%   [EVAL] batch:   54 | acc: 37.50%,  total acc: 65.91%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 65.85%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 65.79%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 65.84%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 65.57%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 65.52%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 64.96%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 64.21%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 63.79%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 63.28%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 63.65%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 64.20%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 64.74%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 65.26%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 65.76%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 66.46%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 66.41%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 66.52%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 66.55%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 66.83%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 67.27%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 67.61%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 67.47%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 66.61%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 65.78%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 64.97%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 64.56%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 64.46%   [EVAL] batch:   83 | acc: 93.75%,  total acc: 64.81%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 64.93%   [EVAL] batch:   85 | acc: 93.75%,  total acc: 65.26%   [EVAL] batch:   86 | acc: 81.25%,  total acc: 65.45%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 65.87%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 65.97%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 66.28%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 66.58%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 66.87%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 67.15%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 67.30%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 67.65%   
cur_acc_llm:  [0.8617424242424242, 0.4140625, 0.9107142857142857, 0.7222222222222222, 0.5673076923076923, 0.8616071428571429]
his_acc_llm:  [0.8617424242424242, 0.721875, 0.7465277777777778, 0.7130281690140845, 0.6889880952380952, 0.6765463917525774]
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 79.69%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 80.77%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 78.57%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 3.12%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 2.08%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 3.12%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 6.25%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 5.21%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 12.50%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 17.19%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 24.31%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 26.88%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 29.55%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 32.29%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 34.13%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 36.16%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 38.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 39.84%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 41.91%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 43.06%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 44.08%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 46.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 48.81%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 51.14%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 53.26%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 55.21%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 57.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 58.65%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 59.95%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 61.38%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 63.54%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 64.31%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 65.23%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 65.15%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 65.99%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 65.00%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 64.93%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 64.19%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 64.47%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 64.42%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 64.84%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 65.24%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 65.77%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 66.57%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 67.33%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 68.06%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 69.41%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 70.05%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 69.01%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 68.62%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 68.01%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 67.79%   [EVAL] batch:   52 | acc: 37.50%,  total acc: 67.22%   [EVAL] batch:   53 | acc: 6.25%,  total acc: 66.09%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 64.89%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 63.73%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 62.61%   [EVAL] batch:   57 | acc: 6.25%,  total acc: 61.64%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 60.59%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 60.21%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 60.76%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 60.18%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 59.23%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 58.30%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 58.56%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 59.19%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 59.79%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 60.39%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 60.96%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 61.52%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 61.53%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 61.46%   [EVAL] batch:   72 | acc: 62.50%,  total acc: 61.47%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 61.23%   [EVAL] batch:   74 | acc: 43.75%,  total acc: 61.00%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 60.61%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 60.15%   [EVAL] batch:   77 | acc: 6.25%,  total acc: 59.46%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 58.70%   [EVAL] batch:   79 | acc: 6.25%,  total acc: 58.05%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 57.56%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 57.55%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 57.76%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 57.96%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 58.16%   [EVAL] batch:   85 | acc: 93.75%,  total acc: 58.58%   [EVAL] batch:   86 | acc: 81.25%,  total acc: 58.84%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 59.23%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 59.62%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 59.65%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 59.82%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 60.19%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 60.35%   [EVAL] batch:   93 | acc: 81.25%,  total acc: 60.57%   [EVAL] batch:   94 | acc: 62.50%,  total acc: 60.59%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 60.87%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 60.82%   
cur_acc:  ['0.8712', '0.7109', '0.8616', '0.6007', '0.5625', '0.7857']
his_acc:  ['0.8712', '0.6266', '0.6921', '0.6752', '0.6168', '0.6082']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 7.5626974CurrentTrain: epoch  0, batch     1 | loss: 7.1351404CurrentTrain: epoch  1, batch     0 | loss: 6.3147030CurrentTrain: epoch  1, batch     1 | loss: 5.8903604CurrentTrain: epoch  2, batch     0 | loss: 5.6448412CurrentTrain: epoch  2, batch     1 | loss: 4.6145644CurrentTrain: epoch  3, batch     0 | loss: 5.2525072CurrentTrain: epoch  3, batch     1 | loss: 3.3285306CurrentTrain: epoch  4, batch     0 | loss: 4.1618204CurrentTrain: epoch  4, batch     1 | loss: 4.5879121CurrentTrain: epoch  5, batch     0 | loss: 3.6966214CurrentTrain: epoch  5, batch     1 | loss: 4.5267692CurrentTrain: epoch  6, batch     0 | loss: 3.7502820CurrentTrain: epoch  6, batch     1 | loss: 3.7689967CurrentTrain: epoch  7, batch     0 | loss: 3.6678095CurrentTrain: epoch  7, batch     1 | loss: 3.2878621CurrentTrain: epoch  8, batch     0 | loss: 2.5353844CurrentTrain: epoch  8, batch     1 | loss: 4.3117709CurrentTrain: epoch  9, batch     0 | loss: 3.4372151CurrentTrain: epoch  9, batch     1 | loss: 2.3854890
Mixup data size:  111
MixupTrain:  epoch  0, batch     2 | loss: 1.9167755MixupTrain:  epoch  0, batch     3 | loss: 3.1245213MixupTrain:  epoch  0, batch     4 | loss: 1.8748236MixupTrain:  epoch  0, batch     5 | loss: 1.8974894MixupTrain:  epoch  0, batch     6 | loss: 1.8069646
MemoryTrain:  epoch  0, batch     0 | loss: 0.5846670MemoryTrain:  epoch  0, batch     1 | loss: 0.0585716MemoryTrain:  epoch  0, batch     2 | loss: 0.0192392MemoryTrain:  epoch  1, batch     0 | loss: 0.3816866MemoryTrain:  epoch  1, batch     1 | loss: 0.1771372MemoryTrain:  epoch  1, batch     2 | loss: 0.8700941MemoryTrain:  epoch  2, batch     0 | loss: 0.0925949MemoryTrain:  epoch  2, batch     1 | loss: 0.1868886MemoryTrain:  epoch  2, batch     2 | loss: 0.2710485MemoryTrain:  epoch  3, batch     0 | loss: 0.1734979MemoryTrain:  epoch  3, batch     1 | loss: 0.1066572MemoryTrain:  epoch  3, batch     2 | loss: 0.0657760MemoryTrain:  epoch  4, batch     0 | loss: 0.0736284MemoryTrain:  epoch  4, batch     1 | loss: 0.0502027MemoryTrain:  epoch  4, batch     2 | loss: 0.0551316MemoryTrain:  epoch  5, batch     0 | loss: 0.0585858MemoryTrain:  epoch  5, batch     1 | loss: 0.0468613MemoryTrain:  epoch  5, batch     2 | loss: 0.0158661MemoryTrain:  epoch  6, batch     0 | loss: 0.0435136MemoryTrain:  epoch  6, batch     1 | loss: 0.0243313MemoryTrain:  epoch  6, batch     2 | loss: 0.0157252MemoryTrain:  epoch  7, batch     0 | loss: 0.0381277MemoryTrain:  epoch  7, batch     1 | loss: 0.0452780MemoryTrain:  epoch  7, batch     2 | loss: 0.0322637MemoryTrain:  epoch  8, batch     0 | loss: 0.0644322MemoryTrain:  epoch  8, batch     1 | loss: 0.0458220MemoryTrain:  epoch  8, batch     2 | loss: 0.0061639MemoryTrain:  epoch  9, batch     0 | loss: 0.0254944MemoryTrain:  epoch  9, batch     1 | loss: 0.0664389MemoryTrain:  epoch  9, batch     2 | loss: 0.0289250
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 9.7477913CurrentTrain: epoch  0, batch     1 | loss: 8.0830374CurrentTrain: epoch  1, batch     0 | loss: 8.8576679CurrentTrain: epoch  1, batch     1 | loss: 6.7375703CurrentTrain: epoch  2, batch     0 | loss: 8.6421509CurrentTrain: epoch  2, batch     1 | loss: 6.0325561CurrentTrain: epoch  3, batch     0 | loss: 7.2190938CurrentTrain: epoch  3, batch     1 | loss: 6.4569564CurrentTrain: epoch  4, batch     0 | loss: 6.3360338CurrentTrain: epoch  4, batch     1 | loss: 6.6497207CurrentTrain: epoch  5, batch     0 | loss: 6.1410561CurrentTrain: epoch  5, batch     1 | loss: 5.4362106CurrentTrain: epoch  6, batch     0 | loss: 5.8813982CurrentTrain: epoch  6, batch     1 | loss: 5.1234488CurrentTrain: epoch  7, batch     0 | loss: 5.6705732CurrentTrain: epoch  7, batch     1 | loss: 4.2577872CurrentTrain: epoch  8, batch     0 | loss: 5.0339108CurrentTrain: epoch  8, batch     1 | loss: 4.8953538CurrentTrain: epoch  9, batch     0 | loss: 5.0644336CurrentTrain: epoch  9, batch     1 | loss: 3.7663503
Mixup data size:  111
MixupTrain:  epoch  0, batch     1 | loss: 4.8270952MixupTrain:  epoch  0, batch     2 | loss: 3.4827573MixupTrain:  epoch  0, batch     3 | loss: 3.6947500MixupTrain:  epoch  0, batch     5 | loss: 4.5653720
MemoryTrain:  epoch  0, batch     0 | loss: 1.1687243MemoryTrain:  epoch  0, batch     1 | loss: 0.5694965MemoryTrain:  epoch  0, batch     2 | loss: 0.8089848MemoryTrain:  epoch  1, batch     0 | loss: 1.1863906MemoryTrain:  epoch  1, batch     1 | loss: 1.2800214MemoryTrain:  epoch  1, batch     2 | loss: 0.1290453MemoryTrain:  epoch  2, batch     0 | loss: 0.9913582MemoryTrain:  epoch  2, batch     1 | loss: 0.6746775MemoryTrain:  epoch  2, batch     2 | loss: 1.2095660MemoryTrain:  epoch  3, batch     0 | loss: 0.5993044MemoryTrain:  epoch  3, batch     1 | loss: 0.8522874MemoryTrain:  epoch  3, batch     2 | loss: 0.4465025MemoryTrain:  epoch  4, batch     0 | loss: 0.6778526MemoryTrain:  epoch  4, batch     1 | loss: 0.5141211MemoryTrain:  epoch  4, batch     2 | loss: 0.2717386MemoryTrain:  epoch  5, batch     0 | loss: 0.7425919MemoryTrain:  epoch  5, batch     1 | loss: 0.3350193MemoryTrain:  epoch  5, batch     2 | loss: 0.1342294MemoryTrain:  epoch  6, batch     0 | loss: 0.3935869MemoryTrain:  epoch  6, batch     1 | loss: 0.7102365MemoryTrain:  epoch  6, batch     2 | loss: 0.1193760MemoryTrain:  epoch  7, batch     0 | loss: 0.6286301MemoryTrain:  epoch  7, batch     1 | loss: 0.4771117MemoryTrain:  epoch  7, batch     2 | loss: 0.0764102MemoryTrain:  epoch  8, batch     0 | loss: 0.3863381MemoryTrain:  epoch  8, batch     1 | loss: 0.5350387MemoryTrain:  epoch  8, batch     2 | loss: 0.1675583MemoryTrain:  epoch  9, batch     0 | loss: 0.4191163MemoryTrain:  epoch  9, batch     1 | loss: 0.4654812MemoryTrain:  epoch  9, batch     2 | loss: 0.0872717
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 29.17%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 21.88%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 17.50%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 14.58%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 16.96%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 25.78%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 31.25%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 37.50%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 42.61%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 46.88%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 50.96%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 54.46%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 57.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 60.16%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 64.24%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 64.80%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 65.31%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 65.62%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 1.56%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 1.25%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 1.04%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 9.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 21.09%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 29.17%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 36.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 40.34%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 44.79%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 45.67%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 44.20%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 46.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 46.88%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 48.53%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 49.65%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 50.66%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 52.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 54.46%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 56.53%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 58.15%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 59.64%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 61.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 62.74%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 63.66%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 64.51%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 65.52%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 65.73%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 66.21%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 65.91%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 66.91%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 65.89%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 65.10%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 64.19%   [EVAL] batch:   37 | acc: 37.50%,  total acc: 63.49%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 62.82%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 63.26%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 63.99%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 64.83%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 66.39%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 66.85%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 67.55%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 68.23%   [EVAL] batch:   48 | acc: 6.25%,  total acc: 66.96%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 65.62%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 65.56%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 65.99%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 65.92%   [EVAL] batch:   53 | acc: 18.75%,  total acc: 65.05%   [EVAL] batch:   54 | acc: 31.25%,  total acc: 64.43%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 64.06%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 63.71%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 63.25%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 62.71%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 62.60%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 63.01%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 62.50%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 61.61%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 61.13%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 61.54%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 62.12%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 62.69%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 63.24%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 63.77%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 64.29%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 64.44%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 64.41%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 64.55%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 64.61%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 64.42%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 64.64%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 64.53%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 64.18%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 63.37%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 62.58%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 61.81%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 61.66%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 61.82%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 62.13%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 61.69%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 61.26%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 60.92%   [EVAL] batch:   87 | acc: 31.25%,  total acc: 60.58%   [EVAL] batch:   88 | acc: 6.25%,  total acc: 59.97%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 59.65%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 60.03%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 60.26%   [EVAL] batch:   92 | acc: 87.50%,  total acc: 60.55%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 60.84%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 60.72%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 61.07%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 61.08%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 60.78%   [EVAL] batch:   98 | acc: 31.25%,  total acc: 60.48%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 60.06%   [EVAL] batch:  100 | acc: 0.00%,  total acc: 59.47%   [EVAL] batch:  101 | acc: 0.00%,  total acc: 58.88%   [EVAL] batch:  102 | acc: 0.00%,  total acc: 58.31%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 58.29%   [EVAL] batch:  104 | acc: 81.25%,  total acc: 58.51%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 58.73%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 59.05%   [EVAL] batch:  107 | acc: 93.75%,  total acc: 59.38%   [EVAL] batch:  108 | acc: 93.75%,  total acc: 59.69%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 60.06%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 60.42%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 60.77%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 61.12%   [EVAL] batch:  113 | acc: 93.75%,  total acc: 61.40%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 61.68%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 61.80%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 61.91%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 62.24%   [EVAL] batch:  118 | acc: 18.75%,  total acc: 61.87%   
cur_acc_llm:  [0.8617424242424242, 0.4140625, 0.9107142857142857, 0.7222222222222222, 0.5673076923076923, 0.8616071428571429, 0.65625]
his_acc_llm:  [0.8617424242424242, 0.721875, 0.7465277777777778, 0.7130281690140845, 0.6889880952380952, 0.6765463917525774, 0.6186974789915967]
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 29.17%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 28.12%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 23.75%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 22.92%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 26.79%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 31.25%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 34.03%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 37.50%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 40.34%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 43.23%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 45.19%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 47.32%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 47.92%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 48.83%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 48.53%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 48.61%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 50.33%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 52.19%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 53.57%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 52.84%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 3.12%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 4.17%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 4.69%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 7.50%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 6.25%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 14.29%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 20.31%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 27.78%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 31.25%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 34.66%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 36.98%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 37.50%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 38.84%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 41.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 42.19%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 44.12%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 45.14%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 46.05%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 48.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 50.60%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 52.84%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 54.89%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 56.77%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 58.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 60.10%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 61.34%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 62.72%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 63.79%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 64.17%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 64.72%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 65.43%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 65.34%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 66.18%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 65.36%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 65.10%   [EVAL] batch:   36 | acc: 25.00%,  total acc: 64.02%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 64.31%   [EVAL] batch:   38 | acc: 43.75%,  total acc: 63.78%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 64.22%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 64.33%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 65.03%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 65.84%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 66.62%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 67.36%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 68.07%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 68.37%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 67.00%   [EVAL] batch:   50 | acc: 31.25%,  total acc: 66.30%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 66.23%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 65.80%   [EVAL] batch:   53 | acc: 6.25%,  total acc: 64.70%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 63.52%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 62.39%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 61.29%   [EVAL] batch:   57 | acc: 6.25%,  total acc: 60.34%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 59.32%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 58.96%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 59.43%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 58.87%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 58.04%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 57.23%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 57.50%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 58.14%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 58.77%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 59.38%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 59.96%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 60.54%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 60.56%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 60.59%   [EVAL] batch:   72 | acc: 62.50%,  total acc: 60.62%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 60.39%   [EVAL] batch:   74 | acc: 37.50%,  total acc: 60.08%   [EVAL] batch:   75 | acc: 6.25%,  total acc: 59.38%   [EVAL] batch:   76 | acc: 12.50%,  total acc: 58.77%   [EVAL] batch:   77 | acc: 6.25%,  total acc: 58.09%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 57.36%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 56.64%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 56.02%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 56.10%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 56.40%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 56.70%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 56.25%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 55.96%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 55.60%   [EVAL] batch:   87 | acc: 31.25%,  total acc: 55.33%   [EVAL] batch:   88 | acc: 37.50%,  total acc: 55.13%   [EVAL] batch:   89 | acc: 37.50%,  total acc: 54.93%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 55.15%   [EVAL] batch:   91 | acc: 87.50%,  total acc: 55.50%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 55.71%   [EVAL] batch:   93 | acc: 81.25%,  total acc: 55.98%   [EVAL] batch:   94 | acc: 62.50%,  total acc: 56.05%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 56.32%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 56.06%   [EVAL] batch:   98 | acc: 31.25%,  total acc: 55.81%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 55.44%   [EVAL] batch:  100 | acc: 18.75%,  total acc: 55.07%   [EVAL] batch:  101 | acc: 12.50%,  total acc: 54.66%   [EVAL] batch:  102 | acc: 18.75%,  total acc: 54.31%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 54.33%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 54.40%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 54.42%   [EVAL] batch:  106 | acc: 75.00%,  total acc: 54.61%   [EVAL] batch:  107 | acc: 62.50%,  total acc: 54.69%   [EVAL] batch:  108 | acc: 81.25%,  total acc: 54.93%   [EVAL] batch:  109 | acc: 68.75%,  total acc: 55.06%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 55.18%   [EVAL] batch:  111 | acc: 56.25%,  total acc: 55.19%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 55.14%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 55.15%   [EVAL] batch:  114 | acc: 62.50%,  total acc: 55.22%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 55.44%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 55.66%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 55.88%   [EVAL] batch:  118 | acc: 18.75%,  total acc: 55.57%   
cur_acc:  ['0.8712', '0.7109', '0.8616', '0.6007', '0.5625', '0.7857', '0.5284']
his_acc:  ['0.8712', '0.6266', '0.6921', '0.6752', '0.6168', '0.6082', '0.5557']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 5.9287519CurrentTrain: epoch  0, batch     1 | loss: 7.0705562CurrentTrain: epoch  1, batch     0 | loss: 5.0657530CurrentTrain: epoch  1, batch     1 | loss: 4.2119293CurrentTrain: epoch  2, batch     0 | loss: 4.1743426CurrentTrain: epoch  2, batch     1 | loss: 3.0492413CurrentTrain: epoch  3, batch     0 | loss: 3.9745059CurrentTrain: epoch  3, batch     1 | loss: 2.5736909CurrentTrain: epoch  4, batch     0 | loss: 3.4140811CurrentTrain: epoch  4, batch     1 | loss: 2.5239906CurrentTrain: epoch  5, batch     0 | loss: 2.8266270CurrentTrain: epoch  5, batch     1 | loss: 2.5110447CurrentTrain: epoch  6, batch     0 | loss: 2.9399867CurrentTrain: epoch  6, batch     1 | loss: 2.0250232CurrentTrain: epoch  7, batch     0 | loss: 2.5785074CurrentTrain: epoch  7, batch     1 | loss: 1.9323175CurrentTrain: epoch  8, batch     0 | loss: 2.2699993CurrentTrain: epoch  8, batch     1 | loss: 1.9841379CurrentTrain: epoch  9, batch     0 | loss: 1.9122386CurrentTrain: epoch  9, batch     1 | loss: 2.1901352
Mixup data size:  120

MemoryTrain:  epoch  0, batch     0 | loss: 1.4483275MemoryTrain:  epoch  0, batch     1 | loss: 0.8947068MemoryTrain:  epoch  0, batch     2 | loss: 0.2932646MemoryTrain:  epoch  1, batch     0 | loss: 1.0952256MemoryTrain:  epoch  1, batch     1 | loss: 1.8049675MemoryTrain:  epoch  1, batch     2 | loss: 0.7201082MemoryTrain:  epoch  2, batch     0 | loss: 0.2832078MemoryTrain:  epoch  2, batch     1 | loss: 0.6083040MemoryTrain:  epoch  2, batch     2 | loss: 0.8468417MemoryTrain:  epoch  3, batch     0 | loss: 0.4424820MemoryTrain:  epoch  3, batch     1 | loss: 0.1448246MemoryTrain:  epoch  3, batch     2 | loss: 0.0762636MemoryTrain:  epoch  4, batch     0 | loss: 0.0487303MemoryTrain:  epoch  4, batch     1 | loss: 0.2038118MemoryTrain:  epoch  4, batch     2 | loss: 0.3851791MemoryTrain:  epoch  5, batch     0 | loss: 0.0630883MemoryTrain:  epoch  5, batch     1 | loss: 0.2462797MemoryTrain:  epoch  5, batch     2 | loss: 0.1342025MemoryTrain:  epoch  6, batch     0 | loss: 0.0590403MemoryTrain:  epoch  6, batch     1 | loss: 0.0529282MemoryTrain:  epoch  6, batch     2 | loss: 0.0214293MemoryTrain:  epoch  7, batch     0 | loss: 0.0537155MemoryTrain:  epoch  7, batch     1 | loss: 0.0400183MemoryTrain:  epoch  7, batch     2 | loss: 0.0166305MemoryTrain:  epoch  8, batch     0 | loss: 0.0565508MemoryTrain:  epoch  8, batch     1 | loss: 0.0266218MemoryTrain:  epoch  8, batch     2 | loss: 0.0578749MemoryTrain:  epoch  9, batch     0 | loss: 0.0283792MemoryTrain:  epoch  9, batch     1 | loss: 0.0189308MemoryTrain:  epoch  9, batch     2 | loss: 0.0691511
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 8.3962460CurrentTrain: epoch  0, batch     1 | loss: 7.8926425CurrentTrain: epoch  1, batch     0 | loss: 7.1286802CurrentTrain: epoch  1, batch     1 | loss: 7.4140048CurrentTrain: epoch  2, batch     0 | loss: 6.5499363CurrentTrain: epoch  2, batch     1 | loss: 4.4310570CurrentTrain: epoch  3, batch     0 | loss: 6.0998726CurrentTrain: epoch  3, batch     1 | loss: 4.8887167CurrentTrain: epoch  4, batch     0 | loss: 5.6365852CurrentTrain: epoch  4, batch     1 | loss: 4.6670198CurrentTrain: epoch  5, batch     0 | loss: 5.1979365CurrentTrain: epoch  5, batch     1 | loss: 4.9980631CurrentTrain: epoch  6, batch     0 | loss: 4.7810345CurrentTrain: epoch  6, batch     1 | loss: 4.9095869CurrentTrain: epoch  7, batch     0 | loss: 4.2943115CurrentTrain: epoch  7, batch     1 | loss: 4.3604612CurrentTrain: epoch  8, batch     0 | loss: 4.4784842CurrentTrain: epoch  8, batch     1 | loss: 3.5609093CurrentTrain: epoch  9, batch     0 | loss: 4.0010548CurrentTrain: epoch  9, batch     1 | loss: 3.8135169
Mixup data size:  121
MixupTrain:  epoch  0, batch     2 | loss: 3.8235891MixupTrain:  epoch  0, batch     3 | loss: 3.3668228MixupTrain:  epoch  0, batch     4 | loss: 3.1332277MixupTrain:  epoch  0, batch     7 | loss: 2.8368107
MemoryTrain:  epoch  0, batch     0 | loss: 0.5389397MemoryTrain:  epoch  0, batch     1 | loss: 0.4007471MemoryTrain:  epoch  0, batch     2 | loss: 0.9245907MemoryTrain:  epoch  1, batch     0 | loss: 1.2120556MemoryTrain:  epoch  1, batch     1 | loss: 0.6622171MemoryTrain:  epoch  1, batch     2 | loss: 1.0650244MemoryTrain:  epoch  2, batch     0 | loss: 0.8400910MemoryTrain:  epoch  2, batch     1 | loss: 0.7118419MemoryTrain:  epoch  2, batch     2 | loss: 0.5110690MemoryTrain:  epoch  3, batch     0 | loss: 0.4705174MemoryTrain:  epoch  3, batch     1 | loss: 0.5912299MemoryTrain:  epoch  3, batch     2 | loss: 0.2979301MemoryTrain:  epoch  4, batch     0 | loss: 0.5127176MemoryTrain:  epoch  4, batch     1 | loss: 0.4962415MemoryTrain:  epoch  4, batch     2 | loss: 0.3344500MemoryTrain:  epoch  5, batch     0 | loss: 0.3847855MemoryTrain:  epoch  5, batch     1 | loss: 0.5289176MemoryTrain:  epoch  5, batch     2 | loss: 0.1614445MemoryTrain:  epoch  6, batch     0 | loss: 0.3897953MemoryTrain:  epoch  6, batch     1 | loss: 0.4103812MemoryTrain:  epoch  6, batch     2 | loss: 0.2217453MemoryTrain:  epoch  7, batch     0 | loss: 0.4375352MemoryTrain:  epoch  7, batch     1 | loss: 0.4132294MemoryTrain:  epoch  7, batch     2 | loss: 0.2411992MemoryTrain:  epoch  8, batch     0 | loss: 0.3733999MemoryTrain:  epoch  8, batch     1 | loss: 0.4131929MemoryTrain:  epoch  8, batch     2 | loss: 0.1335124MemoryTrain:  epoch  9, batch     0 | loss: 0.3407691MemoryTrain:  epoch  9, batch     1 | loss: 0.3764458MemoryTrain:  epoch  9, batch     2 | loss: 0.1949508
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 20.83%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 15.62%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 16.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 26.04%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 33.93%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 39.06%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 43.75%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 46.25%   [EVAL] batch:   10 | acc: 12.50%,  total acc: 43.18%   [EVAL] batch:   11 | acc: 25.00%,  total acc: 41.67%   [EVAL] batch:   12 | acc: 18.75%,  total acc: 39.90%   [EVAL] batch:   13 | acc: 6.25%,  total acc: 37.50%   [EVAL] batch:   14 | acc: 0.00%,  total acc: 35.00%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 1.56%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 1.25%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 1.04%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 9.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 21.09%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 29.17%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 35.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 39.77%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 44.79%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 45.67%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 44.64%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 46.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 47.27%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 48.90%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 50.00%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 50.99%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 52.50%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 54.46%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 55.97%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 57.34%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 58.59%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 60.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 61.54%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   27 | acc: 75.00%,  total acc: 62.95%   [EVAL] batch:   28 | acc: 75.00%,  total acc: 63.36%   [EVAL] batch:   29 | acc: 62.50%,  total acc: 63.33%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 63.51%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 63.28%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 63.07%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 63.97%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 63.04%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 61.81%   [EVAL] batch:   36 | acc: 25.00%,  total acc: 60.81%   [EVAL] batch:   37 | acc: 25.00%,  total acc: 59.87%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 59.29%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 59.06%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 59.91%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 60.71%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 61.48%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 62.36%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 63.19%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 63.86%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 64.63%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 65.36%   [EVAL] batch:   48 | acc: 6.25%,  total acc: 64.16%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 62.88%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 62.99%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 63.46%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 63.56%   [EVAL] batch:   53 | acc: 25.00%,  total acc: 62.85%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 61.70%   [EVAL] batch:   55 | acc: 12.50%,  total acc: 60.83%   [EVAL] batch:   56 | acc: 6.25%,  total acc: 59.87%   [EVAL] batch:   57 | acc: 6.25%,  total acc: 58.94%   [EVAL] batch:   58 | acc: 18.75%,  total acc: 58.26%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 58.02%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 58.50%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 57.96%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 57.74%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 57.91%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 58.37%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 59.00%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 59.61%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 60.20%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 60.78%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 61.34%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 61.44%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 61.37%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 61.56%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 61.57%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 61.50%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 61.84%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 61.93%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 61.78%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 61.00%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 60.23%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 59.49%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 59.15%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 59.11%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 59.38%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 58.75%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 58.21%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 57.69%   [EVAL] batch:   87 | acc: 25.00%,  total acc: 57.32%   [EVAL] batch:   88 | acc: 6.25%,  total acc: 56.74%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 56.46%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 56.87%   [EVAL] batch:   91 | acc: 87.50%,  total acc: 57.20%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 57.39%   [EVAL] batch:   93 | acc: 75.00%,  total acc: 57.58%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 57.50%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 57.81%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 57.93%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 57.65%   [EVAL] batch:   98 | acc: 18.75%,  total acc: 57.26%   [EVAL] batch:   99 | acc: 12.50%,  total acc: 56.81%   [EVAL] batch:  100 | acc: 0.00%,  total acc: 56.25%   [EVAL] batch:  101 | acc: 0.00%,  total acc: 55.70%   [EVAL] batch:  102 | acc: 0.00%,  total acc: 55.16%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 55.17%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 55.54%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 55.78%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 56.13%   [EVAL] batch:  107 | acc: 87.50%,  total acc: 56.42%   [EVAL] batch:  108 | acc: 93.75%,  total acc: 56.77%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 57.10%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 57.49%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 57.81%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 58.13%   [EVAL] batch:  113 | acc: 93.75%,  total acc: 58.44%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 58.75%   [EVAL] batch:  115 | acc: 68.75%,  total acc: 58.84%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 59.03%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 59.16%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 59.14%   [EVAL] batch:  119 | acc: 6.25%,  total acc: 58.70%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 58.32%   [EVAL] batch:  121 | acc: 6.25%,  total acc: 57.89%   [EVAL] batch:  122 | acc: 18.75%,  total acc: 57.57%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 57.61%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 57.75%   [EVAL] batch:  125 | acc: 87.50%,  total acc: 57.99%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 58.07%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 58.25%   [EVAL] batch:  128 | acc: 18.75%,  total acc: 57.95%   [EVAL] batch:  129 | acc: 18.75%,  total acc: 57.64%   [EVAL] batch:  130 | acc: 18.75%,  total acc: 57.35%   [EVAL] batch:  131 | acc: 12.50%,  total acc: 57.01%   [EVAL] batch:  132 | acc: 0.00%,  total acc: 56.58%   
cur_acc_llm:  [0.8617424242424242, 0.4140625, 0.9107142857142857, 0.7222222222222222, 0.5673076923076923, 0.8616071428571429, 0.65625, 0.35]
his_acc_llm:  [0.8617424242424242, 0.721875, 0.7465277777777778, 0.7130281690140845, 0.6889880952380952, 0.6765463917525774, 0.6186974789915967, 0.5657894736842105]
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 34.38%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 38.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 46.88%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 54.46%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 59.38%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 63.19%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 65.00%   [EVAL] batch:   10 | acc: 18.75%,  total acc: 60.80%   [EVAL] batch:   11 | acc: 0.00%,  total acc: 55.73%   [EVAL] batch:   12 | acc: 0.00%,  total acc: 51.44%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 47.77%   [EVAL] batch:   14 | acc: 0.00%,  total acc: 44.58%   
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 9.38%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 8.33%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 7.81%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 10.00%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 8.33%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 15.18%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 21.09%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 28.47%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 31.25%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 34.09%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 36.46%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 36.54%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 37.95%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 40.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 41.41%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 43.38%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 44.44%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 45.39%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 47.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 50.00%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 52.27%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 54.35%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 58.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 59.62%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 60.88%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 62.28%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 63.36%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 64.17%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 64.92%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 65.82%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 65.72%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 66.54%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 65.71%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 65.45%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 64.70%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 65.13%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 64.74%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 65.00%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 65.40%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 66.07%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 66.86%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 67.61%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 68.33%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 69.02%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 69.68%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 69.13%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 67.88%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 67.28%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 67.43%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 67.10%   [EVAL] batch:   53 | acc: 6.25%,  total acc: 65.97%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 64.77%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 63.62%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 62.50%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 61.42%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 60.38%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 60.10%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 60.55%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 59.98%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 59.03%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 58.11%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 58.37%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 59.00%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 59.61%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 60.20%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 60.78%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 61.34%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 61.27%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 61.20%   [EVAL] batch:   72 | acc: 62.50%,  total acc: 61.22%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 60.98%   [EVAL] batch:   74 | acc: 37.50%,  total acc: 60.67%   [EVAL] batch:   75 | acc: 12.50%,  total acc: 60.03%   [EVAL] batch:   76 | acc: 12.50%,  total acc: 59.42%   [EVAL] batch:   77 | acc: 6.25%,  total acc: 58.73%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 57.99%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 57.27%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 56.56%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 56.33%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 56.10%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 56.32%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 55.88%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 55.52%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 55.17%   [EVAL] batch:   87 | acc: 25.00%,  total acc: 54.83%   [EVAL] batch:   88 | acc: 37.50%,  total acc: 54.63%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 54.37%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 54.60%   [EVAL] batch:   91 | acc: 87.50%,  total acc: 54.96%   [EVAL] batch:   92 | acc: 68.75%,  total acc: 55.11%   [EVAL] batch:   93 | acc: 81.25%,  total acc: 55.39%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 55.39%   [EVAL] batch:   95 | acc: 68.75%,  total acc: 55.53%   [EVAL] batch:   96 | acc: 31.25%,  total acc: 55.28%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 55.04%   [EVAL] batch:   98 | acc: 18.75%,  total acc: 54.67%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 54.31%   [EVAL] batch:  100 | acc: 6.25%,  total acc: 53.84%   [EVAL] batch:  101 | acc: 6.25%,  total acc: 53.37%   [EVAL] batch:  102 | acc: 18.75%,  total acc: 53.03%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 53.00%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 53.10%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 53.18%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 53.56%   [EVAL] batch:  107 | acc: 68.75%,  total acc: 53.70%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 53.90%   [EVAL] batch:  109 | acc: 68.75%,  total acc: 54.03%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 54.11%   [EVAL] batch:  111 | acc: 56.25%,  total acc: 54.13%   [EVAL] batch:  112 | acc: 43.75%,  total acc: 54.04%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 54.06%   [EVAL] batch:  114 | acc: 62.50%,  total acc: 54.13%   [EVAL] batch:  115 | acc: 68.75%,  total acc: 54.26%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 54.43%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 54.50%   [EVAL] batch:  118 | acc: 50.00%,  total acc: 54.46%   [EVAL] batch:  119 | acc: 25.00%,  total acc: 54.22%   [EVAL] batch:  120 | acc: 25.00%,  total acc: 53.98%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 53.94%   [EVAL] batch:  122 | acc: 50.00%,  total acc: 53.91%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 54.13%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 54.50%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 54.86%   [EVAL] batch:  126 | acc: 87.50%,  total acc: 55.12%   [EVAL] batch:  127 | acc: 87.50%,  total acc: 55.37%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 55.18%   [EVAL] batch:  129 | acc: 0.00%,  total acc: 54.76%   [EVAL] batch:  130 | acc: 0.00%,  total acc: 54.34%   [EVAL] batch:  131 | acc: 0.00%,  total acc: 53.93%   [EVAL] batch:  132 | acc: 0.00%,  total acc: 53.52%   
cur_acc:  ['0.8712', '0.7109', '0.8616', '0.6007', '0.5625', '0.7857', '0.5284', '0.4458']
his_acc:  ['0.8712', '0.6266', '0.6921', '0.6752', '0.6168', '0.6082', '0.5557', '0.5352']
--------Round  4
seed:  500
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 5 6 4 2 1 3 0]
prepared data!
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 13.1369038CurrentTrain: epoch  0, batch     1 | loss: 13.0373726CurrentTrain: epoch  0, batch     2 | loss: 12.4351044CurrentTrain: epoch  0, batch     3 | loss: 11.6711407CurrentTrain: epoch  0, batch     4 | loss: 11.0974941CurrentTrain: epoch  0, batch     5 | loss: 11.7820444CurrentTrain: epoch  0, batch     6 | loss: 11.9453363CurrentTrain: epoch  0, batch     7 | loss: 11.6975632CurrentTrain: epoch  0, batch     8 | loss: 11.2913265CurrentTrain: epoch  0, batch     9 | loss: 11.4836006CurrentTrain: epoch  0, batch    10 | loss: 11.3519020CurrentTrain: epoch  0, batch    11 | loss: 11.5662994CurrentTrain: epoch  0, batch    12 | loss: 11.6299801CurrentTrain: epoch  0, batch    13 | loss: 11.5267467CurrentTrain: epoch  0, batch    14 | loss: 11.5683689CurrentTrain: epoch  0, batch    15 | loss: 11.3945446CurrentTrain: epoch  0, batch    16 | loss: 11.7257395CurrentTrain: epoch  0, batch    17 | loss: 11.0480938CurrentTrain: epoch  0, batch    18 | loss: 11.0310850CurrentTrain: epoch  0, batch    19 | loss: 11.2807770CurrentTrain: epoch  0, batch    20 | loss: 11.4678831CurrentTrain: epoch  0, batch    21 | loss: 11.6282692CurrentTrain: epoch  0, batch    22 | loss: 11.2502785CurrentTrain: epoch  0, batch    23 | loss: 11.1669788CurrentTrain: epoch  0, batch    24 | loss: 11.1275101CurrentTrain: epoch  0, batch    25 | loss: 11.2685356CurrentTrain: epoch  0, batch    26 | loss: 11.6091537CurrentTrain: epoch  0, batch    27 | loss: 10.9002323CurrentTrain: epoch  0, batch    28 | loss: 11.1251764CurrentTrain: epoch  0, batch    29 | loss: 10.9207382CurrentTrain: epoch  0, batch    30 | loss: 10.9629250CurrentTrain: epoch  0, batch    31 | loss: 10.6759710CurrentTrain: epoch  0, batch    32 | loss: 10.9638491CurrentTrain: epoch  0, batch    33 | loss: 10.7090025CurrentTrain: epoch  0, batch    34 | loss: 11.0502825CurrentTrain: epoch  0, batch    35 | loss: 11.1235428CurrentTrain: epoch  0, batch    36 | loss: 10.7558470CurrentTrain: epoch  0, batch    37 | loss: 10.8481121CurrentTrain: epoch  1, batch     0 | loss: 11.3128128CurrentTrain: epoch  1, batch     1 | loss: 11.1181622CurrentTrain: epoch  1, batch     2 | loss: 11.2402554CurrentTrain: epoch  1, batch     3 | loss: 10.9639578CurrentTrain: epoch  1, batch     4 | loss: 11.6944437CurrentTrain: epoch  1, batch     5 | loss: 11.6118231CurrentTrain: epoch  1, batch     6 | loss: 11.6072474CurrentTrain: epoch  1, batch     7 | loss: 11.4176502CurrentTrain: epoch  1, batch     8 | loss: 11.2480640CurrentTrain: epoch  1, batch     9 | loss: 10.9231491CurrentTrain: epoch  1, batch    10 | loss: 10.7772770CurrentTrain: epoch  1, batch    11 | loss: 10.8043470CurrentTrain: epoch  1, batch    12 | loss: 10.6628704CurrentTrain: epoch  1, batch    13 | loss: 10.7114410CurrentTrain: epoch  1, batch    14 | loss: 11.1721745CurrentTrain: epoch  1, batch    15 | loss: 11.2441368CurrentTrain: epoch  1, batch    16 | loss: 10.9226236CurrentTrain: epoch  1, batch    17 | loss: 10.7881660CurrentTrain: epoch  1, batch    18 | loss: 11.0272274CurrentTrain: epoch  1, batch    19 | loss: 10.9922333CurrentTrain: epoch  1, batch    20 | loss: 10.8597736CurrentTrain: epoch  1, batch    21 | loss: 10.6463728CurrentTrain: epoch  1, batch    22 | loss: 10.4706707CurrentTrain: epoch  1, batch    23 | loss: 10.4464531CurrentTrain: epoch  1, batch    24 | loss: 10.5685844CurrentTrain: epoch  1, batch    25 | loss: 10.7401466CurrentTrain: epoch  1, batch    26 | loss: 10.6795502CurrentTrain: epoch  1, batch    27 | loss: 10.7126217CurrentTrain: epoch  1, batch    28 | loss: 10.5359402CurrentTrain: epoch  1, batch    29 | loss: 10.5957155CurrentTrain: epoch  1, batch    30 | loss: 10.5172472CurrentTrain: epoch  1, batch    31 | loss: 10.6073990CurrentTrain: epoch  1, batch    32 | loss: 10.5724297CurrentTrain: epoch  1, batch    33 | loss: 10.4968224CurrentTrain: epoch  1, batch    34 | loss: 10.3208160CurrentTrain: epoch  1, batch    35 | loss: 10.5587502CurrentTrain: epoch  1, batch    36 | loss: 10.2620554CurrentTrain: epoch  1, batch    37 | loss: 10.1387215CurrentTrain: epoch  2, batch     0 | loss: 10.1827698CurrentTrain: epoch  2, batch     1 | loss: 10.0708771CurrentTrain: epoch  2, batch     2 | loss: 10.0479813CurrentTrain: epoch  2, batch     3 | loss: 10.0009975CurrentTrain: epoch  2, batch     4 | loss: 10.0624886CurrentTrain: epoch  2, batch     5 | loss: 10.1559515CurrentTrain: epoch  2, batch     6 | loss: 10.1526003CurrentTrain: epoch  2, batch     7 | loss: 10.1625891CurrentTrain: epoch  2, batch     8 | loss: 10.3594770CurrentTrain: epoch  2, batch     9 | loss: 10.0773106CurrentTrain: epoch  2, batch    10 | loss: 10.1058712CurrentTrain: epoch  2, batch    11 | loss: 9.8958979CurrentTrain: epoch  2, batch    12 | loss: 10.0743599CurrentTrain: epoch  2, batch    13 | loss: 10.0858097CurrentTrain: epoch  2, batch    14 | loss: 9.9650784CurrentTrain: epoch  2, batch    15 | loss: 9.6931114CurrentTrain: epoch  2, batch    16 | loss: 9.8998814CurrentTrain: epoch  2, batch    17 | loss: 9.6171532CurrentTrain: epoch  2, batch    18 | loss: 9.5558062CurrentTrain: epoch  2, batch    19 | loss: 9.6112843CurrentTrain: epoch  2, batch    20 | loss: 9.0937557CurrentTrain: epoch  2, batch    21 | loss: 9.2507839CurrentTrain: epoch  2, batch    22 | loss: 9.0728655CurrentTrain: epoch  2, batch    23 | loss: 9.0498524CurrentTrain: epoch  2, batch    24 | loss: 9.2149029CurrentTrain: epoch  2, batch    25 | loss: 9.1025372CurrentTrain: epoch  2, batch    26 | loss: 9.3729744CurrentTrain: epoch  2, batch    27 | loss: 9.1882477CurrentTrain: epoch  2, batch    28 | loss: 9.0536385CurrentTrain: epoch  2, batch    29 | loss: 8.9838905CurrentTrain: epoch  2, batch    30 | loss: 9.3588877CurrentTrain: epoch  2, batch    31 | loss: 9.1374588CurrentTrain: epoch  2, batch    32 | loss: 9.4509182CurrentTrain: epoch  2, batch    33 | loss: 8.1281910CurrentTrain: epoch  2, batch    34 | loss: 8.4415379CurrentTrain: epoch  2, batch    35 | loss: 8.4449854CurrentTrain: epoch  2, batch    36 | loss: 8.2464819CurrentTrain: epoch  2, batch    37 | loss: 9.0098457CurrentTrain: epoch  3, batch     0 | loss: 8.9155674CurrentTrain: epoch  3, batch     1 | loss: 8.3603687CurrentTrain: epoch  3, batch     2 | loss: 8.0590677CurrentTrain: epoch  3, batch     3 | loss: 8.4179163CurrentTrain: epoch  3, batch     4 | loss: 8.7363205CurrentTrain: epoch  3, batch     5 | loss: 8.3476677CurrentTrain: epoch  3, batch     6 | loss: 8.1493473CurrentTrain: epoch  3, batch     7 | loss: 8.0815458CurrentTrain: epoch  3, batch     8 | loss: 8.4256420CurrentTrain: epoch  3, batch     9 | loss: 8.1489954CurrentTrain: epoch  3, batch    10 | loss: 7.6238461CurrentTrain: epoch  3, batch    11 | loss: 8.3817196CurrentTrain: epoch  3, batch    12 | loss: 7.7227249CurrentTrain: epoch  3, batch    13 | loss: 7.5998783CurrentTrain: epoch  3, batch    14 | loss: 8.5078487CurrentTrain: epoch  3, batch    15 | loss: 8.1224737CurrentTrain: epoch  3, batch    16 | loss: 7.9170108CurrentTrain: epoch  3, batch    17 | loss: 8.0298424CurrentTrain: epoch  3, batch    18 | loss: 7.5161762CurrentTrain: epoch  3, batch    19 | loss: 8.0945721CurrentTrain: epoch  3, batch    20 | loss: 8.0587597CurrentTrain: epoch  3, batch    21 | loss: 8.1999626CurrentTrain: epoch  3, batch    22 | loss: 8.0644293CurrentTrain: epoch  3, batch    23 | loss: 7.9673500CurrentTrain: epoch  3, batch    24 | loss: 7.2745519CurrentTrain: epoch  3, batch    25 | loss: 7.9880195CurrentTrain: epoch  3, batch    26 | loss: 6.7142143CurrentTrain: epoch  3, batch    27 | loss: 7.2741499CurrentTrain: epoch  3, batch    28 | loss: 7.4727468CurrentTrain: epoch  3, batch    29 | loss: 7.0572181CurrentTrain: epoch  3, batch    30 | loss: 6.9802537CurrentTrain: epoch  3, batch    31 | loss: 6.8487124CurrentTrain: epoch  3, batch    32 | loss: 7.5617476CurrentTrain: epoch  3, batch    33 | loss: 7.0136385CurrentTrain: epoch  3, batch    34 | loss: 7.5518999CurrentTrain: epoch  3, batch    35 | loss: 7.3372993CurrentTrain: epoch  3, batch    36 | loss: 7.8434529CurrentTrain: epoch  3, batch    37 | loss: 8.5312881CurrentTrain: epoch  4, batch     0 | loss: 7.8899632CurrentTrain: epoch  4, batch     1 | loss: 8.0642920CurrentTrain: epoch  4, batch     2 | loss: 6.6419086CurrentTrain: epoch  4, batch     3 | loss: 7.8584986CurrentTrain: epoch  4, batch     4 | loss: 7.2308435CurrentTrain: epoch  4, batch     5 | loss: 6.7200532CurrentTrain: epoch  4, batch     6 | loss: 7.6266351CurrentTrain: epoch  4, batch     7 | loss: 6.6600657CurrentTrain: epoch  4, batch     8 | loss: 6.2428489CurrentTrain: epoch  4, batch     9 | loss: 7.7468815CurrentTrain: epoch  4, batch    10 | loss: 6.9504051CurrentTrain: epoch  4, batch    11 | loss: 7.2704620CurrentTrain: epoch  4, batch    12 | loss: 7.4368978CurrentTrain: epoch  4, batch    13 | loss: 6.7698946CurrentTrain: epoch  4, batch    14 | loss: 8.0327663CurrentTrain: epoch  4, batch    15 | loss: 7.7644081CurrentTrain: epoch  4, batch    16 | loss: 7.2025056CurrentTrain: epoch  4, batch    17 | loss: 6.3401647CurrentTrain: epoch  4, batch    18 | loss: 6.6394262CurrentTrain: epoch  4, batch    19 | loss: 7.0110002CurrentTrain: epoch  4, batch    20 | loss: 6.2610989CurrentTrain: epoch  4, batch    21 | loss: 6.3409824CurrentTrain: epoch  4, batch    22 | loss: 6.8997207CurrentTrain: epoch  4, batch    23 | loss: 6.0653930CurrentTrain: epoch  4, batch    24 | loss: 6.8205972CurrentTrain: epoch  4, batch    25 | loss: 6.7061052CurrentTrain: epoch  4, batch    26 | loss: 7.0430470CurrentTrain: epoch  4, batch    27 | loss: 7.0845571CurrentTrain: epoch  4, batch    28 | loss: 7.1382246CurrentTrain: epoch  4, batch    29 | loss: 6.6860132CurrentTrain: epoch  4, batch    30 | loss: 6.2179704CurrentTrain: epoch  4, batch    31 | loss: 6.2472019CurrentTrain: epoch  4, batch    32 | loss: 6.6464386CurrentTrain: epoch  4, batch    33 | loss: 6.9324827CurrentTrain: epoch  4, batch    34 | loss: 5.6804781CurrentTrain: epoch  4, batch    35 | loss: 6.3936524CurrentTrain: epoch  4, batch    36 | loss: 6.4903984CurrentTrain: epoch  4, batch    37 | loss: 7.5837808CurrentTrain: epoch  5, batch     0 | loss: 6.2470565CurrentTrain: epoch  5, batch     1 | loss: 6.0773096CurrentTrain: epoch  5, batch     2 | loss: 6.6751995CurrentTrain: epoch  5, batch     3 | loss: 6.2743745CurrentTrain: epoch  5, batch     4 | loss: 5.8382421CurrentTrain: epoch  5, batch     5 | loss: 6.3822198CurrentTrain: epoch  5, batch     6 | loss: 6.9653673CurrentTrain: epoch  5, batch     7 | loss: 6.9568114CurrentTrain: epoch  5, batch     8 | loss: 6.9344206CurrentTrain: epoch  5, batch     9 | loss: 7.3789506CurrentTrain: epoch  5, batch    10 | loss: 7.2409067CurrentTrain: epoch  5, batch    11 | loss: 6.4125218CurrentTrain: epoch  5, batch    12 | loss: 5.6734228CurrentTrain: epoch  5, batch    13 | loss: 6.7001424CurrentTrain: epoch  5, batch    14 | loss: 7.7175570CurrentTrain: epoch  5, batch    15 | loss: 6.9639883CurrentTrain: epoch  5, batch    16 | loss: 6.7602167CurrentTrain: epoch  5, batch    17 | loss: 5.7499266CurrentTrain: epoch  5, batch    18 | loss: 5.9505410CurrentTrain: epoch  5, batch    19 | loss: 7.3093567CurrentTrain: epoch  5, batch    20 | loss: 6.0010595CurrentTrain: epoch  5, batch    21 | loss: 7.3913593CurrentTrain: epoch  5, batch    22 | loss: 6.2899499CurrentTrain: epoch  5, batch    23 | loss: 6.2404742CurrentTrain: epoch  5, batch    24 | loss: 6.0625057CurrentTrain: epoch  5, batch    25 | loss: 6.5176649CurrentTrain: epoch  5, batch    26 | loss: 6.3394575CurrentTrain: epoch  5, batch    27 | loss: 5.7222371CurrentTrain: epoch  5, batch    28 | loss: 6.4153242CurrentTrain: epoch  5, batch    29 | loss: 5.9688692CurrentTrain: epoch  5, batch    30 | loss: 5.7773380CurrentTrain: epoch  5, batch    31 | loss: 5.7091637CurrentTrain: epoch  5, batch    32 | loss: 6.9256535CurrentTrain: epoch  5, batch    33 | loss: 6.7398329CurrentTrain: epoch  5, batch    34 | loss: 6.5368919CurrentTrain: epoch  5, batch    35 | loss: 6.2372928CurrentTrain: epoch  5, batch    36 | loss: 5.9985228CurrentTrain: epoch  5, batch    37 | loss: 6.1251740CurrentTrain: epoch  6, batch     0 | loss: 5.4779859CurrentTrain: epoch  6, batch     1 | loss: 5.9997802CurrentTrain: epoch  6, batch     2 | loss: 6.2369781CurrentTrain: epoch  6, batch     3 | loss: 5.8254290CurrentTrain: epoch  6, batch     4 | loss: 5.4455638CurrentTrain: epoch  6, batch     5 | loss: 6.1745787CurrentTrain: epoch  6, batch     6 | loss: 6.6108227CurrentTrain: epoch  6, batch     7 | loss: 5.8815475CurrentTrain: epoch  6, batch     8 | loss: 5.2971034CurrentTrain: epoch  6, batch     9 | loss: 6.2118402CurrentTrain: epoch  6, batch    10 | loss: 5.7937045CurrentTrain: epoch  6, batch    11 | loss: 5.8338838CurrentTrain: epoch  6, batch    12 | loss: 5.9660053CurrentTrain: epoch  6, batch    13 | loss: 5.8740149CurrentTrain: epoch  6, batch    14 | loss: 5.4782724CurrentTrain: epoch  6, batch    15 | loss: 6.4794502CurrentTrain: epoch  6, batch    16 | loss: 6.1661425CurrentTrain: epoch  6, batch    17 | loss: 6.1871347CurrentTrain: epoch  6, batch    18 | loss: 5.5150414CurrentTrain: epoch  6, batch    19 | loss: 5.5309620CurrentTrain: epoch  6, batch    20 | loss: 5.3949518CurrentTrain: epoch  6, batch    21 | loss: 5.2553930CurrentTrain: epoch  6, batch    22 | loss: 5.8035340CurrentTrain: epoch  6, batch    23 | loss: 6.0837464CurrentTrain: epoch  6, batch    24 | loss: 5.4396167CurrentTrain: epoch  6, batch    25 | loss: 6.2938933CurrentTrain: epoch  6, batch    26 | loss: 5.4653749CurrentTrain: epoch  6, batch    27 | loss: 5.6668386CurrentTrain: epoch  6, batch    28 | loss: 5.9256396CurrentTrain: epoch  6, batch    29 | loss: 6.9256334CurrentTrain: epoch  6, batch    30 | loss: 6.5750389CurrentTrain: epoch  6, batch    31 | loss: 7.9492860CurrentTrain: epoch  6, batch    32 | loss: 5.8575611CurrentTrain: epoch  6, batch    33 | loss: 6.0261059CurrentTrain: epoch  6, batch    34 | loss: 6.5986633CurrentTrain: epoch  6, batch    35 | loss: 6.1685324CurrentTrain: epoch  6, batch    36 | loss: 7.3541470CurrentTrain: epoch  6, batch    37 | loss: 6.4275880CurrentTrain: epoch  7, batch     0 | loss: 5.8035164CurrentTrain: epoch  7, batch     1 | loss: 6.4142432CurrentTrain: epoch  7, batch     2 | loss: 5.9127593CurrentTrain: epoch  7, batch     3 | loss: 6.6917534CurrentTrain: epoch  7, batch     4 | loss: 6.2323074CurrentTrain: epoch  7, batch     5 | loss: 6.3317728CurrentTrain: epoch  7, batch     6 | loss: 5.8224502CurrentTrain: epoch  7, batch     7 | loss: 6.5155001CurrentTrain: epoch  7, batch     8 | loss: 5.4192619CurrentTrain: epoch  7, batch     9 | loss: 5.6203232CurrentTrain: epoch  7, batch    10 | loss: 5.7016611CurrentTrain: epoch  7, batch    11 | loss: 5.7671194CurrentTrain: epoch  7, batch    12 | loss: 6.0848770CurrentTrain: epoch  7, batch    13 | loss: 6.1890125CurrentTrain: epoch  7, batch    14 | loss: 5.6437187CurrentTrain: epoch  7, batch    15 | loss: 5.5347099CurrentTrain: epoch  7, batch    16 | loss: 6.2274785CurrentTrain: epoch  7, batch    17 | loss: 5.4382896CurrentTrain: epoch  7, batch    18 | loss: 5.9097281CurrentTrain: epoch  7, batch    19 | loss: 5.3637266CurrentTrain: epoch  7, batch    20 | loss: 6.1634493CurrentTrain: epoch  7, batch    21 | loss: 6.0481491CurrentTrain: epoch  7, batch    22 | loss: 5.8434443CurrentTrain: epoch  7, batch    23 | loss: 5.8023767CurrentTrain: epoch  7, batch    24 | loss: 5.2502489CurrentTrain: epoch  7, batch    25 | loss: 5.8916450CurrentTrain: epoch  7, batch    26 | loss: 5.7025037CurrentTrain: epoch  7, batch    27 | loss: 6.1374702CurrentTrain: epoch  7, batch    28 | loss: 5.5148029CurrentTrain: epoch  7, batch    29 | loss: 5.5812092CurrentTrain: epoch  7, batch    30 | loss: 5.2324266CurrentTrain: epoch  7, batch    31 | loss: 6.0149755CurrentTrain: epoch  7, batch    32 | loss: 5.2574530CurrentTrain: epoch  7, batch    33 | loss: 6.5060048CurrentTrain: epoch  7, batch    34 | loss: 5.9127049CurrentTrain: epoch  7, batch    35 | loss: 5.1567707CurrentTrain: epoch  7, batch    36 | loss: 5.8431587CurrentTrain: epoch  7, batch    37 | loss: 5.9268699CurrentTrain: epoch  8, batch     0 | loss: 5.7769437CurrentTrain: epoch  8, batch     1 | loss: 5.5343714CurrentTrain: epoch  8, batch     2 | loss: 5.7224860CurrentTrain: epoch  8, batch     3 | loss: 6.8293648CurrentTrain: epoch  8, batch     4 | loss: 5.8172421CurrentTrain: epoch  8, batch     5 | loss: 5.8640819CurrentTrain: epoch  8, batch     6 | loss: 5.2667012CurrentTrain: epoch  8, batch     7 | loss: 5.8213477CurrentTrain: epoch  8, batch     8 | loss: 5.6497908CurrentTrain: epoch  8, batch     9 | loss: 5.4323835CurrentTrain: epoch  8, batch    10 | loss: 5.7326994CurrentTrain: epoch  8, batch    11 | loss: 6.7045126CurrentTrain: epoch  8, batch    12 | loss: 6.1693382CurrentTrain: epoch  8, batch    13 | loss: 5.7883744CurrentTrain: epoch  8, batch    14 | loss: 6.3085122CurrentTrain: epoch  8, batch    15 | loss: 5.9522734CurrentTrain: epoch  8, batch    16 | loss: 5.7884340CurrentTrain: epoch  8, batch    17 | loss: 5.3557515CurrentTrain: epoch  8, batch    18 | loss: 5.0274491CurrentTrain: epoch  8, batch    19 | loss: 5.1147375CurrentTrain: epoch  8, batch    20 | loss: 5.7549787CurrentTrain: epoch  8, batch    21 | loss: 5.6558943CurrentTrain: epoch  8, batch    22 | loss: 5.8615680CurrentTrain: epoch  8, batch    23 | loss: 5.0974045CurrentTrain: epoch  8, batch    24 | loss: 5.2224398CurrentTrain: epoch  8, batch    25 | loss: 5.2419028CurrentTrain: epoch  8, batch    26 | loss: 5.6862774CurrentTrain: epoch  8, batch    27 | loss: 5.1615820CurrentTrain: epoch  8, batch    28 | loss: 5.6121101CurrentTrain: epoch  8, batch    29 | loss: 5.3006630CurrentTrain: epoch  8, batch    30 | loss: 5.6362371CurrentTrain: epoch  8, batch    31 | loss: 5.1180496CurrentTrain: epoch  8, batch    32 | loss: 5.3361416CurrentTrain: epoch  8, batch    33 | loss: 5.1625042CurrentTrain: epoch  8, batch    34 | loss: 5.4055924CurrentTrain: epoch  8, batch    35 | loss: 5.6362095CurrentTrain: epoch  8, batch    36 | loss: 5.3467407CurrentTrain: epoch  8, batch    37 | loss: 4.9246368CurrentTrain: epoch  9, batch     0 | loss: 5.8001041CurrentTrain: epoch  9, batch     1 | loss: 6.0324574CurrentTrain: epoch  9, batch     2 | loss: 5.4171133CurrentTrain: epoch  9, batch     3 | loss: 5.1360874CurrentTrain: epoch  9, batch     4 | loss: 5.4426212CurrentTrain: epoch  9, batch     5 | loss: 5.2098708CurrentTrain: epoch  9, batch     6 | loss: 5.1076374CurrentTrain: epoch  9, batch     7 | loss: 5.2482800CurrentTrain: epoch  9, batch     8 | loss: 5.5651712CurrentTrain: epoch  9, batch     9 | loss: 5.1867018CurrentTrain: epoch  9, batch    10 | loss: 4.9117689CurrentTrain: epoch  9, batch    11 | loss: 5.0657301CurrentTrain: epoch  9, batch    12 | loss: 5.1256647CurrentTrain: epoch  9, batch    13 | loss: 5.2879429CurrentTrain: epoch  9, batch    14 | loss: 5.1503868CurrentTrain: epoch  9, batch    15 | loss: 4.9031978CurrentTrain: epoch  9, batch    16 | loss: 5.6363530CurrentTrain: epoch  9, batch    17 | loss: 5.2631092CurrentTrain: epoch  9, batch    18 | loss: 5.6299706CurrentTrain: epoch  9, batch    19 | loss: 5.2537565CurrentTrain: epoch  9, batch    20 | loss: 5.4786072CurrentTrain: epoch  9, batch    21 | loss: 4.9377623CurrentTrain: epoch  9, batch    22 | loss: 5.2943535CurrentTrain: epoch  9, batch    23 | loss: 4.8663940CurrentTrain: epoch  9, batch    24 | loss: 5.1714892CurrentTrain: epoch  9, batch    25 | loss: 5.6191926CurrentTrain: epoch  9, batch    26 | loss: 5.1047659CurrentTrain: epoch  9, batch    27 | loss: 5.5756903CurrentTrain: epoch  9, batch    28 | loss: 5.3380127CurrentTrain: epoch  9, batch    29 | loss: 5.0618935CurrentTrain: epoch  9, batch    30 | loss: 5.2545643CurrentTrain: epoch  9, batch    31 | loss: 4.8110294CurrentTrain: epoch  9, batch    32 | loss: 5.0222988CurrentTrain: epoch  9, batch    33 | loss: 5.0287380CurrentTrain: epoch  9, batch    34 | loss: 4.8396549CurrentTrain: epoch  9, batch    35 | loss: 4.8420043CurrentTrain: epoch  9, batch    36 | loss: 6.1314425CurrentTrain: epoch  9, batch    37 | loss: 5.0639515
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 14.4299545CurrentTrain: epoch  0, batch     1 | loss: 14.3829556CurrentTrain: epoch  0, batch     2 | loss: 14.3699360CurrentTrain: epoch  0, batch     3 | loss: 14.2887030CurrentTrain: epoch  0, batch     4 | loss: 14.0287628CurrentTrain: epoch  0, batch     5 | loss: 13.7280521CurrentTrain: epoch  0, batch     6 | loss: 13.5375795CurrentTrain: epoch  0, batch     7 | loss: 13.3972359CurrentTrain: epoch  0, batch     8 | loss: 13.6227055CurrentTrain: epoch  0, batch     9 | loss: 13.7363558CurrentTrain: epoch  0, batch    10 | loss: 13.3462906CurrentTrain: epoch  0, batch    11 | loss: 13.7561274CurrentTrain: epoch  0, batch    12 | loss: 13.4257565CurrentTrain: epoch  0, batch    13 | loss: 13.5193691CurrentTrain: epoch  0, batch    14 | loss: 13.2977772CurrentTrain: epoch  0, batch    15 | loss: 13.1226931CurrentTrain: epoch  0, batch    16 | loss: 11.6866741CurrentTrain: epoch  0, batch    17 | loss: 12.9006968CurrentTrain: epoch  0, batch    18 | loss: 13.2520132CurrentTrain: epoch  0, batch    19 | loss: 12.4820433CurrentTrain: epoch  0, batch    20 | loss: 12.1116028CurrentTrain: epoch  0, batch    21 | loss: 12.6111326CurrentTrain: epoch  0, batch    22 | loss: 12.9693775CurrentTrain: epoch  0, batch    23 | loss: 12.8699446CurrentTrain: epoch  0, batch    24 | loss: 11.9681387CurrentTrain: epoch  0, batch    25 | loss: 12.3101883CurrentTrain: epoch  0, batch    26 | loss: 12.3255444CurrentTrain: epoch  0, batch    27 | loss: 10.6600962CurrentTrain: epoch  0, batch    28 | loss: 11.8359718CurrentTrain: epoch  0, batch    29 | loss: 11.5120811CurrentTrain: epoch  0, batch    30 | loss: 12.7507019CurrentTrain: epoch  0, batch    31 | loss: 11.6828308CurrentTrain: epoch  0, batch    32 | loss: 12.1919336CurrentTrain: epoch  0, batch    33 | loss: 11.6167850CurrentTrain: epoch  0, batch    34 | loss: 11.0959835CurrentTrain: epoch  0, batch    35 | loss: 12.0352612CurrentTrain: epoch  0, batch    36 | loss: 11.5313540CurrentTrain: epoch  0, batch    37 | loss: 10.6296434CurrentTrain: epoch  1, batch     0 | loss: 10.7343111CurrentTrain: epoch  1, batch     1 | loss: 11.1207314CurrentTrain: epoch  1, batch     2 | loss: 11.7161264CurrentTrain: epoch  1, batch     3 | loss: 9.9748583CurrentTrain: epoch  1, batch     4 | loss: 10.5294304CurrentTrain: epoch  1, batch     5 | loss: 10.9319458CurrentTrain: epoch  1, batch     6 | loss: 11.0467625CurrentTrain: epoch  1, batch     7 | loss: 10.9861679CurrentTrain: epoch  1, batch     8 | loss: 10.8957577CurrentTrain: epoch  1, batch     9 | loss: 10.4972944CurrentTrain: epoch  1, batch    10 | loss: 10.1447897CurrentTrain: epoch  1, batch    11 | loss: 9.6248512CurrentTrain: epoch  1, batch    12 | loss: 11.0337477CurrentTrain: epoch  1, batch    13 | loss: 9.1644697CurrentTrain: epoch  1, batch    14 | loss: 9.6867056CurrentTrain: epoch  1, batch    15 | loss: 11.2921953CurrentTrain: epoch  1, batch    16 | loss: 10.2608004CurrentTrain: epoch  1, batch    17 | loss: 9.5539875CurrentTrain: epoch  1, batch    18 | loss: 8.5153999CurrentTrain: epoch  1, batch    19 | loss: 9.5112257CurrentTrain: epoch  1, batch    20 | loss: 9.8511209CurrentTrain: epoch  1, batch    21 | loss: 9.8524532CurrentTrain: epoch  1, batch    22 | loss: 9.0017481CurrentTrain: epoch  1, batch    23 | loss: 8.5945845CurrentTrain: epoch  1, batch    24 | loss: 10.5213804CurrentTrain: epoch  1, batch    25 | loss: 8.6676950CurrentTrain: epoch  1, batch    26 | loss: 10.1797514CurrentTrain: epoch  1, batch    27 | loss: 8.5544796CurrentTrain: epoch  1, batch    28 | loss: 10.2944326CurrentTrain: epoch  1, batch    29 | loss: 9.9311638CurrentTrain: epoch  1, batch    30 | loss: 8.8557281CurrentTrain: epoch  1, batch    31 | loss: 9.3524475CurrentTrain: epoch  1, batch    32 | loss: 8.9235582CurrentTrain: epoch  1, batch    33 | loss: 9.0023441CurrentTrain: epoch  1, batch    34 | loss: 10.5905037CurrentTrain: epoch  1, batch    35 | loss: 8.3012218CurrentTrain: epoch  1, batch    36 | loss: 9.3534431CurrentTrain: epoch  1, batch    37 | loss: 10.1173477CurrentTrain: epoch  2, batch     0 | loss: 10.7727528CurrentTrain: epoch  2, batch     1 | loss: 8.7013063CurrentTrain: epoch  2, batch     2 | loss: 8.6367245CurrentTrain: epoch  2, batch     3 | loss: 7.5223188CurrentTrain: epoch  2, batch     4 | loss: 9.5091124CurrentTrain: epoch  2, batch     5 | loss: 8.0785656CurrentTrain: epoch  2, batch     6 | loss: 9.3709774CurrentTrain: epoch  2, batch     7 | loss: 8.7546940CurrentTrain: epoch  2, batch     8 | loss: 7.4052901CurrentTrain: epoch  2, batch     9 | loss: 8.6137161CurrentTrain: epoch  2, batch    10 | loss: 9.3137779CurrentTrain: epoch  2, batch    11 | loss: 8.6996517CurrentTrain: epoch  2, batch    12 | loss: 9.2017708CurrentTrain: epoch  2, batch    13 | loss: 9.3012772CurrentTrain: epoch  2, batch    14 | loss: 7.2963963CurrentTrain: epoch  2, batch    15 | loss: 8.1838074CurrentTrain: epoch  2, batch    16 | loss: 8.3935719CurrentTrain: epoch  2, batch    17 | loss: 8.3237448CurrentTrain: epoch  2, batch    18 | loss: 7.8167295CurrentTrain: epoch  2, batch    19 | loss: 7.7322955CurrentTrain: epoch  2, batch    20 | loss: 8.2612972CurrentTrain: epoch  2, batch    21 | loss: 8.4906673CurrentTrain: epoch  2, batch    22 | loss: 10.0072098CurrentTrain: epoch  2, batch    23 | loss: 8.0755386CurrentTrain: epoch  2, batch    24 | loss: 7.6811590CurrentTrain: epoch  2, batch    25 | loss: 7.6394806CurrentTrain: epoch  2, batch    26 | loss: 7.7266102CurrentTrain: epoch  2, batch    27 | loss: 8.2893763CurrentTrain: epoch  2, batch    28 | loss: 8.1672182CurrentTrain: epoch  2, batch    29 | loss: 7.9174919CurrentTrain: epoch  2, batch    30 | loss: 8.9416361CurrentTrain: epoch  2, batch    31 | loss: 9.5858583CurrentTrain: epoch  2, batch    32 | loss: 7.2121382CurrentTrain: epoch  2, batch    33 | loss: 7.5821385CurrentTrain: epoch  2, batch    34 | loss: 7.6137881CurrentTrain: epoch  2, batch    35 | loss: 7.6004243CurrentTrain: epoch  2, batch    36 | loss: 7.6703262CurrentTrain: epoch  2, batch    37 | loss: 7.8837695CurrentTrain: epoch  3, batch     0 | loss: 9.3263397CurrentTrain: epoch  3, batch     1 | loss: 6.5875635CurrentTrain: epoch  3, batch     2 | loss: 8.8538971CurrentTrain: epoch  3, batch     3 | loss: 7.7862587CurrentTrain: epoch  3, batch     4 | loss: 7.5270481CurrentTrain: epoch  3, batch     5 | loss: 7.4000196CurrentTrain: epoch  3, batch     6 | loss: 7.7331672CurrentTrain: epoch  3, batch     7 | loss: 7.0172653CurrentTrain: epoch  3, batch     8 | loss: 9.0980892CurrentTrain: epoch  3, batch     9 | loss: 7.7776732CurrentTrain: epoch  3, batch    10 | loss: 6.8673310CurrentTrain: epoch  3, batch    11 | loss: 8.0533056CurrentTrain: epoch  3, batch    12 | loss: 7.1856861CurrentTrain: epoch  3, batch    13 | loss: 6.9492145CurrentTrain: epoch  3, batch    14 | loss: 7.3498225CurrentTrain: epoch  3, batch    15 | loss: 9.0491295CurrentTrain: epoch  3, batch    16 | loss: 7.9316611CurrentTrain: epoch  3, batch    17 | loss: 7.1555319CurrentTrain: epoch  3, batch    18 | loss: 8.6392689CurrentTrain: epoch  3, batch    19 | loss: 8.2036953CurrentTrain: epoch  3, batch    20 | loss: 7.9417658CurrentTrain: epoch  3, batch    21 | loss: 6.9880481CurrentTrain: epoch  3, batch    22 | loss: 7.4051366CurrentTrain: epoch  3, batch    23 | loss: 7.5365052CurrentTrain: epoch  3, batch    24 | loss: 8.9229784CurrentTrain: epoch  3, batch    25 | loss: 7.0522938CurrentTrain: epoch  3, batch    26 | loss: 7.7065039CurrentTrain: epoch  3, batch    27 | loss: 8.0160875CurrentTrain: epoch  3, batch    28 | loss: 7.3632030CurrentTrain: epoch  3, batch    29 | loss: 6.7353888CurrentTrain: epoch  3, batch    30 | loss: 6.7355742CurrentTrain: epoch  3, batch    31 | loss: 8.0579634CurrentTrain: epoch  3, batch    32 | loss: 7.1417961CurrentTrain: epoch  3, batch    33 | loss: 7.0544205CurrentTrain: epoch  3, batch    34 | loss: 6.7728500CurrentTrain: epoch  3, batch    35 | loss: 6.3157349CurrentTrain: epoch  3, batch    36 | loss: 6.9010415CurrentTrain: epoch  3, batch    37 | loss: 6.3480682CurrentTrain: epoch  4, batch     0 | loss: 5.9664178CurrentTrain: epoch  4, batch     1 | loss: 7.3551226CurrentTrain: epoch  4, batch     2 | loss: 6.7966633CurrentTrain: epoch  4, batch     3 | loss: 6.8589845CurrentTrain: epoch  4, batch     4 | loss: 7.2731638CurrentTrain: epoch  4, batch     5 | loss: 7.4077854CurrentTrain: epoch  4, batch     6 | loss: 7.3778811CurrentTrain: epoch  4, batch     7 | loss: 6.8913851CurrentTrain: epoch  4, batch     8 | loss: 7.5026751CurrentTrain: epoch  4, batch     9 | loss: 6.7321515CurrentTrain: epoch  4, batch    10 | loss: 6.3133774CurrentTrain: epoch  4, batch    11 | loss: 7.4155293CurrentTrain: epoch  4, batch    12 | loss: 6.9367747CurrentTrain: epoch  4, batch    13 | loss: 6.6989603CurrentTrain: epoch  4, batch    14 | loss: 6.3397803CurrentTrain: epoch  4, batch    15 | loss: 6.9263268CurrentTrain: epoch  4, batch    16 | loss: 6.2493358CurrentTrain: epoch  4, batch    17 | loss: 7.3920002CurrentTrain: epoch  4, batch    18 | loss: 6.3767619CurrentTrain: epoch  4, batch    19 | loss: 6.8112369CurrentTrain: epoch  4, batch    20 | loss: 6.2270646CurrentTrain: epoch  4, batch    21 | loss: 6.9307714CurrentTrain: epoch  4, batch    22 | loss: 6.7781878CurrentTrain: epoch  4, batch    23 | loss: 6.3468604CurrentTrain: epoch  4, batch    24 | loss: 7.1719227CurrentTrain: epoch  4, batch    25 | loss: 7.1205268CurrentTrain: epoch  4, batch    26 | loss: 6.1695485CurrentTrain: epoch  4, batch    27 | loss: 6.5705481CurrentTrain: epoch  4, batch    28 | loss: 6.5115623CurrentTrain: epoch  4, batch    29 | loss: 5.1684704CurrentTrain: epoch  4, batch    30 | loss: 6.0168700CurrentTrain: epoch  4, batch    31 | loss: 6.2164617CurrentTrain: epoch  4, batch    32 | loss: 7.7865233CurrentTrain: epoch  4, batch    33 | loss: 8.0008268CurrentTrain: epoch  4, batch    34 | loss: 7.1927371CurrentTrain: epoch  4, batch    35 | loss: 6.8104873CurrentTrain: epoch  4, batch    36 | loss: 7.1424246CurrentTrain: epoch  4, batch    37 | loss: 7.0864706CurrentTrain: epoch  5, batch     0 | loss: 6.8430004CurrentTrain: epoch  5, batch     1 | loss: 5.5488715CurrentTrain: epoch  5, batch     2 | loss: 7.0592685CurrentTrain: epoch  5, batch     3 | loss: 6.2902608CurrentTrain: epoch  5, batch     4 | loss: 6.9034204CurrentTrain: epoch  5, batch     5 | loss: 6.1813788CurrentTrain: epoch  5, batch     6 | loss: 5.7197990CurrentTrain: epoch  5, batch     7 | loss: 6.0099859CurrentTrain: epoch  5, batch     8 | loss: 5.7239685CurrentTrain: epoch  5, batch     9 | loss: 6.6836476CurrentTrain: epoch  5, batch    10 | loss: 6.1085038CurrentTrain: epoch  5, batch    11 | loss: 6.4434175CurrentTrain: epoch  5, batch    12 | loss: 6.0853152CurrentTrain: epoch  5, batch    13 | loss: 6.8116512CurrentTrain: epoch  5, batch    14 | loss: 6.3648934CurrentTrain: epoch  5, batch    15 | loss: 5.8284087CurrentTrain: epoch  5, batch    16 | loss: 6.3208809CurrentTrain: epoch  5, batch    17 | loss: 5.8747206CurrentTrain: epoch  5, batch    18 | loss: 5.7980008CurrentTrain: epoch  5, batch    19 | loss: 5.9835305CurrentTrain: epoch  5, batch    20 | loss: 5.7564473CurrentTrain: epoch  5, batch    21 | loss: 5.9164014CurrentTrain: epoch  5, batch    22 | loss: 6.6367822CurrentTrain: epoch  5, batch    23 | loss: 5.8399563CurrentTrain: epoch  5, batch    24 | loss: 5.9988503CurrentTrain: epoch  5, batch    25 | loss: 6.4998002CurrentTrain: epoch  5, batch    26 | loss: 5.9559369CurrentTrain: epoch  5, batch    27 | loss: 7.1323915CurrentTrain: epoch  5, batch    28 | loss: 5.8363919CurrentTrain: epoch  5, batch    29 | loss: 7.3406973CurrentTrain: epoch  5, batch    30 | loss: 5.5722513CurrentTrain: epoch  5, batch    31 | loss: 6.3652444CurrentTrain: epoch  5, batch    32 | loss: 6.3796096CurrentTrain: epoch  5, batch    33 | loss: 6.2298903CurrentTrain: epoch  5, batch    34 | loss: 6.3047314CurrentTrain: epoch  5, batch    35 | loss: 5.8569565CurrentTrain: epoch  5, batch    36 | loss: 5.8517923CurrentTrain: epoch  5, batch    37 | loss: 5.5496321CurrentTrain: epoch  6, batch     0 | loss: 6.6371999CurrentTrain: epoch  6, batch     1 | loss: 6.1155472CurrentTrain: epoch  6, batch     2 | loss: 5.7862248CurrentTrain: epoch  6, batch     3 | loss: 5.9722290CurrentTrain: epoch  6, batch     4 | loss: 5.5984268CurrentTrain: epoch  6, batch     5 | loss: 5.8735766CurrentTrain: epoch  6, batch     6 | loss: 5.6494751CurrentTrain: epoch  6, batch     7 | loss: 5.6908183CurrentTrain: epoch  6, batch     8 | loss: 5.7082558CurrentTrain: epoch  6, batch     9 | loss: 6.3780909CurrentTrain: epoch  6, batch    10 | loss: 6.1720757CurrentTrain: epoch  6, batch    11 | loss: 5.3966932CurrentTrain: epoch  6, batch    12 | loss: 5.8738446CurrentTrain: epoch  6, batch    13 | loss: 5.0486879CurrentTrain: epoch  6, batch    14 | loss: 6.0103426CurrentTrain: epoch  6, batch    15 | loss: 5.4896302CurrentTrain: epoch  6, batch    16 | loss: 5.7174211CurrentTrain: epoch  6, batch    17 | loss: 5.3082881CurrentTrain: epoch  6, batch    18 | loss: 5.6656828CurrentTrain: epoch  6, batch    19 | loss: 5.5848346CurrentTrain: epoch  6, batch    20 | loss: 5.1850824CurrentTrain: epoch  6, batch    21 | loss: 5.9977694CurrentTrain: epoch  6, batch    22 | loss: 5.3412733CurrentTrain: epoch  6, batch    23 | loss: 5.6447086CurrentTrain: epoch  6, batch    24 | loss: 5.2766533CurrentTrain: epoch  6, batch    25 | loss: 6.2781725CurrentTrain: epoch  6, batch    26 | loss: 5.3576922CurrentTrain: epoch  6, batch    27 | loss: 5.5259914CurrentTrain: epoch  6, batch    28 | loss: 5.6568389CurrentTrain: epoch  6, batch    29 | loss: 5.2544050CurrentTrain: epoch  6, batch    30 | loss: 5.3155103CurrentTrain: epoch  6, batch    31 | loss: 5.2623219CurrentTrain: epoch  6, batch    32 | loss: 5.0918479CurrentTrain: epoch  6, batch    33 | loss: 5.3618488CurrentTrain: epoch  6, batch    34 | loss: 5.3825650CurrentTrain: epoch  6, batch    35 | loss: 5.8926768CurrentTrain: epoch  6, batch    36 | loss: 5.3024468CurrentTrain: epoch  6, batch    37 | loss: 5.8205328CurrentTrain: epoch  7, batch     0 | loss: 6.1486297CurrentTrain: epoch  7, batch     1 | loss: 5.1428351CurrentTrain: epoch  7, batch     2 | loss: 5.6285844CurrentTrain: epoch  7, batch     3 | loss: 5.0538964CurrentTrain: epoch  7, batch     4 | loss: 5.0844536CurrentTrain: epoch  7, batch     5 | loss: 5.1480656CurrentTrain: epoch  7, batch     6 | loss: 5.6770563CurrentTrain: epoch  7, batch     7 | loss: 5.3292875CurrentTrain: epoch  7, batch     8 | loss: 5.8094850CurrentTrain: epoch  7, batch     9 | loss: 5.6531153CurrentTrain: epoch  7, batch    10 | loss: 5.1343894CurrentTrain: epoch  7, batch    11 | loss: 4.9461517CurrentTrain: epoch  7, batch    12 | loss: 5.8278489CurrentTrain: epoch  7, batch    13 | loss: 4.9570680CurrentTrain: epoch  7, batch    14 | loss: 5.1741037CurrentTrain: epoch  7, batch    15 | loss: 5.6404772CurrentTrain: epoch  7, batch    16 | loss: 5.1634617CurrentTrain: epoch  7, batch    17 | loss: 5.0246019CurrentTrain: epoch  7, batch    18 | loss: 5.2188439CurrentTrain: epoch  7, batch    19 | loss: 5.0651503CurrentTrain: epoch  7, batch    20 | loss: 4.9929953CurrentTrain: epoch  7, batch    21 | loss: 5.0373278CurrentTrain: epoch  7, batch    22 | loss: 5.1960139CurrentTrain: epoch  7, batch    23 | loss: 7.2064519CurrentTrain: epoch  7, batch    24 | loss: 5.4703293CurrentTrain: epoch  7, batch    25 | loss: 5.1589589CurrentTrain: epoch  7, batch    26 | loss: 5.2436872CurrentTrain: epoch  7, batch    27 | loss: 5.2411036CurrentTrain: epoch  7, batch    28 | loss: 5.1527724CurrentTrain: epoch  7, batch    29 | loss: 5.9657140CurrentTrain: epoch  7, batch    30 | loss: 5.3298259CurrentTrain: epoch  7, batch    31 | loss: 5.3764181CurrentTrain: epoch  7, batch    32 | loss: 5.0913105CurrentTrain: epoch  7, batch    33 | loss: 5.5476422CurrentTrain: epoch  7, batch    34 | loss: 4.8169193CurrentTrain: epoch  7, batch    35 | loss: 5.3128109CurrentTrain: epoch  7, batch    36 | loss: 5.0324211CurrentTrain: epoch  7, batch    37 | loss: 4.9649339CurrentTrain: epoch  8, batch     0 | loss: 5.1264176CurrentTrain: epoch  8, batch     1 | loss: 5.6786222CurrentTrain: epoch  8, batch     2 | loss: 5.3363948CurrentTrain: epoch  8, batch     3 | loss: 5.5512276CurrentTrain: epoch  8, batch     4 | loss: 4.9734983CurrentTrain: epoch  8, batch     5 | loss: 4.9982185CurrentTrain: epoch  8, batch     6 | loss: 5.0698314CurrentTrain: epoch  8, batch     7 | loss: 5.0114684CurrentTrain: epoch  8, batch     8 | loss: 4.9510975CurrentTrain: epoch  8, batch     9 | loss: 5.0524116CurrentTrain: epoch  8, batch    10 | loss: 5.1874118CurrentTrain: epoch  8, batch    11 | loss: 5.1580701CurrentTrain: epoch  8, batch    12 | loss: 4.9409304CurrentTrain: epoch  8, batch    13 | loss: 5.0339499CurrentTrain: epoch  8, batch    14 | loss: 5.2872944CurrentTrain: epoch  8, batch    15 | loss: 5.7889647CurrentTrain: epoch  8, batch    16 | loss: 4.9236474CurrentTrain: epoch  8, batch    17 | loss: 4.9692621CurrentTrain: epoch  8, batch    18 | loss: 5.2595711CurrentTrain: epoch  8, batch    19 | loss: 5.0316935CurrentTrain: epoch  8, batch    20 | loss: 4.9527097CurrentTrain: epoch  8, batch    21 | loss: 4.8882360CurrentTrain: epoch  8, batch    22 | loss: 5.1469336CurrentTrain: epoch  8, batch    23 | loss: 4.8694124CurrentTrain: epoch  8, batch    24 | loss: 5.5099583CurrentTrain: epoch  8, batch    25 | loss: 5.2041888CurrentTrain: epoch  8, batch    26 | loss: 5.0782833CurrentTrain: epoch  8, batch    27 | loss: 4.9696250CurrentTrain: epoch  8, batch    28 | loss: 5.0900950CurrentTrain: epoch  8, batch    29 | loss: 4.9042459CurrentTrain: epoch  8, batch    30 | loss: 5.3005819CurrentTrain: epoch  8, batch    31 | loss: 5.2579422CurrentTrain: epoch  8, batch    32 | loss: 4.9576950CurrentTrain: epoch  8, batch    33 | loss: 4.9616976CurrentTrain: epoch  8, batch    34 | loss: 4.9084826CurrentTrain: epoch  8, batch    35 | loss: 4.8485136CurrentTrain: epoch  8, batch    36 | loss: 4.9968472CurrentTrain: epoch  8, batch    37 | loss: 5.0228281CurrentTrain: epoch  9, batch     0 | loss: 4.9601083CurrentTrain: epoch  9, batch     1 | loss: 4.9155145CurrentTrain: epoch  9, batch     2 | loss: 5.0182829CurrentTrain: epoch  9, batch     3 | loss: 6.6370959CurrentTrain: epoch  9, batch     4 | loss: 4.8278713CurrentTrain: epoch  9, batch     5 | loss: 5.4596958CurrentTrain: epoch  9, batch     6 | loss: 5.6393347CurrentTrain: epoch  9, batch     7 | loss: 5.0596375CurrentTrain: epoch  9, batch     8 | loss: 5.3484030CurrentTrain: epoch  9, batch     9 | loss: 4.8897443CurrentTrain: epoch  9, batch    10 | loss: 5.2060342CurrentTrain: epoch  9, batch    11 | loss: 4.9240842CurrentTrain: epoch  9, batch    12 | loss: 4.9354753CurrentTrain: epoch  9, batch    13 | loss: 5.1034698CurrentTrain: epoch  9, batch    14 | loss: 5.1006441CurrentTrain: epoch  9, batch    15 | loss: 5.0293179CurrentTrain: epoch  9, batch    16 | loss: 5.4520321CurrentTrain: epoch  9, batch    17 | loss: 5.0063505CurrentTrain: epoch  9, batch    18 | loss: 4.8775616CurrentTrain: epoch  9, batch    19 | loss: 4.9384046CurrentTrain: epoch  9, batch    20 | loss: 4.9178586CurrentTrain: epoch  9, batch    21 | loss: 4.8346334CurrentTrain: epoch  9, batch    22 | loss: 4.7832880CurrentTrain: epoch  9, batch    23 | loss: 5.4621501CurrentTrain: epoch  9, batch    24 | loss: 4.9041667CurrentTrain: epoch  9, batch    25 | loss: 4.9616184CurrentTrain: epoch  9, batch    26 | loss: 4.7822571CurrentTrain: epoch  9, batch    27 | loss: 4.8491678CurrentTrain: epoch  9, batch    28 | loss: 5.1886015CurrentTrain: epoch  9, batch    29 | loss: 4.7462578CurrentTrain: epoch  9, batch    30 | loss: 4.8076072CurrentTrain: epoch  9, batch    31 | loss: 4.6945486CurrentTrain: epoch  9, batch    32 | loss: 4.7734170CurrentTrain: epoch  9, batch    33 | loss: 4.7804785CurrentTrain: epoch  9, batch    34 | loss: 4.9274373CurrentTrain: epoch  9, batch    35 | loss: 4.8892646CurrentTrain: epoch  9, batch    36 | loss: 5.0390854CurrentTrain: epoch  9, batch    37 | loss: 5.4390049
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 85.29%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 83.88%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 84.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.51%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 86.14%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.74%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.96%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.39%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.91%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.26%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 87.69%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 85.29%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 83.88%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 84.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.51%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 86.14%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.74%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.96%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.39%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.91%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.26%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 87.69%   
cur_acc_llm:  [0.8768939393939394]
his_acc_llm:  [0.8768939393939394]
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.23%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.60%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.20%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.26%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.95%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 88.31%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.67%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 87.12%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.23%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.60%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.20%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.26%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.95%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 88.31%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.67%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 87.12%   
cur_acc:  ['0.8712']
his_acc:  ['0.8712']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 4.2733860CurrentTrain: epoch  0, batch     1 | loss: 5.7102776CurrentTrain: epoch  1, batch     0 | loss: 3.8824081CurrentTrain: epoch  1, batch     1 | loss: 3.3205991CurrentTrain: epoch  2, batch     0 | loss: 2.6370990CurrentTrain: epoch  2, batch     1 | loss: 2.6588502CurrentTrain: epoch  3, batch     0 | loss: 2.3730941CurrentTrain: epoch  3, batch     1 | loss: 2.2796795CurrentTrain: epoch  4, batch     0 | loss: 2.1961267CurrentTrain: epoch  4, batch     1 | loss: 1.9745718CurrentTrain: epoch  5, batch     0 | loss: 1.9696065CurrentTrain: epoch  5, batch     1 | loss: 1.7904074CurrentTrain: epoch  6, batch     0 | loss: 1.8728852CurrentTrain: epoch  6, batch     1 | loss: 1.8331596CurrentTrain: epoch  7, batch     0 | loss: 1.8321319CurrentTrain: epoch  7, batch     1 | loss: 1.7704442CurrentTrain: epoch  8, batch     0 | loss: 1.8336653CurrentTrain: epoch  8, batch     1 | loss: 1.7792947CurrentTrain: epoch  9, batch     0 | loss: 1.7870293CurrentTrain: epoch  9, batch     1 | loss: 1.7072772
Mixup data size:  61
MixupTrain:  epoch  0, batch     0 | loss: 7.1573779MixupTrain:  epoch  0, batch     2 | loss: 7.9929705MixupTrain:  epoch  0, batch     3 | loss: 7.3125948
MemoryTrain:  epoch  0, batch     0 | loss: 1.6686381MemoryTrain:  epoch  1, batch     0 | loss: 2.0135458MemoryTrain:  epoch  2, batch     0 | loss: 0.9249216MemoryTrain:  epoch  3, batch     0 | loss: 0.1810241MemoryTrain:  epoch  4, batch     0 | loss: 0.6461607MemoryTrain:  epoch  5, batch     0 | loss: 0.0705623MemoryTrain:  epoch  6, batch     0 | loss: 0.0489697MemoryTrain:  epoch  7, batch     0 | loss: 0.0528147MemoryTrain:  epoch  8, batch     0 | loss: 0.0232646MemoryTrain:  epoch  9, batch     0 | loss: 0.0346459
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 6.2911215CurrentTrain: epoch  0, batch     1 | loss: 6.1493206CurrentTrain: epoch  1, batch     0 | loss: 5.1385956CurrentTrain: epoch  1, batch     1 | loss: 4.8658648CurrentTrain: epoch  2, batch     0 | loss: 4.6489563CurrentTrain: epoch  2, batch     1 | loss: 3.9262547CurrentTrain: epoch  3, batch     0 | loss: 4.7293530CurrentTrain: epoch  3, batch     1 | loss: 3.6280844CurrentTrain: epoch  4, batch     0 | loss: 4.5281363CurrentTrain: epoch  4, batch     1 | loss: 2.8001747CurrentTrain: epoch  5, batch     0 | loss: 3.2229590CurrentTrain: epoch  5, batch     1 | loss: 3.2906637CurrentTrain: epoch  6, batch     0 | loss: 3.2026064CurrentTrain: epoch  6, batch     1 | loss: 2.8483462CurrentTrain: epoch  7, batch     0 | loss: 3.0238452CurrentTrain: epoch  7, batch     1 | loss: 2.5815511CurrentTrain: epoch  8, batch     0 | loss: 2.6980226CurrentTrain: epoch  8, batch     1 | loss: 2.5678751CurrentTrain: epoch  9, batch     0 | loss: 2.4921846CurrentTrain: epoch  9, batch     1 | loss: 2.4838431
Mixup data size:  61
MixupTrain:  epoch  0, batch     0 | loss: 4.8756149MixupTrain:  epoch  0, batch     1 | loss: 4.9472031
MemoryTrain:  epoch  0, batch     0 | loss: 2.6335127MemoryTrain:  epoch  1, batch     0 | loss: 2.5406280MemoryTrain:  epoch  2, batch     0 | loss: 1.9950398MemoryTrain:  epoch  3, batch     0 | loss: 1.7546749MemoryTrain:  epoch  4, batch     0 | loss: 1.3064233MemoryTrain:  epoch  5, batch     0 | loss: 1.1620448MemoryTrain:  epoch  6, batch     0 | loss: 0.7408618MemoryTrain:  epoch  7, batch     0 | loss: 0.8425632MemoryTrain:  epoch  8, batch     0 | loss: 0.6478938MemoryTrain:  epoch  9, batch     0 | loss: 0.5123680
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 96.09%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 95.14%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 95.45%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 92.79%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.55%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.93%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 84.03%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 84.72%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 81.70%   [EVAL] batch:   28 | acc: 12.50%,  total acc: 79.31%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 76.88%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 74.40%   [EVAL] batch:   31 | acc: 6.25%,  total acc: 72.27%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 71.78%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 72.24%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 73.04%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 73.44%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 74.16%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 74.84%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 75.48%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 76.09%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 76.68%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 76.79%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 77.33%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 77.84%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 78.19%   
cur_acc_llm:  [0.8768939393939394, 0.9278846153846154]
his_acc_llm:  [0.8768939393939394, 0.7819444444444444]
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 67.71%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 62.50%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 66.41%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 72.73%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 73.96%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 71.63%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.29%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.44%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.24%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.97%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.64%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.82%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 86.85%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 86.88%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 86.69%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.11%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 87.68%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 87.86%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 87.85%   [EVAL] batch:   36 | acc: 25.00%,  total acc: 86.15%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 84.70%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 83.49%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 82.97%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 83.38%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 83.18%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 83.58%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 83.52%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 83.47%   
cur_acc:  ['0.8712', '0.7163']
his_acc:  ['0.8712', '0.8347']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 6.0162115CurrentTrain: epoch  0, batch     1 | loss: 5.4786973CurrentTrain: epoch  1, batch     0 | loss: 4.5098653CurrentTrain: epoch  1, batch     1 | loss: 4.5902529CurrentTrain: epoch  2, batch     0 | loss: 4.0554142CurrentTrain: epoch  2, batch     1 | loss: 5.2751966CurrentTrain: epoch  3, batch     0 | loss: 4.3784294CurrentTrain: epoch  3, batch     1 | loss: 3.4970343CurrentTrain: epoch  4, batch     0 | loss: 3.5513861CurrentTrain: epoch  4, batch     1 | loss: 3.0031521CurrentTrain: epoch  5, batch     0 | loss: 3.4187355CurrentTrain: epoch  5, batch     1 | loss: 2.4483943CurrentTrain: epoch  6, batch     0 | loss: 2.5826464CurrentTrain: epoch  6, batch     1 | loss: 2.8466167CurrentTrain: epoch  7, batch     0 | loss: 2.4679952CurrentTrain: epoch  7, batch     1 | loss: 2.6049037CurrentTrain: epoch  8, batch     0 | loss: 2.1003690CurrentTrain: epoch  8, batch     1 | loss: 2.2789829CurrentTrain: epoch  9, batch     0 | loss: 2.1349792CurrentTrain: epoch  9, batch     1 | loss: 1.9951518
Mixup data size:  70
MixupTrain:  epoch  0, batch     2 | loss: 7.8029667MixupTrain:  epoch  0, batch     4 | loss: 5.8182513
MemoryTrain:  epoch  0, batch     0 | loss: 0.6771557MemoryTrain:  epoch  1, batch     0 | loss: 1.8784982MemoryTrain:  epoch  2, batch     0 | loss: 0.9453706MemoryTrain:  epoch  3, batch     0 | loss: 0.5224907MemoryTrain:  epoch  4, batch     0 | loss: 0.1834705MemoryTrain:  epoch  5, batch     0 | loss: 0.1286550MemoryTrain:  epoch  6, batch     0 | loss: 0.0807085MemoryTrain:  epoch  7, batch     0 | loss: 0.0858043MemoryTrain:  epoch  8, batch     0 | loss: 0.0586208MemoryTrain:  epoch  9, batch     0 | loss: 0.0561755
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 9.1998997CurrentTrain: epoch  0, batch     1 | loss: 8.3045454CurrentTrain: epoch  1, batch     0 | loss: 7.4604053CurrentTrain: epoch  1, batch     1 | loss: 6.9499984CurrentTrain: epoch  2, batch     0 | loss: 6.7131376CurrentTrain: epoch  2, batch     1 | loss: 6.9938388CurrentTrain: epoch  3, batch     0 | loss: 5.7125282CurrentTrain: epoch  3, batch     1 | loss: 7.3858376CurrentTrain: epoch  4, batch     0 | loss: 6.0557528CurrentTrain: epoch  4, batch     1 | loss: 6.0690064CurrentTrain: epoch  5, batch     0 | loss: 5.9425993CurrentTrain: epoch  5, batch     1 | loss: 5.0102983CurrentTrain: epoch  6, batch     0 | loss: 5.4470673CurrentTrain: epoch  6, batch     1 | loss: 5.0443225CurrentTrain: epoch  7, batch     0 | loss: 5.0685778CurrentTrain: epoch  7, batch     1 | loss: 4.8696823CurrentTrain: epoch  8, batch     0 | loss: 4.6109018CurrentTrain: epoch  8, batch     1 | loss: 3.6564679CurrentTrain: epoch  9, batch     0 | loss: 4.0028563CurrentTrain: epoch  9, batch     1 | loss: 3.4931142
Mixup data size:  70
MixupTrain:  epoch  0, batch     0 | loss: 6.1544564
MemoryTrain:  epoch  0, batch     0 | loss: 2.0818563MemoryTrain:  epoch  1, batch     0 | loss: 2.3713529MemoryTrain:  epoch  2, batch     0 | loss: 2.0248449MemoryTrain:  epoch  3, batch     0 | loss: 1.5974191MemoryTrain:  epoch  4, batch     0 | loss: 1.5595188MemoryTrain:  epoch  5, batch     0 | loss: 1.1443682MemoryTrain:  epoch  6, batch     0 | loss: 1.0154334MemoryTrain:  epoch  7, batch     0 | loss: 0.7338517MemoryTrain:  epoch  8, batch     0 | loss: 0.6229379MemoryTrain:  epoch  9, batch     0 | loss: 0.6042902
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 94.44%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 86.36%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 83.33%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 79.33%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 77.68%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 74.58%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.95%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 87.08%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 85.55%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.93%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 84.03%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 84.72%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 81.70%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 79.09%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 76.46%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 73.99%   [EVAL] batch:   31 | acc: 0.00%,  total acc: 71.68%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 71.02%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 71.51%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 72.32%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 72.74%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 73.48%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 74.01%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 74.52%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 74.84%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 74.54%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 73.96%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 73.55%   [EVAL] batch:   43 | acc: 31.25%,  total acc: 72.59%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 71.94%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 72.15%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 72.47%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 73.34%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 73.88%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 74.39%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 74.88%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 75.35%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 75.69%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 75.45%   [EVAL] batch:   55 | acc: 31.25%,  total acc: 74.67%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 74.23%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 73.60%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 73.31%   [EVAL] batch:   59 | acc: 25.00%,  total acc: 72.50%   
cur_acc_llm:  [0.8768939393939394, 0.9278846153846154, 0.7458333333333333]
his_acc_llm:  [0.8768939393939394, 0.7819444444444444, 0.725]
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 18.75%,  total acc: 77.84%   [EVAL] batch:   11 | acc: 12.50%,  total acc: 72.40%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 67.79%   [EVAL] batch:   13 | acc: 6.25%,  total acc: 63.39%   [EVAL] batch:   14 | acc: 0.00%,  total acc: 59.17%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 39.29%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 46.88%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 52.78%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 56.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 60.80%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 63.02%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 64.90%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 65.18%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 65.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 65.23%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 65.81%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 65.46%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 66.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 68.15%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 69.60%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 70.65%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 73.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 74.04%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 74.77%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.67%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 76.51%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 76.88%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 77.22%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 77.93%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 78.60%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 79.04%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 79.64%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 79.86%   [EVAL] batch:   36 | acc: 25.00%,  total acc: 78.38%   [EVAL] batch:   37 | acc: 37.50%,  total acc: 77.30%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 76.28%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 75.62%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 75.76%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 75.30%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 75.44%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 74.72%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 74.59%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 74.87%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 75.13%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 75.51%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 75.75%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 75.86%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 76.08%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 76.30%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 76.62%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 76.25%   [EVAL] batch:   55 | acc: 18.75%,  total acc: 75.22%   [EVAL] batch:   56 | acc: 12.50%,  total acc: 74.12%   [EVAL] batch:   57 | acc: 12.50%,  total acc: 73.06%   [EVAL] batch:   58 | acc: 6.25%,  total acc: 71.93%   [EVAL] batch:   59 | acc: 0.00%,  total acc: 70.73%   
cur_acc:  ['0.8712', '0.7163', '0.5917']
his_acc:  ['0.8712', '0.8347', '0.7073']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 7.8496809CurrentTrain: epoch  0, batch     1 | loss: 7.9298739CurrentTrain: epoch  1, batch     0 | loss: 7.0211077CurrentTrain: epoch  1, batch     1 | loss: 6.9664111CurrentTrain: epoch  2, batch     0 | loss: 6.8992376CurrentTrain: epoch  2, batch     1 | loss: 5.4732752CurrentTrain: epoch  3, batch     0 | loss: 6.1581955CurrentTrain: epoch  3, batch     1 | loss: 5.7586370CurrentTrain: epoch  4, batch     0 | loss: 6.3888617CurrentTrain: epoch  4, batch     1 | loss: 4.5553684CurrentTrain: epoch  5, batch     0 | loss: 6.3067446CurrentTrain: epoch  5, batch     1 | loss: 3.9682710CurrentTrain: epoch  6, batch     0 | loss: 5.2315536CurrentTrain: epoch  6, batch     1 | loss: 4.8317847CurrentTrain: epoch  7, batch     0 | loss: 4.9843540CurrentTrain: epoch  7, batch     1 | loss: 5.1394238CurrentTrain: epoch  8, batch     0 | loss: 4.8932209CurrentTrain: epoch  8, batch     1 | loss: 4.2632360CurrentTrain: epoch  9, batch     0 | loss: 4.4289193CurrentTrain: epoch  9, batch     1 | loss: 4.3157163
Mixup data size:  79
MixupTrain:  epoch  0, batch     0 | loss: 5.2210123MixupTrain:  epoch  0, batch     2 | loss: 6.9042964MixupTrain:  epoch  0, batch     3 | loss: 4.0295728
MemoryTrain:  epoch  0, batch     0 | loss: 1.0054508MemoryTrain:  epoch  0, batch     1 | loss: 0.7045081MemoryTrain:  epoch  1, batch     0 | loss: 1.2777362MemoryTrain:  epoch  1, batch     1 | loss: 0.6135268MemoryTrain:  epoch  2, batch     0 | loss: 0.7222659MemoryTrain:  epoch  2, batch     1 | loss: 1.0905979MemoryTrain:  epoch  3, batch     0 | loss: 0.3732529MemoryTrain:  epoch  3, batch     1 | loss: 0.9789585MemoryTrain:  epoch  4, batch     0 | loss: 0.3567257MemoryTrain:  epoch  4, batch     1 | loss: 0.3421051MemoryTrain:  epoch  5, batch     0 | loss: 0.1135846MemoryTrain:  epoch  5, batch     1 | loss: 0.3290803MemoryTrain:  epoch  6, batch     0 | loss: 0.1654902MemoryTrain:  epoch  6, batch     1 | loss: 0.1214357MemoryTrain:  epoch  7, batch     0 | loss: 0.0738877MemoryTrain:  epoch  7, batch     1 | loss: 0.1510576MemoryTrain:  epoch  8, batch     0 | loss: 0.0425645MemoryTrain:  epoch  8, batch     1 | loss: 0.0422648MemoryTrain:  epoch  9, batch     0 | loss: 0.0327391MemoryTrain:  epoch  9, batch     1 | loss: 0.1214258
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 10.1294842CurrentTrain: epoch  0, batch     1 | loss: 9.4807587CurrentTrain: epoch  1, batch     0 | loss: 8.6855068CurrentTrain: epoch  1, batch     1 | loss: 8.5774956CurrentTrain: epoch  2, batch     0 | loss: 8.1749907CurrentTrain: epoch  2, batch     1 | loss: 7.1753802CurrentTrain: epoch  3, batch     0 | loss: 7.9399614CurrentTrain: epoch  3, batch     1 | loss: 6.4643974CurrentTrain: epoch  4, batch     0 | loss: 7.5965004CurrentTrain: epoch  4, batch     1 | loss: 5.3216844CurrentTrain: epoch  5, batch     0 | loss: 5.8242655CurrentTrain: epoch  5, batch     1 | loss: 6.8982010CurrentTrain: epoch  6, batch     0 | loss: 6.0759239CurrentTrain: epoch  6, batch     1 | loss: 6.1371431CurrentTrain: epoch  7, batch     0 | loss: 5.8516068CurrentTrain: epoch  7, batch     1 | loss: 6.0917187CurrentTrain: epoch  8, batch     0 | loss: 5.6091728CurrentTrain: epoch  8, batch     1 | loss: 4.8002729CurrentTrain: epoch  9, batch     0 | loss: 4.7397504CurrentTrain: epoch  9, batch     1 | loss: 5.7738414
Mixup data size:  80

MemoryTrain:  epoch  0, batch     0 | loss: 1.8734099MemoryTrain:  epoch  0, batch     1 | loss: 0.4237736MemoryTrain:  epoch  1, batch     0 | loss: 2.1023569MemoryTrain:  epoch  1, batch     1 | loss: 0.7770377MemoryTrain:  epoch  2, batch     0 | loss: 1.2635915MemoryTrain:  epoch  2, batch     1 | loss: 1.1817567MemoryTrain:  epoch  3, batch     0 | loss: 1.0253096MemoryTrain:  epoch  3, batch     1 | loss: 0.6366347MemoryTrain:  epoch  4, batch     0 | loss: 0.9572983MemoryTrain:  epoch  4, batch     1 | loss: 0.2587460MemoryTrain:  epoch  5, batch     0 | loss: 0.9721521MemoryTrain:  epoch  5, batch     1 | loss: 0.1796537MemoryTrain:  epoch  6, batch     0 | loss: 0.8618873MemoryTrain:  epoch  6, batch     1 | loss: 0.4257671MemoryTrain:  epoch  7, batch     0 | loss: 0.5630646MemoryTrain:  epoch  7, batch     1 | loss: 0.6827001MemoryTrain:  epoch  8, batch     0 | loss: 0.5462642MemoryTrain:  epoch  8, batch     1 | loss: 0.2753940MemoryTrain:  epoch  9, batch     0 | loss: 0.5601681MemoryTrain:  epoch  9, batch     1 | loss: 0.2531760
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 20.83%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 21.88%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 22.50%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 22.92%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 24.11%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 31.25%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 35.42%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 38.12%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 40.91%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 43.75%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 44.71%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 45.54%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 44.58%   [EVAL] batch:   15 | acc: 31.25%,  total acc: 43.75%   [EVAL] batch:   16 | acc: 37.50%,  total acc: 43.38%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 43.40%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 43.09%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 43.75%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 43.45%   [EVAL] batch:   21 | acc: 18.75%,  total acc: 42.33%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 66.67%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 69.64%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 80.11%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 80.73%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 80.29%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 77.68%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 77.50%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 76.47%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 76.04%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 75.66%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 76.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 77.38%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 78.41%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 79.35%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 80.21%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 81.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.73%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 79.17%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 76.34%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 73.71%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 71.25%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 68.95%   [EVAL] batch:   31 | acc: 6.25%,  total acc: 66.99%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 66.48%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 67.10%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 67.86%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 68.06%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 68.58%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 69.24%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 69.71%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 69.53%   [EVAL] batch:   40 | acc: 0.00%,  total acc: 67.84%   [EVAL] batch:   41 | acc: 6.25%,  total acc: 66.37%   [EVAL] batch:   42 | acc: 25.00%,  total acc: 65.41%   [EVAL] batch:   43 | acc: 6.25%,  total acc: 64.06%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 63.19%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 63.45%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 63.96%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 64.58%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 65.18%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 65.75%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 66.42%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 67.07%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 67.69%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 68.17%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 68.18%   [EVAL] batch:   55 | acc: 12.50%,  total acc: 67.19%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 66.01%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 64.87%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 63.77%   [EVAL] batch:   59 | acc: 12.50%,  total acc: 62.92%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 62.40%   [EVAL] batch:   61 | acc: 12.50%,  total acc: 61.59%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 60.91%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 60.35%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 59.81%   [EVAL] batch:   65 | acc: 25.00%,  total acc: 59.28%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 59.51%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 59.74%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 59.69%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 59.82%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 60.12%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 60.07%   [EVAL] batch:   72 | acc: 56.25%,  total acc: 60.02%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 59.80%   [EVAL] batch:   74 | acc: 37.50%,  total acc: 59.50%   [EVAL] batch:   75 | acc: 18.75%,  total acc: 58.96%   [EVAL] batch:   76 | acc: 43.75%,  total acc: 58.77%   [EVAL] batch:   77 | acc: 43.75%,  total acc: 58.57%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 58.47%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 58.28%   [EVAL] batch:   80 | acc: 31.25%,  total acc: 57.95%   
cur_acc_llm:  [0.8768939393939394, 0.9278846153846154, 0.7458333333333333, 0.42329545454545453]
his_acc_llm:  [0.8768939393939394, 0.7819444444444444, 0.725, 0.5794753086419753]
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 39.06%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 31.25%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 28.12%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 26.79%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 29.69%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 32.64%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 33.75%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 34.66%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 34.90%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 36.54%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 39.29%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 42.92%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 45.31%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 47.79%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 48.96%   [EVAL] batch:   18 | acc: 12.50%,  total acc: 47.04%   [EVAL] batch:   19 | acc: 12.50%,  total acc: 45.31%   [EVAL] batch:   20 | acc: 6.25%,  total acc: 43.45%   [EVAL] batch:   21 | acc: 6.25%,  total acc: 41.76%   
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 20.83%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 21.88%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 22.50%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 22.92%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 30.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 39.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 45.83%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 50.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 55.11%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 57.81%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 58.17%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 58.04%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 59.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 58.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 59.93%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 60.07%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 60.20%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 61.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 63.39%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 65.06%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 66.58%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 69.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 70.43%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 71.30%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 72.32%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 73.28%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 73.75%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 74.40%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 75.76%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 76.29%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 76.96%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 77.26%   [EVAL] batch:   36 | acc: 25.00%,  total acc: 75.84%   [EVAL] batch:   37 | acc: 25.00%,  total acc: 74.51%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 72.92%   [EVAL] batch:   39 | acc: 43.75%,  total acc: 72.19%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 72.26%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 71.88%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 71.80%   [EVAL] batch:   43 | acc: 43.75%,  total acc: 71.16%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 70.97%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 71.06%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 71.54%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 71.74%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 72.07%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 72.50%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 72.79%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 73.20%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 73.58%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 73.96%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 73.64%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 72.32%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 71.05%   [EVAL] batch:   57 | acc: 6.25%,  total acc: 69.94%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 68.75%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 68.33%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 68.14%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 67.74%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 66.87%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 65.82%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 65.00%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 64.02%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 63.81%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 63.60%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 63.50%   [EVAL] batch:   69 | acc: 37.50%,  total acc: 63.12%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 62.94%   [EVAL] batch:   71 | acc: 37.50%,  total acc: 62.59%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 62.76%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 63.01%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 63.33%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 63.65%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 63.80%   [EVAL] batch:   77 | acc: 31.25%,  total acc: 63.38%   [EVAL] batch:   78 | acc: 6.25%,  total acc: 62.66%   [EVAL] batch:   79 | acc: 12.50%,  total acc: 62.03%   [EVAL] batch:   80 | acc: 12.50%,  total acc: 61.42%   
cur_acc:  ['0.8712', '0.7163', '0.5917', '0.4176']
his_acc:  ['0.8712', '0.8347', '0.7073', '0.6142']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 6.0047665CurrentTrain: epoch  0, batch     1 | loss: 7.2073922CurrentTrain: epoch  1, batch     0 | loss: 5.5969129CurrentTrain: epoch  1, batch     1 | loss: 4.6628332CurrentTrain: epoch  2, batch     0 | loss: 4.0418386CurrentTrain: epoch  2, batch     1 | loss: 4.3964887CurrentTrain: epoch  3, batch     0 | loss: 3.2530999CurrentTrain: epoch  3, batch     1 | loss: 3.8030608CurrentTrain: epoch  4, batch     0 | loss: 3.2324181CurrentTrain: epoch  4, batch     1 | loss: 3.2531555CurrentTrain: epoch  5, batch     0 | loss: 2.5354748CurrentTrain: epoch  5, batch     1 | loss: 3.2918179CurrentTrain: epoch  6, batch     0 | loss: 2.4548972CurrentTrain: epoch  6, batch     1 | loss: 2.6993933CurrentTrain: epoch  7, batch     0 | loss: 2.3279128CurrentTrain: epoch  7, batch     1 | loss: 2.0691178CurrentTrain: epoch  8, batch     0 | loss: 2.1316242CurrentTrain: epoch  8, batch     1 | loss: 1.9226816CurrentTrain: epoch  9, batch     0 | loss: 2.0140486CurrentTrain: epoch  9, batch     1 | loss: 2.0664527
Mixup data size:  91
MixupTrain:  epoch  0, batch     3 | loss: 4.7474093MixupTrain:  epoch  0, batch     5 | loss: 3.8152998
MemoryTrain:  epoch  0, batch     0 | loss: 0.7422891MemoryTrain:  epoch  0, batch     1 | loss: 0.5528576MemoryTrain:  epoch  1, batch     0 | loss: 0.7353634MemoryTrain:  epoch  1, batch     1 | loss: 1.1304160MemoryTrain:  epoch  2, batch     0 | loss: 0.4853433MemoryTrain:  epoch  2, batch     1 | loss: 0.3688501MemoryTrain:  epoch  3, batch     0 | loss: 0.3685054MemoryTrain:  epoch  3, batch     1 | loss: 0.2130462MemoryTrain:  epoch  4, batch     0 | loss: 0.1889295MemoryTrain:  epoch  4, batch     1 | loss: 0.1426162MemoryTrain:  epoch  5, batch     0 | loss: 0.0998203MemoryTrain:  epoch  5, batch     1 | loss: 0.1132486MemoryTrain:  epoch  6, batch     0 | loss: 0.0794631MemoryTrain:  epoch  6, batch     1 | loss: 0.0858131MemoryTrain:  epoch  7, batch     0 | loss: 0.0414319MemoryTrain:  epoch  7, batch     1 | loss: 0.0405943MemoryTrain:  epoch  8, batch     0 | loss: 0.0323662MemoryTrain:  epoch  8, batch     1 | loss: 0.0302339MemoryTrain:  epoch  9, batch     0 | loss: 0.0240812MemoryTrain:  epoch  9, batch     1 | loss: 0.0408020
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 8.1937218CurrentTrain: epoch  0, batch     1 | loss: 7.3128796CurrentTrain: epoch  1, batch     0 | loss: 7.3690028CurrentTrain: epoch  1, batch     1 | loss: 5.5808110CurrentTrain: epoch  2, batch     0 | loss: 6.2753534CurrentTrain: epoch  2, batch     1 | loss: 5.0556593CurrentTrain: epoch  3, batch     0 | loss: 5.5322351CurrentTrain: epoch  3, batch     1 | loss: 4.4409428CurrentTrain: epoch  4, batch     0 | loss: 4.9450016CurrentTrain: epoch  4, batch     1 | loss: 3.9958348CurrentTrain: epoch  5, batch     0 | loss: 3.9467628CurrentTrain: epoch  5, batch     1 | loss: 4.4719815CurrentTrain: epoch  6, batch     0 | loss: 4.5293989CurrentTrain: epoch  6, batch     1 | loss: 2.9797804CurrentTrain: epoch  7, batch     0 | loss: 3.3122365CurrentTrain: epoch  7, batch     1 | loss: 3.7387848CurrentTrain: epoch  8, batch     0 | loss: 3.4529982CurrentTrain: epoch  8, batch     1 | loss: 2.7653198CurrentTrain: epoch  9, batch     0 | loss: 3.0977330CurrentTrain: epoch  9, batch     1 | loss: 2.5847764
Mixup data size:  90
MixupTrain:  epoch  0, batch     0 | loss: 5.4628049MixupTrain:  epoch  0, batch     1 | loss: 4.6528915MixupTrain:  epoch  0, batch     2 | loss: 4.1708389MixupTrain:  epoch  0, batch     3 | loss: 3.8817943MixupTrain:  epoch  0, batch     4 | loss: 4.1111280MixupTrain:  epoch  0, batch     5 | loss: 3.4934647
MemoryTrain:  epoch  0, batch     0 | loss: 1.1971184MemoryTrain:  epoch  0, batch     1 | loss: 0.3875805MemoryTrain:  epoch  1, batch     0 | loss: 1.1222026MemoryTrain:  epoch  1, batch     1 | loss: 0.9661360MemoryTrain:  epoch  2, batch     0 | loss: 1.1836386MemoryTrain:  epoch  2, batch     1 | loss: 0.8189116MemoryTrain:  epoch  3, batch     0 | loss: 0.8683290MemoryTrain:  epoch  3, batch     1 | loss: 0.5075006MemoryTrain:  epoch  4, batch     0 | loss: 0.7374039MemoryTrain:  epoch  4, batch     1 | loss: 0.2946268MemoryTrain:  epoch  5, batch     0 | loss: 0.6878989MemoryTrain:  epoch  5, batch     1 | loss: 0.3239125MemoryTrain:  epoch  6, batch     0 | loss: 0.3909158MemoryTrain:  epoch  6, batch     1 | loss: 0.3670514MemoryTrain:  epoch  7, batch     0 | loss: 0.6623492MemoryTrain:  epoch  7, batch     1 | loss: 0.2802436MemoryTrain:  epoch  8, batch     0 | loss: 0.3746261MemoryTrain:  epoch  8, batch     1 | loss: 0.2783070MemoryTrain:  epoch  9, batch     0 | loss: 0.3089506MemoryTrain:  epoch  9, batch     1 | loss: 0.2321135
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 92.97%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 92.79%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 93.30%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 94.14%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 90.62%   
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 12.50%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 11.25%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 13.54%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 22.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 32.03%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 39.58%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 45.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 50.57%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 54.17%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 54.81%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 53.57%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 53.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 53.91%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 54.78%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 55.56%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 57.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 59.23%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 61.08%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 65.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 66.83%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 64.81%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 62.50%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 60.34%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 58.33%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 56.45%   [EVAL] batch:   31 | acc: 0.00%,  total acc: 54.69%   [EVAL] batch:   32 | acc: 12.50%,  total acc: 53.41%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 53.49%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 53.57%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 53.47%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 54.73%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 55.76%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 56.73%   [EVAL] batch:   39 | acc: 43.75%,  total acc: 56.41%   [EVAL] batch:   40 | acc: 0.00%,  total acc: 55.03%   [EVAL] batch:   41 | acc: 0.00%,  total acc: 53.72%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 52.47%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 51.28%   [EVAL] batch:   44 | acc: 6.25%,  total acc: 50.28%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 50.54%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 51.20%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 51.95%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 52.55%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 52.88%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 53.80%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 54.69%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 55.54%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 56.48%   [EVAL] batch:   55 | acc: 12.50%,  total acc: 55.69%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 54.71%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 53.77%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 52.86%   [EVAL] batch:   59 | acc: 0.00%,  total acc: 51.98%   [EVAL] batch:   60 | acc: 6.25%,  total acc: 51.23%   [EVAL] batch:   61 | acc: 12.50%,  total acc: 50.60%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 50.00%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 49.61%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 49.04%   [EVAL] batch:   65 | acc: 31.25%,  total acc: 48.77%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 48.69%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 48.53%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 48.82%   [EVAL] batch:   69 | acc: 56.25%,  total acc: 48.93%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 49.12%   [EVAL] batch:   71 | acc: 37.50%,  total acc: 48.96%   [EVAL] batch:   72 | acc: 43.75%,  total acc: 48.89%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 48.99%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 49.17%   [EVAL] batch:   75 | acc: 50.00%,  total acc: 49.18%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 49.19%   [EVAL] batch:   77 | acc: 31.25%,  total acc: 48.96%   [EVAL] batch:   78 | acc: 18.75%,  total acc: 48.58%   [EVAL] batch:   79 | acc: 18.75%,  total acc: 48.20%   [EVAL] batch:   80 | acc: 31.25%,  total acc: 47.99%   [EVAL] batch:   81 | acc: 100.00%,  total acc: 48.63%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 49.10%   [EVAL] batch:   83 | acc: 93.75%,  total acc: 49.63%   [EVAL] batch:   84 | acc: 87.50%,  total acc: 50.07%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 50.65%   [EVAL] batch:   86 | acc: 81.25%,  total acc: 51.01%   [EVAL] batch:   87 | acc: 100.00%,  total acc: 51.56%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 52.04%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 52.50%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 52.75%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 53.19%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 53.70%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 54.19%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 54.67%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 55.14%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 55.61%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 56.06%   [EVAL] batch:   98 | acc: 6.25%,  total acc: 55.56%   
cur_acc_llm:  [0.8768939393939394, 0.9278846153846154, 0.7458333333333333, 0.42329545454545453, 0.90625]
his_acc_llm:  [0.8768939393939394, 0.7819444444444444, 0.725, 0.5794753086419753, 0.5555555555555556]
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 75.57%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 77.60%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 79.33%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 80.80%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 82.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 83.20%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 80.90%   
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 9.38%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 6.25%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 5.21%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 15.18%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 25.78%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 33.33%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 38.75%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 43.75%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 47.40%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 49.52%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 50.89%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 52.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 52.73%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 54.04%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 54.51%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 54.93%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 56.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.33%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 60.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 61.96%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 63.54%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 65.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 66.35%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 67.36%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 68.53%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 69.61%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 70.21%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 70.97%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 71.21%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 70.40%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 69.82%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 69.10%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 68.07%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 67.11%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 65.87%   [EVAL] batch:   39 | acc: 31.25%,  total acc: 65.00%   [EVAL] batch:   40 | acc: 12.50%,  total acc: 63.72%   [EVAL] batch:   41 | acc: 31.25%,  total acc: 62.95%   [EVAL] batch:   42 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:   43 | acc: 31.25%,  total acc: 61.79%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 60.97%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 60.33%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 59.31%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 58.98%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 58.67%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 57.88%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 58.46%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 59.13%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 59.79%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 60.30%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 60.23%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 59.15%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 58.11%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 57.11%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 56.14%   [EVAL] batch:   59 | acc: 6.25%,  total acc: 55.31%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 55.12%   [EVAL] batch:   61 | acc: 37.50%,  total acc: 54.84%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 54.07%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 53.22%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 52.60%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 51.80%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 51.68%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 51.38%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 51.27%   [EVAL] batch:   69 | acc: 31.25%,  total acc: 50.98%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 51.06%   [EVAL] batch:   71 | acc: 37.50%,  total acc: 50.87%   [EVAL] batch:   72 | acc: 62.50%,  total acc: 51.03%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 51.35%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 51.50%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 51.56%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 51.62%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 51.28%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 50.63%   [EVAL] batch:   79 | acc: 6.25%,  total acc: 50.08%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 49.77%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 50.23%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 50.60%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 51.04%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 51.25%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 51.38%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 51.51%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 51.99%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 52.39%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 52.43%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 52.61%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 52.85%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 53.36%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 53.86%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 54.34%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 54.82%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 55.28%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 55.74%   [EVAL] batch:   98 | acc: 6.25%,  total acc: 55.24%   
cur_acc:  ['0.8712', '0.7163', '0.5917', '0.4176', '0.8090']
his_acc:  ['0.8712', '0.8347', '0.7073', '0.6142', '0.5524']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 5.0082636CurrentTrain: epoch  0, batch     1 | loss: 4.2153888CurrentTrain: epoch  1, batch     0 | loss: 3.0020556CurrentTrain: epoch  1, batch     1 | loss: 3.5632722CurrentTrain: epoch  2, batch     0 | loss: 3.1310358CurrentTrain: epoch  2, batch     1 | loss: 1.9935629CurrentTrain: epoch  3, batch     0 | loss: 2.0486288CurrentTrain: epoch  3, batch     1 | loss: 2.9439924CurrentTrain: epoch  4, batch     0 | loss: 2.4039588CurrentTrain: epoch  4, batch     1 | loss: 1.7590551CurrentTrain: epoch  5, batch     0 | loss: 2.3354740CurrentTrain: epoch  5, batch     1 | loss: 1.8245062CurrentTrain: epoch  6, batch     0 | loss: 2.0850279CurrentTrain: epoch  6, batch     1 | loss: 2.1973615CurrentTrain: epoch  7, batch     0 | loss: 2.0819292CurrentTrain: epoch  7, batch     1 | loss: 1.8975993CurrentTrain: epoch  8, batch     0 | loss: 1.8420115CurrentTrain: epoch  8, batch     1 | loss: 1.8458875CurrentTrain: epoch  9, batch     0 | loss: 1.7856766CurrentTrain: epoch  9, batch     1 | loss: 1.7811688
Mixup data size:  101
MixupTrain:  epoch  0, batch     4 | loss: 3.2997110MixupTrain:  epoch  0, batch     6 | loss: 2.9065102
MemoryTrain:  epoch  0, batch     0 | loss: 1.2704899MemoryTrain:  epoch  0, batch     1 | loss: 1.0823163MemoryTrain:  epoch  1, batch     0 | loss: 1.2924672MemoryTrain:  epoch  1, batch     1 | loss: 0.9827785MemoryTrain:  epoch  2, batch     0 | loss: 0.9234692MemoryTrain:  epoch  2, batch     1 | loss: 0.4003182MemoryTrain:  epoch  3, batch     0 | loss: 0.4677406MemoryTrain:  epoch  3, batch     1 | loss: 0.6928014MemoryTrain:  epoch  4, batch     0 | loss: 0.4194285MemoryTrain:  epoch  4, batch     1 | loss: 0.3410268MemoryTrain:  epoch  5, batch     0 | loss: 0.1751790MemoryTrain:  epoch  5, batch     1 | loss: 0.4504325MemoryTrain:  epoch  6, batch     0 | loss: 0.2094340MemoryTrain:  epoch  6, batch     1 | loss: 0.1197172MemoryTrain:  epoch  7, batch     0 | loss: 0.1011254MemoryTrain:  epoch  7, batch     1 | loss: 0.1018651MemoryTrain:  epoch  8, batch     0 | loss: 0.1339673MemoryTrain:  epoch  8, batch     1 | loss: 0.0291448MemoryTrain:  epoch  9, batch     0 | loss: 0.0298967MemoryTrain:  epoch  9, batch     1 | loss: 0.0692888
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 6.5967455CurrentTrain: epoch  0, batch     1 | loss: 6.9907198CurrentTrain: epoch  1, batch     0 | loss: 5.2048616CurrentTrain: epoch  1, batch     1 | loss: 4.5578928CurrentTrain: epoch  2, batch     0 | loss: 4.3831530CurrentTrain: epoch  2, batch     1 | loss: 4.5352187CurrentTrain: epoch  3, batch     0 | loss: 4.1281147CurrentTrain: epoch  3, batch     1 | loss: 3.2944045CurrentTrain: epoch  4, batch     0 | loss: 3.5014093CurrentTrain: epoch  4, batch     1 | loss: 3.3239968CurrentTrain: epoch  5, batch     0 | loss: 3.2809155CurrentTrain: epoch  5, batch     1 | loss: 2.8879771CurrentTrain: epoch  6, batch     0 | loss: 2.8172026CurrentTrain: epoch  6, batch     1 | loss: 2.7345338CurrentTrain: epoch  7, batch     0 | loss: 2.7270761CurrentTrain: epoch  7, batch     1 | loss: 2.5366516CurrentTrain: epoch  8, batch     0 | loss: 2.5167048CurrentTrain: epoch  8, batch     1 | loss: 2.1763628CurrentTrain: epoch  9, batch     0 | loss: 2.4036598CurrentTrain: epoch  9, batch     1 | loss: 2.0685689
Mixup data size:  101
MixupTrain:  epoch  0, batch     4 | loss: 3.4272400MixupTrain:  epoch  0, batch     6 | loss: 2.5500916
MemoryTrain:  epoch  0, batch     0 | loss: 1.9326166MemoryTrain:  epoch  0, batch     1 | loss: 0.9114004MemoryTrain:  epoch  1, batch     0 | loss: 1.3790886MemoryTrain:  epoch  1, batch     1 | loss: 1.4993420MemoryTrain:  epoch  2, batch     0 | loss: 1.2117342MemoryTrain:  epoch  2, batch     1 | loss: 1.0597358MemoryTrain:  epoch  3, batch     0 | loss: 0.9622895MemoryTrain:  epoch  3, batch     1 | loss: 0.5247251MemoryTrain:  epoch  4, batch     0 | loss: 0.7335498MemoryTrain:  epoch  4, batch     1 | loss: 0.9307398MemoryTrain:  epoch  5, batch     0 | loss: 0.5197342MemoryTrain:  epoch  5, batch     1 | loss: 0.7270142MemoryTrain:  epoch  6, batch     0 | loss: 0.7753736MemoryTrain:  epoch  6, batch     1 | loss: 0.4411033MemoryTrain:  epoch  7, batch     0 | loss: 0.9822237MemoryTrain:  epoch  7, batch     1 | loss: 0.3711880MemoryTrain:  epoch  8, batch     0 | loss: 0.4755177MemoryTrain:  epoch  8, batch     1 | loss: 0.6142804MemoryTrain:  epoch  9, batch     0 | loss: 0.3908126MemoryTrain:  epoch  9, batch     1 | loss: 0.5974193
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 98.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 98.96%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 98.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 97.92%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 93.75%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 86.61%   
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 18.75%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 15.62%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 16.25%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 16.67%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 25.00%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 34.38%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 41.67%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 46.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 51.70%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 54.69%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 54.33%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 52.23%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 52.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 52.73%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 53.31%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 54.17%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 54.93%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 55.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.04%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 59.94%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 61.41%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 63.02%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 64.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 65.87%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 63.89%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 61.61%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 59.70%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 57.71%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 55.85%   [EVAL] batch:   31 | acc: 0.00%,  total acc: 54.10%   [EVAL] batch:   32 | acc: 18.75%,  total acc: 53.03%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 53.12%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 53.57%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 53.65%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 54.73%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 55.92%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 57.05%   [EVAL] batch:   39 | acc: 43.75%,  total acc: 56.72%   [EVAL] batch:   40 | acc: 0.00%,  total acc: 55.34%   [EVAL] batch:   41 | acc: 0.00%,  total acc: 54.02%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 52.76%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 51.56%   [EVAL] batch:   44 | acc: 6.25%,  total acc: 50.56%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 51.22%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 51.86%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 52.86%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 53.83%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 54.50%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 55.02%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 55.77%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 56.01%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 56.60%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 56.70%   [EVAL] batch:   55 | acc: 12.50%,  total acc: 55.92%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 54.93%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 53.99%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 53.07%   [EVAL] batch:   59 | acc: 0.00%,  total acc: 52.19%   [EVAL] batch:   60 | acc: 0.00%,  total acc: 51.33%   [EVAL] batch:   61 | acc: 6.25%,  total acc: 50.60%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 49.80%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 49.02%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 48.27%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 47.54%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 47.29%   [EVAL] batch:   67 | acc: 6.25%,  total acc: 46.69%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 46.74%   [EVAL] batch:   69 | acc: 31.25%,  total acc: 46.52%   [EVAL] batch:   70 | acc: 18.75%,  total acc: 46.13%   [EVAL] batch:   71 | acc: 37.50%,  total acc: 46.01%   [EVAL] batch:   72 | acc: 56.25%,  total acc: 46.15%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 46.28%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 46.50%   [EVAL] batch:   75 | acc: 50.00%,  total acc: 46.55%   [EVAL] batch:   76 | acc: 43.75%,  total acc: 46.51%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 46.15%   [EVAL] batch:   78 | acc: 6.25%,  total acc: 45.65%   [EVAL] batch:   79 | acc: 12.50%,  total acc: 45.23%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 44.98%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 44.44%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 43.98%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 43.45%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 42.94%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 42.44%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 42.03%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 42.61%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 43.19%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 43.82%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 44.37%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 44.90%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 45.50%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 46.08%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 46.64%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 47.20%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 47.74%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 48.28%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 48.80%   [EVAL] batch:   99 | acc: 93.75%,  total acc: 49.25%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 49.75%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 50.25%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 50.73%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 51.20%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 51.61%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 52.06%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 52.45%   [EVAL] batch:  107 | acc: 56.25%,  total acc: 52.49%   [EVAL] batch:  108 | acc: 68.75%,  total acc: 52.64%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 52.95%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 53.04%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 53.12%   
cur_acc_llm:  [0.8768939393939394, 0.9278846153846154, 0.7458333333333333, 0.42329545454545453, 0.90625, 0.8660714285714286]
his_acc_llm:  [0.8768939393939394, 0.7819444444444444, 0.725, 0.5794753086419753, 0.5555555555555556, 0.53125]
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 99.22%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 98.61%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 92.50%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 87.50%   
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 6.25%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 4.17%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 3.12%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 3.75%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 3.12%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 13.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 24.22%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 32.64%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 38.75%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 43.75%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 47.40%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 47.60%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 46.43%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 48.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 48.83%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 50.37%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 51.04%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 51.64%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 53.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 55.36%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 57.39%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 59.24%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 60.94%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 63.94%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 65.05%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 66.29%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 67.46%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 67.92%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 69.73%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 69.32%   [EVAL] batch:   33 | acc: 37.50%,  total acc: 68.38%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 67.86%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 67.36%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 66.39%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 65.46%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 64.74%   [EVAL] batch:   39 | acc: 31.25%,  total acc: 63.91%   [EVAL] batch:   40 | acc: 18.75%,  total acc: 62.80%   [EVAL] batch:   41 | acc: 31.25%,  total acc: 62.05%   [EVAL] batch:   42 | acc: 43.75%,  total acc: 61.63%   [EVAL] batch:   43 | acc: 18.75%,  total acc: 60.65%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 59.31%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 58.29%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 57.31%   [EVAL] batch:   47 | acc: 18.75%,  total acc: 56.51%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 55.61%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 54.62%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 55.27%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 56.13%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 56.84%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 57.52%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 57.50%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 56.47%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 55.48%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 54.53%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 53.60%   [EVAL] batch:   59 | acc: 0.00%,  total acc: 52.71%   [EVAL] batch:   60 | acc: 12.50%,  total acc: 52.05%   [EVAL] batch:   61 | acc: 6.25%,  total acc: 51.31%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 50.50%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 49.71%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 49.04%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 48.30%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 47.76%   [EVAL] batch:   67 | acc: 6.25%,  total acc: 47.15%   [EVAL] batch:   68 | acc: 12.50%,  total acc: 46.65%   [EVAL] batch:   69 | acc: 6.25%,  total acc: 46.07%   [EVAL] batch:   70 | acc: 25.00%,  total acc: 45.77%   [EVAL] batch:   71 | acc: 18.75%,  total acc: 45.40%   [EVAL] batch:   72 | acc: 37.50%,  total acc: 45.29%   [EVAL] batch:   73 | acc: 31.25%,  total acc: 45.10%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 45.33%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 45.15%   [EVAL] batch:   76 | acc: 18.75%,  total acc: 44.81%   [EVAL] batch:   77 | acc: 12.50%,  total acc: 44.39%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 43.83%   [EVAL] batch:   79 | acc: 6.25%,  total acc: 43.36%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 42.90%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 42.38%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 41.87%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 41.37%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 40.88%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 40.41%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 39.94%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 40.48%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 41.01%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 41.32%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 41.62%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 41.98%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 42.61%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 43.22%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 43.82%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 44.40%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 44.97%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 45.54%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 46.09%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 46.62%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 47.15%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 47.67%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 48.18%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 48.68%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 49.17%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 49.59%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 50.00%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 49.88%   [EVAL] batch:  108 | acc: 81.25%,  total acc: 50.17%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 50.57%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 50.73%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 50.84%   
cur_acc:  ['0.8712', '0.7163', '0.5917', '0.4176', '0.8090', '0.8750']
his_acc:  ['0.8712', '0.8347', '0.7073', '0.6142', '0.5524', '0.5084']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 5.6928329CurrentTrain: epoch  0, batch     1 | loss: 6.3805633CurrentTrain: epoch  1, batch     0 | loss: 4.5631027CurrentTrain: epoch  1, batch     1 | loss: 4.4991722CurrentTrain: epoch  2, batch     0 | loss: 3.5722337CurrentTrain: epoch  2, batch     1 | loss: 4.7279458CurrentTrain: epoch  3, batch     0 | loss: 3.4246912CurrentTrain: epoch  3, batch     1 | loss: 4.5751815CurrentTrain: epoch  4, batch     0 | loss: 3.5975685CurrentTrain: epoch  4, batch     1 | loss: 3.4679914CurrentTrain: epoch  5, batch     0 | loss: 3.3340299CurrentTrain: epoch  5, batch     1 | loss: 2.8586717CurrentTrain: epoch  6, batch     0 | loss: 2.6425657CurrentTrain: epoch  6, batch     1 | loss: 3.2306154CurrentTrain: epoch  7, batch     0 | loss: 2.2017219CurrentTrain: epoch  7, batch     1 | loss: 3.1041658CurrentTrain: epoch  8, batch     0 | loss: 2.3950362CurrentTrain: epoch  8, batch     1 | loss: 2.2882094CurrentTrain: epoch  9, batch     0 | loss: 2.1483226CurrentTrain: epoch  9, batch     1 | loss: 2.2905533
Mixup data size:  110

MemoryTrain:  epoch  0, batch     0 | loss: 1.2048780MemoryTrain:  epoch  0, batch     1 | loss: 0.9931356MemoryTrain:  epoch  0, batch     2 | loss: 0.0703028MemoryTrain:  epoch  1, batch     0 | loss: 1.4953475MemoryTrain:  epoch  1, batch     1 | loss: 1.1535648MemoryTrain:  epoch  1, batch     2 | loss: 0.9196567MemoryTrain:  epoch  2, batch     0 | loss: 0.5515251MemoryTrain:  epoch  2, batch     1 | loss: 0.3943561MemoryTrain:  epoch  2, batch     2 | loss: 0.7940326MemoryTrain:  epoch  3, batch     0 | loss: 0.3481809MemoryTrain:  epoch  3, batch     1 | loss: 0.1345926MemoryTrain:  epoch  3, batch     2 | loss: 0.1827183MemoryTrain:  epoch  4, batch     0 | loss: 0.1742038MemoryTrain:  epoch  4, batch     1 | loss: 0.2746416MemoryTrain:  epoch  4, batch     2 | loss: 0.1176836MemoryTrain:  epoch  5, batch     0 | loss: 0.1852321MemoryTrain:  epoch  5, batch     1 | loss: 0.0949766MemoryTrain:  epoch  5, batch     2 | loss: 0.2136105MemoryTrain:  epoch  6, batch     0 | loss: 0.1409229MemoryTrain:  epoch  6, batch     1 | loss: 0.0888515MemoryTrain:  epoch  6, batch     2 | loss: 0.1034224MemoryTrain:  epoch  7, batch     0 | loss: 0.0532545MemoryTrain:  epoch  7, batch     1 | loss: 0.0378064MemoryTrain:  epoch  7, batch     2 | loss: 0.1489301MemoryTrain:  epoch  8, batch     0 | loss: 0.0347783MemoryTrain:  epoch  8, batch     1 | loss: 0.0663576MemoryTrain:  epoch  8, batch     2 | loss: 0.1419405MemoryTrain:  epoch  9, batch     0 | loss: 0.0303330MemoryTrain:  epoch  9, batch     1 | loss: 0.0519524MemoryTrain:  epoch  9, batch     2 | loss: 0.0223230
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 7.9548883CurrentTrain: epoch  0, batch     1 | loss: 6.6203861CurrentTrain: epoch  1, batch     0 | loss: 6.9940205CurrentTrain: epoch  1, batch     1 | loss: 6.4783525CurrentTrain: epoch  2, batch     0 | loss: 5.7693624CurrentTrain: epoch  2, batch     1 | loss: 6.2917490CurrentTrain: epoch  3, batch     0 | loss: 5.7517929CurrentTrain: epoch  3, batch     1 | loss: 5.4528084CurrentTrain: epoch  4, batch     0 | loss: 6.0776610CurrentTrain: epoch  4, batch     1 | loss: 3.9127774CurrentTrain: epoch  5, batch     0 | loss: 5.2031837CurrentTrain: epoch  5, batch     1 | loss: 5.1473327CurrentTrain: epoch  6, batch     0 | loss: 4.3558578CurrentTrain: epoch  6, batch     1 | loss: 5.1471071CurrentTrain: epoch  7, batch     0 | loss: 4.5143499CurrentTrain: epoch  7, batch     1 | loss: 3.6825910CurrentTrain: epoch  8, batch     0 | loss: 3.8296926CurrentTrain: epoch  8, batch     1 | loss: 4.3160157CurrentTrain: epoch  9, batch     0 | loss: 4.0293007CurrentTrain: epoch  9, batch     1 | loss: 3.3031087
Mixup data size:  110
MixupTrain:  epoch  0, batch     0 | loss: 4.2043131MixupTrain:  epoch  0, batch     1 | loss: 3.6826552MixupTrain:  epoch  0, batch     2 | loss: 4.3584607MixupTrain:  epoch  0, batch     3 | loss: 3.8384321MixupTrain:  epoch  0, batch     4 | loss: 3.7509308
MemoryTrain:  epoch  0, batch     0 | loss: 0.7886262MemoryTrain:  epoch  0, batch     1 | loss: 1.2133546MemoryTrain:  epoch  0, batch     2 | loss: 0.9529740MemoryTrain:  epoch  1, batch     0 | loss: 0.7725992MemoryTrain:  epoch  1, batch     1 | loss: 1.5950751MemoryTrain:  epoch  1, batch     2 | loss: 1.8959837MemoryTrain:  epoch  2, batch     0 | loss: 0.7746184MemoryTrain:  epoch  2, batch     1 | loss: 1.0103767MemoryTrain:  epoch  2, batch     2 | loss: 1.0110846MemoryTrain:  epoch  3, batch     0 | loss: 0.6142130MemoryTrain:  epoch  3, batch     1 | loss: 0.8466556MemoryTrain:  epoch  3, batch     2 | loss: 0.3544879MemoryTrain:  epoch  4, batch     0 | loss: 0.6002877MemoryTrain:  epoch  4, batch     1 | loss: 0.5384848MemoryTrain:  epoch  4, batch     2 | loss: 0.3373178MemoryTrain:  epoch  5, batch     0 | loss: 0.6265053MemoryTrain:  epoch  5, batch     1 | loss: 0.6339407MemoryTrain:  epoch  5, batch     2 | loss: 0.5389678MemoryTrain:  epoch  6, batch     0 | loss: 0.5100696MemoryTrain:  epoch  6, batch     1 | loss: 0.5729064MemoryTrain:  epoch  6, batch     2 | loss: 0.1998850MemoryTrain:  epoch  7, batch     0 | loss: 0.3808509MemoryTrain:  epoch  7, batch     1 | loss: 0.7268915MemoryTrain:  epoch  7, batch     2 | loss: 0.0564624MemoryTrain:  epoch  8, batch     0 | loss: 0.5346090MemoryTrain:  epoch  8, batch     1 | loss: 0.4459020MemoryTrain:  epoch  8, batch     2 | loss: 0.0974841MemoryTrain:  epoch  9, batch     0 | loss: 0.5497074MemoryTrain:  epoch  9, batch     1 | loss: 0.5358950MemoryTrain:  epoch  9, batch     2 | loss: 0.0604292
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 48.44%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 46.25%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 44.79%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 43.75%   [EVAL] batch:    7 | acc: 18.75%,  total acc: 40.62%   [EVAL] batch:    8 | acc: 0.00%,  total acc: 36.11%   [EVAL] batch:    9 | acc: 12.50%,  total acc: 33.75%   [EVAL] batch:   10 | acc: 12.50%,  total acc: 31.82%   [EVAL] batch:   11 | acc: 0.00%,  total acc: 29.17%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 30.29%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 30.80%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 12.50%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 16.67%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 15.62%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 17.50%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 18.75%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 26.79%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 35.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 43.06%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 48.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 52.84%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 54.46%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 54.58%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 54.69%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 55.51%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 56.58%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 57.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 59.82%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 61.65%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 63.04%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 66.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 67.31%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 65.28%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 62.95%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 60.78%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 58.75%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 56.85%   [EVAL] batch:   31 | acc: 0.00%,  total acc: 55.08%   [EVAL] batch:   32 | acc: 12.50%,  total acc: 53.79%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 53.68%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 53.93%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 53.82%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 54.90%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 55.92%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 56.89%   [EVAL] batch:   39 | acc: 43.75%,  total acc: 56.56%   [EVAL] batch:   40 | acc: 0.00%,  total acc: 55.18%   [EVAL] batch:   41 | acc: 0.00%,  total acc: 53.87%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 52.62%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 51.42%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 50.28%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 49.32%   [EVAL] batch:   46 | acc: 6.25%,  total acc: 48.40%   [EVAL] batch:   47 | acc: 0.00%,  total acc: 47.40%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 46.68%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 45.88%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 46.20%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 46.75%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 47.05%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 47.57%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 47.84%   [EVAL] batch:   55 | acc: 12.50%,  total acc: 47.21%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 46.38%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 45.58%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 44.81%   [EVAL] batch:   59 | acc: 0.00%,  total acc: 44.06%   [EVAL] batch:   60 | acc: 0.00%,  total acc: 43.34%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 42.64%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 41.96%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 41.31%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 40.67%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 40.06%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 39.93%   [EVAL] batch:   67 | acc: 6.25%,  total acc: 39.43%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 39.49%   [EVAL] batch:   69 | acc: 25.00%,  total acc: 39.29%   [EVAL] batch:   70 | acc: 18.75%,  total acc: 39.00%   [EVAL] batch:   71 | acc: 18.75%,  total acc: 38.72%   [EVAL] batch:   72 | acc: 62.50%,  total acc: 39.04%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 39.61%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 40.25%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 40.95%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 41.31%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 41.27%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 40.74%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 40.23%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 39.74%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 39.25%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 38.78%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 38.32%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 37.87%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 37.43%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 37.00%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 37.64%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 38.27%   [EVAL] batch:   89 | acc: 81.25%,  total acc: 38.75%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 39.22%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 39.67%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 40.32%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 40.96%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 41.58%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 42.19%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 42.78%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 43.37%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 43.94%   [EVAL] batch:   99 | acc: 93.75%,  total acc: 44.44%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 44.99%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 45.53%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 46.06%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 46.57%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 47.02%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 47.52%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 47.96%   [EVAL] batch:  107 | acc: 62.50%,  total acc: 48.09%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 48.34%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 48.69%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 48.82%   [EVAL] batch:  111 | acc: 81.25%,  total acc: 49.11%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 49.17%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 49.18%   [EVAL] batch:  114 | acc: 37.50%,  total acc: 49.08%   [EVAL] batch:  115 | acc: 37.50%,  total acc: 48.98%   [EVAL] batch:  116 | acc: 43.75%,  total acc: 48.93%   [EVAL] batch:  117 | acc: 31.25%,  total acc: 48.78%   [EVAL] batch:  118 | acc: 31.25%,  total acc: 48.63%   [EVAL] batch:  119 | acc: 18.75%,  total acc: 48.39%   [EVAL] batch:  120 | acc: 6.25%,  total acc: 48.04%   [EVAL] batch:  121 | acc: 12.50%,  total acc: 47.75%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 47.41%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 47.08%   [EVAL] batch:  124 | acc: 43.75%,  total acc: 47.05%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 46.92%   
cur_acc_llm:  [0.8768939393939394, 0.9278846153846154, 0.7458333333333333, 0.42329545454545453, 0.90625, 0.8660714285714286, 0.3080357142857143]
his_acc_llm:  [0.8768939393939394, 0.7819444444444444, 0.725, 0.5794753086419753, 0.5555555555555556, 0.53125, 0.46924603174603174]
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 66.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 67.71%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 66.96%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 73.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 74.43%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 73.44%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 73.56%   [EVAL] batch:   13 | acc: 6.25%,  total acc: 68.75%   
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 9.38%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 8.33%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 7.81%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 8.75%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 7.29%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 16.96%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 27.34%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 35.42%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 41.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 46.02%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 49.48%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 49.52%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 48.66%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 50.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 50.39%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 51.84%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 52.43%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 52.63%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 53.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 55.95%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 57.95%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 59.78%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 61.46%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 63.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 64.42%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 65.51%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 66.74%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 67.89%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 68.33%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 69.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 70.12%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 69.32%   [EVAL] batch:   33 | acc: 37.50%,  total acc: 68.38%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 67.86%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 67.19%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 66.39%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 65.46%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 64.58%   [EVAL] batch:   39 | acc: 12.50%,  total acc: 63.28%   [EVAL] batch:   40 | acc: 12.50%,  total acc: 62.04%   [EVAL] batch:   41 | acc: 6.25%,  total acc: 60.71%   [EVAL] batch:   42 | acc: 18.75%,  total acc: 59.74%   [EVAL] batch:   43 | acc: 6.25%,  total acc: 58.52%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 57.22%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 55.98%   [EVAL] batch:   46 | acc: 0.00%,  total acc: 54.79%   [EVAL] batch:   47 | acc: 0.00%,  total acc: 53.65%   [EVAL] batch:   48 | acc: 6.25%,  total acc: 52.68%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 51.62%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 50.61%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 49.76%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 48.82%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 49.07%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 49.20%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 48.33%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 47.48%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 46.66%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 45.87%   [EVAL] batch:   59 | acc: 6.25%,  total acc: 45.21%   [EVAL] batch:   60 | acc: 6.25%,  total acc: 44.57%   [EVAL] batch:   61 | acc: 6.25%,  total acc: 43.95%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 43.35%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 42.68%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 42.12%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 41.48%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 41.04%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 40.44%   [EVAL] batch:   68 | acc: 12.50%,  total acc: 40.04%   [EVAL] batch:   69 | acc: 6.25%,  total acc: 39.55%   [EVAL] batch:   70 | acc: 12.50%,  total acc: 39.17%   [EVAL] batch:   71 | acc: 6.25%,  total acc: 38.72%   [EVAL] batch:   72 | acc: 37.50%,  total acc: 38.70%   [EVAL] batch:   73 | acc: 31.25%,  total acc: 38.60%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 38.83%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 38.73%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 38.56%   [EVAL] batch:   77 | acc: 12.50%,  total acc: 38.22%   [EVAL] batch:   78 | acc: 6.25%,  total acc: 37.82%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 37.34%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 36.96%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 36.51%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 36.07%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 35.64%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 35.22%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 34.88%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 34.48%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 35.01%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 35.53%   [EVAL] batch:   89 | acc: 37.50%,  total acc: 35.56%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 35.16%   [EVAL] batch:   91 | acc: 18.75%,  total acc: 34.99%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 35.69%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 36.37%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 37.04%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 37.70%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 38.34%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 38.97%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 39.58%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 40.19%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 40.78%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 41.36%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 41.93%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 42.49%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 43.04%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 43.51%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 43.98%   [EVAL] batch:  107 | acc: 43.75%,  total acc: 43.98%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 44.27%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 44.66%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 44.65%   [EVAL] batch:  111 | acc: 75.00%,  total acc: 44.92%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 45.08%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 45.45%   [EVAL] batch:  114 | acc: 62.50%,  total acc: 45.60%   [EVAL] batch:  115 | acc: 56.25%,  total acc: 45.69%   [EVAL] batch:  116 | acc: 68.75%,  total acc: 45.89%   [EVAL] batch:  117 | acc: 68.75%,  total acc: 46.08%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 46.22%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 46.61%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 46.95%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 47.28%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 47.51%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 47.68%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 47.80%   [EVAL] batch:  125 | acc: 6.25%,  total acc: 47.47%   
cur_acc:  ['0.8712', '0.7163', '0.5917', '0.4176', '0.8090', '0.8750', '0.6875']
his_acc:  ['0.8712', '0.8347', '0.7073', '0.6142', '0.5524', '0.5084', '0.4747']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 5.7765479CurrentTrain: epoch  0, batch     1 | loss: 6.4219623CurrentTrain: epoch  1, batch     0 | loss: 4.7770786CurrentTrain: epoch  1, batch     1 | loss: 4.6211433CurrentTrain: epoch  2, batch     0 | loss: 4.4446440CurrentTrain: epoch  2, batch     1 | loss: 2.9857373CurrentTrain: epoch  3, batch     0 | loss: 2.7882102CurrentTrain: epoch  3, batch     1 | loss: 3.9234679CurrentTrain: epoch  4, batch     0 | loss: 3.6586475CurrentTrain: epoch  4, batch     1 | loss: 1.9514012CurrentTrain: epoch  5, batch     0 | loss: 2.5292478CurrentTrain: epoch  5, batch     1 | loss: 2.9921231CurrentTrain: epoch  6, batch     0 | loss: 2.6157401CurrentTrain: epoch  6, batch     1 | loss: 2.2816064CurrentTrain: epoch  7, batch     0 | loss: 2.4260497CurrentTrain: epoch  7, batch     1 | loss: 1.8963476CurrentTrain: epoch  8, batch     0 | loss: 2.0404270CurrentTrain: epoch  8, batch     1 | loss: 1.9796491CurrentTrain: epoch  9, batch     0 | loss: 2.0599532CurrentTrain: epoch  9, batch     1 | loss: 1.7135408
Mixup data size:  119
MixupTrain:  epoch  0, batch     0 | loss: 3.4472379MixupTrain:  epoch  0, batch     1 | loss: 3.8065209MixupTrain:  epoch  0, batch     2 | loss: 3.2483548MixupTrain:  epoch  0, batch     3 | loss: 3.7294641MixupTrain:  epoch  0, batch     4 | loss: 2.9618250MixupTrain:  epoch  0, batch     5 | loss: 3.5681084MixupTrain:  epoch  0, batch     6 | loss: 3.3104827MixupTrain:  epoch  0, batch     7 | loss: 2.7309129
MemoryTrain:  epoch  0, batch     0 | loss: 0.3498984MemoryTrain:  epoch  0, batch     1 | loss: 0.6042835MemoryTrain:  epoch  0, batch     2 | loss: 0.2216067MemoryTrain:  epoch  1, batch     0 | loss: 1.0439981MemoryTrain:  epoch  1, batch     1 | loss: 0.8010128MemoryTrain:  epoch  1, batch     2 | loss: 0.6505536MemoryTrain:  epoch  2, batch     0 | loss: 0.2230665MemoryTrain:  epoch  2, batch     1 | loss: 0.4471246MemoryTrain:  epoch  2, batch     2 | loss: 0.5015718MemoryTrain:  epoch  3, batch     0 | loss: 0.1882708MemoryTrain:  epoch  3, batch     1 | loss: 0.2473169MemoryTrain:  epoch  3, batch     2 | loss: 0.0453584MemoryTrain:  epoch  4, batch     0 | loss: 0.1635573MemoryTrain:  epoch  4, batch     1 | loss: 0.0769301MemoryTrain:  epoch  4, batch     2 | loss: 0.0401512MemoryTrain:  epoch  5, batch     0 | loss: 0.0366425MemoryTrain:  epoch  5, batch     1 | loss: 0.1213213MemoryTrain:  epoch  5, batch     2 | loss: 0.1314189MemoryTrain:  epoch  6, batch     0 | loss: 0.0668078MemoryTrain:  epoch  6, batch     1 | loss: 0.0568373MemoryTrain:  epoch  6, batch     2 | loss: 0.0264663MemoryTrain:  epoch  7, batch     0 | loss: 0.0492681MemoryTrain:  epoch  7, batch     1 | loss: 0.0380078MemoryTrain:  epoch  7, batch     2 | loss: 0.0348335MemoryTrain:  epoch  8, batch     0 | loss: 0.0247869MemoryTrain:  epoch  8, batch     1 | loss: 0.0471455MemoryTrain:  epoch  8, batch     2 | loss: 0.0281307MemoryTrain:  epoch  9, batch     0 | loss: 0.0180887MemoryTrain:  epoch  9, batch     1 | loss: 0.0431040MemoryTrain:  epoch  9, batch     2 | loss: 0.0225446
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 6.4895153CurrentTrain: epoch  0, batch     1 | loss: 7.9369164CurrentTrain: epoch  1, batch     0 | loss: 6.8065791CurrentTrain: epoch  1, batch     1 | loss: 4.8058419CurrentTrain: epoch  2, batch     0 | loss: 5.2501664CurrentTrain: epoch  2, batch     1 | loss: 5.4995666CurrentTrain: epoch  3, batch     0 | loss: 4.9979839CurrentTrain: epoch  3, batch     1 | loss: 4.2454023CurrentTrain: epoch  4, batch     0 | loss: 4.0671849CurrentTrain: epoch  4, batch     1 | loss: 4.3592582CurrentTrain: epoch  5, batch     0 | loss: 4.1767607CurrentTrain: epoch  5, batch     1 | loss: 3.1959391CurrentTrain: epoch  6, batch     0 | loss: 3.6021729CurrentTrain: epoch  6, batch     1 | loss: 3.6208272CurrentTrain: epoch  7, batch     0 | loss: 3.4619532CurrentTrain: epoch  7, batch     1 | loss: 3.0589709CurrentTrain: epoch  8, batch     0 | loss: 3.0349603CurrentTrain: epoch  8, batch     1 | loss: 2.9049411CurrentTrain: epoch  9, batch     0 | loss: 2.8397408CurrentTrain: epoch  9, batch     1 | loss: 2.7686851
Mixup data size:  121
MixupTrain:  epoch  0, batch     5 | loss: 3.1209724MixupTrain:  epoch  0, batch     7 | loss: 2.9043660
MemoryTrain:  epoch  0, batch     0 | loss: 0.8277469MemoryTrain:  epoch  0, batch     1 | loss: 0.8526782MemoryTrain:  epoch  0, batch     2 | loss: 0.8607327MemoryTrain:  epoch  1, batch     0 | loss: 1.2314717MemoryTrain:  epoch  1, batch     1 | loss: 0.8170065MemoryTrain:  epoch  1, batch     2 | loss: 1.6835382MemoryTrain:  epoch  2, batch     0 | loss: 0.9821789MemoryTrain:  epoch  2, batch     1 | loss: 0.9423021MemoryTrain:  epoch  2, batch     2 | loss: 0.7015464MemoryTrain:  epoch  3, batch     0 | loss: 0.7873476MemoryTrain:  epoch  3, batch     1 | loss: 0.8643919MemoryTrain:  epoch  3, batch     2 | loss: 0.5791996MemoryTrain:  epoch  4, batch     0 | loss: 0.6862375MemoryTrain:  epoch  4, batch     1 | loss: 0.5677977MemoryTrain:  epoch  4, batch     2 | loss: 0.5202029MemoryTrain:  epoch  5, batch     0 | loss: 0.6402253MemoryTrain:  epoch  5, batch     1 | loss: 0.6137304MemoryTrain:  epoch  5, batch     2 | loss: 0.1888041MemoryTrain:  epoch  6, batch     0 | loss: 0.4431544MemoryTrain:  epoch  6, batch     1 | loss: 0.4113765MemoryTrain:  epoch  6, batch     2 | loss: 0.3989419MemoryTrain:  epoch  7, batch     0 | loss: 0.4601880MemoryTrain:  epoch  7, batch     1 | loss: 0.3553526MemoryTrain:  epoch  7, batch     2 | loss: 0.2814633MemoryTrain:  epoch  8, batch     0 | loss: 0.3951297MemoryTrain:  epoch  8, batch     1 | loss: 0.4436491MemoryTrain:  epoch  8, batch     2 | loss: 0.3679466MemoryTrain:  epoch  9, batch     0 | loss: 0.4305881MemoryTrain:  epoch  9, batch     1 | loss: 0.4107803MemoryTrain:  epoch  9, batch     2 | loss: 0.1452562
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 60.94%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 50.00%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 44.79%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 41.07%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 35.94%   
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 29.17%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 29.69%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 32.50%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 32.29%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 36.61%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 44.53%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 49.31%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 53.12%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 54.55%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 56.77%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 55.77%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 53.57%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 53.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 53.91%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 54.78%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 55.56%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 57.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 59.23%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 61.08%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 65.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 66.83%   [EVAL] batch:   26 | acc: 18.75%,  total acc: 65.05%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 62.72%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 60.56%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 58.54%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 56.65%   [EVAL] batch:   31 | acc: 0.00%,  total acc: 54.88%   [EVAL] batch:   32 | acc: 12.50%,  total acc: 53.60%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 53.49%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 53.21%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 52.95%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 54.05%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 55.10%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 56.09%   [EVAL] batch:   39 | acc: 37.50%,  total acc: 55.62%   [EVAL] batch:   40 | acc: 0.00%,  total acc: 54.27%   [EVAL] batch:   41 | acc: 0.00%,  total acc: 52.98%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 51.74%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 50.57%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 49.44%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 48.64%   [EVAL] batch:   46 | acc: 6.25%,  total acc: 47.74%   [EVAL] batch:   47 | acc: 6.25%,  total acc: 46.88%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 46.17%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 45.62%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 45.96%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 46.63%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 46.93%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 47.45%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 47.73%   [EVAL] batch:   55 | acc: 12.50%,  total acc: 47.10%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 46.27%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 45.47%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 44.70%   [EVAL] batch:   59 | acc: 0.00%,  total acc: 43.96%   [EVAL] batch:   60 | acc: 0.00%,  total acc: 43.24%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 42.54%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 41.87%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 41.21%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 40.58%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 39.96%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 39.93%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 39.80%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 39.95%   [EVAL] batch:   69 | acc: 43.75%,  total acc: 40.00%   [EVAL] batch:   70 | acc: 31.25%,  total acc: 39.88%   [EVAL] batch:   71 | acc: 37.50%,  total acc: 39.84%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 40.24%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 41.05%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 41.67%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 42.35%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 42.78%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 42.71%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 42.17%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 41.64%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 41.13%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 40.62%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 40.14%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 39.66%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 39.19%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 38.74%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 38.29%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 38.92%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 39.54%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 39.93%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 40.38%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 40.83%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 41.47%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 42.09%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 42.70%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 43.29%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 43.88%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 44.45%   [EVAL] batch:   98 | acc: 6.25%,  total acc: 44.07%   [EVAL] batch:   99 | acc: 0.00%,  total acc: 43.62%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 43.81%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 44.36%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 44.90%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 45.43%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 45.89%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 46.40%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 46.85%   [EVAL] batch:  107 | acc: 62.50%,  total acc: 46.99%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 47.13%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 47.50%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 47.64%   [EVAL] batch:  111 | acc: 75.00%,  total acc: 47.88%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 48.12%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 48.14%   [EVAL] batch:  114 | acc: 31.25%,  total acc: 47.99%   [EVAL] batch:  115 | acc: 31.25%,  total acc: 47.84%   [EVAL] batch:  116 | acc: 31.25%,  total acc: 47.70%   [EVAL] batch:  117 | acc: 12.50%,  total acc: 47.40%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 47.58%   [EVAL] batch:  119 | acc: 68.75%,  total acc: 47.76%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 48.04%   [EVAL] batch:  121 | acc: 68.75%,  total acc: 48.21%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 48.32%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 48.54%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 48.75%   [EVAL] batch:  125 | acc: 50.00%,  total acc: 48.76%   [EVAL] batch:  126 | acc: 81.25%,  total acc: 49.02%   [EVAL] batch:  127 | acc: 75.00%,  total acc: 49.22%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 49.22%   [EVAL] batch:  129 | acc: 18.75%,  total acc: 48.99%   [EVAL] batch:  130 | acc: 12.50%,  total acc: 48.71%   [EVAL] batch:  131 | acc: 25.00%,  total acc: 48.53%   [EVAL] batch:  132 | acc: 6.25%,  total acc: 48.21%   
cur_acc_llm:  [0.8768939393939394, 0.9278846153846154, 0.7458333333333333, 0.42329545454545453, 0.90625, 0.8660714285714286, 0.3080357142857143, 0.359375]
his_acc_llm:  [0.8768939393939394, 0.7819444444444444, 0.725, 0.5794753086419753, 0.5555555555555556, 0.53125, 0.46924603174603174, 0.48214285714285715]
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 65.62%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 61.61%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 54.69%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 3.12%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 4.17%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 3.12%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 2.50%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 2.08%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 8.04%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 14.84%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 22.22%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 27.50%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 28.41%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 31.25%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 30.80%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 33.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 35.16%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 37.50%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 38.89%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 40.13%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 42.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 44.94%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 47.44%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 49.73%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 51.82%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 53.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 55.53%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 56.94%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 58.48%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 59.91%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 60.62%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 61.69%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 62.89%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 61.58%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 61.25%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 60.76%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 60.14%   [EVAL] batch:   37 | acc: 37.50%,  total acc: 59.54%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 58.65%   [EVAL] batch:   39 | acc: 25.00%,  total acc: 57.81%   [EVAL] batch:   40 | acc: 18.75%,  total acc: 56.86%   [EVAL] batch:   41 | acc: 31.25%,  total acc: 56.25%   [EVAL] batch:   42 | acc: 43.75%,  total acc: 55.96%   [EVAL] batch:   43 | acc: 18.75%,  total acc: 55.11%   [EVAL] batch:   44 | acc: 6.25%,  total acc: 54.03%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 52.85%   [EVAL] batch:   46 | acc: 0.00%,  total acc: 51.73%   [EVAL] batch:   47 | acc: 6.25%,  total acc: 50.78%   [EVAL] batch:   48 | acc: 6.25%,  total acc: 49.87%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 48.88%   [EVAL] batch:   50 | acc: 12.50%,  total acc: 48.16%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 48.20%   [EVAL] batch:   52 | acc: 25.00%,  total acc: 47.76%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 48.15%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 48.30%   [EVAL] batch:   55 | acc: 6.25%,  total acc: 47.54%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 46.71%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 45.91%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 45.13%   [EVAL] batch:   59 | acc: 0.00%,  total acc: 44.38%   [EVAL] batch:   60 | acc: 0.00%,  total acc: 43.65%   [EVAL] batch:   61 | acc: 12.50%,  total acc: 43.15%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 42.46%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 41.80%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 41.25%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 40.62%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 40.39%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 39.98%   [EVAL] batch:   68 | acc: 18.75%,  total acc: 39.67%   [EVAL] batch:   69 | acc: 12.50%,  total acc: 39.29%   [EVAL] batch:   70 | acc: 31.25%,  total acc: 39.17%   [EVAL] batch:   71 | acc: 6.25%,  total acc: 38.72%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 38.53%   [EVAL] batch:   73 | acc: 31.25%,  total acc: 38.43%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 38.75%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 38.65%   [EVAL] batch:   76 | acc: 18.75%,  total acc: 38.39%   [EVAL] batch:   77 | acc: 12.50%,  total acc: 38.06%   [EVAL] batch:   78 | acc: 6.25%,  total acc: 37.66%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 37.19%   [EVAL] batch:   80 | acc: 12.50%,  total acc: 36.88%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 36.43%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 35.99%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 35.57%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 35.15%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 34.81%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 34.41%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 34.94%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 35.39%   [EVAL] batch:   89 | acc: 6.25%,  total acc: 35.07%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 34.68%   [EVAL] batch:   91 | acc: 25.00%,  total acc: 34.58%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 35.28%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 35.97%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 36.64%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 37.30%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 37.95%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 38.58%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 39.20%   [EVAL] batch:   99 | acc: 93.75%,  total acc: 39.75%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 40.22%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 40.81%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 41.38%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 41.95%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 42.50%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 42.98%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 43.46%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 43.23%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 43.06%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 43.47%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 43.47%   [EVAL] batch:  111 | acc: 75.00%,  total acc: 43.75%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 43.86%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 43.91%   [EVAL] batch:  114 | acc: 37.50%,  total acc: 43.86%   [EVAL] batch:  115 | acc: 18.75%,  total acc: 43.64%   [EVAL] batch:  116 | acc: 31.25%,  total acc: 43.54%   [EVAL] batch:  117 | acc: 37.50%,  total acc: 43.49%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 43.59%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 44.01%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 44.37%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 44.72%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 44.87%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 45.06%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 45.20%   [EVAL] batch:  125 | acc: 50.00%,  total acc: 45.24%   [EVAL] batch:  126 | acc: 93.75%,  total acc: 45.62%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 45.90%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 46.08%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 46.15%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 46.09%   [EVAL] batch:  131 | acc: 37.50%,  total acc: 46.02%   [EVAL] batch:  132 | acc: 18.75%,  total acc: 45.82%   
cur_acc:  ['0.8712', '0.7163', '0.5917', '0.4176', '0.8090', '0.8750', '0.6875', '0.5469']
his_acc:  ['0.8712', '0.8347', '0.7073', '0.6142', '0.5524', '0.5084', '0.4747', '0.4582']
--------Round  5
seed:  600
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 2 0 1 6 3 4 5]
prepared data!
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 13.2699137CurrentTrain: epoch  0, batch     1 | loss: 13.0349169CurrentTrain: epoch  0, batch     2 | loss: 12.6073008CurrentTrain: epoch  0, batch     3 | loss: 11.8539896CurrentTrain: epoch  0, batch     4 | loss: 11.2079945CurrentTrain: epoch  0, batch     5 | loss: 11.8491402CurrentTrain: epoch  0, batch     6 | loss: 11.4090023CurrentTrain: epoch  0, batch     7 | loss: 11.1886883CurrentTrain: epoch  0, batch     8 | loss: 11.4154663CurrentTrain: epoch  0, batch     9 | loss: 11.3785753CurrentTrain: epoch  0, batch    10 | loss: 11.3978901CurrentTrain: epoch  0, batch    11 | loss: 11.3255196CurrentTrain: epoch  0, batch    12 | loss: 11.1559601CurrentTrain: epoch  0, batch    13 | loss: 11.3144436CurrentTrain: epoch  0, batch    14 | loss: 11.2481680CurrentTrain: epoch  0, batch    15 | loss: 11.1330242CurrentTrain: epoch  0, batch    16 | loss: 11.7222233CurrentTrain: epoch  0, batch    17 | loss: 11.1412373CurrentTrain: epoch  0, batch    18 | loss: 11.3631248CurrentTrain: epoch  0, batch    19 | loss: 11.9036522CurrentTrain: epoch  0, batch    20 | loss: 11.8856964CurrentTrain: epoch  0, batch    21 | loss: 11.4772911CurrentTrain: epoch  0, batch    22 | loss: 11.4085913CurrentTrain: epoch  0, batch    23 | loss: 11.0210171CurrentTrain: epoch  0, batch    24 | loss: 11.4919405CurrentTrain: epoch  0, batch    25 | loss: 10.8387871CurrentTrain: epoch  0, batch    26 | loss: 11.2882843CurrentTrain: epoch  0, batch    27 | loss: 11.0961685CurrentTrain: epoch  0, batch    28 | loss: 10.9203043CurrentTrain: epoch  0, batch    29 | loss: 11.1676865CurrentTrain: epoch  0, batch    30 | loss: 10.9225330CurrentTrain: epoch  0, batch    31 | loss: 10.8831654CurrentTrain: epoch  0, batch    32 | loss: 11.0527344CurrentTrain: epoch  0, batch    33 | loss: 11.2414694CurrentTrain: epoch  0, batch    34 | loss: 11.0515804CurrentTrain: epoch  0, batch    35 | loss: 11.2132215CurrentTrain: epoch  0, batch    36 | loss: 11.1434345CurrentTrain: epoch  0, batch    37 | loss: 11.5819588CurrentTrain: epoch  1, batch     0 | loss: 11.0186729CurrentTrain: epoch  1, batch     1 | loss: 10.9889679CurrentTrain: epoch  1, batch     2 | loss: 10.9766836CurrentTrain: epoch  1, batch     3 | loss: 11.3056889CurrentTrain: epoch  1, batch     4 | loss: 11.0735960CurrentTrain: epoch  1, batch     5 | loss: 11.2124615CurrentTrain: epoch  1, batch     6 | loss: 11.3634281CurrentTrain: epoch  1, batch     7 | loss: 10.9519882CurrentTrain: epoch  1, batch     8 | loss: 11.2270584CurrentTrain: epoch  1, batch     9 | loss: 11.2332649CurrentTrain: epoch  1, batch    10 | loss: 11.0244465CurrentTrain: epoch  1, batch    11 | loss: 10.9847393CurrentTrain: epoch  1, batch    12 | loss: 11.4977436CurrentTrain: epoch  1, batch    13 | loss: 11.0555496CurrentTrain: epoch  1, batch    14 | loss: 11.3819571CurrentTrain: epoch  1, batch    15 | loss: 11.1643906CurrentTrain: epoch  1, batch    16 | loss: 10.7147923CurrentTrain: epoch  1, batch    17 | loss: 10.6679020CurrentTrain: epoch  1, batch    18 | loss: 10.5748405CurrentTrain: epoch  1, batch    19 | loss: 10.6852741CurrentTrain: epoch  1, batch    20 | loss: 10.7043524CurrentTrain: epoch  1, batch    21 | loss: 10.5820980CurrentTrain: epoch  1, batch    22 | loss: 10.5978088CurrentTrain: epoch  1, batch    23 | loss: 10.9671478CurrentTrain: epoch  1, batch    24 | loss: 10.9833488CurrentTrain: epoch  1, batch    25 | loss: 10.7622929CurrentTrain: epoch  1, batch    26 | loss: 10.7374687CurrentTrain: epoch  1, batch    27 | loss: 10.9292850CurrentTrain: epoch  1, batch    28 | loss: 10.6947899CurrentTrain: epoch  1, batch    29 | loss: 10.7696056CurrentTrain: epoch  1, batch    30 | loss: 10.6285639CurrentTrain: epoch  1, batch    31 | loss: 10.4252119CurrentTrain: epoch  1, batch    32 | loss: 10.6772470CurrentTrain: epoch  1, batch    33 | loss: 10.5843010CurrentTrain: epoch  1, batch    34 | loss: 10.4906864CurrentTrain: epoch  1, batch    35 | loss: 10.3648300CurrentTrain: epoch  1, batch    36 | loss: 10.2615328CurrentTrain: epoch  1, batch    37 | loss: 10.3219070CurrentTrain: epoch  2, batch     0 | loss: 10.3448448CurrentTrain: epoch  2, batch     1 | loss: 10.4710999CurrentTrain: epoch  2, batch     2 | loss: 10.4818401CurrentTrain: epoch  2, batch     3 | loss: 10.6899853CurrentTrain: epoch  2, batch     4 | loss: 10.3674946CurrentTrain: epoch  2, batch     5 | loss: 10.5064764CurrentTrain: epoch  2, batch     6 | loss: 10.3505373CurrentTrain: epoch  2, batch     7 | loss: 10.5242233CurrentTrain: epoch  2, batch     8 | loss: 10.5228643CurrentTrain: epoch  2, batch     9 | loss: 10.5512524CurrentTrain: epoch  2, batch    10 | loss: 10.7242832CurrentTrain: epoch  2, batch    11 | loss: 10.3197384CurrentTrain: epoch  2, batch    12 | loss: 9.9815292CurrentTrain: epoch  2, batch    13 | loss: 10.0698137CurrentTrain: epoch  2, batch    14 | loss: 10.4987335CurrentTrain: epoch  2, batch    15 | loss: 10.2592087CurrentTrain: epoch  2, batch    16 | loss: 9.9872532CurrentTrain: epoch  2, batch    17 | loss: 10.0964308CurrentTrain: epoch  2, batch    18 | loss: 9.9945526CurrentTrain: epoch  2, batch    19 | loss: 10.4417019CurrentTrain: epoch  2, batch    20 | loss: 10.2193947CurrentTrain: epoch  2, batch    21 | loss: 10.1088562CurrentTrain: epoch  2, batch    22 | loss: 9.9592056CurrentTrain: epoch  2, batch    23 | loss: 9.7837181CurrentTrain: epoch  2, batch    24 | loss: 10.0074549CurrentTrain: epoch  2, batch    25 | loss: 9.6991539CurrentTrain: epoch  2, batch    26 | loss: 9.6103859CurrentTrain: epoch  2, batch    27 | loss: 9.8149920CurrentTrain: epoch  2, batch    28 | loss: 9.8460636CurrentTrain: epoch  2, batch    29 | loss: 9.3233614CurrentTrain: epoch  2, batch    30 | loss: 9.9296589CurrentTrain: epoch  2, batch    31 | loss: 9.9129562CurrentTrain: epoch  2, batch    32 | loss: 9.5902834CurrentTrain: epoch  2, batch    33 | loss: 9.4747543CurrentTrain: epoch  2, batch    34 | loss: 9.2415104CurrentTrain: epoch  2, batch    35 | loss: 9.2825966CurrentTrain: epoch  2, batch    36 | loss: 9.7286940CurrentTrain: epoch  2, batch    37 | loss: 9.3613043CurrentTrain: epoch  3, batch     0 | loss: 9.6664209CurrentTrain: epoch  3, batch     1 | loss: 9.0133438CurrentTrain: epoch  3, batch     2 | loss: 9.0404196CurrentTrain: epoch  3, batch     3 | loss: 9.0257978CurrentTrain: epoch  3, batch     4 | loss: 8.7165890CurrentTrain: epoch  3, batch     5 | loss: 8.8687572CurrentTrain: epoch  3, batch     6 | loss: 8.7968292CurrentTrain: epoch  3, batch     7 | loss: 8.3674545CurrentTrain: epoch  3, batch     8 | loss: 8.8142929CurrentTrain: epoch  3, batch     9 | loss: 9.0219336CurrentTrain: epoch  3, batch    10 | loss: 8.8273001CurrentTrain: epoch  3, batch    11 | loss: 8.7197104CurrentTrain: epoch  3, batch    12 | loss: 9.1104259CurrentTrain: epoch  3, batch    13 | loss: 8.9001932CurrentTrain: epoch  3, batch    14 | loss: 8.8926811CurrentTrain: epoch  3, batch    15 | loss: 9.0416698CurrentTrain: epoch  3, batch    16 | loss: 8.2999325CurrentTrain: epoch  3, batch    17 | loss: 9.0239239CurrentTrain: epoch  3, batch    18 | loss: 9.0961723CurrentTrain: epoch  3, batch    19 | loss: 8.3509312CurrentTrain: epoch  3, batch    20 | loss: 8.1085224CurrentTrain: epoch  3, batch    21 | loss: 7.8954830CurrentTrain: epoch  3, batch    22 | loss: 7.8019357CurrentTrain: epoch  3, batch    23 | loss: 8.2089758CurrentTrain: epoch  3, batch    24 | loss: 8.2021713CurrentTrain: epoch  3, batch    25 | loss: 7.8548474CurrentTrain: epoch  3, batch    26 | loss: 8.5154247CurrentTrain: epoch  3, batch    27 | loss: 7.8120880CurrentTrain: epoch  3, batch    28 | loss: 7.0001059CurrentTrain: epoch  3, batch    29 | loss: 7.7111697CurrentTrain: epoch  3, batch    30 | loss: 8.1202164CurrentTrain: epoch  3, batch    31 | loss: 7.4989500CurrentTrain: epoch  3, batch    32 | loss: 7.3348341CurrentTrain: epoch  3, batch    33 | loss: 8.2596989CurrentTrain: epoch  3, batch    34 | loss: 8.2230587CurrentTrain: epoch  3, batch    35 | loss: 7.5905008CurrentTrain: epoch  3, batch    36 | loss: 7.4141803CurrentTrain: epoch  3, batch    37 | loss: 7.9046302CurrentTrain: epoch  4, batch     0 | loss: 7.7255883CurrentTrain: epoch  4, batch     1 | loss: 7.4876919CurrentTrain: epoch  4, batch     2 | loss: 7.4564123CurrentTrain: epoch  4, batch     3 | loss: 6.8331828CurrentTrain: epoch  4, batch     4 | loss: 6.9241829CurrentTrain: epoch  4, batch     5 | loss: 7.1142883CurrentTrain: epoch  4, batch     6 | loss: 7.0730257CurrentTrain: epoch  4, batch     7 | loss: 7.5535903CurrentTrain: epoch  4, batch     8 | loss: 7.5491548CurrentTrain: epoch  4, batch     9 | loss: 6.9534054CurrentTrain: epoch  4, batch    10 | loss: 7.1300316CurrentTrain: epoch  4, batch    11 | loss: 7.1838989CurrentTrain: epoch  4, batch    12 | loss: 6.6806788CurrentTrain: epoch  4, batch    13 | loss: 7.4807386CurrentTrain: epoch  4, batch    14 | loss: 8.3969774CurrentTrain: epoch  4, batch    15 | loss: 7.6988802CurrentTrain: epoch  4, batch    16 | loss: 7.8448529CurrentTrain: epoch  4, batch    17 | loss: 7.1733723CurrentTrain: epoch  4, batch    18 | loss: 7.4184823CurrentTrain: epoch  4, batch    19 | loss: 7.3227491CurrentTrain: epoch  4, batch    20 | loss: 7.2597733CurrentTrain: epoch  4, batch    21 | loss: 7.5224633CurrentTrain: epoch  4, batch    22 | loss: 7.4188223CurrentTrain: epoch  4, batch    23 | loss: 6.7600303CurrentTrain: epoch  4, batch    24 | loss: 6.8451204CurrentTrain: epoch  4, batch    25 | loss: 6.5537224CurrentTrain: epoch  4, batch    26 | loss: 6.4524994CurrentTrain: epoch  4, batch    27 | loss: 6.2401280CurrentTrain: epoch  4, batch    28 | loss: 7.5600238CurrentTrain: epoch  4, batch    29 | loss: 6.4155598CurrentTrain: epoch  4, batch    30 | loss: 6.8843026CurrentTrain: epoch  4, batch    31 | loss: 6.5907135CurrentTrain: epoch  4, batch    32 | loss: 6.1270676CurrentTrain: epoch  4, batch    33 | loss: 6.4113860CurrentTrain: epoch  4, batch    34 | loss: 7.2078505CurrentTrain: epoch  4, batch    35 | loss: 7.9337058CurrentTrain: epoch  4, batch    36 | loss: 7.8324757CurrentTrain: epoch  4, batch    37 | loss: 7.1302443CurrentTrain: epoch  5, batch     0 | loss: 7.3006520CurrentTrain: epoch  5, batch     1 | loss: 6.0812531CurrentTrain: epoch  5, batch     2 | loss: 6.6118288CurrentTrain: epoch  5, batch     3 | loss: 7.9261007CurrentTrain: epoch  5, batch     4 | loss: 7.0659914CurrentTrain: epoch  5, batch     5 | loss: 7.4706755CurrentTrain: epoch  5, batch     6 | loss: 7.0167427CurrentTrain: epoch  5, batch     7 | loss: 6.7195520CurrentTrain: epoch  5, batch     8 | loss: 6.6689453CurrentTrain: epoch  5, batch     9 | loss: 5.8334122CurrentTrain: epoch  5, batch    10 | loss: 5.8594561CurrentTrain: epoch  5, batch    11 | loss: 6.2385664CurrentTrain: epoch  5, batch    12 | loss: 6.2378139CurrentTrain: epoch  5, batch    13 | loss: 6.2312632CurrentTrain: epoch  5, batch    14 | loss: 6.5413303CurrentTrain: epoch  5, batch    15 | loss: 6.4867382CurrentTrain: epoch  5, batch    16 | loss: 6.7018600CurrentTrain: epoch  5, batch    17 | loss: 6.8860312CurrentTrain: epoch  5, batch    18 | loss: 6.1066408CurrentTrain: epoch  5, batch    19 | loss: 6.7492704CurrentTrain: epoch  5, batch    20 | loss: 6.7829742CurrentTrain: epoch  5, batch    21 | loss: 5.9791975CurrentTrain: epoch  5, batch    22 | loss: 6.9360781CurrentTrain: epoch  5, batch    23 | loss: 6.8026676CurrentTrain: epoch  5, batch    24 | loss: 7.0092287CurrentTrain: epoch  5, batch    25 | loss: 6.1244774CurrentTrain: epoch  5, batch    26 | loss: 5.9877181CurrentTrain: epoch  5, batch    27 | loss: 6.3135843CurrentTrain: epoch  5, batch    28 | loss: 6.5764809CurrentTrain: epoch  5, batch    29 | loss: 6.3734837CurrentTrain: epoch  5, batch    30 | loss: 6.2585239CurrentTrain: epoch  5, batch    31 | loss: 6.6171908CurrentTrain: epoch  5, batch    32 | loss: 5.7739072CurrentTrain: epoch  5, batch    33 | loss: 6.5326853CurrentTrain: epoch  5, batch    34 | loss: 6.1406746CurrentTrain: epoch  5, batch    35 | loss: 6.5829792CurrentTrain: epoch  5, batch    36 | loss: 6.2385893CurrentTrain: epoch  5, batch    37 | loss: 6.6765842CurrentTrain: epoch  6, batch     0 | loss: 6.6751051CurrentTrain: epoch  6, batch     1 | loss: 5.8952942CurrentTrain: epoch  6, batch     2 | loss: 6.3156776CurrentTrain: epoch  6, batch     3 | loss: 6.3552198CurrentTrain: epoch  6, batch     4 | loss: 6.1336279CurrentTrain: epoch  6, batch     5 | loss: 6.2621546CurrentTrain: epoch  6, batch     6 | loss: 6.2624831CurrentTrain: epoch  6, batch     7 | loss: 6.4456339CurrentTrain: epoch  6, batch     8 | loss: 6.0488358CurrentTrain: epoch  6, batch     9 | loss: 6.0439939CurrentTrain: epoch  6, batch    10 | loss: 6.3296652CurrentTrain: epoch  6, batch    11 | loss: 5.8923349CurrentTrain: epoch  6, batch    12 | loss: 5.8737440CurrentTrain: epoch  6, batch    13 | loss: 5.6309524CurrentTrain: epoch  6, batch    14 | loss: 5.9759035CurrentTrain: epoch  6, batch    15 | loss: 5.4874516CurrentTrain: epoch  6, batch    16 | loss: 5.2914362CurrentTrain: epoch  6, batch    17 | loss: 5.4816618CurrentTrain: epoch  6, batch    18 | loss: 5.9985824CurrentTrain: epoch  6, batch    19 | loss: 5.5620365CurrentTrain: epoch  6, batch    20 | loss: 5.6926618CurrentTrain: epoch  6, batch    21 | loss: 5.3980274CurrentTrain: epoch  6, batch    22 | loss: 5.3237047CurrentTrain: epoch  6, batch    23 | loss: 7.2622170CurrentTrain: epoch  6, batch    24 | loss: 6.1817498CurrentTrain: epoch  6, batch    25 | loss: 6.9548073CurrentTrain: epoch  6, batch    26 | loss: 6.1040668CurrentTrain: epoch  6, batch    27 | loss: 6.3963342CurrentTrain: epoch  6, batch    28 | loss: 6.3625312CurrentTrain: epoch  6, batch    29 | loss: 6.6322069CurrentTrain: epoch  6, batch    30 | loss: 5.8799124CurrentTrain: epoch  6, batch    31 | loss: 6.2638960CurrentTrain: epoch  6, batch    32 | loss: 5.8702002CurrentTrain: epoch  6, batch    33 | loss: 6.5460720CurrentTrain: epoch  6, batch    34 | loss: 5.5608187CurrentTrain: epoch  6, batch    35 | loss: 6.1437402CurrentTrain: epoch  6, batch    36 | loss: 5.6790237CurrentTrain: epoch  6, batch    37 | loss: 7.3245459CurrentTrain: epoch  7, batch     0 | loss: 5.6522269CurrentTrain: epoch  7, batch     1 | loss: 6.4001441CurrentTrain: epoch  7, batch     2 | loss: 5.8140421CurrentTrain: epoch  7, batch     3 | loss: 5.8384843CurrentTrain: epoch  7, batch     4 | loss: 5.7649274CurrentTrain: epoch  7, batch     5 | loss: 5.6686091CurrentTrain: epoch  7, batch     6 | loss: 5.5077410CurrentTrain: epoch  7, batch     7 | loss: 5.7542276CurrentTrain: epoch  7, batch     8 | loss: 5.6726933CurrentTrain: epoch  7, batch     9 | loss: 5.7175255CurrentTrain: epoch  7, batch    10 | loss: 6.6560946CurrentTrain: epoch  7, batch    11 | loss: 5.3827648CurrentTrain: epoch  7, batch    12 | loss: 6.0653553CurrentTrain: epoch  7, batch    13 | loss: 5.6590219CurrentTrain: epoch  7, batch    14 | loss: 5.6235833CurrentTrain: epoch  7, batch    15 | loss: 5.6839304CurrentTrain: epoch  7, batch    16 | loss: 5.7770219CurrentTrain: epoch  7, batch    17 | loss: 6.4401016CurrentTrain: epoch  7, batch    18 | loss: 5.9425974CurrentTrain: epoch  7, batch    19 | loss: 6.9784632CurrentTrain: epoch  7, batch    20 | loss: 6.4094920CurrentTrain: epoch  7, batch    21 | loss: 6.4665127CurrentTrain: epoch  7, batch    22 | loss: 5.9274349CurrentTrain: epoch  7, batch    23 | loss: 5.6210194CurrentTrain: epoch  7, batch    24 | loss: 5.9982500CurrentTrain: epoch  7, batch    25 | loss: 6.3731866CurrentTrain: epoch  7, batch    26 | loss: 5.9429746CurrentTrain: epoch  7, batch    27 | loss: 6.3777771CurrentTrain: epoch  7, batch    28 | loss: 5.4543066CurrentTrain: epoch  7, batch    29 | loss: 6.7369404CurrentTrain: epoch  7, batch    30 | loss: 6.5710325CurrentTrain: epoch  7, batch    31 | loss: 6.3338461CurrentTrain: epoch  7, batch    32 | loss: 5.5344748CurrentTrain: epoch  7, batch    33 | loss: 5.3678913CurrentTrain: epoch  7, batch    34 | loss: 5.6227431CurrentTrain: epoch  7, batch    35 | loss: 6.2368145CurrentTrain: epoch  7, batch    36 | loss: 5.2329044CurrentTrain: epoch  7, batch    37 | loss: 6.1363659CurrentTrain: epoch  8, batch     0 | loss: 5.9397540CurrentTrain: epoch  8, batch     1 | loss: 6.2769380CurrentTrain: epoch  8, batch     2 | loss: 5.9463606CurrentTrain: epoch  8, batch     3 | loss: 5.4056168CurrentTrain: epoch  8, batch     4 | loss: 5.8697028CurrentTrain: epoch  8, batch     5 | loss: 5.9541821CurrentTrain: epoch  8, batch     6 | loss: 5.7141132CurrentTrain: epoch  8, batch     7 | loss: 5.7643914CurrentTrain: epoch  8, batch     8 | loss: 5.6096749CurrentTrain: epoch  8, batch     9 | loss: 5.3135018CurrentTrain: epoch  8, batch    10 | loss: 5.5128827CurrentTrain: epoch  8, batch    11 | loss: 5.2480841CurrentTrain: epoch  8, batch    12 | loss: 5.4453564CurrentTrain: epoch  8, batch    13 | loss: 5.1816435CurrentTrain: epoch  8, batch    14 | loss: 5.3119688CurrentTrain: epoch  8, batch    15 | loss: 5.1533513CurrentTrain: epoch  8, batch    16 | loss: 5.8991318CurrentTrain: epoch  8, batch    17 | loss: 5.5405622CurrentTrain: epoch  8, batch    18 | loss: 5.6997948CurrentTrain: epoch  8, batch    19 | loss: 5.4165244CurrentTrain: epoch  8, batch    20 | loss: 5.3590436CurrentTrain: epoch  8, batch    21 | loss: 6.2555466CurrentTrain: epoch  8, batch    22 | loss: 5.2106371CurrentTrain: epoch  8, batch    23 | loss: 5.7932954CurrentTrain: epoch  8, batch    24 | loss: 5.4117217CurrentTrain: epoch  8, batch    25 | loss: 5.1741972CurrentTrain: epoch  8, batch    26 | loss: 5.4503007CurrentTrain: epoch  8, batch    27 | loss: 6.1794252CurrentTrain: epoch  8, batch    28 | loss: 6.2955217CurrentTrain: epoch  8, batch    29 | loss: 5.6176586CurrentTrain: epoch  8, batch    30 | loss: 5.8143187CurrentTrain: epoch  8, batch    31 | loss: 5.2587128CurrentTrain: epoch  8, batch    32 | loss: 5.5069823CurrentTrain: epoch  8, batch    33 | loss: 5.4541316CurrentTrain: epoch  8, batch    34 | loss: 5.6600943CurrentTrain: epoch  8, batch    35 | loss: 6.0764809CurrentTrain: epoch  8, batch    36 | loss: 6.5768099CurrentTrain: epoch  8, batch    37 | loss: 4.8943768CurrentTrain: epoch  9, batch     0 | loss: 5.9729719CurrentTrain: epoch  9, batch     1 | loss: 5.5455170CurrentTrain: epoch  9, batch     2 | loss: 5.7906232CurrentTrain: epoch  9, batch     3 | loss: 5.3831697CurrentTrain: epoch  9, batch     4 | loss: 6.1459732CurrentTrain: epoch  9, batch     5 | loss: 5.7663193CurrentTrain: epoch  9, batch     6 | loss: 5.7005186CurrentTrain: epoch  9, batch     7 | loss: 5.3880525CurrentTrain: epoch  9, batch     8 | loss: 5.9787779CurrentTrain: epoch  9, batch     9 | loss: 6.3317361CurrentTrain: epoch  9, batch    10 | loss: 5.2918186CurrentTrain: epoch  9, batch    11 | loss: 5.4756174CurrentTrain: epoch  9, batch    12 | loss: 5.3993492CurrentTrain: epoch  9, batch    13 | loss: 5.2913456CurrentTrain: epoch  9, batch    14 | loss: 5.6240864CurrentTrain: epoch  9, batch    15 | loss: 5.2339211CurrentTrain: epoch  9, batch    16 | loss: 5.1432853CurrentTrain: epoch  9, batch    17 | loss: 5.3790650CurrentTrain: epoch  9, batch    18 | loss: 5.4179015CurrentTrain: epoch  9, batch    19 | loss: 5.9508538CurrentTrain: epoch  9, batch    20 | loss: 4.9340792CurrentTrain: epoch  9, batch    21 | loss: 5.2727442CurrentTrain: epoch  9, batch    22 | loss: 5.6437583CurrentTrain: epoch  9, batch    23 | loss: 5.2040510CurrentTrain: epoch  9, batch    24 | loss: 5.5292068CurrentTrain: epoch  9, batch    25 | loss: 5.4834423CurrentTrain: epoch  9, batch    26 | loss: 5.2166891CurrentTrain: epoch  9, batch    27 | loss: 5.1914816CurrentTrain: epoch  9, batch    28 | loss: 6.4779654CurrentTrain: epoch  9, batch    29 | loss: 5.4032869CurrentTrain: epoch  9, batch    30 | loss: 5.3249359CurrentTrain: epoch  9, batch    31 | loss: 5.5565834CurrentTrain: epoch  9, batch    32 | loss: 5.0292773CurrentTrain: epoch  9, batch    33 | loss: 5.2960625CurrentTrain: epoch  9, batch    34 | loss: 5.1728468CurrentTrain: epoch  9, batch    35 | loss: 4.9023743CurrentTrain: epoch  9, batch    36 | loss: 6.0239468CurrentTrain: epoch  9, batch    37 | loss: 4.7891827
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 14.7442274CurrentTrain: epoch  0, batch     1 | loss: 14.4393091CurrentTrain: epoch  0, batch     2 | loss: 14.5904427CurrentTrain: epoch  0, batch     3 | loss: 14.9955006CurrentTrain: epoch  0, batch     4 | loss: 14.4345999CurrentTrain: epoch  0, batch     5 | loss: 14.0185127CurrentTrain: epoch  0, batch     6 | loss: 14.2110167CurrentTrain: epoch  0, batch     7 | loss: 13.4758205CurrentTrain: epoch  0, batch     8 | loss: 14.1811676CurrentTrain: epoch  0, batch     9 | loss: 14.5847721CurrentTrain: epoch  0, batch    10 | loss: 13.6597452CurrentTrain: epoch  0, batch    11 | loss: 13.8180733CurrentTrain: epoch  0, batch    12 | loss: 13.0475330CurrentTrain: epoch  0, batch    13 | loss: 12.5300789CurrentTrain: epoch  0, batch    14 | loss: 13.4116011CurrentTrain: epoch  0, batch    15 | loss: 12.6210289CurrentTrain: epoch  0, batch    16 | loss: 13.3143654CurrentTrain: epoch  0, batch    17 | loss: 13.6838636CurrentTrain: epoch  0, batch    18 | loss: 12.9519634CurrentTrain: epoch  0, batch    19 | loss: 13.0967312CurrentTrain: epoch  0, batch    20 | loss: 13.2326393CurrentTrain: epoch  0, batch    21 | loss: 12.8843164CurrentTrain: epoch  0, batch    22 | loss: 12.3639584CurrentTrain: epoch  0, batch    23 | loss: 12.1596613CurrentTrain: epoch  0, batch    24 | loss: 12.4185047CurrentTrain: epoch  0, batch    25 | loss: 12.3374329CurrentTrain: epoch  0, batch    26 | loss: 12.4182377CurrentTrain: epoch  0, batch    27 | loss: 12.4554510CurrentTrain: epoch  0, batch    28 | loss: 12.3221350CurrentTrain: epoch  0, batch    29 | loss: 11.5806150CurrentTrain: epoch  0, batch    30 | loss: 11.5034866CurrentTrain: epoch  0, batch    31 | loss: 11.5836878CurrentTrain: epoch  0, batch    32 | loss: 11.9233170CurrentTrain: epoch  0, batch    33 | loss: 12.2540741CurrentTrain: epoch  0, batch    34 | loss: 12.4540005CurrentTrain: epoch  0, batch    35 | loss: 12.1754780CurrentTrain: epoch  0, batch    36 | loss: 12.3364296CurrentTrain: epoch  0, batch    37 | loss: 12.6726017CurrentTrain: epoch  1, batch     0 | loss: 11.4739637CurrentTrain: epoch  1, batch     1 | loss: 11.9450579CurrentTrain: epoch  1, batch     2 | loss: 11.0868225CurrentTrain: epoch  1, batch     3 | loss: 11.2657652CurrentTrain: epoch  1, batch     4 | loss: 10.9571056CurrentTrain: epoch  1, batch     5 | loss: 10.1292372CurrentTrain: epoch  1, batch     6 | loss: 10.5353174CurrentTrain: epoch  1, batch     7 | loss: 10.4999495CurrentTrain: epoch  1, batch     8 | loss: 8.7728548CurrentTrain: epoch  1, batch     9 | loss: 12.2832870CurrentTrain: epoch  1, batch    10 | loss: 10.5483379CurrentTrain: epoch  1, batch    11 | loss: 10.7573929CurrentTrain: epoch  1, batch    12 | loss: 10.4305983CurrentTrain: epoch  1, batch    13 | loss: 10.9339857CurrentTrain: epoch  1, batch    14 | loss: 10.5807056CurrentTrain: epoch  1, batch    15 | loss: 10.0960999CurrentTrain: epoch  1, batch    16 | loss: 10.5365992CurrentTrain: epoch  1, batch    17 | loss: 9.8112698CurrentTrain: epoch  1, batch    18 | loss: 10.0281334CurrentTrain: epoch  1, batch    19 | loss: 10.1190395CurrentTrain: epoch  1, batch    20 | loss: 10.5912933CurrentTrain: epoch  1, batch    21 | loss: 10.2032022CurrentTrain: epoch  1, batch    22 | loss: 9.4419584CurrentTrain: epoch  1, batch    23 | loss: 10.2326374CurrentTrain: epoch  1, batch    24 | loss: 9.5130177CurrentTrain: epoch  1, batch    25 | loss: 10.0067091CurrentTrain: epoch  1, batch    26 | loss: 9.6069717CurrentTrain: epoch  1, batch    27 | loss: 9.3766403CurrentTrain: epoch  1, batch    28 | loss: 8.7771091CurrentTrain: epoch  1, batch    29 | loss: 9.6774702CurrentTrain: epoch  1, batch    30 | loss: 9.9983006CurrentTrain: epoch  1, batch    31 | loss: 10.1154900CurrentTrain: epoch  1, batch    32 | loss: 9.6523924CurrentTrain: epoch  1, batch    33 | loss: 10.9627342CurrentTrain: epoch  1, batch    34 | loss: 9.5134945CurrentTrain: epoch  1, batch    35 | loss: 9.6507835CurrentTrain: epoch  1, batch    36 | loss: 9.0922203CurrentTrain: epoch  1, batch    37 | loss: 9.2106752CurrentTrain: epoch  2, batch     0 | loss: 8.5897913CurrentTrain: epoch  2, batch     1 | loss: 8.6466188CurrentTrain: epoch  2, batch     2 | loss: 9.1419125CurrentTrain: epoch  2, batch     3 | loss: 10.2794914CurrentTrain: epoch  2, batch     4 | loss: 9.1481495CurrentTrain: epoch  2, batch     5 | loss: 9.3412228CurrentTrain: epoch  2, batch     6 | loss: 8.6736212CurrentTrain: epoch  2, batch     7 | loss: 8.1150808CurrentTrain: epoch  2, batch     8 | loss: 8.6841774CurrentTrain: epoch  2, batch     9 | loss: 7.9290652CurrentTrain: epoch  2, batch    10 | loss: 8.6326447CurrentTrain: epoch  2, batch    11 | loss: 8.4072027CurrentTrain: epoch  2, batch    12 | loss: 8.8765354CurrentTrain: epoch  2, batch    13 | loss: 9.0750265CurrentTrain: epoch  2, batch    14 | loss: 8.6240835CurrentTrain: epoch  2, batch    15 | loss: 8.6254911CurrentTrain: epoch  2, batch    16 | loss: 8.7224798CurrentTrain: epoch  2, batch    17 | loss: 7.7152123CurrentTrain: epoch  2, batch    18 | loss: 9.2045889CurrentTrain: epoch  2, batch    19 | loss: 9.0904016CurrentTrain: epoch  2, batch    20 | loss: 7.8130398CurrentTrain: epoch  2, batch    21 | loss: 7.1833930CurrentTrain: epoch  2, batch    22 | loss: 7.6135945CurrentTrain: epoch  2, batch    23 | loss: 8.2749224CurrentTrain: epoch  2, batch    24 | loss: 7.9266090CurrentTrain: epoch  2, batch    25 | loss: 8.1867085CurrentTrain: epoch  2, batch    26 | loss: 9.1022091CurrentTrain: epoch  2, batch    27 | loss: 8.3064575CurrentTrain: epoch  2, batch    28 | loss: 9.4157896CurrentTrain: epoch  2, batch    29 | loss: 7.9761958CurrentTrain: epoch  2, batch    30 | loss: 8.6132803CurrentTrain: epoch  2, batch    31 | loss: 7.8172574CurrentTrain: epoch  2, batch    32 | loss: 8.8158226CurrentTrain: epoch  2, batch    33 | loss: 8.0239506CurrentTrain: epoch  2, batch    34 | loss: 7.3093839CurrentTrain: epoch  2, batch    35 | loss: 9.1269884CurrentTrain: epoch  2, batch    36 | loss: 8.1167240CurrentTrain: epoch  2, batch    37 | loss: 6.9535518CurrentTrain: epoch  3, batch     0 | loss: 7.9825191CurrentTrain: epoch  3, batch     1 | loss: 8.5658789CurrentTrain: epoch  3, batch     2 | loss: 8.6652269CurrentTrain: epoch  3, batch     3 | loss: 8.0464344CurrentTrain: epoch  3, batch     4 | loss: 7.0758052CurrentTrain: epoch  3, batch     5 | loss: 9.2641087CurrentTrain: epoch  3, batch     6 | loss: 7.2803626CurrentTrain: epoch  3, batch     7 | loss: 9.0607872CurrentTrain: epoch  3, batch     8 | loss: 7.3654160CurrentTrain: epoch  3, batch     9 | loss: 7.9201093CurrentTrain: epoch  3, batch    10 | loss: 7.3643765CurrentTrain: epoch  3, batch    11 | loss: 8.8547678CurrentTrain: epoch  3, batch    12 | loss: 8.3684225CurrentTrain: epoch  3, batch    13 | loss: 7.0775576CurrentTrain: epoch  3, batch    14 | loss: 6.7578025CurrentTrain: epoch  3, batch    15 | loss: 6.6201715CurrentTrain: epoch  3, batch    16 | loss: 7.3221607CurrentTrain: epoch  3, batch    17 | loss: 6.8295183CurrentTrain: epoch  3, batch    18 | loss: 8.2216396CurrentTrain: epoch  3, batch    19 | loss: 7.1156802CurrentTrain: epoch  3, batch    20 | loss: 8.0072088CurrentTrain: epoch  3, batch    21 | loss: 6.5869503CurrentTrain: epoch  3, batch    22 | loss: 8.0312548CurrentTrain: epoch  3, batch    23 | loss: 8.0823641CurrentTrain: epoch  3, batch    24 | loss: 7.0243893CurrentTrain: epoch  3, batch    25 | loss: 7.7158194CurrentTrain: epoch  3, batch    26 | loss: 6.5578079CurrentTrain: epoch  3, batch    27 | loss: 7.1242118CurrentTrain: epoch  3, batch    28 | loss: 6.8547211CurrentTrain: epoch  3, batch    29 | loss: 6.8613262CurrentTrain: epoch  3, batch    30 | loss: 8.9640226CurrentTrain: epoch  3, batch    31 | loss: 7.1063256CurrentTrain: epoch  3, batch    32 | loss: 8.2895164CurrentTrain: epoch  3, batch    33 | loss: 8.2005939CurrentTrain: epoch  3, batch    34 | loss: 6.8292713CurrentTrain: epoch  3, batch    35 | loss: 8.0703793CurrentTrain: epoch  3, batch    36 | loss: 7.2129993CurrentTrain: epoch  3, batch    37 | loss: 8.0274115CurrentTrain: epoch  4, batch     0 | loss: 7.6430883CurrentTrain: epoch  4, batch     1 | loss: 7.9114022CurrentTrain: epoch  4, batch     2 | loss: 7.3843660CurrentTrain: epoch  4, batch     3 | loss: 6.2892470CurrentTrain: epoch  4, batch     4 | loss: 7.1283693CurrentTrain: epoch  4, batch     5 | loss: 7.7022157CurrentTrain: epoch  4, batch     6 | loss: 7.5556755CurrentTrain: epoch  4, batch     7 | loss: 7.3783517CurrentTrain: epoch  4, batch     8 | loss: 6.9317451CurrentTrain: epoch  4, batch     9 | loss: 6.8362875CurrentTrain: epoch  4, batch    10 | loss: 7.1276445CurrentTrain: epoch  4, batch    11 | loss: 6.9111080CurrentTrain: epoch  4, batch    12 | loss: 6.8506231CurrentTrain: epoch  4, batch    13 | loss: 7.5526333CurrentTrain: epoch  4, batch    14 | loss: 7.9192038CurrentTrain: epoch  4, batch    15 | loss: 6.9678683CurrentTrain: epoch  4, batch    16 | loss: 6.8811960CurrentTrain: epoch  4, batch    17 | loss: 7.3522191CurrentTrain: epoch  4, batch    18 | loss: 7.4163342CurrentTrain: epoch  4, batch    19 | loss: 7.0052285CurrentTrain: epoch  4, batch    20 | loss: 8.3670092CurrentTrain: epoch  4, batch    21 | loss: 6.7970462CurrentTrain: epoch  4, batch    22 | loss: 5.9826674CurrentTrain: epoch  4, batch    23 | loss: 6.9708366CurrentTrain: epoch  4, batch    24 | loss: 7.5191717CurrentTrain: epoch  4, batch    25 | loss: 6.5850143CurrentTrain: epoch  4, batch    26 | loss: 8.1271963CurrentTrain: epoch  4, batch    27 | loss: 6.8870339CurrentTrain: epoch  4, batch    28 | loss: 7.1794653CurrentTrain: epoch  4, batch    29 | loss: 6.5591593CurrentTrain: epoch  4, batch    30 | loss: 7.6343274CurrentTrain: epoch  4, batch    31 | loss: 6.5293221CurrentTrain: epoch  4, batch    32 | loss: 7.1355686CurrentTrain: epoch  4, batch    33 | loss: 6.7397013CurrentTrain: epoch  4, batch    34 | loss: 5.8892474CurrentTrain: epoch  4, batch    35 | loss: 6.7100439CurrentTrain: epoch  4, batch    36 | loss: 6.1463213CurrentTrain: epoch  4, batch    37 | loss: 6.0406084CurrentTrain: epoch  5, batch     0 | loss: 6.0411711CurrentTrain: epoch  5, batch     1 | loss: 6.2926292CurrentTrain: epoch  5, batch     2 | loss: 5.5542154CurrentTrain: epoch  5, batch     3 | loss: 5.3855209CurrentTrain: epoch  5, batch     4 | loss: 5.4963355CurrentTrain: epoch  5, batch     5 | loss: 6.4418898CurrentTrain: epoch  5, batch     6 | loss: 6.3067198CurrentTrain: epoch  5, batch     7 | loss: 5.8808837CurrentTrain: epoch  5, batch     8 | loss: 6.3593721CurrentTrain: epoch  5, batch     9 | loss: 6.3629804CurrentTrain: epoch  5, batch    10 | loss: 6.1406102CurrentTrain: epoch  5, batch    11 | loss: 5.6270480CurrentTrain: epoch  5, batch    12 | loss: 6.1190662CurrentTrain: epoch  5, batch    13 | loss: 6.8148022CurrentTrain: epoch  5, batch    14 | loss: 7.2670593CurrentTrain: epoch  5, batch    15 | loss: 7.5133491CurrentTrain: epoch  5, batch    16 | loss: 6.7610583CurrentTrain: epoch  5, batch    17 | loss: 7.2561226CurrentTrain: epoch  5, batch    18 | loss: 6.1402473CurrentTrain: epoch  5, batch    19 | loss: 6.5286956CurrentTrain: epoch  5, batch    20 | loss: 6.7156096CurrentTrain: epoch  5, batch    21 | loss: 5.8922348CurrentTrain: epoch  5, batch    22 | loss: 6.1777153CurrentTrain: epoch  5, batch    23 | loss: 6.3274641CurrentTrain: epoch  5, batch    24 | loss: 7.2320905CurrentTrain: epoch  5, batch    25 | loss: 6.9294662CurrentTrain: epoch  5, batch    26 | loss: 7.9127107CurrentTrain: epoch  5, batch    27 | loss: 5.8184242CurrentTrain: epoch  5, batch    28 | loss: 6.9393024CurrentTrain: epoch  5, batch    29 | loss: 6.4814992CurrentTrain: epoch  5, batch    30 | loss: 6.2508206CurrentTrain: epoch  5, batch    31 | loss: 6.3710585CurrentTrain: epoch  5, batch    32 | loss: 8.0238857CurrentTrain: epoch  5, batch    33 | loss: 7.3188295CurrentTrain: epoch  5, batch    34 | loss: 7.9191294CurrentTrain: epoch  5, batch    35 | loss: 6.1371541CurrentTrain: epoch  5, batch    36 | loss: 8.8248749CurrentTrain: epoch  5, batch    37 | loss: 6.6311221CurrentTrain: epoch  6, batch     0 | loss: 5.3741608CurrentTrain: epoch  6, batch     1 | loss: 6.1649380CurrentTrain: epoch  6, batch     2 | loss: 6.9099154CurrentTrain: epoch  6, batch     3 | loss: 7.4086180CurrentTrain: epoch  6, batch     4 | loss: 7.0337992CurrentTrain: epoch  6, batch     5 | loss: 6.4074779CurrentTrain: epoch  6, batch     6 | loss: 6.9465318CurrentTrain: epoch  6, batch     7 | loss: 6.2499728CurrentTrain: epoch  6, batch     8 | loss: 6.5746984CurrentTrain: epoch  6, batch     9 | loss: 5.9017687CurrentTrain: epoch  6, batch    10 | loss: 5.6568365CurrentTrain: epoch  6, batch    11 | loss: 7.0612679CurrentTrain: epoch  6, batch    12 | loss: 6.1299977CurrentTrain: epoch  6, batch    13 | loss: 6.5125585CurrentTrain: epoch  6, batch    14 | loss: 5.7008033CurrentTrain: epoch  6, batch    15 | loss: 6.1563034CurrentTrain: epoch  6, batch    16 | loss: 6.4428310CurrentTrain: epoch  6, batch    17 | loss: 5.7946434CurrentTrain: epoch  6, batch    18 | loss: 5.3966432CurrentTrain: epoch  6, batch    19 | loss: 5.4208651CurrentTrain: epoch  6, batch    20 | loss: 5.5191293CurrentTrain: epoch  6, batch    21 | loss: 6.7078562CurrentTrain: epoch  6, batch    22 | loss: 6.2748227CurrentTrain: epoch  6, batch    23 | loss: 5.6630740CurrentTrain: epoch  6, batch    24 | loss: 5.3713694CurrentTrain: epoch  6, batch    25 | loss: 5.5181556CurrentTrain: epoch  6, batch    26 | loss: 5.5500422CurrentTrain: epoch  6, batch    27 | loss: 5.8632407CurrentTrain: epoch  6, batch    28 | loss: 5.9203696CurrentTrain: epoch  6, batch    29 | loss: 5.4786434CurrentTrain: epoch  6, batch    30 | loss: 5.4818544CurrentTrain: epoch  6, batch    31 | loss: 7.0142965CurrentTrain: epoch  6, batch    32 | loss: 5.6069517CurrentTrain: epoch  6, batch    33 | loss: 5.8480692CurrentTrain: epoch  6, batch    34 | loss: 5.5617709CurrentTrain: epoch  6, batch    35 | loss: 6.0076513CurrentTrain: epoch  6, batch    36 | loss: 6.4384599CurrentTrain: epoch  6, batch    37 | loss: 5.1998692CurrentTrain: epoch  7, batch     0 | loss: 5.4721565CurrentTrain: epoch  7, batch     1 | loss: 5.2780104CurrentTrain: epoch  7, batch     2 | loss: 5.8497796CurrentTrain: epoch  7, batch     3 | loss: 6.1159272CurrentTrain: epoch  7, batch     4 | loss: 5.5039353CurrentTrain: epoch  7, batch     5 | loss: 5.3717103CurrentTrain: epoch  7, batch     6 | loss: 5.4322515CurrentTrain: epoch  7, batch     7 | loss: 6.8619962CurrentTrain: epoch  7, batch     8 | loss: 5.2523718CurrentTrain: epoch  7, batch     9 | loss: 6.3707399CurrentTrain: epoch  7, batch    10 | loss: 5.5814228CurrentTrain: epoch  7, batch    11 | loss: 5.3081489CurrentTrain: epoch  7, batch    12 | loss: 6.0176687CurrentTrain: epoch  7, batch    13 | loss: 5.8430448CurrentTrain: epoch  7, batch    14 | loss: 6.4451818CurrentTrain: epoch  7, batch    15 | loss: 5.3106251CurrentTrain: epoch  7, batch    16 | loss: 5.3537936CurrentTrain: epoch  7, batch    17 | loss: 6.0586820CurrentTrain: epoch  7, batch    18 | loss: 5.6978841CurrentTrain: epoch  7, batch    19 | loss: 5.3147898CurrentTrain: epoch  7, batch    20 | loss: 5.7935886CurrentTrain: epoch  7, batch    21 | loss: 5.2868447CurrentTrain: epoch  7, batch    22 | loss: 5.9833136CurrentTrain: epoch  7, batch    23 | loss: 5.4459262CurrentTrain: epoch  7, batch    24 | loss: 5.1385102CurrentTrain: epoch  7, batch    25 | loss: 5.6710224CurrentTrain: epoch  7, batch    26 | loss: 5.3526940CurrentTrain: epoch  7, batch    27 | loss: 5.2502441CurrentTrain: epoch  7, batch    28 | loss: 6.0589228CurrentTrain: epoch  7, batch    29 | loss: 5.5304232CurrentTrain: epoch  7, batch    30 | loss: 6.9949427CurrentTrain: epoch  7, batch    31 | loss: 5.5200539CurrentTrain: epoch  7, batch    32 | loss: 5.3592472CurrentTrain: epoch  7, batch    33 | loss: 5.2001510CurrentTrain: epoch  7, batch    34 | loss: 7.9950056CurrentTrain: epoch  7, batch    35 | loss: 6.4582968CurrentTrain: epoch  7, batch    36 | loss: 5.7632742CurrentTrain: epoch  7, batch    37 | loss: 5.7971311CurrentTrain: epoch  8, batch     0 | loss: 5.3631330CurrentTrain: epoch  8, batch     1 | loss: 6.4469213CurrentTrain: epoch  8, batch     2 | loss: 5.2411375CurrentTrain: epoch  8, batch     3 | loss: 5.5550504CurrentTrain: epoch  8, batch     4 | loss: 6.1316724CurrentTrain: epoch  8, batch     5 | loss: 5.5358486CurrentTrain: epoch  8, batch     6 | loss: 5.1718097CurrentTrain: epoch  8, batch     7 | loss: 5.7481642CurrentTrain: epoch  8, batch     8 | loss: 5.3769512CurrentTrain: epoch  8, batch     9 | loss: 5.4986663CurrentTrain: epoch  8, batch    10 | loss: 5.5281940CurrentTrain: epoch  8, batch    11 | loss: 5.1398468CurrentTrain: epoch  8, batch    12 | loss: 5.4536009CurrentTrain: epoch  8, batch    13 | loss: 5.0662355CurrentTrain: epoch  8, batch    14 | loss: 5.9160385CurrentTrain: epoch  8, batch    15 | loss: 5.8194580CurrentTrain: epoch  8, batch    16 | loss: 5.6845770CurrentTrain: epoch  8, batch    17 | loss: 5.0295539CurrentTrain: epoch  8, batch    18 | loss: 5.2725682CurrentTrain: epoch  8, batch    19 | loss: 5.1351891CurrentTrain: epoch  8, batch    20 | loss: 5.8727107CurrentTrain: epoch  8, batch    21 | loss: 5.3750567CurrentTrain: epoch  8, batch    22 | loss: 5.3113832CurrentTrain: epoch  8, batch    23 | loss: 5.2143211CurrentTrain: epoch  8, batch    24 | loss: 5.2488394CurrentTrain: epoch  8, batch    25 | loss: 5.1769133CurrentTrain: epoch  8, batch    26 | loss: 5.4647417CurrentTrain: epoch  8, batch    27 | loss: 5.9104357CurrentTrain: epoch  8, batch    28 | loss: 5.3568401CurrentTrain: epoch  8, batch    29 | loss: 5.1928339CurrentTrain: epoch  8, batch    30 | loss: 5.1897635CurrentTrain: epoch  8, batch    31 | loss: 5.2377467CurrentTrain: epoch  8, batch    32 | loss: 5.5624838CurrentTrain: epoch  8, batch    33 | loss: 5.9927464CurrentTrain: epoch  8, batch    34 | loss: 5.2213607CurrentTrain: epoch  8, batch    35 | loss: 4.9897966CurrentTrain: epoch  8, batch    36 | loss: 4.9569068CurrentTrain: epoch  8, batch    37 | loss: 4.6935377CurrentTrain: epoch  9, batch     0 | loss: 5.0643306CurrentTrain: epoch  9, batch     1 | loss: 5.0225234CurrentTrain: epoch  9, batch     2 | loss: 5.2656245CurrentTrain: epoch  9, batch     3 | loss: 4.9557567CurrentTrain: epoch  9, batch     4 | loss: 5.1010752CurrentTrain: epoch  9, batch     5 | loss: 5.1430206CurrentTrain: epoch  9, batch     6 | loss: 5.3956957CurrentTrain: epoch  9, batch     7 | loss: 4.8316989CurrentTrain: epoch  9, batch     8 | loss: 5.2101717CurrentTrain: epoch  9, batch     9 | loss: 5.2869573CurrentTrain: epoch  9, batch    10 | loss: 4.9811606CurrentTrain: epoch  9, batch    11 | loss: 5.1471443CurrentTrain: epoch  9, batch    12 | loss: 5.9932437CurrentTrain: epoch  9, batch    13 | loss: 5.7932906CurrentTrain: epoch  9, batch    14 | loss: 4.9316058CurrentTrain: epoch  9, batch    15 | loss: 5.3358250CurrentTrain: epoch  9, batch    16 | loss: 4.7659035CurrentTrain: epoch  9, batch    17 | loss: 5.5065408CurrentTrain: epoch  9, batch    18 | loss: 4.9394073CurrentTrain: epoch  9, batch    19 | loss: 4.8658123CurrentTrain: epoch  9, batch    20 | loss: 6.2085476CurrentTrain: epoch  9, batch    21 | loss: 5.2284784CurrentTrain: epoch  9, batch    22 | loss: 6.0498948CurrentTrain: epoch  9, batch    23 | loss: 6.2422867CurrentTrain: epoch  9, batch    24 | loss: 5.0957670CurrentTrain: epoch  9, batch    25 | loss: 5.5123034CurrentTrain: epoch  9, batch    26 | loss: 5.2657990CurrentTrain: epoch  9, batch    27 | loss: 4.7913523CurrentTrain: epoch  9, batch    28 | loss: 5.3441954CurrentTrain: epoch  9, batch    29 | loss: 4.9204383CurrentTrain: epoch  9, batch    30 | loss: 5.1175294CurrentTrain: epoch  9, batch    31 | loss: 5.1102953CurrentTrain: epoch  9, batch    32 | loss: 5.4190035CurrentTrain: epoch  9, batch    33 | loss: 5.9996181CurrentTrain: epoch  9, batch    34 | loss: 5.1164765CurrentTrain: epoch  9, batch    35 | loss: 5.1681604CurrentTrain: epoch  9, batch    36 | loss: 5.1305404CurrentTrain: epoch  9, batch    37 | loss: 6.2217026
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 87.08%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 85.55%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.93%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 84.03%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 83.88%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 84.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.51%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.73%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.17%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.58%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.91%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 88.87%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 87.31%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 87.08%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 85.55%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.93%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 84.03%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 83.88%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 84.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.51%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.73%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.17%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.58%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.91%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 88.87%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 87.31%   
cur_acc_llm:  [0.8731060606060606]
his_acc_llm:  [0.8731060606060606]
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.63%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.05%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.68%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.78%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.04%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.93%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 87.92%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.74%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.63%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.05%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.68%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.78%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.04%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.93%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 87.92%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.74%   
cur_acc:  ['0.8674']
his_acc:  ['0.8674']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 6.2832770CurrentTrain: epoch  0, batch     1 | loss: 5.3978105CurrentTrain: epoch  1, batch     0 | loss: 4.9652519CurrentTrain: epoch  1, batch     1 | loss: 4.7545619CurrentTrain: epoch  2, batch     0 | loss: 4.0527053CurrentTrain: epoch  2, batch     1 | loss: 3.7592990CurrentTrain: epoch  3, batch     0 | loss: 3.1818829CurrentTrain: epoch  3, batch     1 | loss: 2.9932895CurrentTrain: epoch  4, batch     0 | loss: 2.8738232CurrentTrain: epoch  4, batch     1 | loss: 2.6044049CurrentTrain: epoch  5, batch     0 | loss: 2.4712996CurrentTrain: epoch  5, batch     1 | loss: 2.1644380CurrentTrain: epoch  6, batch     0 | loss: 2.2084105CurrentTrain: epoch  6, batch     1 | loss: 2.0165787CurrentTrain: epoch  7, batch     0 | loss: 2.0115490CurrentTrain: epoch  7, batch     1 | loss: 2.0194793CurrentTrain: epoch  8, batch     0 | loss: 1.9291005CurrentTrain: epoch  8, batch     1 | loss: 1.8842252CurrentTrain: epoch  9, batch     0 | loss: 1.7950604CurrentTrain: epoch  9, batch     1 | loss: 1.8140280
Mixup data size:  61
MixupTrain:  epoch  0, batch     0 | loss: 6.0399555MixupTrain:  epoch  0, batch     1 | loss: 6.5236551MixupTrain:  epoch  0, batch     2 | loss: 5.1799715MixupTrain:  epoch  0, batch     3 | loss: 4.3104962
MemoryTrain:  epoch  0, batch     0 | loss: 2.1204610MemoryTrain:  epoch  1, batch     0 | loss: 2.1417196MemoryTrain:  epoch  2, batch     0 | loss: 1.3781366MemoryTrain:  epoch  3, batch     0 | loss: 1.1411686MemoryTrain:  epoch  4, batch     0 | loss: 0.7939660MemoryTrain:  epoch  5, batch     0 | loss: 0.3971966MemoryTrain:  epoch  6, batch     0 | loss: 0.3392812MemoryTrain:  epoch  7, batch     0 | loss: 0.1491096MemoryTrain:  epoch  8, batch     0 | loss: 0.1741012MemoryTrain:  epoch  9, batch     0 | loss: 0.1295433
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 9.7220402CurrentTrain: epoch  0, batch     1 | loss: 7.3722029CurrentTrain: epoch  1, batch     0 | loss: 7.7796245CurrentTrain: epoch  1, batch     1 | loss: 8.7673559CurrentTrain: epoch  2, batch     0 | loss: 7.9032207CurrentTrain: epoch  2, batch     1 | loss: 6.2393498CurrentTrain: epoch  3, batch     0 | loss: 7.4731140CurrentTrain: epoch  3, batch     1 | loss: 5.3317094CurrentTrain: epoch  4, batch     0 | loss: 6.7977180CurrentTrain: epoch  4, batch     1 | loss: 5.7607517CurrentTrain: epoch  5, batch     0 | loss: 6.2876081CurrentTrain: epoch  5, batch     1 | loss: 5.6752152CurrentTrain: epoch  6, batch     0 | loss: 5.9879751CurrentTrain: epoch  6, batch     1 | loss: 4.2334681CurrentTrain: epoch  7, batch     0 | loss: 4.8255787CurrentTrain: epoch  7, batch     1 | loss: 4.3547163CurrentTrain: epoch  8, batch     0 | loss: 4.8087535CurrentTrain: epoch  8, batch     1 | loss: 4.4919281CurrentTrain: epoch  9, batch     0 | loss: 4.8396301CurrentTrain: epoch  9, batch     1 | loss: 3.2851202
Mixup data size:  61
MixupTrain:  epoch  0, batch     0 | loss: 6.7211681MixupTrain:  epoch  0, batch     1 | loss: 5.3154642MixupTrain:  epoch  0, batch     2 | loss: 6.8791219MixupTrain:  epoch  0, batch     3 | loss: 6.1152775
MemoryTrain:  epoch  0, batch     0 | loss: 1.5709670MemoryTrain:  epoch  1, batch     0 | loss: 1.7567093MemoryTrain:  epoch  2, batch     0 | loss: 1.7104666MemoryTrain:  epoch  3, batch     0 | loss: 1.3854207MemoryTrain:  epoch  4, batch     0 | loss: 0.9124223MemoryTrain:  epoch  5, batch     0 | loss: 0.8283893MemoryTrain:  epoch  6, batch     0 | loss: 1.0634712MemoryTrain:  epoch  7, batch     0 | loss: 0.8448453MemoryTrain:  epoch  8, batch     0 | loss: 0.4698811MemoryTrain:  epoch  9, batch     0 | loss: 0.6328497
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 95.54%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 94.53%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 92.61%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 93.23%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 94.20%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 94.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 94.92%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 95.22%   [EVAL] batch:   17 | acc: 18.75%,  total acc: 90.97%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 79.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 83.17%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 78.75%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 76.95%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 75.37%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 73.61%   [EVAL] batch:   18 | acc: 43.75%,  total acc: 72.04%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 70.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 72.32%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 73.58%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 74.46%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 75.52%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 76.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.40%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 78.01%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 79.53%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.65%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 80.86%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.44%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 81.80%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 81.96%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 82.47%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 82.94%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 83.22%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 83.49%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.91%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 83.23%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 83.48%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 83.87%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 84.23%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 84.58%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 84.92%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 85.24%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 85.55%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 85.84%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 85.25%   
cur_acc_llm:  [0.8731060606060606, 0.9097222222222222]
his_acc_llm:  [0.8731060606060606, 0.8525]
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 27.08%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 23.44%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 26.25%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 22.92%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 31.25%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 36.72%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 38.89%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 44.38%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 48.86%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 53.12%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 56.73%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 59.82%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 64.84%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 66.91%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 64.58%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 66.25%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 64.58%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 75.69%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 77.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 80.36%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 78.52%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 78.31%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 77.43%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 76.97%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 77.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 78.57%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 79.55%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 80.43%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 82.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.69%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 83.10%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.71%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.27%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 84.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.08%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 85.35%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 84.28%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 82.72%   [EVAL] batch:   34 | acc: 18.75%,  total acc: 80.89%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 79.34%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 78.04%   [EVAL] batch:   37 | acc: 12.50%,  total acc: 76.32%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 75.64%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 76.09%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 75.30%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 75.74%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 76.16%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 76.56%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 77.58%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 78.06%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 78.52%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 78.95%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 78.62%   
cur_acc:  ['0.8674', '0.6458']
his_acc:  ['0.8674', '0.7863']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 5.6465230CurrentTrain: epoch  0, batch     1 | loss: 6.0862060CurrentTrain: epoch  1, batch     0 | loss: 4.7048101CurrentTrain: epoch  1, batch     1 | loss: 4.4230232CurrentTrain: epoch  2, batch     0 | loss: 4.1182699CurrentTrain: epoch  2, batch     1 | loss: 4.1307321CurrentTrain: epoch  3, batch     0 | loss: 3.7925224CurrentTrain: epoch  3, batch     1 | loss: 4.0846353CurrentTrain: epoch  4, batch     0 | loss: 2.9352267CurrentTrain: epoch  4, batch     1 | loss: 3.9729636CurrentTrain: epoch  5, batch     0 | loss: 4.1148720CurrentTrain: epoch  5, batch     1 | loss: 2.7128384CurrentTrain: epoch  6, batch     0 | loss: 3.5240185CurrentTrain: epoch  6, batch     1 | loss: 2.5759101CurrentTrain: epoch  7, batch     0 | loss: 3.1908278CurrentTrain: epoch  7, batch     1 | loss: 2.2948532CurrentTrain: epoch  8, batch     0 | loss: 2.9353499CurrentTrain: epoch  8, batch     1 | loss: 2.2413752CurrentTrain: epoch  9, batch     0 | loss: 2.6511598CurrentTrain: epoch  9, batch     1 | loss: 2.3436174
Mixup data size:  70
MixupTrain:  epoch  0, batch     2 | loss: 4.4182819MixupTrain:  epoch  0, batch     3 | loss: 4.0833856
MemoryTrain:  epoch  0, batch     0 | loss: 1.6268743MemoryTrain:  epoch  1, batch     0 | loss: 1.7457850MemoryTrain:  epoch  2, batch     0 | loss: 1.3134975MemoryTrain:  epoch  3, batch     0 | loss: 0.7332628MemoryTrain:  epoch  4, batch     0 | loss: 0.2438319MemoryTrain:  epoch  5, batch     0 | loss: 0.2415366MemoryTrain:  epoch  6, batch     0 | loss: 0.2586542MemoryTrain:  epoch  7, batch     0 | loss: 0.1384469MemoryTrain:  epoch  8, batch     0 | loss: 0.0531253MemoryTrain:  epoch  9, batch     0 | loss: 0.0469602
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 8.3543825CurrentTrain: epoch  0, batch     1 | loss: 7.4044037CurrentTrain: epoch  1, batch     0 | loss: 8.3031969CurrentTrain: epoch  1, batch     1 | loss: 5.4514046CurrentTrain: epoch  2, batch     0 | loss: 5.9059358CurrentTrain: epoch  2, batch     1 | loss: 6.9431429CurrentTrain: epoch  3, batch     0 | loss: 5.5255041CurrentTrain: epoch  3, batch     1 | loss: 5.1385231CurrentTrain: epoch  4, batch     0 | loss: 5.4610872CurrentTrain: epoch  4, batch     1 | loss: 4.4201918CurrentTrain: epoch  5, batch     0 | loss: 4.9605637CurrentTrain: epoch  5, batch     1 | loss: 4.1895366CurrentTrain: epoch  6, batch     0 | loss: 3.7402763CurrentTrain: epoch  6, batch     1 | loss: 5.1020899CurrentTrain: epoch  7, batch     0 | loss: 3.8590131CurrentTrain: epoch  7, batch     1 | loss: 3.0040743CurrentTrain: epoch  8, batch     0 | loss: 3.7364841CurrentTrain: epoch  8, batch     1 | loss: 2.6476879CurrentTrain: epoch  9, batch     0 | loss: 3.1377175CurrentTrain: epoch  9, batch     1 | loss: 2.9896061
Mixup data size:  71
MixupTrain:  epoch  0, batch     0 | loss: 4.8831982MixupTrain:  epoch  0, batch     1 | loss: 5.3992994MixupTrain:  epoch  0, batch     2 | loss: 6.6482862MixupTrain:  epoch  0, batch     3 | loss: 4.6235112MixupTrain:  epoch  0, batch     4 | loss: 3.9162894
MemoryTrain:  epoch  0, batch     0 | loss: 2.6294191MemoryTrain:  epoch  1, batch     0 | loss: 2.5265589MemoryTrain:  epoch  2, batch     0 | loss: 2.0860369MemoryTrain:  epoch  3, batch     0 | loss: 2.0773349MemoryTrain:  epoch  4, batch     0 | loss: 1.5998585MemoryTrain:  epoch  5, batch     0 | loss: 1.4584076MemoryTrain:  epoch  6, batch     0 | loss: 1.1959513MemoryTrain:  epoch  7, batch     0 | loss: 1.1279252MemoryTrain:  epoch  8, batch     0 | loss: 1.0137389MemoryTrain:  epoch  9, batch     0 | loss: 0.7613026
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 46.25%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 38.54%   [EVAL] batch:    6 | acc: 0.00%,  total acc: 33.04%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 28.91%   
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 9.38%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 8.33%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 9.38%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 8.75%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 7.29%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 16.07%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 26.56%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 34.03%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 40.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 45.45%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 49.48%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 51.44%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 50.00%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 48.75%   [EVAL] batch:   15 | acc: 25.00%,  total acc: 47.27%   [EVAL] batch:   16 | acc: 25.00%,  total acc: 45.96%   [EVAL] batch:   17 | acc: 37.50%,  total acc: 45.49%   [EVAL] batch:   18 | acc: 25.00%,  total acc: 44.41%   [EVAL] batch:   19 | acc: 43.75%,  total acc: 44.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 47.02%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 49.43%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 51.36%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 53.39%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 55.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 56.97%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 58.33%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 59.82%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 60.99%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 62.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 63.31%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 64.06%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 65.15%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 65.99%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 66.79%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 67.71%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 68.58%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 69.24%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 70.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 70.78%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 70.43%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 70.98%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 71.66%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 72.30%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 73.51%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 74.07%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 74.61%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 75.13%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 75.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 75.49%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 75.60%   [EVAL] batch:   52 | acc: 37.50%,  total acc: 74.88%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 73.50%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 72.16%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 70.87%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 69.63%   
cur_acc_llm:  [0.8731060606060606, 0.9097222222222222, 0.2890625]
his_acc_llm:  [0.8731060606060606, 0.8525, 0.6962719298245614]
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 47.50%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 40.62%   [EVAL] batch:    6 | acc: 0.00%,  total acc: 34.82%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 30.47%   
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 21.88%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 22.50%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 22.92%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 31.25%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 39.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 45.83%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 50.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 53.98%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 56.77%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 59.13%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 58.93%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 60.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 59.77%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 60.66%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 60.76%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 60.86%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 61.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 63.69%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 65.34%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 66.85%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 68.23%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 69.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 70.67%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 71.53%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 72.54%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 73.06%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 73.75%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 74.19%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 74.80%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 74.05%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 71.88%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 70.54%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 68.75%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 67.23%   [EVAL] batch:   37 | acc: 18.75%,  total acc: 65.95%   [EVAL] batch:   38 | acc: 43.75%,  total acc: 65.38%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 66.09%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 65.24%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 65.77%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 66.28%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 66.90%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 68.34%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 69.02%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 69.66%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 70.28%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 70.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 71.20%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 71.03%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 70.52%   [EVAL] batch:   53 | acc: 6.25%,  total acc: 69.33%   [EVAL] batch:   54 | acc: 6.25%,  total acc: 68.18%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 66.96%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 65.79%   
cur_acc:  ['0.8674', '0.6458', '0.3047']
his_acc:  ['0.8674', '0.7863', '0.6579']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 3.8330348CurrentTrain: epoch  0, batch     1 | loss: 4.3210526CurrentTrain: epoch  1, batch     0 | loss: 2.9477079CurrentTrain: epoch  1, batch     1 | loss: 2.7640622CurrentTrain: epoch  2, batch     0 | loss: 2.2942164CurrentTrain: epoch  2, batch     1 | loss: 2.2872107CurrentTrain: epoch  3, batch     0 | loss: 2.0515532CurrentTrain: epoch  3, batch     1 | loss: 2.0204494CurrentTrain: epoch  4, batch     0 | loss: 2.0008757CurrentTrain: epoch  4, batch     1 | loss: 1.8854921CurrentTrain: epoch  5, batch     0 | loss: 1.8805761CurrentTrain: epoch  5, batch     1 | loss: 1.7872998CurrentTrain: epoch  6, batch     0 | loss: 1.7651229CurrentTrain: epoch  6, batch     1 | loss: 1.7746135CurrentTrain: epoch  7, batch     0 | loss: 1.7499874CurrentTrain: epoch  7, batch     1 | loss: 1.8031487CurrentTrain: epoch  8, batch     0 | loss: 1.7940961CurrentTrain: epoch  8, batch     1 | loss: 1.7529658CurrentTrain: epoch  9, batch     0 | loss: 1.7341144CurrentTrain: epoch  9, batch     1 | loss: 1.7452837
Mixup data size:  80
MixupTrain:  epoch  0, batch     4 | loss: 2.6516268
MemoryTrain:  epoch  0, batch     0 | loss: 2.2539210MemoryTrain:  epoch  0, batch     1 | loss: 1.0774344MemoryTrain:  epoch  1, batch     0 | loss: 1.8716333MemoryTrain:  epoch  1, batch     1 | loss: 3.1172788MemoryTrain:  epoch  2, batch     0 | loss: 1.8365430MemoryTrain:  epoch  2, batch     1 | loss: 1.2385353MemoryTrain:  epoch  3, batch     0 | loss: 1.3437580MemoryTrain:  epoch  3, batch     1 | loss: 0.3988969MemoryTrain:  epoch  4, batch     0 | loss: 0.8460469MemoryTrain:  epoch  4, batch     1 | loss: 0.7820584MemoryTrain:  epoch  5, batch     0 | loss: 0.9290185MemoryTrain:  epoch  5, batch     1 | loss: 0.4412849MemoryTrain:  epoch  6, batch     0 | loss: 0.2090106MemoryTrain:  epoch  6, batch     1 | loss: 1.9530731MemoryTrain:  epoch  7, batch     0 | loss: 0.3159391MemoryTrain:  epoch  7, batch     1 | loss: 1.0444243MemoryTrain:  epoch  8, batch     0 | loss: 0.5140505MemoryTrain:  epoch  8, batch     1 | loss: 0.1392953MemoryTrain:  epoch  9, batch     0 | loss: 0.3115117MemoryTrain:  epoch  9, batch     1 | loss: 0.5059592
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 5.8547521CurrentTrain: epoch  0, batch     1 | loss: 6.2380490CurrentTrain: epoch  1, batch     0 | loss: 5.0266614CurrentTrain: epoch  1, batch     1 | loss: 4.6216307CurrentTrain: epoch  2, batch     0 | loss: 4.4155445CurrentTrain: epoch  2, batch     1 | loss: 3.8196931CurrentTrain: epoch  3, batch     0 | loss: 3.6756392CurrentTrain: epoch  3, batch     1 | loss: 3.5096655CurrentTrain: epoch  4, batch     0 | loss: 3.4306860CurrentTrain: epoch  4, batch     1 | loss: 2.8361964CurrentTrain: epoch  5, batch     0 | loss: 3.0947635CurrentTrain: epoch  5, batch     1 | loss: 3.5747774CurrentTrain: epoch  6, batch     0 | loss: 2.9923406CurrentTrain: epoch  6, batch     1 | loss: 2.8492703CurrentTrain: epoch  7, batch     0 | loss: 2.6263385CurrentTrain: epoch  7, batch     1 | loss: 2.5610905CurrentTrain: epoch  8, batch     0 | loss: 2.5433526CurrentTrain: epoch  8, batch     1 | loss: 2.3794465CurrentTrain: epoch  9, batch     0 | loss: 2.3246894CurrentTrain: epoch  9, batch     1 | loss: 2.2746296
Mixup data size:  80
MixupTrain:  epoch  0, batch     4 | loss: 5.8121761
MemoryTrain:  epoch  0, batch     0 | loss: 2.2009633MemoryTrain:  epoch  0, batch     1 | loss: 1.2239343MemoryTrain:  epoch  1, batch     0 | loss: 2.6319609MemoryTrain:  epoch  1, batch     1 | loss: 0.6647556MemoryTrain:  epoch  2, batch     0 | loss: 1.4586272MemoryTrain:  epoch  2, batch     1 | loss: 2.3002038MemoryTrain:  epoch  3, batch     0 | loss: 1.5249557MemoryTrain:  epoch  3, batch     1 | loss: 1.7104501MemoryTrain:  epoch  4, batch     0 | loss: 1.2284560MemoryTrain:  epoch  4, batch     1 | loss: 0.9891303MemoryTrain:  epoch  5, batch     0 | loss: 1.1464214MemoryTrain:  epoch  5, batch     1 | loss: 0.5587536MemoryTrain:  epoch  6, batch     0 | loss: 1.2820232MemoryTrain:  epoch  6, batch     1 | loss: 0.9029040MemoryTrain:  epoch  7, batch     0 | loss: 0.8205755MemoryTrain:  epoch  7, batch     1 | loss: 0.6185288MemoryTrain:  epoch  8, batch     0 | loss: 1.0258508MemoryTrain:  epoch  8, batch     1 | loss: 0.1758184MemoryTrain:  epoch  9, batch     0 | loss: 0.7554879MemoryTrain:  epoch  9, batch     1 | loss: 0.3640015
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 97.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 97.66%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 95.14%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 86.16%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 9.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 21.09%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 29.17%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 35.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 41.48%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 45.31%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 45.67%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 42.41%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 41.67%   [EVAL] batch:   15 | acc: 25.00%,  total acc: 40.62%   [EVAL] batch:   16 | acc: 25.00%,  total acc: 39.71%   [EVAL] batch:   17 | acc: 37.50%,  total acc: 39.58%   [EVAL] batch:   18 | acc: 25.00%,  total acc: 38.82%   [EVAL] batch:   19 | acc: 43.75%,  total acc: 39.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 41.96%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 44.60%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 46.47%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 48.70%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 50.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 52.64%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 54.17%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 55.80%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 56.90%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 58.13%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 59.27%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 60.35%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 60.80%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 60.85%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 60.54%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 59.90%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 59.29%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 59.05%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 58.81%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 59.69%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 59.60%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 60.42%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 61.19%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 62.07%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 62.92%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 63.72%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 64.49%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 65.23%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 65.94%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 66.12%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 66.54%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 66.35%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 65.09%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 63.89%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 62.73%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 61.61%   [EVAL] batch:   56 | acc: 25.00%,  total acc: 60.96%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 61.53%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 61.97%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 62.60%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 63.22%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 63.81%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 64.38%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 64.94%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 65.48%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 65.34%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 65.49%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 65.81%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 65.94%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 66.07%   [EVAL] batch:   70 | acc: 31.25%,  total acc: 65.58%   
cur_acc_llm:  [0.8731060606060606, 0.9097222222222222, 0.2890625, 0.8616071428571429]
his_acc_llm:  [0.8731060606060606, 0.8525, 0.6962719298245614, 0.6558098591549296]
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 98.61%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 96.25%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 18.75%,  total acc: 82.21%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 79.02%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 34.38%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 36.25%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 36.46%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 43.75%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 50.00%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 55.56%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 58.13%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 61.36%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 63.54%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 63.46%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 61.16%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 62.08%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 61.72%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 63.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 66.76%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 68.21%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 70.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 72.69%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 73.66%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 74.35%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 75.40%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 75.98%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 74.81%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 72.61%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 70.71%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 68.75%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 66.89%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 65.30%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 64.58%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 65.31%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 64.63%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 65.18%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 65.70%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 66.34%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 67.08%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 67.80%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 68.48%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 69.77%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 70.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 70.47%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 70.31%   [EVAL] batch:   52 | acc: 25.00%,  total acc: 69.46%   [EVAL] batch:   53 | acc: 6.25%,  total acc: 68.29%   [EVAL] batch:   54 | acc: 6.25%,  total acc: 67.16%   [EVAL] batch:   55 | acc: 12.50%,  total acc: 66.18%   [EVAL] batch:   56 | acc: 25.00%,  total acc: 65.46%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 66.06%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 66.63%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 67.73%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 68.25%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 69.24%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 69.71%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 69.79%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 69.96%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 69.30%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 69.02%   [EVAL] batch:   69 | acc: 37.50%,  total acc: 68.57%   [EVAL] batch:   70 | acc: 12.50%,  total acc: 67.78%   
cur_acc:  ['0.8674', '0.6458', '0.3047', '0.7902']
his_acc:  ['0.8674', '0.7863', '0.6579', '0.6778']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 6.2426639CurrentTrain: epoch  0, batch     1 | loss: 4.5540462CurrentTrain: epoch  1, batch     0 | loss: 4.9219542CurrentTrain: epoch  1, batch     1 | loss: 4.9369450CurrentTrain: epoch  2, batch     0 | loss: 4.8245707CurrentTrain: epoch  2, batch     1 | loss: 4.0881190CurrentTrain: epoch  3, batch     0 | loss: 4.1741600CurrentTrain: epoch  3, batch     1 | loss: 3.9863603CurrentTrain: epoch  4, batch     0 | loss: 3.2728422CurrentTrain: epoch  4, batch     1 | loss: 4.3252606CurrentTrain: epoch  5, batch     0 | loss: 3.3330793CurrentTrain: epoch  5, batch     1 | loss: 3.3032184CurrentTrain: epoch  6, batch     0 | loss: 2.7969930CurrentTrain: epoch  6, batch     1 | loss: 3.3612301CurrentTrain: epoch  7, batch     0 | loss: 2.6825910CurrentTrain: epoch  7, batch     1 | loss: 3.0857131CurrentTrain: epoch  8, batch     0 | loss: 2.5582314CurrentTrain: epoch  8, batch     1 | loss: 2.5556734CurrentTrain: epoch  9, batch     0 | loss: 2.3468149CurrentTrain: epoch  9, batch     1 | loss: 2.4923379
Mixup data size:  91
MixupTrain:  epoch  0, batch     5 | loss: 2.6357871
MemoryTrain:  epoch  0, batch     0 | loss: 0.3460348MemoryTrain:  epoch  0, batch     1 | loss: 0.0732571MemoryTrain:  epoch  1, batch     0 | loss: 1.4391644MemoryTrain:  epoch  1, batch     1 | loss: 0.4902638MemoryTrain:  epoch  2, batch     0 | loss: 0.6481478MemoryTrain:  epoch  2, batch     1 | loss: 0.4798182MemoryTrain:  epoch  3, batch     0 | loss: 0.2432736MemoryTrain:  epoch  3, batch     1 | loss: 0.3121357MemoryTrain:  epoch  4, batch     0 | loss: 0.2160238MemoryTrain:  epoch  4, batch     1 | loss: 0.0664126MemoryTrain:  epoch  5, batch     0 | loss: 0.0723398MemoryTrain:  epoch  5, batch     1 | loss: 0.1822471MemoryTrain:  epoch  6, batch     0 | loss: 0.0750395MemoryTrain:  epoch  6, batch     1 | loss: 0.1519528MemoryTrain:  epoch  7, batch     0 | loss: 0.0552089MemoryTrain:  epoch  7, batch     1 | loss: 0.0592873MemoryTrain:  epoch  8, batch     0 | loss: 0.0504380MemoryTrain:  epoch  8, batch     1 | loss: 0.0756001MemoryTrain:  epoch  9, batch     0 | loss: 0.0492421MemoryTrain:  epoch  9, batch     1 | loss: 0.0324226
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 8.3253336CurrentTrain: epoch  0, batch     1 | loss: 8.4271669CurrentTrain: epoch  1, batch     0 | loss: 7.3676667CurrentTrain: epoch  1, batch     1 | loss: 6.9808311CurrentTrain: epoch  2, batch     0 | loss: 6.9580684CurrentTrain: epoch  2, batch     1 | loss: 5.8901863CurrentTrain: epoch  3, batch     0 | loss: 6.2986031CurrentTrain: epoch  3, batch     1 | loss: 6.0839148CurrentTrain: epoch  4, batch     0 | loss: 5.7470627CurrentTrain: epoch  4, batch     1 | loss: 5.0980816CurrentTrain: epoch  5, batch     0 | loss: 6.0377221CurrentTrain: epoch  5, batch     1 | loss: 4.3337817CurrentTrain: epoch  6, batch     0 | loss: 5.0920115CurrentTrain: epoch  6, batch     1 | loss: 4.3739405CurrentTrain: epoch  7, batch     0 | loss: 4.4716678CurrentTrain: epoch  7, batch     1 | loss: 4.5077286CurrentTrain: epoch  8, batch     0 | loss: 3.8229671CurrentTrain: epoch  8, batch     1 | loss: 4.7689815CurrentTrain: epoch  9, batch     0 | loss: 3.7403491CurrentTrain: epoch  9, batch     1 | loss: 3.8386667
Mixup data size:  91
MixupTrain:  epoch  0, batch     2 | loss: 4.7569893MixupTrain:  epoch  0, batch     3 | loss: 3.6172719MixupTrain:  epoch  0, batch     5 | loss: 3.3432740
MemoryTrain:  epoch  0, batch     0 | loss: 1.3787572MemoryTrain:  epoch  0, batch     1 | loss: 1.0031319MemoryTrain:  epoch  1, batch     0 | loss: 1.3228170MemoryTrain:  epoch  1, batch     1 | loss: 2.3759191MemoryTrain:  epoch  2, batch     0 | loss: 1.1667469MemoryTrain:  epoch  2, batch     1 | loss: 1.9463545MemoryTrain:  epoch  3, batch     0 | loss: 1.2419660MemoryTrain:  epoch  3, batch     1 | loss: 0.5806378MemoryTrain:  epoch  4, batch     0 | loss: 0.7197615MemoryTrain:  epoch  4, batch     1 | loss: 0.9624124MemoryTrain:  epoch  5, batch     0 | loss: 0.5685556MemoryTrain:  epoch  5, batch     1 | loss: 0.6072104MemoryTrain:  epoch  6, batch     0 | loss: 0.7182078MemoryTrain:  epoch  6, batch     1 | loss: 0.3266776MemoryTrain:  epoch  7, batch     0 | loss: 0.6289973MemoryTrain:  epoch  7, batch     1 | loss: 0.5775858MemoryTrain:  epoch  8, batch     0 | loss: 0.6330236MemoryTrain:  epoch  8, batch     1 | loss: 0.4766678MemoryTrain:  epoch  9, batch     0 | loss: 0.4596208MemoryTrain:  epoch  9, batch     1 | loss: 0.4093803
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 82.29%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 79.33%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 77.23%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 74.17%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 9.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 21.09%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 29.17%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 36.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 42.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 46.88%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 47.12%   [EVAL] batch:   13 | acc: 6.25%,  total acc: 44.20%   [EVAL] batch:   14 | acc: 12.50%,  total acc: 42.08%   [EVAL] batch:   15 | acc: 25.00%,  total acc: 41.02%   [EVAL] batch:   16 | acc: 25.00%,  total acc: 40.07%   [EVAL] batch:   17 | acc: 37.50%,  total acc: 39.93%   [EVAL] batch:   18 | acc: 25.00%,  total acc: 39.14%   [EVAL] batch:   19 | acc: 43.75%,  total acc: 39.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 42.26%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 44.89%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 47.01%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 49.22%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 51.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 53.12%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 54.63%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 57.33%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 58.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 59.88%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 60.74%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 60.23%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 59.19%   [EVAL] batch:   34 | acc: 18.75%,  total acc: 58.04%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 56.94%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 55.74%   [EVAL] batch:   37 | acc: 12.50%,  total acc: 54.61%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 54.17%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 55.16%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 54.73%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 54.76%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 55.23%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 55.97%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 56.94%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 57.88%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 58.78%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 59.64%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 60.46%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 60.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 61.15%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 61.06%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 59.91%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 58.80%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 57.73%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 56.70%   [EVAL] batch:   56 | acc: 25.00%,  total acc: 56.14%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 56.79%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 57.20%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 57.92%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 58.61%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 59.27%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 59.92%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 60.55%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 61.15%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 61.36%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 61.47%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 61.86%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 62.05%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 62.23%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 62.41%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 62.59%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 63.01%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 63.51%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 63.92%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 64.31%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 64.53%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 64.98%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 65.03%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 65.47%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 65.43%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 65.09%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 65.06%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 64.88%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 64.71%   
cur_acc_llm:  [0.8731060606060606, 0.9097222222222222, 0.2890625, 0.8616071428571429, 0.7416666666666667]
his_acc_llm:  [0.8731060606060606, 0.8525, 0.6962719298245614, 0.6558098591549296, 0.6470588235294118]
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 12.50%,  total acc: 77.84%   [EVAL] batch:   11 | acc: 12.50%,  total acc: 72.40%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 67.79%   [EVAL] batch:   13 | acc: 6.25%,  total acc: 63.39%   [EVAL] batch:   14 | acc: 6.25%,  total acc: 59.58%   
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 27.08%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 25.00%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 23.75%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 23.96%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 31.25%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 39.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 45.83%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 49.38%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 53.41%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 56.73%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 54.91%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 56.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 57.35%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 57.64%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 57.89%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 59.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 61.01%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 62.78%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 64.40%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 65.89%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 67.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 68.51%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 69.44%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 71.34%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 72.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 72.98%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 73.63%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 72.54%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 70.40%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 68.39%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 66.49%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 64.70%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 62.99%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 62.34%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 63.12%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 62.50%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 62.65%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 63.08%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 63.64%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 64.44%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 65.22%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 65.96%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 67.35%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 67.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 68.14%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 68.03%   [EVAL] batch:   52 | acc: 31.25%,  total acc: 67.33%   [EVAL] batch:   53 | acc: 6.25%,  total acc: 66.20%   [EVAL] batch:   54 | acc: 6.25%,  total acc: 65.11%   [EVAL] batch:   55 | acc: 6.25%,  total acc: 64.06%   [EVAL] batch:   56 | acc: 25.00%,  total acc: 63.38%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 64.01%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 64.51%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 65.10%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 65.68%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 66.23%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 66.77%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 67.19%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 67.69%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 67.61%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 67.82%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 67.37%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 67.21%   [EVAL] batch:   69 | acc: 43.75%,  total acc: 66.88%   [EVAL] batch:   70 | acc: 37.50%,  total acc: 66.46%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 66.49%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 66.87%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 67.06%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 67.33%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 67.60%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 67.94%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 68.35%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 68.59%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 68.98%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 68.44%   [EVAL] batch:   81 | acc: 6.25%,  total acc: 67.68%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 67.09%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 66.37%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 65.74%   
cur_acc:  ['0.8674', '0.6458', '0.3047', '0.7902', '0.5958']
his_acc:  ['0.8674', '0.7863', '0.6579', '0.6778', '0.6574']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 6.3373785CurrentTrain: epoch  0, batch     1 | loss: 5.8327928CurrentTrain: epoch  1, batch     0 | loss: 4.9859810CurrentTrain: epoch  1, batch     1 | loss: 4.1213422CurrentTrain: epoch  2, batch     0 | loss: 3.6795797CurrentTrain: epoch  2, batch     1 | loss: 4.4026284CurrentTrain: epoch  3, batch     0 | loss: 3.6826620CurrentTrain: epoch  3, batch     1 | loss: 2.8335621CurrentTrain: epoch  4, batch     0 | loss: 3.1183629CurrentTrain: epoch  4, batch     1 | loss: 2.5590940CurrentTrain: epoch  5, batch     0 | loss: 2.4699497CurrentTrain: epoch  5, batch     1 | loss: 2.3325732CurrentTrain: epoch  6, batch     0 | loss: 2.3872466CurrentTrain: epoch  6, batch     1 | loss: 2.5022273CurrentTrain: epoch  7, batch     0 | loss: 2.4521930CurrentTrain: epoch  7, batch     1 | loss: 2.0334537CurrentTrain: epoch  8, batch     0 | loss: 2.4691844CurrentTrain: epoch  8, batch     1 | loss: 2.0252681CurrentTrain: epoch  9, batch     0 | loss: 2.0391152CurrentTrain: epoch  9, batch     1 | loss: 2.3432133
Mixup data size:  101
MixupTrain:  epoch  0, batch     0 | loss: 2.9746959MixupTrain:  epoch  0, batch     1 | loss: 2.7976888MixupTrain:  epoch  0, batch     2 | loss: 2.3508772MixupTrain:  epoch  0, batch     3 | loss: 2.8367208MixupTrain:  epoch  0, batch     4 | loss: 2.7148193MixupTrain:  epoch  0, batch     5 | loss: 2.7757304MixupTrain:  epoch  0, batch     6 | loss: 2.5606351
MemoryTrain:  epoch  0, batch     0 | loss: 0.3649971MemoryTrain:  epoch  0, batch     1 | loss: 0.7881874MemoryTrain:  epoch  1, batch     0 | loss: 0.4435120MemoryTrain:  epoch  1, batch     1 | loss: 0.8170559MemoryTrain:  epoch  2, batch     0 | loss: 0.3021166MemoryTrain:  epoch  2, batch     1 | loss: 1.1459049MemoryTrain:  epoch  3, batch     0 | loss: 0.2777382MemoryTrain:  epoch  3, batch     1 | loss: 0.3976537MemoryTrain:  epoch  4, batch     0 | loss: 0.1875426MemoryTrain:  epoch  4, batch     1 | loss: 0.1012725MemoryTrain:  epoch  5, batch     0 | loss: 0.0629591MemoryTrain:  epoch  5, batch     1 | loss: 0.1693330MemoryTrain:  epoch  6, batch     0 | loss: 0.0706003MemoryTrain:  epoch  6, batch     1 | loss: 0.0786899MemoryTrain:  epoch  7, batch     0 | loss: 0.0250776MemoryTrain:  epoch  7, batch     1 | loss: 0.0898268MemoryTrain:  epoch  8, batch     0 | loss: 0.0308643MemoryTrain:  epoch  8, batch     1 | loss: 0.0394022MemoryTrain:  epoch  9, batch     0 | loss: 0.0909322MemoryTrain:  epoch  9, batch     1 | loss: 0.0252677
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 8.9490242CurrentTrain: epoch  0, batch     1 | loss: 9.0605106CurrentTrain: epoch  1, batch     0 | loss: 8.0467911CurrentTrain: epoch  1, batch     1 | loss: 7.0041881CurrentTrain: epoch  2, batch     0 | loss: 6.8475475CurrentTrain: epoch  2, batch     1 | loss: 6.5347261CurrentTrain: epoch  3, batch     0 | loss: 6.6286530CurrentTrain: epoch  3, batch     1 | loss: 5.6228490CurrentTrain: epoch  4, batch     0 | loss: 6.2556190CurrentTrain: epoch  4, batch     1 | loss: 4.9627056CurrentTrain: epoch  5, batch     0 | loss: 6.0215716CurrentTrain: epoch  5, batch     1 | loss: 4.3174591CurrentTrain: epoch  6, batch     0 | loss: 4.9713411CurrentTrain: epoch  6, batch     1 | loss: 4.4174232CurrentTrain: epoch  7, batch     0 | loss: 5.2278786CurrentTrain: epoch  7, batch     1 | loss: 3.4990528CurrentTrain: epoch  8, batch     0 | loss: 4.6000242CurrentTrain: epoch  8, batch     1 | loss: 3.8267288CurrentTrain: epoch  9, batch     0 | loss: 3.8812246CurrentTrain: epoch  9, batch     1 | loss: 3.3956201
Mixup data size:  100
MixupTrain:  epoch  0, batch     0 | loss: 4.8047588MixupTrain:  epoch  0, batch     1 | loss: 5.4412304MixupTrain:  epoch  0, batch     2 | loss: 5.0465047MixupTrain:  epoch  0, batch     3 | loss: 4.4932065MixupTrain:  epoch  0, batch     4 | loss: 3.5678614MixupTrain:  epoch  0, batch     5 | loss: 4.6330068MixupTrain:  epoch  0, batch     6 | loss: 3.3125643
MemoryTrain:  epoch  0, batch     0 | loss: 1.0339458MemoryTrain:  epoch  0, batch     1 | loss: 0.9783887MemoryTrain:  epoch  1, batch     0 | loss: 1.3965225MemoryTrain:  epoch  1, batch     1 | loss: 1.4624832MemoryTrain:  epoch  2, batch     0 | loss: 1.3260624MemoryTrain:  epoch  2, batch     1 | loss: 0.8528295MemoryTrain:  epoch  3, batch     0 | loss: 0.8362578MemoryTrain:  epoch  3, batch     1 | loss: 0.8563347MemoryTrain:  epoch  4, batch     0 | loss: 0.9193680MemoryTrain:  epoch  4, batch     1 | loss: 0.7335018MemoryTrain:  epoch  5, batch     0 | loss: 0.6578743MemoryTrain:  epoch  5, batch     1 | loss: 0.7758099MemoryTrain:  epoch  6, batch     0 | loss: 0.6533065MemoryTrain:  epoch  6, batch     1 | loss: 0.5513355MemoryTrain:  epoch  7, batch     0 | loss: 0.4689999MemoryTrain:  epoch  7, batch     1 | loss: 0.7123632MemoryTrain:  epoch  8, batch     0 | loss: 0.4664811MemoryTrain:  epoch  8, batch     1 | loss: 0.6569754MemoryTrain:  epoch  9, batch     0 | loss: 0.5362268MemoryTrain:  epoch  9, batch     1 | loss: 0.6084621
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 79.33%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 73.66%   
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 9.38%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 8.33%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 7.81%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 8.75%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 8.33%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 17.86%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 28.12%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 35.42%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 41.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 47.16%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 51.56%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 51.44%   [EVAL] batch:   13 | acc: 6.25%,  total acc: 48.21%   [EVAL] batch:   14 | acc: 37.50%,  total acc: 47.50%   [EVAL] batch:   15 | acc: 25.00%,  total acc: 46.09%   [EVAL] batch:   16 | acc: 25.00%,  total acc: 44.85%   [EVAL] batch:   17 | acc: 37.50%,  total acc: 44.44%   [EVAL] batch:   18 | acc: 25.00%,  total acc: 43.42%   [EVAL] batch:   19 | acc: 43.75%,  total acc: 43.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 46.13%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 48.58%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 50.54%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 52.60%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 54.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 57.64%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 59.15%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 60.34%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 61.46%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 63.48%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 64.02%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 63.97%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 63.57%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 63.02%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 62.16%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 62.01%   [EVAL] batch:   38 | acc: 43.75%,  total acc: 61.54%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 61.25%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 60.52%   [EVAL] batch:   41 | acc: 6.25%,  total acc: 59.23%   [EVAL] batch:   42 | acc: 18.75%,  total acc: 58.28%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 58.66%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 59.58%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 60.46%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 61.30%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 62.11%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 62.88%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 62.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 63.48%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 63.34%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 62.26%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 61.11%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 60.00%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 58.93%   [EVAL] batch:   56 | acc: 12.50%,  total acc: 58.11%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 58.41%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 58.26%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 58.96%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 59.63%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 60.28%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 60.91%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 61.52%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 62.12%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 62.22%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 61.85%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 61.95%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 62.14%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 62.23%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 62.24%   [EVAL] batch:   71 | acc: 43.75%,  total acc: 61.98%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 62.24%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 62.58%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 62.26%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 62.26%   [EVAL] batch:   78 | acc: 18.75%,  total acc: 61.71%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 62.19%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 61.65%   [EVAL] batch:   81 | acc: 6.25%,  total acc: 60.98%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 60.39%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 59.90%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 59.34%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 59.30%   [EVAL] batch:   86 | acc: 87.50%,  total acc: 59.63%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 59.80%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 59.76%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 59.93%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 60.16%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 60.39%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 60.82%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 61.17%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 61.45%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 61.65%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 61.86%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 61.93%   [EVAL] batch:   98 | acc: 0.00%,  total acc: 61.30%   
cur_acc_llm:  [0.8731060606060606, 0.9097222222222222, 0.2890625, 0.8616071428571429, 0.7416666666666667, 0.7366071428571429]
his_acc_llm:  [0.8731060606060606, 0.8525, 0.6962719298245614, 0.6558098591549296, 0.6470588235294118, 0.6130050505050505]
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 79.69%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 79.33%   [EVAL] batch:   13 | acc: 6.25%,  total acc: 74.11%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 47.92%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 45.31%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 45.00%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 45.83%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 51.79%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 57.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 65.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 67.61%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 69.27%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 66.96%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 67.08%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 66.41%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 66.91%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 66.45%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 67.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 69.05%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 70.45%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 74.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 75.69%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 77.16%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 77.50%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 77.82%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 78.32%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 77.08%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 74.82%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 72.68%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 70.66%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 68.75%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 66.94%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 66.03%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 66.56%   [EVAL] batch:   40 | acc: 18.75%,  total acc: 65.40%   [EVAL] batch:   41 | acc: 31.25%,  total acc: 64.58%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 64.24%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 64.49%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 65.28%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 66.03%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 66.76%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 67.45%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 68.11%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 68.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   52 | acc: 31.25%,  total acc: 68.04%   [EVAL] batch:   53 | acc: 6.25%,  total acc: 66.90%   [EVAL] batch:   54 | acc: 6.25%,  total acc: 65.80%   [EVAL] batch:   55 | acc: 6.25%,  total acc: 64.73%   [EVAL] batch:   56 | acc: 25.00%,  total acc: 64.04%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 64.66%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 65.15%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 65.73%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 66.29%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 66.83%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 67.36%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 67.77%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 68.27%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 68.18%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 68.19%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 67.46%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 67.39%   [EVAL] batch:   69 | acc: 43.75%,  total acc: 67.05%   [EVAL] batch:   70 | acc: 37.50%,  total acc: 66.64%   [EVAL] batch:   71 | acc: 37.50%,  total acc: 66.23%   [EVAL] batch:   72 | acc: 56.25%,  total acc: 66.10%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 65.88%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 65.75%   [EVAL] batch:   75 | acc: 50.00%,  total acc: 65.54%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 65.34%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 65.54%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 65.27%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 65.70%   [EVAL] batch:   80 | acc: 12.50%,  total acc: 65.05%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 64.25%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 63.48%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 62.72%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 62.06%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 62.21%   [EVAL] batch:   86 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 62.64%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 62.99%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 63.33%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 63.39%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 63.45%   [EVAL] batch:   92 | acc: 81.25%,  total acc: 63.64%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 63.90%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 64.08%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 64.26%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 64.24%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 64.29%   [EVAL] batch:   98 | acc: 6.25%,  total acc: 63.70%   
cur_acc:  ['0.8674', '0.6458', '0.3047', '0.7902', '0.5958', '0.7411']
his_acc:  ['0.8674', '0.7863', '0.6579', '0.6778', '0.6574', '0.6370']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 7.6522474CurrentTrain: epoch  0, batch     1 | loss: 7.4863911CurrentTrain: epoch  1, batch     0 | loss: 6.2941179CurrentTrain: epoch  1, batch     1 | loss: 6.1712270CurrentTrain: epoch  2, batch     0 | loss: 6.0101709CurrentTrain: epoch  2, batch     1 | loss: 5.4788833CurrentTrain: epoch  3, batch     0 | loss: 5.5050516CurrentTrain: epoch  3, batch     1 | loss: 5.7397723CurrentTrain: epoch  4, batch     0 | loss: 5.3243017CurrentTrain: epoch  4, batch     1 | loss: 3.7217162CurrentTrain: epoch  5, batch     0 | loss: 4.0436945CurrentTrain: epoch  5, batch     1 | loss: 4.9164476CurrentTrain: epoch  6, batch     0 | loss: 4.3905277CurrentTrain: epoch  6, batch     1 | loss: 3.1884069CurrentTrain: epoch  7, batch     0 | loss: 3.4824843CurrentTrain: epoch  7, batch     1 | loss: 3.1899559CurrentTrain: epoch  8, batch     0 | loss: 3.2043290CurrentTrain: epoch  8, batch     1 | loss: 2.7367833CurrentTrain: epoch  9, batch     0 | loss: 2.9128351CurrentTrain: epoch  9, batch     1 | loss: 2.4200287
Mixup data size:  110
MixupTrain:  epoch  0, batch     4 | loss: 2.4850373MixupTrain:  epoch  0, batch     6 | loss: 1.8440199
MemoryTrain:  epoch  0, batch     0 | loss: 0.8309698MemoryTrain:  epoch  0, batch     1 | loss: 0.3120776MemoryTrain:  epoch  0, batch     2 | loss: 0.7482637MemoryTrain:  epoch  1, batch     0 | loss: 0.9204454MemoryTrain:  epoch  1, batch     1 | loss: 1.0990505MemoryTrain:  epoch  1, batch     2 | loss: 0.4741973MemoryTrain:  epoch  2, batch     0 | loss: 0.4302500MemoryTrain:  epoch  2, batch     1 | loss: 0.7242975MemoryTrain:  epoch  2, batch     2 | loss: 0.0771205MemoryTrain:  epoch  3, batch     0 | loss: 0.0903357MemoryTrain:  epoch  3, batch     1 | loss: 0.3011117MemoryTrain:  epoch  3, batch     2 | loss: 0.7240580MemoryTrain:  epoch  4, batch     0 | loss: 0.1262901MemoryTrain:  epoch  4, batch     1 | loss: 0.2136364MemoryTrain:  epoch  4, batch     2 | loss: 0.4231969MemoryTrain:  epoch  5, batch     0 | loss: 0.1350679MemoryTrain:  epoch  5, batch     1 | loss: 0.0559883MemoryTrain:  epoch  5, batch     2 | loss: 0.3088658MemoryTrain:  epoch  6, batch     0 | loss: 0.1710740MemoryTrain:  epoch  6, batch     1 | loss: 0.2066132MemoryTrain:  epoch  6, batch     2 | loss: 0.0668494MemoryTrain:  epoch  7, batch     0 | loss: 0.2107621MemoryTrain:  epoch  7, batch     1 | loss: 0.6471586MemoryTrain:  epoch  7, batch     2 | loss: 0.0371638MemoryTrain:  epoch  8, batch     0 | loss: 0.3092691MemoryTrain:  epoch  8, batch     1 | loss: 0.0396450MemoryTrain:  epoch  8, batch     2 | loss: 0.0743788MemoryTrain:  epoch  9, batch     0 | loss: 0.0515167MemoryTrain:  epoch  9, batch     1 | loss: 0.0806803MemoryTrain:  epoch  9, batch     2 | loss: 0.0440917
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 10.1634216CurrentTrain: epoch  0, batch     1 | loss: 8.5070963CurrentTrain: epoch  1, batch     0 | loss: 9.8672714CurrentTrain: epoch  1, batch     1 | loss: 6.4895411CurrentTrain: epoch  2, batch     0 | loss: 7.9375010CurrentTrain: epoch  2, batch     1 | loss: 7.2089405CurrentTrain: epoch  3, batch     0 | loss: 7.0153093CurrentTrain: epoch  3, batch     1 | loss: 7.0211716CurrentTrain: epoch  4, batch     0 | loss: 7.6471648CurrentTrain: epoch  4, batch     1 | loss: 5.2687440CurrentTrain: epoch  5, batch     0 | loss: 5.7252598CurrentTrain: epoch  5, batch     1 | loss: 7.5291538CurrentTrain: epoch  6, batch     0 | loss: 5.9337349CurrentTrain: epoch  6, batch     1 | loss: 6.1048021CurrentTrain: epoch  7, batch     0 | loss: 6.3747292CurrentTrain: epoch  7, batch     1 | loss: 4.3056602CurrentTrain: epoch  8, batch     0 | loss: 5.8001690CurrentTrain: epoch  8, batch     1 | loss: 4.5311460CurrentTrain: epoch  9, batch     0 | loss: 5.1560488CurrentTrain: epoch  9, batch     1 | loss: 4.0552950
Mixup data size:  110
MixupTrain:  epoch  0, batch     0 | loss: 5.2447895MixupTrain:  epoch  0, batch     4 | loss: 3.9056673
MemoryTrain:  epoch  0, batch     0 | loss: 1.0545309MemoryTrain:  epoch  0, batch     1 | loss: 1.1550584MemoryTrain:  epoch  0, batch     2 | loss: 0.0796813MemoryTrain:  epoch  1, batch     0 | loss: 1.3548094MemoryTrain:  epoch  1, batch     1 | loss: 0.8713152MemoryTrain:  epoch  1, batch     2 | loss: 0.7414128MemoryTrain:  epoch  2, batch     0 | loss: 0.8875294MemoryTrain:  epoch  2, batch     1 | loss: 0.7453226MemoryTrain:  epoch  2, batch     2 | loss: 0.2535481MemoryTrain:  epoch  3, batch     0 | loss: 0.9760256MemoryTrain:  epoch  3, batch     1 | loss: 0.6303768MemoryTrain:  epoch  3, batch     2 | loss: 0.8320837MemoryTrain:  epoch  4, batch     0 | loss: 0.9652383MemoryTrain:  epoch  4, batch     1 | loss: 0.5524376MemoryTrain:  epoch  4, batch     2 | loss: 0.4106104MemoryTrain:  epoch  5, batch     0 | loss: 0.5077390MemoryTrain:  epoch  5, batch     1 | loss: 0.7641541MemoryTrain:  epoch  5, batch     2 | loss: 0.2142426MemoryTrain:  epoch  6, batch     0 | loss: 0.6064659MemoryTrain:  epoch  6, batch     1 | loss: 0.6405485MemoryTrain:  epoch  6, batch     2 | loss: 0.1867988MemoryTrain:  epoch  7, batch     0 | loss: 0.5795728MemoryTrain:  epoch  7, batch     1 | loss: 0.6449689MemoryTrain:  epoch  7, batch     2 | loss: 0.1027439MemoryTrain:  epoch  8, batch     0 | loss: 0.5424925MemoryTrain:  epoch  8, batch     1 | loss: 0.4010781MemoryTrain:  epoch  8, batch     2 | loss: 0.1729802MemoryTrain:  epoch  9, batch     0 | loss: 0.4436447MemoryTrain:  epoch  9, batch     1 | loss: 0.4575405MemoryTrain:  epoch  9, batch     2 | loss: 0.1539373
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 43.75%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 44.79%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 43.75%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 43.06%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 43.12%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 44.89%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 45.31%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 45.19%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 49.11%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 52.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 55.47%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 58.09%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 60.42%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 60.20%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 60.62%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 61.01%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 59.66%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 10.71%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 21.88%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 29.86%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 36.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 42.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 46.88%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 47.12%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 44.64%   [EVAL] batch:   14 | acc: 18.75%,  total acc: 42.92%   [EVAL] batch:   15 | acc: 37.50%,  total acc: 42.58%   [EVAL] batch:   16 | acc: 37.50%,  total acc: 42.28%   [EVAL] batch:   17 | acc: 37.50%,  total acc: 42.01%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 41.45%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 41.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 44.64%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 47.16%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 48.64%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 50.52%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 52.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 54.09%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 54.63%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 56.03%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 57.11%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 57.71%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 58.27%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 58.79%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 58.14%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 56.62%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 55.36%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 53.99%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 52.70%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 51.48%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 50.64%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 51.09%   [EVAL] batch:   40 | acc: 18.75%,  total acc: 50.30%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 50.45%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 50.73%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 51.42%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 52.50%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 53.53%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 54.52%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 55.47%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 56.38%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 56.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 57.23%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 57.21%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 56.25%   [EVAL] batch:   53 | acc: 6.25%,  total acc: 55.32%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 54.32%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 53.35%   [EVAL] batch:   56 | acc: 18.75%,  total acc: 52.74%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 53.34%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 53.50%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 54.27%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 55.02%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 55.75%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 56.35%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 57.03%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 57.69%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 57.39%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 56.53%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 56.34%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 56.61%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 56.88%   [EVAL] batch:   70 | acc: 37.50%,  total acc: 56.60%   [EVAL] batch:   71 | acc: 6.25%,  total acc: 55.90%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 55.48%   [EVAL] batch:   73 | acc: 12.50%,  total acc: 54.90%   [EVAL] batch:   74 | acc: 25.00%,  total acc: 54.50%   [EVAL] batch:   75 | acc: 25.00%,  total acc: 54.11%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 53.90%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 53.85%   [EVAL] batch:   78 | acc: 25.00%,  total acc: 53.48%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 54.06%   [EVAL] batch:   80 | acc: 12.50%,  total acc: 53.55%   [EVAL] batch:   81 | acc: 6.25%,  total acc: 52.97%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 52.33%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 51.71%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 51.25%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 51.60%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 51.15%   [EVAL] batch:   87 | acc: 6.25%,  total acc: 50.64%   [EVAL] batch:   88 | acc: 12.50%,  total acc: 50.21%   [EVAL] batch:   89 | acc: 25.00%,  total acc: 49.93%   [EVAL] batch:   90 | acc: 12.50%,  total acc: 49.52%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 49.52%   [EVAL] batch:   92 | acc: 81.25%,  total acc: 49.87%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 50.33%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 50.72%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 51.11%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 51.29%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 51.34%   [EVAL] batch:   98 | acc: 0.00%,  total acc: 50.82%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 51.00%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 50.99%   [EVAL] batch:  101 | acc: 50.00%,  total acc: 50.98%   [EVAL] batch:  102 | acc: 31.25%,  total acc: 50.79%   [EVAL] batch:  103 | acc: 37.50%,  total acc: 50.66%   [EVAL] batch:  104 | acc: 37.50%,  total acc: 50.54%   [EVAL] batch:  105 | acc: 43.75%,  total acc: 50.47%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 50.41%   [EVAL] batch:  107 | acc: 43.75%,  total acc: 50.35%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 50.34%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 50.40%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 50.34%   [EVAL] batch:  111 | acc: 68.75%,  total acc: 50.50%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 50.94%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 51.37%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 51.79%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 52.21%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 52.51%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 52.60%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 52.63%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 52.86%   [EVAL] batch:  120 | acc: 0.00%,  total acc: 52.43%   
cur_acc_llm:  [0.8731060606060606, 0.9097222222222222, 0.2890625, 0.8616071428571429, 0.7416666666666667, 0.7366071428571429, 0.5965909090909091]
his_acc_llm:  [0.8731060606060606, 0.8525, 0.6962719298245614, 0.6558098591549296, 0.6470588235294118, 0.6130050505050505, 0.5242768595041323]
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 18.75%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 15.62%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 13.75%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 12.50%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 16.07%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 24.22%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 30.56%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 35.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 39.77%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 42.71%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 44.23%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 48.21%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 50.83%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 53.52%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 58.33%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 58.22%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 57.81%   [EVAL] batch:   20 | acc: 50.00%,  total acc: 57.44%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 56.53%   
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 47.92%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 42.19%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 38.75%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 39.58%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 45.54%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 52.34%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 57.64%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 61.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 64.20%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 66.15%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 66.35%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 64.73%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 65.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 64.45%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 65.07%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 64.93%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 64.80%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 65.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 67.56%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 69.03%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 70.38%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 71.61%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 72.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.80%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 74.54%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.45%   [EVAL] batch:   28 | acc: 81.25%,  total acc: 75.65%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 75.83%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 76.41%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 76.76%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 75.57%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 73.35%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 71.25%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 69.27%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 67.40%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 65.62%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 64.90%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 65.47%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 64.63%   [EVAL] batch:   41 | acc: 25.00%,  total acc: 63.69%   [EVAL] batch:   42 | acc: 37.50%,  total acc: 63.08%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 63.35%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 64.17%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 65.69%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 67.09%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 67.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 67.77%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 67.67%   [EVAL] batch:   52 | acc: 18.75%,  total acc: 66.75%   [EVAL] batch:   53 | acc: 6.25%,  total acc: 65.62%   [EVAL] batch:   54 | acc: 6.25%,  total acc: 64.55%   [EVAL] batch:   55 | acc: 6.25%,  total acc: 63.50%   [EVAL] batch:   56 | acc: 25.00%,  total acc: 62.83%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 63.47%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 63.88%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 64.48%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 65.06%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 66.17%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 66.70%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 67.21%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 67.14%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 66.51%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 65.72%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 65.67%   [EVAL] batch:   69 | acc: 50.00%,  total acc: 65.45%   [EVAL] batch:   70 | acc: 31.25%,  total acc: 64.96%   [EVAL] batch:   71 | acc: 43.75%,  total acc: 64.67%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 64.73%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 64.70%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 64.75%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 64.72%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 64.61%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 64.82%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 64.64%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 65.00%   [EVAL] batch:   80 | acc: 12.50%,  total acc: 64.35%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 63.57%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 62.80%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 62.05%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 61.40%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 61.48%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 61.35%   [EVAL] batch:   87 | acc: 37.50%,  total acc: 61.08%   [EVAL] batch:   88 | acc: 43.75%,  total acc: 60.88%   [EVAL] batch:   89 | acc: 43.75%,  total acc: 60.69%   [EVAL] batch:   90 | acc: 18.75%,  total acc: 60.23%   [EVAL] batch:   91 | acc: 43.75%,  total acc: 60.05%   [EVAL] batch:   92 | acc: 81.25%,  total acc: 60.28%   [EVAL] batch:   93 | acc: 81.25%,  total acc: 60.51%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 60.66%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 60.87%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 60.89%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 60.84%   [EVAL] batch:   98 | acc: 6.25%,  total acc: 60.29%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 59.94%   [EVAL] batch:  100 | acc: 25.00%,  total acc: 59.59%   [EVAL] batch:  101 | acc: 6.25%,  total acc: 59.07%   [EVAL] batch:  102 | acc: 6.25%,  total acc: 58.56%   [EVAL] batch:  103 | acc: 6.25%,  total acc: 58.05%   [EVAL] batch:  104 | acc: 6.25%,  total acc: 57.56%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 57.72%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 57.83%   [EVAL] batch:  107 | acc: 87.50%,  total acc: 58.10%   [EVAL] batch:  108 | acc: 87.50%,  total acc: 58.37%   [EVAL] batch:  109 | acc: 81.25%,  total acc: 58.58%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 58.56%   [EVAL] batch:  111 | acc: 81.25%,  total acc: 58.76%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 59.07%   [EVAL] batch:  113 | acc: 93.75%,  total acc: 59.38%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 59.67%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 59.97%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 60.10%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 60.22%   [EVAL] batch:  118 | acc: 37.50%,  total acc: 60.03%   [EVAL] batch:  119 | acc: 62.50%,  total acc: 60.05%   [EVAL] batch:  120 | acc: 6.25%,  total acc: 59.61%   
cur_acc:  ['0.8674', '0.6458', '0.3047', '0.7902', '0.5958', '0.7411', '0.5653']
his_acc:  ['0.8674', '0.7863', '0.6579', '0.6778', '0.6574', '0.6370', '0.5961']
start training teacher model
CurrentTrain: epoch  0, batch     0 | loss: 4.8348427CurrentTrain: epoch  0, batch     1 | loss: 5.6463718CurrentTrain: epoch  1, batch     0 | loss: 3.6776433CurrentTrain: epoch  1, batch     1 | loss: 3.4763281CurrentTrain: epoch  2, batch     0 | loss: 3.0295224CurrentTrain: epoch  2, batch     1 | loss: 2.6775758CurrentTrain: epoch  3, batch     0 | loss: 2.5253944CurrentTrain: epoch  3, batch     1 | loss: 2.1766536CurrentTrain: epoch  4, batch     0 | loss: 2.1719398CurrentTrain: epoch  4, batch     1 | loss: 2.3265369CurrentTrain: epoch  5, batch     0 | loss: 2.1367569CurrentTrain: epoch  5, batch     1 | loss: 2.1709752CurrentTrain: epoch  6, batch     0 | loss: 2.0212159CurrentTrain: epoch  6, batch     1 | loss: 1.8372304CurrentTrain: epoch  7, batch     0 | loss: 1.9100749CurrentTrain: epoch  7, batch     1 | loss: 1.8316674CurrentTrain: epoch  8, batch     0 | loss: 1.7977647CurrentTrain: epoch  8, batch     1 | loss: 1.8471181CurrentTrain: epoch  9, batch     0 | loss: 1.8146738CurrentTrain: epoch  9, batch     1 | loss: 1.8412917
Mixup data size:  120
MixupTrain:  epoch  0, batch     2 | loss: 1.6370356
MemoryTrain:  epoch  0, batch     0 | loss: 0.1910701MemoryTrain:  epoch  0, batch     1 | loss: 0.5806023MemoryTrain:  epoch  0, batch     2 | loss: 0.5704492MemoryTrain:  epoch  1, batch     0 | loss: 0.5893779MemoryTrain:  epoch  1, batch     1 | loss: 1.5572097MemoryTrain:  epoch  1, batch     2 | loss: 0.7692600MemoryTrain:  epoch  2, batch     0 | loss: 0.1644556MemoryTrain:  epoch  2, batch     1 | loss: 0.7107400MemoryTrain:  epoch  2, batch     2 | loss: 1.2250077MemoryTrain:  epoch  3, batch     0 | loss: 0.1017142MemoryTrain:  epoch  3, batch     1 | loss: 0.1285667MemoryTrain:  epoch  3, batch     2 | loss: 0.1195554MemoryTrain:  epoch  4, batch     0 | loss: 0.1580521MemoryTrain:  epoch  4, batch     1 | loss: 0.0894464MemoryTrain:  epoch  4, batch     2 | loss: 1.0979820MemoryTrain:  epoch  5, batch     0 | loss: 0.4424010MemoryTrain:  epoch  5, batch     1 | loss: 0.0809309MemoryTrain:  epoch  5, batch     2 | loss: 0.2483318MemoryTrain:  epoch  6, batch     0 | loss: 0.0435769MemoryTrain:  epoch  6, batch     1 | loss: 0.0724713MemoryTrain:  epoch  6, batch     2 | loss: 0.0905908MemoryTrain:  epoch  7, batch     0 | loss: 0.0599928MemoryTrain:  epoch  7, batch     1 | loss: 0.0480609MemoryTrain:  epoch  7, batch     2 | loss: 0.0790865MemoryTrain:  epoch  8, batch     0 | loss: 0.1011101MemoryTrain:  epoch  8, batch     1 | loss: 0.0395848MemoryTrain:  epoch  8, batch     2 | loss: 0.0603677MemoryTrain:  epoch  9, batch     0 | loss: 0.0367064MemoryTrain:  epoch  9, batch     1 | loss: 0.0267656MemoryTrain:  epoch  9, batch     2 | loss: 0.0619833
Start training student model
CurrentTrain: epoch  0, batch     0 | loss: 5.8844781CurrentTrain: epoch  0, batch     1 | loss: 6.2966671CurrentTrain: epoch  1, batch     0 | loss: 4.6399460CurrentTrain: epoch  1, batch     1 | loss: 3.6797926CurrentTrain: epoch  2, batch     0 | loss: 3.8118372CurrentTrain: epoch  2, batch     1 | loss: 3.2216928CurrentTrain: epoch  3, batch     0 | loss: 3.3965359CurrentTrain: epoch  3, batch     1 | loss: 2.8467333CurrentTrain: epoch  4, batch     0 | loss: 2.9642973CurrentTrain: epoch  4, batch     1 | loss: 2.6749806CurrentTrain: epoch  5, batch     0 | loss: 2.7090130CurrentTrain: epoch  5, batch     1 | loss: 2.3634372CurrentTrain: epoch  6, batch     0 | loss: 2.6330106CurrentTrain: epoch  6, batch     1 | loss: 2.2808323CurrentTrain: epoch  7, batch     0 | loss: 2.4636700CurrentTrain: epoch  7, batch     1 | loss: 2.2094870CurrentTrain: epoch  8, batch     0 | loss: 2.2804830CurrentTrain: epoch  8, batch     1 | loss: 2.2370520CurrentTrain: epoch  9, batch     0 | loss: 2.1717081CurrentTrain: epoch  9, batch     1 | loss: 2.0040302
Mixup data size:  120
MixupTrain:  epoch  0, batch     1 | loss: 3.5486033MixupTrain:  epoch  0, batch     5 | loss: 3.3345158
MemoryTrain:  epoch  0, batch     0 | loss: 1.1134503MemoryTrain:  epoch  0, batch     1 | loss: 0.4122209MemoryTrain:  epoch  0, batch     2 | loss: 0.7088373MemoryTrain:  epoch  1, batch     0 | loss: 0.6979725MemoryTrain:  epoch  1, batch     1 | loss: 1.1674608MemoryTrain:  epoch  1, batch     2 | loss: 1.0841870MemoryTrain:  epoch  2, batch     0 | loss: 0.4828734MemoryTrain:  epoch  2, batch     1 | loss: 0.9335461MemoryTrain:  epoch  2, batch     2 | loss: 0.9827978MemoryTrain:  epoch  3, batch     0 | loss: 0.4635749MemoryTrain:  epoch  3, batch     1 | loss: 0.8415446MemoryTrain:  epoch  3, batch     2 | loss: 0.3239551MemoryTrain:  epoch  4, batch     0 | loss: 0.6953256MemoryTrain:  epoch  4, batch     1 | loss: 0.5150650MemoryTrain:  epoch  4, batch     2 | loss: 0.4383588MemoryTrain:  epoch  5, batch     0 | loss: 0.5824950MemoryTrain:  epoch  5, batch     1 | loss: 0.6854308MemoryTrain:  epoch  5, batch     2 | loss: 0.2399304MemoryTrain:  epoch  6, batch     0 | loss: 0.6129747MemoryTrain:  epoch  6, batch     1 | loss: 0.3665803MemoryTrain:  epoch  6, batch     2 | loss: 0.5424666MemoryTrain:  epoch  7, batch     0 | loss: 0.3595273MemoryTrain:  epoch  7, batch     1 | loss: 0.6518349MemoryTrain:  epoch  7, batch     2 | loss: 0.3705709MemoryTrain:  epoch  8, batch     0 | loss: 0.2690609MemoryTrain:  epoch  8, batch     1 | loss: 0.4102056MemoryTrain:  epoch  8, batch     2 | loss: 0.2284318MemoryTrain:  epoch  9, batch     0 | loss: 0.3612522MemoryTrain:  epoch  9, batch     1 | loss: 0.3397512MemoryTrain:  epoch  9, batch     2 | loss: 0.3895643
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 78.98%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 77.88%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 3.12%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 6.25%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 4.69%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 5.00%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 4.17%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 14.29%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 25.00%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 32.64%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 38.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 44.32%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 48.96%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 49.04%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 46.43%   [EVAL] batch:   14 | acc: 37.50%,  total acc: 45.83%   [EVAL] batch:   15 | acc: 37.50%,  total acc: 45.31%   [EVAL] batch:   16 | acc: 37.50%,  total acc: 44.85%   [EVAL] batch:   17 | acc: 37.50%,  total acc: 44.44%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 43.75%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 44.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 46.73%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 49.15%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 51.09%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 53.12%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 55.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 56.73%   [EVAL] batch:   26 | acc: 62.50%,  total acc: 56.94%   [EVAL] batch:   27 | acc: 62.50%,  total acc: 57.14%   [EVAL] batch:   28 | acc: 81.25%,  total acc: 57.97%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 58.33%   [EVAL] batch:   30 | acc: 50.00%,  total acc: 58.06%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 58.20%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 57.58%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 56.25%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 54.64%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 53.30%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 52.03%   [EVAL] batch:   37 | acc: 12.50%,  total acc: 50.99%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 50.16%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 50.94%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 50.61%   [EVAL] batch:   41 | acc: 18.75%,  total acc: 49.85%   [EVAL] batch:   42 | acc: 18.75%,  total acc: 49.13%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 49.57%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 50.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 51.63%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 52.66%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 53.65%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 54.59%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 54.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 55.51%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 55.53%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 54.60%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 53.59%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 52.61%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 51.67%   [EVAL] batch:   56 | acc: 12.50%,  total acc: 50.99%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 51.40%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 51.59%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 52.40%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 53.18%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 53.93%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 54.66%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 55.37%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 56.06%   [EVAL] batch:   65 | acc: 31.25%,  total acc: 55.68%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 54.85%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 54.69%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 55.07%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 55.36%   [EVAL] batch:   70 | acc: 37.50%,  total acc: 55.11%   [EVAL] batch:   71 | acc: 12.50%,  total acc: 54.51%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 54.11%   [EVAL] batch:   73 | acc: 12.50%,  total acc: 53.55%   [EVAL] batch:   74 | acc: 31.25%,  total acc: 53.25%   [EVAL] batch:   75 | acc: 18.75%,  total acc: 52.80%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 52.60%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 52.72%   [EVAL] batch:   78 | acc: 25.00%,  total acc: 52.37%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 52.97%   [EVAL] batch:   80 | acc: 12.50%,  total acc: 52.47%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 51.83%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 51.20%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 50.60%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 50.07%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 50.29%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 49.86%   [EVAL] batch:   87 | acc: 0.00%,  total acc: 49.29%   [EVAL] batch:   88 | acc: 6.25%,  total acc: 48.81%   [EVAL] batch:   89 | acc: 12.50%,  total acc: 48.40%   [EVAL] batch:   90 | acc: 6.25%,  total acc: 47.94%   [EVAL] batch:   91 | acc: 43.75%,  total acc: 47.89%   [EVAL] batch:   92 | acc: 62.50%,  total acc: 48.05%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 48.47%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 48.82%   [EVAL] batch:   95 | acc: 75.00%,  total acc: 49.09%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 49.23%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 49.30%   [EVAL] batch:   98 | acc: 0.00%,  total acc: 48.80%   [EVAL] batch:   99 | acc: 56.25%,  total acc: 48.88%   [EVAL] batch:  100 | acc: 37.50%,  total acc: 48.76%   [EVAL] batch:  101 | acc: 43.75%,  total acc: 48.71%   [EVAL] batch:  102 | acc: 37.50%,  total acc: 48.60%   [EVAL] batch:  103 | acc: 43.75%,  total acc: 48.56%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 48.69%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 48.76%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 48.95%   [EVAL] batch:  107 | acc: 75.00%,  total acc: 49.19%   [EVAL] batch:  108 | acc: 81.25%,  total acc: 49.48%   [EVAL] batch:  109 | acc: 75.00%,  total acc: 49.72%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 49.89%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 50.22%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 50.27%   [EVAL] batch:  114 | acc: 37.50%,  total acc: 50.16%   [EVAL] batch:  115 | acc: 50.00%,  total acc: 50.16%   [EVAL] batch:  116 | acc: 43.75%,  total acc: 50.11%   [EVAL] batch:  117 | acc: 6.25%,  total acc: 49.74%   [EVAL] batch:  118 | acc: 6.25%,  total acc: 49.37%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 48.96%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 48.92%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 49.18%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 49.39%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 49.60%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 49.95%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 50.35%   [EVAL] batch:  126 | acc: 100.00%,  total acc: 50.74%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 50.98%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 51.02%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 51.15%   [EVAL] batch:  130 | acc: 87.50%,  total acc: 51.43%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 51.75%   [EVAL] batch:  132 | acc: 56.25%,  total acc: 51.79%   
cur_acc_llm:  [0.8731060606060606, 0.9097222222222222, 0.2890625, 0.8616071428571429, 0.7416666666666667, 0.7366071428571429, 0.5965909090909091, 0.7788461538461539]
his_acc_llm:  [0.8731060606060606, 0.8525, 0.6962719298245614, 0.6558098591549296, 0.6470588235294118, 0.6130050505050505, 0.5242768595041323, 0.5178571428571429]
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 45.83%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 48.44%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 47.50%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 47.92%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 49.11%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 53.12%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 55.56%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 56.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 60.23%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 61.54%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 35.42%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 31.25%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 28.75%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 29.17%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 36.61%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 44.53%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 50.69%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 55.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 58.52%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 60.94%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 61.54%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 60.27%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 61.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 60.94%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 61.76%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 61.81%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 61.84%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 63.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 64.88%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 66.48%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 69.27%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 70.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 71.63%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 72.22%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 72.99%   [EVAL] batch:   28 | acc: 81.25%,  total acc: 73.28%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 73.75%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 73.99%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 74.22%   [EVAL] batch:   32 | acc: 31.25%,  total acc: 72.92%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 70.77%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 68.75%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 66.84%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 65.03%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 63.32%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 62.34%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 62.97%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 62.35%   [EVAL] batch:   41 | acc: 6.25%,  total acc: 61.01%   [EVAL] batch:   42 | acc: 6.25%,  total acc: 59.74%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 59.80%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 60.69%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 61.41%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 62.10%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 62.89%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 63.65%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 63.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 64.34%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 64.42%   [EVAL] batch:   52 | acc: 25.00%,  total acc: 63.68%   [EVAL] batch:   53 | acc: 6.25%,  total acc: 62.62%   [EVAL] batch:   54 | acc: 6.25%,  total acc: 61.59%   [EVAL] batch:   55 | acc: 6.25%,  total acc: 60.60%   [EVAL] batch:   56 | acc: 25.00%,  total acc: 59.98%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 60.67%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 61.23%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 61.88%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 63.10%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 63.69%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 64.26%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 64.81%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 64.39%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 63.62%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 62.87%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 62.77%   [EVAL] batch:   69 | acc: 50.00%,  total acc: 62.59%   [EVAL] batch:   70 | acc: 37.50%,  total acc: 62.24%   [EVAL] batch:   71 | acc: 25.00%,  total acc: 61.72%   [EVAL] batch:   72 | acc: 31.25%,  total acc: 61.30%   [EVAL] batch:   73 | acc: 31.25%,  total acc: 60.90%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 60.92%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 60.94%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 60.96%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 61.22%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 61.08%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 61.48%   [EVAL] batch:   80 | acc: 12.50%,  total acc: 60.88%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 60.14%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 59.41%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 58.71%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 58.09%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 58.21%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 57.54%   [EVAL] batch:   87 | acc: 0.00%,  total acc: 56.89%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 56.25%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 55.62%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 55.01%   [EVAL] batch:   91 | acc: 25.00%,  total acc: 54.69%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 54.91%   [EVAL] batch:   93 | acc: 81.25%,  total acc: 55.19%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 55.33%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 55.60%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 55.67%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 55.80%   [EVAL] batch:   98 | acc: 6.25%,  total acc: 55.30%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 55.06%   [EVAL] batch:  100 | acc: 37.50%,  total acc: 54.89%   [EVAL] batch:  101 | acc: 6.25%,  total acc: 54.41%   [EVAL] batch:  102 | acc: 6.25%,  total acc: 53.94%   [EVAL] batch:  103 | acc: 6.25%,  total acc: 53.49%   [EVAL] batch:  104 | acc: 18.75%,  total acc: 53.15%   [EVAL] batch:  105 | acc: 50.00%,  total acc: 53.12%   [EVAL] batch:  106 | acc: 75.00%,  total acc: 53.33%   [EVAL] batch:  107 | acc: 75.00%,  total acc: 53.53%   [EVAL] batch:  108 | acc: 68.75%,  total acc: 53.67%   [EVAL] batch:  109 | acc: 68.75%,  total acc: 53.81%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 53.77%   [EVAL] batch:  111 | acc: 81.25%,  total acc: 54.02%   [EVAL] batch:  112 | acc: 81.25%,  total acc: 54.26%   [EVAL] batch:  113 | acc: 75.00%,  total acc: 54.44%   [EVAL] batch:  114 | acc: 62.50%,  total acc: 54.51%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 54.69%   [EVAL] batch:  116 | acc: 43.75%,  total acc: 54.59%   [EVAL] batch:  117 | acc: 0.00%,  total acc: 54.13%   [EVAL] batch:  118 | acc: 0.00%,  total acc: 53.68%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 53.23%   [EVAL] batch:  120 | acc: 31.25%,  total acc: 53.05%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 53.02%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 53.05%   [EVAL] batch:  123 | acc: 50.00%,  total acc: 53.02%   [EVAL] batch:  124 | acc: 50.00%,  total acc: 53.00%   [EVAL] batch:  125 | acc: 43.75%,  total acc: 52.93%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 52.95%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 53.17%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 53.39%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 53.46%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 53.82%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 54.02%   [EVAL] batch:  132 | acc: 56.25%,  total acc: 54.04%   
cur_acc:  ['0.8674', '0.6458', '0.3047', '0.7902', '0.5958', '0.7411', '0.5653', '0.6154']
his_acc:  ['0.8674', '0.7863', '0.6579', '0.6778', '0.6574', '0.6370', '0.5961', '0.5404']
----------END
his_acc mean:  [0.8684 0.7751 0.6906 0.6611 0.6045 0.5753 0.5358 0.5147]
