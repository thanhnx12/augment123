#############params############
cuda
Task=Tacred, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=0, gen_num=0
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 1 1 0]
Losses:  11.742430686950684 11.097755432128906 0.6446751952171326
CurrentTrain: epoch  0, batch     0 | loss: 11.7424307Losses:  10.333988189697266 9.687150955200195 0.6468377113342285
CurrentTrain: epoch  0, batch     1 | loss: 10.3339882Losses:  9.682689666748047 9.051858901977539 0.630831241607666
CurrentTrain: epoch  0, batch     2 | loss: 9.6826897Losses:  10.841571807861328 10.192617416381836 0.6489545702934265
CurrentTrain: epoch  0, batch     3 | loss: 10.8415718Losses:  10.635313034057617 10.010042190551758 0.6252712607383728
CurrentTrain: epoch  0, batch     4 | loss: 10.6353130Losses:  8.106466293334961 7.483902931213379 0.6225630044937134
CurrentTrain: epoch  0, batch     5 | loss: 8.1064663Losses:  8.164183616638184 7.538421630859375 0.6257620453834534
CurrentTrain: epoch  0, batch     6 | loss: 8.1641836Losses:  9.709968566894531 9.102129936218262 0.6078383326530457
CurrentTrain: epoch  0, batch     7 | loss: 9.7099686Losses:  10.889812469482422 10.285554885864258 0.6042577624320984
CurrentTrain: epoch  0, batch     8 | loss: 10.8898125Losses:  9.45296573638916 8.871994018554688 0.5809720754623413
CurrentTrain: epoch  0, batch     9 | loss: 9.4529657Losses:  8.931120872497559 8.325279235839844 0.6058417558670044
CurrentTrain: epoch  0, batch    10 | loss: 8.9311209Losses:  8.045401573181152 7.440408706665039 0.6049928069114685
CurrentTrain: epoch  0, batch    11 | loss: 8.0454016Losses:  8.99172306060791 8.405635833740234 0.5860869884490967
CurrentTrain: epoch  0, batch    12 | loss: 8.9917231Losses:  9.016788482666016 8.4634370803833 0.553351879119873
CurrentTrain: epoch  0, batch    13 | loss: 9.0167885Losses:  9.909475326538086 9.357450485229492 0.5520252585411072
CurrentTrain: epoch  0, batch    14 | loss: 9.9094753Losses:  7.927152156829834 7.368189334869385 0.5589627027511597
CurrentTrain: epoch  0, batch    15 | loss: 7.9271522Losses:  11.065825462341309 10.53089714050293 0.534928560256958
CurrentTrain: epoch  0, batch    16 | loss: 11.0658255Losses:  13.334943771362305 12.863218307495117 0.4717256426811218
CurrentTrain: epoch  0, batch    17 | loss: 13.3349438Losses:  14.4230318069458 13.917756080627441 0.5052757859230042
CurrentTrain: epoch  0, batch    18 | loss: 14.4230318Losses:  8.781991958618164 8.267520904541016 0.5144706964492798
CurrentTrain: epoch  0, batch    19 | loss: 8.7819920Losses:  8.671622276306152 8.172554969787598 0.4990672767162323
CurrentTrain: epoch  0, batch    20 | loss: 8.6716223Losses:  10.48132038116455 9.972290992736816 0.5090292692184448
CurrentTrain: epoch  0, batch    21 | loss: 10.4813204Losses:  9.852076530456543 9.413785934448242 0.4382905066013336
CurrentTrain: epoch  0, batch    22 | loss: 9.8520765Losses:  8.920172691345215 8.483912467956543 0.4362604022026062
CurrentTrain: epoch  0, batch    23 | loss: 8.9201727Losses:  8.891386985778809 8.488458633422852 0.4029283821582794
CurrentTrain: epoch  0, batch    24 | loss: 8.8913870Losses:  7.829853534698486 7.421664714813232 0.40818867087364197
CurrentTrain: epoch  0, batch    25 | loss: 7.8298535Losses:  10.031412124633789 9.591859817504883 0.43955209851264954
CurrentTrain: epoch  0, batch    26 | loss: 10.0314121Losses:  8.648688316345215 8.319355010986328 0.3293328881263733
CurrentTrain: epoch  0, batch    27 | loss: 8.6486883Losses:  10.338783264160156 9.935342788696289 0.40344059467315674
CurrentTrain: epoch  0, batch    28 | loss: 10.3387833Losses:  7.166714668273926 6.79232120513916 0.37439343333244324
CurrentTrain: epoch  0, batch    29 | loss: 7.1667147Losses:  10.418858528137207 10.084404945373535 0.3344535827636719
CurrentTrain: epoch  0, batch    30 | loss: 10.4188585Losses:  8.583473205566406 8.216657638549805 0.3668159246444702
CurrentTrain: epoch  0, batch    31 | loss: 8.5834732Losses:  8.908548355102539 8.597274780273438 0.3112732768058777
CurrentTrain: epoch  0, batch    32 | loss: 8.9085484Losses:  8.392274856567383 8.07878303527832 0.3134922981262207
CurrentTrain: epoch  0, batch    33 | loss: 8.3922749Losses:  11.717947959899902 11.493783950805664 0.2241644561290741
CurrentTrain: epoch  0, batch    34 | loss: 11.7179480Losses:  10.469712257385254 10.185918807983398 0.283793568611145
CurrentTrain: epoch  0, batch    35 | loss: 10.4697123Losses:  8.434896469116211 8.222024917602539 0.21287187933921814
CurrentTrain: epoch  0, batch    36 | loss: 8.4348965Losses:  1.9561777114868164 1.72908353805542 0.22709420323371887
CurrentTrain: epoch  0, batch    37 | loss: 1.9561777Losses:  6.664836883544922 6.4722771644592285 0.19255989789962769
CurrentTrain: epoch  1, batch     0 | loss: 6.6648369Losses:  6.506313800811768 6.318108558654785 0.18820501863956451
CurrentTrain: epoch  1, batch     1 | loss: 6.5063138Losses:  7.79002571105957 7.620945930480957 0.16907986998558044
CurrentTrain: epoch  1, batch     2 | loss: 7.7900257Losses:  5.825500011444092 5.651247978210449 0.17425213754177094
CurrentTrain: epoch  1, batch     3 | loss: 5.8255000Losses:  7.056986331939697 6.884312629699707 0.17267349362373352
CurrentTrain: epoch  1, batch     4 | loss: 7.0569863Losses:  9.780126571655273 9.659622192382812 0.12050392478704453
CurrentTrain: epoch  1, batch     5 | loss: 9.7801266Losses:  7.229093074798584 7.091419219970703 0.13767394423484802
CurrentTrain: epoch  1, batch     6 | loss: 7.2290931Losses:  7.870462894439697 7.680586338043213 0.1898764818906784
CurrentTrain: epoch  1, batch     7 | loss: 7.8704629Losses:  8.798274040222168 8.674181938171387 0.12409193813800812
CurrentTrain: epoch  1, batch     8 | loss: 8.7982740Losses:  6.454392910003662 6.336073398590088 0.1183195412158966
CurrentTrain: epoch  1, batch     9 | loss: 6.4543929Losses:  8.940353393554688 8.790189743041992 0.15016400814056396
CurrentTrain: epoch  1, batch    10 | loss: 8.9403534Losses:  5.65721321105957 5.558161735534668 0.09905146062374115
CurrentTrain: epoch  1, batch    11 | loss: 5.6572132Losses:  6.536581039428711 6.392498016357422 0.14408302307128906
CurrentTrain: epoch  1, batch    12 | loss: 6.5365810Losses:  5.549919128417969 5.450331687927246 0.09958742558956146
CurrentTrain: epoch  1, batch    13 | loss: 5.5499191Losses:  7.177423477172852 7.077495574951172 0.09992782771587372
CurrentTrain: epoch  1, batch    14 | loss: 7.1774235Losses:  6.382794380187988 6.293243408203125 0.08955085277557373
CurrentTrain: epoch  1, batch    15 | loss: 6.3827944Losses:  6.424330234527588 6.340051651000977 0.08427874743938446
CurrentTrain: epoch  1, batch    16 | loss: 6.4243302Losses:  8.157562255859375 8.07773208618164 0.07982996106147766
CurrentTrain: epoch  1, batch    17 | loss: 8.1575623Losses:  5.991546154022217 5.910602569580078 0.08094345033168793
CurrentTrain: epoch  1, batch    18 | loss: 5.9915462Losses:  8.871557235717773 8.772783279418945 0.09877356886863708
CurrentTrain: epoch  1, batch    19 | loss: 8.8715572Losses:  6.3911051750183105 6.321202278137207 0.06990299373865128
CurrentTrain: epoch  1, batch    20 | loss: 6.3911052Losses:  9.582718849182129 9.514349937438965 0.06836935877799988
CurrentTrain: epoch  1, batch    21 | loss: 9.5827188Losses:  6.944822788238525 6.881629943847656 0.06319303810596466
CurrentTrain: epoch  1, batch    22 | loss: 6.9448228Losses:  6.399271011352539 6.33425760269165 0.06501343101263046
CurrentTrain: epoch  1, batch    23 | loss: 6.3992710Losses:  7.711657524108887 7.634570121765137 0.07708723843097687
CurrentTrain: epoch  1, batch    24 | loss: 7.7116575Losses:  7.595707416534424 7.516462326049805 0.07924529910087585
CurrentTrain: epoch  1, batch    25 | loss: 7.5957074Losses:  9.28756332397461 9.19852352142334 0.08903951942920685
CurrentTrain: epoch  1, batch    26 | loss: 9.2875633Losses:  9.37779712677002 9.299543380737305 0.07825355231761932
CurrentTrain: epoch  1, batch    27 | loss: 9.3777971Losses:  11.072752952575684 11.008774757385254 0.06397850811481476
CurrentTrain: epoch  1, batch    28 | loss: 11.0727530Losses:  7.559472560882568 7.4848952293396 0.07457725703716278
CurrentTrain: epoch  1, batch    29 | loss: 7.5594726Losses:  5.330200672149658 5.270892143249512 0.05930846929550171
CurrentTrain: epoch  1, batch    30 | loss: 5.3302007Losses:  8.471656799316406 8.402935028076172 0.06872177124023438
CurrentTrain: epoch  1, batch    31 | loss: 8.4716568Losses:  5.568482875823975 5.5110883712768555 0.05739453434944153
CurrentTrain: epoch  1, batch    32 | loss: 5.5684829Losses:  6.355734348297119 6.294520854949951 0.061213426291942596
CurrentTrain: epoch  1, batch    33 | loss: 6.3557343Losses:  6.6113200187683105 6.535775661468506 0.07554441690444946
CurrentTrain: epoch  1, batch    34 | loss: 6.6113200Losses:  6.191036224365234 6.143096923828125 0.04793945327401161
CurrentTrain: epoch  1, batch    35 | loss: 6.1910362Losses:  6.386229038238525 6.3147430419921875 0.07148595154285431
CurrentTrain: epoch  1, batch    36 | loss: 6.3862290Losses:  0.7921339273452759 0.733490526676178 0.0586433932185173
CurrentTrain: epoch  1, batch    37 | loss: 0.7921339Losses:  4.553555488586426 4.50423526763916 0.04932017624378204
CurrentTrain: epoch  2, batch     0 | loss: 4.5535555Losses:  6.177295207977295 6.119368553161621 0.05792646110057831
CurrentTrain: epoch  2, batch     1 | loss: 6.1772952Losses:  5.170393466949463 5.120536804199219 0.04985658824443817
CurrentTrain: epoch  2, batch     2 | loss: 5.1703935Losses:  9.124093055725098 9.06126594543457 0.06282714009284973
CurrentTrain: epoch  2, batch     3 | loss: 9.1240931Losses:  8.606626510620117 8.552818298339844 0.05380808189511299
CurrentTrain: epoch  2, batch     4 | loss: 8.6066265Losses:  12.432670593261719 12.38003921508789 0.05263137444853783
CurrentTrain: epoch  2, batch     5 | loss: 12.4326706Losses:  5.089132785797119 5.024393081665039 0.0647398829460144
CurrentTrain: epoch  2, batch     6 | loss: 5.0891328Losses:  6.481082916259766 6.4316558837890625 0.04942700266838074
CurrentTrain: epoch  2, batch     7 | loss: 6.4810829Losses:  6.191067218780518 6.144507884979248 0.04655944183468819
CurrentTrain: epoch  2, batch     8 | loss: 6.1910672Losses:  5.891530990600586 5.841564178466797 0.049966759979724884
CurrentTrain: epoch  2, batch     9 | loss: 5.8915310Losses:  5.946541786193848 5.902839660644531 0.043702058494091034
CurrentTrain: epoch  2, batch    10 | loss: 5.9465418Losses:  6.421492576599121 6.379446029663086 0.04204640910029411
CurrentTrain: epoch  2, batch    11 | loss: 6.4214926Losses:  5.323663234710693 5.280691146850586 0.04297216236591339
CurrentTrain: epoch  2, batch    12 | loss: 5.3236632Losses:  5.608700752258301 5.568911075592041 0.039789509028196335
CurrentTrain: epoch  2, batch    13 | loss: 5.6087008Losses:  9.493566513061523 9.442516326904297 0.05105026811361313
CurrentTrain: epoch  2, batch    14 | loss: 9.4935665Losses:  6.501212120056152 6.4611382484436035 0.04007392004132271
CurrentTrain: epoch  2, batch    15 | loss: 6.5012121Losses:  5.936513900756836 5.893267631530762 0.04324647784233093
CurrentTrain: epoch  2, batch    16 | loss: 5.9365139Losses:  5.296524524688721 5.259593486785889 0.03693101555109024
CurrentTrain: epoch  2, batch    17 | loss: 5.2965245Losses:  5.642758369445801 5.606410026550293 0.036348309367895126
CurrentTrain: epoch  2, batch    18 | loss: 5.6427584Losses:  6.110016345977783 6.0704474449157715 0.039568960666656494
CurrentTrain: epoch  2, batch    19 | loss: 6.1100163Losses:  6.626999378204346 6.58961820602417 0.03738109767436981
CurrentTrain: epoch  2, batch    20 | loss: 6.6269994Losses:  5.881922721862793 5.843235969543457 0.0386868417263031
CurrentTrain: epoch  2, batch    21 | loss: 5.8819227Losses:  6.908724308013916 6.865092754364014 0.043631650507450104
CurrentTrain: epoch  2, batch    22 | loss: 6.9087243Losses:  4.572251796722412 4.5336995124816895 0.03855207934975624
CurrentTrain: epoch  2, batch    23 | loss: 4.5722518Losses:  5.15457820892334 5.118108749389648 0.03646957874298096
CurrentTrain: epoch  2, batch    24 | loss: 5.1545782Losses:  5.1643195152282715 5.129740238189697 0.03457915782928467
CurrentTrain: epoch  2, batch    25 | loss: 5.1643195Losses:  6.631636142730713 6.596243381500244 0.035392872989177704
CurrentTrain: epoch  2, batch    26 | loss: 6.6316361Losses:  6.49612283706665 6.457194805145264 0.03892788290977478
CurrentTrain: epoch  2, batch    27 | loss: 6.4961228Losses:  6.1299591064453125 6.08870792388916 0.041251420974731445
CurrentTrain: epoch  2, batch    28 | loss: 6.1299591Losses:  5.410103797912598 5.373762607574463 0.03634129837155342
CurrentTrain: epoch  2, batch    29 | loss: 5.4101038Losses:  8.385705947875977 8.344379425048828 0.04132668673992157
CurrentTrain: epoch  2, batch    30 | loss: 8.3857059Losses:  5.115055084228516 5.080065727233887 0.03498920425772667
CurrentTrain: epoch  2, batch    31 | loss: 5.1150551Losses:  8.500493049621582 8.462727546691895 0.03776560723781586
CurrentTrain: epoch  2, batch    32 | loss: 8.5004930Losses:  5.741404056549072 5.7049689292907715 0.036435212939977646
CurrentTrain: epoch  2, batch    33 | loss: 5.7414041Losses:  8.279803276062012 8.244691848754883 0.03511156141757965
CurrentTrain: epoch  2, batch    34 | loss: 8.2798033Losses:  7.6719746589660645 7.640997409820557 0.030977169051766396
CurrentTrain: epoch  2, batch    35 | loss: 7.6719747Losses:  8.122865676879883 8.086021423339844 0.0368439257144928
CurrentTrain: epoch  2, batch    36 | loss: 8.1228657Losses:  2.2532477378845215 2.2121787071228027 0.04106906056404114
CurrentTrain: epoch  2, batch    37 | loss: 2.2532477Losses:  6.070667266845703 6.034420967102051 0.03624645620584488
CurrentTrain: epoch  3, batch     0 | loss: 6.0706673Losses:  13.303509712219238 13.253754615783691 0.04975496232509613
CurrentTrain: epoch  3, batch     1 | loss: 13.3035097Losses:  5.960570335388184 5.927103042602539 0.03346732631325722
CurrentTrain: epoch  3, batch     2 | loss: 5.9605703Losses:  5.752050399780273 5.713151931762695 0.03889835625886917
CurrentTrain: epoch  3, batch     3 | loss: 5.7520504Losses:  5.342809200286865 5.307088375091553 0.035720694810152054
CurrentTrain: epoch  3, batch     4 | loss: 5.3428092Losses:  5.580495834350586 5.545866966247559 0.034628674387931824
CurrentTrain: epoch  3, batch     5 | loss: 5.5804958Losses:  7.955310821533203 7.9080729484558105 0.04723771661520004
CurrentTrain: epoch  3, batch     6 | loss: 7.9553108Losses:  4.457183361053467 4.42880392074585 0.0283795278519392
CurrentTrain: epoch  3, batch     7 | loss: 4.4571834Losses:  5.292176246643066 5.259554862976074 0.03262128680944443
CurrentTrain: epoch  3, batch     8 | loss: 5.2921762Losses:  4.4778642654418945 4.450589179992676 0.027275046333670616
CurrentTrain: epoch  3, batch     9 | loss: 4.4778643Losses:  5.384069919586182 5.351998329162598 0.032071392983198166
CurrentTrain: epoch  3, batch    10 | loss: 5.3840699Losses:  7.0894999504089355 7.046213626861572 0.04328654706478119
CurrentTrain: epoch  3, batch    11 | loss: 7.0895000Losses:  4.70516300201416 4.675145149230957 0.03001774102449417
CurrentTrain: epoch  3, batch    12 | loss: 4.7051630Losses:  5.543691158294678 5.506856918334961 0.036834102123975754
CurrentTrain: epoch  3, batch    13 | loss: 5.5436912Losses:  5.057943344116211 5.02767276763916 0.03027036227285862
CurrentTrain: epoch  3, batch    14 | loss: 5.0579433Losses:  5.683302402496338 5.653861045837402 0.0294414684176445
CurrentTrain: epoch  3, batch    15 | loss: 5.6833024Losses:  6.06000280380249 6.027890205383301 0.032112378627061844
CurrentTrain: epoch  3, batch    16 | loss: 6.0600028Losses:  6.629159450531006 6.591897010803223 0.03726235404610634
CurrentTrain: epoch  3, batch    17 | loss: 6.6291595Losses:  6.1424560546875 6.11541223526001 0.027043886482715607
CurrentTrain: epoch  3, batch    18 | loss: 6.1424561Losses:  6.387239456176758 6.35292387008667 0.0343158133327961
CurrentTrain: epoch  3, batch    19 | loss: 6.3872395Losses:  11.96445369720459 11.936786651611328 0.027666881680488586
CurrentTrain: epoch  3, batch    20 | loss: 11.9644537Losses:  5.777203559875488 5.748263359069824 0.02894025668501854
CurrentTrain: epoch  3, batch    21 | loss: 5.7772036Losses:  6.391369819641113 6.358616828918457 0.032752875238657
CurrentTrain: epoch  3, batch    22 | loss: 6.3913698Losses:  7.082221508026123 7.049676895141602 0.03254484385251999
CurrentTrain: epoch  3, batch    23 | loss: 7.0822215Losses:  5.261382102966309 5.231514930725098 0.02986716665327549
CurrentTrain: epoch  3, batch    24 | loss: 5.2613821Losses:  8.711326599121094 8.67535400390625 0.03597274422645569
CurrentTrain: epoch  3, batch    25 | loss: 8.7113266Losses:  6.025894641876221 5.99625301361084 0.029641464352607727
CurrentTrain: epoch  3, batch    26 | loss: 6.0258946Losses:  7.068115711212158 7.04148006439209 0.026635846123099327
CurrentTrain: epoch  3, batch    27 | loss: 7.0681157Losses:  4.384164333343506 4.358881950378418 0.025282181799411774
CurrentTrain: epoch  3, batch    28 | loss: 4.3841643Losses:  7.112571716308594 7.067432880401611 0.045138947665691376
CurrentTrain: epoch  3, batch    29 | loss: 7.1125717Losses:  5.039188861846924 5.010081768035889 0.02910718135535717
CurrentTrain: epoch  3, batch    30 | loss: 5.0391889Losses:  5.844265937805176 5.815802097320557 0.02846364863216877
CurrentTrain: epoch  3, batch    31 | loss: 5.8442659Losses:  9.426154136657715 9.388620376586914 0.0375334732234478
CurrentTrain: epoch  3, batch    32 | loss: 9.4261541Losses:  7.131561756134033 7.087041854858398 0.04452008008956909
CurrentTrain: epoch  3, batch    33 | loss: 7.1315618Losses:  5.288720607757568 5.261273384094238 0.02744712121784687
CurrentTrain: epoch  3, batch    34 | loss: 5.2887206Losses:  5.575571060180664 5.549128532409668 0.026442736387252808
CurrentTrain: epoch  3, batch    35 | loss: 5.5755711Losses:  5.3680243492126465 5.3396406173706055 0.02838369831442833
CurrentTrain: epoch  3, batch    36 | loss: 5.3680243Losses:  3.2686686515808105 3.2332966327667236 0.03537210449576378
CurrentTrain: epoch  3, batch    37 | loss: 3.2686687Losses:  5.61061429977417 5.579682350158691 0.030932094901800156
CurrentTrain: epoch  4, batch     0 | loss: 5.6106143Losses:  5.2107954025268555 5.183874607086182 0.026921004056930542
CurrentTrain: epoch  4, batch     1 | loss: 5.2107954Losses:  6.650404930114746 6.615009784698486 0.035395003855228424
CurrentTrain: epoch  4, batch     2 | loss: 6.6504049Losses:  6.676063537597656 6.643431663513184 0.03263184800744057
CurrentTrain: epoch  4, batch     3 | loss: 6.6760635Losses:  7.863539218902588 7.826532363891602 0.03700689598917961
CurrentTrain: epoch  4, batch     4 | loss: 7.8635392Losses:  4.946159362792969 4.9153032302856445 0.030856098979711533
CurrentTrain: epoch  4, batch     5 | loss: 4.9461594Losses:  7.578346252441406 7.547675132751465 0.03067108429968357
CurrentTrain: epoch  4, batch     6 | loss: 7.5783463Losses:  4.365391731262207 4.339643478393555 0.025748400017619133
CurrentTrain: epoch  4, batch     7 | loss: 4.3653917Losses:  9.536269187927246 9.494853973388672 0.0414155013859272
CurrentTrain: epoch  4, batch     8 | loss: 9.5362692Losses:  4.629459381103516 4.602865219116211 0.026594290509819984
CurrentTrain: epoch  4, batch     9 | loss: 4.6294594Losses:  7.561782360076904 7.526117324829102 0.03566484898328781
CurrentTrain: epoch  4, batch    10 | loss: 7.5617824Losses:  4.695863246917725 4.672096252441406 0.02376721426844597
CurrentTrain: epoch  4, batch    11 | loss: 4.6958632Losses:  8.525879859924316 8.490253448486328 0.035626597702503204
CurrentTrain: epoch  4, batch    12 | loss: 8.5258799Losses:  4.107036113739014 4.081744194030762 0.025291945785284042
CurrentTrain: epoch  4, batch    13 | loss: 4.1070361Losses:  8.145743370056152 8.115498542785645 0.030244572088122368
CurrentTrain: epoch  4, batch    14 | loss: 8.1457434Losses:  7.684916019439697 7.6579999923706055 0.02691607177257538
CurrentTrain: epoch  4, batch    15 | loss: 7.6849160Losses:  4.847947597503662 4.821537017822266 0.02641080692410469
CurrentTrain: epoch  4, batch    16 | loss: 4.8479476Losses:  5.987005710601807 5.96080207824707 0.026203718036413193
CurrentTrain: epoch  4, batch    17 | loss: 5.9870057Losses:  4.927087783813477 4.903458118438721 0.023629549890756607
CurrentTrain: epoch  4, batch    18 | loss: 4.9270878Losses:  4.367514133453369 4.343545436859131 0.023968931287527084
CurrentTrain: epoch  4, batch    19 | loss: 4.3675141Losses:  5.739813327789307 5.7063307762146 0.03348248451948166
CurrentTrain: epoch  4, batch    20 | loss: 5.7398133Losses:  5.550689220428467 5.526140213012695 0.02454923838376999
CurrentTrain: epoch  4, batch    21 | loss: 5.5506892Losses:  4.644715309143066 4.6194963455200195 0.025218890979886055
CurrentTrain: epoch  4, batch    22 | loss: 4.6447153Losses:  5.761317253112793 5.723731517791748 0.03758582845330238
CurrentTrain: epoch  4, batch    23 | loss: 5.7613173Losses:  5.820443630218506 5.785871982574463 0.034571439027786255
CurrentTrain: epoch  4, batch    24 | loss: 5.8204436Losses:  8.408454895019531 8.371488571166992 0.0369662269949913
CurrentTrain: epoch  4, batch    25 | loss: 8.4084549Losses:  5.115331649780273 5.09000301361084 0.02532879076898098
CurrentTrain: epoch  4, batch    26 | loss: 5.1153316Losses:  6.010937213897705 5.969534873962402 0.04140257462859154
CurrentTrain: epoch  4, batch    27 | loss: 6.0109372Losses:  3.589160203933716 3.56486177444458 0.02429836429655552
CurrentTrain: epoch  4, batch    28 | loss: 3.5891602Losses:  5.759217262268066 5.733837127685547 0.025380278006196022
CurrentTrain: epoch  4, batch    29 | loss: 5.7592173Losses:  6.001983165740967 5.974268913269043 0.02771407552063465
CurrentTrain: epoch  4, batch    30 | loss: 6.0019832Losses:  9.809694290161133 9.769266128540039 0.04042808711528778
CurrentTrain: epoch  4, batch    31 | loss: 9.8096943Losses:  5.108055591583252 5.075388431549072 0.03266715258359909
CurrentTrain: epoch  4, batch    32 | loss: 5.1080556Losses:  5.966211795806885 5.933070182800293 0.033141475170850754
CurrentTrain: epoch  4, batch    33 | loss: 5.9662118Losses:  6.870269298553467 6.835588455200195 0.03468100726604462
CurrentTrain: epoch  4, batch    34 | loss: 6.8702693Losses:  5.1115264892578125 5.079295635223389 0.032230813056230545
CurrentTrain: epoch  4, batch    35 | loss: 5.1115265Losses:  5.258035182952881 5.23023796081543 0.027797440066933632
CurrentTrain: epoch  4, batch    36 | loss: 5.2580352Losses:  0.6939632892608643 0.6672481894493103 0.026715124025940895
CurrentTrain: epoch  4, batch    37 | loss: 0.6939633Losses:  6.148266792297363 6.112568378448486 0.035698264837265015
CurrentTrain: epoch  5, batch     0 | loss: 6.1482668Losses:  5.25213623046875 5.224791526794434 0.027344876900315285
CurrentTrain: epoch  5, batch     1 | loss: 5.2521362Losses:  6.876016139984131 6.83163595199585 0.04438010975718498
CurrentTrain: epoch  5, batch     2 | loss: 6.8760161Losses:  7.6027936935424805 7.569514274597168 0.03327950835227966
CurrentTrain: epoch  5, batch     3 | loss: 7.6027937Losses:  5.399079322814941 5.361838340759277 0.03724098950624466
CurrentTrain: epoch  5, batch     4 | loss: 5.3990793Losses:  5.775023937225342 5.741148471832275 0.03387541323900223
CurrentTrain: epoch  5, batch     5 | loss: 5.7750239Losses:  4.46079158782959 4.431683540344238 0.029107876121997833
CurrentTrain: epoch  5, batch     6 | loss: 4.4607916Losses:  5.395351409912109 5.363708019256592 0.0316435731947422
CurrentTrain: epoch  5, batch     7 | loss: 5.3953514Losses:  6.0165581703186035 5.988526821136475 0.028031382709741592
CurrentTrain: epoch  5, batch     8 | loss: 6.0165582Losses:  5.779178619384766 5.74763822555542 0.03154054284095764
CurrentTrain: epoch  5, batch     9 | loss: 5.7791786Losses:  4.653745174407959 4.6308979988098145 0.022847212851047516
CurrentTrain: epoch  5, batch    10 | loss: 4.6537452Losses:  5.138914108276367 5.108675956726074 0.030238384380936623
CurrentTrain: epoch  5, batch    11 | loss: 5.1389141Losses:  4.166599750518799 4.143825054168701 0.022774679586291313
CurrentTrain: epoch  5, batch    12 | loss: 4.1665998Losses:  4.533432483673096 4.508453369140625 0.02497899904847145
CurrentTrain: epoch  5, batch    13 | loss: 4.5334325Losses:  4.089609146118164 4.065402030944824 0.024206912145018578
CurrentTrain: epoch  5, batch    14 | loss: 4.0896091Losses:  7.963688850402832 7.938636302947998 0.025052769109606743
CurrentTrain: epoch  5, batch    15 | loss: 7.9636889Losses:  5.679997444152832 5.650607109069824 0.029390478506684303
CurrentTrain: epoch  5, batch    16 | loss: 5.6799974Losses:  6.321979522705078 6.294198989868164 0.027780629694461823
CurrentTrain: epoch  5, batch    17 | loss: 6.3219795Losses:  6.034102916717529 6.006376266479492 0.02772659808397293
CurrentTrain: epoch  5, batch    18 | loss: 6.0341029Losses:  6.805134296417236 6.7722554206848145 0.03287896513938904
CurrentTrain: epoch  5, batch    19 | loss: 6.8051343Losses:  5.108908176422119 5.080358505249023 0.028549499809741974
CurrentTrain: epoch  5, batch    20 | loss: 5.1089082Losses:  5.233476161956787 5.20925235748291 0.02422378398478031
CurrentTrain: epoch  5, batch    21 | loss: 5.2334762Losses:  4.346893310546875 4.322414875030518 0.024478454142808914
CurrentTrain: epoch  5, batch    22 | loss: 4.3468933Losses:  4.459029674530029 4.436514854431152 0.022514795884490013
CurrentTrain: epoch  5, batch    23 | loss: 4.4590297Losses:  5.474348068237305 5.446133613586426 0.02821439318358898
CurrentTrain: epoch  5, batch    24 | loss: 5.4743481Losses:  4.208710670471191 4.185894966125488 0.022815579548478127
CurrentTrain: epoch  5, batch    25 | loss: 4.2087107Losses:  4.752066135406494 4.723635673522949 0.028430627658963203
CurrentTrain: epoch  5, batch    26 | loss: 4.7520661Losses:  9.732405662536621 9.700010299682617 0.03239540010690689
CurrentTrain: epoch  5, batch    27 | loss: 9.7324057Losses:  8.220785140991211 8.192559242248535 0.028225867077708244
CurrentTrain: epoch  5, batch    28 | loss: 8.2207851Losses:  3.817887783050537 3.794132709503174 0.023755107074975967
CurrentTrain: epoch  5, batch    29 | loss: 3.8178878Losses:  6.052962303161621 6.019827842712402 0.03313432261347771
CurrentTrain: epoch  5, batch    30 | loss: 6.0529623Losses:  6.710521221160889 6.678993225097656 0.03152807056903839
CurrentTrain: epoch  5, batch    31 | loss: 6.7105212Losses:  9.21972370147705 9.176216125488281 0.04350782185792923
CurrentTrain: epoch  5, batch    32 | loss: 9.2197237Losses:  6.246303081512451 6.219918251037598 0.026384975761175156
CurrentTrain: epoch  5, batch    33 | loss: 6.2463031Losses:  3.5814661979675293 3.5590291023254395 0.022437050938606262
CurrentTrain: epoch  5, batch    34 | loss: 3.5814662Losses:  7.666911602020264 7.641125679016113 0.025786157697439194
CurrentTrain: epoch  5, batch    35 | loss: 7.6669116Losses:  4.583884239196777 4.55854606628418 0.025338320061564445
CurrentTrain: epoch  5, batch    36 | loss: 4.5838842Losses:  1.3067328929901123 1.2640082836151123 0.04272455722093582
CurrentTrain: epoch  5, batch    37 | loss: 1.3067329Losses:  5.687108039855957 5.6642608642578125 0.022847039625048637
CurrentTrain: epoch  6, batch     0 | loss: 5.6871080Losses:  6.411678791046143 6.377363681793213 0.03431489318609238
CurrentTrain: epoch  6, batch     1 | loss: 6.4116788Losses:  8.234879493713379 8.199579238891602 0.03530064597725868
CurrentTrain: epoch  6, batch     2 | loss: 8.2348795Losses:  4.641305446624756 4.614950180053711 0.026355484500527382
CurrentTrain: epoch  6, batch     3 | loss: 4.6413054Losses:  5.8869781494140625 5.853158950805664 0.033819228410720825
CurrentTrain: epoch  6, batch     4 | loss: 5.8869781Losses:  5.236964702606201 5.209977626800537 0.026986856013536453
CurrentTrain: epoch  6, batch     5 | loss: 5.2369647Losses:  9.402688026428223 9.375001907348633 0.02768613025546074
CurrentTrain: epoch  6, batch     6 | loss: 9.4026880Losses:  4.556553840637207 4.531797409057617 0.02475643716752529
CurrentTrain: epoch  6, batch     7 | loss: 4.5565538Losses:  5.772378921508789 5.750080108642578 0.022298911586403847
CurrentTrain: epoch  6, batch     8 | loss: 5.7723789Losses:  5.045915603637695 5.013505935668945 0.03240948170423508
CurrentTrain: epoch  6, batch     9 | loss: 5.0459156Losses:  3.8842570781707764 3.858947277069092 0.02530972845852375
CurrentTrain: epoch  6, batch    10 | loss: 3.8842571Losses:  5.025879383087158 5.0004377365112305 0.02544175647199154
CurrentTrain: epoch  6, batch    11 | loss: 5.0258794Losses:  3.926927089691162 3.902421236038208 0.024505792185664177
CurrentTrain: epoch  6, batch    12 | loss: 3.9269271Losses:  5.560999393463135 5.531398773193359 0.029600439593195915
CurrentTrain: epoch  6, batch    13 | loss: 5.5609994Losses:  4.1189165115356445 4.093789577484131 0.025126881897449493
CurrentTrain: epoch  6, batch    14 | loss: 4.1189165Losses:  4.082807540893555 4.058841705322266 0.023966021835803986
CurrentTrain: epoch  6, batch    15 | loss: 4.0828075Losses:  7.02118444442749 6.985333442687988 0.0358511246740818
CurrentTrain: epoch  6, batch    16 | loss: 7.0211844Losses:  6.568779468536377 6.539606094360352 0.02917318046092987
CurrentTrain: epoch  6, batch    17 | loss: 6.5687795Losses:  5.494274139404297 5.4683837890625 0.02589024417102337
CurrentTrain: epoch  6, batch    18 | loss: 5.4942741Losses:  7.1621503829956055 7.130190372467041 0.031959980726242065
CurrentTrain: epoch  6, batch    19 | loss: 7.1621504Losses:  9.268795013427734 9.232524871826172 0.03627019375562668
CurrentTrain: epoch  6, batch    20 | loss: 9.2687950Losses:  4.121557235717773 4.092654228210449 0.02890290878713131
CurrentTrain: epoch  6, batch    21 | loss: 4.1215572Losses:  6.0122785568237305 5.9853339195251465 0.026944734156131744
CurrentTrain: epoch  6, batch    22 | loss: 6.0122786Losses:  3.792945623397827 3.7695748805999756 0.023370761424303055
CurrentTrain: epoch  6, batch    23 | loss: 3.7929456Losses:  5.899935722351074 5.8668341636657715 0.033101458102464676
CurrentTrain: epoch  6, batch    24 | loss: 5.8999357Losses:  7.964031219482422 7.920680046081543 0.04335127770900726
CurrentTrain: epoch  6, batch    25 | loss: 7.9640312Losses:  7.586964130401611 7.551802635192871 0.035161394625902176
CurrentTrain: epoch  6, batch    26 | loss: 7.5869641Losses:  6.00332498550415 5.963150978088379 0.040174126625061035
CurrentTrain: epoch  6, batch    27 | loss: 6.0033250Losses:  6.419156551361084 6.38697624206543 0.03218036890029907
CurrentTrain: epoch  6, batch    28 | loss: 6.4191566Losses:  5.893606662750244 5.861989974975586 0.031616777181625366
CurrentTrain: epoch  6, batch    29 | loss: 5.8936067Losses:  6.42781925201416 6.40380859375 0.02401088736951351
CurrentTrain: epoch  6, batch    30 | loss: 6.4278193Losses:  5.921768665313721 5.89178991317749 0.02997858077287674
CurrentTrain: epoch  6, batch    31 | loss: 5.9217687Losses:  5.0826735496521 5.060461044311523 0.022212404757738113
CurrentTrain: epoch  6, batch    32 | loss: 5.0826735Losses:  4.5069427490234375 4.482466220855713 0.02447647787630558
CurrentTrain: epoch  6, batch    33 | loss: 4.5069427Losses:  4.766449451446533 4.741813659667969 0.02463594824075699
CurrentTrain: epoch  6, batch    34 | loss: 4.7664495Losses:  4.433709144592285 4.406999588012695 0.02670949324965477
CurrentTrain: epoch  6, batch    35 | loss: 4.4337091Losses:  4.207294464111328 4.184577465057373 0.022716766223311424
CurrentTrain: epoch  6, batch    36 | loss: 4.2072945Losses:  0.5711076855659485 0.5431772470474243 0.02793041616678238
CurrentTrain: epoch  6, batch    37 | loss: 0.5711077Losses:  10.816956520080566 10.774123191833496 0.04283306375145912
CurrentTrain: epoch  7, batch     0 | loss: 10.8169565Losses:  4.439677715301514 4.412635326385498 0.027042262256145477
CurrentTrain: epoch  7, batch     1 | loss: 4.4396777Losses:  6.754695415496826 6.709209442138672 0.04548608884215355
CurrentTrain: epoch  7, batch     2 | loss: 6.7546954Losses:  7.089788436889648 7.056709289550781 0.033079084008932114
CurrentTrain: epoch  7, batch     3 | loss: 7.0897884Losses:  6.424039840698242 6.392827033996582 0.031213026493787766
CurrentTrain: epoch  7, batch     4 | loss: 6.4240398Losses:  5.167471885681152 5.14321756362915 0.024254268035292625
CurrentTrain: epoch  7, batch     5 | loss: 5.1674719Losses:  4.498584747314453 4.474709510803223 0.023875467479228973
CurrentTrain: epoch  7, batch     6 | loss: 4.4985847Losses:  4.6474928855896 4.625753402709961 0.02173963561654091
CurrentTrain: epoch  7, batch     7 | loss: 4.6474929Losses:  6.2548933029174805 6.225159168243408 0.029734158888459206
CurrentTrain: epoch  7, batch     8 | loss: 6.2548933Losses:  5.726510047912598 5.700446605682373 0.026063544675707817
CurrentTrain: epoch  7, batch     9 | loss: 5.7265100Losses:  5.0239996910095215 4.996417045593262 0.027582578361034393
CurrentTrain: epoch  7, batch    10 | loss: 5.0239997Losses:  6.053798675537109 6.024035453796387 0.029763376340270042
CurrentTrain: epoch  7, batch    11 | loss: 6.0537987Losses:  4.6977667808532715 4.672892093658447 0.024874817579984665
CurrentTrain: epoch  7, batch    12 | loss: 4.6977668Losses:  5.363770961761475 5.339611053466797 0.024159900844097137
CurrentTrain: epoch  7, batch    13 | loss: 5.3637710Losses:  7.547582149505615 7.5159478187561035 0.03163444995880127
CurrentTrain: epoch  7, batch    14 | loss: 7.5475821Losses:  4.192984580993652 4.165926933288574 0.027057461440563202
CurrentTrain: epoch  7, batch    15 | loss: 4.1929846Losses:  4.597501754760742 4.570886135101318 0.026615416631102562
CurrentTrain: epoch  7, batch    16 | loss: 4.5975018Losses:  5.812267303466797 5.7826619148254395 0.02960541471838951
CurrentTrain: epoch  7, batch    17 | loss: 5.8122673Losses:  5.740444183349609 5.702648162841797 0.0377962589263916
CurrentTrain: epoch  7, batch    18 | loss: 5.7404442Losses:  5.919330596923828 5.893604278564453 0.02572615258395672
CurrentTrain: epoch  7, batch    19 | loss: 5.9193306Losses:  5.473161220550537 5.448186874389648 0.024974465370178223
CurrentTrain: epoch  7, batch    20 | loss: 5.4731612Losses:  4.9359893798828125 4.910824775695801 0.025164663791656494
CurrentTrain: epoch  7, batch    21 | loss: 4.9359894Losses:  5.14460563659668 5.114617824554443 0.029987836256623268
CurrentTrain: epoch  7, batch    22 | loss: 5.1446056Losses:  9.479870796203613 9.431537628173828 0.0483328178524971
CurrentTrain: epoch  7, batch    23 | loss: 9.4798708Losses:  5.7168192863464355 5.6888203620910645 0.027999041602015495
CurrentTrain: epoch  7, batch    24 | loss: 5.7168193Losses:  6.182756423950195 6.158510208129883 0.024246297776699066
CurrentTrain: epoch  7, batch    25 | loss: 6.1827564Losses:  8.80776309967041 8.769299507141113 0.03846346586942673
CurrentTrain: epoch  7, batch    26 | loss: 8.8077631Losses:  5.912871837615967 5.886971950531006 0.02589971199631691
CurrentTrain: epoch  7, batch    27 | loss: 5.9128718Losses:  7.788203716278076 7.751699924468994 0.036503832787275314
CurrentTrain: epoch  7, batch    28 | loss: 7.7882037Losses:  4.778932571411133 4.752389907836914 0.026542749255895615
CurrentTrain: epoch  7, batch    29 | loss: 4.7789326Losses:  4.922987461090088 4.896385192871094 0.02660241536796093
CurrentTrain: epoch  7, batch    30 | loss: 4.9229875Losses:  6.176986217498779 6.144052982330322 0.03293341025710106
CurrentTrain: epoch  7, batch    31 | loss: 6.1769862Losses:  4.554721832275391 4.525404930114746 0.029316971078515053
CurrentTrain: epoch  7, batch    32 | loss: 4.5547218Losses:  5.705427169799805 5.680956840515137 0.024470312520861626
CurrentTrain: epoch  7, batch    33 | loss: 5.7054272Losses:  6.655494689941406 6.626967430114746 0.028527429327368736
CurrentTrain: epoch  7, batch    34 | loss: 6.6554947Losses:  3.532170057296753 3.510565996170044 0.021603967994451523
CurrentTrain: epoch  7, batch    35 | loss: 3.5321701Losses:  6.42868709564209 6.392877578735352 0.03580954670906067
CurrentTrain: epoch  7, batch    36 | loss: 6.4286871Losses:  1.4745540618896484 1.4307665824890137 0.04378752410411835
CurrentTrain: epoch  7, batch    37 | loss: 1.4745541Losses:  7.239208698272705 7.207282066345215 0.031926728785037994
CurrentTrain: epoch  8, batch     0 | loss: 7.2392087Losses:  4.503911018371582 4.474987506866455 0.02892352268099785
CurrentTrain: epoch  8, batch     1 | loss: 4.5039110Losses:  9.528928756713867 9.497032165527344 0.03189624473452568
CurrentTrain: epoch  8, batch     2 | loss: 9.5289288Losses:  4.478719234466553 4.452827453613281 0.025891872122883797
CurrentTrain: epoch  8, batch     3 | loss: 4.4787192Losses:  4.704005718231201 4.678051471710205 0.02595413289964199
CurrentTrain: epoch  8, batch     4 | loss: 4.7040057Losses:  4.6959028244018555 4.6673712730407715 0.02853165753185749
CurrentTrain: epoch  8, batch     5 | loss: 4.6959028Losses:  7.25449800491333 7.222309589385986 0.03218856453895569
CurrentTrain: epoch  8, batch     6 | loss: 7.2544980Losses:  6.841022968292236 6.817883491516113 0.023139452561736107
CurrentTrain: epoch  8, batch     7 | loss: 6.8410230Losses:  5.611073017120361 5.586949825286865 0.024123363196849823
CurrentTrain: epoch  8, batch     8 | loss: 5.6110730Losses:  3.844184160232544 3.8210196495056152 0.02316461317241192
CurrentTrain: epoch  8, batch     9 | loss: 3.8441842Losses:  6.639744758605957 6.59839391708374 0.04135064780712128
CurrentTrain: epoch  8, batch    10 | loss: 6.6397448Losses:  6.045943260192871 6.01961088180542 0.026332436129450798
CurrentTrain: epoch  8, batch    11 | loss: 6.0459433Losses:  4.797348976135254 4.771549701690674 0.02579905278980732
CurrentTrain: epoch  8, batch    12 | loss: 4.7973490Losses:  5.104278087615967 5.078473091125488 0.025805210694670677
CurrentTrain: epoch  8, batch    13 | loss: 5.1042781Losses:  4.630893707275391 4.600986003875732 0.02990766055881977
CurrentTrain: epoch  8, batch    14 | loss: 4.6308937Losses:  4.789827823638916 4.763046741485596 0.026781104505062103
CurrentTrain: epoch  8, batch    15 | loss: 4.7898278Losses:  7.360779762268066 7.330848217010498 0.029931610450148582
CurrentTrain: epoch  8, batch    16 | loss: 7.3607798Losses:  8.765947341918945 8.73316764831543 0.03277941048145294
CurrentTrain: epoch  8, batch    17 | loss: 8.7659473Losses:  4.831575393676758 4.802233695983887 0.02934151142835617
CurrentTrain: epoch  8, batch    18 | loss: 4.8315754Losses:  5.77017879486084 5.744117736816406 0.02606087364256382
CurrentTrain: epoch  8, batch    19 | loss: 5.7701788Losses:  7.807822227478027 7.766152381896973 0.041669659316539764
CurrentTrain: epoch  8, batch    20 | loss: 7.8078222Losses:  5.581604957580566 5.556620121002197 0.024985045194625854
CurrentTrain: epoch  8, batch    21 | loss: 5.5816050Losses:  7.196810245513916 7.1556243896484375 0.04118587076663971
CurrentTrain: epoch  8, batch    22 | loss: 7.1968102Losses:  5.278564453125 5.246269702911377 0.032294757664203644
CurrentTrain: epoch  8, batch    23 | loss: 5.2785645Losses:  5.7781195640563965 5.7491455078125 0.028974246233701706
CurrentTrain: epoch  8, batch    24 | loss: 5.7781196Losses:  5.514341354370117 5.479154586791992 0.0351867750287056
CurrentTrain: epoch  8, batch    25 | loss: 5.5143414Losses:  6.858025550842285 6.820271968841553 0.03775355592370033
CurrentTrain: epoch  8, batch    26 | loss: 6.8580256Losses:  6.254868507385254 6.217350006103516 0.03751854598522186
CurrentTrain: epoch  8, batch    27 | loss: 6.2548685Losses:  6.203677654266357 6.17623233795166 0.027445264160633087
CurrentTrain: epoch  8, batch    28 | loss: 6.2036777Losses:  8.09621810913086 8.067026138305664 0.029192283749580383
CurrentTrain: epoch  8, batch    29 | loss: 8.0962181Losses:  5.117384910583496 5.094925403594971 0.0224594883620739
CurrentTrain: epoch  8, batch    30 | loss: 5.1173849Losses:  9.582969665527344 9.541584968566895 0.041384220123291016
CurrentTrain: epoch  8, batch    31 | loss: 9.5829697Losses:  5.84352970123291 5.805657386779785 0.03787224367260933
CurrentTrain: epoch  8, batch    32 | loss: 5.8435297Losses:  4.743416786193848 4.717663764953613 0.025752931833267212
CurrentTrain: epoch  8, batch    33 | loss: 4.7434168Losses:  3.813291311264038 3.792555332183838 0.020736096426844597
CurrentTrain: epoch  8, batch    34 | loss: 3.8132913Losses:  5.418346881866455 5.386636257171631 0.031710442155599594
CurrentTrain: epoch  8, batch    35 | loss: 5.4183469Losses:  3.4605047702789307 3.438344955444336 0.022159822285175323
CurrentTrain: epoch  8, batch    36 | loss: 3.4605048Losses:  1.6517003774642944 1.6039128303527832 0.04778752103447914
CurrentTrain: epoch  8, batch    37 | loss: 1.6517004Losses:  5.042552947998047 5.014333724975586 0.028219087049365044
CurrentTrain: epoch  9, batch     0 | loss: 5.0425529Losses:  9.624299049377441 9.59191608428955 0.03238258883357048
CurrentTrain: epoch  9, batch     1 | loss: 9.6242990Losses:  3.9993679523468018 3.9761948585510254 0.023173116147518158
CurrentTrain: epoch  9, batch     2 | loss: 3.9993680Losses:  4.862605571746826 4.839771747589111 0.022833989933133125
CurrentTrain: epoch  9, batch     3 | loss: 4.8626056Losses:  5.189721584320068 5.161336898803711 0.02838456816971302
CurrentTrain: epoch  9, batch     4 | loss: 5.1897216Losses:  5.2303786277771 5.201452255249023 0.02892618626356125
CurrentTrain: epoch  9, batch     5 | loss: 5.2303786Losses:  5.941648006439209 5.9163384437561035 0.025309555232524872
CurrentTrain: epoch  9, batch     6 | loss: 5.9416480Losses:  4.833848476409912 4.805333137512207 0.02851537987589836
CurrentTrain: epoch  9, batch     7 | loss: 4.8338485Losses:  3.515223503112793 3.4937639236450195 0.02145964652299881
CurrentTrain: epoch  9, batch     8 | loss: 3.5152235Losses:  9.940921783447266 9.916505813598633 0.024415694177150726
CurrentTrain: epoch  9, batch     9 | loss: 9.9409218Losses:  6.514042377471924 6.4690937995910645 0.0449487529695034
CurrentTrain: epoch  9, batch    10 | loss: 6.5140424Losses:  7.635592460632324 7.6044182777404785 0.03117397055029869
CurrentTrain: epoch  9, batch    11 | loss: 7.6355925Losses:  4.716357231140137 4.690191268920898 0.026166092604398727
CurrentTrain: epoch  9, batch    12 | loss: 4.7163572Losses:  5.336630821228027 5.304358005523682 0.032272595912218094
CurrentTrain: epoch  9, batch    13 | loss: 5.3366308Losses:  7.154888153076172 7.118605613708496 0.036282554268836975
CurrentTrain: epoch  9, batch    14 | loss: 7.1548882Losses:  4.492259502410889 4.466357707977295 0.025901976972818375
CurrentTrain: epoch  9, batch    15 | loss: 4.4922595Losses:  5.940903186798096 5.907212257385254 0.03369107097387314
CurrentTrain: epoch  9, batch    16 | loss: 5.9409032Losses:  7.872065544128418 7.84568977355957 0.026375802233815193
CurrentTrain: epoch  9, batch    17 | loss: 7.8720655Losses:  6.586544990539551 6.563897609710693 0.02264731004834175
CurrentTrain: epoch  9, batch    18 | loss: 6.5865450Losses:  6.189791202545166 6.158984184265137 0.03080703131854534
CurrentTrain: epoch  9, batch    19 | loss: 6.1897912Losses:  5.516347408294678 5.488958358764648 0.027389243245124817
CurrentTrain: epoch  9, batch    20 | loss: 5.5163474Losses:  6.001528263092041 5.975188732147217 0.026339344680309296
CurrentTrain: epoch  9, batch    21 | loss: 6.0015283Losses:  7.116098880767822 7.080063343048096 0.036035530269145966
CurrentTrain: epoch  9, batch    22 | loss: 7.1160989Losses:  8.730257034301758 8.687541961669922 0.042714618146419525
CurrentTrain: epoch  9, batch    23 | loss: 8.7302570Losses:  5.883915424346924 5.855114459991455 0.028801104053854942
CurrentTrain: epoch  9, batch    24 | loss: 5.8839154Losses:  5.017563343048096 4.992555618286133 0.025007717311382294
CurrentTrain: epoch  9, batch    25 | loss: 5.0175633Losses:  6.560589790344238 6.529630184173584 0.03095960058271885
CurrentTrain: epoch  9, batch    26 | loss: 6.5605898Losses:  6.2795305252075195 6.254328727722168 0.025201978161931038
CurrentTrain: epoch  9, batch    27 | loss: 6.2795305Losses:  4.140978813171387 4.117496013641357 0.023482784628868103
CurrentTrain: epoch  9, batch    28 | loss: 4.1409788Losses:  6.499505996704102 6.459136962890625 0.04036901146173477
CurrentTrain: epoch  9, batch    29 | loss: 6.4995060Losses:  8.808815002441406 8.7659912109375 0.042823467403650284
CurrentTrain: epoch  9, batch    30 | loss: 8.8088150Losses:  6.931208610534668 6.898499488830566 0.03270914778113365
CurrentTrain: epoch  9, batch    31 | loss: 6.9312086Losses:  5.188249111175537 5.156643867492676 0.031605228781700134
CurrentTrain: epoch  9, batch    32 | loss: 5.1882491Losses:  5.452417373657227 5.429609298706055 0.022807888686656952
CurrentTrain: epoch  9, batch    33 | loss: 5.4524174Losses:  7.27229642868042 7.239918231964111 0.03237808868288994
CurrentTrain: epoch  9, batch    34 | loss: 7.2722964Losses:  5.167095184326172 5.134844779968262 0.03225032240152359
CurrentTrain: epoch  9, batch    35 | loss: 5.1670952Losses:  6.113507270812988 6.083031177520752 0.030476033687591553
CurrentTrain: epoch  9, batch    36 | loss: 6.1135073Losses:  0.7595065832138062 0.7217288017272949 0.03777777776122093
CurrentTrain: epoch  9, batch    37 | loss: 0.7595066
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 89.73%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 88.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 86.72%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 85.29%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 84.03%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 84.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.51%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 86.14%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.74%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.96%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.39%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.79%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 88.54%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 88.71%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.87%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 87.31%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 89.73%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 88.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 86.72%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 85.29%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 84.03%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 84.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.51%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 86.14%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.74%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.96%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.39%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.79%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 88.54%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 88.71%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.87%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 87.31%   
cur_acc:  ['0.8731']
his_acc:  ['0.8731']
Clustering into  4  clusters
Clusters:  [1 0 0 1 0 2 1 1 3 0 1]
Losses:  9.019569396972656 8.478761672973633 0.5408079624176025
CurrentTrain: epoch  0, batch     0 | loss: 9.0195694Losses:  2.994476556777954 2.7469723224639893 0.24750427901744843
CurrentTrain: epoch  0, batch     1 | loss: 2.9944766Losses:  8.836648941040039 8.540536880493164 0.29611217975616455
CurrentTrain: epoch  1, batch     0 | loss: 8.8366489Losses:  2.296833038330078 1.9886224269866943 0.308210551738739
CurrentTrain: epoch  1, batch     1 | loss: 2.2968330Losses:  7.83317756652832 7.60854434967041 0.22463326156139374
CurrentTrain: epoch  2, batch     0 | loss: 7.8331776Losses:  2.736800193786621 2.4357118606567383 0.30108845233917236
CurrentTrain: epoch  2, batch     1 | loss: 2.7368002Losses:  8.608125686645508 8.377721786499023 0.2304040789604187
CurrentTrain: epoch  3, batch     0 | loss: 8.6081257Losses:  2.870408535003662 2.800631284713745 0.06977726519107819
CurrentTrain: epoch  3, batch     1 | loss: 2.8704085Losses:  7.1923441886901855 6.954904556274414 0.23743951320648193
CurrentTrain: epoch  4, batch     0 | loss: 7.1923442Losses:  2.5585546493530273 2.326322555541992 0.23223213851451874
CurrentTrain: epoch  4, batch     1 | loss: 2.5585546Losses:  7.550696849822998 7.340188503265381 0.21050815284252167
CurrentTrain: epoch  5, batch     0 | loss: 7.5506968Losses:  2.3655431270599365 2.1291186809539795 0.23642438650131226
CurrentTrain: epoch  5, batch     1 | loss: 2.3655431Losses:  6.868369102478027 6.647005081176758 0.22136402130126953
CurrentTrain: epoch  6, batch     0 | loss: 6.8683691Losses:  1.959918737411499 1.766271948814392 0.19364683330059052
CurrentTrain: epoch  6, batch     1 | loss: 1.9599187Losses:  7.208345413208008 7.017756938934326 0.19058863818645477
CurrentTrain: epoch  7, batch     0 | loss: 7.2083454Losses:  2.9239675998687744 2.709388494491577 0.21457910537719727
CurrentTrain: epoch  7, batch     1 | loss: 2.9239676Losses:  6.8972907066345215 6.692758083343506 0.20453283190727234
CurrentTrain: epoch  8, batch     0 | loss: 6.8972907Losses:  3.354762554168701 3.16810941696167 0.1866532415151596
CurrentTrain: epoch  8, batch     1 | loss: 3.3547626Losses:  5.73515510559082 5.565635681152344 0.16951924562454224
CurrentTrain: epoch  9, batch     0 | loss: 5.7351551Losses:  1.5274550914764404 1.3600229024887085 0.16743212938308716
CurrentTrain: epoch  9, batch     1 | loss: 1.5274551
Losses:  0.29133763909339905 -0.0 0.29133763909339905
MemoryTrain:  epoch  0, batch     0 | loss: 0.2913376Losses:  0.26870256662368774 -0.0 0.26870256662368774
MemoryTrain:  epoch  1, batch     0 | loss: 0.2687026Losses:  0.2525485157966614 -0.0 0.2525485157966614
MemoryTrain:  epoch  2, batch     0 | loss: 0.2525485Losses:  0.24272330105304718 -0.0 0.24272330105304718
MemoryTrain:  epoch  3, batch     0 | loss: 0.2427233Losses:  0.23054289817810059 -0.0 0.23054289817810059
MemoryTrain:  epoch  4, batch     0 | loss: 0.2305429Losses:  0.22155910730361938 -0.0 0.22155910730361938
MemoryTrain:  epoch  5, batch     0 | loss: 0.2215591Losses:  0.22149422764778137 -0.0 0.22149422764778137
MemoryTrain:  epoch  6, batch     0 | loss: 0.2214942Losses:  0.22137904167175293 -0.0 0.22137904167175293
MemoryTrain:  epoch  7, batch     0 | loss: 0.2213790Losses:  0.2137259691953659 -0.0 0.2137259691953659
MemoryTrain:  epoch  8, batch     0 | loss: 0.2137260Losses:  0.21750673651695251 -0.0 0.21750673651695251
MemoryTrain:  epoch  9, batch     0 | loss: 0.2175067
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 70.31%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 69.44%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 69.32%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 70.31%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 63.84%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 90.18%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 89.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 87.11%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 85.66%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 84.38%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 86.20%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.26%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.95%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 88.31%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.48%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 88.45%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 87.50%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 87.14%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 87.15%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 86.49%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 86.51%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 86.22%   [EVAL] batch:   39 | acc: 37.50%,  total acc: 85.00%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 84.60%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 84.08%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 83.87%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 83.66%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 81.79%   [EVAL] batch:   46 | acc: 0.00%,  total acc: 80.05%   
cur_acc:  ['0.8731', '0.6384']
his_acc:  ['0.8731', '0.8005']
Clustering into  7  clusters
Clusters:  [1 0 2 1 0 4 6 1 5 0 1 1 2 2 3 2]
Losses:  7.855841159820557 7.543062686920166 0.31277844309806824
CurrentTrain: epoch  0, batch     0 | loss: 7.8558412Losses:  3.9713876247406006 3.720425844192505 0.2509617805480957
CurrentTrain: epoch  0, batch     1 | loss: 3.9713876Losses:  5.7582855224609375 5.554720878601074 0.20356450974941254
CurrentTrain: epoch  1, batch     0 | loss: 5.7582855Losses:  4.181468963623047 3.86329984664917 0.3181690573692322
CurrentTrain: epoch  1, batch     1 | loss: 4.1814690Losses:  6.678280830383301 6.460218906402588 0.21806201338768005
CurrentTrain: epoch  2, batch     0 | loss: 6.6782808Losses:  2.0246315002441406 1.779865026473999 0.24476651847362518
CurrentTrain: epoch  2, batch     1 | loss: 2.0246315Losses:  6.6039814949035645 6.414083957672119 0.1898975372314453
CurrentTrain: epoch  3, batch     0 | loss: 6.6039815Losses:  2.0886011123657227 1.833161473274231 0.2554396986961365
CurrentTrain: epoch  3, batch     1 | loss: 2.0886011Losses:  6.483919143676758 6.322170734405518 0.16174836456775665
CurrentTrain: epoch  4, batch     0 | loss: 6.4839191Losses:  2.4785306453704834 2.2856972217559814 0.1928335279226303
CurrentTrain: epoch  4, batch     1 | loss: 2.4785306Losses:  5.957325458526611 5.796616554260254 0.16070881485939026
CurrentTrain: epoch  5, batch     0 | loss: 5.9573255Losses:  2.5529510974884033 2.38167142868042 0.1712796688079834
CurrentTrain: epoch  5, batch     1 | loss: 2.5529511Losses:  5.491288185119629 5.329132556915283 0.16215543448925018
CurrentTrain: epoch  6, batch     0 | loss: 5.4912882Losses:  1.8997923135757446 1.7399470806121826 0.1598452627658844
CurrentTrain: epoch  6, batch     1 | loss: 1.8997923Losses:  8.181102752685547 8.023641586303711 0.15746119618415833
CurrentTrain: epoch  7, batch     0 | loss: 8.1811028Losses:  2.938976764678955 2.7888689041137695 0.15010780096054077
CurrentTrain: epoch  7, batch     1 | loss: 2.9389768Losses:  5.990753173828125 5.844862937927246 0.14589032530784607
CurrentTrain: epoch  8, batch     0 | loss: 5.9907532Losses:  2.7426745891571045 2.5859315395355225 0.15674299001693726
CurrentTrain: epoch  8, batch     1 | loss: 2.7426746Losses:  6.979515552520752 6.833409309387207 0.14610624313354492
CurrentTrain: epoch  9, batch     0 | loss: 6.9795156Losses:  4.343778133392334 4.220338821411133 0.12343946099281311
CurrentTrain: epoch  9, batch     1 | loss: 4.3437781
Losses:  0.5524749159812927 -0.0 0.5524749159812927
MemoryTrain:  epoch  0, batch     0 | loss: 0.5524749Losses:  0.5430755615234375 -0.0 0.5430755615234375
MemoryTrain:  epoch  1, batch     0 | loss: 0.5430756Losses:  0.5335413813591003 -0.0 0.5335413813591003
MemoryTrain:  epoch  2, batch     0 | loss: 0.5335414Losses:  0.5320751667022705 -0.0 0.5320751667022705
MemoryTrain:  epoch  3, batch     0 | loss: 0.5320752Losses:  0.5293641686439514 -0.0 0.5293641686439514
MemoryTrain:  epoch  4, batch     0 | loss: 0.5293642Losses:  0.5177076458930969 -0.0 0.5177076458930969
MemoryTrain:  epoch  5, batch     0 | loss: 0.5177076Losses:  0.5133868455886841 -0.0 0.5133868455886841
MemoryTrain:  epoch  6, batch     0 | loss: 0.5133868Losses:  0.5099501609802246 -0.0 0.5099501609802246
MemoryTrain:  epoch  7, batch     0 | loss: 0.5099502Losses:  0.5210673213005066 -0.0 0.5210673213005066
MemoryTrain:  epoch  8, batch     0 | loss: 0.5210673Losses:  0.5029916167259216 -0.0 0.5029916167259216
MemoryTrain:  epoch  9, batch     0 | loss: 0.5029916
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 50.00%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 43.75%   [EVAL] batch:    6 | acc: 12.50%,  total acc: 39.29%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 34.38%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 18.75%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 27.08%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 26.56%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 32.50%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 38.39%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 44.53%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 48.61%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 51.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 55.11%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 56.77%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 57.21%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 55.80%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 57.08%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 57.03%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 57.35%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 57.64%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 58.22%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 59.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 61.61%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 63.35%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 67.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 68.99%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 69.91%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 70.54%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 71.34%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 71.67%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 72.18%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 72.85%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 73.30%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 72.43%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 71.79%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 70.83%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 69.26%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 68.91%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 68.43%   [EVAL] batch:   39 | acc: 43.75%,  total acc: 67.81%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 67.68%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 67.86%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 67.44%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 67.61%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 67.92%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 67.26%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 67.42%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 68.10%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 68.24%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 67.00%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 65.69%   [EVAL] batch:   51 | acc: 12.50%,  total acc: 64.66%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 63.68%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 62.50%   
cur_acc:  ['0.8731', '0.6384', '0.3438']
his_acc:  ['0.8731', '0.8005', '0.6250']
Clustering into  9  clusters
Clusters:  [0 3 2 0 8 5 6 0 1 3 0 0 2 2 4 2 7 2 5 1 0]
Losses:  9.31239128112793 8.813793182373047 0.4985977113246918
CurrentTrain: epoch  0, batch     0 | loss: 9.3123913Losses:  3.6665420532226562 3.193624258041382 0.4729178845882416
CurrentTrain: epoch  0, batch     1 | loss: 3.6665421Losses:  5.864407539367676 5.484993934631348 0.3794136345386505
CurrentTrain: epoch  1, batch     0 | loss: 5.8644075Losses:  1.8696426153182983 1.3625123500823975 0.5071302652359009
CurrentTrain: epoch  1, batch     1 | loss: 1.8696426Losses:  7.9173760414123535 7.505019187927246 0.4123566746711731
CurrentTrain: epoch  2, batch     0 | loss: 7.9173760Losses:  2.962266445159912 2.6886990070343018 0.2735675573348999
CurrentTrain: epoch  2, batch     1 | loss: 2.9622664Losses:  6.140505790710449 5.757989883422852 0.38251596689224243
CurrentTrain: epoch  3, batch     0 | loss: 6.1405058Losses:  1.7525867223739624 1.407480001449585 0.34510675072669983
CurrentTrain: epoch  3, batch     1 | loss: 1.7525867Losses:  6.413938522338867 6.063437461853027 0.35050082206726074
CurrentTrain: epoch  4, batch     0 | loss: 6.4139385Losses:  1.772017002105713 1.4352937936782837 0.336723268032074
CurrentTrain: epoch  4, batch     1 | loss: 1.7720170Losses:  5.973553657531738 5.637038230895996 0.3365156352519989
CurrentTrain: epoch  5, batch     0 | loss: 5.9735537Losses:  2.110969066619873 1.8614442348480225 0.2495247721672058
CurrentTrain: epoch  5, batch     1 | loss: 2.1109691Losses:  6.502554893493652 6.169154644012451 0.33340033888816833
CurrentTrain: epoch  6, batch     0 | loss: 6.5025549Losses:  3.1361382007598877 2.805880308151245 0.3302578628063202
CurrentTrain: epoch  6, batch     1 | loss: 3.1361382Losses:  5.58284330368042 5.257063865661621 0.32577964663505554
CurrentTrain: epoch  7, batch     0 | loss: 5.5828433Losses:  1.7214560508728027 1.3886677026748657 0.3327883780002594
CurrentTrain: epoch  7, batch     1 | loss: 1.7214561Losses:  5.449249267578125 5.128625869750977 0.3206233084201813
CurrentTrain: epoch  8, batch     0 | loss: 5.4492493Losses:  1.802199363708496 1.4862948656082153 0.3159044682979584
CurrentTrain: epoch  8, batch     1 | loss: 1.8021994Losses:  5.848877429962158 5.535287857055664 0.3135896623134613
CurrentTrain: epoch  9, batch     0 | loss: 5.8488774Losses:  1.9418962001800537 1.6888198852539062 0.25307637453079224
CurrentTrain: epoch  9, batch     1 | loss: 1.9418962
Losses:  0.6415945291519165 -0.0 0.6415945291519165
MemoryTrain:  epoch  0, batch     0 | loss: 0.6415945Losses:  0.24160028994083405 -0.0 0.24160028994083405
MemoryTrain:  epoch  0, batch     1 | loss: 0.2416003Losses:  0.7163441181182861 -0.0 0.7163441181182861
MemoryTrain:  epoch  1, batch     0 | loss: 0.7163441Losses:  0.14685875177383423 -0.0 0.14685875177383423
MemoryTrain:  epoch  1, batch     1 | loss: 0.1468588Losses:  0.6941104531288147 -0.0 0.6941104531288147
MemoryTrain:  epoch  2, batch     0 | loss: 0.6941105Losses:  0.1766633689403534 -0.0 0.1766633689403534
MemoryTrain:  epoch  2, batch     1 | loss: 0.1766634Losses:  0.6787292957305908 -0.0 0.6787292957305908
MemoryTrain:  epoch  3, batch     0 | loss: 0.6787293Losses:  0.31364917755126953 -0.0 0.31364917755126953
MemoryTrain:  epoch  3, batch     1 | loss: 0.3136492Losses:  0.6734601855278015 -0.0 0.6734601855278015
MemoryTrain:  epoch  4, batch     0 | loss: 0.6734602Losses:  0.1481020748615265 -0.0 0.1481020748615265
MemoryTrain:  epoch  4, batch     1 | loss: 0.1481021Losses:  0.5463282465934753 -0.0 0.5463282465934753
MemoryTrain:  epoch  5, batch     0 | loss: 0.5463282Losses:  0.3332919776439667 -0.0 0.3332919776439667
MemoryTrain:  epoch  5, batch     1 | loss: 0.3332920Losses:  0.6118528842926025 -0.0 0.6118528842926025
MemoryTrain:  epoch  6, batch     0 | loss: 0.6118529Losses:  0.298051655292511 -0.0 0.298051655292511
MemoryTrain:  epoch  6, batch     1 | loss: 0.2980517Losses:  0.6006602644920349 -0.0 0.6006602644920349
MemoryTrain:  epoch  7, batch     0 | loss: 0.6006603Losses:  0.21589958667755127 -0.0 0.21589958667755127
MemoryTrain:  epoch  7, batch     1 | loss: 0.2158996Losses:  0.700088620185852 -0.0 0.700088620185852
MemoryTrain:  epoch  8, batch     0 | loss: 0.7000886Losses:  0.33391276001930237 -0.0 0.33391276001930237
MemoryTrain:  epoch  8, batch     1 | loss: 0.3339128Losses:  0.5920827984809875 -0.0 0.5920827984809875
MemoryTrain:  epoch  9, batch     0 | loss: 0.5920828Losses:  0.48213204741477966 -0.0 0.48213204741477966
MemoryTrain:  epoch  9, batch     1 | loss: 0.4821320
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 69.64%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 69.44%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 70.45%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 69.79%   [EVAL] batch:   12 | acc: 6.25%,  total acc: 64.90%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 27.08%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 25.00%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 31.25%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 37.50%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 42.19%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 45.83%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 48.75%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 50.57%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 52.60%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 52.88%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 52.23%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 53.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 53.91%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 54.41%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 54.86%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 55.92%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 57.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 59.52%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 61.36%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 63.04%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 66.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 67.31%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 67.82%   [EVAL] batch:   27 | acc: 75.00%,  total acc: 68.08%   [EVAL] batch:   28 | acc: 75.00%,  total acc: 68.32%   [EVAL] batch:   29 | acc: 62.50%,  total acc: 68.12%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 68.15%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 68.16%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 68.56%   [EVAL] batch:   33 | acc: 37.50%,  total acc: 67.65%   [EVAL] batch:   34 | acc: 18.75%,  total acc: 66.25%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 64.58%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 62.84%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 61.35%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 60.10%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 59.84%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 60.37%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 60.71%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 60.47%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 60.51%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 60.83%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 60.19%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 60.51%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 61.33%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 61.61%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 60.50%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 59.31%   [EVAL] batch:   51 | acc: 12.50%,  total acc: 58.41%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 57.55%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 57.75%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 58.07%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 58.59%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 58.99%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 59.05%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 59.11%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 58.75%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 59.12%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 59.17%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 59.33%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 59.77%   [EVAL] batch:   64 | acc: 43.75%,  total acc: 59.52%   [EVAL] batch:   65 | acc: 25.00%,  total acc: 59.00%   
cur_acc:  ['0.8731', '0.6384', '0.3438', '0.6490']
his_acc:  ['0.8731', '0.8005', '0.6250', '0.5900']
Clustering into  12  clusters
Clusters:  [ 4  1  0  4  8  3  6  2  5  1  2  4 11  0  9  0  7 10  3  5  2  0  0  0
  3  5]
Losses:  9.13304328918457 8.755537986755371 0.3775053918361664
CurrentTrain: epoch  0, batch     0 | loss: 9.1330433Losses:  2.307020664215088 2.1354668140411377 0.17155373096466064
CurrentTrain: epoch  0, batch     1 | loss: 2.3070207Losses:  8.32653522491455 8.110857009887695 0.21567866206169128
CurrentTrain: epoch  1, batch     0 | loss: 8.3265352Losses:  3.314786911010742 3.144763469696045 0.17002350091934204
CurrentTrain: epoch  1, batch     1 | loss: 3.3147869Losses:  8.62350082397461 8.454051971435547 0.16944855451583862
CurrentTrain: epoch  2, batch     0 | loss: 8.6235008Losses:  2.618582010269165 2.471951961517334 0.14663007855415344
CurrentTrain: epoch  2, batch     1 | loss: 2.6185820Losses:  7.280942440032959 7.132209777832031 0.1487325131893158
CurrentTrain: epoch  3, batch     0 | loss: 7.2809424Losses:  2.0595943927764893 1.920786738395691 0.13880768418312073
CurrentTrain: epoch  3, batch     1 | loss: 2.0595944Losses:  6.654984474182129 6.519619941711426 0.13536447286605835
CurrentTrain: epoch  4, batch     0 | loss: 6.6549845Losses:  4.628479480743408 4.570292949676514 0.05818669870495796
CurrentTrain: epoch  4, batch     1 | loss: 4.6284795Losses:  7.390109539031982 7.262396812438965 0.12771281599998474
CurrentTrain: epoch  5, batch     0 | loss: 7.3901095Losses:  2.510594606399536 2.3816637992858887 0.12893086671829224
CurrentTrain: epoch  5, batch     1 | loss: 2.5105946Losses:  6.839079856872559 6.719010353088379 0.1200694665312767
CurrentTrain: epoch  6, batch     0 | loss: 6.8390799Losses:  3.841480016708374 3.789058208465576 0.05242183804512024
CurrentTrain: epoch  6, batch     1 | loss: 3.8414800Losses:  5.956478595733643 5.8371686935424805 0.11930970847606659
CurrentTrain: epoch  7, batch     0 | loss: 5.9564786Losses:  3.143427610397339 3.0194613933563232 0.12396630644798279
CurrentTrain: epoch  7, batch     1 | loss: 3.1434276Losses:  6.870471000671387 6.751688480377197 0.11878260970115662
CurrentTrain: epoch  8, batch     0 | loss: 6.8704710Losses:  1.9982725381851196 1.8791024684906006 0.11917003989219666
CurrentTrain: epoch  8, batch     1 | loss: 1.9982725Losses:  6.3278584480285645 6.2125139236450195 0.11534455418586731
CurrentTrain: epoch  9, batch     0 | loss: 6.3278584Losses:  2.46163272857666 2.3424313068389893 0.11920134723186493
CurrentTrain: epoch  9, batch     1 | loss: 2.4616327
Losses:  0.6548749804496765 -0.0 0.6548749804496765
MemoryTrain:  epoch  0, batch     0 | loss: 0.6548750Losses:  0.7120168805122375 -0.0 0.7120168805122375
MemoryTrain:  epoch  0, batch     1 | loss: 0.7120169Losses:  0.9054891467094421 -0.0 0.9054891467094421
MemoryTrain:  epoch  1, batch     0 | loss: 0.9054891Losses:  0.5799771547317505 -0.0 0.5799771547317505
MemoryTrain:  epoch  1, batch     1 | loss: 0.5799772Losses:  0.852690577507019 -0.0 0.852690577507019
MemoryTrain:  epoch  2, batch     0 | loss: 0.8526906Losses:  0.7140087485313416 -0.0 0.7140087485313416
MemoryTrain:  epoch  2, batch     1 | loss: 0.7140087Losses:  0.6763242483139038 -0.0 0.6763242483139038
MemoryTrain:  epoch  3, batch     0 | loss: 0.6763242Losses:  0.623616635799408 -0.0 0.623616635799408
MemoryTrain:  epoch  3, batch     1 | loss: 0.6236166Losses:  0.7313411831855774 -0.0 0.7313411831855774
MemoryTrain:  epoch  4, batch     0 | loss: 0.7313412Losses:  0.5732868909835815 -0.0 0.5732868909835815
MemoryTrain:  epoch  4, batch     1 | loss: 0.5732869Losses:  0.8252459764480591 -0.0 0.8252459764480591
MemoryTrain:  epoch  5, batch     0 | loss: 0.8252460Losses:  0.5808118581771851 -0.0 0.5808118581771851
MemoryTrain:  epoch  5, batch     1 | loss: 0.5808119Losses:  0.7900162935256958 -0.0 0.7900162935256958
MemoryTrain:  epoch  6, batch     0 | loss: 0.7900163Losses:  0.6537530422210693 -0.0 0.6537530422210693
MemoryTrain:  epoch  6, batch     1 | loss: 0.6537530Losses:  0.6661837100982666 -0.0 0.6661837100982666
MemoryTrain:  epoch  7, batch     0 | loss: 0.6661837Losses:  0.6664289236068726 -0.0 0.6664289236068726
MemoryTrain:  epoch  7, batch     1 | loss: 0.6664289Losses:  0.7589240074157715 -0.0 0.7589240074157715
MemoryTrain:  epoch  8, batch     0 | loss: 0.7589240Losses:  0.6963552832603455 -0.0 0.6963552832603455
MemoryTrain:  epoch  8, batch     1 | loss: 0.6963553Losses:  0.7048575282096863 -0.0 0.7048575282096863
MemoryTrain:  epoch  9, batch     0 | loss: 0.7048575Losses:  0.6808343529701233 -0.0 0.6808343529701233
MemoryTrain:  epoch  9, batch     1 | loss: 0.6808344
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 27.08%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 23.44%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 21.25%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 20.83%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 20.54%   [EVAL] batch:    7 | acc: 18.75%,  total acc: 20.31%   [EVAL] batch:    8 | acc: 18.75%,  total acc: 20.14%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 23.12%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 23.86%   [EVAL] batch:   11 | acc: 25.00%,  total acc: 23.96%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 25.48%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 29.91%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 33.33%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 36.33%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 38.97%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 41.67%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 42.11%   [EVAL] batch:   19 | acc: 31.25%,  total acc: 41.56%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 41.37%   [EVAL] batch:   21 | acc: 6.25%,  total acc: 39.77%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 39.58%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 34.38%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 38.75%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 39.58%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 41.07%   [EVAL] batch:    7 | acc: 25.00%,  total acc: 39.06%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 42.36%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 45.00%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 44.32%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 43.75%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 41.35%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 41.52%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 43.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 44.53%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 45.59%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 46.53%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 48.03%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 50.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 52.38%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 54.55%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 56.52%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 58.33%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 60.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 61.54%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 62.27%   [EVAL] batch:   27 | acc: 75.00%,  total acc: 62.72%   [EVAL] batch:   28 | acc: 75.00%,  total acc: 63.15%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 63.54%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 64.11%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 64.45%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 64.96%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 63.60%   [EVAL] batch:   34 | acc: 18.75%,  total acc: 62.32%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 60.94%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 59.29%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 57.89%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 56.73%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 56.88%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 57.93%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 58.63%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 59.30%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 59.66%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 60.42%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 59.92%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 59.84%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 60.42%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 59.82%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 59.50%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 59.19%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 59.38%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 59.08%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 59.61%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 60.00%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 60.49%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 60.53%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 59.81%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 58.79%   [EVAL] batch:   59 | acc: 6.25%,  total acc: 57.92%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 57.99%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 58.37%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 58.63%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 58.89%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 58.37%   [EVAL] batch:   65 | acc: 25.00%,  total acc: 57.86%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 57.56%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 57.17%   [EVAL] batch:   68 | acc: 12.50%,  total acc: 56.52%   [EVAL] batch:   69 | acc: 6.25%,  total acc: 55.80%   [EVAL] batch:   70 | acc: 18.75%,  total acc: 55.28%   [EVAL] batch:   71 | acc: 18.75%,  total acc: 54.77%   [EVAL] batch:   72 | acc: 12.50%,  total acc: 54.20%   [EVAL] batch:   73 | acc: 18.75%,  total acc: 53.72%   [EVAL] batch:   74 | acc: 31.25%,  total acc: 53.42%   [EVAL] batch:   75 | acc: 43.75%,  total acc: 53.29%   [EVAL] batch:   76 | acc: 31.25%,  total acc: 53.00%   [EVAL] batch:   77 | acc: 31.25%,  total acc: 52.72%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 52.77%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 53.05%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 53.47%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 53.89%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 54.07%   [EVAL] batch:   83 | acc: 93.75%,  total acc: 54.54%   [EVAL] batch:   84 | acc: 37.50%,  total acc: 54.34%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 54.14%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 53.81%   [EVAL] batch:   87 | acc: 6.25%,  total acc: 53.27%   
cur_acc:  ['0.8731', '0.6384', '0.3438', '0.6490', '0.3977']
his_acc:  ['0.8731', '0.8005', '0.6250', '0.5900', '0.5327']
Clustering into  14  clusters
Clusters:  [ 4  3  1  4  0  2  9  4  7  3  5  4 12  1 10 13  6  8  2  7  5  1  1  1
  2  7 10 11  4  1  0]
Losses:  7.619037628173828 7.102917194366455 0.5161205530166626
CurrentTrain: epoch  0, batch     0 | loss: 7.6190376Losses:  2.817345380783081 2.386647939682007 0.4306974411010742
CurrentTrain: epoch  0, batch     1 | loss: 2.8173454Losses:  9.357069969177246 8.923332214355469 0.4337376654148102
CurrentTrain: epoch  1, batch     0 | loss: 9.3570700Losses:  7.186969757080078 6.949354648590088 0.2376151978969574
CurrentTrain: epoch  1, batch     1 | loss: 7.1869698Losses:  7.400822162628174 7.00127649307251 0.399545818567276
CurrentTrain: epoch  2, batch     0 | loss: 7.4008222Losses:  4.15911340713501 3.7834901809692383 0.37562307715415955
CurrentTrain: epoch  2, batch     1 | loss: 4.1591134Losses:  7.591344833374023 7.191153526306152 0.40019139647483826
CurrentTrain: epoch  3, batch     0 | loss: 7.5913448Losses:  2.7604968547821045 2.468759298324585 0.2917376160621643
CurrentTrain: epoch  3, batch     1 | loss: 2.7604969Losses:  6.945995807647705 6.563365459442139 0.38263025879859924
CurrentTrain: epoch  4, batch     0 | loss: 6.9459958Losses:  1.9346396923065186 1.7044861316680908 0.23015357553958893
CurrentTrain: epoch  4, batch     1 | loss: 1.9346397Losses:  5.676646709442139 5.3104658126831055 0.3661811053752899
CurrentTrain: epoch  5, batch     0 | loss: 5.6766467Losses:  2.2874257564544678 1.9130439758300781 0.37438175082206726
CurrentTrain: epoch  5, batch     1 | loss: 2.2874258Losses:  5.422609806060791 5.063180446624756 0.3594295382499695
CurrentTrain: epoch  6, batch     0 | loss: 5.4226098Losses:  1.2663885354995728 0.9079059958457947 0.3584825098514557
CurrentTrain: epoch  6, batch     1 | loss: 1.2663885Losses:  6.113441467285156 5.769539833068848 0.34390151500701904
CurrentTrain: epoch  7, batch     0 | loss: 6.1134415Losses:  2.1614832878112793 1.8574014902114868 0.3040817975997925
CurrentTrain: epoch  7, batch     1 | loss: 2.1614833Losses:  5.711897850036621 5.36718225479126 0.3447154462337494
CurrentTrain: epoch  8, batch     0 | loss: 5.7118979Losses:  1.7982795238494873 1.4660370349884033 0.3322424590587616
CurrentTrain: epoch  8, batch     1 | loss: 1.7982795Losses:  5.654288291931152 5.325539588928223 0.3287486135959625
CurrentTrain: epoch  9, batch     0 | loss: 5.6542883Losses:  1.8212867975234985 1.4913902282714844 0.32989656925201416
CurrentTrain: epoch  9, batch     1 | loss: 1.8212868
Losses:  0.9202136397361755 -0.0 0.9202136397361755
MemoryTrain:  epoch  0, batch     0 | loss: 0.9202136Losses:  0.6253455877304077 -0.0 0.6253455877304077
MemoryTrain:  epoch  0, batch     1 | loss: 0.6253456Losses:  0.847151517868042 -0.0 0.847151517868042
MemoryTrain:  epoch  1, batch     0 | loss: 0.8471515Losses:  0.6910625696182251 -0.0 0.6910625696182251
MemoryTrain:  epoch  1, batch     1 | loss: 0.6910626Losses:  0.852282702922821 -0.0 0.852282702922821
MemoryTrain:  epoch  2, batch     0 | loss: 0.8522827Losses:  0.8841415643692017 -0.0 0.8841415643692017
MemoryTrain:  epoch  2, batch     1 | loss: 0.8841416Losses:  0.7864140868186951 -0.0 0.7864140868186951
MemoryTrain:  epoch  3, batch     0 | loss: 0.7864141Losses:  0.6682500839233398 -0.0 0.6682500839233398
MemoryTrain:  epoch  3, batch     1 | loss: 0.6682501Losses:  0.9090754985809326 -0.0 0.9090754985809326
MemoryTrain:  epoch  4, batch     0 | loss: 0.9090755Losses:  0.7866104245185852 -0.0 0.7866104245185852
MemoryTrain:  epoch  4, batch     1 | loss: 0.7866104Losses:  0.7156654000282288 -0.0 0.7156654000282288
MemoryTrain:  epoch  5, batch     0 | loss: 0.7156654Losses:  0.6589555144309998 -0.0 0.6589555144309998
MemoryTrain:  epoch  5, batch     1 | loss: 0.6589555Losses:  0.7919167876243591 -0.0 0.7919167876243591
MemoryTrain:  epoch  6, batch     0 | loss: 0.7919168Losses:  0.7607995271682739 -0.0 0.7607995271682739
MemoryTrain:  epoch  6, batch     1 | loss: 0.7607995Losses:  0.7535500526428223 -0.0 0.7535500526428223
MemoryTrain:  epoch  7, batch     0 | loss: 0.7535501Losses:  0.73600834608078 -0.0 0.73600834608078
MemoryTrain:  epoch  7, batch     1 | loss: 0.7360083Losses:  0.7183351516723633 -0.0 0.7183351516723633
MemoryTrain:  epoch  8, batch     0 | loss: 0.7183352Losses:  0.803662121295929 -0.0 0.803662121295929
MemoryTrain:  epoch  8, batch     1 | loss: 0.8036621Losses:  0.8847990036010742 -0.0 0.8847990036010742
MemoryTrain:  epoch  9, batch     0 | loss: 0.8847990Losses:  0.6069340109825134 -0.0 0.6069340109825134
MemoryTrain:  epoch  9, batch     1 | loss: 0.6069340
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 98.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 98.96%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 99.11%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 99.22%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 97.92%   [EVAL] batch:    9 | acc: 6.25%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 18.75%,  total acc: 82.39%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 82.29%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 79.81%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 77.23%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 22.92%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 20.31%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 23.75%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 23.96%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 23.21%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 20.31%   [EVAL] batch:    8 | acc: 12.50%,  total acc: 19.44%   [EVAL] batch:    9 | acc: 6.25%,  total acc: 18.12%   [EVAL] batch:   10 | acc: 0.00%,  total acc: 16.48%   [EVAL] batch:   11 | acc: 18.75%,  total acc: 16.67%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 16.35%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 17.86%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 21.67%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 23.44%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 26.47%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 28.47%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 31.25%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 34.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 37.20%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 40.06%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 42.66%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 45.05%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 47.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 49.28%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 50.46%   [EVAL] batch:   27 | acc: 81.25%,  total acc: 51.56%   [EVAL] batch:   28 | acc: 81.25%,  total acc: 52.59%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 53.54%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 54.23%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 54.88%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 55.49%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 55.15%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 54.46%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 54.69%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 54.22%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 54.28%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 53.53%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 54.06%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 55.18%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 55.95%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 56.54%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 56.82%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 57.64%   [EVAL] batch:   45 | acc: 43.75%,  total acc: 57.34%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 57.58%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 58.07%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 57.65%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 58.13%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 58.70%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 59.38%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 60.02%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 60.65%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 61.02%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 61.50%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 61.40%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 61.64%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 62.18%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 62.40%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 61.89%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 61.39%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 60.91%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 60.64%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 59.71%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 59.00%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 58.49%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 57.90%   [EVAL] batch:   68 | acc: 12.50%,  total acc: 57.25%   [EVAL] batch:   69 | acc: 6.25%,  total acc: 56.52%   [EVAL] batch:   70 | acc: 12.50%,  total acc: 55.90%   [EVAL] batch:   71 | acc: 6.25%,  total acc: 55.21%   [EVAL] batch:   72 | acc: 12.50%,  total acc: 54.62%   [EVAL] batch:   73 | acc: 37.50%,  total acc: 54.39%   [EVAL] batch:   74 | acc: 18.75%,  total acc: 53.92%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 53.95%   [EVAL] batch:   76 | acc: 31.25%,  total acc: 53.65%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 53.45%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 53.32%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 53.52%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 53.78%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 53.81%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 53.84%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 53.87%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 53.82%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 53.63%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 53.45%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 53.84%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 54.35%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 54.86%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 55.36%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 55.77%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 56.72%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 57.17%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 57.62%   [EVAL] batch:   96 | acc: 12.50%,  total acc: 57.15%   [EVAL] batch:   97 | acc: 0.00%,  total acc: 56.57%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 56.82%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 56.88%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 56.81%   
cur_acc:  ['0.8731', '0.6384', '0.3438', '0.6490', '0.3977', '0.7723']
his_acc:  ['0.8731', '0.8005', '0.6250', '0.5900', '0.5327', '0.5681']
Clustering into  17  clusters
Clusters:  [ 1  3  9  1  0  8 13  1  4  3  5  1 14  9  6 15  2 10  8  4  5  9  9 16
  8  4  6  7  1  9 11  1  2 12  0  4]
Losses:  9.214705467224121 8.743387222290039 0.47131848335266113
CurrentTrain: epoch  0, batch     0 | loss: 9.2147055Losses:  6.628317356109619 6.336271286010742 0.2920462489128113
CurrentTrain: epoch  0, batch     1 | loss: 6.6283174Losses:  7.591973781585693 7.140348434448242 0.4516253173351288
CurrentTrain: epoch  1, batch     0 | loss: 7.5919738Losses:  2.3700969219207764 1.9869314432144165 0.383165568113327
CurrentTrain: epoch  1, batch     1 | loss: 2.3700969Losses:  6.438275337219238 5.952932357788086 0.4853430688381195
CurrentTrain: epoch  2, batch     0 | loss: 6.4382753Losses:  2.4781856536865234 2.1243202686309814 0.3538653552532196
CurrentTrain: epoch  2, batch     1 | loss: 2.4781857Losses:  6.3628363609313965 5.956883907318115 0.40595266222953796
CurrentTrain: epoch  3, batch     0 | loss: 6.3628364Losses:  3.07466721534729 2.654460906982422 0.42020636796951294
CurrentTrain: epoch  3, batch     1 | loss: 3.0746672Losses:  6.568233013153076 6.1564226150512695 0.4118104577064514
CurrentTrain: epoch  4, batch     0 | loss: 6.5682330Losses:  2.4222254753112793 1.9986364841461182 0.4235890507698059
CurrentTrain: epoch  4, batch     1 | loss: 2.4222255Losses:  7.601435661315918 7.18943452835083 0.4120010435581207
CurrentTrain: epoch  5, batch     0 | loss: 7.6014357Losses:  1.9534859657287598 1.7229137420654297 0.23057225346565247
CurrentTrain: epoch  5, batch     1 | loss: 1.9534860Losses:  6.099729061126709 5.633920669555664 0.4658084809780121
CurrentTrain: epoch  6, batch     0 | loss: 6.0997291Losses:  2.540907621383667 2.1656391620635986 0.3752685487270355
CurrentTrain: epoch  6, batch     1 | loss: 2.5409076Losses:  6.337573528289795 5.915727615356445 0.4218459725379944
CurrentTrain: epoch  7, batch     0 | loss: 6.3375735Losses:  2.065863609313965 1.705398678779602 0.3604649603366852
CurrentTrain: epoch  7, batch     1 | loss: 2.0658636Losses:  6.164139270782471 5.78596305847168 0.37817618250846863
CurrentTrain: epoch  8, batch     0 | loss: 6.1641393Losses:  2.9400877952575684 2.526319742202759 0.4137681722640991
CurrentTrain: epoch  8, batch     1 | loss: 2.9400878Losses:  5.674499034881592 5.290467739105225 0.3840315043926239
CurrentTrain: epoch  9, batch     0 | loss: 5.6744990Losses:  2.3973255157470703 2.026907205581665 0.3704183101654053
CurrentTrain: epoch  9, batch     1 | loss: 2.3973255
Losses:  0.8896196484565735 -0.0 0.8896196484565735
MemoryTrain:  epoch  0, batch     0 | loss: 0.8896196Losses:  0.9595451354980469 -0.0 0.9595451354980469
MemoryTrain:  epoch  0, batch     1 | loss: 0.9595451Losses:  0.3200022876262665 -0.0 0.3200022876262665
MemoryTrain:  epoch  0, batch     2 | loss: 0.3200023Losses:  0.8847876787185669 -0.0 0.8847876787185669
MemoryTrain:  epoch  1, batch     0 | loss: 0.8847877Losses:  0.8506358861923218 -0.0 0.8506358861923218
MemoryTrain:  epoch  1, batch     1 | loss: 0.8506359Losses:  0.25800585746765137 -0.0 0.25800585746765137
MemoryTrain:  epoch  1, batch     2 | loss: 0.2580059Losses:  0.8948896527290344 -0.0 0.8948896527290344
MemoryTrain:  epoch  2, batch     0 | loss: 0.8948897Losses:  0.8476273417472839 -0.0 0.8476273417472839
MemoryTrain:  epoch  2, batch     1 | loss: 0.8476273Losses:  0.15557105839252472 -0.0 0.15557105839252472
MemoryTrain:  epoch  2, batch     2 | loss: 0.1555711Losses:  0.7156798243522644 -0.0 0.7156798243522644
MemoryTrain:  epoch  3, batch     0 | loss: 0.7156798Losses:  0.9662427306175232 -0.0 0.9662427306175232
MemoryTrain:  epoch  3, batch     1 | loss: 0.9662427Losses:  0.1989179104566574 -0.0 0.1989179104566574
MemoryTrain:  epoch  3, batch     2 | loss: 0.1989179Losses:  0.9990985989570618 -0.0 0.9990985989570618
MemoryTrain:  epoch  4, batch     0 | loss: 0.9990986Losses:  0.7675995230674744 -0.0 0.7675995230674744
MemoryTrain:  epoch  4, batch     1 | loss: 0.7675995Losses:  0.29083794355392456 -0.0 0.29083794355392456
MemoryTrain:  epoch  4, batch     2 | loss: 0.2908379Losses:  0.809916615486145 -0.0 0.809916615486145
MemoryTrain:  epoch  5, batch     0 | loss: 0.8099166Losses:  0.8487321734428406 -0.0 0.8487321734428406
MemoryTrain:  epoch  5, batch     1 | loss: 0.8487322Losses:  0.2890395224094391 -0.0 0.2890395224094391
MemoryTrain:  epoch  5, batch     2 | loss: 0.2890395Losses:  0.8701788783073425 -0.0 0.8701788783073425
MemoryTrain:  epoch  6, batch     0 | loss: 0.8701789Losses:  0.8968720436096191 -0.0 0.8968720436096191
MemoryTrain:  epoch  6, batch     1 | loss: 0.8968720Losses:  0.09213283658027649 -0.0 0.09213283658027649
MemoryTrain:  epoch  6, batch     2 | loss: 0.0921328Losses:  0.821354329586029 -0.0 0.821354329586029
MemoryTrain:  epoch  7, batch     0 | loss: 0.8213543Losses:  0.9161977767944336 -0.0 0.9161977767944336
MemoryTrain:  epoch  7, batch     1 | loss: 0.9161978Losses:  0.31234532594680786 -0.0 0.31234532594680786
MemoryTrain:  epoch  7, batch     2 | loss: 0.3123453Losses:  0.8433749675750732 -0.0 0.8433749675750732
MemoryTrain:  epoch  8, batch     0 | loss: 0.8433750Losses:  0.8606234788894653 -0.0 0.8606234788894653
MemoryTrain:  epoch  8, batch     1 | loss: 0.8606235Losses:  0.2945953905582428 -0.0 0.2945953905582428
MemoryTrain:  epoch  8, batch     2 | loss: 0.2945954Losses:  0.888167142868042 -0.0 0.888167142868042
MemoryTrain:  epoch  9, batch     0 | loss: 0.8881671Losses:  0.819828987121582 -0.0 0.819828987121582
MemoryTrain:  epoch  9, batch     1 | loss: 0.8198290Losses:  0.31747448444366455 -0.0 0.31747448444366455
MemoryTrain:  epoch  9, batch     2 | loss: 0.3174745
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 76.88%   [EVAL] batch:   10 | acc: 18.75%,  total acc: 71.59%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 68.75%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 66.35%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 63.84%   [EVAL] batch:   14 | acc: 6.25%,  total acc: 60.00%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 54.69%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 57.50%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 52.08%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 47.32%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 41.41%   [EVAL] batch:    8 | acc: 6.25%,  total acc: 37.50%   [EVAL] batch:    9 | acc: 0.00%,  total acc: 33.75%   [EVAL] batch:   10 | acc: 6.25%,  total acc: 31.25%   [EVAL] batch:   11 | acc: 6.25%,  total acc: 29.17%   [EVAL] batch:   12 | acc: 6.25%,  total acc: 27.40%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 28.57%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 30.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 32.42%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 34.93%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 36.81%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 39.14%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 41.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 44.64%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 47.16%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 49.46%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 51.56%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 53.50%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 54.81%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 55.56%   [EVAL] batch:   27 | acc: 81.25%,  total acc: 56.47%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 57.76%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 58.33%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 58.67%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 59.77%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 59.47%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 58.09%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 56.61%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 55.03%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 53.55%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 52.14%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 50.96%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 51.41%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 52.59%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 53.12%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 53.92%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 54.40%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 55.00%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 54.35%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 53.59%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 53.26%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 52.81%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 53.50%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 54.41%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 55.17%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 56.01%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 56.71%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 57.16%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 57.70%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 57.68%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 58.08%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 58.69%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 59.06%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 59.53%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 59.13%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 59.47%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 58.56%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 57.77%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 57.28%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 56.80%   [EVAL] batch:   68 | acc: 18.75%,  total acc: 56.25%   [EVAL] batch:   69 | acc: 12.50%,  total acc: 55.62%   [EVAL] batch:   70 | acc: 25.00%,  total acc: 55.19%   [EVAL] batch:   71 | acc: 12.50%,  total acc: 54.60%   [EVAL] batch:   72 | acc: 12.50%,  total acc: 54.02%   [EVAL] batch:   73 | acc: 0.00%,  total acc: 53.29%   [EVAL] batch:   74 | acc: 18.75%,  total acc: 52.83%   [EVAL] batch:   75 | acc: 12.50%,  total acc: 52.30%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 51.95%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 51.52%   [EVAL] batch:   78 | acc: 25.00%,  total acc: 51.19%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 51.17%   [EVAL] batch:   80 | acc: 37.50%,  total acc: 51.00%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 50.61%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 50.23%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 49.93%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 49.63%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 49.27%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 48.85%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 49.22%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 49.51%   [EVAL] batch:   89 | acc: 87.50%,  total acc: 49.93%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 50.27%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 50.48%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 50.74%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 51.26%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 51.78%   [EVAL] batch:   95 | acc: 62.50%,  total acc: 51.89%   [EVAL] batch:   96 | acc: 0.00%,  total acc: 51.35%   [EVAL] batch:   97 | acc: 6.25%,  total acc: 50.89%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 51.20%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 51.38%   [EVAL] batch:  100 | acc: 75.00%,  total acc: 51.61%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 51.78%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 51.94%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 52.10%   [EVAL] batch:  104 | acc: 81.25%,  total acc: 52.38%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 52.59%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 52.92%   [EVAL] batch:  107 | acc: 93.75%,  total acc: 53.30%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 53.50%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 53.81%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 53.83%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 53.57%   [EVAL] batch:  112 | acc: 31.25%,  total acc: 53.37%   [EVAL] batch:  113 | acc: 43.75%,  total acc: 53.29%   [EVAL] batch:  114 | acc: 25.00%,  total acc: 53.04%   [EVAL] batch:  115 | acc: 6.25%,  total acc: 52.64%   
cur_acc:  ['0.8731', '0.6384', '0.3438', '0.6490', '0.3977', '0.7723', '0.6000']
his_acc:  ['0.8731', '0.8005', '0.6250', '0.5900', '0.5327', '0.5681', '0.5264']
Clustering into  18  clusters
Clusters:  [ 2  4  5  2  0 11 13  2  1  4  7  2 14  5  6 16  3 10 11  1  7  5  5 17
 11  1  6  8  2  5 12  2  3 15  0  1  2  9  7  1  0]
Losses:  9.350995063781738 8.727798461914062 0.6231966018676758
CurrentTrain: epoch  0, batch     0 | loss: 9.3509951Losses:  3.2845377922058105 2.977513551712036 0.3070243000984192
CurrentTrain: epoch  0, batch     1 | loss: 3.2845378Losses:  7.032484531402588 6.490270137786865 0.5422143340110779
CurrentTrain: epoch  1, batch     0 | loss: 7.0324845Losses:  1.8994402885437012 1.3804203271865845 0.5190199017524719
CurrentTrain: epoch  1, batch     1 | loss: 1.8994403Losses:  9.013813018798828 8.493961334228516 0.5198519825935364
CurrentTrain: epoch  2, batch     0 | loss: 9.0138130Losses:  3.5240633487701416 3.248828411102295 0.2752348780632019
CurrentTrain: epoch  2, batch     1 | loss: 3.5240633Losses:  7.013773441314697 6.543663024902344 0.47011032700538635
CurrentTrain: epoch  3, batch     0 | loss: 7.0137734Losses:  2.5597541332244873 2.1344544887542725 0.4252997040748596
CurrentTrain: epoch  3, batch     1 | loss: 2.5597541Losses:  6.564476013183594 6.109872817993164 0.45460301637649536
CurrentTrain: epoch  4, batch     0 | loss: 6.5644760Losses:  3.1414144039154053 2.673579692840576 0.46783462166786194
CurrentTrain: epoch  4, batch     1 | loss: 3.1414144Losses:  7.709577560424805 7.265923976898193 0.443653404712677
CurrentTrain: epoch  5, batch     0 | loss: 7.7095776Losses:  3.769090175628662 3.351522207260132 0.4175679683685303
CurrentTrain: epoch  5, batch     1 | loss: 3.7690902Losses:  6.332138538360596 5.904741287231445 0.42739740014076233
CurrentTrain: epoch  6, batch     0 | loss: 6.3321385Losses:  2.081397771835327 1.6273785829544067 0.454019159078598
CurrentTrain: epoch  6, batch     1 | loss: 2.0813978Losses:  6.027710914611816 5.593633651733398 0.43407726287841797
CurrentTrain: epoch  7, batch     0 | loss: 6.0277109Losses:  2.4408457279205322 2.0078530311584473 0.43299269676208496
CurrentTrain: epoch  7, batch     1 | loss: 2.4408457Losses:  6.844607353210449 6.424041748046875 0.4205656051635742
CurrentTrain: epoch  8, batch     0 | loss: 6.8446074Losses:  2.696535348892212 2.409940004348755 0.2865953743457794
CurrentTrain: epoch  8, batch     1 | loss: 2.6965353Losses:  7.320643424987793 6.897433757781982 0.42320963740348816
CurrentTrain: epoch  9, batch     0 | loss: 7.3206434Losses:  2.6685986518859863 2.417095422744751 0.2515031695365906
CurrentTrain: epoch  9, batch     1 | loss: 2.6685987
Losses:  0.8913822174072266 -0.0 0.8913822174072266
MemoryTrain:  epoch  0, batch     0 | loss: 0.8913822Losses:  0.9771326780319214 -0.0 0.9771326780319214
MemoryTrain:  epoch  0, batch     1 | loss: 0.9771327Losses:  0.2532985806465149 -0.0 0.2532985806465149
MemoryTrain:  epoch  0, batch     2 | loss: 0.2532986Losses:  0.9557784795761108 -0.0 0.9557784795761108
MemoryTrain:  epoch  1, batch     0 | loss: 0.9557785Losses:  0.7998569011688232 -0.0 0.7998569011688232
MemoryTrain:  epoch  1, batch     1 | loss: 0.7998569Losses:  0.6451765298843384 -0.0 0.6451765298843384
MemoryTrain:  epoch  1, batch     2 | loss: 0.6451765Losses:  0.6375440955162048 -0.0 0.6375440955162048
MemoryTrain:  epoch  2, batch     0 | loss: 0.6375441Losses:  1.0339980125427246 -0.0 1.0339980125427246
MemoryTrain:  epoch  2, batch     1 | loss: 1.0339980Losses:  0.5875411033630371 -0.0 0.5875411033630371
MemoryTrain:  epoch  2, batch     2 | loss: 0.5875411Losses:  0.8400065302848816 -0.0 0.8400065302848816
MemoryTrain:  epoch  3, batch     0 | loss: 0.8400065Losses:  0.7639861106872559 -0.0 0.7639861106872559
MemoryTrain:  epoch  3, batch     1 | loss: 0.7639861Losses:  0.7167655825614929 -0.0 0.7167655825614929
MemoryTrain:  epoch  3, batch     2 | loss: 0.7167656Losses:  0.7355743050575256 -0.0 0.7355743050575256
MemoryTrain:  epoch  4, batch     0 | loss: 0.7355743Losses:  0.9358829855918884 -0.0 0.9358829855918884
MemoryTrain:  epoch  4, batch     1 | loss: 0.9358830Losses:  0.483018159866333 -0.0 0.483018159866333
MemoryTrain:  epoch  4, batch     2 | loss: 0.4830182Losses:  0.7953884601593018 -0.0 0.7953884601593018
MemoryTrain:  epoch  5, batch     0 | loss: 0.7953885Losses:  0.8251349925994873 -0.0 0.8251349925994873
MemoryTrain:  epoch  5, batch     1 | loss: 0.8251350Losses:  0.5921655893325806 -0.0 0.5921655893325806
MemoryTrain:  epoch  5, batch     2 | loss: 0.5921656Losses:  0.7815597653388977 -0.0 0.7815597653388977
MemoryTrain:  epoch  6, batch     0 | loss: 0.7815598Losses:  0.8176760077476501 -0.0 0.8176760077476501
MemoryTrain:  epoch  6, batch     1 | loss: 0.8176760Losses:  0.34701892733573914 -0.0 0.34701892733573914
MemoryTrain:  epoch  6, batch     2 | loss: 0.3470189Losses:  0.8449617028236389 -0.0 0.8449617028236389
MemoryTrain:  epoch  7, batch     0 | loss: 0.8449617Losses:  0.7649936676025391 -0.0 0.7649936676025391
MemoryTrain:  epoch  7, batch     1 | loss: 0.7649937Losses:  0.779313325881958 -0.0 0.779313325881958
MemoryTrain:  epoch  7, batch     2 | loss: 0.7793133Losses:  0.7925610542297363 -0.0 0.7925610542297363
MemoryTrain:  epoch  8, batch     0 | loss: 0.7925611Losses:  0.8744763731956482 -0.0 0.8744763731956482
MemoryTrain:  epoch  8, batch     1 | loss: 0.8744764Losses:  0.4650030732154846 -0.0 0.4650030732154846
MemoryTrain:  epoch  8, batch     2 | loss: 0.4650031Losses:  0.8697808384895325 -0.0 0.8697808384895325
MemoryTrain:  epoch  9, batch     0 | loss: 0.8697808Losses:  0.5738537907600403 -0.0 0.5738537907600403
MemoryTrain:  epoch  9, batch     1 | loss: 0.5738538Losses:  0.7714298367500305 -0.0 0.7714298367500305
MemoryTrain:  epoch  9, batch     2 | loss: 0.7714298
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 25.00%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 28.12%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 27.50%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 26.04%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 30.36%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 35.16%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 35.42%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 40.00%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 42.61%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 47.40%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 51.44%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 54.91%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 57.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 60.55%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 62.87%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 60.76%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 66.25%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 63.54%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 57.14%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 50.00%   [EVAL] batch:    8 | acc: 0.00%,  total acc: 44.44%   [EVAL] batch:    9 | acc: 0.00%,  total acc: 40.00%   [EVAL] batch:   10 | acc: 6.25%,  total acc: 36.93%   [EVAL] batch:   11 | acc: 6.25%,  total acc: 34.38%   [EVAL] batch:   12 | acc: 18.75%,  total acc: 33.17%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 31.70%   [EVAL] batch:   14 | acc: 12.50%,  total acc: 30.42%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 29.69%   [EVAL] batch:   16 | acc: 6.25%,  total acc: 28.31%   [EVAL] batch:   17 | acc: 6.25%,  total acc: 27.08%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 27.30%   [EVAL] batch:   19 | acc: 31.25%,  total acc: 27.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 30.95%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 34.09%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 36.96%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 39.58%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 42.00%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 43.75%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 44.91%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 46.65%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 48.28%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 49.38%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 50.81%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 51.95%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 51.89%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 50.92%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 49.64%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 48.26%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 46.96%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 45.72%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 44.71%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 45.16%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 46.34%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 46.73%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 47.82%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 48.15%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 49.03%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 48.51%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 48.40%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 48.83%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 48.47%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 49.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 50.00%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 50.84%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 51.77%   [EVAL] batch:   53 | acc: 31.25%,  total acc: 51.39%   [EVAL] batch:   54 | acc: 31.25%,  total acc: 51.02%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 50.89%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 50.77%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 51.40%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 52.22%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 52.71%   [EVAL] batch:   60 | acc: 25.00%,  total acc: 52.25%   [EVAL] batch:   61 | acc: 6.25%,  total acc: 51.51%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 50.69%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 49.90%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 49.23%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 48.58%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 48.32%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 47.79%   [EVAL] batch:   68 | acc: 18.75%,  total acc: 47.37%   [EVAL] batch:   69 | acc: 12.50%,  total acc: 46.88%   [EVAL] batch:   70 | acc: 18.75%,  total acc: 46.48%   [EVAL] batch:   71 | acc: 12.50%,  total acc: 46.01%   [EVAL] batch:   72 | acc: 6.25%,  total acc: 45.46%   [EVAL] batch:   73 | acc: 18.75%,  total acc: 45.10%   [EVAL] batch:   74 | acc: 12.50%,  total acc: 44.67%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 44.49%   [EVAL] batch:   76 | acc: 6.25%,  total acc: 43.99%   [EVAL] batch:   77 | acc: 31.25%,  total acc: 43.83%   [EVAL] batch:   78 | acc: 25.00%,  total acc: 43.59%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 43.59%   [EVAL] batch:   80 | acc: 37.50%,  total acc: 43.52%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 43.37%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 43.30%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 43.01%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 42.72%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 42.22%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 41.81%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 42.26%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 42.77%   [EVAL] batch:   89 | acc: 87.50%,  total acc: 43.26%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 43.68%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 44.02%   [EVAL] batch:   92 | acc: 81.25%,  total acc: 44.42%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 45.01%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 45.59%   [EVAL] batch:   95 | acc: 62.50%,  total acc: 45.77%   [EVAL] batch:   96 | acc: 6.25%,  total acc: 45.36%   [EVAL] batch:   97 | acc: 6.25%,  total acc: 44.96%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 45.27%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 45.44%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 45.54%   [EVAL] batch:  101 | acc: 50.00%,  total acc: 45.59%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 46.00%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 46.39%   [EVAL] batch:  104 | acc: 81.25%,  total acc: 46.73%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 47.05%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 47.43%   [EVAL] batch:  107 | acc: 93.75%,  total acc: 47.86%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 48.11%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 48.47%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 48.54%   [EVAL] batch:  111 | acc: 12.50%,  total acc: 48.21%   [EVAL] batch:  112 | acc: 25.00%,  total acc: 48.01%   [EVAL] batch:  113 | acc: 37.50%,  total acc: 47.92%   [EVAL] batch:  114 | acc: 25.00%,  total acc: 47.72%   [EVAL] batch:  115 | acc: 31.25%,  total acc: 47.58%   [EVAL] batch:  116 | acc: 6.25%,  total acc: 47.22%   [EVAL] batch:  117 | acc: 37.50%,  total acc: 47.14%   [EVAL] batch:  118 | acc: 18.75%,  total acc: 46.90%   [EVAL] batch:  119 | acc: 37.50%,  total acc: 46.82%   [EVAL] batch:  120 | acc: 18.75%,  total acc: 46.59%   [EVAL] batch:  121 | acc: 37.50%,  total acc: 46.52%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 46.80%   [EVAL] batch:  123 | acc: 31.25%,  total acc: 46.67%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 46.95%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 47.12%   [EVAL] batch:  126 | acc: 87.50%,  total acc: 47.44%   [EVAL] batch:  127 | acc: 100.00%,  total acc: 47.85%   [EVAL] batch:  128 | acc: 100.00%,  total acc: 48.26%   [EVAL] batch:  129 | acc: 100.00%,  total acc: 48.65%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 49.05%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 49.43%   [EVAL] batch:  132 | acc: 62.50%,  total acc: 49.53%   
cur_acc:  ['0.8731', '0.6384', '0.3438', '0.6490', '0.3977', '0.7723', '0.6000', '0.6076']
his_acc:  ['0.8731', '0.8005', '0.6250', '0.5900', '0.5327', '0.5681', '0.5264', '0.4953']
--------Round  1
seed:  200
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 1 1 0]
Losses:  8.787555694580078 8.155998229980469 0.6315569877624512
CurrentTrain: epoch  0, batch     0 | loss: 8.7875557Losses:  10.110486030578613 9.469221115112305 0.641264796257019
CurrentTrain: epoch  0, batch     1 | loss: 10.1104860Losses:  12.647870063781738 12.029464721679688 0.6184056997299194
CurrentTrain: epoch  0, batch     2 | loss: 12.6478701Losses:  11.584057807922363 10.982751846313477 0.6013060212135315
CurrentTrain: epoch  0, batch     3 | loss: 11.5840578Losses:  9.167985916137695 8.538777351379395 0.6292083263397217
CurrentTrain: epoch  0, batch     4 | loss: 9.1679859Losses:  12.686342239379883 12.066719055175781 0.6196234226226807
CurrentTrain: epoch  0, batch     5 | loss: 12.6863422Losses:  10.639695167541504 10.033846855163574 0.6058483123779297
CurrentTrain: epoch  0, batch     6 | loss: 10.6396952Losses:  13.011667251586914 12.437002182006836 0.5746654272079468
CurrentTrain: epoch  0, batch     7 | loss: 13.0116673Losses:  9.611761093139648 9.025859832763672 0.5859014987945557
CurrentTrain: epoch  0, batch     8 | loss: 9.6117611Losses:  9.60853099822998 9.009432792663574 0.5990978479385376
CurrentTrain: epoch  0, batch     9 | loss: 9.6085310Losses:  7.358580589294434 6.793262481689453 0.5653179883956909
CurrentTrain: epoch  0, batch    10 | loss: 7.3585806Losses:  9.324036598205566 8.725137710571289 0.5988985896110535
CurrentTrain: epoch  0, batch    11 | loss: 9.3240366Losses:  11.805990219116211 11.225506782531738 0.5804836750030518
CurrentTrain: epoch  0, batch    12 | loss: 11.8059902Losses:  11.79110050201416 11.23773193359375 0.5533684492111206
CurrentTrain: epoch  0, batch    13 | loss: 11.7911005Losses:  8.775156021118164 8.247888565063477 0.5272670984268188
CurrentTrain: epoch  0, batch    14 | loss: 8.7751560Losses:  8.12130355834961 7.577033996582031 0.544269323348999
CurrentTrain: epoch  0, batch    15 | loss: 8.1213036Losses:  7.896849632263184 7.325648307800293 0.5712012052536011
CurrentTrain: epoch  0, batch    16 | loss: 7.8968496Losses:  9.675012588500977 9.166423797607422 0.5085892081260681
CurrentTrain: epoch  0, batch    17 | loss: 9.6750126Losses:  12.781641960144043 12.25007438659668 0.5315673351287842
CurrentTrain: epoch  0, batch    18 | loss: 12.7816420Losses:  9.569437026977539 9.06727123260498 0.5021653175354004
CurrentTrain: epoch  0, batch    19 | loss: 9.5694370Losses:  10.334747314453125 9.830842971801758 0.5039044618606567
CurrentTrain: epoch  0, batch    20 | loss: 10.3347473Losses:  13.75457763671875 13.315969467163086 0.43860769271850586
CurrentTrain: epoch  0, batch    21 | loss: 13.7545776Losses:  10.724376678466797 10.248916625976562 0.4754596948623657
CurrentTrain: epoch  0, batch    22 | loss: 10.7243767Losses:  9.241059303283691 8.768342018127441 0.4727171063423157
CurrentTrain: epoch  0, batch    23 | loss: 9.2410593Losses:  7.31610631942749 6.886634826660156 0.42947137355804443
CurrentTrain: epoch  0, batch    24 | loss: 7.3161063Losses:  7.245250225067139 6.806788444519043 0.4384617507457733
CurrentTrain: epoch  0, batch    25 | loss: 7.2452502Losses:  16.296951293945312 15.982810020446777 0.31414085626602173
CurrentTrain: epoch  0, batch    26 | loss: 16.2969513Losses:  11.92505168914795 11.476563453674316 0.44848835468292236
CurrentTrain: epoch  0, batch    27 | loss: 11.9250517Losses:  8.205181121826172 7.796840667724609 0.4083400070667267
CurrentTrain: epoch  0, batch    28 | loss: 8.2051811Losses:  7.6977667808532715 7.301493167877197 0.396273672580719
CurrentTrain: epoch  0, batch    29 | loss: 7.6977668Losses:  11.907785415649414 11.541349411010742 0.3664358854293823
CurrentTrain: epoch  0, batch    30 | loss: 11.9077854Losses:  9.052021026611328 8.70459270477295 0.3474279046058655
CurrentTrain: epoch  0, batch    31 | loss: 9.0520210Losses:  10.81455135345459 10.519344329833984 0.2952071726322174
CurrentTrain: epoch  0, batch    32 | loss: 10.8145514Losses:  8.114314079284668 7.826205730438232 0.2881079316139221
CurrentTrain: epoch  0, batch    33 | loss: 8.1143141Losses:  9.923049926757812 9.668342590332031 0.25470733642578125
CurrentTrain: epoch  0, batch    34 | loss: 9.9230499Losses:  12.87564754486084 12.601689338684082 0.27395790815353394
CurrentTrain: epoch  0, batch    35 | loss: 12.8756475Losses:  9.219391822814941 8.938507080078125 0.2808849811553955
CurrentTrain: epoch  0, batch    36 | loss: 9.2193918Losses:  3.8549928665161133 3.580688953399658 0.27430400252342224
CurrentTrain: epoch  0, batch    37 | loss: 3.8549929Losses:  10.246006965637207 10.004556655883789 0.24144986271858215
CurrentTrain: epoch  1, batch     0 | loss: 10.2460070Losses:  6.346192359924316 6.093052864074707 0.2531396746635437
CurrentTrain: epoch  1, batch     1 | loss: 6.3461924Losses:  7.64719820022583 7.432306289672852 0.21489179134368896
CurrentTrain: epoch  1, batch     2 | loss: 7.6471982Losses:  10.031188011169434 9.846919059753418 0.1842685341835022
CurrentTrain: epoch  1, batch     3 | loss: 10.0311880Losses:  12.016854286193848 11.804838180541992 0.21201638877391815
CurrentTrain: epoch  1, batch     4 | loss: 12.0168543Losses:  8.932168006896973 8.745326042175293 0.1868421584367752
CurrentTrain: epoch  1, batch     5 | loss: 8.9321680Losses:  9.19750690460205 9.01336669921875 0.18413987755775452
CurrentTrain: epoch  1, batch     6 | loss: 9.1975069Losses:  7.055068016052246 6.902090072631836 0.1529780626296997
CurrentTrain: epoch  1, batch     7 | loss: 7.0550680Losses:  8.880463600158691 8.705669403076172 0.1747945249080658
CurrentTrain: epoch  1, batch     8 | loss: 8.8804636Losses:  6.053748607635498 5.921298980712891 0.1324496567249298
CurrentTrain: epoch  1, batch     9 | loss: 6.0537486Losses:  6.439387321472168 6.2335381507873535 0.205849289894104
CurrentTrain: epoch  1, batch    10 | loss: 6.4393873Losses:  9.959407806396484 9.793773651123047 0.1656344085931778
CurrentTrain: epoch  1, batch    11 | loss: 9.9594078Losses:  7.866021633148193 7.74668550491333 0.11933605372905731
CurrentTrain: epoch  1, batch    12 | loss: 7.8660216Losses:  9.71337890625 9.590498924255371 0.12288033962249756
CurrentTrain: epoch  1, batch    13 | loss: 9.7133789Losses:  6.364236831665039 6.250658988952637 0.11357761174440384
CurrentTrain: epoch  1, batch    14 | loss: 6.3642368Losses:  6.082636833190918 5.971590042114258 0.11104663461446762
CurrentTrain: epoch  1, batch    15 | loss: 6.0826368Losses:  6.126950740814209 6.019743919372559 0.10720673948526382
CurrentTrain: epoch  1, batch    16 | loss: 6.1269507Losses:  5.698990345001221 5.598716735839844 0.10027344524860382
CurrentTrain: epoch  1, batch    17 | loss: 5.6989903Losses:  6.0186309814453125 5.927423000335693 0.09120820462703705
CurrentTrain: epoch  1, batch    18 | loss: 6.0186310Losses:  8.492337226867676 8.386857986450195 0.10547900199890137
CurrentTrain: epoch  1, batch    19 | loss: 8.4923372Losses:  8.473990440368652 8.394474029541016 0.07951628416776657
CurrentTrain: epoch  1, batch    20 | loss: 8.4739904Losses:  7.673065662384033 7.598510265350342 0.07455545663833618
CurrentTrain: epoch  1, batch    21 | loss: 7.6730657Losses:  9.630996704101562 9.548300743103027 0.08269606530666351
CurrentTrain: epoch  1, batch    22 | loss: 9.6309967Losses:  6.690401077270508 6.610583782196045 0.07981746643781662
CurrentTrain: epoch  1, batch    23 | loss: 6.6904011Losses:  8.150158882141113 8.077369689941406 0.07278900593519211
CurrentTrain: epoch  1, batch    24 | loss: 8.1501589Losses:  6.86221981048584 6.789600849151611 0.07261918485164642
CurrentTrain: epoch  1, batch    25 | loss: 6.8622198Losses:  7.127957820892334 7.051304817199707 0.07665296643972397
CurrentTrain: epoch  1, batch    26 | loss: 7.1279578Losses:  6.38873291015625 6.299186706542969 0.08954627066850662
CurrentTrain: epoch  1, batch    27 | loss: 6.3887329Losses:  6.084529876708984 6.024828910827637 0.05970092490315437
CurrentTrain: epoch  1, batch    28 | loss: 6.0845299Losses:  12.295243263244629 12.232645034790039 0.06259796023368835
CurrentTrain: epoch  1, batch    29 | loss: 12.2952433Losses:  5.960917949676514 5.893799304962158 0.06711849570274353
CurrentTrain: epoch  1, batch    30 | loss: 5.9609179Losses:  10.688373565673828 10.62094783782959 0.06742565333843231
CurrentTrain: epoch  1, batch    31 | loss: 10.6883736Losses:  8.834510803222656 8.753952026367188 0.08055847883224487
CurrentTrain: epoch  1, batch    32 | loss: 8.8345108Losses:  6.555233478546143 6.493011474609375 0.06222204118967056
CurrentTrain: epoch  1, batch    33 | loss: 6.5552335Losses:  7.500263690948486 7.429254531860352 0.07100938260555267
CurrentTrain: epoch  1, batch    34 | loss: 7.5002637Losses:  7.717146396636963 7.662208080291748 0.05493827536702156
CurrentTrain: epoch  1, batch    35 | loss: 7.7171464Losses:  6.4082512855529785 6.341216564178467 0.06703449040651321
CurrentTrain: epoch  1, batch    36 | loss: 6.4082513Losses:  3.6036581993103027 3.5496532917022705 0.054004792124032974
CurrentTrain: epoch  1, batch    37 | loss: 3.6036582Losses:  5.130492687225342 5.079120635986328 0.05137203633785248
CurrentTrain: epoch  2, batch     0 | loss: 5.1304927Losses:  6.533319473266602 6.479094505310059 0.054224833846092224
CurrentTrain: epoch  2, batch     1 | loss: 6.5333195Losses:  4.86536169052124 4.819601058959961 0.04576074331998825
CurrentTrain: epoch  2, batch     2 | loss: 4.8653617Losses:  5.8566694259643555 5.808175563812256 0.048493642359972
CurrentTrain: epoch  2, batch     3 | loss: 5.8566694Losses:  7.973424911499023 7.917009353637695 0.056415680795907974
CurrentTrain: epoch  2, batch     4 | loss: 7.9734249Losses:  7.223745346069336 7.168214797973633 0.05553049221634865
CurrentTrain: epoch  2, batch     5 | loss: 7.2237453Losses:  6.213044166564941 6.167349815368652 0.045694462954998016
CurrentTrain: epoch  2, batch     6 | loss: 6.2130442Losses:  5.930850505828857 5.880867958068848 0.04998274892568588
CurrentTrain: epoch  2, batch     7 | loss: 5.9308505Losses:  7.321917533874512 7.274576663970947 0.04734090343117714
CurrentTrain: epoch  2, batch     8 | loss: 7.3219175Losses:  5.662116050720215 5.6159844398498535 0.046131741255521774
CurrentTrain: epoch  2, batch     9 | loss: 5.6621161Losses:  8.329909324645996 8.282482147216797 0.047426968812942505
CurrentTrain: epoch  2, batch    10 | loss: 8.3299093Losses:  7.747247695922852 7.701444149017334 0.04580378532409668
CurrentTrain: epoch  2, batch    11 | loss: 7.7472477Losses:  5.972415924072266 5.923527717590332 0.0488884337246418
CurrentTrain: epoch  2, batch    12 | loss: 5.9724159Losses:  5.314347267150879 5.271461486816406 0.04288559779524803
CurrentTrain: epoch  2, batch    13 | loss: 5.3143473Losses:  5.198417663574219 5.149825096130371 0.04859277978539467
CurrentTrain: epoch  2, batch    14 | loss: 5.1984177Losses:  5.916807651519775 5.871626853942871 0.04518059641122818
CurrentTrain: epoch  2, batch    15 | loss: 5.9168077Losses:  10.799985885620117 10.758975982666016 0.04101007804274559
CurrentTrain: epoch  2, batch    16 | loss: 10.7999859Losses:  6.9759907722473145 6.928947925567627 0.04704280197620392
CurrentTrain: epoch  2, batch    17 | loss: 6.9759908Losses:  7.62518310546875 7.585267066955566 0.039916202425956726
CurrentTrain: epoch  2, batch    18 | loss: 7.6251831Losses:  4.777529716491699 4.738406658172607 0.03912312537431717
CurrentTrain: epoch  2, batch    19 | loss: 4.7775297Losses:  6.38397216796875 6.3482255935668945 0.03574657440185547
CurrentTrain: epoch  2, batch    20 | loss: 6.3839722Losses:  4.889693260192871 4.853924751281738 0.03576865792274475
CurrentTrain: epoch  2, batch    21 | loss: 4.8896933Losses:  10.101237297058105 10.06440258026123 0.036834679543972015
CurrentTrain: epoch  2, batch    22 | loss: 10.1012373Losses:  6.48185396194458 6.438302040100098 0.04355187714099884
CurrentTrain: epoch  2, batch    23 | loss: 6.4818540Losses:  5.847826957702637 5.8074727058410645 0.040354471653699875
CurrentTrain: epoch  2, batch    24 | loss: 5.8478270Losses:  5.227432727813721 5.190544128417969 0.03688867762684822
CurrentTrain: epoch  2, batch    25 | loss: 5.2274327Losses:  7.546085357666016 7.5079569816589355 0.03812851384282112
CurrentTrain: epoch  2, batch    26 | loss: 7.5460854Losses:  7.319743633270264 7.262706756591797 0.05703669786453247
CurrentTrain: epoch  2, batch    27 | loss: 7.3197436Losses:  6.0628180503845215 6.026360034942627 0.0364580862224102
CurrentTrain: epoch  2, batch    28 | loss: 6.0628181Losses:  6.6595988273620605 6.621476173400879 0.038122545927762985
CurrentTrain: epoch  2, batch    29 | loss: 6.6595988Losses:  7.381645679473877 7.342288017272949 0.03935752809047699
CurrentTrain: epoch  2, batch    30 | loss: 7.3816457Losses:  8.849082946777344 8.80634880065918 0.0427338071167469
CurrentTrain: epoch  2, batch    31 | loss: 8.8490829Losses:  11.089946746826172 11.047428131103516 0.04251893237233162
CurrentTrain: epoch  2, batch    32 | loss: 11.0899467Losses:  5.694134712219238 5.652422904968262 0.04171156883239746
CurrentTrain: epoch  2, batch    33 | loss: 5.6941347Losses:  6.195013523101807 6.160120010375977 0.03489365801215172
CurrentTrain: epoch  2, batch    34 | loss: 6.1950135Losses:  7.584790229797363 7.54514217376709 0.03964824974536896
CurrentTrain: epoch  2, batch    35 | loss: 7.5847902Losses:  8.05396556854248 8.012097358703613 0.04186803102493286
CurrentTrain: epoch  2, batch    36 | loss: 8.0539656Losses:  1.65402352809906 1.616621732711792 0.037401825189590454
CurrentTrain: epoch  2, batch    37 | loss: 1.6540235Losses:  6.455592632293701 6.420833587646484 0.034759074449539185
CurrentTrain: epoch  3, batch     0 | loss: 6.4555926Losses:  5.661195278167725 5.628944396972656 0.03225106745958328
CurrentTrain: epoch  3, batch     1 | loss: 5.6611953Losses:  4.61077356338501 4.579841136932373 0.030932210385799408
CurrentTrain: epoch  3, batch     2 | loss: 4.6107736Losses:  5.5116987228393555 5.474742889404297 0.03695560246706009
CurrentTrain: epoch  3, batch     3 | loss: 5.5116987Losses:  5.523148536682129 5.490914344787598 0.032234277576208115
CurrentTrain: epoch  3, batch     4 | loss: 5.5231485Losses:  5.727823734283447 5.692224502563477 0.03559933602809906
CurrentTrain: epoch  3, batch     5 | loss: 5.7278237Losses:  6.40401554107666 6.372013092041016 0.0320025309920311
CurrentTrain: epoch  3, batch     6 | loss: 6.4040155Losses:  4.216060638427734 4.186245441436768 0.029815100133419037
CurrentTrain: epoch  3, batch     7 | loss: 4.2160606Losses:  9.365673065185547 9.315266609191895 0.05040687695145607
CurrentTrain: epoch  3, batch     8 | loss: 9.3656731Losses:  7.279581069946289 7.244457244873047 0.035124048590660095
CurrentTrain: epoch  3, batch     9 | loss: 7.2795811Losses:  5.720055103302002 5.685830593109131 0.03422435000538826
CurrentTrain: epoch  3, batch    10 | loss: 5.7200551Losses:  12.596820831298828 12.560884475708008 0.03593628853559494
CurrentTrain: epoch  3, batch    11 | loss: 12.5968208Losses:  6.44761848449707 6.411238670349121 0.03637963533401489
CurrentTrain: epoch  3, batch    12 | loss: 6.4476185Losses:  4.200977802276611 4.172325134277344 0.02865283563733101
CurrentTrain: epoch  3, batch    13 | loss: 4.2009778Losses:  5.4519944190979 5.420948505401611 0.03104570135474205
CurrentTrain: epoch  3, batch    14 | loss: 5.4519944Losses:  5.374879360198975 5.341784477233887 0.03309471160173416
CurrentTrain: epoch  3, batch    15 | loss: 5.3748794Losses:  5.324450969696045 5.295510292053223 0.02894083596765995
CurrentTrain: epoch  3, batch    16 | loss: 5.3244510Losses:  7.261292934417725 7.217535972595215 0.043757058680057526
CurrentTrain: epoch  3, batch    17 | loss: 7.2612929Losses:  4.570120811462402 4.542514801025391 0.027606002986431122
CurrentTrain: epoch  3, batch    18 | loss: 4.5701208Losses:  5.077149391174316 5.045788764953613 0.03136056661605835
CurrentTrain: epoch  3, batch    19 | loss: 5.0771494Losses:  5.653166770935059 5.622282028198242 0.030884820967912674
CurrentTrain: epoch  3, batch    20 | loss: 5.6531668Losses:  6.467483043670654 6.434920787811279 0.03256238251924515
CurrentTrain: epoch  3, batch    21 | loss: 6.4674830Losses:  5.450290203094482 5.415755271911621 0.03453516215085983
CurrentTrain: epoch  3, batch    22 | loss: 5.4502902Losses:  4.439492702484131 4.412566184997559 0.02692645788192749
CurrentTrain: epoch  3, batch    23 | loss: 4.4394927Losses:  5.348100662231445 5.314336776733398 0.03376379609107971
CurrentTrain: epoch  3, batch    24 | loss: 5.3481007Losses:  5.99436616897583 5.962310791015625 0.032055579125881195
CurrentTrain: epoch  3, batch    25 | loss: 5.9943662Losses:  6.306975841522217 6.274166584014893 0.032809171825647354
CurrentTrain: epoch  3, batch    26 | loss: 6.3069758Losses:  6.580149173736572 6.553155899047852 0.026993412524461746
CurrentTrain: epoch  3, batch    27 | loss: 6.5801492Losses:  8.97315788269043 8.94412612915039 0.029031451791524887
CurrentTrain: epoch  3, batch    28 | loss: 8.9731579Losses:  8.831341743469238 8.786880493164062 0.044461414217948914
CurrentTrain: epoch  3, batch    29 | loss: 8.8313417Losses:  4.7386980056762695 4.710280895233154 0.028417302295565605
CurrentTrain: epoch  3, batch    30 | loss: 4.7386980Losses:  7.510490417480469 7.456438064575195 0.05405228212475777
CurrentTrain: epoch  3, batch    31 | loss: 7.5104904Losses:  5.996068954467773 5.967470169067383 0.028598621487617493
CurrentTrain: epoch  3, batch    32 | loss: 5.9960690Losses:  5.265803813934326 5.236150741577148 0.029653096571564674
CurrentTrain: epoch  3, batch    33 | loss: 5.2658038Losses:  4.867105007171631 4.835822105407715 0.031282927840948105
CurrentTrain: epoch  3, batch    34 | loss: 4.8671050Losses:  5.253918170928955 5.2234039306640625 0.030514422804117203
CurrentTrain: epoch  3, batch    35 | loss: 5.2539182Losses:  4.981908798217773 4.948480129241943 0.03342881798744202
CurrentTrain: epoch  3, batch    36 | loss: 4.9819088Losses:  1.3198250532150269 1.2909374237060547 0.028887620195746422
CurrentTrain: epoch  3, batch    37 | loss: 1.3198251Losses:  4.4573187828063965 4.426918983459473 0.030399713665246964
CurrentTrain: epoch  4, batch     0 | loss: 4.4573188Losses:  5.776116371154785 5.747354507446289 0.028761953115463257
CurrentTrain: epoch  4, batch     1 | loss: 5.7761164Losses:  6.0457763671875 6.008763790130615 0.037012409418821335
CurrentTrain: epoch  4, batch     2 | loss: 6.0457764Losses:  7.7866339683532715 7.754591941833496 0.03204184025526047
CurrentTrain: epoch  4, batch     3 | loss: 7.7866340Losses:  5.3311967849731445 5.304731845855713 0.02646508440375328
CurrentTrain: epoch  4, batch     4 | loss: 5.3311968Losses:  7.0968337059021 7.063543319702148 0.03329046070575714
CurrentTrain: epoch  4, batch     5 | loss: 7.0968337Losses:  6.385885715484619 6.346810340881348 0.03907526284456253
CurrentTrain: epoch  4, batch     6 | loss: 6.3858857Losses:  5.00063943862915 4.971208095550537 0.029431108385324478
CurrentTrain: epoch  4, batch     7 | loss: 5.0006394Losses:  8.559464454650879 8.519327163696289 0.04013724997639656
CurrentTrain: epoch  4, batch     8 | loss: 8.5594645Losses:  6.438089370727539 6.40288782119751 0.03520144522190094
CurrentTrain: epoch  4, batch     9 | loss: 6.4380894Losses:  4.544743537902832 4.518924236297607 0.025819499045610428
CurrentTrain: epoch  4, batch    10 | loss: 4.5447435Losses:  5.147770404815674 5.115425109863281 0.032345082610845566
CurrentTrain: epoch  4, batch    11 | loss: 5.1477704Losses:  5.094579219818115 5.064671993255615 0.02990737184882164
CurrentTrain: epoch  4, batch    12 | loss: 5.0945792Losses:  4.658491134643555 4.6294050216674805 0.029086222872138023
CurrentTrain: epoch  4, batch    13 | loss: 4.6584911Losses:  4.456689834594727 4.429394245147705 0.02729540504515171
CurrentTrain: epoch  4, batch    14 | loss: 4.4566898Losses:  6.331554889678955 6.298394203186035 0.03316090628504753
CurrentTrain: epoch  4, batch    15 | loss: 6.3315549Losses:  5.234672546386719 5.2069807052612305 0.02769169583916664
CurrentTrain: epoch  4, batch    16 | loss: 5.2346725Losses:  5.80422830581665 5.779366493225098 0.024861736223101616
CurrentTrain: epoch  4, batch    17 | loss: 5.8042283Losses:  5.545341968536377 5.506092548370361 0.039249274879693985
CurrentTrain: epoch  4, batch    18 | loss: 5.5453420Losses:  5.887942790985107 5.849764347076416 0.03817860037088394
CurrentTrain: epoch  4, batch    19 | loss: 5.8879428Losses:  4.936498165130615 4.908570289611816 0.02792775072157383
CurrentTrain: epoch  4, batch    20 | loss: 4.9364982Losses:  8.929286003112793 8.89834976196289 0.03093579225242138
CurrentTrain: epoch  4, batch    21 | loss: 8.9292860Losses:  5.191553115844727 5.160609245300293 0.030943768098950386
CurrentTrain: epoch  4, batch    22 | loss: 5.1915531Losses:  10.141547203063965 10.111988067626953 0.029558774083852768
CurrentTrain: epoch  4, batch    23 | loss: 10.1415472Losses:  5.344688415527344 5.313320636749268 0.03136801719665527
CurrentTrain: epoch  4, batch    24 | loss: 5.3446884Losses:  6.1956257820129395 6.163762092590332 0.03186345472931862
CurrentTrain: epoch  4, batch    25 | loss: 6.1956258Losses:  5.39116907119751 5.3660969734191895 0.02507212571799755
CurrentTrain: epoch  4, batch    26 | loss: 5.3911691Losses:  6.736086368560791 6.7081146240234375 0.027971800416707993
CurrentTrain: epoch  4, batch    27 | loss: 6.7360864Losses:  5.634974479675293 5.602053642272949 0.03292066603899002
CurrentTrain: epoch  4, batch    28 | loss: 5.6349745Losses:  6.012434482574463 5.983234882354736 0.02919957786798477
CurrentTrain: epoch  4, batch    29 | loss: 6.0124345Losses:  5.749530792236328 5.7242560386657715 0.0252749752253294
CurrentTrain: epoch  4, batch    30 | loss: 5.7495308Losses:  4.406650066375732 4.382845401763916 0.023804523050785065
CurrentTrain: epoch  4, batch    31 | loss: 4.4066501Losses:  6.634546756744385 6.599276542663574 0.03527003526687622
CurrentTrain: epoch  4, batch    32 | loss: 6.6345468Losses:  5.193093776702881 5.159110069274902 0.033983901143074036
CurrentTrain: epoch  4, batch    33 | loss: 5.1930938Losses:  3.6590025424957275 3.6364338397979736 0.02256881818175316
CurrentTrain: epoch  4, batch    34 | loss: 3.6590025Losses:  7.25323486328125 7.218175888061523 0.03505890071392059
CurrentTrain: epoch  4, batch    35 | loss: 7.2532349Losses:  5.742297649383545 5.707980155944824 0.034317657351493835
CurrentTrain: epoch  4, batch    36 | loss: 5.7422976Losses:  0.938157320022583 0.9086816310882568 0.02947567217051983
CurrentTrain: epoch  4, batch    37 | loss: 0.9381573Losses:  5.358642101287842 5.328239440917969 0.03040289506316185
CurrentTrain: epoch  5, batch     0 | loss: 5.3586421Losses:  5.878187656402588 5.848883152008057 0.029304467141628265
CurrentTrain: epoch  5, batch     1 | loss: 5.8781877Losses:  6.43271017074585 6.400993347167969 0.03171679377555847
CurrentTrain: epoch  5, batch     2 | loss: 6.4327102Losses:  7.130446910858154 7.093426704406738 0.037020280957221985
CurrentTrain: epoch  5, batch     3 | loss: 7.1304469Losses:  5.947443008422852 5.913470268249512 0.0339728407561779
CurrentTrain: epoch  5, batch     4 | loss: 5.9474430Losses:  5.743852615356445 5.714958190917969 0.028894338756799698
CurrentTrain: epoch  5, batch     5 | loss: 5.7438526Losses:  6.843676567077637 6.815912246704102 0.027764396741986275
CurrentTrain: epoch  5, batch     6 | loss: 6.8436766Losses:  4.952129364013672 4.926372051239014 0.025757404044270515
CurrentTrain: epoch  5, batch     7 | loss: 4.9521294Losses:  5.309904098510742 5.281410217285156 0.02849365770816803
CurrentTrain: epoch  5, batch     8 | loss: 5.3099041Losses:  5.562649726867676 5.532747745513916 0.02990216575562954
CurrentTrain: epoch  5, batch     9 | loss: 5.5626497Losses:  4.378043174743652 4.3532609939575195 0.02478202059864998
CurrentTrain: epoch  5, batch    10 | loss: 4.3780432Losses:  5.310534954071045 5.276101112365723 0.03443364426493645
CurrentTrain: epoch  5, batch    11 | loss: 5.3105350Losses:  11.823744773864746 11.785127639770508 0.03861682489514351
CurrentTrain: epoch  5, batch    12 | loss: 11.8237448Losses:  6.8267107009887695 6.801827907562256 0.024882839992642403
CurrentTrain: epoch  5, batch    13 | loss: 6.8267107Losses:  3.894243001937866 3.8722479343414307 0.021995173767209053
CurrentTrain: epoch  5, batch    14 | loss: 3.8942430Losses:  7.961362361907959 7.923768997192383 0.03759352117776871
CurrentTrain: epoch  5, batch    15 | loss: 7.9613624Losses:  6.696800708770752 6.661060333251953 0.03574042022228241
CurrentTrain: epoch  5, batch    16 | loss: 6.6968007Losses:  6.800050735473633 6.773166179656982 0.026884334161877632
CurrentTrain: epoch  5, batch    17 | loss: 6.8000507Losses:  6.222495079040527 6.195093631744385 0.027401307597756386
CurrentTrain: epoch  5, batch    18 | loss: 6.2224951Losses:  8.818256378173828 8.782212257385254 0.03604403883218765
CurrentTrain: epoch  5, batch    19 | loss: 8.8182564Losses:  4.806975364685059 4.782503128051758 0.02447228878736496
CurrentTrain: epoch  5, batch    20 | loss: 4.8069754Losses:  5.530208110809326 5.503130912780762 0.027077186852693558
CurrentTrain: epoch  5, batch    21 | loss: 5.5302081Losses:  6.21818733215332 6.1901960372924805 0.027991507202386856
CurrentTrain: epoch  5, batch    22 | loss: 6.2181873Losses:  7.901657581329346 7.863917827606201 0.03773966431617737
CurrentTrain: epoch  5, batch    23 | loss: 7.9016576Losses:  5.084761142730713 5.059319496154785 0.025441640987992287
CurrentTrain: epoch  5, batch    24 | loss: 5.0847611Losses:  6.78790807723999 6.756889820098877 0.031018195673823357
CurrentTrain: epoch  5, batch    25 | loss: 6.7879081Losses:  5.1375885009765625 5.114831447601318 0.022757258266210556
CurrentTrain: epoch  5, batch    26 | loss: 5.1375885Losses:  5.646896839141846 5.618372917175293 0.028524011373519897
CurrentTrain: epoch  5, batch    27 | loss: 5.6468968Losses:  5.290797710418701 5.265481948852539 0.025315655395388603
CurrentTrain: epoch  5, batch    28 | loss: 5.2907977Losses:  6.083809852600098 6.054727554321289 0.029082518070936203
CurrentTrain: epoch  5, batch    29 | loss: 6.0838099Losses:  8.320281982421875 8.27901840209961 0.04126348719000816
CurrentTrain: epoch  5, batch    30 | loss: 8.3202820Losses:  6.079122066497803 6.0486159324646 0.030506212264299393
CurrentTrain: epoch  5, batch    31 | loss: 6.0791221Losses:  7.3252153396606445 7.29111385345459 0.03410142660140991
CurrentTrain: epoch  5, batch    32 | loss: 7.3252153Losses:  6.958761215209961 6.913141250610352 0.045619748532772064
CurrentTrain: epoch  5, batch    33 | loss: 6.9587612Losses:  7.4252214431762695 7.3883256912231445 0.036895688623189926
CurrentTrain: epoch  5, batch    34 | loss: 7.4252214Losses:  9.660697937011719 9.621427536010742 0.03927076607942581
CurrentTrain: epoch  5, batch    35 | loss: 9.6606979Losses:  7.895151615142822 7.859413146972656 0.03573829308152199
CurrentTrain: epoch  5, batch    36 | loss: 7.8951516Losses:  1.133836030960083 1.0996657609939575 0.03417032212018967
CurrentTrain: epoch  5, batch    37 | loss: 1.1338360Losses:  4.7586750984191895 4.7300238609313965 0.028651215136051178
CurrentTrain: epoch  6, batch     0 | loss: 4.7586751Losses:  3.9285941123962402 3.905874490737915 0.02271967940032482
CurrentTrain: epoch  6, batch     1 | loss: 3.9285941Losses:  6.914144515991211 6.87332820892334 0.04081634804606438
CurrentTrain: epoch  6, batch     2 | loss: 6.9141445Losses:  6.793971538543701 6.754181385040283 0.039790309965610504
CurrentTrain: epoch  6, batch     3 | loss: 6.7939715Losses:  4.52532958984375 4.500317096710205 0.025012368336319923
CurrentTrain: epoch  6, batch     4 | loss: 4.5253296Losses:  5.907379627227783 5.873229503631592 0.034150101244449615
CurrentTrain: epoch  6, batch     5 | loss: 5.9073796Losses:  6.746500015258789 6.703548431396484 0.04295160621404648
CurrentTrain: epoch  6, batch     6 | loss: 6.7465000Losses:  7.083929061889648 7.049130439758301 0.03479867801070213
CurrentTrain: epoch  6, batch     7 | loss: 7.0839291Losses:  6.195868968963623 6.164511680603027 0.03135721758008003
CurrentTrain: epoch  6, batch     8 | loss: 6.1958690Losses:  5.369955539703369 5.343382835388184 0.0265726987272501
CurrentTrain: epoch  6, batch     9 | loss: 5.3699555Losses:  6.048151969909668 6.0206756591796875 0.027476463466882706
CurrentTrain: epoch  6, batch    10 | loss: 6.0481520Losses:  4.428731441497803 4.400484085083008 0.028247423470020294
CurrentTrain: epoch  6, batch    11 | loss: 4.4287314Losses:  10.57589340209961 10.53042984008789 0.045463480055332184
CurrentTrain: epoch  6, batch    12 | loss: 10.5758934Losses:  6.149033546447754 6.121868133544922 0.0271652452647686
CurrentTrain: epoch  6, batch    13 | loss: 6.1490335Losses:  6.250588893890381 6.220427513122559 0.03016146831214428
CurrentTrain: epoch  6, batch    14 | loss: 6.2505889Losses:  5.807223796844482 5.780923843383789 0.026299983263015747
CurrentTrain: epoch  6, batch    15 | loss: 5.8072238Losses:  4.390437602996826 4.36767578125 0.022761862725019455
CurrentTrain: epoch  6, batch    16 | loss: 4.3904376Losses:  5.070728302001953 5.0416107177734375 0.02911742962896824
CurrentTrain: epoch  6, batch    17 | loss: 5.0707283Losses:  5.334181785583496 5.30265998840332 0.03152159973978996
CurrentTrain: epoch  6, batch    18 | loss: 5.3341818Losses:  5.826452732086182 5.802664756774902 0.023788167163729668
CurrentTrain: epoch  6, batch    19 | loss: 5.8264527Losses:  4.990716457366943 4.965649604797363 0.02506689541041851
CurrentTrain: epoch  6, batch    20 | loss: 4.9907165Losses:  5.407694339752197 5.382205486297607 0.025488823652267456
CurrentTrain: epoch  6, batch    21 | loss: 5.4076943Losses:  4.859073162078857 4.835619926452637 0.023453325033187866
CurrentTrain: epoch  6, batch    22 | loss: 4.8590732Losses:  11.774670600891113 11.737017631530762 0.03765328228473663
CurrentTrain: epoch  6, batch    23 | loss: 11.7746706Losses:  5.323318004608154 5.300754547119141 0.022563688457012177
CurrentTrain: epoch  6, batch    24 | loss: 5.3233180Losses:  10.159031867980957 10.121294975280762 0.0377366803586483
CurrentTrain: epoch  6, batch    25 | loss: 10.1590319Losses:  4.464993000030518 4.442685604095459 0.022307539358735085
CurrentTrain: epoch  6, batch    26 | loss: 4.4649930Losses:  4.22920560836792 4.20362663269043 0.02557893842458725
CurrentTrain: epoch  6, batch    27 | loss: 4.2292056Losses:  10.443490982055664 10.398433685302734 0.0450577586889267
CurrentTrain: epoch  6, batch    28 | loss: 10.4434910Losses:  6.381412506103516 6.356566429138184 0.02484598010778427
CurrentTrain: epoch  6, batch    29 | loss: 6.3814125Losses:  8.772658348083496 8.744356155395508 0.028302237391471863
CurrentTrain: epoch  6, batch    30 | loss: 8.7726583Losses:  3.9250123500823975 3.902927875518799 0.02208438143134117
CurrentTrain: epoch  6, batch    31 | loss: 3.9250124Losses:  5.159753799438477 5.132341384887695 0.027412615716457367
CurrentTrain: epoch  6, batch    32 | loss: 5.1597538Losses:  5.867568016052246 5.841388702392578 0.02617933228611946
CurrentTrain: epoch  6, batch    33 | loss: 5.8675680Losses:  9.223408699035645 9.18985366821289 0.033555399626493454
CurrentTrain: epoch  6, batch    34 | loss: 9.2234087Losses:  6.065277576446533 6.035269737243652 0.030007850378751755
CurrentTrain: epoch  6, batch    35 | loss: 6.0652776Losses:  15.710065841674805 15.681005477905273 0.029060285538434982
CurrentTrain: epoch  6, batch    36 | loss: 15.7100658Losses:  1.7310556173324585 1.6947433948516846 0.036312274634838104
CurrentTrain: epoch  6, batch    37 | loss: 1.7310556Losses:  9.56884479522705 9.539485931396484 0.029359107837080956
CurrentTrain: epoch  7, batch     0 | loss: 9.5688448Losses:  5.242628574371338 5.215844631195068 0.026783715933561325
CurrentTrain: epoch  7, batch     1 | loss: 5.2426286Losses:  6.710989952087402 6.677788257598877 0.03320155665278435
CurrentTrain: epoch  7, batch     2 | loss: 6.7109900Losses:  4.6392316818237305 4.618335723876953 0.020895756781101227
CurrentTrain: epoch  7, batch     3 | loss: 4.6392317Losses:  7.376553058624268 7.351151943206787 0.025401022285223007
CurrentTrain: epoch  7, batch     4 | loss: 7.3765531Losses:  4.550665855407715 4.526520729064941 0.024145297706127167
CurrentTrain: epoch  7, batch     5 | loss: 4.5506659Losses:  8.1791353225708 8.126710891723633 0.05242461338639259
CurrentTrain: epoch  7, batch     6 | loss: 8.1791353Losses:  5.538268089294434 5.50515604019165 0.03311183676123619
CurrentTrain: epoch  7, batch     7 | loss: 5.5382681Losses:  6.156103134155273 6.127030372619629 0.02907261811196804
CurrentTrain: epoch  7, batch     8 | loss: 6.1561031Losses:  4.107364654541016 4.086209774017334 0.02115495316684246
CurrentTrain: epoch  7, batch     9 | loss: 4.1073647Losses:  7.454293727874756 7.42988395690918 0.0244097039103508
CurrentTrain: epoch  7, batch    10 | loss: 7.4542937Losses:  7.207198143005371 7.165318489074707 0.041879571974277496
CurrentTrain: epoch  7, batch    11 | loss: 7.2071981Losses:  7.512763977050781 7.467890739440918 0.0448731854557991
CurrentTrain: epoch  7, batch    12 | loss: 7.5127640Losses:  5.1333112716674805 5.10992956161499 0.023381590843200684
CurrentTrain: epoch  7, batch    13 | loss: 5.1333113Losses:  5.557421684265137 5.532236576080322 0.0251852385699749
CurrentTrain: epoch  7, batch    14 | loss: 5.5574217Losses:  10.359972953796387 10.324234008789062 0.035738755017519
CurrentTrain: epoch  7, batch    15 | loss: 10.3599730Losses:  5.862868309020996 5.839735984802246 0.02313223108649254
CurrentTrain: epoch  7, batch    16 | loss: 5.8628683Losses:  7.8768181800842285 7.844076156616211 0.03274214640259743
CurrentTrain: epoch  7, batch    17 | loss: 7.8768182Losses:  5.257051467895508 5.22707462310791 0.02997707761824131
CurrentTrain: epoch  7, batch    18 | loss: 5.2570515Losses:  4.1515212059021 4.128645420074463 0.022875584661960602
CurrentTrain: epoch  7, batch    19 | loss: 4.1515212Losses:  7.3345770835876465 7.287771224975586 0.04680575802922249
CurrentTrain: epoch  7, batch    20 | loss: 7.3345771Losses:  7.253660202026367 7.215871810913086 0.03778816759586334
CurrentTrain: epoch  7, batch    21 | loss: 7.2536602Losses:  8.380681991577148 8.34763240814209 0.033049557358026505
CurrentTrain: epoch  7, batch    22 | loss: 8.3806820Losses:  6.220353603363037 6.185487747192383 0.0348660871386528
CurrentTrain: epoch  7, batch    23 | loss: 6.2203536Losses:  7.904421329498291 7.869913101196289 0.03450804203748703
CurrentTrain: epoch  7, batch    24 | loss: 7.9044213Losses:  9.216546058654785 9.183772087097168 0.03277358412742615
CurrentTrain: epoch  7, batch    25 | loss: 9.2165461Losses:  6.319173812866211 6.2891154289245605 0.03005828708410263
CurrentTrain: epoch  7, batch    26 | loss: 6.3191738Losses:  11.790167808532715 11.740797996520996 0.049369536340236664
CurrentTrain: epoch  7, batch    27 | loss: 11.7901678Losses:  6.961237907409668 6.927355766296387 0.033882152289152145
CurrentTrain: epoch  7, batch    28 | loss: 6.9612379Losses:  4.408398628234863 4.386625289916992 0.02177322842180729
CurrentTrain: epoch  7, batch    29 | loss: 4.4083986Losses:  4.6144208908081055 4.587004661560059 0.02741638571023941
CurrentTrain: epoch  7, batch    30 | loss: 4.6144209Losses:  9.43910026550293 9.410072326660156 0.029028210788965225
CurrentTrain: epoch  7, batch    31 | loss: 9.4391003Losses:  5.965537071228027 5.936802864074707 0.028734037652611732
CurrentTrain: epoch  7, batch    32 | loss: 5.9655371Losses:  6.365129470825195 6.326622009277344 0.038507457822561264
CurrentTrain: epoch  7, batch    33 | loss: 6.3651295Losses:  7.996587753295898 7.9575090408325195 0.039078760892152786
CurrentTrain: epoch  7, batch    34 | loss: 7.9965878Losses:  4.974380016326904 4.948397636413574 0.02598259039223194
CurrentTrain: epoch  7, batch    35 | loss: 4.9743800Losses:  4.892537593841553 4.866260528564453 0.02627710811793804
CurrentTrain: epoch  7, batch    36 | loss: 4.8925376Losses:  2.930091142654419 2.8945913314819336 0.03549981862306595
CurrentTrain: epoch  7, batch    37 | loss: 2.9300911Losses:  5.067693710327148 5.045424461364746 0.02226906642317772
CurrentTrain: epoch  8, batch     0 | loss: 5.0676937Losses:  4.067859649658203 4.043517589569092 0.024342110380530357
CurrentTrain: epoch  8, batch     1 | loss: 4.0678596Losses:  5.775179862976074 5.745965957641602 0.029213933274149895
CurrentTrain: epoch  8, batch     2 | loss: 5.7751799Losses:  4.720341205596924 4.693025588989258 0.027315473183989525
CurrentTrain: epoch  8, batch     3 | loss: 4.7203412Losses:  5.150820732116699 5.123774528503418 0.027046380564570427
CurrentTrain: epoch  8, batch     4 | loss: 5.1508207Losses:  5.174942970275879 5.146725654602051 0.02821752242743969
CurrentTrain: epoch  8, batch     5 | loss: 5.1749430Losses:  6.841402053833008 6.8112473487854 0.030154643580317497
CurrentTrain: epoch  8, batch     6 | loss: 6.8414021Losses:  5.38142204284668 5.3585004806518555 0.022921374067664146
CurrentTrain: epoch  8, batch     7 | loss: 5.3814220Losses:  5.7480292320251465 5.720430374145508 0.027598639950156212
CurrentTrain: epoch  8, batch     8 | loss: 5.7480292Losses:  4.492099285125732 4.467524528503418 0.02457488514482975
CurrentTrain: epoch  8, batch     9 | loss: 4.4920993Losses:  5.105030536651611 5.082433223724365 0.022597430273890495
CurrentTrain: epoch  8, batch    10 | loss: 5.1050305Losses:  5.7233428955078125 5.694500923156738 0.02884187363088131
CurrentTrain: epoch  8, batch    11 | loss: 5.7233429Losses:  5.005532264709473 4.977245330810547 0.028286824002861977
CurrentTrain: epoch  8, batch    12 | loss: 5.0055323Losses:  4.13599157333374 4.112384796142578 0.023606788367033005
CurrentTrain: epoch  8, batch    13 | loss: 4.1359916Losses:  4.580716133117676 4.555333137512207 0.025382835417985916
CurrentTrain: epoch  8, batch    14 | loss: 4.5807161Losses:  6.44551944732666 6.415012836456299 0.030506791546940804
CurrentTrain: epoch  8, batch    15 | loss: 6.4455194Losses:  3.9444973468780518 3.9199626445770264 0.024534696713089943
CurrentTrain: epoch  8, batch    16 | loss: 3.9444973Losses:  13.071760177612305 13.035261154174805 0.03649923577904701
CurrentTrain: epoch  8, batch    17 | loss: 13.0717602Losses:  7.263158321380615 7.229193210601807 0.03396500647068024
CurrentTrain: epoch  8, batch    18 | loss: 7.2631583Losses:  5.101986885070801 5.066910743713379 0.035075996071100235
CurrentTrain: epoch  8, batch    19 | loss: 5.1019869Losses:  5.38873291015625 5.366102695465088 0.022630061954259872
CurrentTrain: epoch  8, batch    20 | loss: 5.3887329Losses:  4.542511940002441 4.51613712310791 0.026374932378530502
CurrentTrain: epoch  8, batch    21 | loss: 4.5425119Losses:  6.82560920715332 6.78507137298584 0.04053764417767525
CurrentTrain: epoch  8, batch    22 | loss: 6.8256092Losses:  5.479480266571045 5.446028232574463 0.03345199301838875
CurrentTrain: epoch  8, batch    23 | loss: 5.4794803Losses:  12.146745681762695 12.106122016906738 0.04062383994460106
CurrentTrain: epoch  8, batch    24 | loss: 12.1467457Losses:  11.041839599609375 10.997833251953125 0.04400680586695671
CurrentTrain: epoch  8, batch    25 | loss: 11.0418396Losses:  5.41964864730835 5.390789031982422 0.028859708458185196
CurrentTrain: epoch  8, batch    26 | loss: 5.4196486Losses:  5.824044704437256 5.796426773071289 0.027617890387773514
CurrentTrain: epoch  8, batch    27 | loss: 5.8240447Losses:  5.300917148590088 5.276136875152588 0.02478039264678955
CurrentTrain: epoch  8, batch    28 | loss: 5.3009171Losses:  5.311079978942871 5.284551620483398 0.026528354734182358
CurrentTrain: epoch  8, batch    29 | loss: 5.3110800Losses:  4.819269180297852 4.792153358459473 0.027115706354379654
CurrentTrain: epoch  8, batch    30 | loss: 4.8192692Losses:  4.171741962432861 4.150190353393555 0.0215515848249197
CurrentTrain: epoch  8, batch    31 | loss: 4.1717420Losses:  5.2893195152282715 5.260400772094727 0.02891877479851246
CurrentTrain: epoch  8, batch    32 | loss: 5.2893195Losses:  5.05857515335083 5.02735710144043 0.031217820942401886
CurrentTrain: epoch  8, batch    33 | loss: 5.0585752Losses:  4.785533905029297 4.763618469238281 0.02191563881933689
CurrentTrain: epoch  8, batch    34 | loss: 4.7855339Losses:  5.38178014755249 5.352236270904541 0.029543960466980934
CurrentTrain: epoch  8, batch    35 | loss: 5.3817801Losses:  11.158291816711426 11.10006332397461 0.05822881683707237
CurrentTrain: epoch  8, batch    36 | loss: 11.1582918Losses:  2.9506423473358154 2.914834976196289 0.035807304084300995
CurrentTrain: epoch  8, batch    37 | loss: 2.9506423Losses:  4.557297706604004 4.530597686767578 0.026700034737586975
CurrentTrain: epoch  9, batch     0 | loss: 4.5572977Losses:  6.235315799713135 6.2039794921875 0.03133653849363327
CurrentTrain: epoch  9, batch     1 | loss: 6.2353158Losses:  6.633637428283691 6.604316711425781 0.029320906847715378
CurrentTrain: epoch  9, batch     2 | loss: 6.6336374Losses:  5.723928928375244 5.69094181060791 0.03298727422952652
CurrentTrain: epoch  9, batch     3 | loss: 5.7239289Losses:  6.024724006652832 5.993756294250488 0.03096769191324711
CurrentTrain: epoch  9, batch     4 | loss: 6.0247240Losses:  4.005080699920654 3.981034755706787 0.02404574304819107
CurrentTrain: epoch  9, batch     5 | loss: 4.0050807Losses:  4.478065490722656 4.4536027908325195 0.02446286752820015
CurrentTrain: epoch  9, batch     6 | loss: 4.4780655Losses:  7.055358409881592 7.026854515075684 0.02850373648107052
CurrentTrain: epoch  9, batch     7 | loss: 7.0553584Losses:  4.80088996887207 4.773150444030762 0.02773958444595337
CurrentTrain: epoch  9, batch     8 | loss: 4.8008900Losses:  5.691018104553223 5.6671600341796875 0.02385818213224411
CurrentTrain: epoch  9, batch     9 | loss: 5.6910181Losses:  3.4879422187805176 3.4668500423431396 0.021092159673571587
CurrentTrain: epoch  9, batch    10 | loss: 3.4879422Losses:  6.285571098327637 6.2615065574646 0.024064479395747185
CurrentTrain: epoch  9, batch    11 | loss: 6.2855711Losses:  5.182713985443115 5.152898788452148 0.029815388843417168
CurrentTrain: epoch  9, batch    12 | loss: 5.1827140Losses:  4.476247787475586 4.447726726531982 0.028521114960312843
CurrentTrain: epoch  9, batch    13 | loss: 4.4762478Losses:  4.415579795837402 4.389896392822266 0.025683455169200897
CurrentTrain: epoch  9, batch    14 | loss: 4.4155798Losses:  5.626323223114014 5.6024274826049805 0.023895954713225365
CurrentTrain: epoch  9, batch    15 | loss: 5.6263232Losses:  3.6849441528320312 3.66127872467041 0.023665525019168854
CurrentTrain: epoch  9, batch    16 | loss: 3.6849442Losses:  4.825470447540283 4.798471450805664 0.02699892409145832
CurrentTrain: epoch  9, batch    17 | loss: 4.8254704Losses:  8.080856323242188 8.032144546508789 0.04871160537004471
CurrentTrain: epoch  9, batch    18 | loss: 8.0808563Losses:  7.868095874786377 7.832767486572266 0.035328492522239685
CurrentTrain: epoch  9, batch    19 | loss: 7.8680959Losses:  6.955105304718018 6.924286365509033 0.03081870637834072
CurrentTrain: epoch  9, batch    20 | loss: 6.9551053Losses:  4.648711681365967 4.619197845458984 0.02951386757194996
CurrentTrain: epoch  9, batch    21 | loss: 4.6487117Losses:  3.40236759185791 3.3815932273864746 0.020774392411112785
CurrentTrain: epoch  9, batch    22 | loss: 3.4023676Losses:  5.278717041015625 5.251813888549805 0.02690313383936882
CurrentTrain: epoch  9, batch    23 | loss: 5.2787170Losses:  6.002892017364502 5.974740028381348 0.02815181016921997
CurrentTrain: epoch  9, batch    24 | loss: 6.0028920Losses:  4.088436603546143 4.065902233123779 0.02253449335694313
CurrentTrain: epoch  9, batch    25 | loss: 4.0884366Losses:  7.775758743286133 7.7404584884643555 0.03530007228255272
CurrentTrain: epoch  9, batch    26 | loss: 7.7757587Losses:  4.796046733856201 4.767200469970703 0.028846273198723793
CurrentTrain: epoch  9, batch    27 | loss: 4.7960467Losses:  6.220315456390381 6.188096046447754 0.032219503074884415
CurrentTrain: epoch  9, batch    28 | loss: 6.2203155Losses:  4.69966983795166 4.672051429748535 0.027618179097771645
CurrentTrain: epoch  9, batch    29 | loss: 4.6996698Losses:  5.698523044586182 5.668711185455322 0.02981209009885788
CurrentTrain: epoch  9, batch    30 | loss: 5.6985230Losses:  4.701918601989746 4.675947189331055 0.025971438735723495
CurrentTrain: epoch  9, batch    31 | loss: 4.7019186Losses:  7.093037128448486 7.057757377624512 0.03527970239520073
CurrentTrain: epoch  9, batch    32 | loss: 7.0930371Losses:  7.760732650756836 7.721179485321045 0.039553090929985046
CurrentTrain: epoch  9, batch    33 | loss: 7.7607327Losses:  5.3950581550598145 5.372132301330566 0.022925684228539467
CurrentTrain: epoch  9, batch    34 | loss: 5.3950582Losses:  4.801572799682617 4.774085521697998 0.027487074956297874
CurrentTrain: epoch  9, batch    35 | loss: 4.8015728Losses:  4.508354187011719 4.4862060546875 0.022148162126541138
CurrentTrain: epoch  9, batch    36 | loss: 4.5083542Losses:  1.4763165712356567 1.4293954372406006 0.046921100467443466
CurrentTrain: epoch  9, batch    37 | loss: 1.4763166
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 90.18%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 89.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 87.11%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 85.66%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 84.38%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.73%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.17%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.58%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 88.71%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.87%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 87.31%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 90.18%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 89.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 87.11%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 85.66%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 84.38%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.73%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.17%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.58%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 88.71%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.87%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 87.31%   
cur_acc:  ['0.8731']
his_acc:  ['0.8731']
Clustering into  4  clusters
Clusters:  [3 1 1 3 0 2 3 0 1 0 0]
Losses:  8.223956108093262 8.064876556396484 0.15907946228981018
CurrentTrain: epoch  0, batch     0 | loss: 8.2239561Losses:  3.3445615768432617 3.209808826446533 0.13475273549556732
CurrentTrain: epoch  0, batch     1 | loss: 3.3445616Losses:  7.804227828979492 7.642569065093994 0.16165883839130402
CurrentTrain: epoch  1, batch     0 | loss: 7.8042278Losses:  1.8358874320983887 1.6983582973480225 0.1375291347503662
CurrentTrain: epoch  1, batch     1 | loss: 1.8358874Losses:  7.506972312927246 7.355471134185791 0.15150094032287598
CurrentTrain: epoch  2, batch     0 | loss: 7.5069723Losses:  2.765988349914551 2.6449601650238037 0.1210281103849411
CurrentTrain: epoch  2, batch     1 | loss: 2.7659883Losses:  7.371542453765869 7.247615814208984 0.1239268034696579
CurrentTrain: epoch  3, batch     0 | loss: 7.3715425Losses:  5.48414945602417 5.48414945602417 -0.0
CurrentTrain: epoch  3, batch     1 | loss: 5.4841495Losses:  8.17949390411377 8.051995277404785 0.12749868631362915
CurrentTrain: epoch  4, batch     0 | loss: 8.1794939Losses:  3.4567947387695312 3.3320372104644775 0.12475745379924774
CurrentTrain: epoch  4, batch     1 | loss: 3.4567947Losses:  5.885144233703613 5.770069122314453 0.1150750145316124
CurrentTrain: epoch  5, batch     0 | loss: 5.8851442Losses:  1.7516721487045288 1.6271684169769287 0.12450374662876129
CurrentTrain: epoch  5, batch     1 | loss: 1.7516721Losses:  6.614487171173096 6.499587059020996 0.11490026116371155
CurrentTrain: epoch  6, batch     0 | loss: 6.6144872Losses:  1.9345674514770508 1.8127045631408691 0.12186285108327866
CurrentTrain: epoch  6, batch     1 | loss: 1.9345675Losses:  6.753129959106445 6.638212203979492 0.11491771787405014
CurrentTrain: epoch  7, batch     0 | loss: 6.7531300Losses:  2.8134422302246094 2.7547900676727295 0.05865227431058884
CurrentTrain: epoch  7, batch     1 | loss: 2.8134422Losses:  7.367215156555176 7.249807357788086 0.11740767955780029
CurrentTrain: epoch  8, batch     0 | loss: 7.3672152Losses:  2.282923460006714 2.1673946380615234 0.115528903901577
CurrentTrain: epoch  8, batch     1 | loss: 2.2829235Losses:  5.77656888961792 5.657729148864746 0.1188395768404007
CurrentTrain: epoch  9, batch     0 | loss: 5.7765689Losses:  1.2872774600982666 1.1767171621322632 0.1105603501200676
CurrentTrain: epoch  9, batch     1 | loss: 1.2872775
Losses:  0.23502127826213837 -0.0 0.23502127826213837
MemoryTrain:  epoch  0, batch     0 | loss: 0.2350213Losses:  0.22019441425800323 -0.0 0.22019441425800323
MemoryTrain:  epoch  1, batch     0 | loss: 0.2201944Losses:  0.20904091000556946 -0.0 0.20904091000556946
MemoryTrain:  epoch  2, batch     0 | loss: 0.2090409Losses:  0.2075863927602768 -0.0 0.2075863927602768
MemoryTrain:  epoch  3, batch     0 | loss: 0.2075864Losses:  0.20385587215423584 -0.0 0.20385587215423584
MemoryTrain:  epoch  4, batch     0 | loss: 0.2038559Losses:  0.20374538004398346 -0.0 0.20374538004398346
MemoryTrain:  epoch  5, batch     0 | loss: 0.2037454Losses:  0.20382735133171082 -0.0 0.20382735133171082
MemoryTrain:  epoch  6, batch     0 | loss: 0.2038274Losses:  0.20284081995487213 -0.0 0.20284081995487213
MemoryTrain:  epoch  7, batch     0 | loss: 0.2028408Losses:  0.20164909958839417 -0.0 0.20164909958839417
MemoryTrain:  epoch  8, batch     0 | loss: 0.2016491Losses:  0.1999993622303009 -0.0 0.1999993622303009
MemoryTrain:  epoch  9, batch     0 | loss: 0.1999994
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 32.81%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 33.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 41.67%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 49.11%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 52.34%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 54.86%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 55.00%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 53.98%   [EVAL] batch:   11 | acc: 18.75%,  total acc: 51.04%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 48.08%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 45.98%   [EVAL] batch:   14 | acc: 18.75%,  total acc: 44.17%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 67.19%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 79.86%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 83.85%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 82.69%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 79.46%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 77.73%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 77.57%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 76.74%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 76.32%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 76.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 77.98%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 78.98%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 79.89%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 80.73%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 81.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.21%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 82.64%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.26%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 83.84%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 83.87%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 84.18%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 83.33%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 81.80%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 80.36%   [EVAL] batch:   35 | acc: 31.25%,  total acc: 78.99%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 78.04%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 77.80%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 77.88%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 77.90%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 77.68%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 77.03%   [EVAL] batch:   43 | acc: 18.75%,  total acc: 75.71%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 74.58%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 73.23%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 72.21%   
cur_acc:  ['0.8731', '0.4417']
his_acc:  ['0.8731', '0.7221']
Clustering into  7  clusters
Clusters:  [1 3 3 1 0 5 1 4 6 0 0 4 1 2 3 1]
Losses:  9.287426948547363 8.63292407989502 0.6545028686523438
CurrentTrain: epoch  0, batch     0 | loss: 9.2874269Losses:  5.33206033706665 4.9474406242370605 0.38461965322494507
CurrentTrain: epoch  0, batch     1 | loss: 5.3320603Losses:  7.542502403259277 7.018984317779541 0.5235181450843811
CurrentTrain: epoch  1, batch     0 | loss: 7.5425024Losses:  3.8602488040924072 3.368314504623413 0.49193426966667175
CurrentTrain: epoch  1, batch     1 | loss: 3.8602488Losses:  6.962225914001465 6.514730453491211 0.4474952220916748
CurrentTrain: epoch  2, batch     0 | loss: 6.9622259Losses:  2.7262821197509766 2.3042190074920654 0.42206311225891113
CurrentTrain: epoch  2, batch     1 | loss: 2.7262821Losses:  6.588849067687988 6.205920696258545 0.38292837142944336
CurrentTrain: epoch  3, batch     0 | loss: 6.5888491Losses:  2.8461289405822754 2.468834400177002 0.377294659614563
CurrentTrain: epoch  3, batch     1 | loss: 2.8461289Losses:  6.529487609863281 6.182767391204834 0.3467201292514801
CurrentTrain: epoch  4, batch     0 | loss: 6.5294876Losses:  2.4858367443084717 2.177377462387085 0.3084593415260315
CurrentTrain: epoch  4, batch     1 | loss: 2.4858367Losses:  6.7227606773376465 6.419736862182617 0.30302366614341736
CurrentTrain: epoch  5, batch     0 | loss: 6.7227607Losses:  1.8833644390106201 1.5685389041900635 0.31482553482055664
CurrentTrain: epoch  5, batch     1 | loss: 1.8833644Losses:  7.10835075378418 6.812597274780273 0.2957534193992615
CurrentTrain: epoch  6, batch     0 | loss: 7.1083508Losses:  2.7325785160064697 2.4306399822235107 0.30193862318992615
CurrentTrain: epoch  6, batch     1 | loss: 2.7325785Losses:  7.280879974365234 6.98256778717041 0.29831230640411377
CurrentTrain: epoch  7, batch     0 | loss: 7.2808800Losses:  2.056485414505005 1.7796131372451782 0.2768722176551819
CurrentTrain: epoch  7, batch     1 | loss: 2.0564854Losses:  6.968679428100586 6.693940162658691 0.27473920583724976
CurrentTrain: epoch  8, batch     0 | loss: 6.9686794Losses:  2.5090911388397217 2.2404255867004395 0.2686656415462494
CurrentTrain: epoch  8, batch     1 | loss: 2.5090911Losses:  6.6610612869262695 6.390714645385742 0.27034661173820496
CurrentTrain: epoch  9, batch     0 | loss: 6.6610613Losses:  1.8067092895507812 1.5385050773620605 0.2682042419910431
CurrentTrain: epoch  9, batch     1 | loss: 1.8067093
Losses:  0.6756518483161926 -0.0 0.6756518483161926
MemoryTrain:  epoch  0, batch     0 | loss: 0.6756518Losses:  0.6376161575317383 -0.0 0.6376161575317383
MemoryTrain:  epoch  1, batch     0 | loss: 0.6376162Losses:  0.6103119254112244 -0.0 0.6103119254112244
MemoryTrain:  epoch  2, batch     0 | loss: 0.6103119Losses:  0.5772119760513306 -0.0 0.5772119760513306
MemoryTrain:  epoch  3, batch     0 | loss: 0.5772120Losses:  0.5557376742362976 -0.0 0.5557376742362976
MemoryTrain:  epoch  4, batch     0 | loss: 0.5557377Losses:  0.5452824234962463 -0.0 0.5452824234962463
MemoryTrain:  epoch  5, batch     0 | loss: 0.5452824Losses:  0.5387943983078003 -0.0 0.5387943983078003
MemoryTrain:  epoch  6, batch     0 | loss: 0.5387944Losses:  0.5344963073730469 -0.0 0.5344963073730469
MemoryTrain:  epoch  7, batch     0 | loss: 0.5344963Losses:  0.5346616506576538 -0.0 0.5346616506576538
MemoryTrain:  epoch  8, batch     0 | loss: 0.5346617Losses:  0.5377370119094849 -0.0 0.5377370119094849
MemoryTrain:  epoch  9, batch     0 | loss: 0.5377370
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 46.25%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 45.83%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 44.64%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 48.44%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 51.70%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 52.08%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 53.37%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 53.12%   
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 39.06%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 41.25%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 38.54%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 45.54%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 52.34%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 57.64%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 60.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 64.20%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 67.31%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 67.41%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 67.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 67.19%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 67.28%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 67.01%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 66.78%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 67.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 69.05%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 70.45%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 74.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 75.69%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 77.37%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 77.71%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 78.23%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 78.71%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 78.03%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 75.74%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 73.93%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 72.22%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 71.11%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 70.89%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 70.35%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 70.78%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 70.12%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 70.35%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 68.75%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 67.22%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 65.90%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 64.76%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 64.19%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 64.03%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 64.00%   [EVAL] batch:   50 | acc: 31.25%,  total acc: 63.36%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 62.98%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 62.62%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 62.27%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 62.17%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 62.28%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 62.18%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 62.18%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 62.29%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 61.89%   
cur_acc:  ['0.8731', '0.4417', '0.5312']
his_acc:  ['0.8731', '0.7221', '0.6189']
Clustering into  9  clusters
Clusters:  [4 1 1 8 0 7 4 2 5 0 3 2 4 3 1 4 4 6 4 3 0]
Losses:  7.827013969421387 7.422014236450195 0.40499991178512573
CurrentTrain: epoch  0, batch     0 | loss: 7.8270140Losses:  3.3512282371520996 3.0203545093536377 0.33087360858917236
CurrentTrain: epoch  0, batch     1 | loss: 3.3512282Losses:  7.889564037322998 7.514028549194336 0.37553560733795166
CurrentTrain: epoch  1, batch     0 | loss: 7.8895640Losses:  2.5081615447998047 2.1334447860717773 0.3747168183326721
CurrentTrain: epoch  1, batch     1 | loss: 2.5081615Losses:  7.443943977355957 7.07025671005249 0.3736874759197235
CurrentTrain: epoch  2, batch     0 | loss: 7.4439440Losses:  2.1571812629699707 1.8365529775619507 0.3206283450126648
CurrentTrain: epoch  2, batch     1 | loss: 2.1571813Losses:  8.108251571655273 7.792725086212158 0.3155263662338257
CurrentTrain: epoch  3, batch     0 | loss: 8.1082516Losses:  3.5807437896728516 3.2419593334198 0.3387845456600189
CurrentTrain: epoch  3, batch     1 | loss: 3.5807438Losses:  6.658912181854248 6.340055465698242 0.3188565969467163
CurrentTrain: epoch  4, batch     0 | loss: 6.6589122Losses:  2.820425510406494 2.5185861587524414 0.3018394708633423
CurrentTrain: epoch  4, batch     1 | loss: 2.8204255Losses:  6.397302627563477 6.095117568969727 0.3021852374076843
CurrentTrain: epoch  5, batch     0 | loss: 6.3973026Losses:  2.238187789916992 1.9562526941299438 0.2819352149963379
CurrentTrain: epoch  5, batch     1 | loss: 2.2381878Losses:  6.198195457458496 5.934254169464111 0.2639414668083191
CurrentTrain: epoch  6, batch     0 | loss: 6.1981955Losses:  2.159166097640991 1.8837791681289673 0.27538686990737915
CurrentTrain: epoch  6, batch     1 | loss: 2.1591661Losses:  6.96053409576416 6.685817718505859 0.27471643686294556
CurrentTrain: epoch  7, batch     0 | loss: 6.9605341Losses:  2.806286334991455 2.5395801067352295 0.2667061388492584
CurrentTrain: epoch  7, batch     1 | loss: 2.8062863Losses:  5.905391216278076 5.638618469238281 0.2667725384235382
CurrentTrain: epoch  8, batch     0 | loss: 5.9053912Losses:  1.3317205905914307 1.0878649950027466 0.24385562539100647
CurrentTrain: epoch  8, batch     1 | loss: 1.3317206Losses:  7.180839538574219 6.933439254760742 0.24740031361579895
CurrentTrain: epoch  9, batch     0 | loss: 7.1808395Losses:  3.1423239707946777 2.8961448669433594 0.24617914855480194
CurrentTrain: epoch  9, batch     1 | loss: 3.1423240
Losses:  0.7350143194198608 -0.0 0.7350143194198608
MemoryTrain:  epoch  0, batch     0 | loss: 0.7350143Losses:  0.17428550124168396 -0.0 0.17428550124168396
MemoryTrain:  epoch  0, batch     1 | loss: 0.1742855Losses:  0.6759957671165466 -0.0 0.6759957671165466
MemoryTrain:  epoch  1, batch     0 | loss: 0.6759958Losses:  0.11074569821357727 -0.0 0.11074569821357727
MemoryTrain:  epoch  1, batch     1 | loss: 0.1107457Losses:  0.6902192234992981 -0.0 0.6902192234992981
MemoryTrain:  epoch  2, batch     0 | loss: 0.6902192Losses:  0.20228396356105804 -0.0 0.20228396356105804
MemoryTrain:  epoch  2, batch     1 | loss: 0.2022840Losses:  0.6542828679084778 -0.0 0.6542828679084778
MemoryTrain:  epoch  3, batch     0 | loss: 0.6542829Losses:  0.433220773935318 -0.0 0.433220773935318
MemoryTrain:  epoch  3, batch     1 | loss: 0.4332208Losses:  0.7020227313041687 -0.0 0.7020227313041687
MemoryTrain:  epoch  4, batch     0 | loss: 0.7020227Losses:  0.5125253796577454 -0.0 0.5125253796577454
MemoryTrain:  epoch  4, batch     1 | loss: 0.5125254Losses:  0.6635199785232544 -0.0 0.6635199785232544
MemoryTrain:  epoch  5, batch     0 | loss: 0.6635200Losses:  0.26087695360183716 -0.0 0.26087695360183716
MemoryTrain:  epoch  5, batch     1 | loss: 0.2608770Losses:  0.4879336953163147 -0.0 0.4879336953163147
MemoryTrain:  epoch  6, batch     0 | loss: 0.4879337Losses:  0.5976409912109375 -0.0 0.5976409912109375
MemoryTrain:  epoch  6, batch     1 | loss: 0.5976410Losses:  0.6848690509796143 -0.0 0.6848690509796143
MemoryTrain:  epoch  7, batch     0 | loss: 0.6848691Losses:  0.47858771681785583 -0.0 0.47858771681785583
MemoryTrain:  epoch  7, batch     1 | loss: 0.4785877Losses:  0.6261037588119507 -0.0 0.6261037588119507
MemoryTrain:  epoch  8, batch     0 | loss: 0.6261038Losses:  0.21512049436569214 -0.0 0.21512049436569214
MemoryTrain:  epoch  8, batch     1 | loss: 0.2151205Losses:  0.5448794364929199 -0.0 0.5448794364929199
MemoryTrain:  epoch  9, batch     0 | loss: 0.5448794Losses:  0.5082792043685913 -0.0 0.5082792043685913
MemoryTrain:  epoch  9, batch     1 | loss: 0.5082792
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 45.83%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 43.75%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 46.25%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 46.88%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 50.89%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 53.91%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 52.08%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 54.37%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 56.82%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 59.90%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 67.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 70.96%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 68.40%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 67.19%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 66.25%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 61.46%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 66.07%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 73.61%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 77.27%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 78.65%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 78.85%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 77.08%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 75.78%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 75.37%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 74.65%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 73.36%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 73.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 74.70%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 75.85%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 76.63%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 77.60%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 78.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.33%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 79.86%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.58%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 81.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.06%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 82.42%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 81.06%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 79.23%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 77.14%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 75.17%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 73.48%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 72.86%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 72.60%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 72.97%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 72.41%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 73.07%   [EVAL] batch:   42 | acc: 6.25%,  total acc: 71.51%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 69.89%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 68.33%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 66.85%   [EVAL] batch:   46 | acc: 6.25%,  total acc: 65.56%   [EVAL] batch:   47 | acc: 25.00%,  total acc: 64.71%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 63.78%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 62.62%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 61.52%   [EVAL] batch:   51 | acc: 0.00%,  total acc: 60.34%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 59.32%   [EVAL] batch:   53 | acc: 18.75%,  total acc: 58.56%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 59.09%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 59.15%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 59.43%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 59.48%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 59.32%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 59.27%   [EVAL] batch:   60 | acc: 18.75%,  total acc: 58.61%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 58.37%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 58.33%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 58.20%   [EVAL] batch:   64 | acc: 56.25%,  total acc: 58.17%   [EVAL] batch:   65 | acc: 43.75%,  total acc: 57.95%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 57.93%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 58.36%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 58.15%   [EVAL] batch:   69 | acc: 62.50%,  total acc: 58.21%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 58.54%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 58.85%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 59.42%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 59.88%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 60.42%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 60.94%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 61.36%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 61.54%   
cur_acc:  ['0.8731', '0.4417', '0.5312', '0.6840']
his_acc:  ['0.8731', '0.7221', '0.6189', '0.6154']
Clustering into  12  clusters
Clusters:  [ 4  3  0 10 11  6  4  2  7  9  1  2  4  1  3  4  4  5  4  1  8  0  0  0
  6  1]
Losses:  8.464009284973145 8.251203536987305 0.21280597150325775
CurrentTrain: epoch  0, batch     0 | loss: 8.4640093Losses:  2.812283992767334 2.5449657440185547 0.2673182189464569
CurrentTrain: epoch  0, batch     1 | loss: 2.8122840Losses:  9.268186569213867 9.041015625 0.22717103362083435
CurrentTrain: epoch  1, batch     0 | loss: 9.2681866Losses:  7.553179740905762 7.392529010772705 0.1606508195400238
CurrentTrain: epoch  1, batch     1 | loss: 7.5531797Losses:  8.29887580871582 8.098042488098145 0.20083317160606384
CurrentTrain: epoch  2, batch     0 | loss: 8.2988758Losses:  2.7078475952148438 2.5174520015716553 0.1903955042362213
CurrentTrain: epoch  2, batch     1 | loss: 2.7078476Losses:  6.7740478515625 6.598023891448975 0.1760239452123642
CurrentTrain: epoch  3, batch     0 | loss: 6.7740479Losses:  2.57452654838562 2.404071807861328 0.17045465111732483
CurrentTrain: epoch  3, batch     1 | loss: 2.5745265Losses:  6.773946285247803 6.6132097244262695 0.16073647141456604
CurrentTrain: epoch  4, batch     0 | loss: 6.7739463Losses:  1.5365965366363525 1.3850975036621094 0.1514989733695984
CurrentTrain: epoch  4, batch     1 | loss: 1.5365965Losses:  6.695713520050049 6.547431468963623 0.1482819765806198
CurrentTrain: epoch  5, batch     0 | loss: 6.6957135Losses:  1.5018582344055176 1.3578685522079468 0.14398972690105438
CurrentTrain: epoch  5, batch     1 | loss: 1.5018582Losses:  6.114419460296631 5.9794111251831055 0.13500836491584778
CurrentTrain: epoch  6, batch     0 | loss: 6.1144195Losses:  1.9771957397460938 1.8357208967208862 0.1414748877286911
CurrentTrain: epoch  6, batch     1 | loss: 1.9771957Losses:  8.015236854553223 7.876908302307129 0.13832879066467285
CurrentTrain: epoch  7, batch     0 | loss: 8.0152369Losses:  2.088550329208374 1.9583715200424194 0.13017889857292175
CurrentTrain: epoch  7, batch     1 | loss: 2.0885503Losses:  6.404114246368408 6.2749786376953125 0.12913542985916138
CurrentTrain: epoch  8, batch     0 | loss: 6.4041142Losses:  2.523332118988037 2.3911192417144775 0.13221292197704315
CurrentTrain: epoch  8, batch     1 | loss: 2.5233321Losses:  6.4579668045043945 6.336614608764648 0.12135224044322968
CurrentTrain: epoch  9, batch     0 | loss: 6.4579668Losses:  2.4727182388305664 2.3449275493621826 0.1277906745672226
CurrentTrain: epoch  9, batch     1 | loss: 2.4727182
Losses:  0.7779361009597778 -0.0 0.7779361009597778
MemoryTrain:  epoch  0, batch     0 | loss: 0.7779361Losses:  0.7287019491195679 -0.0 0.7287019491195679
MemoryTrain:  epoch  0, batch     1 | loss: 0.7287019Losses:  0.9501946568489075 -0.0 0.9501946568489075
MemoryTrain:  epoch  1, batch     0 | loss: 0.9501947Losses:  0.6193822622299194 -0.0 0.6193822622299194
MemoryTrain:  epoch  1, batch     1 | loss: 0.6193823Losses:  0.6893870830535889 -0.0 0.6893870830535889
MemoryTrain:  epoch  2, batch     0 | loss: 0.6893871Losses:  0.7622642517089844 -0.0 0.7622642517089844
MemoryTrain:  epoch  2, batch     1 | loss: 0.7622643Losses:  0.7797917723655701 -0.0 0.7797917723655701
MemoryTrain:  epoch  3, batch     0 | loss: 0.7797918Losses:  0.6019687652587891 -0.0 0.6019687652587891
MemoryTrain:  epoch  3, batch     1 | loss: 0.6019688Losses:  0.7956104278564453 -0.0 0.7956104278564453
MemoryTrain:  epoch  4, batch     0 | loss: 0.7956104Losses:  0.564245879650116 -0.0 0.564245879650116
MemoryTrain:  epoch  4, batch     1 | loss: 0.5642459Losses:  0.6492699980735779 -0.0 0.6492699980735779
MemoryTrain:  epoch  5, batch     0 | loss: 0.6492700Losses:  0.747201681137085 -0.0 0.747201681137085
MemoryTrain:  epoch  5, batch     1 | loss: 0.7472017Losses:  0.6883148550987244 -0.0 0.6883148550987244
MemoryTrain:  epoch  6, batch     0 | loss: 0.6883149Losses:  0.43624910712242126 -0.0 0.43624910712242126
MemoryTrain:  epoch  6, batch     1 | loss: 0.4362491Losses:  0.7297366261482239 -0.0 0.7297366261482239
MemoryTrain:  epoch  7, batch     0 | loss: 0.7297366Losses:  0.6510944366455078 -0.0 0.6510944366455078
MemoryTrain:  epoch  7, batch     1 | loss: 0.6510944Losses:  0.7886473536491394 -0.0 0.7886473536491394
MemoryTrain:  epoch  8, batch     0 | loss: 0.7886474Losses:  0.6180815100669861 -0.0 0.6180815100669861
MemoryTrain:  epoch  8, batch     1 | loss: 0.6180815Losses:  0.7772152423858643 -0.0 0.7772152423858643
MemoryTrain:  epoch  9, batch     0 | loss: 0.7772152Losses:  0.5370185375213623 -0.0 0.5370185375213623
MemoryTrain:  epoch  9, batch     1 | loss: 0.5370185
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 51.56%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 41.25%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 34.38%   [EVAL] batch:    6 | acc: 12.50%,  total acc: 31.25%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 36.72%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 36.81%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 39.38%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 42.61%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 43.75%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 45.09%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 45.83%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 46.09%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 47.06%   [EVAL] batch:   17 | acc: 37.50%,  total acc: 46.53%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 46.05%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 46.25%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 46.13%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 45.74%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 60.00%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 59.38%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 53.57%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 46.88%   [EVAL] batch:    8 | acc: 25.00%,  total acc: 44.44%   [EVAL] batch:    9 | acc: 0.00%,  total acc: 40.00%   [EVAL] batch:   10 | acc: 25.00%,  total acc: 38.64%   [EVAL] batch:   11 | acc: 18.75%,  total acc: 36.98%   [EVAL] batch:   12 | acc: 6.25%,  total acc: 34.62%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 34.38%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 37.08%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 38.28%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 40.44%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 41.67%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 42.76%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 44.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 47.32%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 49.72%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 51.90%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 53.91%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 55.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 57.45%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 58.33%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 59.60%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 60.78%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 61.67%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 62.70%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 63.48%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 62.69%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 61.21%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 59.82%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 58.33%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 56.93%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 56.74%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 57.05%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 57.97%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 57.77%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 58.48%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 57.41%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 56.11%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 54.86%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 53.67%   [EVAL] batch:   46 | acc: 6.25%,  total acc: 52.66%   [EVAL] batch:   47 | acc: 31.25%,  total acc: 52.21%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 51.53%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 50.75%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 49.88%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 49.04%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 48.23%   [EVAL] batch:   53 | acc: 31.25%,  total acc: 47.92%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 48.75%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 49.22%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 49.78%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 50.22%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 50.42%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 50.83%   [EVAL] batch:   60 | acc: 25.00%,  total acc: 50.41%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 50.50%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 50.79%   [EVAL] batch:   63 | acc: 62.50%,  total acc: 50.98%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 51.15%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 51.23%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 51.31%   [EVAL] batch:   67 | acc: 81.25%,  total acc: 51.75%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 51.63%   [EVAL] batch:   69 | acc: 37.50%,  total acc: 51.43%   [EVAL] batch:   70 | acc: 43.75%,  total acc: 51.32%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 51.56%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 52.23%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 52.79%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 53.42%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 54.03%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 54.55%   [EVAL] batch:   77 | acc: 93.75%,  total acc: 55.05%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 55.38%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 55.47%   [EVAL] batch:   80 | acc: 43.75%,  total acc: 55.32%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 54.65%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 53.99%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 53.35%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 53.09%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 53.27%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 53.09%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 53.34%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 53.37%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 53.40%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 53.43%   [EVAL] batch:   91 | acc: 43.75%,  total acc: 53.33%   [EVAL] batch:   92 | acc: 68.75%,  total acc: 53.49%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 53.46%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 53.49%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 53.39%   [EVAL] batch:   96 | acc: 37.50%,  total acc: 53.22%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 53.19%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 53.22%   [EVAL] batch:   99 | acc: 12.50%,  total acc: 52.81%   
cur_acc:  ['0.8731', '0.4417', '0.5312', '0.6840', '0.4574']
his_acc:  ['0.8731', '0.7221', '0.6189', '0.6154', '0.5281']
Clustering into  14  clusters
Clusters:  [ 1  8  0  1 13  2  1  3  7 11  4  3  1  4  8  1  1 12  1  4  9  0  0  5
  2  4  1 10  0  6  0]
Losses:  7.898602485656738 7.5325422286987305 0.3660604953765869
CurrentTrain: epoch  0, batch     0 | loss: 7.8986025Losses:  2.370276927947998 2.049877643585205 0.32039928436279297
CurrentTrain: epoch  0, batch     1 | loss: 2.3702769Losses:  10.220666885375977 9.859445571899414 0.3612217903137207
CurrentTrain: epoch  1, batch     0 | loss: 10.2206669Losses:  4.613636016845703 4.384609699249268 0.2290264070034027
CurrentTrain: epoch  1, batch     1 | loss: 4.6136360Losses:  6.373810768127441 6.021977424621582 0.3518335223197937
CurrentTrain: epoch  2, batch     0 | loss: 6.3738108Losses:  1.7150849103927612 1.4527968168258667 0.26228809356689453
CurrentTrain: epoch  2, batch     1 | loss: 1.7150849Losses:  6.276881217956543 5.999568939208984 0.2773122787475586
CurrentTrain: epoch  3, batch     0 | loss: 6.2768812Losses:  1.6531224250793457 1.296854019165039 0.35626834630966187
CurrentTrain: epoch  3, batch     1 | loss: 1.6531224Losses:  6.508289337158203 6.2052130699157715 0.3030761480331421
CurrentTrain: epoch  4, batch     0 | loss: 6.5082893Losses:  2.1971445083618164 1.947939157485962 0.2492053508758545
CurrentTrain: epoch  4, batch     1 | loss: 2.1971445Losses:  7.963411808013916 7.671864032745361 0.2915477454662323
CurrentTrain: epoch  5, batch     0 | loss: 7.9634118Losses:  3.098458766937256 2.849583148956299 0.24887564778327942
CurrentTrain: epoch  5, batch     1 | loss: 3.0984588Losses:  6.834105491638184 6.545980453491211 0.28812479972839355
CurrentTrain: epoch  6, batch     0 | loss: 6.8341055Losses:  1.1105061769485474 0.8717049360275269 0.23880122601985931
CurrentTrain: epoch  6, batch     1 | loss: 1.1105062Losses:  8.238838195800781 7.993618965148926 0.24521896243095398
CurrentTrain: epoch  7, batch     0 | loss: 8.2388382Losses:  2.883484363555908 2.427398681640625 0.4560856521129608
CurrentTrain: epoch  7, batch     1 | loss: 2.8834844Losses:  7.5268402099609375 7.273189544677734 0.2536505460739136
CurrentTrain: epoch  8, batch     0 | loss: 7.5268402Losses:  3.42447566986084 3.152693033218384 0.2717825472354889
CurrentTrain: epoch  8, batch     1 | loss: 3.4244757Losses:  8.95964527130127 8.722387313842773 0.2372584044933319
CurrentTrain: epoch  9, batch     0 | loss: 8.9596453Losses:  3.8370230197906494 3.6831467151641846 0.15387624502182007
CurrentTrain: epoch  9, batch     1 | loss: 3.8370230
Losses:  0.7027374505996704 -0.0 0.7027374505996704
MemoryTrain:  epoch  0, batch     0 | loss: 0.7027375Losses:  0.834362268447876 -0.0 0.834362268447876
MemoryTrain:  epoch  0, batch     1 | loss: 0.8343623Losses:  0.8138951063156128 -0.0 0.8138951063156128
MemoryTrain:  epoch  1, batch     0 | loss: 0.8138951Losses:  0.7655506134033203 -0.0 0.7655506134033203
MemoryTrain:  epoch  1, batch     1 | loss: 0.7655506Losses:  0.7044158577919006 -0.0 0.7044158577919006
MemoryTrain:  epoch  2, batch     0 | loss: 0.7044159Losses:  0.8028609156608582 -0.0 0.8028609156608582
MemoryTrain:  epoch  2, batch     1 | loss: 0.8028609Losses:  0.859618067741394 -0.0 0.859618067741394
MemoryTrain:  epoch  3, batch     0 | loss: 0.8596181Losses:  0.7733259201049805 -0.0 0.7733259201049805
MemoryTrain:  epoch  3, batch     1 | loss: 0.7733259Losses:  0.9007938504219055 -0.0 0.9007938504219055
MemoryTrain:  epoch  4, batch     0 | loss: 0.9007939Losses:  0.6593074798583984 -0.0 0.6593074798583984
MemoryTrain:  epoch  4, batch     1 | loss: 0.6593075Losses:  0.7952083945274353 -0.0 0.7952083945274353
MemoryTrain:  epoch  5, batch     0 | loss: 0.7952084Losses:  0.6591911911964417 -0.0 0.6591911911964417
MemoryTrain:  epoch  5, batch     1 | loss: 0.6591912Losses:  0.8328168988227844 -0.0 0.8328168988227844
MemoryTrain:  epoch  6, batch     0 | loss: 0.8328169Losses:  0.7180016040802002 -0.0 0.7180016040802002
MemoryTrain:  epoch  6, batch     1 | loss: 0.7180016Losses:  0.847089946269989 -0.0 0.847089946269989
MemoryTrain:  epoch  7, batch     0 | loss: 0.8470899Losses:  0.8252704739570618 -0.0 0.8252704739570618
MemoryTrain:  epoch  7, batch     1 | loss: 0.8252705Losses:  0.7157676219940186 -0.0 0.7157676219940186
MemoryTrain:  epoch  8, batch     0 | loss: 0.7157676Losses:  0.8024833798408508 -0.0 0.8024833798408508
MemoryTrain:  epoch  8, batch     1 | loss: 0.8024834Losses:  0.7153199911117554 -0.0 0.7153199911117554
MemoryTrain:  epoch  9, batch     0 | loss: 0.7153200Losses:  0.7313640117645264 -0.0 0.7313640117645264
MemoryTrain:  epoch  9, batch     1 | loss: 0.7313640
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 53.12%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 50.00%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 47.92%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 44.64%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 39.84%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 32.81%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 36.25%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 36.46%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 41.96%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 44.53%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 48.61%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 52.27%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 54.17%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 53.85%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 52.68%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 54.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 54.30%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 55.51%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 55.90%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 57.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 59.52%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 61.36%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 63.04%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 66.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 67.31%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 67.82%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 69.40%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 70.00%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 70.56%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 70.90%   [EVAL] batch:   32 | acc: 31.25%,  total acc: 69.70%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 67.83%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 66.25%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 64.93%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 63.34%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 62.83%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 62.81%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 63.10%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 61.63%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 60.23%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 58.89%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 57.61%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 56.78%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 56.64%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 56.12%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 55.25%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 54.29%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 53.61%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 52.71%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 52.55%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 53.30%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 53.57%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 54.06%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 54.31%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 54.34%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 54.48%   [EVAL] batch:   60 | acc: 25.00%,  total acc: 54.00%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 53.93%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 53.97%   [EVAL] batch:   63 | acc: 62.50%,  total acc: 54.10%   [EVAL] batch:   64 | acc: 56.25%,  total acc: 54.13%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 54.17%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 54.29%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 54.78%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 54.62%   [EVAL] batch:   69 | acc: 31.25%,  total acc: 54.29%   [EVAL] batch:   70 | acc: 25.00%,  total acc: 53.87%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 53.91%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 54.54%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 55.15%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 55.75%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 56.33%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 56.90%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 57.21%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 57.12%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 56.95%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 56.48%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 55.79%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 55.12%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 54.54%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 54.49%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 54.72%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 54.74%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 55.04%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 55.34%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 55.49%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 55.63%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 55.71%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 55.91%   [EVAL] batch:   93 | acc: 56.25%,  total acc: 55.92%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 56.05%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 55.99%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 55.99%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 55.99%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 56.12%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 56.37%   [EVAL] batch:  101 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:  102 | acc: 37.50%,  total acc: 56.07%   [EVAL] batch:  103 | acc: 37.50%,  total acc: 55.89%   [EVAL] batch:  104 | acc: 50.00%,  total acc: 55.83%   [EVAL] batch:  105 | acc: 25.00%,  total acc: 55.54%   [EVAL] batch:  106 | acc: 6.25%,  total acc: 55.08%   
cur_acc:  ['0.8731', '0.4417', '0.5312', '0.6840', '0.4574', '0.3984']
his_acc:  ['0.8731', '0.7221', '0.6189', '0.6154', '0.5281', '0.5508']
Clustering into  17  clusters
Clusters:  [ 1  3  0  1 16  5  1  2 15 13  4 12  1  4  3 11  1  9 11  4  8  0  0  7
  5  4  1 14  0  6  0  2 10  5  4 11]
Losses:  7.860810279846191 7.486400127410889 0.37441039085388184
CurrentTrain: epoch  0, batch     0 | loss: 7.8608103Losses:  2.922128915786743 2.6027231216430664 0.319405734539032
CurrentTrain: epoch  0, batch     1 | loss: 2.9221289Losses:  5.942878246307373 5.586703777313232 0.3561742603778839
CurrentTrain: epoch  1, batch     0 | loss: 5.9428782Losses:  1.364472508430481 1.0204896926879883 0.3439828157424927
CurrentTrain: epoch  1, batch     1 | loss: 1.3644725Losses:  5.641217231750488 5.315556526184082 0.32566091418266296
CurrentTrain: epoch  2, batch     0 | loss: 5.6412172Losses:  2.409456253051758 2.0663609504699707 0.34309524297714233
CurrentTrain: epoch  2, batch     1 | loss: 2.4094563Losses:  5.506457328796387 5.188855171203613 0.3176020085811615
CurrentTrain: epoch  3, batch     0 | loss: 5.5064573Losses:  1.9180830717086792 1.5447968244552612 0.3732862174510956
CurrentTrain: epoch  3, batch     1 | loss: 1.9180831Losses:  5.800799369812012 5.47797155380249 0.32282787561416626
CurrentTrain: epoch  4, batch     0 | loss: 5.8007994Losses:  1.8981839418411255 1.5807225704193115 0.3174613416194916
CurrentTrain: epoch  4, batch     1 | loss: 1.8981839Losses:  7.732254505157471 7.412642002105713 0.3196125626564026
CurrentTrain: epoch  5, batch     0 | loss: 7.7322545Losses:  3.9191291332244873 3.6757185459136963 0.2434106171131134
CurrentTrain: epoch  5, batch     1 | loss: 3.9191291Losses:  5.745362281799316 5.427231788635254 0.31813040375709534
CurrentTrain: epoch  6, batch     0 | loss: 5.7453623Losses:  1.849738597869873 1.5271130800247192 0.3226255476474762
CurrentTrain: epoch  6, batch     1 | loss: 1.8497386Losses:  5.368633270263672 5.060176849365234 0.30845651030540466
CurrentTrain: epoch  7, batch     0 | loss: 5.3686333Losses:  2.039623975753784 1.7062898874282837 0.3333340585231781
CurrentTrain: epoch  7, batch     1 | loss: 2.0396240Losses:  7.5052490234375 7.176400661468506 0.3288484513759613
CurrentTrain: epoch  8, batch     0 | loss: 7.5052490Losses:  1.9393219947814941 1.6820470094680786 0.25727495551109314
CurrentTrain: epoch  8, batch     1 | loss: 1.9393220Losses:  7.4047417640686035 7.0931243896484375 0.311617374420166
CurrentTrain: epoch  9, batch     0 | loss: 7.4047418Losses:  3.9329755306243896 3.6801211833953857 0.25285425782203674
CurrentTrain: epoch  9, batch     1 | loss: 3.9329755
Losses:  0.8779374957084656 -0.0 0.8779374957084656
MemoryTrain:  epoch  0, batch     0 | loss: 0.8779375Losses:  0.7579570412635803 -0.0 0.7579570412635803
MemoryTrain:  epoch  0, batch     1 | loss: 0.7579570Losses:  0.5253541469573975 -0.0 0.5253541469573975
MemoryTrain:  epoch  0, batch     2 | loss: 0.5253541Losses:  0.7743644714355469 -0.0 0.7743644714355469
MemoryTrain:  epoch  1, batch     0 | loss: 0.7743645Losses:  0.9060953259468079 -0.0 0.9060953259468079
MemoryTrain:  epoch  1, batch     1 | loss: 0.9060953Losses:  0.22822228074073792 -0.0 0.22822228074073792
MemoryTrain:  epoch  1, batch     2 | loss: 0.2282223Losses:  0.8687707781791687 -0.0 0.8687707781791687
MemoryTrain:  epoch  2, batch     0 | loss: 0.8687708Losses:  0.8885931968688965 -0.0 0.8885931968688965
MemoryTrain:  epoch  2, batch     1 | loss: 0.8885932Losses:  0.18871435523033142 -0.0 0.18871435523033142
MemoryTrain:  epoch  2, batch     2 | loss: 0.1887144Losses:  0.8571969866752625 -0.0 0.8571969866752625
MemoryTrain:  epoch  3, batch     0 | loss: 0.8571970Losses:  0.7199369072914124 -0.0 0.7199369072914124
MemoryTrain:  epoch  3, batch     1 | loss: 0.7199369Losses:  0.24982528388500214 -0.0 0.24982528388500214
MemoryTrain:  epoch  3, batch     2 | loss: 0.2498253Losses:  0.8087224960327148 -0.0 0.8087224960327148
MemoryTrain:  epoch  4, batch     0 | loss: 0.8087225Losses:  0.9158719778060913 -0.0 0.9158719778060913
MemoryTrain:  epoch  4, batch     1 | loss: 0.9158720Losses:  0.10251110792160034 -0.0 0.10251110792160034
MemoryTrain:  epoch  4, batch     2 | loss: 0.1025111Losses:  0.8470997214317322 -0.0 0.8470997214317322
MemoryTrain:  epoch  5, batch     0 | loss: 0.8470997Losses:  0.8282332420349121 -0.0 0.8282332420349121
MemoryTrain:  epoch  5, batch     1 | loss: 0.8282332Losses:  0.2951141595840454 -0.0 0.2951141595840454
MemoryTrain:  epoch  5, batch     2 | loss: 0.2951142Losses:  0.9787649512290955 -0.0 0.9787649512290955
MemoryTrain:  epoch  6, batch     0 | loss: 0.9787650Losses:  0.8151405453681946 -0.0 0.8151405453681946
MemoryTrain:  epoch  6, batch     1 | loss: 0.8151405Losses:  0.3056465983390808 -0.0 0.3056465983390808
MemoryTrain:  epoch  6, batch     2 | loss: 0.3056466Losses:  0.7958407998085022 -0.0 0.7958407998085022
MemoryTrain:  epoch  7, batch     0 | loss: 0.7958408Losses:  0.8420963287353516 -0.0 0.8420963287353516
MemoryTrain:  epoch  7, batch     1 | loss: 0.8420963Losses:  0.43943190574645996 -0.0 0.43943190574645996
MemoryTrain:  epoch  7, batch     2 | loss: 0.4394319Losses:  0.9727787375450134 -0.0 0.9727787375450134
MemoryTrain:  epoch  8, batch     0 | loss: 0.9727787Losses:  0.7176342606544495 -0.0 0.7176342606544495
MemoryTrain:  epoch  8, batch     1 | loss: 0.7176343Losses:  0.1622714102268219 -0.0 0.1622714102268219
MemoryTrain:  epoch  8, batch     2 | loss: 0.1622714Losses:  0.9077296257019043 -0.0 0.9077296257019043
MemoryTrain:  epoch  9, batch     0 | loss: 0.9077296Losses:  0.88987135887146 -0.0 0.88987135887146
MemoryTrain:  epoch  9, batch     1 | loss: 0.8898714Losses:  0.16623395681381226 -0.0 0.16623395681381226
MemoryTrain:  epoch  9, batch     2 | loss: 0.1662340
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 25.00%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 0.00%,  total acc: 65.97%   [EVAL] batch:    9 | acc: 0.00%,  total acc: 59.38%   [EVAL] batch:   10 | acc: 6.25%,  total acc: 54.55%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 53.12%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 50.00%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 34.38%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 35.00%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 34.38%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 41.07%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 40.62%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 43.75%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 45.00%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 46.02%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 46.88%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 46.63%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 45.54%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 47.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 48.05%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 49.63%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 50.35%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 50.99%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 52.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 55.06%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 57.10%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 58.97%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 60.68%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 62.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 63.70%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 64.35%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 65.40%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 66.16%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 66.67%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 66.94%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 67.58%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 66.86%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 65.62%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 65.00%   [EVAL] batch:   35 | acc: 31.25%,  total acc: 64.06%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 62.50%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 62.17%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 62.02%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 62.35%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 62.80%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 61.34%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 59.94%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 58.61%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 57.34%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 56.52%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 56.12%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 55.61%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 54.87%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 53.92%   [EVAL] batch:   51 | acc: 31.25%,  total acc: 53.49%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 52.71%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 52.55%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 53.18%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 53.68%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 54.06%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 54.42%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 54.56%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 54.79%   [EVAL] batch:   60 | acc: 18.75%,  total acc: 54.20%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 54.23%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 53.87%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 53.91%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 53.56%   [EVAL] batch:   65 | acc: 43.75%,  total acc: 53.41%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 53.26%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 53.77%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 53.62%   [EVAL] batch:   69 | acc: 18.75%,  total acc: 53.12%   [EVAL] batch:   70 | acc: 37.50%,  total acc: 52.90%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 52.95%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 53.34%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 53.97%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 54.50%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 54.85%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 55.36%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 55.77%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 56.09%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 56.17%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 56.10%   [EVAL] batch:   81 | acc: 12.50%,  total acc: 55.56%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 54.89%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 54.39%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 54.49%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 54.72%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 54.38%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 54.69%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 54.78%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 54.72%   [EVAL] batch:   90 | acc: 43.75%,  total acc: 54.60%   [EVAL] batch:   91 | acc: 25.00%,  total acc: 54.28%   [EVAL] batch:   92 | acc: 12.50%,  total acc: 53.83%   [EVAL] batch:   93 | acc: 12.50%,  total acc: 53.39%   [EVAL] batch:   94 | acc: 0.00%,  total acc: 52.83%   [EVAL] batch:   95 | acc: 31.25%,  total acc: 52.60%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 52.71%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 52.74%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 52.90%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 53.06%   [EVAL] batch:  100 | acc: 81.25%,  total acc: 53.34%   [EVAL] batch:  101 | acc: 25.00%,  total acc: 53.06%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 53.03%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 53.12%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 53.21%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 53.24%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 53.27%   [EVAL] batch:  107 | acc: 75.00%,  total acc: 53.47%   [EVAL] batch:  108 | acc: 81.25%,  total acc: 53.73%   [EVAL] batch:  109 | acc: 81.25%,  total acc: 53.98%   [EVAL] batch:  110 | acc: 81.25%,  total acc: 54.22%   [EVAL] batch:  111 | acc: 87.50%,  total acc: 54.52%   [EVAL] batch:  112 | acc: 81.25%,  total acc: 54.76%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 54.71%   [EVAL] batch:  114 | acc: 6.25%,  total acc: 54.29%   [EVAL] batch:  115 | acc: 0.00%,  total acc: 53.83%   [EVAL] batch:  116 | acc: 6.25%,  total acc: 53.42%   [EVAL] batch:  117 | acc: 25.00%,  total acc: 53.18%   [EVAL] batch:  118 | acc: 25.00%,  total acc: 52.94%   
cur_acc:  ['0.8731', '0.4417', '0.5312', '0.6840', '0.4574', '0.3984', '0.5000']
his_acc:  ['0.8731', '0.7221', '0.6189', '0.6154', '0.5281', '0.5508', '0.5294']
Clustering into  19  clusters
Clusters:  [ 0  4  5  0  2 11  0  3 15 18  1 13  0  1  4  7  0  9  7  1  2  5  5 17
 11  1  0 14  5  6 16  3 10 11  1  7  6  8  0  5 12]
Losses:  8.669041633605957 8.258420944213867 0.41062042117118835
CurrentTrain: epoch  0, batch     0 | loss: 8.6690416Losses:  4.091914176940918 3.832523822784424 0.2593904137611389
CurrentTrain: epoch  0, batch     1 | loss: 4.0919142Losses:  7.400445461273193 7.010451793670654 0.38999372720718384
CurrentTrain: epoch  1, batch     0 | loss: 7.4004455Losses:  2.889643669128418 2.5502066612243652 0.3394370675086975
CurrentTrain: epoch  1, batch     1 | loss: 2.8896437Losses:  5.99471378326416 5.616144180297852 0.37856945395469666
CurrentTrain: epoch  2, batch     0 | loss: 5.9947138Losses:  2.3400373458862305 1.9557230472564697 0.3843144178390503
CurrentTrain: epoch  2, batch     1 | loss: 2.3400373Losses:  6.00657320022583 5.6393656730651855 0.3672073781490326
CurrentTrain: epoch  3, batch     0 | loss: 6.0065732Losses:  1.807801365852356 1.4614132642745972 0.3463880717754364
CurrentTrain: epoch  3, batch     1 | loss: 1.8078014Losses:  6.458895206451416 6.109044075012207 0.3498510420322418
CurrentTrain: epoch  4, batch     0 | loss: 6.4588952Losses:  3.5523197650909424 3.201517343521118 0.35080233216285706
CurrentTrain: epoch  4, batch     1 | loss: 3.5523198Losses:  6.716681957244873 6.368503093719482 0.348178893327713
CurrentTrain: epoch  5, batch     0 | loss: 6.7166820Losses:  3.1179890632629395 2.7777481079101562 0.34024104475975037
CurrentTrain: epoch  5, batch     1 | loss: 3.1179891Losses:  6.570295810699463 6.243936061859131 0.32635974884033203
CurrentTrain: epoch  6, batch     0 | loss: 6.5702958Losses:  2.605135679244995 2.359805107116699 0.24533048272132874
CurrentTrain: epoch  6, batch     1 | loss: 2.6051357Losses:  6.103404521942139 5.781549453735352 0.3218550384044647
CurrentTrain: epoch  7, batch     0 | loss: 6.1034045Losses:  2.320072650909424 1.9943357706069946 0.3257368505001068
CurrentTrain: epoch  7, batch     1 | loss: 2.3200727Losses:  5.575259208679199 5.25840950012207 0.31684961915016174
CurrentTrain: epoch  8, batch     0 | loss: 5.5752592Losses:  1.7165206670761108 1.3943655490875244 0.32215508818626404
CurrentTrain: epoch  8, batch     1 | loss: 1.7165207Losses:  4.9849934577941895 4.672106742858887 0.3128865361213684
CurrentTrain: epoch  9, batch     0 | loss: 4.9849935Losses:  1.253122329711914 0.934494137763977 0.3186282515525818
CurrentTrain: epoch  9, batch     1 | loss: 1.2531223
Losses:  0.9541212320327759 -0.0 0.9541212320327759
MemoryTrain:  epoch  0, batch     0 | loss: 0.9541212Losses:  0.8944815397262573 -0.0 0.8944815397262573
MemoryTrain:  epoch  0, batch     1 | loss: 0.8944815Losses:  0.34001046419143677 -0.0 0.34001046419143677
MemoryTrain:  epoch  0, batch     2 | loss: 0.3400105Losses:  1.035976767539978 -0.0 1.035976767539978
MemoryTrain:  epoch  1, batch     0 | loss: 1.0359768Losses:  0.8847585320472717 -0.0 0.8847585320472717
MemoryTrain:  epoch  1, batch     1 | loss: 0.8847585Losses:  0.6356695890426636 -0.0 0.6356695890426636
MemoryTrain:  epoch  1, batch     2 | loss: 0.6356696Losses:  0.7102891206741333 -0.0 0.7102891206741333
MemoryTrain:  epoch  2, batch     0 | loss: 0.7102891Losses:  0.9124031066894531 -0.0 0.9124031066894531
MemoryTrain:  epoch  2, batch     1 | loss: 0.9124031Losses:  0.769805371761322 -0.0 0.769805371761322
MemoryTrain:  epoch  2, batch     2 | loss: 0.7698054Losses:  0.7980248332023621 -0.0 0.7980248332023621
MemoryTrain:  epoch  3, batch     0 | loss: 0.7980248Losses:  0.993147075176239 -0.0 0.993147075176239
MemoryTrain:  epoch  3, batch     1 | loss: 0.9931471Losses:  0.6158744692802429 -0.0 0.6158744692802429
MemoryTrain:  epoch  3, batch     2 | loss: 0.6158745Losses:  0.8366092443466187 -0.0 0.8366092443466187
MemoryTrain:  epoch  4, batch     0 | loss: 0.8366092Losses:  0.9162232279777527 -0.0 0.9162232279777527
MemoryTrain:  epoch  4, batch     1 | loss: 0.9162232Losses:  0.7128324508666992 -0.0 0.7128324508666992
MemoryTrain:  epoch  4, batch     2 | loss: 0.7128325Losses:  0.9513821601867676 -0.0 0.9513821601867676
MemoryTrain:  epoch  5, batch     0 | loss: 0.9513822Losses:  0.8092069625854492 -0.0 0.8092069625854492
MemoryTrain:  epoch  5, batch     1 | loss: 0.8092070Losses:  0.4589025676250458 -0.0 0.4589025676250458
MemoryTrain:  epoch  5, batch     2 | loss: 0.4589026Losses:  0.8453070521354675 -0.0 0.8453070521354675
MemoryTrain:  epoch  6, batch     0 | loss: 0.8453071Losses:  0.6731446385383606 -0.0 0.6731446385383606
MemoryTrain:  epoch  6, batch     1 | loss: 0.6731446Losses:  0.6537421345710754 -0.0 0.6537421345710754
MemoryTrain:  epoch  6, batch     2 | loss: 0.6537421Losses:  0.6258411407470703 -0.0 0.6258411407470703
MemoryTrain:  epoch  7, batch     0 | loss: 0.6258411Losses:  1.0902179479599 -0.0 1.0902179479599
MemoryTrain:  epoch  7, batch     1 | loss: 1.0902179Losses:  0.5878407955169678 -0.0 0.5878407955169678
MemoryTrain:  epoch  7, batch     2 | loss: 0.5878408Losses:  0.7444180846214294 -0.0 0.7444180846214294
MemoryTrain:  epoch  8, batch     0 | loss: 0.7444181Losses:  0.8606135845184326 -0.0 0.8606135845184326
MemoryTrain:  epoch  8, batch     1 | loss: 0.8606136Losses:  0.67633056640625 -0.0 0.67633056640625
MemoryTrain:  epoch  8, batch     2 | loss: 0.6763306Losses:  0.7511574625968933 -0.0 0.7511574625968933
MemoryTrain:  epoch  9, batch     0 | loss: 0.7511575Losses:  0.8868541717529297 -0.0 0.8868541717529297
MemoryTrain:  epoch  9, batch     1 | loss: 0.8868542Losses:  0.5871332883834839 -0.0 0.5871332883834839
MemoryTrain:  epoch  9, batch     2 | loss: 0.5871333
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 98.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 98.96%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 99.11%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 99.22%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 97.92%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 93.75%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 85.27%   
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 41.67%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 40.62%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 40.00%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 36.46%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 39.29%   [EVAL] batch:    7 | acc: 25.00%,  total acc: 37.50%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 39.58%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 38.75%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 39.20%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 41.15%   [EVAL] batch:   12 | acc: 25.00%,  total acc: 39.90%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 39.29%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 41.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 42.58%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 44.12%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 45.14%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 46.05%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 48.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 50.60%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 52.84%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 54.89%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 56.77%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 58.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 60.10%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 60.65%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 61.61%   [EVAL] batch:   28 | acc: 81.25%,  total acc: 62.28%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 62.71%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 62.90%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 63.09%   [EVAL] batch:   32 | acc: 25.00%,  total acc: 61.93%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 60.11%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 58.39%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 57.12%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 55.74%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 55.76%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 55.77%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 56.41%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 56.40%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 56.70%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 55.38%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 54.12%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 52.92%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 51.77%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 51.06%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 50.91%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 50.51%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 49.62%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 48.77%   [EVAL] batch:   51 | acc: 37.50%,  total acc: 48.56%   [EVAL] batch:   52 | acc: 25.00%,  total acc: 48.11%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 48.03%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 48.75%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 49.22%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 49.67%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 50.11%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 50.21%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 50.52%   [EVAL] batch:   60 | acc: 12.50%,  total acc: 49.90%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 49.40%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 48.71%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 48.34%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 47.88%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 47.44%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 47.11%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 47.70%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 47.64%   [EVAL] batch:   69 | acc: 31.25%,  total acc: 47.41%   [EVAL] batch:   70 | acc: 37.50%,  total acc: 47.27%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 47.40%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 47.95%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 48.65%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 49.33%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 49.92%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 50.49%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 50.88%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 50.71%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 50.62%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 50.62%   [EVAL] batch:   81 | acc: 12.50%,  total acc: 50.15%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 49.70%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 49.40%   [EVAL] batch:   84 | acc: 37.50%,  total acc: 49.26%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 49.27%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 48.92%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 49.08%   [EVAL] batch:   88 | acc: 43.75%,  total acc: 49.02%   [EVAL] batch:   89 | acc: 37.50%,  total acc: 48.89%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 48.97%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 48.98%   [EVAL] batch:   92 | acc: 43.75%,  total acc: 48.92%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 48.74%   [EVAL] batch:   94 | acc: 18.75%,  total acc: 48.42%   [EVAL] batch:   95 | acc: 31.25%,  total acc: 48.24%   [EVAL] batch:   96 | acc: 43.75%,  total acc: 48.20%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 48.21%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 48.36%   [EVAL] batch:   99 | acc: 56.25%,  total acc: 48.44%   [EVAL] batch:  100 | acc: 81.25%,  total acc: 48.76%   [EVAL] batch:  101 | acc: 18.75%,  total acc: 48.47%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 48.67%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 48.98%   [EVAL] batch:  104 | acc: 81.25%,  total acc: 49.29%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 49.53%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 49.71%   [EVAL] batch:  107 | acc: 62.50%,  total acc: 49.83%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 50.06%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 50.11%   [EVAL] batch:  110 | acc: 75.00%,  total acc: 50.34%   [EVAL] batch:  111 | acc: 81.25%,  total acc: 50.61%   [EVAL] batch:  112 | acc: 81.25%,  total acc: 50.88%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 51.04%   [EVAL] batch:  114 | acc: 12.50%,  total acc: 50.71%   [EVAL] batch:  115 | acc: 12.50%,  total acc: 50.38%   [EVAL] batch:  116 | acc: 31.25%,  total acc: 50.21%   [EVAL] batch:  117 | acc: 18.75%,  total acc: 49.95%   [EVAL] batch:  118 | acc: 31.25%,  total acc: 49.79%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 50.21%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 50.57%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 50.97%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 51.37%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 51.76%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 52.15%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 52.53%   [EVAL] batch:  126 | acc: 100.00%,  total acc: 52.90%   [EVAL] batch:  127 | acc: 75.00%,  total acc: 53.08%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 53.15%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 53.27%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 53.39%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 53.55%   [EVAL] batch:  132 | acc: 37.50%,  total acc: 53.43%   
cur_acc:  ['0.8731', '0.4417', '0.5312', '0.6840', '0.4574', '0.3984', '0.5000', '0.8527']
his_acc:  ['0.8731', '0.7221', '0.6189', '0.6154', '0.5281', '0.5508', '0.5294', '0.5343']
--------Round  2
seed:  300
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 1 1 0]
Losses:  8.228696823120117 7.578368663787842 0.6503285765647888
CurrentTrain: epoch  0, batch     0 | loss: 8.2286968Losses:  14.310324668884277 13.679214477539062 0.6311103105545044
CurrentTrain: epoch  0, batch     1 | loss: 14.3103247Losses:  10.178342819213867 9.550174713134766 0.6281676292419434
CurrentTrain: epoch  0, batch     2 | loss: 10.1783428Losses:  13.05922794342041 12.439594268798828 0.619633674621582
CurrentTrain: epoch  0, batch     3 | loss: 13.0592279Losses:  9.632525444030762 9.016773223876953 0.6157524585723877
CurrentTrain: epoch  0, batch     4 | loss: 9.6325254Losses:  11.60946273803711 10.983749389648438 0.6257132291793823
CurrentTrain: epoch  0, batch     5 | loss: 11.6094627Losses:  9.509137153625488 8.899896621704102 0.6092403531074524
CurrentTrain: epoch  0, batch     6 | loss: 9.5091372Losses:  7.986505508422852 7.38487434387207 0.6016314029693604
CurrentTrain: epoch  0, batch     7 | loss: 7.9865055Losses:  11.127476692199707 10.579601287841797 0.5478756427764893
CurrentTrain: epoch  0, batch     8 | loss: 11.1274767Losses:  9.349281311035156 8.774073600769043 0.5752080678939819
CurrentTrain: epoch  0, batch     9 | loss: 9.3492813Losses:  11.823172569274902 11.270353317260742 0.5528193712234497
CurrentTrain: epoch  0, batch    10 | loss: 11.8231726Losses:  12.199341773986816 11.64611530303955 0.5532267093658447
CurrentTrain: epoch  0, batch    11 | loss: 12.1993418Losses:  11.97850513458252 11.466453552246094 0.5120518207550049
CurrentTrain: epoch  0, batch    12 | loss: 11.9785051Losses:  14.919459342956543 14.398799896240234 0.5206595659255981
CurrentTrain: epoch  0, batch    13 | loss: 14.9194593Losses:  11.387133598327637 10.85323715209961 0.5338960289955139
CurrentTrain: epoch  0, batch    14 | loss: 11.3871336Losses:  8.941629409790039 8.468688011169434 0.47294121980667114
CurrentTrain: epoch  0, batch    15 | loss: 8.9416294Losses:  8.43052864074707 7.943759918212891 0.486768513917923
CurrentTrain: epoch  0, batch    16 | loss: 8.4305286Losses:  9.271010398864746 8.809152603149414 0.46185755729675293
CurrentTrain: epoch  0, batch    17 | loss: 9.2710104Losses:  8.310212135314941 7.804401397705078 0.5058103799819946
CurrentTrain: epoch  0, batch    18 | loss: 8.3102121Losses:  7.990596771240234 7.515317916870117 0.4752790629863739
CurrentTrain: epoch  0, batch    19 | loss: 7.9905968Losses:  11.01735782623291 10.588714599609375 0.4286428689956665
CurrentTrain: epoch  0, batch    20 | loss: 11.0173578Losses:  10.70142936706543 10.217533111572266 0.4838966131210327
CurrentTrain: epoch  0, batch    21 | loss: 10.7014294Losses:  8.822492599487305 8.383126258850098 0.4393659234046936
CurrentTrain: epoch  0, batch    22 | loss: 8.8224926Losses:  8.406900405883789 8.042110443115234 0.3647894859313965
CurrentTrain: epoch  0, batch    23 | loss: 8.4069004Losses:  7.991963863372803 7.642483711242676 0.34948015213012695
CurrentTrain: epoch  0, batch    24 | loss: 7.9919639Losses:  9.328484535217285 8.992817878723145 0.33566635847091675
CurrentTrain: epoch  0, batch    25 | loss: 9.3284845Losses:  9.736856460571289 9.316167831420898 0.42068880796432495
CurrentTrain: epoch  0, batch    26 | loss: 9.7368565Losses:  12.696521759033203 12.362276077270508 0.3342457115650177
CurrentTrain: epoch  0, batch    27 | loss: 12.6965218Losses:  7.073488235473633 6.757729530334473 0.3157586455345154
CurrentTrain: epoch  0, batch    28 | loss: 7.0734882Losses:  7.577298641204834 7.210219383239746 0.3670791983604431
CurrentTrain: epoch  0, batch    29 | loss: 7.5772986Losses:  7.006742477416992 6.663440704345703 0.3433019518852234
CurrentTrain: epoch  0, batch    30 | loss: 7.0067425Losses:  9.22933578491211 8.908156394958496 0.3211793005466461
CurrentTrain: epoch  0, batch    31 | loss: 9.2293358Losses:  8.390254974365234 8.06062126159668 0.3296341300010681
CurrentTrain: epoch  0, batch    32 | loss: 8.3902550Losses:  11.173123359680176 10.939568519592285 0.2335549294948578
CurrentTrain: epoch  0, batch    33 | loss: 11.1731234Losses:  7.244992256164551 6.935703277587891 0.3092888593673706
CurrentTrain: epoch  0, batch    34 | loss: 7.2449923Losses:  8.3689546585083 8.126934051513672 0.24202066659927368
CurrentTrain: epoch  0, batch    35 | loss: 8.3689547Losses:  7.179884433746338 6.893710136413574 0.286174476146698
CurrentTrain: epoch  0, batch    36 | loss: 7.1798844Losses:  3.0497546195983887 2.836320161819458 0.21343456208705902
CurrentTrain: epoch  0, batch    37 | loss: 3.0497546Losses:  11.740644454956055 11.573596954345703 0.16704794764518738
CurrentTrain: epoch  1, batch     0 | loss: 11.7406445Losses:  8.283238410949707 8.037235260009766 0.24600297212600708
CurrentTrain: epoch  1, batch     1 | loss: 8.2832384Losses:  7.533370018005371 7.3556671142578125 0.17770275473594666
CurrentTrain: epoch  1, batch     2 | loss: 7.5333700Losses:  7.220553874969482 7.078551292419434 0.14200261235237122
CurrentTrain: epoch  1, batch     3 | loss: 7.2205539Losses:  8.30922794342041 8.115474700927734 0.19375330209732056
CurrentTrain: epoch  1, batch     4 | loss: 8.3092279Losses:  9.078680992126465 8.946769714355469 0.1319112628698349
CurrentTrain: epoch  1, batch     5 | loss: 9.0786810Losses:  11.960265159606934 11.812572479248047 0.14769284427165985
CurrentTrain: epoch  1, batch     6 | loss: 11.9602652Losses:  7.2624053955078125 7.111878395080566 0.1505271941423416
CurrentTrain: epoch  1, batch     7 | loss: 7.2624054Losses:  7.727890968322754 7.599564552307129 0.1283261924982071
CurrentTrain: epoch  1, batch     8 | loss: 7.7278910Losses:  8.855412483215332 8.719342231750488 0.13607004284858704
CurrentTrain: epoch  1, batch     9 | loss: 8.8554125Losses:  6.094362258911133 5.976573944091797 0.11778812110424042
CurrentTrain: epoch  1, batch    10 | loss: 6.0943623Losses:  6.112604141235352 6.0012407302856445 0.11136332154273987
CurrentTrain: epoch  1, batch    11 | loss: 6.1126041Losses:  6.993866920471191 6.901392936706543 0.09247404336929321
CurrentTrain: epoch  1, batch    12 | loss: 6.9938669Losses:  8.159505844116211 8.05760383605957 0.10190221667289734
CurrentTrain: epoch  1, batch    13 | loss: 8.1595058Losses:  5.8905558586120605 5.798411846160889 0.09214403480291367
CurrentTrain: epoch  1, batch    14 | loss: 5.8905559Losses:  4.8651018142700195 4.765986442565918 0.09911520779132843
CurrentTrain: epoch  1, batch    15 | loss: 4.8651018Losses:  10.249536514282227 10.153373718261719 0.09616278111934662
CurrentTrain: epoch  1, batch    16 | loss: 10.2495365Losses:  8.903812408447266 8.7988920211792 0.10492084920406342
CurrentTrain: epoch  1, batch    17 | loss: 8.9038124Losses:  6.5681681632995605 6.490499496459961 0.0776686891913414
CurrentTrain: epoch  1, batch    18 | loss: 6.5681682Losses:  8.782952308654785 8.70250415802002 0.08044837415218353
CurrentTrain: epoch  1, batch    19 | loss: 8.7829523Losses:  6.553466320037842 6.466998100280762 0.08646827191114426
CurrentTrain: epoch  1, batch    20 | loss: 6.5534663Losses:  7.3959550857543945 7.320897579193115 0.0750574916601181
CurrentTrain: epoch  1, batch    21 | loss: 7.3959551Losses:  9.256645202636719 9.193626403808594 0.06301884353160858
CurrentTrain: epoch  1, batch    22 | loss: 9.2566452Losses:  6.319954872131348 6.251455307006836 0.06849955022335052
CurrentTrain: epoch  1, batch    23 | loss: 6.3199549Losses:  9.793410301208496 9.71541976928711 0.0779908299446106
CurrentTrain: epoch  1, batch    24 | loss: 9.7934103Losses:  5.449105262756348 5.382435321807861 0.06666994094848633
CurrentTrain: epoch  1, batch    25 | loss: 5.4491053Losses:  6.249699115753174 6.184498310089111 0.06520063430070877
CurrentTrain: epoch  1, batch    26 | loss: 6.2496991Losses:  7.14396333694458 7.0722150802612305 0.07174811512231827
CurrentTrain: epoch  1, batch    27 | loss: 7.1439633Losses:  9.008241653442383 8.944668769836426 0.06357318162918091
CurrentTrain: epoch  1, batch    28 | loss: 9.0082417Losses:  5.555611610412598 5.487612247467041 0.06799916923046112
CurrentTrain: epoch  1, batch    29 | loss: 5.5556116Losses:  5.838594436645508 5.784233093261719 0.05436139926314354
CurrentTrain: epoch  1, batch    30 | loss: 5.8385944Losses:  6.890219211578369 6.825961112976074 0.06425796449184418
CurrentTrain: epoch  1, batch    31 | loss: 6.8902192Losses:  5.710042476654053 5.649266242980957 0.060776468366384506
CurrentTrain: epoch  1, batch    32 | loss: 5.7100425Losses:  7.689474582672119 7.632861614227295 0.056612834334373474
CurrentTrain: epoch  1, batch    33 | loss: 7.6894746Losses:  9.330012321472168 9.26545524597168 0.06455712765455246
CurrentTrain: epoch  1, batch    34 | loss: 9.3300123Losses:  6.748012065887451 6.694651126861572 0.05336081236600876
CurrentTrain: epoch  1, batch    35 | loss: 6.7480121Losses:  6.006385326385498 5.954825401306152 0.05155988037586212
CurrentTrain: epoch  1, batch    36 | loss: 6.0063853Losses:  3.844533920288086 3.7535312175750732 0.09100279211997986
CurrentTrain: epoch  1, batch    37 | loss: 3.8445339Losses:  5.733936786651611 5.681055545806885 0.052881233394145966
CurrentTrain: epoch  2, batch     0 | loss: 5.7339368Losses:  7.21619987487793 7.166739463806152 0.04946064576506615
CurrentTrain: epoch  2, batch     1 | loss: 7.2161999Losses:  5.086301326751709 5.0428266525268555 0.04347483813762665
CurrentTrain: epoch  2, batch     2 | loss: 5.0863013Losses:  6.252872467041016 6.208446979522705 0.04442526772618294
CurrentTrain: epoch  2, batch     3 | loss: 6.2528725Losses:  6.244792461395264 6.194748878479004 0.05004352703690529
CurrentTrain: epoch  2, batch     4 | loss: 6.2447925Losses:  7.1671671867370605 7.122006416320801 0.045160941779613495
CurrentTrain: epoch  2, batch     5 | loss: 7.1671672Losses:  5.186629295349121 5.143561840057373 0.04306722432374954
CurrentTrain: epoch  2, batch     6 | loss: 5.1866293Losses:  7.7906670570373535 7.739139556884766 0.05152738839387894
CurrentTrain: epoch  2, batch     7 | loss: 7.7906671Losses:  6.242091655731201 6.1900129318237305 0.05207879841327667
CurrentTrain: epoch  2, batch     8 | loss: 6.2420917Losses:  5.543865203857422 5.4990739822387695 0.04479106515645981
CurrentTrain: epoch  2, batch     9 | loss: 5.5438652Losses:  9.196364402770996 9.149093627929688 0.04727069288492203
CurrentTrain: epoch  2, batch    10 | loss: 9.1963644Losses:  8.578864097595215 8.52360725402832 0.05525721237063408
CurrentTrain: epoch  2, batch    11 | loss: 8.5788641Losses:  6.551023960113525 6.505985260009766 0.045038897544145584
CurrentTrain: epoch  2, batch    12 | loss: 6.5510240Losses:  7.651463031768799 7.603057861328125 0.04840517044067383
CurrentTrain: epoch  2, batch    13 | loss: 7.6514630Losses:  4.100904941558838 4.0644755363464355 0.03642934933304787
CurrentTrain: epoch  2, batch    14 | loss: 4.1009049Losses:  8.885422706604004 8.842839241027832 0.04258302226662636
CurrentTrain: epoch  2, batch    15 | loss: 8.8854227Losses:  5.2223992347717285 5.17718505859375 0.045214153826236725
CurrentTrain: epoch  2, batch    16 | loss: 5.2223992Losses:  5.982236862182617 5.940904140472412 0.04133274778723717
CurrentTrain: epoch  2, batch    17 | loss: 5.9822369Losses:  5.9322590827941895 5.892156600952148 0.040102552622556686
CurrentTrain: epoch  2, batch    18 | loss: 5.9322591Losses:  7.153300762176514 7.112458229064941 0.040842439979314804
CurrentTrain: epoch  2, batch    19 | loss: 7.1533008Losses:  5.47955322265625 5.430442810058594 0.049110230058431625
CurrentTrain: epoch  2, batch    20 | loss: 5.4795532Losses:  6.386862754821777 6.3413848876953125 0.045477885752916336
CurrentTrain: epoch  2, batch    21 | loss: 6.3868628Losses:  7.968135356903076 7.9176926612854 0.050442639738321304
CurrentTrain: epoch  2, batch    22 | loss: 7.9681354Losses:  6.307694911956787 6.271790504455566 0.035904549062252045
CurrentTrain: epoch  2, batch    23 | loss: 6.3076949Losses:  5.092360496520996 5.056962013244629 0.035398393869400024
CurrentTrain: epoch  2, batch    24 | loss: 5.0923605Losses:  6.991478443145752 6.955729961395264 0.035748474299907684
CurrentTrain: epoch  2, batch    25 | loss: 6.9914784Losses:  5.851267337799072 5.817135810852051 0.03413161262869835
CurrentTrain: epoch  2, batch    26 | loss: 5.8512673Losses:  5.296388149261475 5.257073402404785 0.039314836263656616
CurrentTrain: epoch  2, batch    27 | loss: 5.2963881Losses:  6.005842685699463 5.971634864807129 0.03420793265104294
CurrentTrain: epoch  2, batch    28 | loss: 6.0058427Losses:  6.224541664123535 6.186938285827637 0.03760345280170441
CurrentTrain: epoch  2, batch    29 | loss: 6.2245417Losses:  5.674530982971191 5.638083457946777 0.03644745051860809
CurrentTrain: epoch  2, batch    30 | loss: 5.6745310Losses:  6.897122859954834 6.864136695861816 0.03298606723546982
CurrentTrain: epoch  2, batch    31 | loss: 6.8971229Losses:  4.883441925048828 4.853280544281006 0.03016146831214428
CurrentTrain: epoch  2, batch    32 | loss: 4.8834419Losses:  6.189422607421875 6.154766082763672 0.034656595438718796
CurrentTrain: epoch  2, batch    33 | loss: 6.1894226Losses:  7.400132179260254 7.361144065856934 0.03898819163441658
CurrentTrain: epoch  2, batch    34 | loss: 7.4001322Losses:  4.012885093688965 3.9815597534179688 0.0313253290951252
CurrentTrain: epoch  2, batch    35 | loss: 4.0128851Losses:  12.039655685424805 11.97415828704834 0.06549762189388275
CurrentTrain: epoch  2, batch    36 | loss: 12.0396557Losses:  1.5664044618606567 1.5248076915740967 0.04159676283597946
CurrentTrain: epoch  2, batch    37 | loss: 1.5664045Losses:  7.400807857513428 7.367462158203125 0.03334563225507736
CurrentTrain: epoch  3, batch     0 | loss: 7.4008079Losses:  9.228119850158691 9.177189826965332 0.050929680466651917
CurrentTrain: epoch  3, batch     1 | loss: 9.2281199Losses:  7.49998140335083 7.461336135864258 0.03864521160721779
CurrentTrain: epoch  3, batch     2 | loss: 7.4999814Losses:  4.782988548278809 4.746715545654297 0.036272838711738586
CurrentTrain: epoch  3, batch     3 | loss: 4.7829885Losses:  4.956023693084717 4.921548843383789 0.03447464480996132
CurrentTrain: epoch  3, batch     4 | loss: 4.9560237Losses:  7.265502452850342 7.228410720825195 0.037091612815856934
CurrentTrain: epoch  3, batch     5 | loss: 7.2655025Losses:  6.039085865020752 6.008471488952637 0.030614549294114113
CurrentTrain: epoch  3, batch     6 | loss: 6.0390859Losses:  5.079647064208984 5.042964458465576 0.03668265789747238
CurrentTrain: epoch  3, batch     7 | loss: 5.0796471Losses:  12.654105186462402 12.593673706054688 0.06043156981468201
CurrentTrain: epoch  3, batch     8 | loss: 12.6541052Losses:  5.249287128448486 5.2182159423828125 0.031071355566382408
CurrentTrain: epoch  3, batch     9 | loss: 5.2492871Losses:  4.668970108032227 4.638178825378418 0.030791079625487328
CurrentTrain: epoch  3, batch    10 | loss: 4.6689701Losses:  5.7010626792907715 5.667840003967285 0.03322277590632439
CurrentTrain: epoch  3, batch    11 | loss: 5.7010627Losses:  4.150662899017334 4.121493339538574 0.02916935831308365
CurrentTrain: epoch  3, batch    12 | loss: 4.1506629Losses:  5.14162540435791 5.108270645141602 0.03335491567850113
CurrentTrain: epoch  3, batch    13 | loss: 5.1416254Losses:  7.853263854980469 7.8142595291137695 0.0390041284263134
CurrentTrain: epoch  3, batch    14 | loss: 7.8532639Losses:  4.5073652267456055 4.481159210205078 0.026206040754914284
CurrentTrain: epoch  3, batch    15 | loss: 4.5073652Losses:  4.974163055419922 4.945585250854492 0.028577767312526703
CurrentTrain: epoch  3, batch    16 | loss: 4.9741631Losses:  5.637705326080322 5.604842185974121 0.03286316245794296
CurrentTrain: epoch  3, batch    17 | loss: 5.6377053Losses:  4.2614970207214355 4.231215000152588 0.03028198517858982
CurrentTrain: epoch  3, batch    18 | loss: 4.2614970Losses:  8.439864158630371 8.402281761169434 0.03758244216442108
CurrentTrain: epoch  3, batch    19 | loss: 8.4398642Losses:  4.962615489959717 4.934396743774414 0.028218913823366165
CurrentTrain: epoch  3, batch    20 | loss: 4.9626155Losses:  7.682274341583252 7.645998001098633 0.03627611696720123
CurrentTrain: epoch  3, batch    21 | loss: 7.6822743Losses:  7.665005683898926 7.63081169128418 0.03419393673539162
CurrentTrain: epoch  3, batch    22 | loss: 7.6650057Losses:  5.622696876525879 5.581366539001465 0.04133051633834839
CurrentTrain: epoch  3, batch    23 | loss: 5.6226969Losses:  7.0285444259643555 6.995449542999268 0.03309488296508789
CurrentTrain: epoch  3, batch    24 | loss: 7.0285444Losses:  7.409868240356445 7.376245498657227 0.03362260386347771
CurrentTrain: epoch  3, batch    25 | loss: 7.4098682Losses:  6.701850414276123 6.668562889099121 0.033287517726421356
CurrentTrain: epoch  3, batch    26 | loss: 6.7018504Losses:  8.229010581970215 8.199092864990234 0.029917610809206963
CurrentTrain: epoch  3, batch    27 | loss: 8.2290106Losses:  9.464935302734375 9.427908897399902 0.037026192992925644
CurrentTrain: epoch  3, batch    28 | loss: 9.4649353Losses:  9.560402870178223 9.518198013305664 0.04220481961965561
CurrentTrain: epoch  3, batch    29 | loss: 9.5604029Losses:  5.973782539367676 5.942399978637695 0.03138274699449539
CurrentTrain: epoch  3, batch    30 | loss: 5.9737825Losses:  5.969399452209473 5.9281206130981445 0.04127870127558708
CurrentTrain: epoch  3, batch    31 | loss: 5.9693995Losses:  12.008563995361328 11.966806411743164 0.04175730049610138
CurrentTrain: epoch  3, batch    32 | loss: 12.0085640Losses:  9.809654235839844 9.779712677001953 0.029941212385892868
CurrentTrain: epoch  3, batch    33 | loss: 9.8096542Losses:  5.485717296600342 5.45012903213501 0.035588059574365616
CurrentTrain: epoch  3, batch    34 | loss: 5.4857173Losses:  4.26635217666626 4.241064548492432 0.025287527590990067
CurrentTrain: epoch  3, batch    35 | loss: 4.2663522Losses:  4.982485771179199 4.953187942504883 0.02929798513650894
CurrentTrain: epoch  3, batch    36 | loss: 4.9824858Losses:  1.1175211668014526 1.0807230472564697 0.036798082292079926
CurrentTrain: epoch  3, batch    37 | loss: 1.1175212Losses:  9.582904815673828 9.543495178222656 0.03940995782613754
CurrentTrain: epoch  4, batch     0 | loss: 9.5829048Losses:  13.165154457092285 13.124542236328125 0.04061198979616165
CurrentTrain: epoch  4, batch     1 | loss: 13.1651545Losses:  9.78426742553711 9.749547958374023 0.03471923619508743
CurrentTrain: epoch  4, batch     2 | loss: 9.7842674Losses:  5.72786283493042 5.694120407104492 0.03374224528670311
CurrentTrain: epoch  4, batch     3 | loss: 5.7278628Losses:  5.089416980743408 5.061983585357666 0.02743343450129032
CurrentTrain: epoch  4, batch     4 | loss: 5.0894170Losses:  5.857915878295898 5.822417259216309 0.03549844026565552
CurrentTrain: epoch  4, batch     5 | loss: 5.8579159Losses:  5.330366611480713 5.30394172668457 0.026424681767821312
CurrentTrain: epoch  4, batch     6 | loss: 5.3303666Losses:  3.7109413146972656 3.6867311000823975 0.024210285395383835
CurrentTrain: epoch  4, batch     7 | loss: 3.7109413Losses:  8.828719139099121 8.795695304870605 0.033023953437805176
CurrentTrain: epoch  4, batch     8 | loss: 8.8287191Losses:  5.315307140350342 5.289543151855469 0.025763968005776405
CurrentTrain: epoch  4, batch     9 | loss: 5.3153071Losses:  4.923101902008057 4.8947553634643555 0.02834673412144184
CurrentTrain: epoch  4, batch    10 | loss: 4.9231019Losses:  5.126112461090088 5.102231025695801 0.023881403729319572
CurrentTrain: epoch  4, batch    11 | loss: 5.1261125Losses:  4.645992279052734 4.618212699890137 0.027779584750533104
CurrentTrain: epoch  4, batch    12 | loss: 4.6459923Losses:  9.372386932373047 9.333388328552246 0.03899816423654556
CurrentTrain: epoch  4, batch    13 | loss: 9.3723869Losses:  8.132437705993652 8.083993911743164 0.0484442375600338
CurrentTrain: epoch  4, batch    14 | loss: 8.1324377Losses:  4.213957786560059 4.185833930969238 0.028124051168560982
CurrentTrain: epoch  4, batch    15 | loss: 4.2139578Losses:  10.892572402954102 10.855659484863281 0.03691263496875763
CurrentTrain: epoch  4, batch    16 | loss: 10.8925724Losses:  4.253614902496338 4.227475166320801 0.02613961510360241
CurrentTrain: epoch  4, batch    17 | loss: 4.2536149Losses:  5.51146936416626 5.484158992767334 0.027310369536280632
CurrentTrain: epoch  4, batch    18 | loss: 5.5114694Losses:  5.991696357727051 5.954995632171631 0.036700811237096786
CurrentTrain: epoch  4, batch    19 | loss: 5.9916964Losses:  6.892475605010986 6.856141090393066 0.03633451461791992
CurrentTrain: epoch  4, batch    20 | loss: 6.8924756Losses:  5.677732944488525 5.641608238220215 0.03612469881772995
CurrentTrain: epoch  4, batch    21 | loss: 5.6777329Losses:  6.743683815002441 6.717402458190918 0.02628123015165329
CurrentTrain: epoch  4, batch    22 | loss: 6.7436838Losses:  4.266005516052246 4.23675537109375 0.02925005555152893
CurrentTrain: epoch  4, batch    23 | loss: 4.2660055Losses:  4.88308572769165 4.85153341293335 0.03155209496617317
CurrentTrain: epoch  4, batch    24 | loss: 4.8830857Losses:  5.486778259277344 5.458259105682373 0.028519319370388985
CurrentTrain: epoch  4, batch    25 | loss: 5.4867783Losses:  7.2422099113464355 7.203969955444336 0.038240041583776474
CurrentTrain: epoch  4, batch    26 | loss: 7.2422099Losses:  4.343948841094971 4.32071590423584 0.02323298528790474
CurrentTrain: epoch  4, batch    27 | loss: 4.3439488Losses:  7.678620338439941 7.647542476654053 0.031077712774276733
CurrentTrain: epoch  4, batch    28 | loss: 7.6786203Losses:  4.203838348388672 4.1804022789001465 0.023436207324266434
CurrentTrain: epoch  4, batch    29 | loss: 4.2038383Losses:  5.325307369232178 5.293909549713135 0.03139788284897804
CurrentTrain: epoch  4, batch    30 | loss: 5.3253074Losses:  10.996792793273926 10.970837593078613 0.025955641642212868
CurrentTrain: epoch  4, batch    31 | loss: 10.9967928Losses:  5.847732067108154 5.818332672119141 0.029399449005723
CurrentTrain: epoch  4, batch    32 | loss: 5.8477321Losses:  6.784396648406982 6.754911422729492 0.029485046863555908
CurrentTrain: epoch  4, batch    33 | loss: 6.7843966Losses:  5.301816940307617 5.279272556304932 0.02254428341984749
CurrentTrain: epoch  4, batch    34 | loss: 5.3018169Losses:  6.73971700668335 6.705471038818359 0.03424612060189247
CurrentTrain: epoch  4, batch    35 | loss: 6.7397170Losses:  6.535419464111328 6.50016450881958 0.03525484353303909
CurrentTrain: epoch  4, batch    36 | loss: 6.5354195Losses:  2.308511972427368 2.2579102516174316 0.05060170590877533
CurrentTrain: epoch  4, batch    37 | loss: 2.3085120Losses:  4.614124298095703 4.584383010864258 0.02974121645092964
CurrentTrain: epoch  5, batch     0 | loss: 4.6141243Losses:  4.878220081329346 4.849122047424316 0.02909814938902855
CurrentTrain: epoch  5, batch     1 | loss: 4.8782201Losses:  11.833012580871582 11.784185409545898 0.048827074468135834
CurrentTrain: epoch  5, batch     2 | loss: 11.8330126Losses:  5.05195951461792 5.020354270935059 0.031605228781700134
CurrentTrain: epoch  5, batch     3 | loss: 5.0519595Losses:  5.108922481536865 5.081266403198242 0.027655981481075287
CurrentTrain: epoch  5, batch     4 | loss: 5.1089225Losses:  5.397334098815918 5.372980117797852 0.024353744462132454
CurrentTrain: epoch  5, batch     5 | loss: 5.3973341Losses:  5.137753009796143 5.110590934753418 0.027162229642271996
CurrentTrain: epoch  5, batch     6 | loss: 5.1377530Losses:  6.5640692710876465 6.527925968170166 0.03614344075322151
CurrentTrain: epoch  5, batch     7 | loss: 6.5640693Losses:  4.428684711456299 4.40494441986084 0.02374032326042652
CurrentTrain: epoch  5, batch     8 | loss: 4.4286847Losses:  5.486316204071045 5.460133075714111 0.026183338835835457
CurrentTrain: epoch  5, batch     9 | loss: 5.4863162Losses:  5.264873504638672 5.239229202270508 0.02564435638487339
CurrentTrain: epoch  5, batch    10 | loss: 5.2648735Losses:  5.051315784454346 5.028070449829102 0.02324521355330944
CurrentTrain: epoch  5, batch    11 | loss: 5.0513158Losses:  6.472086429595947 6.439610958099365 0.032475490123033524
CurrentTrain: epoch  5, batch    12 | loss: 6.4720864Losses:  9.133692741394043 9.104768753051758 0.02892421744763851
CurrentTrain: epoch  5, batch    13 | loss: 9.1336927Losses:  9.073761940002441 9.049501419067383 0.02426014468073845
CurrentTrain: epoch  5, batch    14 | loss: 9.0737619Losses:  5.576323509216309 5.547564506530762 0.02875923179090023
CurrentTrain: epoch  5, batch    15 | loss: 5.5763235Losses:  5.327193260192871 5.295165061950684 0.03202821686863899
CurrentTrain: epoch  5, batch    16 | loss: 5.3271933Losses:  4.5260443687438965 4.50016450881958 0.025879673659801483
CurrentTrain: epoch  5, batch    17 | loss: 4.5260444Losses:  5.820780277252197 5.79517936706543 0.025600843131542206
CurrentTrain: epoch  5, batch    18 | loss: 5.8207803Losses:  4.3553361892700195 4.333065032958984 0.022271281108260155
CurrentTrain: epoch  5, batch    19 | loss: 4.3553362Losses:  5.104132175445557 5.076447010040283 0.027685318142175674
CurrentTrain: epoch  5, batch    20 | loss: 5.1041322Losses:  7.41933536529541 7.389341831207275 0.029993562027812004
CurrentTrain: epoch  5, batch    21 | loss: 7.4193354Losses:  9.70538330078125 9.674530982971191 0.03085268847644329
CurrentTrain: epoch  5, batch    22 | loss: 9.7053833Losses:  12.887454986572266 12.839106559753418 0.048348721116781235
CurrentTrain: epoch  5, batch    23 | loss: 12.8874550Losses:  7.01266622543335 6.982377052307129 0.030289040878415108
CurrentTrain: epoch  5, batch    24 | loss: 7.0126662Losses:  4.761609077453613 4.7347307205200195 0.026878416538238525
CurrentTrain: epoch  5, batch    25 | loss: 4.7616091Losses:  5.242913722991943 5.21909236907959 0.02382134273648262
CurrentTrain: epoch  5, batch    26 | loss: 5.2429137Losses:  4.205821990966797 4.1828718185424805 0.02295038476586342
CurrentTrain: epoch  5, batch    27 | loss: 4.2058220Losses:  6.930450916290283 6.895176410675049 0.035274580121040344
CurrentTrain: epoch  5, batch    28 | loss: 6.9304509Losses:  6.4685845375061035 6.442445278167725 0.026139266788959503
CurrentTrain: epoch  5, batch    29 | loss: 6.4685845Losses:  7.219998359680176 7.175834655761719 0.04416382312774658
CurrentTrain: epoch  5, batch    30 | loss: 7.2199984Losses:  6.7964768409729 6.766593933105469 0.029883017763495445
CurrentTrain: epoch  5, batch    31 | loss: 6.7964768Losses:  4.5488505363464355 4.520430564880371 0.028420083224773407
CurrentTrain: epoch  5, batch    32 | loss: 4.5488505Losses:  5.739284038543701 5.716183662414551 0.02310043014585972
CurrentTrain: epoch  5, batch    33 | loss: 5.7392840Losses:  8.274438858032227 8.238344192504883 0.03609480336308479
CurrentTrain: epoch  5, batch    34 | loss: 8.2744389Losses:  4.718360900878906 4.694011688232422 0.024349091574549675
CurrentTrain: epoch  5, batch    35 | loss: 4.7183609Losses:  5.272985935211182 5.249147415161133 0.023838337510824203
CurrentTrain: epoch  5, batch    36 | loss: 5.2729859Losses:  3.3698596954345703 3.318100929260254 0.051758673042058945
CurrentTrain: epoch  5, batch    37 | loss: 3.3698597Losses:  7.995595932006836 7.970215320587158 0.025380626320838928
CurrentTrain: epoch  6, batch     0 | loss: 7.9955959Losses:  4.473043441772461 4.445484638214111 0.027558572590351105
CurrentTrain: epoch  6, batch     1 | loss: 4.4730434Losses:  6.776500701904297 6.751589298248291 0.02491132915019989
CurrentTrain: epoch  6, batch     2 | loss: 6.7765007Losses:  7.665863037109375 7.636969089508057 0.028894105926156044
CurrentTrain: epoch  6, batch     3 | loss: 7.6658630Losses:  6.065190315246582 6.033560276031494 0.031630173325538635
CurrentTrain: epoch  6, batch     4 | loss: 6.0651903Losses:  9.210869789123535 9.168558120727539 0.04231183975934982
CurrentTrain: epoch  6, batch     5 | loss: 9.2108698Losses:  6.067317485809326 6.0363569259643555 0.03096064180135727
CurrentTrain: epoch  6, batch     6 | loss: 6.0673175Losses:  3.9428160190582275 3.9191925525665283 0.023623554036021233
CurrentTrain: epoch  6, batch     7 | loss: 3.9428160Losses:  5.604727745056152 5.5753583908081055 0.029369179159402847
CurrentTrain: epoch  6, batch     8 | loss: 5.6047277Losses:  9.395737648010254 9.36561107635498 0.030126536265015602
CurrentTrain: epoch  6, batch     9 | loss: 9.3957376Losses:  9.020581245422363 8.976492881774902 0.044087909162044525
CurrentTrain: epoch  6, batch    10 | loss: 9.0205812Losses:  5.919758319854736 5.886929512023926 0.032828669995069504
CurrentTrain: epoch  6, batch    11 | loss: 5.9197583Losses:  4.603311538696289 4.576725959777832 0.02658558450639248
CurrentTrain: epoch  6, batch    12 | loss: 4.6033115Losses:  6.632084369659424 6.603456497192383 0.02862781472504139
CurrentTrain: epoch  6, batch    13 | loss: 6.6320844Losses:  7.392979621887207 7.357296943664551 0.035682618618011475
CurrentTrain: epoch  6, batch    14 | loss: 7.3929796Losses:  5.611545562744141 5.581160545349121 0.030384913086891174
CurrentTrain: epoch  6, batch    15 | loss: 5.6115456Losses:  4.154024124145508 4.130155563354492 0.023868484422564507
CurrentTrain: epoch  6, batch    16 | loss: 4.1540241Losses:  5.421709060668945 5.394577980041504 0.02713130973279476
CurrentTrain: epoch  6, batch    17 | loss: 5.4217091Losses:  4.883569717407227 4.856237888336182 0.027331944555044174
CurrentTrain: epoch  6, batch    18 | loss: 4.8835697Losses:  5.496883392333984 5.467983722686768 0.028899550437927246
CurrentTrain: epoch  6, batch    19 | loss: 5.4968834Losses:  4.790365219116211 4.762674331665039 0.027690652757883072
CurrentTrain: epoch  6, batch    20 | loss: 4.7903652Losses:  4.25701379776001 4.232800483703613 0.024213427677750587
CurrentTrain: epoch  6, batch    21 | loss: 4.2570138Losses:  4.275840759277344 4.25162410736084 0.024216802790760994
CurrentTrain: epoch  6, batch    22 | loss: 4.2758408Losses:  7.489576816558838 7.4488348960876465 0.040742017328739166
CurrentTrain: epoch  6, batch    23 | loss: 7.4895768Losses:  4.948952674865723 4.922776222229004 0.02617625519633293
CurrentTrain: epoch  6, batch    24 | loss: 4.9489527Losses:  6.658726215362549 6.625509738922119 0.03321637213230133
CurrentTrain: epoch  6, batch    25 | loss: 6.6587262Losses:  7.207536220550537 7.178170680999756 0.02936558984220028
CurrentTrain: epoch  6, batch    26 | loss: 7.2075362Losses:  6.895392894744873 6.856557369232178 0.03883540630340576
CurrentTrain: epoch  6, batch    27 | loss: 6.8953929Losses:  5.575791835784912 5.551774024963379 0.024017928168177605
CurrentTrain: epoch  6, batch    28 | loss: 5.5757918Losses:  5.158774375915527 5.1289286613464355 0.029845530167222023
CurrentTrain: epoch  6, batch    29 | loss: 5.1587744Losses:  7.188177585601807 7.156713008880615 0.03146442025899887
CurrentTrain: epoch  6, batch    30 | loss: 7.1881776Losses:  8.457744598388672 8.427267074584961 0.030477652326226234
CurrentTrain: epoch  6, batch    31 | loss: 8.4577446Losses:  4.229466915130615 4.204641819000244 0.024825165048241615
CurrentTrain: epoch  6, batch    32 | loss: 4.2294669Losses:  4.993865966796875 4.969986438751221 0.023879310116171837
CurrentTrain: epoch  6, batch    33 | loss: 4.9938660Losses:  6.752994537353516 6.728909492492676 0.024085018783807755
CurrentTrain: epoch  6, batch    34 | loss: 6.7529945Losses:  8.102937698364258 8.0545072555542 0.04843049496412277
CurrentTrain: epoch  6, batch    35 | loss: 8.1029377Losses:  3.4940688610076904 3.4720325469970703 0.022036338225007057
CurrentTrain: epoch  6, batch    36 | loss: 3.4940689Losses:  1.321799397468567 1.276993751525879 0.044805653393268585
CurrentTrain: epoch  6, batch    37 | loss: 1.3217994Losses:  6.6932759284973145 6.657591819763184 0.03568388521671295
CurrentTrain: epoch  7, batch     0 | loss: 6.6932759Losses:  3.4402153491973877 3.4187464714050293 0.021468980237841606
CurrentTrain: epoch  7, batch     1 | loss: 3.4402153Losses:  9.347018241882324 9.316770553588867 0.030247636139392853
CurrentTrain: epoch  7, batch     2 | loss: 9.3470182Losses:  7.893115520477295 7.864894390106201 0.028221115469932556
CurrentTrain: epoch  7, batch     3 | loss: 7.8931155Losses:  6.3210015296936035 6.297290802001953 0.023710520938038826
CurrentTrain: epoch  7, batch     4 | loss: 6.3210015Losses:  6.968100547790527 6.939028739929199 0.029071634635329247
CurrentTrain: epoch  7, batch     5 | loss: 6.9681005Losses:  3.79191255569458 3.770773410797119 0.021139254793524742
CurrentTrain: epoch  7, batch     6 | loss: 3.7919126Losses:  3.8715834617614746 3.85036039352417 0.02122311107814312
CurrentTrain: epoch  7, batch     7 | loss: 3.8715835Losses:  6.102884292602539 6.067960739135742 0.03492359071969986
CurrentTrain: epoch  7, batch     8 | loss: 6.1028843Losses:  7.327087879180908 7.296204566955566 0.030883319675922394
CurrentTrain: epoch  7, batch     9 | loss: 7.3270879Losses:  4.108334064483643 4.082910537719727 0.025423627346754074
CurrentTrain: epoch  7, batch    10 | loss: 4.1083341Losses:  5.336872577667236 5.306519508361816 0.030352937057614326
CurrentTrain: epoch  7, batch    11 | loss: 5.3368726Losses:  6.294754981994629 6.255221366882324 0.039533838629722595
CurrentTrain: epoch  7, batch    12 | loss: 6.2947550Losses:  5.99064826965332 5.962512969970703 0.028135351836681366
CurrentTrain: epoch  7, batch    13 | loss: 5.9906483Losses:  6.428913593292236 6.386837005615234 0.0420764721930027
CurrentTrain: epoch  7, batch    14 | loss: 6.4289136Losses:  6.886693477630615 6.852892875671387 0.03380073606967926
CurrentTrain: epoch  7, batch    15 | loss: 6.8866935Losses:  5.78125 5.752468109130859 0.02878176048398018
CurrentTrain: epoch  7, batch    16 | loss: 5.7812500Losses:  7.9113359451293945 7.884321212768555 0.02701464667916298
CurrentTrain: epoch  7, batch    17 | loss: 7.9113359Losses:  7.164469242095947 7.141297340393066 0.023171834647655487
CurrentTrain: epoch  7, batch    18 | loss: 7.1644692Losses:  4.171492099761963 4.148797988891602 0.022693924605846405
CurrentTrain: epoch  7, batch    19 | loss: 4.1714921Losses:  4.810356140136719 4.785799026489258 0.024557089433073997
CurrentTrain: epoch  7, batch    20 | loss: 4.8103561Losses:  5.080420970916748 5.050232410430908 0.03018876537680626
CurrentTrain: epoch  7, batch    21 | loss: 5.0804210Losses:  6.1832380294799805 6.154575347900391 0.028662625700235367
CurrentTrain: epoch  7, batch    22 | loss: 6.1832380Losses:  10.222681045532227 10.184684753417969 0.037996143102645874
CurrentTrain: epoch  7, batch    23 | loss: 10.2226810Losses:  5.43164587020874 5.391453742980957 0.04019233211874962
CurrentTrain: epoch  7, batch    24 | loss: 5.4316459Losses:  4.59169864654541 4.56565523147583 0.02604321949183941
CurrentTrain: epoch  7, batch    25 | loss: 4.5916986Losses:  3.778449058532715 3.7541964054107666 0.02425275556743145
CurrentTrain: epoch  7, batch    26 | loss: 3.7784491Losses:  6.249894618988037 6.2127766609191895 0.03711797669529915
CurrentTrain: epoch  7, batch    27 | loss: 6.2498946Losses:  4.414980888366699 4.391003608703613 0.023977193981409073
CurrentTrain: epoch  7, batch    28 | loss: 4.4149809Losses:  8.631621360778809 8.576730728149414 0.054890431463718414
CurrentTrain: epoch  7, batch    29 | loss: 8.6316214Losses:  10.783748626708984 10.745835304260254 0.037912990897893906
CurrentTrain: epoch  7, batch    30 | loss: 10.7837486Losses:  4.93185567855835 4.908290863037109 0.023564640432596207
CurrentTrain: epoch  7, batch    31 | loss: 4.9318557Losses:  4.644647121429443 4.619991302490234 0.02465595304965973
CurrentTrain: epoch  7, batch    32 | loss: 4.6446471Losses:  5.4947638511657715 5.471442699432373 0.023321092128753662
CurrentTrain: epoch  7, batch    33 | loss: 5.4947639Losses:  3.706878662109375 3.683699131011963 0.02317957952618599
CurrentTrain: epoch  7, batch    34 | loss: 3.7068787Losses:  4.786498069763184 4.759439468383789 0.0270586796104908
CurrentTrain: epoch  7, batch    35 | loss: 4.7864981Losses:  4.3712663650512695 4.348020553588867 0.02324567921459675
CurrentTrain: epoch  7, batch    36 | loss: 4.3712664Losses:  1.2886871099472046 1.2534680366516113 0.03521905466914177
CurrentTrain: epoch  7, batch    37 | loss: 1.2886871Losses:  5.101727485656738 5.071230888366699 0.030496615916490555
CurrentTrain: epoch  8, batch     0 | loss: 5.1017275Losses:  5.612473011016846 5.5820112228393555 0.030461637303233147
CurrentTrain: epoch  8, batch     1 | loss: 5.6124730Losses:  4.387147903442383 4.359552383422852 0.02759545110166073
CurrentTrain: epoch  8, batch     2 | loss: 4.3871479Losses:  5.148176193237305 5.11665153503418 0.03152449056506157
CurrentTrain: epoch  8, batch     3 | loss: 5.1481762Losses:  4.722843170166016 4.695197105407715 0.027646008878946304
CurrentTrain: epoch  8, batch     4 | loss: 4.7228432Losses:  7.505873680114746 7.465736389160156 0.04013724997639656
CurrentTrain: epoch  8, batch     5 | loss: 7.5058737Losses:  5.273180961608887 5.238583087921143 0.034597814083099365
CurrentTrain: epoch  8, batch     6 | loss: 5.2731810Losses:  6.770816326141357 6.738256931304932 0.03255920857191086
CurrentTrain: epoch  8, batch     7 | loss: 6.7708163Losses:  3.811924457550049 3.7891814708709717 0.02274310030043125
CurrentTrain: epoch  8, batch     8 | loss: 3.8119245Losses:  4.299368381500244 4.277189254760742 0.022179292514920235
CurrentTrain: epoch  8, batch     9 | loss: 4.2993684Losses:  5.155247211456299 5.130202770233154 0.025044573470950127
CurrentTrain: epoch  8, batch    10 | loss: 5.1552472Losses:  5.673661231994629 5.64634370803833 0.027317561209201813
CurrentTrain: epoch  8, batch    11 | loss: 5.6736612Losses:  7.045961380004883 7.013810634613037 0.03215058892965317
CurrentTrain: epoch  8, batch    12 | loss: 7.0459614Losses:  9.01771354675293 8.96721363067627 0.050500161945819855
CurrentTrain: epoch  8, batch    13 | loss: 9.0177135Losses:  6.58569860458374 6.548274993896484 0.037423472851514816
CurrentTrain: epoch  8, batch    14 | loss: 6.5856986Losses:  3.813419818878174 3.7898998260498047 0.023520104587078094
CurrentTrain: epoch  8, batch    15 | loss: 3.8134198Losses:  7.684121608734131 7.658339023590088 0.02578243985772133
CurrentTrain: epoch  8, batch    16 | loss: 7.6841216Losses:  7.254162788391113 7.215450286865234 0.038712646812200546
CurrentTrain: epoch  8, batch    17 | loss: 7.2541628Losses:  8.95307445526123 8.90848159790039 0.04459298774600029
CurrentTrain: epoch  8, batch    18 | loss: 8.9530745Losses:  6.039003849029541 6.013772964477539 0.025230921804904938
CurrentTrain: epoch  8, batch    19 | loss: 6.0390038Losses:  4.882335662841797 4.856048583984375 0.026287151500582695
CurrentTrain: epoch  8, batch    20 | loss: 4.8823357Losses:  5.133053302764893 5.105449676513672 0.02760368399322033
CurrentTrain: epoch  8, batch    21 | loss: 5.1330533Losses:  8.027153968811035 7.977799892425537 0.04935399070382118
CurrentTrain: epoch  8, batch    22 | loss: 8.0271540Losses:  4.4690141677856445 4.440177917480469 0.028836429119110107
CurrentTrain: epoch  8, batch    23 | loss: 4.4690142Losses:  5.276340007781982 5.242265701293945 0.03407446667551994
CurrentTrain: epoch  8, batch    24 | loss: 5.2763400Losses:  7.054984092712402 7.026834964752197 0.028148911893367767
CurrentTrain: epoch  8, batch    25 | loss: 7.0549841Losses:  4.58622407913208 4.56110954284668 0.025114675983786583
CurrentTrain: epoch  8, batch    26 | loss: 4.5862241Losses:  3.833688974380493 3.8081560134887695 0.025532865896821022
CurrentTrain: epoch  8, batch    27 | loss: 3.8336890Losses:  4.064216613769531 4.038272380828857 0.025944143533706665
CurrentTrain: epoch  8, batch    28 | loss: 4.0642166Losses:  5.327060222625732 5.299178600311279 0.027881726622581482
CurrentTrain: epoch  8, batch    29 | loss: 5.3270602Losses:  6.067978382110596 6.0345258712768555 0.03345245122909546
CurrentTrain: epoch  8, batch    30 | loss: 6.0679784Losses:  3.935141086578369 3.9103784561157227 0.02476271614432335
CurrentTrain: epoch  8, batch    31 | loss: 3.9351411Losses:  6.391997814178467 6.351188659667969 0.0408090241253376
CurrentTrain: epoch  8, batch    32 | loss: 6.3919978Losses:  6.635468006134033 6.59206485748291 0.043403271585702896
CurrentTrain: epoch  8, batch    33 | loss: 6.6354680Losses:  4.192026615142822 4.164990425109863 0.027036229148507118
CurrentTrain: epoch  8, batch    34 | loss: 4.1920266Losses:  7.198779582977295 7.158059120178223 0.04072050005197525
CurrentTrain: epoch  8, batch    35 | loss: 7.1987796Losses:  4.381680011749268 4.354945182800293 0.026734795421361923
CurrentTrain: epoch  8, batch    36 | loss: 4.3816800Losses:  1.4955915212631226 1.4539721012115479 0.04161946475505829
CurrentTrain: epoch  8, batch    37 | loss: 1.4955915Losses:  6.273297309875488 6.237278938293457 0.03601856902241707
CurrentTrain: epoch  9, batch     0 | loss: 6.2732973Losses:  5.219801902770996 5.193918228149414 0.025883624330163002
CurrentTrain: epoch  9, batch     1 | loss: 5.2198019Losses:  5.410304069519043 5.372910499572754 0.037393614649772644
CurrentTrain: epoch  9, batch     2 | loss: 5.4103041Losses:  5.001364707946777 4.977148056030273 0.024216802790760994
CurrentTrain: epoch  9, batch     3 | loss: 5.0013647Losses:  3.736917018890381 3.7150776386260986 0.021839361637830734
CurrentTrain: epoch  9, batch     4 | loss: 3.7369170Losses:  5.9301347732543945 5.900572776794434 0.029562130570411682
CurrentTrain: epoch  9, batch     5 | loss: 5.9301348Losses:  6.831946849822998 6.800971984863281 0.030974742025136948
CurrentTrain: epoch  9, batch     6 | loss: 6.8319468Losses:  5.914278507232666 5.877477645874023 0.03680083900690079
CurrentTrain: epoch  9, batch     7 | loss: 5.9142785Losses:  3.726943254470825 3.703383445739746 0.023559866473078728
CurrentTrain: epoch  9, batch     8 | loss: 3.7269433Losses:  6.512752056121826 6.473324775695801 0.039427436888217926
CurrentTrain: epoch  9, batch     9 | loss: 6.5127521Losses:  7.162095546722412 7.119264602661133 0.042830780148506165
CurrentTrain: epoch  9, batch    10 | loss: 7.1620955Losses:  5.285873889923096 5.255059242248535 0.030814601108431816
CurrentTrain: epoch  9, batch    11 | loss: 5.2858739Losses:  4.64290189743042 4.616462230682373 0.026439543813467026
CurrentTrain: epoch  9, batch    12 | loss: 4.6429019Losses:  5.462679386138916 5.433701038360596 0.028978126123547554
CurrentTrain: epoch  9, batch    13 | loss: 5.4626794Losses:  9.209368705749512 9.158923149108887 0.05044569820165634
CurrentTrain: epoch  9, batch    14 | loss: 9.2093687Losses:  4.157850742340088 4.131830215454102 0.026020687073469162
CurrentTrain: epoch  9, batch    15 | loss: 4.1578507Losses:  4.721200942993164 4.69345760345459 0.027743294835090637
CurrentTrain: epoch  9, batch    16 | loss: 4.7212009Losses:  4.97600793838501 4.9465131759643555 0.02949465438723564
CurrentTrain: epoch  9, batch    17 | loss: 4.9760079Losses:  6.013388156890869 5.980328559875488 0.033059362322092056
CurrentTrain: epoch  9, batch    18 | loss: 6.0133882Losses:  5.439849376678467 5.408336162567139 0.03151305392384529
CurrentTrain: epoch  9, batch    19 | loss: 5.4398494Losses:  6.501260757446289 6.474145889282227 0.027114776894450188
CurrentTrain: epoch  9, batch    20 | loss: 6.5012608Losses:  3.3169538974761963 3.2942054271698 0.02274857833981514
CurrentTrain: epoch  9, batch    21 | loss: 3.3169539Losses:  5.855984687805176 5.8294878005981445 0.0264970101416111
CurrentTrain: epoch  9, batch    22 | loss: 5.8559847Losses:  4.687746047973633 4.664371490478516 0.02337471954524517
CurrentTrain: epoch  9, batch    23 | loss: 4.6877460Losses:  5.0681352615356445 5.035442352294922 0.03269299864768982
CurrentTrain: epoch  9, batch    24 | loss: 5.0681353Losses:  6.550197601318359 6.516314506530762 0.03388295695185661
CurrentTrain: epoch  9, batch    25 | loss: 6.5501976Losses:  5.915454864501953 5.877440929412842 0.03801398724317551
CurrentTrain: epoch  9, batch    26 | loss: 5.9154549Losses:  7.792581081390381 7.747946739196777 0.0446343757212162
CurrentTrain: epoch  9, batch    27 | loss: 7.7925811Losses:  5.5673699378967285 5.5330657958984375 0.03430429473519325
CurrentTrain: epoch  9, batch    28 | loss: 5.5673699Losses:  5.720775127410889 5.691242218017578 0.029532790184020996
CurrentTrain: epoch  9, batch    29 | loss: 5.7207751Losses:  5.436040878295898 5.411074638366699 0.024966035038232803
CurrentTrain: epoch  9, batch    30 | loss: 5.4360409Losses:  5.240617275238037 5.2156081199646 0.025009112432599068
CurrentTrain: epoch  9, batch    31 | loss: 5.2406173Losses:  6.514864444732666 6.478748321533203 0.036116018891334534
CurrentTrain: epoch  9, batch    32 | loss: 6.5148644Losses:  4.34153938293457 4.314035892486572 0.027503717690706253
CurrentTrain: epoch  9, batch    33 | loss: 4.3415394Losses:  5.346343517303467 5.314377307891846 0.03196610137820244
CurrentTrain: epoch  9, batch    34 | loss: 5.3463435Losses:  4.859447956085205 4.830972671508789 0.02847534976899624
CurrentTrain: epoch  9, batch    35 | loss: 4.8594480Losses:  11.798750877380371 11.748931884765625 0.04981853440403938
CurrentTrain: epoch  9, batch    36 | loss: 11.7987509Losses:  2.3957772254943848 2.35062837600708 0.04514886438846588
CurrentTrain: epoch  9, batch    37 | loss: 2.3957772
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 83.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.33%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.02%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 87.04%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 87.05%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 87.28%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 86.67%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 86.49%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 86.52%   [EVAL] batch:   32 | acc: 31.25%,  total acc: 84.85%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 83.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.33%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.02%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 87.04%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 87.05%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 87.28%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 86.67%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 86.49%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 86.52%   [EVAL] batch:   32 | acc: 31.25%,  total acc: 84.85%   
cur_acc:  ['0.8485']
his_acc:  ['0.8485']
Clustering into  4  clusters
Clusters:  [0 2 2 0 0 3 0 1 0 0 0]
Losses:  9.187363624572754 8.830904960632324 0.3564585745334625
CurrentTrain: epoch  0, batch     0 | loss: 9.1873636Losses:  5.831049919128418 5.634433746337891 0.1966162472963333
CurrentTrain: epoch  0, batch     1 | loss: 5.8310499Losses:  9.126443862915039 8.798835754394531 0.32760804891586304
CurrentTrain: epoch  1, batch     0 | loss: 9.1264439Losses:  4.3694634437561035 4.229617118835449 0.1398462951183319
CurrentTrain: epoch  1, batch     1 | loss: 4.3694634Losses:  8.209087371826172 7.961749076843262 0.24733874201774597
CurrentTrain: epoch  2, batch     0 | loss: 8.2090874Losses:  3.9680511951446533 3.761483907699585 0.20656734704971313
CurrentTrain: epoch  2, batch     1 | loss: 3.9680512Losses:  7.600625514984131 7.483890056610107 0.11673562228679657
CurrentTrain: epoch  3, batch     0 | loss: 7.6006255Losses:  4.914463520050049 4.71449613571167 0.19996735453605652
CurrentTrain: epoch  3, batch     1 | loss: 4.9144635Losses:  6.998419761657715 6.891098976135254 0.10732094198465347
CurrentTrain: epoch  4, batch     0 | loss: 6.9984198Losses:  1.5735297203063965 1.451356291770935 0.1221734806895256
CurrentTrain: epoch  4, batch     1 | loss: 1.5735297Losses:  6.198690414428711 6.108600616455078 0.0900895893573761
CurrentTrain: epoch  5, batch     0 | loss: 6.1986904Losses:  2.3349838256835938 2.247530460357666 0.0874534323811531
CurrentTrain: epoch  5, batch     1 | loss: 2.3349838Losses:  6.152637004852295 6.0786452293396 0.07399195432662964
CurrentTrain: epoch  6, batch     0 | loss: 6.1526370Losses:  2.030069589614868 1.9462342262268066 0.08383547514677048
CurrentTrain: epoch  6, batch     1 | loss: 2.0300696Losses:  5.683282852172852 5.616454601287842 0.0668283998966217
CurrentTrain: epoch  7, batch     0 | loss: 5.6832829Losses:  1.299149751663208 1.241454005241394 0.057695697993040085
CurrentTrain: epoch  7, batch     1 | loss: 1.2991498Losses:  6.0666961669921875 6.004988193511963 0.061708081513643265
CurrentTrain: epoch  8, batch     0 | loss: 6.0666962Losses:  1.7338879108428955 1.6702076196670532 0.06368034332990646
CurrentTrain: epoch  8, batch     1 | loss: 1.7338879Losses:  8.264642715454102 8.20907211303711 0.05557086318731308
CurrentTrain: epoch  9, batch     0 | loss: 8.2646427Losses:  4.1120710372924805 4.027721881866455 0.08434931188821793
CurrentTrain: epoch  9, batch     1 | loss: 4.1120710
Losses:  0.3583123981952667 -0.0 0.3583123981952667
MemoryTrain:  epoch  0, batch     0 | loss: 0.3583124Losses:  0.33910641074180603 -0.0 0.33910641074180603
MemoryTrain:  epoch  1, batch     0 | loss: 0.3391064Losses:  0.32414472103118896 -0.0 0.32414472103118896
MemoryTrain:  epoch  2, batch     0 | loss: 0.3241447Losses:  0.30274146795272827 -0.0 0.30274146795272827
MemoryTrain:  epoch  3, batch     0 | loss: 0.3027415Losses:  0.28918927907943726 -0.0 0.28918927907943726
MemoryTrain:  epoch  4, batch     0 | loss: 0.2891893Losses:  0.2934819459915161 -0.0 0.2934819459915161
MemoryTrain:  epoch  5, batch     0 | loss: 0.2934819Losses:  0.2554469704627991 -0.0 0.2554469704627991
MemoryTrain:  epoch  6, batch     0 | loss: 0.2554470Losses:  0.2513781785964966 -0.0 0.2513781785964966
MemoryTrain:  epoch  7, batch     0 | loss: 0.2513782Losses:  0.24209752678871155 -0.0 0.24209752678871155
MemoryTrain:  epoch  8, batch     0 | loss: 0.2420975Losses:  0.23558259010314941 -0.0 0.23558259010314941
MemoryTrain:  epoch  9, batch     0 | loss: 0.2355826
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 86.06%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 88.97%   [EVAL] batch:   17 | acc: 18.75%,  total acc: 85.07%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 81.82%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 83.17%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 80.42%   [EVAL] batch:   15 | acc: 37.50%,  total acc: 77.73%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 77.21%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 76.39%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 75.66%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 75.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 76.49%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 77.56%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 78.53%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 79.43%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 80.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.01%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 81.48%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 82.76%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 82.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.47%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 83.79%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 84.28%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 84.56%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 84.90%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 85.14%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 85.03%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 84.78%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 83.84%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 83.93%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 84.01%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 84.09%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 84.44%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 84.78%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 85.11%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 85.29%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 85.59%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 85.00%   
cur_acc:  ['0.8485', '0.8507']
his_acc:  ['0.8485', '0.8500']
Clustering into  7  clusters
Clusters:  [1 4 4 1 0 3 1 5 1 2 0 6 1 2 4 1]
Losses:  8.276276588439941 7.719778537750244 0.5564982295036316
CurrentTrain: epoch  0, batch     0 | loss: 8.2762766Losses:  2.6400222778320312 2.189087390899658 0.45093491673469543
CurrentTrain: epoch  0, batch     1 | loss: 2.6400223Losses:  7.369036674499512 6.972745418548584 0.39629122614860535
CurrentTrain: epoch  1, batch     0 | loss: 7.3690367Losses:  3.0270450115203857 2.599374532699585 0.4276703894138336
CurrentTrain: epoch  1, batch     1 | loss: 3.0270450Losses:  7.381773471832275 6.973200798034668 0.40857288241386414
CurrentTrain: epoch  2, batch     0 | loss: 7.3817735Losses:  2.972487211227417 2.5948421955108643 0.3776451051235199
CurrentTrain: epoch  2, batch     1 | loss: 2.9724872Losses:  7.262959957122803 6.932039737701416 0.33092010021209717
CurrentTrain: epoch  3, batch     0 | loss: 7.2629600Losses:  2.0003767013549805 1.7091668844223022 0.291209876537323
CurrentTrain: epoch  3, batch     1 | loss: 2.0003767Losses:  6.478048324584961 6.186298370361328 0.29175013303756714
CurrentTrain: epoch  4, batch     0 | loss: 6.4780483Losses:  2.386962890625 2.072653293609619 0.3143097162246704
CurrentTrain: epoch  4, batch     1 | loss: 2.3869629Losses:  7.498629093170166 7.216599464416504 0.2820296883583069
CurrentTrain: epoch  5, batch     0 | loss: 7.4986291Losses:  2.5881195068359375 2.4383270740509033 0.14979244768619537
CurrentTrain: epoch  5, batch     1 | loss: 2.5881195Losses:  6.95725679397583 6.67488431930542 0.2823725938796997
CurrentTrain: epoch  6, batch     0 | loss: 6.9572568Losses:  3.399111747741699 3.20058536529541 0.1985263228416443
CurrentTrain: epoch  6, batch     1 | loss: 3.3991117Losses:  6.407612323760986 6.144927978515625 0.26268449425697327
CurrentTrain: epoch  7, batch     0 | loss: 6.4076123Losses:  2.696282386779785 2.5247201919555664 0.17156223952770233
CurrentTrain: epoch  7, batch     1 | loss: 2.6962824Losses:  5.918948650360107 5.66541051864624 0.2535380423069
CurrentTrain: epoch  8, batch     0 | loss: 5.9189487Losses:  2.0294103622436523 1.7549840211868286 0.27442628145217896
CurrentTrain: epoch  8, batch     1 | loss: 2.0294104Losses:  6.2724289894104 6.021484375 0.2509446144104004
CurrentTrain: epoch  9, batch     0 | loss: 6.2724290Losses:  1.7954862117767334 1.5417197942733765 0.25376635789871216
CurrentTrain: epoch  9, batch     1 | loss: 1.7954862
Losses:  0.583357572555542 -0.0 0.583357572555542
MemoryTrain:  epoch  0, batch     0 | loss: 0.5833576Losses:  0.5412936210632324 -0.0 0.5412936210632324
MemoryTrain:  epoch  1, batch     0 | loss: 0.5412936Losses:  0.5305103063583374 -0.0 0.5305103063583374
MemoryTrain:  epoch  2, batch     0 | loss: 0.5305103Losses:  0.5320974588394165 -0.0 0.5320974588394165
MemoryTrain:  epoch  3, batch     0 | loss: 0.5320975Losses:  0.5169835090637207 -0.0 0.5169835090637207
MemoryTrain:  epoch  4, batch     0 | loss: 0.5169835Losses:  0.5113863945007324 -0.0 0.5113863945007324
MemoryTrain:  epoch  5, batch     0 | loss: 0.5113864Losses:  0.5078797340393066 -0.0 0.5078797340393066
MemoryTrain:  epoch  6, batch     0 | loss: 0.5078797Losses:  0.5098990797996521 -0.0 0.5098990797996521
MemoryTrain:  epoch  7, batch     0 | loss: 0.5098991Losses:  0.5053225755691528 -0.0 0.5053225755691528
MemoryTrain:  epoch  8, batch     0 | loss: 0.5053226Losses:  0.5070716738700867 -0.0 0.5070716738700867
MemoryTrain:  epoch  9, batch     0 | loss: 0.5070717
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 27.08%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 23.44%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 18.75%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 15.62%   [EVAL] batch:    6 | acc: 12.50%,  total acc: 15.18%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 24.22%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 32.64%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 38.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 42.61%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 44.79%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 48.56%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 49.11%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 60.00%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 54.17%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 58.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 67.36%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 69.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 72.16%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 73.96%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 73.08%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 71.88%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 71.25%   [EVAL] batch:   15 | acc: 37.50%,  total acc: 69.14%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 68.40%   [EVAL] batch:   18 | acc: 43.75%,  total acc: 67.11%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 67.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 70.17%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 71.47%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 73.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 74.76%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 75.46%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 76.34%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 77.16%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 77.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 78.23%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 78.71%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 77.65%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 76.29%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 74.82%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 73.26%   [EVAL] batch:   36 | acc: 25.00%,  total acc: 71.96%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 70.23%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 69.07%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 68.59%   [EVAL] batch:   40 | acc: 18.75%,  total acc: 67.38%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 67.86%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 68.17%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 68.47%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 69.17%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 69.70%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 70.35%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 70.96%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 71.56%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 71.20%   [EVAL] batch:   51 | acc: 12.50%,  total acc: 70.07%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 68.87%   [EVAL] batch:   53 | acc: 6.25%,  total acc: 67.71%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 66.48%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 65.29%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 65.02%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 65.41%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 66.00%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 66.35%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 66.39%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 66.63%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 67.06%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 66.41%   
cur_acc:  ['0.8485', '0.8507', '0.4911']
his_acc:  ['0.8485', '0.8500', '0.6641']
Clustering into  9  clusters
Clusters:  [3 1 1 3 0 5 3 6 3 7 0 8 3 7 1 3 4 2 3 1 0]
Losses:  7.070760726928711 6.658451080322266 0.4123098850250244
CurrentTrain: epoch  0, batch     0 | loss: 7.0707607Losses:  3.2469449043273926 2.7866291999816895 0.46031561493873596
CurrentTrain: epoch  0, batch     1 | loss: 3.2469449Losses:  7.075900077819824 6.658796310424805 0.41710397601127625
CurrentTrain: epoch  1, batch     0 | loss: 7.0759001Losses:  2.110649347305298 1.7175219058990479 0.39312753081321716
CurrentTrain: epoch  1, batch     1 | loss: 2.1106493Losses:  6.697081089019775 6.322836875915527 0.3742443025112152
CurrentTrain: epoch  2, batch     0 | loss: 6.6970811Losses:  3.2921929359436035 2.9962666034698486 0.29592621326446533
CurrentTrain: epoch  2, batch     1 | loss: 3.2921929Losses:  6.177087783813477 5.818399906158447 0.3586881160736084
CurrentTrain: epoch  3, batch     0 | loss: 6.1770878Losses:  2.015829563140869 1.6464496850967407 0.3693799376487732
CurrentTrain: epoch  3, batch     1 | loss: 2.0158296Losses:  6.662448406219482 6.3089399337768555 0.35350847244262695
CurrentTrain: epoch  4, batch     0 | loss: 6.6624484Losses:  1.9585624933242798 1.713504433631897 0.245058074593544
CurrentTrain: epoch  4, batch     1 | loss: 1.9585625Losses:  6.526791095733643 6.156617641448975 0.37017354369163513
CurrentTrain: epoch  5, batch     0 | loss: 6.5267911Losses:  2.8527960777282715 2.5682501792907715 0.28454601764678955
CurrentTrain: epoch  5, batch     1 | loss: 2.8527961Losses:  5.7353057861328125 5.393683433532715 0.3416222035884857
CurrentTrain: epoch  6, batch     0 | loss: 5.7353058Losses:  1.8569436073303223 1.522865891456604 0.3340776562690735
CurrentTrain: epoch  6, batch     1 | loss: 1.8569436Losses:  7.076900482177734 6.744387149810791 0.3325132131576538
CurrentTrain: epoch  7, batch     0 | loss: 7.0769005Losses:  3.2023134231567383 2.971989870071411 0.23032361268997192
CurrentTrain: epoch  7, batch     1 | loss: 3.2023134Losses:  5.359347820281982 5.035843849182129 0.32350394129753113
CurrentTrain: epoch  8, batch     0 | loss: 5.3593478Losses:  2.0384862422943115 1.6986714601516724 0.3398148715496063
CurrentTrain: epoch  8, batch     1 | loss: 2.0384862Losses:  6.610041618347168 6.286255836486816 0.32378560304641724
CurrentTrain: epoch  9, batch     0 | loss: 6.6100416Losses:  3.273731231689453 3.0054268836975098 0.2683044672012329
CurrentTrain: epoch  9, batch     1 | loss: 3.2737312
Losses:  0.7360819578170776 -0.0 0.7360819578170776
MemoryTrain:  epoch  0, batch     0 | loss: 0.7360820Losses:  0.2657011151313782 -0.0 0.2657011151313782
MemoryTrain:  epoch  0, batch     1 | loss: 0.2657011Losses:  0.7070568203926086 -0.0 0.7070568203926086
MemoryTrain:  epoch  1, batch     0 | loss: 0.7070568Losses:  0.19000276923179626 -0.0 0.19000276923179626
MemoryTrain:  epoch  1, batch     1 | loss: 0.1900028Losses:  0.6397226452827454 -0.0 0.6397226452827454
MemoryTrain:  epoch  2, batch     0 | loss: 0.6397226Losses:  0.3253384828567505 -0.0 0.3253384828567505
MemoryTrain:  epoch  2, batch     1 | loss: 0.3253385Losses:  0.549214780330658 -0.0 0.549214780330658
MemoryTrain:  epoch  3, batch     0 | loss: 0.5492148Losses:  0.3305199146270752 -0.0 0.3305199146270752
MemoryTrain:  epoch  3, batch     1 | loss: 0.3305199Losses:  0.42324286699295044 -0.0 0.42324286699295044
MemoryTrain:  epoch  4, batch     0 | loss: 0.4232429Losses:  0.27401280403137207 -0.0 0.27401280403137207
MemoryTrain:  epoch  4, batch     1 | loss: 0.2740128Losses:  0.6167376041412354 -0.0 0.6167376041412354
MemoryTrain:  epoch  5, batch     0 | loss: 0.6167376Losses:  0.31185877323150635 -0.0 0.31185877323150635
MemoryTrain:  epoch  5, batch     1 | loss: 0.3118588Losses:  0.609724760055542 -0.0 0.609724760055542
MemoryTrain:  epoch  6, batch     0 | loss: 0.6097248Losses:  0.349051296710968 -0.0 0.349051296710968
MemoryTrain:  epoch  6, batch     1 | loss: 0.3490513Losses:  0.6855270862579346 -0.0 0.6855270862579346
MemoryTrain:  epoch  7, batch     0 | loss: 0.6855271Losses:  0.15331709384918213 -0.0 0.15331709384918213
MemoryTrain:  epoch  7, batch     1 | loss: 0.1533171Losses:  0.6699380874633789 -0.0 0.6699380874633789
MemoryTrain:  epoch  8, batch     0 | loss: 0.6699381Losses:  0.16427628695964813 -0.0 0.16427628695964813
MemoryTrain:  epoch  8, batch     1 | loss: 0.1642763Losses:  0.5007352232933044 -0.0 0.5007352232933044
MemoryTrain:  epoch  9, batch     0 | loss: 0.5007352Losses:  0.3015972375869751 -0.0 0.3015972375869751
MemoryTrain:  epoch  9, batch     1 | loss: 0.3015972
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 98.44%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 95.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 96.43%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 94.53%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 92.36%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 83.17%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 80.36%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 54.17%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 51.56%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 55.00%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 50.00%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 55.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 60.94%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 64.58%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 66.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 69.89%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 72.40%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 71.15%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 68.30%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 66.67%   [EVAL] batch:   15 | acc: 25.00%,  total acc: 64.06%   [EVAL] batch:   16 | acc: 12.50%,  total acc: 61.03%   [EVAL] batch:   17 | acc: 18.75%,  total acc: 58.68%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 57.24%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 56.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.93%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 60.80%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 65.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 66.83%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 67.82%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 68.97%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 70.04%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 70.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 71.57%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 72.27%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 71.78%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 70.22%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 68.57%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 66.67%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 64.86%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 63.32%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 62.18%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 62.34%   [EVAL] batch:   40 | acc: 18.75%,  total acc: 61.28%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 61.90%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 62.35%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 62.93%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 63.75%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 64.54%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 65.29%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 66.02%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 66.71%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 67.25%   [EVAL] batch:   50 | acc: 25.00%,  total acc: 66.42%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 65.26%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 64.27%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 63.31%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 62.16%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 61.05%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 60.86%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 61.21%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 61.55%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 61.88%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 61.89%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 61.79%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 62.00%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 62.21%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 62.79%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 63.35%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 63.81%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 64.15%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 64.58%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 65.09%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 65.40%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 65.71%   [EVAL] batch:   72 | acc: 31.25%,  total acc: 65.24%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 65.20%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 65.42%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 65.38%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 65.34%   [EVAL] batch:   77 | acc: 6.25%,  total acc: 64.58%   
cur_acc:  ['0.8485', '0.8507', '0.4911', '0.8036']
his_acc:  ['0.8485', '0.8500', '0.6641', '0.6458']
Clustering into  12  clusters
Clusters:  [ 2  9  4 11  1  8  2  5  2  3  1  0  2  3  9  2  6  0  2  4 10  7  4  8
  3  2]
Losses:  10.493802070617676 10.140913009643555 0.35288935899734497
CurrentTrain: epoch  0, batch     0 | loss: 10.4938021Losses:  6.631483554840088 6.386454105377197 0.2450295090675354
CurrentTrain: epoch  0, batch     1 | loss: 6.6314836Losses:  6.181222915649414 5.828569412231445 0.35265326499938965
CurrentTrain: epoch  1, batch     0 | loss: 6.1812229Losses:  1.3508327007293701 0.9963087439537048 0.3545239269733429
CurrentTrain: epoch  1, batch     1 | loss: 1.3508327Losses:  6.904731750488281 6.567239761352539 0.3374921381473541
CurrentTrain: epoch  2, batch     0 | loss: 6.9047318Losses:  2.477888345718384 2.133441925048828 0.3444463610649109
CurrentTrain: epoch  2, batch     1 | loss: 2.4778883Losses:  6.184304237365723 5.859017848968506 0.3252863585948944
CurrentTrain: epoch  3, batch     0 | loss: 6.1843042Losses:  1.7585580348968506 1.42585289478302 0.3327051103115082
CurrentTrain: epoch  3, batch     1 | loss: 1.7585580Losses:  5.857598304748535 5.540658950805664 0.316939115524292
CurrentTrain: epoch  4, batch     0 | loss: 5.8575983Losses:  2.794358730316162 2.5349316596984863 0.25942718982696533
CurrentTrain: epoch  4, batch     1 | loss: 2.7943587Losses:  7.59417724609375 7.276200294494629 0.3179771900177002
CurrentTrain: epoch  5, batch     0 | loss: 7.5941772Losses:  3.1842427253723145 2.9460806846618652 0.2381620705127716
CurrentTrain: epoch  5, batch     1 | loss: 3.1842427Losses:  6.395608901977539 6.080146789550781 0.315462201833725
CurrentTrain: epoch  6, batch     0 | loss: 6.3956089Losses:  2.453472137451172 2.132817029953003 0.32065510749816895
CurrentTrain: epoch  6, batch     1 | loss: 2.4534721Losses:  5.241101264953613 4.93387508392334 0.3072262704372406
CurrentTrain: epoch  7, batch     0 | loss: 5.2411013Losses:  3.0773024559020996 2.700873851776123 0.37642863392829895
CurrentTrain: epoch  7, batch     1 | loss: 3.0773025Losses:  5.72599458694458 5.414161682128906 0.3118329346179962
CurrentTrain: epoch  8, batch     0 | loss: 5.7259946Losses:  2.815411329269409 2.502962827682495 0.3124485909938812
CurrentTrain: epoch  8, batch     1 | loss: 2.8154113Losses:  5.295531272888184 4.991146087646484 0.30438530445098877
CurrentTrain: epoch  9, batch     0 | loss: 5.2955313Losses:  1.1318004131317139 0.8180437088012695 0.31375667452812195
CurrentTrain: epoch  9, batch     1 | loss: 1.1318004
Losses:  0.9171882271766663 -0.0 0.9171882271766663
MemoryTrain:  epoch  0, batch     0 | loss: 0.9171882Losses:  0.6342148184776306 -0.0 0.6342148184776306
MemoryTrain:  epoch  0, batch     1 | loss: 0.6342148Losses:  0.8513444066047668 -0.0 0.8513444066047668
MemoryTrain:  epoch  1, batch     0 | loss: 0.8513444Losses:  0.5851410031318665 -0.0 0.5851410031318665
MemoryTrain:  epoch  1, batch     1 | loss: 0.5851410Losses:  0.8432267308235168 -0.0 0.8432267308235168
MemoryTrain:  epoch  2, batch     0 | loss: 0.8432267Losses:  0.340535432100296 -0.0 0.340535432100296
MemoryTrain:  epoch  2, batch     1 | loss: 0.3405354Losses:  0.6180729866027832 -0.0 0.6180729866027832
MemoryTrain:  epoch  3, batch     0 | loss: 0.6180730Losses:  0.42483723163604736 -0.0 0.42483723163604736
MemoryTrain:  epoch  3, batch     1 | loss: 0.4248372Losses:  0.7113697528839111 -0.0 0.7113697528839111
MemoryTrain:  epoch  4, batch     0 | loss: 0.7113698Losses:  0.5478185415267944 -0.0 0.5478185415267944
MemoryTrain:  epoch  4, batch     1 | loss: 0.5478185Losses:  0.6413191556930542 -0.0 0.6413191556930542
MemoryTrain:  epoch  5, batch     0 | loss: 0.6413192Losses:  0.4518560767173767 -0.0 0.4518560767173767
MemoryTrain:  epoch  5, batch     1 | loss: 0.4518561Losses:  0.6586531400680542 -0.0 0.6586531400680542
MemoryTrain:  epoch  6, batch     0 | loss: 0.6586531Losses:  0.5418060421943665 -0.0 0.5418060421943665
MemoryTrain:  epoch  6, batch     1 | loss: 0.5418060Losses:  0.759157657623291 -0.0 0.759157657623291
MemoryTrain:  epoch  7, batch     0 | loss: 0.7591577Losses:  0.5643937587738037 -0.0 0.5643937587738037
MemoryTrain:  epoch  7, batch     1 | loss: 0.5643938Losses:  0.750404417514801 -0.0 0.750404417514801
MemoryTrain:  epoch  8, batch     0 | loss: 0.7504044Losses:  0.7104208469390869 -0.0 0.7104208469390869
MemoryTrain:  epoch  8, batch     1 | loss: 0.7104208Losses:  0.6066460013389587 -0.0 0.6066460013389587
MemoryTrain:  epoch  9, batch     0 | loss: 0.6066460Losses:  0.5609667897224426 -0.0 0.5609667897224426
MemoryTrain:  epoch  9, batch     1 | loss: 0.5609668
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 39.58%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 45.31%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 51.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 55.21%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 58.93%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 61.72%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 59.03%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 57.50%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 58.52%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 58.33%   [EVAL] batch:   12 | acc: 18.75%,  total acc: 55.29%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 47.92%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 46.88%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 46.25%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 42.71%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 50.89%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 57.03%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 61.11%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 63.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 67.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 69.79%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 70.19%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 69.64%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 70.00%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 69.12%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 68.42%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 69.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 73.10%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 75.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.20%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 76.39%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 77.01%   [EVAL] batch:   28 | acc: 81.25%,  total acc: 77.16%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 77.29%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 77.42%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 77.54%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 76.70%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 74.63%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 72.68%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 70.66%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 68.75%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 66.94%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 65.71%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 65.94%   [EVAL] batch:   40 | acc: 25.00%,  total acc: 64.94%   [EVAL] batch:   41 | acc: 25.00%,  total acc: 63.99%   [EVAL] batch:   42 | acc: 25.00%,  total acc: 63.08%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 62.93%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 63.47%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 64.13%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 64.89%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 66.33%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 66.75%   [EVAL] batch:   50 | acc: 31.25%,  total acc: 66.05%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 64.90%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 63.92%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 62.96%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 61.82%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 60.71%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 60.53%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 60.78%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 61.02%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 61.46%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 61.37%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 61.19%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 61.11%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 61.23%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 61.83%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 62.31%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 61.75%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 61.40%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 61.05%   [EVAL] batch:   69 | acc: 18.75%,  total acc: 60.45%   [EVAL] batch:   70 | acc: 25.00%,  total acc: 59.95%   [EVAL] batch:   71 | acc: 25.00%,  total acc: 59.46%   [EVAL] batch:   72 | acc: 6.25%,  total acc: 58.73%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 58.61%   [EVAL] batch:   74 | acc: 43.75%,  total acc: 58.42%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 58.06%   [EVAL] batch:   76 | acc: 31.25%,  total acc: 57.71%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 57.29%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 56.96%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 56.95%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 57.02%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 57.24%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 57.45%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 57.74%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 58.01%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 57.85%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 57.69%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 57.81%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 57.79%   [EVAL] batch:   89 | acc: 18.75%,  total acc: 57.36%   
cur_acc:  ['0.8485', '0.8507', '0.4911', '0.8036', '0.5529']
his_acc:  ['0.8485', '0.8500', '0.6641', '0.6458', '0.5736']
Clustering into  14  clusters
Clusters:  [ 0  9  4 11  1  7  0 12  0 10  1 13  0 10  9  0  6  8  0  4  5  3  4  7
 10  0  0  3  2  1 10]
Losses:  9.155275344848633 8.695960998535156 0.4593145549297333
CurrentTrain: epoch  0, batch     0 | loss: 9.1552753Losses:  3.1888210773468018 2.8633623123168945 0.325458824634552
CurrentTrain: epoch  0, batch     1 | loss: 3.1888211Losses:  9.058754920959473 8.625873565673828 0.4328811764717102
CurrentTrain: epoch  1, batch     0 | loss: 9.0587549Losses:  3.2004499435424805 2.9411706924438477 0.2592793107032776
CurrentTrain: epoch  1, batch     1 | loss: 3.2004499Losses:  7.095917701721191 6.666558265686035 0.4293592572212219
CurrentTrain: epoch  2, batch     0 | loss: 7.0959177Losses:  3.7674708366394043 3.3106753826141357 0.456795334815979
CurrentTrain: epoch  2, batch     1 | loss: 3.7674708Losses:  8.069650650024414 7.837501049041748 0.2321494072675705
CurrentTrain: epoch  3, batch     0 | loss: 8.0696507Losses:  6.566678047180176 6.380271911621094 0.18640606105327606
CurrentTrain: epoch  3, batch     1 | loss: 6.5666780Losses:  8.545425415039062 8.082886695861816 0.46253862977027893
CurrentTrain: epoch  4, batch     0 | loss: 8.5454254Losses:  2.51224422454834 2.2770869731903076 0.23515713214874268
CurrentTrain: epoch  4, batch     1 | loss: 2.5122442Losses:  7.617151260375977 7.187024116516113 0.43012723326683044
CurrentTrain: epoch  5, batch     0 | loss: 7.6171513Losses:  2.5116591453552246 2.2825961112976074 0.22906306385993958
CurrentTrain: epoch  5, batch     1 | loss: 2.5116591Losses:  7.302243232727051 6.843193531036377 0.4590495228767395
CurrentTrain: epoch  6, batch     0 | loss: 7.3022432Losses:  1.847273588180542 1.6242643594741821 0.22300922870635986
CurrentTrain: epoch  6, batch     1 | loss: 1.8472736Losses:  7.26864767074585 6.815281867980957 0.45336589217185974
CurrentTrain: epoch  7, batch     0 | loss: 7.2686477Losses:  5.425493240356445 5.031222343444824 0.39427071809768677
CurrentTrain: epoch  7, batch     1 | loss: 5.4254932Losses:  6.182770252227783 5.722780227661133 0.4599902331829071
CurrentTrain: epoch  8, batch     0 | loss: 6.1827703Losses:  1.9998317956924438 1.6056163311004639 0.3942154347896576
CurrentTrain: epoch  8, batch     1 | loss: 1.9998318Losses:  5.502968788146973 5.066027641296387 0.43694135546684265
CurrentTrain: epoch  9, batch     0 | loss: 5.5029688Losses:  1.4566078186035156 1.0613083839416504 0.3952994644641876
CurrentTrain: epoch  9, batch     1 | loss: 1.4566078
Losses:  0.731982409954071 -0.0 0.731982409954071
MemoryTrain:  epoch  0, batch     0 | loss: 0.7319824Losses:  0.8065447807312012 -0.0 0.8065447807312012
MemoryTrain:  epoch  0, batch     1 | loss: 0.8065448Losses:  0.9354763031005859 -0.0 0.9354763031005859
MemoryTrain:  epoch  1, batch     0 | loss: 0.9354763Losses:  0.5816484689712524 -0.0 0.5816484689712524
MemoryTrain:  epoch  1, batch     1 | loss: 0.5816485Losses:  0.8355669975280762 -0.0 0.8355669975280762
MemoryTrain:  epoch  2, batch     0 | loss: 0.8355670Losses:  0.5713174939155579 -0.0 0.5713174939155579
MemoryTrain:  epoch  2, batch     1 | loss: 0.5713175Losses:  0.8280364274978638 -0.0 0.8280364274978638
MemoryTrain:  epoch  3, batch     0 | loss: 0.8280364Losses:  0.65302973985672 -0.0 0.65302973985672
MemoryTrain:  epoch  3, batch     1 | loss: 0.6530297Losses:  0.7045648694038391 -0.0 0.7045648694038391
MemoryTrain:  epoch  4, batch     0 | loss: 0.7045649Losses:  0.8027052879333496 -0.0 0.8027052879333496
MemoryTrain:  epoch  4, batch     1 | loss: 0.8027053Losses:  0.8656882047653198 -0.0 0.8656882047653198
MemoryTrain:  epoch  5, batch     0 | loss: 0.8656882Losses:  0.6162289381027222 -0.0 0.6162289381027222
MemoryTrain:  epoch  5, batch     1 | loss: 0.6162289Losses:  0.8256350755691528 -0.0 0.8256350755691528
MemoryTrain:  epoch  6, batch     0 | loss: 0.8256351Losses:  0.6204017400741577 -0.0 0.6204017400741577
MemoryTrain:  epoch  6, batch     1 | loss: 0.6204017Losses:  0.8525160551071167 -0.0 0.8525160551071167
MemoryTrain:  epoch  7, batch     0 | loss: 0.8525161Losses:  0.563146710395813 -0.0 0.563146710395813
MemoryTrain:  epoch  7, batch     1 | loss: 0.5631467Losses:  0.9114899635314941 -0.0 0.9114899635314941
MemoryTrain:  epoch  8, batch     0 | loss: 0.9114900Losses:  0.7322822213172913 -0.0 0.7322822213172913
MemoryTrain:  epoch  8, batch     1 | loss: 0.7322822Losses:  0.9218956232070923 -0.0 0.9218956232070923
MemoryTrain:  epoch  9, batch     0 | loss: 0.9218956Losses:  0.6585558652877808 -0.0 0.6585558652877808
MemoryTrain:  epoch  9, batch     1 | loss: 0.6585559
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 54.17%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 54.69%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 57.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 60.42%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 64.29%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 67.36%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 66.88%   [EVAL] batch:   10 | acc: 0.00%,  total acc: 60.80%   [EVAL] batch:   11 | acc: 6.25%,  total acc: 56.25%   [EVAL] batch:   12 | acc: 6.25%,  total acc: 52.40%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 48.66%   [EVAL] batch:   14 | acc: 6.25%,  total acc: 45.83%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 47.92%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 46.88%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 46.25%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 41.67%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 48.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 54.69%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 59.03%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 61.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 65.34%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 67.71%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 66.83%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 65.62%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 66.25%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 65.23%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 65.81%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 65.46%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 65.94%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 66.96%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 67.90%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 69.29%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 70.31%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 71.50%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 72.12%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 72.45%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:   28 | acc: 75.00%,  total acc: 73.28%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 73.33%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 73.59%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 74.02%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 73.11%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 71.14%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 69.29%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 67.53%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 65.71%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 63.98%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 62.98%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 63.91%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 63.41%   [EVAL] batch:   41 | acc: 18.75%,  total acc: 62.35%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 61.19%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 61.22%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 61.81%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 62.64%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 63.43%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 64.19%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 64.92%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 65.50%   [EVAL] batch:   50 | acc: 12.50%,  total acc: 64.46%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 63.34%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 62.15%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 61.00%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 59.89%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 58.82%   [EVAL] batch:   56 | acc: 25.00%,  total acc: 58.22%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 57.97%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 57.84%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 58.02%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 57.79%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 57.56%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 57.24%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 57.42%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 58.08%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 58.14%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 57.37%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 56.53%   [EVAL] batch:   68 | acc: 0.00%,  total acc: 55.71%   [EVAL] batch:   69 | acc: 6.25%,  total acc: 55.00%   [EVAL] batch:   70 | acc: 0.00%,  total acc: 54.23%   [EVAL] batch:   71 | acc: 0.00%,  total acc: 53.47%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 53.08%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 53.21%   [EVAL] batch:   74 | acc: 31.25%,  total acc: 52.92%   [EVAL] batch:   75 | acc: 18.75%,  total acc: 52.47%   [EVAL] batch:   76 | acc: 43.75%,  total acc: 52.35%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 52.00%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 51.98%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 51.95%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 52.01%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 52.29%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 52.33%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 52.68%   [EVAL] batch:   84 | acc: 87.50%,  total acc: 53.09%   [EVAL] batch:   85 | acc: 93.75%,  total acc: 53.56%   [EVAL] batch:   86 | acc: 81.25%,  total acc: 53.88%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 54.26%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 54.28%   [EVAL] batch:   89 | acc: 37.50%,  total acc: 54.10%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 54.12%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 54.14%   [EVAL] batch:   92 | acc: 62.50%,  total acc: 54.23%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 54.19%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 54.41%   [EVAL] batch:   95 | acc: 68.75%,  total acc: 54.56%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 54.96%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 55.10%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 55.43%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 55.19%   [EVAL] batch:  100 | acc: 6.25%,  total acc: 54.70%   [EVAL] batch:  101 | acc: 6.25%,  total acc: 54.23%   [EVAL] batch:  102 | acc: 0.00%,  total acc: 53.70%   [EVAL] batch:  103 | acc: 6.25%,  total acc: 53.25%   [EVAL] batch:  104 | acc: 0.00%,  total acc: 52.74%   
cur_acc:  ['0.8485', '0.8507', '0.4911', '0.8036', '0.5529', '0.4583']
his_acc:  ['0.8485', '0.8500', '0.6641', '0.6458', '0.5736', '0.5274']
Clustering into  17  clusters
Clusters:  [ 0  9 10  0  1  7  0 11 15  4  1 12  0  4  9 15  6  8  0 10 13  3 16  7
  4 15  0  3 14  1  4  0  5  2  6  2]
Losses:  8.808976173400879 8.4693603515625 0.3396155834197998
CurrentTrain: epoch  0, batch     0 | loss: 8.8089762Losses:  6.348995208740234 6.274916172027588 0.07407902181148529
CurrentTrain: epoch  0, batch     1 | loss: 6.3489952Losses:  7.311786651611328 6.9783244132995605 0.33346232771873474
CurrentTrain: epoch  1, batch     0 | loss: 7.3117867Losses:  1.2946956157684326 1.0108399391174316 0.2838556468486786
CurrentTrain: epoch  1, batch     1 | loss: 1.2946956Losses:  7.565232276916504 7.122817039489746 0.4424152076244354
CurrentTrain: epoch  2, batch     0 | loss: 7.5652323Losses:  4.034882068634033 3.765407085418701 0.2694747745990753
CurrentTrain: epoch  2, batch     1 | loss: 4.0348821Losses:  6.960874080657959 6.691342830657959 0.2695313096046448
CurrentTrain: epoch  3, batch     0 | loss: 6.9608741Losses:  3.1764304637908936 2.882453203201294 0.29397720098495483
CurrentTrain: epoch  3, batch     1 | loss: 3.1764305Losses:  8.804076194763184 8.417274475097656 0.3868016004562378
CurrentTrain: epoch  4, batch     0 | loss: 8.8040762Losses:  3.2933309078216553 3.0440821647644043 0.24924874305725098
CurrentTrain: epoch  4, batch     1 | loss: 3.2933309Losses:  6.56695032119751 6.302134037017822 0.2648163139820099
CurrentTrain: epoch  5, batch     0 | loss: 6.5669503Losses:  4.1983561515808105 3.76778244972229 0.43057388067245483
CurrentTrain: epoch  5, batch     1 | loss: 4.1983562Losses:  5.994621276855469 5.735187530517578 0.2594338655471802
CurrentTrain: epoch  6, batch     0 | loss: 5.9946213Losses:  2.802311420440674 2.3680262565612793 0.4342852234840393
CurrentTrain: epoch  6, batch     1 | loss: 2.8023114Losses:  7.204967498779297 6.959146976470947 0.245820552110672
CurrentTrain: epoch  7, batch     0 | loss: 7.2049675Losses:  4.329606056213379 3.8631014823913574 0.46650445461273193
CurrentTrain: epoch  7, batch     1 | loss: 4.3296061Losses:  6.29600191116333 5.942885398864746 0.35311633348464966
CurrentTrain: epoch  8, batch     0 | loss: 6.2960019Losses:  1.7183949947357178 1.4560670852661133 0.2623279094696045
CurrentTrain: epoch  8, batch     1 | loss: 1.7183950Losses:  6.081514358520508 5.756828784942627 0.3246854543685913
CurrentTrain: epoch  9, batch     0 | loss: 6.0815144Losses:  2.6807775497436523 2.3730599880218506 0.3077176511287689
CurrentTrain: epoch  9, batch     1 | loss: 2.6807775
Losses:  0.9832314848899841 -0.0 0.9832314848899841
MemoryTrain:  epoch  0, batch     0 | loss: 0.9832315Losses:  0.8556546568870544 -0.0 0.8556546568870544
MemoryTrain:  epoch  0, batch     1 | loss: 0.8556547Losses:  0.28205814957618713 -0.0 0.28205814957618713
MemoryTrain:  epoch  0, batch     2 | loss: 0.2820581Losses:  1.036843180656433 -0.0 1.036843180656433
MemoryTrain:  epoch  1, batch     0 | loss: 1.0368432Losses:  0.9599242210388184 -0.0 0.9599242210388184
MemoryTrain:  epoch  1, batch     1 | loss: 0.9599242Losses:  0.2886006236076355 -0.0 0.2886006236076355
MemoryTrain:  epoch  1, batch     2 | loss: 0.2886006Losses:  0.8698781132698059 -0.0 0.8698781132698059
MemoryTrain:  epoch  2, batch     0 | loss: 0.8698781Losses:  0.9958646297454834 -0.0 0.9958646297454834
MemoryTrain:  epoch  2, batch     1 | loss: 0.9958646Losses:  0.49829259514808655 -0.0 0.49829259514808655
MemoryTrain:  epoch  2, batch     2 | loss: 0.4982926Losses:  1.0955021381378174 -0.0 1.0955021381378174
MemoryTrain:  epoch  3, batch     0 | loss: 1.0955021Losses:  0.8242033123970032 -0.0 0.8242033123970032
MemoryTrain:  epoch  3, batch     1 | loss: 0.8242033Losses:  0.1827845275402069 -0.0 0.1827845275402069
MemoryTrain:  epoch  3, batch     2 | loss: 0.1827845Losses:  0.9088495969772339 -0.0 0.9088495969772339
MemoryTrain:  epoch  4, batch     0 | loss: 0.9088496Losses:  0.8762750625610352 -0.0 0.8762750625610352
MemoryTrain:  epoch  4, batch     1 | loss: 0.8762751Losses:  0.25988703966140747 -0.0 0.25988703966140747
MemoryTrain:  epoch  4, batch     2 | loss: 0.2598870Losses:  0.8694552779197693 -0.0 0.8694552779197693
MemoryTrain:  epoch  5, batch     0 | loss: 0.8694553Losses:  0.9180580973625183 -0.0 0.9180580973625183
MemoryTrain:  epoch  5, batch     1 | loss: 0.9180581Losses:  0.26122042536735535 -0.0 0.26122042536735535
MemoryTrain:  epoch  5, batch     2 | loss: 0.2612204Losses:  0.9246280193328857 -0.0 0.9246280193328857
MemoryTrain:  epoch  6, batch     0 | loss: 0.9246280Losses:  0.8632718920707703 -0.0 0.8632718920707703
MemoryTrain:  epoch  6, batch     1 | loss: 0.8632719Losses:  0.45876628160476685 -0.0 0.45876628160476685
MemoryTrain:  epoch  6, batch     2 | loss: 0.4587663Losses:  0.8669993877410889 -0.0 0.8669993877410889
MemoryTrain:  epoch  7, batch     0 | loss: 0.8669994Losses:  0.8115941882133484 -0.0 0.8115941882133484
MemoryTrain:  epoch  7, batch     1 | loss: 0.8115942Losses:  0.28809401392936707 -0.0 0.28809401392936707
MemoryTrain:  epoch  7, batch     2 | loss: 0.2880940Losses:  0.8254054188728333 -0.0 0.8254054188728333
MemoryTrain:  epoch  8, batch     0 | loss: 0.8254054Losses:  0.8616923093795776 -0.0 0.8616923093795776
MemoryTrain:  epoch  8, batch     1 | loss: 0.8616923Losses:  0.27666226029396057 -0.0 0.27666226029396057
MemoryTrain:  epoch  8, batch     2 | loss: 0.2766623Losses:  0.9845334887504578 -0.0 0.9845334887504578
MemoryTrain:  epoch  9, batch     0 | loss: 0.9845335Losses:  0.8027702569961548 -0.0 0.8027702569961548
MemoryTrain:  epoch  9, batch     1 | loss: 0.8027703Losses:  0.26781564950942993 -0.0 0.26781564950942993
MemoryTrain:  epoch  9, batch     2 | loss: 0.2678156
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 40.62%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 35.00%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 30.21%   [EVAL] batch:    6 | acc: 6.25%,  total acc: 26.79%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 23.44%   
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 9.38%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 8.33%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 9.38%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 11.25%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 11.46%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 18.75%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 23.44%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 27.78%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 31.87%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 35.23%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 38.54%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 39.90%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 40.62%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 42.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 43.75%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 45.59%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 46.53%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 47.37%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 49.06%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 50.89%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 51.42%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 52.99%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 54.43%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 56.00%   [EVAL] batch:   25 | acc: 81.25%,  total acc: 56.97%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 57.41%   [EVAL] batch:   27 | acc: 68.75%,  total acc: 57.81%   [EVAL] batch:   28 | acc: 68.75%,  total acc: 58.19%   [EVAL] batch:   29 | acc: 43.75%,  total acc: 57.71%   [EVAL] batch:   30 | acc: 62.50%,  total acc: 57.86%   [EVAL] batch:   31 | acc: 50.00%,  total acc: 57.62%   [EVAL] batch:   32 | acc: 31.25%,  total acc: 56.82%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 55.33%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 53.75%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 52.26%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 50.84%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 49.51%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 48.72%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 50.00%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 49.70%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 49.55%   [EVAL] batch:   42 | acc: 37.50%,  total acc: 49.27%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 49.43%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 50.00%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 50.68%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 51.46%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 52.34%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 53.19%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 53.87%   [EVAL] batch:   50 | acc: 18.75%,  total acc: 53.19%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 52.28%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 51.42%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 50.69%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 49.77%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 48.88%   [EVAL] batch:   56 | acc: 25.00%,  total acc: 48.46%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 47.95%   [EVAL] batch:   58 | acc: 25.00%,  total acc: 47.56%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 47.40%   [EVAL] batch:   60 | acc: 25.00%,  total acc: 47.03%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 46.67%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 46.23%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 46.39%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 46.73%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 46.97%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 47.48%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 47.61%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 47.83%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 48.39%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 48.77%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 48.96%   [EVAL] batch:   72 | acc: 43.75%,  total acc: 48.89%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 49.07%   [EVAL] batch:   74 | acc: 43.75%,  total acc: 49.00%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 48.77%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 48.78%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 48.64%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 48.81%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 48.98%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 49.07%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 49.16%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 49.02%   [EVAL] batch:   83 | acc: 43.75%,  total acc: 48.96%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 49.12%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 49.42%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 49.50%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 49.86%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 49.93%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 50.21%   [EVAL] batch:   90 | acc: 12.50%,  total acc: 49.79%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 49.80%   [EVAL] batch:   92 | acc: 50.00%,  total acc: 49.80%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 49.53%   [EVAL] batch:   94 | acc: 62.50%,  total acc: 49.67%   [EVAL] batch:   95 | acc: 62.50%,  total acc: 49.80%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 50.19%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 50.19%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 50.51%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 50.31%   [EVAL] batch:  100 | acc: 6.25%,  total acc: 49.88%   [EVAL] batch:  101 | acc: 0.00%,  total acc: 49.39%   [EVAL] batch:  102 | acc: 0.00%,  total acc: 48.91%   [EVAL] batch:  103 | acc: 0.00%,  total acc: 48.44%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 48.57%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 48.82%   [EVAL] batch:  106 | acc: 12.50%,  total acc: 48.48%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 48.15%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 47.82%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 47.44%   [EVAL] batch:  110 | acc: 6.25%,  total acc: 47.07%   [EVAL] batch:  111 | acc: 0.00%,  total acc: 46.65%   
cur_acc:  ['0.8485', '0.8507', '0.4911', '0.8036', '0.5529', '0.4583', '0.2344']
his_acc:  ['0.8485', '0.8500', '0.6641', '0.6458', '0.5736', '0.5274', '0.4665']
Clustering into  19  clusters
Clusters:  [ 0  4  5  0  2 11  0 15  7  1  2 13  0  1  4  7  6 17  0  5 12  3 14 11
  1  7  0  3  9 18  1  0 10  5  6 16  5  5  8 11  1]
Losses:  8.174589157104492 7.677384376525879 0.49720442295074463
CurrentTrain: epoch  0, batch     0 | loss: 8.1745892Losses:  2.7317938804626465 2.227975606918335 0.5038183927536011
CurrentTrain: epoch  0, batch     1 | loss: 2.7317939Losses:  7.586050987243652 7.0865254402160645 0.49952536821365356
CurrentTrain: epoch  1, batch     0 | loss: 7.5860510Losses:  3.0268566608428955 2.527515411376953 0.49934133887290955
CurrentTrain: epoch  1, batch     1 | loss: 3.0268567Losses:  6.725546836853027 6.228206157684326 0.4973408579826355
CurrentTrain: epoch  2, batch     0 | loss: 6.7255468Losses:  2.4715209007263184 2.011342763900757 0.4601781368255615
CurrentTrain: epoch  2, batch     1 | loss: 2.4715209Losses:  7.534145832061768 7.053393363952637 0.4807524085044861
CurrentTrain: epoch  3, batch     0 | loss: 7.5341458Losses:  2.40061616897583 1.939481496810913 0.46113455295562744
CurrentTrain: epoch  3, batch     1 | loss: 2.4006162Losses:  6.861172676086426 6.399122714996338 0.46204978227615356
CurrentTrain: epoch  4, batch     0 | loss: 6.8611727Losses:  2.869900703430176 2.4146249294281006 0.4552757143974304
CurrentTrain: epoch  4, batch     1 | loss: 2.8699007Losses:  8.315407752990723 7.853734016418457 0.46167388558387756
CurrentTrain: epoch  5, batch     0 | loss: 8.3154078Losses:  2.437849283218384 1.987741470336914 0.4501078724861145
CurrentTrain: epoch  5, batch     1 | loss: 2.4378493Losses:  6.8622236251831055 6.408304691314697 0.4539187550544739
CurrentTrain: epoch  6, batch     0 | loss: 6.8622236Losses:  1.827423334121704 1.3854764699935913 0.4419468641281128
CurrentTrain: epoch  6, batch     1 | loss: 1.8274233Losses:  7.318367004394531 6.870383262634277 0.4479837417602539
CurrentTrain: epoch  7, batch     0 | loss: 7.3183670Losses:  2.239159345626831 1.8044403791427612 0.4347189664840698
CurrentTrain: epoch  7, batch     1 | loss: 2.2391593Losses:  6.707012176513672 6.263179779052734 0.4438323974609375
CurrentTrain: epoch  8, batch     0 | loss: 6.7070122Losses:  2.768078565597534 2.3266987800598145 0.44137972593307495
CurrentTrain: epoch  8, batch     1 | loss: 2.7680786Losses:  7.273088455200195 6.830471992492676 0.44261661171913147
CurrentTrain: epoch  9, batch     0 | loss: 7.2730885Losses:  2.1800615787506104 1.744911789894104 0.4351496994495392
CurrentTrain: epoch  9, batch     1 | loss: 2.1800616
Losses:  0.9160141348838806 -0.0 0.9160141348838806
MemoryTrain:  epoch  0, batch     0 | loss: 0.9160141Losses:  0.8155979514122009 -0.0 0.8155979514122009
MemoryTrain:  epoch  0, batch     1 | loss: 0.8155980Losses:  0.6238507032394409 -0.0 0.6238507032394409
MemoryTrain:  epoch  0, batch     2 | loss: 0.6238507Losses:  0.9821051955223083 -0.0 0.9821051955223083
MemoryTrain:  epoch  1, batch     0 | loss: 0.9821052Losses:  0.7521169781684875 -0.0 0.7521169781684875
MemoryTrain:  epoch  1, batch     1 | loss: 0.7521170Losses:  0.5178982019424438 -0.0 0.5178982019424438
MemoryTrain:  epoch  1, batch     2 | loss: 0.5178982Losses:  0.7934085130691528 -0.0 0.7934085130691528
MemoryTrain:  epoch  2, batch     0 | loss: 0.7934085Losses:  0.8856462240219116 -0.0 0.8856462240219116
MemoryTrain:  epoch  2, batch     1 | loss: 0.8856462Losses:  0.7097365260124207 -0.0 0.7097365260124207
MemoryTrain:  epoch  2, batch     2 | loss: 0.7097365Losses:  1.0073093175888062 -0.0 1.0073093175888062
MemoryTrain:  epoch  3, batch     0 | loss: 1.0073093Losses:  0.9124069213867188 -0.0 0.9124069213867188
MemoryTrain:  epoch  3, batch     1 | loss: 0.9124069Losses:  0.3450276255607605 -0.0 0.3450276255607605
MemoryTrain:  epoch  3, batch     2 | loss: 0.3450276Losses:  0.865571916103363 -0.0 0.865571916103363
MemoryTrain:  epoch  4, batch     0 | loss: 0.8655719Losses:  0.9520635604858398 -0.0 0.9520635604858398
MemoryTrain:  epoch  4, batch     1 | loss: 0.9520636Losses:  0.5674504637718201 -0.0 0.5674504637718201
MemoryTrain:  epoch  4, batch     2 | loss: 0.5674505Losses:  0.8591155409812927 -0.0 0.8591155409812927
MemoryTrain:  epoch  5, batch     0 | loss: 0.8591155Losses:  0.7196545004844666 -0.0 0.7196545004844666
MemoryTrain:  epoch  5, batch     1 | loss: 0.7196545Losses:  0.6219760179519653 -0.0 0.6219760179519653
MemoryTrain:  epoch  5, batch     2 | loss: 0.6219760Losses:  0.8494588136672974 -0.0 0.8494588136672974
MemoryTrain:  epoch  6, batch     0 | loss: 0.8494588Losses:  0.8018386960029602 -0.0 0.8018386960029602
MemoryTrain:  epoch  6, batch     1 | loss: 0.8018387Losses:  0.6155751347541809 -0.0 0.6155751347541809
MemoryTrain:  epoch  6, batch     2 | loss: 0.6155751Losses:  0.8820235133171082 -0.0 0.8820235133171082
MemoryTrain:  epoch  7, batch     0 | loss: 0.8820235Losses:  0.7668926119804382 -0.0 0.7668926119804382
MemoryTrain:  epoch  7, batch     1 | loss: 0.7668926Losses:  0.7434285283088684 -0.0 0.7434285283088684
MemoryTrain:  epoch  7, batch     2 | loss: 0.7434285Losses:  0.6911791563034058 -0.0 0.6911791563034058
MemoryTrain:  epoch  8, batch     0 | loss: 0.6911792Losses:  0.9506880044937134 -0.0 0.9506880044937134
MemoryTrain:  epoch  8, batch     1 | loss: 0.9506880Losses:  0.5866453051567078 -0.0 0.5866453051567078
MemoryTrain:  epoch  8, batch     2 | loss: 0.5866453Losses:  0.8320857286453247 -0.0 0.8320857286453247
MemoryTrain:  epoch  9, batch     0 | loss: 0.8320857Losses:  0.9473561644554138 -0.0 0.9473561644554138
MemoryTrain:  epoch  9, batch     1 | loss: 0.9473562Losses:  0.5458864569664001 -0.0 0.5458864569664001
MemoryTrain:  epoch  9, batch     2 | loss: 0.5458865
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 32.81%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 29.17%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 27.68%   [EVAL] batch:    7 | acc: 31.25%,  total acc: 28.12%   [EVAL] batch:    8 | acc: 0.00%,  total acc: 25.00%   [EVAL] batch:    9 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:   10 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:   11 | acc: 18.75%,  total acc: 24.48%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 23.56%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 25.00%   [EVAL] batch:   14 | acc: 18.75%,  total acc: 24.58%   [EVAL] batch:   15 | acc: 12.50%,  total acc: 23.83%   [EVAL] batch:   16 | acc: 12.50%,  total acc: 23.16%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 23.26%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 24.01%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 25.94%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 27.68%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 28.12%   
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 47.92%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 43.75%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 45.00%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 44.79%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 46.43%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 48.44%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 50.69%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 52.50%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 53.41%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 56.77%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 54.91%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 56.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 57.35%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 57.64%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 57.89%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 59.38%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 61.01%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 64.13%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 65.36%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 66.75%   [EVAL] batch:   25 | acc: 81.25%,  total acc: 67.31%   [EVAL] batch:   26 | acc: 62.50%,  total acc: 67.13%   [EVAL] batch:   27 | acc: 68.75%,  total acc: 67.19%   [EVAL] batch:   28 | acc: 75.00%,  total acc: 67.46%   [EVAL] batch:   29 | acc: 56.25%,  total acc: 67.08%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 67.14%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 66.99%   [EVAL] batch:   32 | acc: 31.25%,  total acc: 65.91%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 63.97%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 62.14%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 60.42%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 58.78%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 57.24%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 56.25%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 57.34%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 56.86%   [EVAL] batch:   41 | acc: 6.25%,  total acc: 55.65%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 54.36%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 54.40%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 55.14%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 55.98%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 56.91%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 57.81%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 58.67%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 59.25%   [EVAL] batch:   50 | acc: 50.00%,  total acc: 59.07%   [EVAL] batch:   51 | acc: 0.00%,  total acc: 57.93%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 56.84%   [EVAL] batch:   53 | acc: 6.25%,  total acc: 55.90%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 54.89%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 53.91%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 53.51%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 53.23%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 52.97%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 53.12%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 52.97%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 52.62%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 52.08%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 52.15%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 52.40%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 52.65%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 53.26%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 53.77%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 54.44%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 55.09%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 55.72%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 55.90%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 55.48%   [EVAL] batch:   73 | acc: 31.25%,  total acc: 55.15%   [EVAL] batch:   74 | acc: 31.25%,  total acc: 54.83%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 54.52%   [EVAL] batch:   76 | acc: 43.75%,  total acc: 54.38%   [EVAL] batch:   77 | acc: 31.25%,  total acc: 54.09%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 54.11%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 54.22%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 54.17%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 54.19%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 54.14%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 54.32%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 54.19%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 53.92%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 53.59%   [EVAL] batch:   87 | acc: 31.25%,  total acc: 53.34%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 53.37%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 53.40%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 52.82%   [EVAL] batch:   91 | acc: 0.00%,  total acc: 52.24%   [EVAL] batch:   92 | acc: 0.00%,  total acc: 51.68%   [EVAL] batch:   93 | acc: 0.00%,  total acc: 51.13%   [EVAL] batch:   94 | acc: 18.75%,  total acc: 50.79%   [EVAL] batch:   95 | acc: 62.50%,  total acc: 50.91%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 51.29%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 51.28%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 51.70%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 51.50%   [EVAL] batch:  100 | acc: 0.00%,  total acc: 50.99%   [EVAL] batch:  101 | acc: 0.00%,  total acc: 50.49%   [EVAL] batch:  102 | acc: 0.00%,  total acc: 50.00%   [EVAL] batch:  103 | acc: 0.00%,  total acc: 49.52%   [EVAL] batch:  104 | acc: 43.75%,  total acc: 49.46%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 49.82%   [EVAL] batch:  106 | acc: 12.50%,  total acc: 49.47%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 49.07%   [EVAL] batch:  108 | acc: 0.00%,  total acc: 48.62%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 48.24%   [EVAL] batch:  110 | acc: 0.00%,  total acc: 47.80%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 47.54%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 47.57%   [EVAL] batch:  113 | acc: 37.50%,  total acc: 47.48%   [EVAL] batch:  114 | acc: 12.50%,  total acc: 47.17%   [EVAL] batch:  115 | acc: 31.25%,  total acc: 47.04%   [EVAL] batch:  116 | acc: 25.00%,  total acc: 46.85%   [EVAL] batch:  117 | acc: 18.75%,  total acc: 46.61%   [EVAL] batch:  118 | acc: 18.75%,  total acc: 46.38%   [EVAL] batch:  119 | acc: 12.50%,  total acc: 46.09%   [EVAL] batch:  120 | acc: 25.00%,  total acc: 45.92%   [EVAL] batch:  121 | acc: 25.00%,  total acc: 45.75%   [EVAL] batch:  122 | acc: 18.75%,  total acc: 45.53%   [EVAL] batch:  123 | acc: 12.50%,  total acc: 45.26%   [EVAL] batch:  124 | acc: 43.75%,  total acc: 45.25%   [EVAL] batch:  125 | acc: 12.50%,  total acc: 44.99%   [EVAL] batch:  126 | acc: 18.75%,  total acc: 44.78%   [EVAL] batch:  127 | acc: 12.50%,  total acc: 44.53%   [EVAL] batch:  128 | acc: 12.50%,  total acc: 44.28%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 44.28%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 44.42%   [EVAL] batch:  131 | acc: 50.00%,  total acc: 44.46%   [EVAL] batch:  132 | acc: 56.25%,  total acc: 44.55%   
cur_acc:  ['0.8485', '0.8507', '0.4911', '0.8036', '0.5529', '0.4583', '0.2344', '0.2812']
his_acc:  ['0.8485', '0.8500', '0.6641', '0.6458', '0.5736', '0.5274', '0.4665', '0.4455']
--------Round  3
seed:  400
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 0 1 2 5 3 4 6]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 1 1 0]
Losses:  13.30769157409668 12.6585693359375 0.6491219997406006
CurrentTrain: epoch  0, batch     0 | loss: 13.3076916Losses:  12.269929885864258 11.647615432739258 0.6223143339157104
CurrentTrain: epoch  0, batch     1 | loss: 12.2699299Losses:  8.104663848876953 7.464748382568359 0.639915943145752
CurrentTrain: epoch  0, batch     2 | loss: 8.1046638Losses:  9.661941528320312 9.028562545776367 0.6333790421485901
CurrentTrain: epoch  0, batch     3 | loss: 9.6619415Losses:  12.243693351745605 11.604108810424805 0.6395842432975769
CurrentTrain: epoch  0, batch     4 | loss: 12.2436934Losses:  10.016448974609375 9.383305549621582 0.6331439018249512
CurrentTrain: epoch  0, batch     5 | loss: 10.0164490Losses:  9.227935791015625 8.609437942504883 0.6184976100921631
CurrentTrain: epoch  0, batch     6 | loss: 9.2279358Losses:  9.567514419555664 8.956304550170898 0.6112098097801208
CurrentTrain: epoch  0, batch     7 | loss: 9.5675144Losses:  13.156340599060059 12.550336837768555 0.6060041189193726
CurrentTrain: epoch  0, batch     8 | loss: 13.1563406Losses:  7.537904739379883 6.940479278564453 0.5974255800247192
CurrentTrain: epoch  0, batch     9 | loss: 7.5379047Losses:  10.198474884033203 9.595657348632812 0.6028177738189697
CurrentTrain: epoch  0, batch    10 | loss: 10.1984749Losses:  10.87177848815918 10.27138900756836 0.6003895998001099
CurrentTrain: epoch  0, batch    11 | loss: 10.8717785Losses:  10.301219940185547 9.731024742126465 0.5701956748962402
CurrentTrain: epoch  0, batch    12 | loss: 10.3012199Losses:  8.091105461120605 7.492297172546387 0.5988086462020874
CurrentTrain: epoch  0, batch    13 | loss: 8.0911055Losses:  9.251090049743652 8.681861877441406 0.5692278146743774
CurrentTrain: epoch  0, batch    14 | loss: 9.2510900Losses:  8.965717315673828 8.402545928955078 0.5631709098815918
CurrentTrain: epoch  0, batch    15 | loss: 8.9657173Losses:  12.957083702087402 12.414392471313477 0.542691171169281
CurrentTrain: epoch  0, batch    16 | loss: 12.9570837Losses:  8.705371856689453 8.150667190551758 0.5547041893005371
CurrentTrain: epoch  0, batch    17 | loss: 8.7053719Losses:  8.766998291015625 8.234879493713379 0.5321192145347595
CurrentTrain: epoch  0, batch    18 | loss: 8.7669983Losses:  10.88134765625 10.3859224319458 0.49542516469955444
CurrentTrain: epoch  0, batch    19 | loss: 10.8813477Losses:  8.579645156860352 8.087913513183594 0.49173155426979065
CurrentTrain: epoch  0, batch    20 | loss: 8.5796452Losses:  12.625225067138672 12.148752212524414 0.4764728546142578
CurrentTrain: epoch  0, batch    21 | loss: 12.6252251Losses:  9.1383695602417 8.685224533081055 0.45314472913742065
CurrentTrain: epoch  0, batch    22 | loss: 9.1383696Losses:  8.582107543945312 8.09518814086914 0.4869196116924286
CurrentTrain: epoch  0, batch    23 | loss: 8.5821075Losses:  8.442511558532715 8.043116569519043 0.3993951082229614
CurrentTrain: epoch  0, batch    24 | loss: 8.4425116Losses:  10.401822090148926 9.979724884033203 0.4220968782901764
CurrentTrain: epoch  0, batch    25 | loss: 10.4018221Losses:  8.308561325073242 7.878303527832031 0.4302576184272766
CurrentTrain: epoch  0, batch    26 | loss: 8.3085613Losses:  10.49734115600586 10.172910690307617 0.3244307041168213
CurrentTrain: epoch  0, batch    27 | loss: 10.4973412Losses:  12.300378799438477 11.941133499145508 0.3592453598976135
CurrentTrain: epoch  0, batch    28 | loss: 12.3003788Losses:  8.02906608581543 7.728051662445068 0.30101442337036133
CurrentTrain: epoch  0, batch    29 | loss: 8.0290661Losses:  9.08988094329834 8.793479919433594 0.29640138149261475
CurrentTrain: epoch  0, batch    30 | loss: 9.0898809Losses:  6.597902774810791 6.259929180145264 0.33797338604927063
CurrentTrain: epoch  0, batch    31 | loss: 6.5979028Losses:  9.459604263305664 9.23849868774414 0.22110538184642792
CurrentTrain: epoch  0, batch    32 | loss: 9.4596043Losses:  14.302261352539062 14.030370712280273 0.27189111709594727
CurrentTrain: epoch  0, batch    33 | loss: 14.3022614Losses:  6.3342413902282715 6.0749831199646 0.2592583894729614
CurrentTrain: epoch  0, batch    34 | loss: 6.3342414Losses:  7.093761920928955 6.886672019958496 0.2070898711681366
CurrentTrain: epoch  0, batch    35 | loss: 7.0937619Losses:  7.689783096313477 7.479328632354736 0.21045437455177307
CurrentTrain: epoch  0, batch    36 | loss: 7.6897831Losses:  2.811103582382202 2.627411365509033 0.1836923062801361
CurrentTrain: epoch  0, batch    37 | loss: 2.8111036Losses:  8.827484130859375 8.640969276428223 0.18651452660560608
CurrentTrain: epoch  1, batch     0 | loss: 8.8274841Losses:  7.654452323913574 7.398923873901367 0.25552845001220703
CurrentTrain: epoch  1, batch     1 | loss: 7.6544523Losses:  16.745159149169922 16.569469451904297 0.17568880319595337
CurrentTrain: epoch  1, batch     2 | loss: 16.7451591Losses:  7.395086288452148 7.244636535644531 0.15044961869716644
CurrentTrain: epoch  1, batch     3 | loss: 7.3950863Losses:  6.618255615234375 6.481447696685791 0.13680794835090637
CurrentTrain: epoch  1, batch     4 | loss: 6.6182556Losses:  9.303181648254395 9.16502571105957 0.13815607130527496
CurrentTrain: epoch  1, batch     5 | loss: 9.3031816Losses:  12.054064750671387 11.903696060180664 0.1503688395023346
CurrentTrain: epoch  1, batch     6 | loss: 12.0540648Losses:  7.575894355773926 7.450542449951172 0.12535187602043152
CurrentTrain: epoch  1, batch     7 | loss: 7.5758944Losses:  8.446534156799316 8.334430694580078 0.11210349202156067
CurrentTrain: epoch  1, batch     8 | loss: 8.4465342Losses:  11.484914779663086 11.374305725097656 0.11060909926891327
CurrentTrain: epoch  1, batch     9 | loss: 11.4849148Losses:  8.667336463928223 8.565147399902344 0.10218870639801025
CurrentTrain: epoch  1, batch    10 | loss: 8.6673365Losses:  7.966826915740967 7.86605978012085 0.10076721012592316
CurrentTrain: epoch  1, batch    11 | loss: 7.9668269Losses:  8.156487464904785 8.019329071044922 0.13715852797031403
CurrentTrain: epoch  1, batch    12 | loss: 8.1564875Losses:  6.262340068817139 6.185589790344238 0.07675039768218994
CurrentTrain: epoch  1, batch    13 | loss: 6.2623401Losses:  7.188113689422607 7.086669921875 0.10144374519586563
CurrentTrain: epoch  1, batch    14 | loss: 7.1881137Losses:  5.866199493408203 5.775318622589111 0.0908811017870903
CurrentTrain: epoch  1, batch    15 | loss: 5.8661995Losses:  6.533401966094971 6.448932647705078 0.08446938544511795
CurrentTrain: epoch  1, batch    16 | loss: 6.5334020Losses:  7.565817832946777 7.4908671379089355 0.0749509334564209
CurrentTrain: epoch  1, batch    17 | loss: 7.5658178Losses:  9.249789237976074 9.16360855102539 0.08618023991584778
CurrentTrain: epoch  1, batch    18 | loss: 9.2497892Losses:  7.545698165893555 7.474541187286377 0.07115687429904938
CurrentTrain: epoch  1, batch    19 | loss: 7.5456982Losses:  6.693136215209961 6.624166488647461 0.06896958500146866
CurrentTrain: epoch  1, batch    20 | loss: 6.6931362Losses:  6.151296615600586 6.076365947723389 0.07493053376674652
CurrentTrain: epoch  1, batch    21 | loss: 6.1512966Losses:  6.187132835388184 6.12367582321167 0.06345689296722412
CurrentTrain: epoch  1, batch    22 | loss: 6.1871328Losses:  6.529754161834717 6.466945648193359 0.06280839443206787
CurrentTrain: epoch  1, batch    23 | loss: 6.5297542Losses:  5.858652591705322 5.791622638702393 0.06703008711338043
CurrentTrain: epoch  1, batch    24 | loss: 5.8586526Losses:  7.21927547454834 7.144994735717773 0.07428096234798431
CurrentTrain: epoch  1, batch    25 | loss: 7.2192755Losses:  5.374753952026367 5.316641807556152 0.05811212584376335
CurrentTrain: epoch  1, batch    26 | loss: 5.3747540Losses:  8.08337688446045 7.906914710998535 0.17646221816539764
CurrentTrain: epoch  1, batch    27 | loss: 8.0833769Losses:  8.535199165344238 8.472213745117188 0.06298559904098511
CurrentTrain: epoch  1, batch    28 | loss: 8.5351992Losses:  5.85535192489624 5.798855781555176 0.05649612471461296
CurrentTrain: epoch  1, batch    29 | loss: 5.8553519Losses:  8.491890907287598 8.429410934448242 0.062479760497808456
CurrentTrain: epoch  1, batch    30 | loss: 8.4918909Losses:  6.213084697723389 6.158246994018555 0.05483756214380264
CurrentTrain: epoch  1, batch    31 | loss: 6.2130847Losses:  7.660547256469727 7.603857040405273 0.05669022351503372
CurrentTrain: epoch  1, batch    32 | loss: 7.6605473Losses:  4.887598514556885 4.82908821105957 0.05851033702492714
CurrentTrain: epoch  1, batch    33 | loss: 4.8875985Losses:  5.904174327850342 5.838733196258545 0.06544095277786255
CurrentTrain: epoch  1, batch    34 | loss: 5.9041743Losses:  4.990485668182373 4.936575412750244 0.05391036719083786
CurrentTrain: epoch  1, batch    35 | loss: 4.9904857Losses:  6.5976457595825195 6.54354190826416 0.054103948175907135
CurrentTrain: epoch  1, batch    36 | loss: 6.5976458Losses:  0.8988468050956726 0.8430160880088806 0.0558306947350502
CurrentTrain: epoch  1, batch    37 | loss: 0.8988468Losses:  6.844789505004883 6.797540187835693 0.04724937304854393
CurrentTrain: epoch  2, batch     0 | loss: 6.8447895Losses:  6.734364986419678 6.683355331420898 0.051009487360715866
CurrentTrain: epoch  2, batch     1 | loss: 6.7343650Losses:  5.998011589050293 5.949502468109131 0.04850908741354942
CurrentTrain: epoch  2, batch     2 | loss: 5.9980116Losses:  6.547121047973633 6.499150276184082 0.047970645129680634
CurrentTrain: epoch  2, batch     3 | loss: 6.5471210Losses:  5.252623558044434 5.207496166229248 0.04512738436460495
CurrentTrain: epoch  2, batch     4 | loss: 5.2526236Losses:  4.974461555480957 4.927558898925781 0.04690255969762802
CurrentTrain: epoch  2, batch     5 | loss: 4.9744616Losses:  6.914163589477539 6.871208190917969 0.04295520484447479
CurrentTrain: epoch  2, batch     6 | loss: 6.9141636Losses:  5.7022247314453125 5.66058349609375 0.041641075164079666
CurrentTrain: epoch  2, batch     7 | loss: 5.7022247Losses:  5.937689781188965 5.898758888244629 0.038930922746658325
CurrentTrain: epoch  2, batch     8 | loss: 5.9376898Losses:  5.677995681762695 5.637235641479492 0.040760040283203125
CurrentTrain: epoch  2, batch     9 | loss: 5.6779957Losses:  5.370161056518555 5.330877304077148 0.03928377479314804
CurrentTrain: epoch  2, batch    10 | loss: 5.3701611Losses:  6.556920051574707 6.517208099365234 0.03971178084611893
CurrentTrain: epoch  2, batch    11 | loss: 6.5569201Losses:  6.716360569000244 6.676339626312256 0.040020834654569626
CurrentTrain: epoch  2, batch    12 | loss: 6.7163606Losses:  6.74078893661499 6.703309535980225 0.0374794527888298
CurrentTrain: epoch  2, batch    13 | loss: 6.7407889Losses:  4.205597400665283 4.166139602661133 0.03945792093873024
CurrentTrain: epoch  2, batch    14 | loss: 4.2055974Losses:  10.122591972351074 10.059720993041992 0.06287063658237457
CurrentTrain: epoch  2, batch    15 | loss: 10.1225920Losses:  5.658863067626953 5.621238708496094 0.037624292075634
CurrentTrain: epoch  2, batch    16 | loss: 5.6588631Losses:  7.565697193145752 7.517902374267578 0.04779496416449547
CurrentTrain: epoch  2, batch    17 | loss: 7.5656972Losses:  5.597912311553955 5.560683250427246 0.03722904622554779
CurrentTrain: epoch  2, batch    18 | loss: 5.5979123Losses:  4.214885234832764 4.176669597625732 0.038215603679418564
CurrentTrain: epoch  2, batch    19 | loss: 4.2148852Losses:  9.478452682495117 9.42881965637207 0.049633368849754333
CurrentTrain: epoch  2, batch    20 | loss: 9.4784527Losses:  6.5759477615356445 6.530662536621094 0.045285020023584366
CurrentTrain: epoch  2, batch    21 | loss: 6.5759478Losses:  5.182569980621338 5.1481099128723145 0.03446001932024956
CurrentTrain: epoch  2, batch    22 | loss: 5.1825700Losses:  9.399325370788574 9.363622665405273 0.03570263460278511
CurrentTrain: epoch  2, batch    23 | loss: 9.3993254Losses:  6.474139213562012 6.436898708343506 0.0372406467795372
CurrentTrain: epoch  2, batch    24 | loss: 6.4741392Losses:  6.022072792053223 5.981836795806885 0.04023584723472595
CurrentTrain: epoch  2, batch    25 | loss: 6.0220728Losses:  5.021255016326904 4.98042106628418 0.040833745151758194
CurrentTrain: epoch  2, batch    26 | loss: 5.0212550Losses:  7.001400470733643 6.954679489135742 0.04672083258628845
CurrentTrain: epoch  2, batch    27 | loss: 7.0014005Losses:  7.394208908081055 7.3550920486450195 0.0391169935464859
CurrentTrain: epoch  2, batch    28 | loss: 7.3942089Losses:  6.584680557250977 6.544059753417969 0.040620580315589905
CurrentTrain: epoch  2, batch    29 | loss: 6.5846806Losses:  7.138827800750732 7.106758117675781 0.03206983208656311
CurrentTrain: epoch  2, batch    30 | loss: 7.1388278Losses:  9.809174537658691 9.765073776245117 0.04410097002983093
CurrentTrain: epoch  2, batch    31 | loss: 9.8091745Losses:  9.035673141479492 9.001752853393555 0.033920466899871826
CurrentTrain: epoch  2, batch    32 | loss: 9.0356731Losses:  6.458285331726074 6.426387310028076 0.031897805631160736
CurrentTrain: epoch  2, batch    33 | loss: 6.4582853Losses:  6.603152275085449 6.5600080490112305 0.043144069612026215
CurrentTrain: epoch  2, batch    34 | loss: 6.6031523Losses:  12.839557647705078 12.789018630981445 0.05053858086466789
CurrentTrain: epoch  2, batch    35 | loss: 12.8395576Losses:  5.791538715362549 5.756180286407471 0.03535864129662514
CurrentTrain: epoch  2, batch    36 | loss: 5.7915387Losses:  1.4871572256088257 1.4480617046356201 0.039095498621463776
CurrentTrain: epoch  2, batch    37 | loss: 1.4871572Losses:  12.806934356689453 12.755690574645996 0.051243893802165985
CurrentTrain: epoch  3, batch     0 | loss: 12.8069344Losses:  6.1897807121276855 6.148048400878906 0.04173226282000542
CurrentTrain: epoch  3, batch     1 | loss: 6.1897807Losses:  8.065496444702148 8.030288696289062 0.035207316279411316
CurrentTrain: epoch  3, batch     2 | loss: 8.0654964Losses:  4.793921947479248 4.763747692108154 0.03017442300915718
CurrentTrain: epoch  3, batch     3 | loss: 4.7939219Losses:  7.467722415924072 7.430812835693359 0.03690964728593826
CurrentTrain: epoch  3, batch     4 | loss: 7.4677224Losses:  6.787631511688232 6.750539302825928 0.03709201514720917
CurrentTrain: epoch  3, batch     5 | loss: 6.7876315Losses:  5.026753902435303 4.998074531555176 0.028679538518190384
CurrentTrain: epoch  3, batch     6 | loss: 5.0267539Losses:  5.059999942779541 5.03199577331543 0.028004083782434464
CurrentTrain: epoch  3, batch     7 | loss: 5.0599999Losses:  6.015719890594482 5.988090515136719 0.027629194781184196
CurrentTrain: epoch  3, batch     8 | loss: 6.0157199Losses:  5.670653343200684 5.6401753425598145 0.03047799877822399
CurrentTrain: epoch  3, batch     9 | loss: 5.6706533Losses:  4.730852127075195 4.697704792022705 0.033147357404232025
CurrentTrain: epoch  3, batch    10 | loss: 4.7308521Losses:  7.0868120193481445 7.053400039672852 0.03341187164187431
CurrentTrain: epoch  3, batch    11 | loss: 7.0868120Losses:  4.091923713684082 4.063596725463867 0.02832680381834507
CurrentTrain: epoch  3, batch    12 | loss: 4.0919237Losses:  6.449226379394531 6.414399147033691 0.034827347844839096
CurrentTrain: epoch  3, batch    13 | loss: 6.4492264Losses:  8.293244361877441 8.24746322631836 0.04578095301985741
CurrentTrain: epoch  3, batch    14 | loss: 8.2932444Losses:  7.075087547302246 7.0364532470703125 0.03863443061709404
CurrentTrain: epoch  3, batch    15 | loss: 7.0750875Losses:  5.536837100982666 5.497117042541504 0.039719969034194946
CurrentTrain: epoch  3, batch    16 | loss: 5.5368371Losses:  4.515492916107178 4.485996723175049 0.0294963326305151
CurrentTrain: epoch  3, batch    17 | loss: 4.5154929Losses:  6.141271114349365 6.105439186096191 0.0358317494392395
CurrentTrain: epoch  3, batch    18 | loss: 6.1412711Losses:  8.937532424926758 8.89706802368164 0.04046443849802017
CurrentTrain: epoch  3, batch    19 | loss: 8.9375324Losses:  5.364058971405029 5.333626747131348 0.0304323248565197
CurrentTrain: epoch  3, batch    20 | loss: 5.3640590Losses:  6.175868034362793 6.143397331237793 0.03247075900435448
CurrentTrain: epoch  3, batch    21 | loss: 6.1758680Losses:  5.749924182891846 5.721014976501465 0.02890927717089653
CurrentTrain: epoch  3, batch    22 | loss: 5.7499242Losses:  3.9824154376983643 3.9567275047302246 0.025687985122203827
CurrentTrain: epoch  3, batch    23 | loss: 3.9824154Losses:  5.653217315673828 5.624668121337891 0.02854938432574272
CurrentTrain: epoch  3, batch    24 | loss: 5.6532173Losses:  5.350209712982178 5.319683074951172 0.030526794493198395
CurrentTrain: epoch  3, batch    25 | loss: 5.3502097Losses:  7.431802749633789 7.397665500640869 0.034137025475502014
CurrentTrain: epoch  3, batch    26 | loss: 7.4318027Losses:  6.01621675491333 5.983842849731445 0.032374050468206406
CurrentTrain: epoch  3, batch    27 | loss: 6.0162168Losses:  6.411444187164307 6.376212120056152 0.0352320596575737
CurrentTrain: epoch  3, batch    28 | loss: 6.4114442Losses:  8.71048355102539 8.673238754272461 0.037244319915771484
CurrentTrain: epoch  3, batch    29 | loss: 8.7104836Losses:  7.58470344543457 7.550199031829834 0.034504588693380356
CurrentTrain: epoch  3, batch    30 | loss: 7.5847034Losses:  5.464385986328125 5.436367034912109 0.028019152581691742
CurrentTrain: epoch  3, batch    31 | loss: 5.4643860Losses:  4.0102972984313965 3.9843459129333496 0.025951460003852844
CurrentTrain: epoch  3, batch    32 | loss: 4.0102973Losses:  6.467951774597168 6.441784858703613 0.026166848838329315
CurrentTrain: epoch  3, batch    33 | loss: 6.4679518Losses:  5.554758071899414 5.529712677001953 0.02504556067287922
CurrentTrain: epoch  3, batch    34 | loss: 5.5547581Losses:  5.251598358154297 5.224788665771484 0.026809481903910637
CurrentTrain: epoch  3, batch    35 | loss: 5.2515984Losses:  6.602919578552246 6.57191276550293 0.031006639823317528
CurrentTrain: epoch  3, batch    36 | loss: 6.6029196Losses:  2.0470006465911865 2.0038094520568848 0.04319128021597862
CurrentTrain: epoch  3, batch    37 | loss: 2.0470006Losses:  4.40277099609375 4.377449035644531 0.02532193250954151
CurrentTrain: epoch  4, batch     0 | loss: 4.4027710Losses:  5.30986213684082 5.284968852996826 0.02489348128437996
CurrentTrain: epoch  4, batch     1 | loss: 5.3098621Losses:  5.355696201324463 5.329316139221191 0.02638009935617447
CurrentTrain: epoch  4, batch     2 | loss: 5.3556962Losses:  4.495013236999512 4.469832897186279 0.025180472061038017
CurrentTrain: epoch  4, batch     3 | loss: 4.4950132Losses:  6.381677150726318 6.351010322570801 0.03066663257777691
CurrentTrain: epoch  4, batch     4 | loss: 6.3816772Losses:  4.745778560638428 4.71975040435791 0.026028236374258995
CurrentTrain: epoch  4, batch     5 | loss: 4.7457786Losses:  9.959098815917969 9.932989120483398 0.02610994130373001
CurrentTrain: epoch  4, batch     6 | loss: 9.9590988Losses:  5.01453161239624 4.989716529846191 0.02481510490179062
CurrentTrain: epoch  4, batch     7 | loss: 5.0145316Losses:  7.121729373931885 7.094048500061035 0.027680912986397743
CurrentTrain: epoch  4, batch     8 | loss: 7.1217294Losses:  7.2798590660095215 7.2429118156433105 0.036947041749954224
CurrentTrain: epoch  4, batch     9 | loss: 7.2798591Losses:  5.597299575805664 5.569307327270508 0.02799220196902752
CurrentTrain: epoch  4, batch    10 | loss: 5.5972996Losses:  6.001320838928223 5.974258899688721 0.027062103152275085
CurrentTrain: epoch  4, batch    11 | loss: 6.0013208Losses:  4.394862174987793 4.371270179748535 0.023591943085193634
CurrentTrain: epoch  4, batch    12 | loss: 4.3948622Losses:  9.181653022766113 9.144305229187012 0.03734825551509857
CurrentTrain: epoch  4, batch    13 | loss: 9.1816530Losses:  11.25554370880127 11.187061309814453 0.06848196685314178
CurrentTrain: epoch  4, batch    14 | loss: 11.2555437Losses:  4.515030860900879 4.492359638214111 0.022671200335025787
CurrentTrain: epoch  4, batch    15 | loss: 4.5150309Losses:  4.8840436935424805 4.854621410369873 0.029422080144286156
CurrentTrain: epoch  4, batch    16 | loss: 4.8840437Losses:  4.870213985443115 4.842741012573242 0.027472753077745438
CurrentTrain: epoch  4, batch    17 | loss: 4.8702140Losses:  6.359716892242432 6.3347978591918945 0.024919118732213974
CurrentTrain: epoch  4, batch    18 | loss: 6.3597169Losses:  7.359180927276611 7.324807643890381 0.03437305986881256
CurrentTrain: epoch  4, batch    19 | loss: 7.3591809Losses:  6.333792686462402 6.302290916442871 0.031501732766628265
CurrentTrain: epoch  4, batch    20 | loss: 6.3337927Losses:  5.806580543518066 5.778742790222168 0.027837613597512245
CurrentTrain: epoch  4, batch    21 | loss: 5.8065805Losses:  5.031203746795654 5.005866050720215 0.025337623432278633
CurrentTrain: epoch  4, batch    22 | loss: 5.0312037Losses:  5.705225944519043 5.677569389343262 0.0276566781103611
CurrentTrain: epoch  4, batch    23 | loss: 5.7052259Losses:  6.942493915557861 6.911308288574219 0.031185409054160118
CurrentTrain: epoch  4, batch    24 | loss: 6.9424939Losses:  4.981197834014893 4.955143928527832 0.026053672656416893
CurrentTrain: epoch  4, batch    25 | loss: 4.9811978Losses:  4.694237232208252 4.669776916503906 0.02446042373776436
CurrentTrain: epoch  4, batch    26 | loss: 4.6942372Losses:  4.5639872550964355 4.539081573486328 0.024905746802687645
CurrentTrain: epoch  4, batch    27 | loss: 4.5639873Losses:  5.670595169067383 5.641693115234375 0.028902214020490646
CurrentTrain: epoch  4, batch    28 | loss: 5.6705952Losses:  10.588767051696777 10.530830383300781 0.05793630704283714
CurrentTrain: epoch  4, batch    29 | loss: 10.5887671Losses:  7.15209436416626 7.123076438903809 0.02901790477335453
CurrentTrain: epoch  4, batch    30 | loss: 7.1520944Losses:  3.878056049346924 3.853670358657837 0.024385739117860794
CurrentTrain: epoch  4, batch    31 | loss: 3.8780560Losses:  7.010912895202637 6.983489990234375 0.027422823011875153
CurrentTrain: epoch  4, batch    32 | loss: 7.0109129Losses:  6.761427879333496 6.720966339111328 0.04046162962913513
CurrentTrain: epoch  4, batch    33 | loss: 6.7614279Losses:  7.352393627166748 7.315114974975586 0.03727860003709793
CurrentTrain: epoch  4, batch    34 | loss: 7.3523936Losses:  12.90021800994873 12.849349975585938 0.05086765065789223
CurrentTrain: epoch  4, batch    35 | loss: 12.9002180Losses:  13.15949821472168 13.116473197937012 0.04302503168582916
CurrentTrain: epoch  4, batch    36 | loss: 13.1594982Losses:  2.2467169761657715 2.2097249031066895 0.03699213266372681
CurrentTrain: epoch  4, batch    37 | loss: 2.2467170Losses:  7.809317588806152 7.782597541809082 0.02671988308429718
CurrentTrain: epoch  5, batch     0 | loss: 7.8093176Losses:  5.934905529022217 5.9104437828063965 0.02446170337498188
CurrentTrain: epoch  5, batch     1 | loss: 5.9349055Losses:  5.610689640045166 5.5731987953186035 0.03749064728617668
CurrentTrain: epoch  5, batch     2 | loss: 5.6106896Losses:  5.011085510253906 4.987240791320801 0.023844506591558456
CurrentTrain: epoch  5, batch     3 | loss: 5.0110855Losses:  4.752048969268799 4.724822044372559 0.02722707949578762
CurrentTrain: epoch  5, batch     4 | loss: 4.7520490Losses:  7.577335357666016 7.551681041717529 0.025654174387454987
CurrentTrain: epoch  5, batch     5 | loss: 7.5773354Losses:  4.925411701202393 4.895986080169678 0.029425667598843575
CurrentTrain: epoch  5, batch     6 | loss: 4.9254117Losses:  5.570183277130127 5.542703151702881 0.02748005837202072
CurrentTrain: epoch  5, batch     7 | loss: 5.5701833Losses:  6.393786907196045 6.368192672729492 0.025594394654035568
CurrentTrain: epoch  5, batch     8 | loss: 6.3937869Losses:  6.068186283111572 6.042348861694336 0.025837216526269913
CurrentTrain: epoch  5, batch     9 | loss: 6.0681863Losses:  6.199550151824951 6.171894550323486 0.02765563502907753
CurrentTrain: epoch  5, batch    10 | loss: 6.1995502Losses:  9.422708511352539 9.386102676391602 0.036605946719646454
CurrentTrain: epoch  5, batch    11 | loss: 9.4227085Losses:  4.587123870849609 4.562543869018555 0.024579770863056183
CurrentTrain: epoch  5, batch    12 | loss: 4.5871239Losses:  5.616791248321533 5.589300155639648 0.027491075918078423
CurrentTrain: epoch  5, batch    13 | loss: 5.6167912Losses:  10.394536018371582 10.355605125427246 0.038931094110012054
CurrentTrain: epoch  5, batch    14 | loss: 10.3945360Losses:  5.6531901359558105 5.620779514312744 0.0324106328189373
CurrentTrain: epoch  5, batch    15 | loss: 5.6531901Losses:  6.985912322998047 6.958492279052734 0.027419865131378174
CurrentTrain: epoch  5, batch    16 | loss: 6.9859123Losses:  6.006938934326172 5.983061790466309 0.023877039551734924
CurrentTrain: epoch  5, batch    17 | loss: 6.0069389Losses:  4.306700706481934 4.282675743103027 0.024025026708841324
CurrentTrain: epoch  5, batch    18 | loss: 4.3067007Losses:  5.116084575653076 5.089239120483398 0.026845689862966537
CurrentTrain: epoch  5, batch    19 | loss: 5.1160846Losses:  3.911515951156616 3.8881380558013916 0.0233779214322567
CurrentTrain: epoch  5, batch    20 | loss: 3.9115160Losses:  4.982569217681885 4.95556640625 0.027002636343240738
CurrentTrain: epoch  5, batch    21 | loss: 4.9825692Losses:  10.853238105773926 10.81745719909668 0.035780563950538635
CurrentTrain: epoch  5, batch    22 | loss: 10.8532381Losses:  5.971587657928467 5.943114280700684 0.028473496437072754
CurrentTrain: epoch  5, batch    23 | loss: 5.9715877Losses:  7.007076740264893 6.976271629333496 0.030805181711912155
CurrentTrain: epoch  5, batch    24 | loss: 7.0070767Losses:  5.219791412353516 5.194202423095703 0.025589222088456154
CurrentTrain: epoch  5, batch    25 | loss: 5.2197914Losses:  5.010578632354736 4.984551429748535 0.02602701634168625
CurrentTrain: epoch  5, batch    26 | loss: 5.0105786Losses:  9.859851837158203 9.796358108520508 0.0634935274720192
CurrentTrain: epoch  5, batch    27 | loss: 9.8598518Losses:  4.0720906257629395 4.048921585083008 0.023169156163930893
CurrentTrain: epoch  5, batch    28 | loss: 4.0720906Losses:  6.127196311950684 6.099713325500488 0.0274830162525177
CurrentTrain: epoch  5, batch    29 | loss: 6.1271963Losses:  5.679309844970703 5.651951789855957 0.027357926592230797
CurrentTrain: epoch  5, batch    30 | loss: 5.6793098Losses:  7.998748302459717 7.963225364685059 0.035522714257240295
CurrentTrain: epoch  5, batch    31 | loss: 7.9987483Losses:  5.711267948150635 5.682096481323242 0.029171384871006012
CurrentTrain: epoch  5, batch    32 | loss: 5.7112679Losses:  4.8804426193237305 4.855334758758545 0.025107933208346367
CurrentTrain: epoch  5, batch    33 | loss: 4.8804426Losses:  9.775077819824219 9.739803314208984 0.035274866968393326
CurrentTrain: epoch  5, batch    34 | loss: 9.7750778Losses:  5.5118184089660645 5.486283302307129 0.025535188615322113
CurrentTrain: epoch  5, batch    35 | loss: 5.5118184Losses:  6.714870929718018 6.675585746765137 0.03928537666797638
CurrentTrain: epoch  5, batch    36 | loss: 6.7148709Losses:  1.806378960609436 1.733860731124878 0.07251818478107452
CurrentTrain: epoch  5, batch    37 | loss: 1.8063790Losses:  8.367283821105957 8.327139854431152 0.04014412313699722
CurrentTrain: epoch  6, batch     0 | loss: 8.3672838Losses:  7.847779273986816 7.820092678070068 0.02768682688474655
CurrentTrain: epoch  6, batch     1 | loss: 7.8477793Losses:  4.6835713386535645 4.656864166259766 0.026707114651799202
CurrentTrain: epoch  6, batch     2 | loss: 4.6835713Losses:  5.7231268882751465 5.692259311676025 0.03086736798286438
CurrentTrain: epoch  6, batch     3 | loss: 5.7231269Losses:  7.250270366668701 7.211296558380127 0.0389738604426384
CurrentTrain: epoch  6, batch     4 | loss: 7.2502704Losses:  6.152815341949463 6.126975059509277 0.02584006078541279
CurrentTrain: epoch  6, batch     5 | loss: 6.1528153Losses:  5.307407855987549 5.279391765594482 0.028015965595841408
CurrentTrain: epoch  6, batch     6 | loss: 5.3074079Losses:  6.025305271148682 5.9992265701293945 0.02607881650328636
CurrentTrain: epoch  6, batch     7 | loss: 6.0253053Losses:  6.412933349609375 6.385475158691406 0.02745813876390457
CurrentTrain: epoch  6, batch     8 | loss: 6.4129333Losses:  8.025007247924805 8.002296447753906 0.022711172699928284
CurrentTrain: epoch  6, batch     9 | loss: 8.0250072Losses:  4.939423561096191 4.913458347320557 0.025965167209506035
CurrentTrain: epoch  6, batch    10 | loss: 4.9394236Losses:  9.74764633178711 9.71452522277832 0.03312152251601219
CurrentTrain: epoch  6, batch    11 | loss: 9.7476463Losses:  4.740046977996826 4.714466094970703 0.025580856949090958
CurrentTrain: epoch  6, batch    12 | loss: 4.7400470Losses:  6.379240036010742 6.350358963012695 0.028881018981337547
CurrentTrain: epoch  6, batch    13 | loss: 6.3792400Losses:  6.940683364868164 6.910178184509277 0.030505403876304626
CurrentTrain: epoch  6, batch    14 | loss: 6.9406834Losses:  4.766989707946777 4.741362571716309 0.025627218186855316
CurrentTrain: epoch  6, batch    15 | loss: 4.7669897Losses:  5.617949485778809 5.585790634155273 0.032158900052309036
CurrentTrain: epoch  6, batch    16 | loss: 5.6179495Losses:  5.289805889129639 5.264698028564453 0.02510787546634674
CurrentTrain: epoch  6, batch    17 | loss: 5.2898059Losses:  9.302587509155273 9.268001556396484 0.034585777670145035
CurrentTrain: epoch  6, batch    18 | loss: 9.3025875Losses:  6.065401554107666 6.037014007568359 0.028387755155563354
CurrentTrain: epoch  6, batch    19 | loss: 6.0654016Losses:  3.603539228439331 3.582024097442627 0.021515125408768654
CurrentTrain: epoch  6, batch    20 | loss: 3.6035392Losses:  4.629624843597412 4.606697082519531 0.022927548736333847
CurrentTrain: epoch  6, batch    21 | loss: 4.6296248Losses:  8.943650245666504 8.906632423400879 0.03701786696910858
CurrentTrain: epoch  6, batch    22 | loss: 8.9436502Losses:  3.7145254611968994 3.6936769485473633 0.020848415791988373
CurrentTrain: epoch  6, batch    23 | loss: 3.7145255Losses:  4.905918121337891 4.882142066955566 0.023775827139616013
CurrentTrain: epoch  6, batch    24 | loss: 4.9059181Losses:  3.758784770965576 3.734874725341797 0.0239100381731987
CurrentTrain: epoch  6, batch    25 | loss: 3.7587848Losses:  3.5533037185668945 3.5310940742492676 0.022209547460079193
CurrentTrain: epoch  6, batch    26 | loss: 3.5533037Losses:  9.163805961608887 9.136911392211914 0.026894662529230118
CurrentTrain: epoch  6, batch    27 | loss: 9.1638060Losses:  9.683914184570312 9.652264595031738 0.03164975345134735
CurrentTrain: epoch  6, batch    28 | loss: 9.6839142Losses:  4.409639358520508 4.387508392333984 0.02213107980787754
CurrentTrain: epoch  6, batch    29 | loss: 4.4096394Losses:  6.553865432739258 6.530350685119629 0.02351468987762928
CurrentTrain: epoch  6, batch    30 | loss: 6.5538654Losses:  5.472183704376221 5.446613788604736 0.02556975930929184
CurrentTrain: epoch  6, batch    31 | loss: 5.4721837Losses:  7.097512722015381 7.058704376220703 0.03880811482667923
CurrentTrain: epoch  6, batch    32 | loss: 7.0975127Losses:  5.034789085388184 5.011237144470215 0.023551715537905693
CurrentTrain: epoch  6, batch    33 | loss: 5.0347891Losses:  8.721230506896973 8.697742462158203 0.023487966507673264
CurrentTrain: epoch  6, batch    34 | loss: 8.7212305Losses:  4.634392738342285 4.61093807220459 0.023454837501049042
CurrentTrain: epoch  6, batch    35 | loss: 4.6343927Losses:  5.003186225891113 4.973163604736328 0.030022772029042244
CurrentTrain: epoch  6, batch    36 | loss: 5.0031862Losses:  1.4007842540740967 1.359915018081665 0.04086922109127045
CurrentTrain: epoch  6, batch    37 | loss: 1.4007843Losses:  4.466475486755371 4.439763069152832 0.0267125703394413
CurrentTrain: epoch  7, batch     0 | loss: 4.4664755Losses:  8.071389198303223 8.033985137939453 0.03740406408905983
CurrentTrain: epoch  7, batch     1 | loss: 8.0713892Losses:  6.551468372344971 6.508309364318848 0.04315919801592827
CurrentTrain: epoch  7, batch     2 | loss: 6.5514684Losses:  4.51265811920166 4.489387512207031 0.023270487785339355
CurrentTrain: epoch  7, batch     3 | loss: 4.5126581Losses:  6.542674541473389 6.515871047973633 0.02680356241762638
CurrentTrain: epoch  7, batch     4 | loss: 6.5426745Losses:  6.8542962074279785 6.813389778137207 0.040906526148319244
CurrentTrain: epoch  7, batch     5 | loss: 6.8542962Losses:  5.107654094696045 5.084316253662109 0.023337919265031815
CurrentTrain: epoch  7, batch     6 | loss: 5.1076541Losses:  10.3823881149292 10.352195739746094 0.030192755162715912
CurrentTrain: epoch  7, batch     7 | loss: 10.3823881Losses:  5.024540901184082 4.997439861297607 0.027100855484604836
CurrentTrain: epoch  7, batch     8 | loss: 5.0245409Losses:  5.030399799346924 5.002575874328613 0.027823932468891144
CurrentTrain: epoch  7, batch     9 | loss: 5.0303998Losses:  6.140862941741943 6.101566314697266 0.03929650038480759
CurrentTrain: epoch  7, batch    10 | loss: 6.1408629Losses:  4.57041072845459 4.5425872802734375 0.02782352641224861
CurrentTrain: epoch  7, batch    11 | loss: 4.5704107Losses:  7.679421901702881 7.651275634765625 0.02814647927880287
CurrentTrain: epoch  7, batch    12 | loss: 7.6794219Losses:  12.773941040039062 12.725936889648438 0.048004504293203354
CurrentTrain: epoch  7, batch    13 | loss: 12.7739410Losses:  7.1840128898620605 7.150051593780518 0.03396143391728401
CurrentTrain: epoch  7, batch    14 | loss: 7.1840129Losses:  6.491887092590332 6.4677019119262695 0.02418532595038414
CurrentTrain: epoch  7, batch    15 | loss: 6.4918871Losses:  12.324922561645508 12.277687072753906 0.047235045582056046
CurrentTrain: epoch  7, batch    16 | loss: 12.3249226Losses:  4.743075370788574 4.717100143432617 0.025975389406085014
CurrentTrain: epoch  7, batch    17 | loss: 4.7430754Losses:  11.047346115112305 11.01102066040039 0.036325663328170776
CurrentTrain: epoch  7, batch    18 | loss: 11.0473461Losses:  4.058762550354004 4.0345458984375 0.024216802790760994
CurrentTrain: epoch  7, batch    19 | loss: 4.0587626Losses:  5.140275001525879 5.115592956542969 0.02468189038336277
CurrentTrain: epoch  7, batch    20 | loss: 5.1402750Losses:  5.062193393707275 5.031428337097168 0.030764838680624962
CurrentTrain: epoch  7, batch    21 | loss: 5.0621934Losses:  7.827138423919678 7.792054176330566 0.03508405387401581
CurrentTrain: epoch  7, batch    22 | loss: 7.8271384Losses:  4.70029354095459 4.673651695251465 0.02664194069802761
CurrentTrain: epoch  7, batch    23 | loss: 4.7002935Losses:  5.240616321563721 5.215245246887207 0.025371095165610313
CurrentTrain: epoch  7, batch    24 | loss: 5.2406163Losses:  8.763257026672363 8.725713729858398 0.03754289075732231
CurrentTrain: epoch  7, batch    25 | loss: 8.7632570Losses:  4.658713340759277 4.6295061111450195 0.029207102954387665
CurrentTrain: epoch  7, batch    26 | loss: 4.6587133Losses:  4.856579303741455 4.832057952880859 0.024521494284272194
CurrentTrain: epoch  7, batch    27 | loss: 4.8565793Losses:  6.540798664093018 6.501948356628418 0.03885037079453468
CurrentTrain: epoch  7, batch    28 | loss: 6.5407987Losses:  4.922431468963623 4.89629602432251 0.026135260239243507
CurrentTrain: epoch  7, batch    29 | loss: 4.9224315Losses:  9.410470008850098 9.383367538452148 0.02710213139653206
CurrentTrain: epoch  7, batch    30 | loss: 9.4104700Losses:  5.742456436157227 5.712131023406982 0.030325470492243767
CurrentTrain: epoch  7, batch    31 | loss: 5.7424564Losses:  6.975845813751221 6.9447245597839355 0.031121278181672096
CurrentTrain: epoch  7, batch    32 | loss: 6.9758458Losses:  4.894138813018799 4.865935325622559 0.028203267604112625
CurrentTrain: epoch  7, batch    33 | loss: 4.8941388Losses:  5.499330997467041 5.473447799682617 0.025883276015520096
CurrentTrain: epoch  7, batch    34 | loss: 5.4993310Losses:  4.6634721755981445 4.638394355773926 0.025078054517507553
CurrentTrain: epoch  7, batch    35 | loss: 4.6634722Losses:  6.490392208099365 6.449099540710449 0.04129260033369064
CurrentTrain: epoch  7, batch    36 | loss: 6.4903922Losses:  5.585654258728027 5.517297744750977 0.06835637986660004
CurrentTrain: epoch  7, batch    37 | loss: 5.5856543Losses:  7.482237339019775 7.441339492797852 0.040897712111473083
CurrentTrain: epoch  8, batch     0 | loss: 7.4822373Losses:  6.799695014953613 6.763945579528809 0.03574921935796738
CurrentTrain: epoch  8, batch     1 | loss: 6.7996950Losses:  4.939810276031494 4.913906097412109 0.02590406872332096
CurrentTrain: epoch  8, batch     2 | loss: 4.9398103Losses:  4.526394367218018 4.503654479980469 0.022740071639418602
CurrentTrain: epoch  8, batch     3 | loss: 4.5263944Losses:  4.133875370025635 4.1111063957214355 0.022768737748265266
CurrentTrain: epoch  8, batch     4 | loss: 4.1338754Losses:  4.67362117767334 4.647736549377441 0.025884553790092468
CurrentTrain: epoch  8, batch     5 | loss: 4.6736212Losses:  5.04115104675293 5.007367134094238 0.03378384932875633
CurrentTrain: epoch  8, batch     6 | loss: 5.0411510Losses:  7.1992950439453125 7.166845798492432 0.032449062913656235
CurrentTrain: epoch  8, batch     7 | loss: 7.1992950Losses:  6.001532077789307 5.968138694763184 0.03339359909296036
CurrentTrain: epoch  8, batch     8 | loss: 6.0015321Losses:  7.182487487792969 7.1441497802734375 0.03833790868520737
CurrentTrain: epoch  8, batch     9 | loss: 7.1824875Losses:  4.312734603881836 4.289783477783203 0.022951316088438034
CurrentTrain: epoch  8, batch    10 | loss: 4.3127346Losses:  5.502866744995117 5.47519588470459 0.02767082490026951
CurrentTrain: epoch  8, batch    11 | loss: 5.5028667Losses:  5.055629730224609 5.02351188659668 0.03211768716573715
CurrentTrain: epoch  8, batch    12 | loss: 5.0556297Losses:  5.402219295501709 5.374166011810303 0.0280534066259861
CurrentTrain: epoch  8, batch    13 | loss: 5.4022193Losses:  5.701046466827393 5.663161277770996 0.037885215133428574
CurrentTrain: epoch  8, batch    14 | loss: 5.7010465Losses:  6.742729187011719 6.712909698486328 0.029819265007972717
CurrentTrain: epoch  8, batch    15 | loss: 6.7427292Losses:  9.751315116882324 9.725568771362305 0.025745902210474014
CurrentTrain: epoch  8, batch    16 | loss: 9.7513151Losses:  4.283809185028076 4.258111953735352 0.025697048753499985
CurrentTrain: epoch  8, batch    17 | loss: 4.2838092Losses:  7.258885860443115 7.221784591674805 0.037101320922374725
CurrentTrain: epoch  8, batch    18 | loss: 7.2588859Losses:  4.312206268310547 4.286839485168457 0.025366563349962234
CurrentTrain: epoch  8, batch    19 | loss: 4.3122063Losses:  5.079998970031738 5.051776885986328 0.02822192758321762
CurrentTrain: epoch  8, batch    20 | loss: 5.0799990Losses:  6.054765701293945 6.028387069702148 0.026378706097602844
CurrentTrain: epoch  8, batch    21 | loss: 6.0547657Losses:  12.15284538269043 12.118014335632324 0.03483080118894577
CurrentTrain: epoch  8, batch    22 | loss: 12.1528454Losses:  4.8212714195251465 4.79185676574707 0.029414787888526917
CurrentTrain: epoch  8, batch    23 | loss: 4.8212714Losses:  6.829110145568848 6.803580284118652 0.025529786944389343
CurrentTrain: epoch  8, batch    24 | loss: 6.8291101Losses:  5.943495750427246 5.920014381408691 0.02348138764500618
CurrentTrain: epoch  8, batch    25 | loss: 5.9434958Losses:  5.975316047668457 5.943143367767334 0.03217286616563797
CurrentTrain: epoch  8, batch    26 | loss: 5.9753160Losses:  8.530329704284668 8.480701446533203 0.04962809383869171
CurrentTrain: epoch  8, batch    27 | loss: 8.5303297Losses:  5.04524564743042 5.017204761505127 0.028040772303938866
CurrentTrain: epoch  8, batch    28 | loss: 5.0452456Losses:  5.52751350402832 5.503159999847412 0.024353278800845146
CurrentTrain: epoch  8, batch    29 | loss: 5.5275135Losses:  6.332120895385742 6.301647663116455 0.030473200604319572
CurrentTrain: epoch  8, batch    30 | loss: 6.3321209Losses:  5.449488639831543 5.422089099884033 0.027399450540542603
CurrentTrain: epoch  8, batch    31 | loss: 5.4494886Losses:  8.835563659667969 8.802620887756348 0.03294263780117035
CurrentTrain: epoch  8, batch    32 | loss: 8.8355637Losses:  8.002527236938477 7.9677019119262695 0.03482567518949509
CurrentTrain: epoch  8, batch    33 | loss: 8.0025272Losses:  5.4289679527282715 5.401972770690918 0.026995154097676277
CurrentTrain: epoch  8, batch    34 | loss: 5.4289680Losses:  4.295067310333252 4.273365020751953 0.021702248603105545
CurrentTrain: epoch  8, batch    35 | loss: 4.2950673Losses:  4.817999839782715 4.79157829284668 0.026421431452035904
CurrentTrain: epoch  8, batch    36 | loss: 4.8179998Losses:  0.4024674594402313 0.3755437731742859 0.02692367322742939
CurrentTrain: epoch  8, batch    37 | loss: 0.4024675Losses:  5.7090911865234375 5.675492286682129 0.03359874337911606
CurrentTrain: epoch  9, batch     0 | loss: 5.7090912Losses:  6.830294132232666 6.794440269470215 0.03585376963019371
CurrentTrain: epoch  9, batch     1 | loss: 6.8302941Losses:  3.852945566177368 3.8293235301971436 0.023621924221515656
CurrentTrain: epoch  9, batch     2 | loss: 3.8529456Losses:  5.586561679840088 5.554819107055664 0.03174237161874771
CurrentTrain: epoch  9, batch     3 | loss: 5.5865617Losses:  6.342552661895752 6.304049015045166 0.03850367292761803
CurrentTrain: epoch  9, batch     4 | loss: 6.3425527Losses:  5.69138240814209 5.662535667419434 0.0288466215133667
CurrentTrain: epoch  9, batch     5 | loss: 5.6913824Losses:  6.688109874725342 6.652065277099609 0.03604455664753914
CurrentTrain: epoch  9, batch     6 | loss: 6.6881099Losses:  5.2492852210998535 5.225128173828125 0.02415710873901844
CurrentTrain: epoch  9, batch     7 | loss: 5.2492852Losses:  4.603850841522217 4.5825114250183105 0.02133934386074543
CurrentTrain: epoch  9, batch     8 | loss: 4.6038508Losses:  9.006752967834473 8.97661018371582 0.030142614617943764
CurrentTrain: epoch  9, batch     9 | loss: 9.0067530Losses:  9.141572952270508 9.11246109008789 0.029112275689840317
CurrentTrain: epoch  9, batch    10 | loss: 9.1415730Losses:  5.09789514541626 5.0733842849731445 0.024510793387889862
CurrentTrain: epoch  9, batch    11 | loss: 5.0978951Losses:  7.219051361083984 7.178138732910156 0.040912702679634094
CurrentTrain: epoch  9, batch    12 | loss: 7.2190514Losses:  5.9628472328186035 5.917839050292969 0.04500841349363327
CurrentTrain: epoch  9, batch    13 | loss: 5.9628472Losses:  5.3706135749816895 5.339262962341309 0.031350746750831604
CurrentTrain: epoch  9, batch    14 | loss: 5.3706136Losses:  4.3499274253845215 4.32721471786499 0.022712919861078262
CurrentTrain: epoch  9, batch    15 | loss: 4.3499274Losses:  4.234236717224121 4.209550380706787 0.024686366319656372
CurrentTrain: epoch  9, batch    16 | loss: 4.2342367Losses:  7.174408912658691 7.139503479003906 0.03490551561117172
CurrentTrain: epoch  9, batch    17 | loss: 7.1744089Losses:  6.556386470794678 6.532130241394043 0.02425607293844223
CurrentTrain: epoch  9, batch    18 | loss: 6.5563865Losses:  4.190176963806152 4.1684489250183105 0.021727969869971275
CurrentTrain: epoch  9, batch    19 | loss: 4.1901770Losses:  4.6022562980651855 4.563082695007324 0.0391736775636673
CurrentTrain: epoch  9, batch    20 | loss: 4.6022563Losses:  7.075135231018066 7.036354064941406 0.038781341165304184
CurrentTrain: epoch  9, batch    21 | loss: 7.0751352Losses:  4.946470737457275 4.912953853607178 0.033517010509967804
CurrentTrain: epoch  9, batch    22 | loss: 4.9464707Losses:  5.7415771484375 5.715035438537598 0.026541821658611298
CurrentTrain: epoch  9, batch    23 | loss: 5.7415771Losses:  4.541531085968018 4.515805244445801 0.02572597749531269
CurrentTrain: epoch  9, batch    24 | loss: 4.5415311Losses:  8.244755744934082 8.201252937316895 0.0435028001666069
CurrentTrain: epoch  9, batch    25 | loss: 8.2447557Losses:  4.410554885864258 4.3839616775512695 0.02659318782389164
CurrentTrain: epoch  9, batch    26 | loss: 4.4105549Losses:  11.443150520324707 11.408263206481934 0.03488686680793762
CurrentTrain: epoch  9, batch    27 | loss: 11.4431505Losses:  5.218830108642578 5.185811519622803 0.033018648624420166
CurrentTrain: epoch  9, batch    28 | loss: 5.2188301Losses:  5.5253214836120605 5.488332748413086 0.036988742649555206
CurrentTrain: epoch  9, batch    29 | loss: 5.5253215Losses:  7.561254978179932 7.523514270782471 0.03774052858352661
CurrentTrain: epoch  9, batch    30 | loss: 7.5612550Losses:  6.92396879196167 6.900655746459961 0.023312939330935478
CurrentTrain: epoch  9, batch    31 | loss: 6.9239688Losses:  5.00520658493042 4.98233699798584 0.022869817912578583
CurrentTrain: epoch  9, batch    32 | loss: 5.0052066Losses:  7.274559020996094 7.23862361907959 0.03593536466360092
CurrentTrain: epoch  9, batch    33 | loss: 7.2745590Losses:  9.791337013244629 9.74769401550293 0.04364260658621788
CurrentTrain: epoch  9, batch    34 | loss: 9.7913370Losses:  4.108532428741455 4.083792686462402 0.02473992295563221
CurrentTrain: epoch  9, batch    35 | loss: 4.1085324Losses:  8.137532234191895 8.103421211242676 0.03411064296960831
CurrentTrain: epoch  9, batch    36 | loss: 8.1375322Losses:  2.254941940307617 2.2052817344665527 0.04966019466519356
CurrentTrain: epoch  9, batch    37 | loss: 2.2549419
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 89.73%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 88.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 86.72%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 85.66%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 84.38%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 84.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.51%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 86.14%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.73%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 87.95%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 88.12%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 88.31%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.48%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.93%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 89.73%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 88.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 86.72%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 85.66%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 84.38%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 84.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.51%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 86.14%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.73%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 87.95%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 88.12%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 88.31%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.48%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.93%   
cur_acc:  ['0.8693']
his_acc:  ['0.8693']
Clustering into  4  clusters
Clusters:  [1 0 0 1 3 2 1 0 0 0 0]
Losses:  7.942043781280518 7.907543659210205 0.03450009599328041
CurrentTrain: epoch  0, batch     0 | loss: 7.9420438Losses:  1.8879060745239258 1.8427685499191284 0.04513752460479736
CurrentTrain: epoch  0, batch     1 | loss: 1.8879061Losses:  9.015058517456055 8.979735374450684 0.03532285615801811
CurrentTrain: epoch  1, batch     0 | loss: 9.0150585Losses:  2.207629919052124 2.16780948638916 0.03982038050889969
CurrentTrain: epoch  1, batch     1 | loss: 2.2076299Losses:  7.055542469024658 7.018494129180908 0.03704854100942612
CurrentTrain: epoch  2, batch     0 | loss: 7.0555425Losses:  3.3209750652313232 3.284996509552002 0.03597855195403099
CurrentTrain: epoch  2, batch     1 | loss: 3.3209751Losses:  5.876044273376465 5.841910362243652 0.03413385897874832
CurrentTrain: epoch  3, batch     0 | loss: 5.8760443Losses:  2.097712278366089 2.0631535053253174 0.03455882892012596
CurrentTrain: epoch  3, batch     1 | loss: 2.0977123Losses:  5.871994972229004 5.834962844848633 0.037032052874565125
CurrentTrain: epoch  4, batch     0 | loss: 5.8719950Losses:  3.028712511062622 2.991274356842041 0.03743816912174225
CurrentTrain: epoch  4, batch     1 | loss: 3.0287125Losses:  8.498701095581055 8.467787742614746 0.030913369730114937
CurrentTrain: epoch  5, batch     0 | loss: 8.4987011Losses:  2.385796546936035 2.3518593311309814 0.033937178552150726
CurrentTrain: epoch  5, batch     1 | loss: 2.3857965Losses:  5.642935752868652 5.61046838760376 0.03246735781431198
CurrentTrain: epoch  6, batch     0 | loss: 5.6429358Losses:  3.3731191158294678 3.3414041996002197 0.03171483054757118
CurrentTrain: epoch  6, batch     1 | loss: 3.3731191Losses:  7.254690170288086 7.223395347595215 0.03129483014345169
CurrentTrain: epoch  7, batch     0 | loss: 7.2546902Losses:  2.476078748703003 2.4377360343933105 0.03834272548556328
CurrentTrain: epoch  7, batch     1 | loss: 2.4760787Losses:  5.769362926483154 5.740585803985596 0.028777359053492546
CurrentTrain: epoch  8, batch     0 | loss: 5.7693629Losses:  2.1391334533691406 2.107602119445801 0.031531304121017456
CurrentTrain: epoch  8, batch     1 | loss: 2.1391335Losses:  5.70515251159668 5.671049118041992 0.03410332649946213
CurrentTrain: epoch  9, batch     0 | loss: 5.7051525Losses:  2.6437301635742188 2.6162924766540527 0.02743772603571415
CurrentTrain: epoch  9, batch     1 | loss: 2.6437302
Losses:  0.23504230380058289 -0.0 0.23504230380058289
MemoryTrain:  epoch  0, batch     0 | loss: 0.2350423Losses:  0.23235580325126648 -0.0 0.23235580325126648
MemoryTrain:  epoch  1, batch     0 | loss: 0.2323558Losses:  0.2199312299489975 -0.0 0.2199312299489975
MemoryTrain:  epoch  2, batch     0 | loss: 0.2199312Losses:  0.2152343988418579 -0.0 0.2152343988418579
MemoryTrain:  epoch  3, batch     0 | loss: 0.2152344Losses:  0.20920990407466888 -0.0 0.20920990407466888
MemoryTrain:  epoch  4, batch     0 | loss: 0.2092099Losses:  0.20766696333885193 -0.0 0.20766696333885193
MemoryTrain:  epoch  5, batch     0 | loss: 0.2076670Losses:  0.20498642325401306 -0.0 0.20498642325401306
MemoryTrain:  epoch  6, batch     0 | loss: 0.2049864Losses:  0.20403116941452026 -0.0 0.20403116941452026
MemoryTrain:  epoch  7, batch     0 | loss: 0.2040312Losses:  0.202254980802536 -0.0 0.202254980802536
MemoryTrain:  epoch  8, batch     0 | loss: 0.2022550Losses:  0.20105843245983124 -0.0 0.20105843245983124
MemoryTrain:  epoch  9, batch     0 | loss: 0.2010584
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 48.44%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 38.75%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 32.29%   [EVAL] batch:    6 | acc: 6.25%,  total acc: 28.57%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 25.00%   
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 12.50%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 9.38%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 11.25%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 10.42%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 18.75%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 27.34%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 32.64%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 38.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 42.05%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 44.79%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 48.08%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 49.55%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 51.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 51.56%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 52.57%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 53.12%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 53.95%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 55.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.04%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 59.94%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 61.68%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 63.02%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 64.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 65.87%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 66.90%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 67.86%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 68.96%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 69.76%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 70.51%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 71.02%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 70.71%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 69.10%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 67.23%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 65.46%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 63.94%   [EVAL] batch:   39 | acc: 0.00%,  total acc: 62.34%   
cur_acc:  ['0.8693', '0.2500']
his_acc:  ['0.8693', '0.6234']
Clustering into  7  clusters
Clusters:  [2 1 1 2 0 3 2 6 1 5 1 5 4 2 1 0]
Losses:  6.963287353515625 6.351313591003418 0.6119738817214966
CurrentTrain: epoch  0, batch     0 | loss: 6.9632874Losses:  2.8407020568847656 2.330873727798462 0.5098282694816589
CurrentTrain: epoch  0, batch     1 | loss: 2.8407021Losses:  8.066896438598633 7.765745162963867 0.3011511564254761
CurrentTrain: epoch  1, batch     0 | loss: 8.0668964Losses:  6.128623962402344 5.825949192047119 0.3026750087738037
CurrentTrain: epoch  1, batch     1 | loss: 6.1286240Losses:  6.622148513793945 6.1669206619262695 0.45522767305374146
CurrentTrain: epoch  2, batch     0 | loss: 6.6221485Losses:  1.6494536399841309 1.2054672241210938 0.4439864754676819
CurrentTrain: epoch  2, batch     1 | loss: 1.6494536Losses:  7.027838230133057 6.59688138961792 0.4309566915035248
CurrentTrain: epoch  3, batch     0 | loss: 7.0278382Losses:  2.168679714202881 1.728771686553955 0.4399080276489258
CurrentTrain: epoch  3, batch     1 | loss: 2.1686797Losses:  6.670239448547363 6.27858304977417 0.39165619015693665
CurrentTrain: epoch  4, batch     0 | loss: 6.6702394Losses:  1.9161357879638672 1.4939584732055664 0.4221773147583008
CurrentTrain: epoch  4, batch     1 | loss: 1.9161358Losses:  6.039828777313232 5.599277973175049 0.44055062532424927
CurrentTrain: epoch  5, batch     0 | loss: 6.0398288Losses:  2.2057292461395264 1.8441870212554932 0.3615421950817108
CurrentTrain: epoch  5, batch     1 | loss: 2.2057292Losses:  5.400197505950928 5.014688014984131 0.3855096995830536
CurrentTrain: epoch  6, batch     0 | loss: 5.4001975Losses:  1.404225468635559 1.0394232273101807 0.3648022711277008
CurrentTrain: epoch  6, batch     1 | loss: 1.4042255Losses:  5.950685501098633 5.577961444854736 0.37272411584854126
CurrentTrain: epoch  7, batch     0 | loss: 5.9506855Losses:  1.9320734739303589 1.5557832717895508 0.3762902319431305
CurrentTrain: epoch  7, batch     1 | loss: 1.9320735Losses:  6.611598014831543 6.246093273162842 0.3655048608779907
CurrentTrain: epoch  8, batch     0 | loss: 6.6115980Losses:  3.1925365924835205 2.8340556621551514 0.35848087072372437
CurrentTrain: epoch  8, batch     1 | loss: 3.1925366Losses:  6.618197441101074 6.259028434753418 0.35916876792907715
CurrentTrain: epoch  9, batch     0 | loss: 6.6181974Losses:  2.477590799331665 2.200138568878174 0.2774522006511688
CurrentTrain: epoch  9, batch     1 | loss: 2.4775908
Losses:  0.5612954497337341 -0.0 0.5612954497337341
MemoryTrain:  epoch  0, batch     0 | loss: 0.5612954Losses:  0.5506779551506042 -0.0 0.5506779551506042
MemoryTrain:  epoch  1, batch     0 | loss: 0.5506780Losses:  0.5427290797233582 -0.0 0.5427290797233582
MemoryTrain:  epoch  2, batch     0 | loss: 0.5427291Losses:  0.5382567644119263 -0.0 0.5382567644119263
MemoryTrain:  epoch  3, batch     0 | loss: 0.5382568Losses:  0.5313268899917603 -0.0 0.5313268899917603
MemoryTrain:  epoch  4, batch     0 | loss: 0.5313269Losses:  0.5220906138420105 -0.0 0.5220906138420105
MemoryTrain:  epoch  5, batch     0 | loss: 0.5220906Losses:  0.5239377021789551 -0.0 0.5239377021789551
MemoryTrain:  epoch  6, batch     0 | loss: 0.5239377Losses:  0.5190388560295105 -0.0 0.5190388560295105
MemoryTrain:  epoch  7, batch     0 | loss: 0.5190389Losses:  0.5135812759399414 -0.0 0.5135812759399414
MemoryTrain:  epoch  8, batch     0 | loss: 0.5135813Losses:  0.5063323974609375 -0.0 0.5063323974609375
MemoryTrain:  epoch  9, batch     0 | loss: 0.5063324
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 65.62%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 63.39%   [EVAL] batch:    7 | acc: 31.25%,  total acc: 59.38%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 56.94%   [EVAL] batch:    9 | acc: 0.00%,  total acc: 51.25%   [EVAL] batch:   10 | acc: 25.00%,  total acc: 48.86%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 51.56%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 52.40%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 51.79%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 21.88%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 26.25%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 25.00%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 32.14%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 39.84%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 45.14%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 48.75%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 49.43%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 52.08%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 54.33%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 54.91%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 54.58%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 54.30%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 54.41%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 52.78%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 53.62%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 54.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 56.85%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 58.81%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 60.60%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 61.98%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 63.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 64.90%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 65.97%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 68.32%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 68.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 69.96%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 70.70%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 71.69%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 70.54%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 68.92%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 67.06%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 65.30%   [EVAL] batch:   38 | acc: 0.00%,  total acc: 63.62%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 63.28%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 64.02%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 64.88%   [EVAL] batch:   42 | acc: 37.50%,  total acc: 64.24%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 64.06%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 63.33%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 62.77%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 62.77%   [EVAL] batch:   47 | acc: 18.75%,  total acc: 61.85%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 61.10%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 60.00%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 59.93%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 60.34%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 60.26%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 59.38%   
cur_acc:  ['0.8693', '0.2500', '0.5179']
his_acc:  ['0.8693', '0.6234', '0.5938']
Clustering into  9  clusters
Clusters:  [4 1 1 4 0 7 4 8 1 2 1 2 5 4 1 3 4 6 4 0 0]
Losses:  7.589801788330078 7.2351484298706055 0.3546534478664398
CurrentTrain: epoch  0, batch     0 | loss: 7.5898018Losses:  3.1450040340423584 2.8900959491729736 0.25490802526474
CurrentTrain: epoch  0, batch     1 | loss: 3.1450040Losses:  8.3502836227417 8.078615188598633 0.271668016910553
CurrentTrain: epoch  1, batch     0 | loss: 8.3502836Losses:  3.7007253170013428 3.4079127311706543 0.2928125858306885
CurrentTrain: epoch  1, batch     1 | loss: 3.7007253Losses:  7.180932521820068 6.94434118270874 0.23659151792526245
CurrentTrain: epoch  2, batch     0 | loss: 7.1809325Losses:  3.6259002685546875 3.3860890865325928 0.23981112241744995
CurrentTrain: epoch  2, batch     1 | loss: 3.6259003Losses:  8.690956115722656 8.483383178710938 0.20757296681404114
CurrentTrain: epoch  3, batch     0 | loss: 8.6909561Losses:  5.024706840515137 4.803223133087158 0.22148382663726807
CurrentTrain: epoch  3, batch     1 | loss: 5.0247068Losses:  6.495900630950928 6.305628776550293 0.1902720183134079
CurrentTrain: epoch  4, batch     0 | loss: 6.4959006Losses:  2.013956308364868 1.8433796167373657 0.17057672142982483
CurrentTrain: epoch  4, batch     1 | loss: 2.0139563Losses:  7.007537841796875 6.802912712097168 0.2046249806880951
CurrentTrain: epoch  5, batch     0 | loss: 7.0075378Losses:  2.232705593109131 2.064286947250366 0.1684187650680542
CurrentTrain: epoch  5, batch     1 | loss: 2.2327056Losses:  7.162684440612793 7.023296356201172 0.139387845993042
CurrentTrain: epoch  6, batch     0 | loss: 7.1626844Losses:  3.438751697540283 3.282346725463867 0.15640507638454437
CurrentTrain: epoch  6, batch     1 | loss: 3.4387517Losses:  6.228496551513672 6.085027694702148 0.1434686779975891
CurrentTrain: epoch  7, batch     0 | loss: 6.2284966Losses:  1.8236316442489624 1.645480751991272 0.17815089225769043
CurrentTrain: epoch  7, batch     1 | loss: 1.8236316Losses:  6.820745468139648 6.669327259063721 0.15141823887825012
CurrentTrain: epoch  8, batch     0 | loss: 6.8207455Losses:  1.8326749801635742 1.7008367776870728 0.13183820247650146
CurrentTrain: epoch  8, batch     1 | loss: 1.8326750Losses:  5.866056442260742 5.729795455932617 0.13626083731651306
CurrentTrain: epoch  9, batch     0 | loss: 5.8660564Losses:  1.7005995512008667 1.5633430480957031 0.1372564733028412
CurrentTrain: epoch  9, batch     1 | loss: 1.7005996
Losses:  0.3952215313911438 -0.0 0.3952215313911438
MemoryTrain:  epoch  0, batch     0 | loss: 0.3952215Losses:  0.42140722274780273 -0.0 0.42140722274780273
MemoryTrain:  epoch  0, batch     1 | loss: 0.4214072Losses:  0.7193834781646729 -0.0 0.7193834781646729
MemoryTrain:  epoch  1, batch     0 | loss: 0.7193835Losses:  0.42422714829444885 -0.0 0.42422714829444885
MemoryTrain:  epoch  1, batch     1 | loss: 0.4242271Losses:  0.6319288015365601 -0.0 0.6319288015365601
MemoryTrain:  epoch  2, batch     0 | loss: 0.6319288Losses:  0.295640230178833 -0.0 0.295640230178833
MemoryTrain:  epoch  2, batch     1 | loss: 0.2956402Losses:  0.7101019024848938 -0.0 0.7101019024848938
MemoryTrain:  epoch  3, batch     0 | loss: 0.7101019Losses:  0.2641504406929016 -0.0 0.2641504406929016
MemoryTrain:  epoch  3, batch     1 | loss: 0.2641504Losses:  0.6923050284385681 -0.0 0.6923050284385681
MemoryTrain:  epoch  4, batch     0 | loss: 0.6923050Losses:  0.4444889426231384 -0.0 0.4444889426231384
MemoryTrain:  epoch  4, batch     1 | loss: 0.4444889Losses:  0.6296597123146057 -0.0 0.6296597123146057
MemoryTrain:  epoch  5, batch     0 | loss: 0.6296597Losses:  0.15937873721122742 -0.0 0.15937873721122742
MemoryTrain:  epoch  5, batch     1 | loss: 0.1593787Losses:  0.5468756556510925 -0.0 0.5468756556510925
MemoryTrain:  epoch  6, batch     0 | loss: 0.5468757Losses:  0.36856937408447266 -0.0 0.36856937408447266
MemoryTrain:  epoch  6, batch     1 | loss: 0.3685694Losses:  0.6200031042098999 -0.0 0.6200031042098999
MemoryTrain:  epoch  7, batch     0 | loss: 0.6200031Losses:  0.3151911795139313 -0.0 0.3151911795139313
MemoryTrain:  epoch  7, batch     1 | loss: 0.3151912Losses:  0.5288753509521484 -0.0 0.5288753509521484
MemoryTrain:  epoch  8, batch     0 | loss: 0.5288754Losses:  0.4883262813091278 -0.0 0.4883262813091278
MemoryTrain:  epoch  8, batch     1 | loss: 0.4883263Losses:  0.5114850997924805 -0.0 0.5114850997924805
MemoryTrain:  epoch  9, batch     0 | loss: 0.5114851Losses:  0.41146165132522583 -0.0 0.41146165132522583
MemoryTrain:  epoch  9, batch     1 | loss: 0.4114617
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 71.59%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 72.40%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 74.04%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 75.89%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 77.50%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 78.52%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 79.78%   [EVAL] batch:   17 | acc: 18.75%,  total acc: 76.39%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 41.67%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 37.50%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 40.00%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 40.62%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 42.86%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 50.00%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 52.08%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 53.75%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 53.41%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 55.21%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 58.17%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 58.93%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 58.75%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 58.20%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 58.82%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 59.03%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 59.54%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 60.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 64.20%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 65.76%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 68.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 69.71%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 70.60%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 71.65%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 72.63%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 73.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 73.99%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 74.61%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 74.62%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 75.18%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 73.93%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 72.22%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 70.27%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 68.42%   [EVAL] batch:   38 | acc: 0.00%,  total acc: 66.67%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 66.25%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 67.07%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 67.41%   [EVAL] batch:   42 | acc: 25.00%,  total acc: 66.42%   [EVAL] batch:   43 | acc: 31.25%,  total acc: 65.62%   [EVAL] batch:   44 | acc: 12.50%,  total acc: 64.44%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 63.59%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 63.30%   [EVAL] batch:   47 | acc: 12.50%,  total acc: 62.24%   [EVAL] batch:   48 | acc: 0.00%,  total acc: 60.97%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 59.75%   [EVAL] batch:   50 | acc: 50.00%,  total acc: 59.56%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 59.98%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 59.91%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 60.19%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 60.45%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 60.71%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 60.75%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 60.99%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 61.23%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 61.46%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 61.99%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 61.39%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 61.71%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 61.82%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 62.02%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 63.06%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 63.60%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 64.04%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 64.55%   [EVAL] batch:   70 | acc: 43.75%,  total acc: 64.26%   
cur_acc:  ['0.8693', '0.2500', '0.5179', '0.7639']
his_acc:  ['0.8693', '0.6234', '0.5938', '0.6426']
Clustering into  12  clusters
Clusters:  [ 1  0  0  1  4  3  1 10  0  2  0  2 11  1  0  8  1  6  1  5  4  7  9  3
  5  1]
Losses:  8.727352142333984 8.324541091918945 0.4028113782405853
CurrentTrain: epoch  0, batch     0 | loss: 8.7273521Losses:  3.616471767425537 3.3134028911590576 0.30306878685951233
CurrentTrain: epoch  0, batch     1 | loss: 3.6164718Losses:  7.780190467834473 7.403754234313965 0.37643635272979736
CurrentTrain: epoch  1, batch     0 | loss: 7.7801905Losses:  3.502379894256592 3.1973118782043457 0.3050680160522461
CurrentTrain: epoch  1, batch     1 | loss: 3.5023799Losses:  6.990995407104492 6.623116493225098 0.367878794670105
CurrentTrain: epoch  2, batch     0 | loss: 6.9909954Losses:  3.478135108947754 3.1294403076171875 0.3486948311328888
CurrentTrain: epoch  2, batch     1 | loss: 3.4781351Losses:  8.46834659576416 8.117460250854492 0.3508860766887665
CurrentTrain: epoch  3, batch     0 | loss: 8.4683466Losses:  3.3654940128326416 3.0884082317352295 0.27708572149276733
CurrentTrain: epoch  3, batch     1 | loss: 3.3654940Losses:  5.91087532043457 5.570076942443848 0.3407982289791107
CurrentTrain: epoch  4, batch     0 | loss: 5.9108753Losses:  1.9691587686538696 1.6245957612991333 0.34456297755241394
CurrentTrain: epoch  4, batch     1 | loss: 1.9691588Losses:  6.756589889526367 6.423253536224365 0.33333656191825867
CurrentTrain: epoch  5, batch     0 | loss: 6.7565899Losses:  2.568934679031372 2.2945077419281006 0.2744269073009491
CurrentTrain: epoch  5, batch     1 | loss: 2.5689347Losses:  7.034173011779785 6.67952823638916 0.35464462637901306
CurrentTrain: epoch  6, batch     0 | loss: 7.0341730Losses:  2.279562473297119 2.0001220703125 0.27944040298461914
CurrentTrain: epoch  6, batch     1 | loss: 2.2795625Losses:  6.126449108123779 5.797750473022461 0.32869869470596313
CurrentTrain: epoch  7, batch     0 | loss: 6.1264491Losses:  3.4059462547302246 3.06917142868042 0.3367748260498047
CurrentTrain: epoch  7, batch     1 | loss: 3.4059463Losses:  7.0402421951293945 6.716868877410889 0.3233732283115387
CurrentTrain: epoch  8, batch     0 | loss: 7.0402422Losses:  4.043050289154053 3.777012348175049 0.2660379409790039
CurrentTrain: epoch  8, batch     1 | loss: 4.0430503Losses:  5.442006587982178 5.123699188232422 0.31830719113349915
CurrentTrain: epoch  9, batch     0 | loss: 5.4420066Losses:  1.736109733581543 1.40636146068573 0.3297483026981354
CurrentTrain: epoch  9, batch     1 | loss: 1.7361097
Losses:  0.9015428423881531 -0.0 0.9015428423881531
MemoryTrain:  epoch  0, batch     0 | loss: 0.9015428Losses:  0.5250434875488281 -0.0 0.5250434875488281
MemoryTrain:  epoch  0, batch     1 | loss: 0.5250435Losses:  0.8236227035522461 -0.0 0.8236227035522461
MemoryTrain:  epoch  1, batch     0 | loss: 0.8236227Losses:  0.64625084400177 -0.0 0.64625084400177
MemoryTrain:  epoch  1, batch     1 | loss: 0.6462508Losses:  0.8474099636077881 -0.0 0.8474099636077881
MemoryTrain:  epoch  2, batch     0 | loss: 0.8474100Losses:  0.5983343124389648 -0.0 0.5983343124389648
MemoryTrain:  epoch  2, batch     1 | loss: 0.5983343Losses:  0.7350540161132812 -0.0 0.7350540161132812
MemoryTrain:  epoch  3, batch     0 | loss: 0.7350540Losses:  0.47543808817863464 -0.0 0.47543808817863464
MemoryTrain:  epoch  3, batch     1 | loss: 0.4754381Losses:  0.6456732749938965 -0.0 0.6456732749938965
MemoryTrain:  epoch  4, batch     0 | loss: 0.6456733Losses:  0.6324874758720398 -0.0 0.6324874758720398
MemoryTrain:  epoch  4, batch     1 | loss: 0.6324875Losses:  0.7200582027435303 -0.0 0.7200582027435303
MemoryTrain:  epoch  5, batch     0 | loss: 0.7200582Losses:  0.48350706696510315 -0.0 0.48350706696510315
MemoryTrain:  epoch  5, batch     1 | loss: 0.4835071Losses:  0.7748250365257263 -0.0 0.7748250365257263
MemoryTrain:  epoch  6, batch     0 | loss: 0.7748250Losses:  0.5483234524726868 -0.0 0.5483234524726868
MemoryTrain:  epoch  6, batch     1 | loss: 0.5483235Losses:  0.8280585408210754 -0.0 0.8280585408210754
MemoryTrain:  epoch  7, batch     0 | loss: 0.8280585Losses:  0.46317562460899353 -0.0 0.46317562460899353
MemoryTrain:  epoch  7, batch     1 | loss: 0.4631756Losses:  0.6949296593666077 -0.0 0.6949296593666077
MemoryTrain:  epoch  8, batch     0 | loss: 0.6949297Losses:  0.46715354919433594 -0.0 0.46715354919433594
MemoryTrain:  epoch  8, batch     1 | loss: 0.4671535Losses:  0.7479628920555115 -0.0 0.7479628920555115
MemoryTrain:  epoch  9, batch     0 | loss: 0.7479629Losses:  0.5280870795249939 -0.0 0.5280870795249939
MemoryTrain:  epoch  9, batch     1 | loss: 0.5280871
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 41.67%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 55.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 60.42%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 64.29%   [EVAL] batch:    7 | acc: 31.25%,  total acc: 60.16%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 59.03%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 56.88%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 54.55%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 54.17%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 50.96%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 46.88%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 48.75%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 51.04%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 55.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 60.94%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 63.19%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 65.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 67.05%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 68.23%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 69.71%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 69.64%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 68.33%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 67.19%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 66.54%   [EVAL] batch:   17 | acc: 37.50%,  total acc: 64.93%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 65.13%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 67.26%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 70.11%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 71.35%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.56%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 73.84%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 74.78%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 75.43%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 75.21%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 75.40%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 76.17%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 76.14%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 76.29%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 74.82%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 73.09%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 71.28%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 69.41%   [EVAL] batch:   38 | acc: 0.00%,  total acc: 67.63%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 67.19%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 67.84%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 68.15%   [EVAL] batch:   42 | acc: 25.00%,  total acc: 67.15%   [EVAL] batch:   43 | acc: 31.25%,  total acc: 66.34%   [EVAL] batch:   44 | acc: 6.25%,  total acc: 65.00%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 64.27%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 63.70%   [EVAL] batch:   47 | acc: 6.25%,  total acc: 62.50%   [EVAL] batch:   48 | acc: 0.00%,  total acc: 61.22%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 60.12%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 60.17%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 60.58%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 60.50%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 60.30%   [EVAL] batch:   54 | acc: 37.50%,  total acc: 59.89%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 59.93%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 59.87%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 59.59%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 59.43%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 59.48%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 60.14%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 59.58%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 59.52%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 59.47%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 59.71%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 60.04%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 60.63%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 61.21%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 61.78%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 62.32%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 62.15%   [EVAL] batch:   71 | acc: 31.25%,  total acc: 61.72%   [EVAL] batch:   72 | acc: 56.25%,  total acc: 61.64%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 61.40%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 61.67%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 62.17%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 62.34%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 62.10%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 61.88%   [EVAL] batch:   80 | acc: 43.75%,  total acc: 61.65%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 61.20%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 60.99%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 60.27%   
cur_acc:  ['0.8693', '0.2500', '0.5179', '0.7639', '0.5096']
his_acc:  ['0.8693', '0.6234', '0.5938', '0.6426', '0.6027']
Clustering into  14  clusters
Clusters:  [ 0  9  1  0  4  7  0 11  2  6  2  6 10  0  1 12  0  5  0  3  4  8  1  7
  3  0 13  0  3  9  0]
Losses:  7.2409210205078125 6.934039115905762 0.306881844997406
CurrentTrain: epoch  0, batch     0 | loss: 7.2409210Losses:  2.980442523956299 2.6846933364868164 0.2957491874694824
CurrentTrain: epoch  0, batch     1 | loss: 2.9804425Losses:  8.671420097351074 8.418777465820312 0.2526422441005707
CurrentTrain: epoch  1, batch     0 | loss: 8.6714201Losses:  5.34442663192749 5.270401954650879 0.07402477413415909
CurrentTrain: epoch  1, batch     1 | loss: 5.3444266Losses:  7.028111457824707 6.778664588928223 0.24944686889648438
CurrentTrain: epoch  2, batch     0 | loss: 7.0281115Losses:  2.5692708492279053 2.323029041290283 0.2462417632341385
CurrentTrain: epoch  2, batch     1 | loss: 2.5692708Losses:  6.062493801116943 5.825522422790527 0.23697160184383392
CurrentTrain: epoch  3, batch     0 | loss: 6.0624938Losses:  2.525259494781494 2.28170108795166 0.24355830252170563
CurrentTrain: epoch  3, batch     1 | loss: 2.5252595Losses:  6.279616355895996 6.046716690063477 0.2328997701406479
CurrentTrain: epoch  4, batch     0 | loss: 6.2796164Losses:  2.6026716232299805 2.434854507446289 0.16781702637672424
CurrentTrain: epoch  4, batch     1 | loss: 2.6026716Losses:  5.941372394561768 5.709939002990723 0.23143330216407776
CurrentTrain: epoch  5, batch     0 | loss: 5.9413724Losses:  1.9074479341506958 1.6812708377838135 0.22617705166339874
CurrentTrain: epoch  5, batch     1 | loss: 1.9074479Losses:  5.409068584442139 5.188748359680176 0.22032023966312408
CurrentTrain: epoch  6, batch     0 | loss: 5.4090686Losses:  1.535151481628418 1.298818588256836 0.23633284866809845
CurrentTrain: epoch  6, batch     1 | loss: 1.5351515Losses:  5.614502429962158 5.39204216003418 0.22246012091636658
CurrentTrain: epoch  7, batch     0 | loss: 5.6145024Losses:  2.413024425506592 2.1891121864318848 0.22391220927238464
CurrentTrain: epoch  7, batch     1 | loss: 2.4130244Losses:  6.495049476623535 6.276432037353516 0.21861739456653595
CurrentTrain: epoch  8, batch     0 | loss: 6.4950495Losses:  3.0765209197998047 2.939652681350708 0.13686811923980713
CurrentTrain: epoch  8, batch     1 | loss: 3.0765209Losses:  7.123272895812988 6.907977104187012 0.21529555320739746
CurrentTrain: epoch  9, batch     0 | loss: 7.1232729Losses:  3.7004613876342773 3.4832077026367188 0.21725359559059143
CurrentTrain: epoch  9, batch     1 | loss: 3.7004614
Losses:  0.8623920679092407 -0.0 0.8623920679092407
MemoryTrain:  epoch  0, batch     0 | loss: 0.8623921Losses:  0.7446939945220947 -0.0 0.7446939945220947
MemoryTrain:  epoch  0, batch     1 | loss: 0.7446940Losses:  0.8595708012580872 -0.0 0.8595708012580872
MemoryTrain:  epoch  1, batch     0 | loss: 0.8595708Losses:  0.7711060047149658 -0.0 0.7711060047149658
MemoryTrain:  epoch  1, batch     1 | loss: 0.7711060Losses:  0.7266371846199036 -0.0 0.7266371846199036
MemoryTrain:  epoch  2, batch     0 | loss: 0.7266372Losses:  0.8022980093955994 -0.0 0.8022980093955994
MemoryTrain:  epoch  2, batch     1 | loss: 0.8022980Losses:  0.7625967264175415 -0.0 0.7625967264175415
MemoryTrain:  epoch  3, batch     0 | loss: 0.7625967Losses:  0.7980337738990784 -0.0 0.7980337738990784
MemoryTrain:  epoch  3, batch     1 | loss: 0.7980338Losses:  0.7892263531684875 -0.0 0.7892263531684875
MemoryTrain:  epoch  4, batch     0 | loss: 0.7892264Losses:  0.7775694727897644 -0.0 0.7775694727897644
MemoryTrain:  epoch  4, batch     1 | loss: 0.7775695Losses:  0.8811910152435303 -0.0 0.8811910152435303
MemoryTrain:  epoch  5, batch     0 | loss: 0.8811910Losses:  0.7523896098136902 -0.0 0.7523896098136902
MemoryTrain:  epoch  5, batch     1 | loss: 0.7523896Losses:  0.7611596584320068 -0.0 0.7611596584320068
MemoryTrain:  epoch  6, batch     0 | loss: 0.7611597Losses:  0.8420367240905762 -0.0 0.8420367240905762
MemoryTrain:  epoch  6, batch     1 | loss: 0.8420367Losses:  0.6782267689704895 -0.0 0.6782267689704895
MemoryTrain:  epoch  7, batch     0 | loss: 0.6782268Losses:  0.7056035995483398 -0.0 0.7056035995483398
MemoryTrain:  epoch  7, batch     1 | loss: 0.7056036Losses:  0.6730400919914246 -0.0 0.6730400919914246
MemoryTrain:  epoch  8, batch     0 | loss: 0.6730401Losses:  0.7223765254020691 -0.0 0.7223765254020691
MemoryTrain:  epoch  8, batch     1 | loss: 0.7223765Losses:  0.8197015523910522 -0.0 0.8197015523910522
MemoryTrain:  epoch  9, batch     0 | loss: 0.8197016Losses:  0.6624404191970825 -0.0 0.6624404191970825
MemoryTrain:  epoch  9, batch     1 | loss: 0.6624404
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 71.59%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 69.27%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 69.71%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 66.07%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 41.67%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 42.19%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 45.00%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 42.71%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 45.54%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 46.88%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 49.31%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 51.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 53.98%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 54.69%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 56.73%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 57.14%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 56.67%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:   17 | acc: 31.25%,  total acc: 54.86%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 55.26%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 55.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.04%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 59.94%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 61.68%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 63.28%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 64.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 66.11%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 66.90%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 68.08%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 69.18%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 69.38%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 69.96%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 70.70%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 71.02%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 70.77%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 69.64%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 68.06%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 66.55%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 64.80%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 63.62%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 63.44%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 64.33%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 64.88%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 64.83%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 64.49%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 64.17%   [EVAL] batch:   45 | acc: 43.75%,  total acc: 63.72%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 64.23%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 63.67%   [EVAL] batch:   48 | acc: 6.25%,  total acc: 62.50%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 61.25%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 61.15%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 61.54%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 61.56%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 61.46%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 61.59%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 62.05%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 62.17%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 62.07%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 61.86%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 61.88%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 62.40%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 61.69%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 60.71%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 59.86%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 59.90%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 60.32%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 60.91%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 61.49%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 62.05%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 62.59%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   71 | acc: 25.00%,  total acc: 61.98%   [EVAL] batch:   72 | acc: 56.25%,  total acc: 61.90%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 61.66%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 61.92%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 62.42%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 62.66%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 62.58%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 62.18%   [EVAL] batch:   79 | acc: 25.00%,  total acc: 61.72%   [EVAL] batch:   80 | acc: 31.25%,  total acc: 61.34%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 60.98%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 60.62%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 60.71%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 60.81%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 60.83%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 60.92%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 61.08%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 61.31%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 61.25%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 61.47%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 61.62%   [EVAL] batch:   92 | acc: 62.50%,  total acc: 61.63%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 61.90%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 61.71%   [EVAL] batch:   95 | acc: 75.00%,  total acc: 61.85%   [EVAL] batch:   96 | acc: 18.75%,  total acc: 61.40%   
cur_acc:  ['0.8693', '0.2500', '0.5179', '0.7639', '0.5096', '0.6607']
his_acc:  ['0.8693', '0.6234', '0.5938', '0.6426', '0.6027', '0.6140']
Clustering into  17  clusters
Clusters:  [ 2  1  0  2 16  4  2 13  0  6 14  6 10  2  0 12  2 11  7  3 15  9  5  4
  3  7  8  2  3  1  7  0  0  0  4  3]
Losses:  7.211300849914551 6.988387584686279 0.22291317582130432
CurrentTrain: epoch  0, batch     0 | loss: 7.2113008Losses:  2.259798288345337 2.0563576221466064 0.20344075560569763
CurrentTrain: epoch  0, batch     1 | loss: 2.2597983Losses:  7.914314270019531 7.716527938842773 0.19778621196746826
CurrentTrain: epoch  1, batch     0 | loss: 7.9143143Losses:  2.878448009490967 2.672968626022339 0.20547926425933838
CurrentTrain: epoch  1, batch     1 | loss: 2.8784480Losses:  7.909628391265869 7.743597030639648 0.16603142023086548
CurrentTrain: epoch  2, batch     0 | loss: 7.9096284Losses:  2.22825288772583 2.0699684619903564 0.1582845151424408
CurrentTrain: epoch  2, batch     1 | loss: 2.2282529Losses:  8.1231689453125 7.970567226409912 0.1526014506816864
CurrentTrain: epoch  3, batch     0 | loss: 8.1231689Losses:  2.809703826904297 2.6208560466766357 0.18884778022766113
CurrentTrain: epoch  3, batch     1 | loss: 2.8097038Losses:  7.884876251220703 7.733176231384277 0.15170004963874817
CurrentTrain: epoch  4, batch     0 | loss: 7.8848763Losses:  2.061344861984253 1.9178643226623535 0.14348064363002777
CurrentTrain: epoch  4, batch     1 | loss: 2.0613449Losses:  8.43637752532959 8.298151016235352 0.13822630047798157
CurrentTrain: epoch  5, batch     0 | loss: 8.4363775Losses:  3.0621628761291504 2.925074577331543 0.13708841800689697
CurrentTrain: epoch  5, batch     1 | loss: 3.0621629Losses:  6.495009899139404 6.363499641418457 0.13151010870933533
CurrentTrain: epoch  6, batch     0 | loss: 6.4950099Losses:  3.0211713314056396 2.884345531463623 0.1368257701396942
CurrentTrain: epoch  6, batch     1 | loss: 3.0211713Losses:  8.576101303100586 8.450898170471191 0.12520337104797363
CurrentTrain: epoch  7, batch     0 | loss: 8.5761013Losses:  9.234365463256836 9.234365463256836 -0.0
CurrentTrain: epoch  7, batch     1 | loss: 9.2343655Losses:  8.498568534851074 8.372697830200195 0.12587058544158936
CurrentTrain: epoch  8, batch     0 | loss: 8.4985685Losses:  2.4946842193603516 2.3687028884887695 0.125981405377388
CurrentTrain: epoch  8, batch     1 | loss: 2.4946842Losses:  7.304281234741211 7.182248115539551 0.1220330223441124
CurrentTrain: epoch  9, batch     0 | loss: 7.3042812Losses:  3.2568676471710205 3.188051462173462 0.06881614029407501
CurrentTrain: epoch  9, batch     1 | loss: 3.2568676
Losses:  0.9708963632583618 -0.0 0.9708963632583618
MemoryTrain:  epoch  0, batch     0 | loss: 0.9708964Losses:  0.8157379627227783 -0.0 0.8157379627227783
MemoryTrain:  epoch  0, batch     1 | loss: 0.8157380Losses:  0.4665866494178772 -0.0 0.4665866494178772
MemoryTrain:  epoch  0, batch     2 | loss: 0.4665866Losses:  0.8493303656578064 -0.0 0.8493303656578064
MemoryTrain:  epoch  1, batch     0 | loss: 0.8493304Losses:  0.8284240961074829 -0.0 0.8284240961074829
MemoryTrain:  epoch  1, batch     1 | loss: 0.8284241Losses:  0.4826239049434662 -0.0 0.4826239049434662
MemoryTrain:  epoch  1, batch     2 | loss: 0.4826239Losses:  0.7618497610092163 -0.0 0.7618497610092163
MemoryTrain:  epoch  2, batch     0 | loss: 0.7618498Losses:  0.7644244432449341 -0.0 0.7644244432449341
MemoryTrain:  epoch  2, batch     1 | loss: 0.7644244Losses:  0.263724148273468 -0.0 0.263724148273468
MemoryTrain:  epoch  2, batch     2 | loss: 0.2637241Losses:  0.8812076449394226 -0.0 0.8812076449394226
MemoryTrain:  epoch  3, batch     0 | loss: 0.8812076Losses:  0.8609252572059631 -0.0 0.8609252572059631
MemoryTrain:  epoch  3, batch     1 | loss: 0.8609253Losses:  0.2962096333503723 -0.0 0.2962096333503723
MemoryTrain:  epoch  3, batch     2 | loss: 0.2962096Losses:  0.7610459923744202 -0.0 0.7610459923744202
MemoryTrain:  epoch  4, batch     0 | loss: 0.7610460Losses:  0.9977856874465942 -0.0 0.9977856874465942
MemoryTrain:  epoch  4, batch     1 | loss: 0.9977857Losses:  0.23852679133415222 -0.0 0.23852679133415222
MemoryTrain:  epoch  4, batch     2 | loss: 0.2385268Losses:  0.7599430084228516 -0.0 0.7599430084228516
MemoryTrain:  epoch  5, batch     0 | loss: 0.7599430Losses:  0.8867385387420654 -0.0 0.8867385387420654
MemoryTrain:  epoch  5, batch     1 | loss: 0.8867385Losses:  0.18799740076065063 -0.0 0.18799740076065063
MemoryTrain:  epoch  5, batch     2 | loss: 0.1879974Losses:  0.7601232528686523 -0.0 0.7601232528686523
MemoryTrain:  epoch  6, batch     0 | loss: 0.7601233Losses:  0.7585347294807434 -0.0 0.7585347294807434
MemoryTrain:  epoch  6, batch     1 | loss: 0.7585347Losses:  0.2835139036178589 -0.0 0.2835139036178589
MemoryTrain:  epoch  6, batch     2 | loss: 0.2835139Losses:  0.8454569578170776 -0.0 0.8454569578170776
MemoryTrain:  epoch  7, batch     0 | loss: 0.8454570Losses:  0.938443660736084 -0.0 0.938443660736084
MemoryTrain:  epoch  7, batch     1 | loss: 0.9384437Losses:  0.3503967225551605 -0.0 0.3503967225551605
MemoryTrain:  epoch  7, batch     2 | loss: 0.3503967Losses:  0.8847988843917847 -0.0 0.8847988843917847
MemoryTrain:  epoch  8, batch     0 | loss: 0.8847989Losses:  0.6736840605735779 -0.0 0.6736840605735779
MemoryTrain:  epoch  8, batch     1 | loss: 0.6736841Losses:  0.07601827383041382 -0.0 0.07601827383041382
MemoryTrain:  epoch  8, batch     2 | loss: 0.0760183Losses:  0.9176506400108337 -0.0 0.9176506400108337
MemoryTrain:  epoch  9, batch     0 | loss: 0.9176506Losses:  0.7041510343551636 -0.0 0.7041510343551636
MemoryTrain:  epoch  9, batch     1 | loss: 0.7041510Losses:  0.4856068193912506 -0.0 0.4856068193912506
MemoryTrain:  epoch  9, batch     2 | loss: 0.4856068
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 22.92%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 20.31%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 18.75%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 16.67%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 17.86%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 21.09%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 25.00%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 29.38%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 30.11%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 31.25%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 32.69%   [EVAL] batch:   13 | acc: 6.25%,  total acc: 30.80%   [EVAL] batch:   14 | acc: 25.00%,  total acc: 30.42%   [EVAL] batch:   15 | acc: 37.50%,  total acc: 30.86%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 32.35%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 31.94%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 32.24%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 33.44%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 34.82%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 34.94%   
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 29.17%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 32.81%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 35.00%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 32.29%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 41.07%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 45.31%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 48.61%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 51.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 54.55%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 56.25%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 55.77%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 54.46%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 54.58%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 54.30%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 54.41%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 54.51%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 54.93%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 55.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.04%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 59.94%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 61.68%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 63.28%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 64.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 66.11%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 66.90%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 67.86%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 69.17%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 69.76%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 70.51%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 70.77%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 69.46%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 67.88%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 66.05%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 64.31%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 62.82%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 63.41%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 64.14%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 63.95%   [EVAL] batch:   43 | acc: 43.75%,  total acc: 63.49%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 62.92%   [EVAL] batch:   45 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 62.23%   [EVAL] batch:   47 | acc: 18.75%,  total acc: 61.33%   [EVAL] batch:   48 | acc: 0.00%,  total acc: 60.08%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 58.88%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 58.95%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 59.38%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 59.32%   [EVAL] batch:   53 | acc: 31.25%,  total acc: 58.80%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 58.98%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 58.82%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 58.77%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 58.30%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 58.37%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 58.65%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 59.22%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 58.57%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 57.64%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 56.74%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 57.02%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 57.67%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 58.30%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 58.92%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 59.51%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 60.09%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 60.04%   [EVAL] batch:   71 | acc: 18.75%,  total acc: 59.46%   [EVAL] batch:   72 | acc: 43.75%,  total acc: 59.25%   [EVAL] batch:   73 | acc: 31.25%,  total acc: 58.87%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 58.75%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 58.96%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 59.09%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 58.97%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 58.23%   [EVAL] batch:   79 | acc: 12.50%,  total acc: 57.66%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 57.02%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 56.71%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 56.33%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 56.32%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 55.88%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 55.96%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 55.53%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 55.11%   [EVAL] batch:   88 | acc: 37.50%,  total acc: 54.92%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 54.65%   [EVAL] batch:   90 | acc: 43.75%,  total acc: 54.53%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 54.62%   [EVAL] batch:   92 | acc: 50.00%,  total acc: 54.57%   [EVAL] batch:   93 | acc: 81.25%,  total acc: 54.85%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 54.61%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 54.56%   [EVAL] batch:   96 | acc: 31.25%,  total acc: 54.32%   [EVAL] batch:   97 | acc: 18.75%,  total acc: 53.95%   [EVAL] batch:   98 | acc: 43.75%,  total acc: 53.85%   [EVAL] batch:   99 | acc: 6.25%,  total acc: 53.37%   [EVAL] batch:  100 | acc: 6.25%,  total acc: 52.91%   [EVAL] batch:  101 | acc: 18.75%,  total acc: 52.57%   [EVAL] batch:  102 | acc: 0.00%,  total acc: 52.06%   [EVAL] batch:  103 | acc: 25.00%,  total acc: 51.80%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 51.85%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 51.89%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 52.04%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 51.85%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 51.83%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 51.70%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 51.41%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 51.17%   [EVAL] batch:  112 | acc: 25.00%,  total acc: 50.94%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 51.04%   [EVAL] batch:  114 | acc: 25.00%,  total acc: 50.82%   [EVAL] batch:  115 | acc: 50.00%,  total acc: 50.81%   [EVAL] batch:  116 | acc: 50.00%,  total acc: 50.80%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 51.01%   [EVAL] batch:  118 | acc: 12.50%,  total acc: 50.68%   
cur_acc:  ['0.8693', '0.2500', '0.5179', '0.7639', '0.5096', '0.6607', '0.3494']
his_acc:  ['0.8693', '0.6234', '0.5938', '0.6426', '0.6027', '0.6140', '0.5068']
Clustering into  19  clusters
Clusters:  [ 0  4  5  0  2 11  0 14  5  6 16  6 17  0  5 12  0 15  7  1  2  3 10 11
  1  7 13  0  1  4  7  5  5 18 11  1  0  3  9  8  1]
Losses:  9.375459671020508 8.875917434692383 0.49954208731651306
CurrentTrain: epoch  0, batch     0 | loss: 9.3754597Losses:  4.393245220184326 4.101141452789307 0.29210376739501953
CurrentTrain: epoch  0, batch     1 | loss: 4.3932452Losses:  6.956559658050537 6.53841495513916 0.4181447923183441
CurrentTrain: epoch  1, batch     0 | loss: 6.9565597Losses:  2.1871583461761475 1.7513947486877441 0.43576356768608093
CurrentTrain: epoch  1, batch     1 | loss: 2.1871583Losses:  6.936755657196045 6.535133361816406 0.40162238478660583
CurrentTrain: epoch  2, batch     0 | loss: 6.9367557Losses:  2.9336905479431152 2.5069918632507324 0.42669877409935
CurrentTrain: epoch  2, batch     1 | loss: 2.9336905Losses:  6.505063056945801 6.111969470977783 0.3930937945842743
CurrentTrain: epoch  3, batch     0 | loss: 6.5050631Losses:  1.6782691478729248 1.2529969215393066 0.4252721965312958
CurrentTrain: epoch  3, batch     1 | loss: 1.6782691Losses:  7.660078048706055 7.268269062042236 0.39180895686149597
CurrentTrain: epoch  4, batch     0 | loss: 7.6600780Losses:  2.5743329524993896 2.140246868133545 0.4340860843658447
CurrentTrain: epoch  4, batch     1 | loss: 2.5743330Losses:  6.958222389221191 6.622034072875977 0.33618855476379395
CurrentTrain: epoch  5, batch     0 | loss: 6.9582224Losses:  4.3093180656433105 3.89225697517395 0.4170610010623932
CurrentTrain: epoch  5, batch     1 | loss: 4.3093181Losses:  6.153277397155762 5.796627521514893 0.3566496968269348
CurrentTrain: epoch  6, batch     0 | loss: 6.1532774Losses:  3.094722270965576 2.6569175720214844 0.43780460953712463
CurrentTrain: epoch  6, batch     1 | loss: 3.0947223Losses:  6.841477394104004 6.408179759979248 0.4332977831363678
CurrentTrain: epoch  7, batch     0 | loss: 6.8414774Losses:  2.3471481800079346 2.1091294288635254 0.23801879584789276
CurrentTrain: epoch  7, batch     1 | loss: 2.3471482Losses:  6.677731990814209 6.284667015075684 0.393064945936203
CurrentTrain: epoch  8, batch     0 | loss: 6.6777320Losses:  2.266336679458618 1.9236327409744263 0.34270399808883667
CurrentTrain: epoch  8, batch     1 | loss: 2.2663367Losses:  6.415868759155273 6.001721382141113 0.4141474664211273
CurrentTrain: epoch  9, batch     0 | loss: 6.4158688Losses:  1.8650789260864258 1.5081547498703003 0.3569242060184479
CurrentTrain: epoch  9, batch     1 | loss: 1.8650789
Losses:  1.0536081790924072 -0.0 1.0536081790924072
MemoryTrain:  epoch  0, batch     0 | loss: 1.0536082Losses:  0.8473389148712158 -0.0 0.8473389148712158
MemoryTrain:  epoch  0, batch     1 | loss: 0.8473389Losses:  0.6663014888763428 -0.0 0.6663014888763428
MemoryTrain:  epoch  0, batch     2 | loss: 0.6663015Losses:  0.8790024518966675 -0.0 0.8790024518966675
MemoryTrain:  epoch  1, batch     0 | loss: 0.8790025Losses:  0.8299806714057922 -0.0 0.8299806714057922
MemoryTrain:  epoch  1, batch     1 | loss: 0.8299807Losses:  0.6620510220527649 -0.0 0.6620510220527649
MemoryTrain:  epoch  1, batch     2 | loss: 0.6620510Losses:  0.7844874262809753 -0.0 0.7844874262809753
MemoryTrain:  epoch  2, batch     0 | loss: 0.7844874Losses:  0.8274101614952087 -0.0 0.8274101614952087
MemoryTrain:  epoch  2, batch     1 | loss: 0.8274102Losses:  0.6375447511672974 -0.0 0.6375447511672974
MemoryTrain:  epoch  2, batch     2 | loss: 0.6375448Losses:  0.8019510507583618 -0.0 0.8019510507583618
MemoryTrain:  epoch  3, batch     0 | loss: 0.8019511Losses:  0.8808619976043701 -0.0 0.8808619976043701
MemoryTrain:  epoch  3, batch     1 | loss: 0.8808620Losses:  0.6284603476524353 -0.0 0.6284603476524353
MemoryTrain:  epoch  3, batch     2 | loss: 0.6284603Losses:  0.794607400894165 -0.0 0.794607400894165
MemoryTrain:  epoch  4, batch     0 | loss: 0.7946074Losses:  0.956558108329773 -0.0 0.956558108329773
MemoryTrain:  epoch  4, batch     1 | loss: 0.9565581Losses:  0.3915007412433624 -0.0 0.3915007412433624
MemoryTrain:  epoch  4, batch     2 | loss: 0.3915007Losses:  0.6054462194442749 -0.0 0.6054462194442749
MemoryTrain:  epoch  5, batch     0 | loss: 0.6054462Losses:  0.8630144000053406 -0.0 0.8630144000053406
MemoryTrain:  epoch  5, batch     1 | loss: 0.8630144Losses:  0.6563230752944946 -0.0 0.6563230752944946
MemoryTrain:  epoch  5, batch     2 | loss: 0.6563231Losses:  0.8466896414756775 -0.0 0.8466896414756775
MemoryTrain:  epoch  6, batch     0 | loss: 0.8466896Losses:  0.9403082728385925 -0.0 0.9403082728385925
MemoryTrain:  epoch  6, batch     1 | loss: 0.9403083Losses:  0.609870433807373 -0.0 0.609870433807373
MemoryTrain:  epoch  6, batch     2 | loss: 0.6098704Losses:  0.8097901344299316 -0.0 0.8097901344299316
MemoryTrain:  epoch  7, batch     0 | loss: 0.8097901Losses:  0.7784430384635925 -0.0 0.7784430384635925
MemoryTrain:  epoch  7, batch     1 | loss: 0.7784430Losses:  0.5877047777175903 -0.0 0.5877047777175903
MemoryTrain:  epoch  7, batch     2 | loss: 0.5877048Losses:  0.8766730427742004 -0.0 0.8766730427742004
MemoryTrain:  epoch  8, batch     0 | loss: 0.8766730Losses:  0.8597248792648315 -0.0 0.8597248792648315
MemoryTrain:  epoch  8, batch     1 | loss: 0.8597249Losses:  0.5356414318084717 -0.0 0.5356414318084717
MemoryTrain:  epoch  8, batch     2 | loss: 0.5356414Losses:  0.7278478145599365 -0.0 0.7278478145599365
MemoryTrain:  epoch  9, batch     0 | loss: 0.7278478Losses:  0.7412926554679871 -0.0 0.7412926554679871
MemoryTrain:  epoch  9, batch     1 | loss: 0.7412927Losses:  0.6486777663230896 -0.0 0.6486777663230896
MemoryTrain:  epoch  9, batch     2 | loss: 0.6486778
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 22.92%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 25.00%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 32.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 39.58%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 46.43%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 50.00%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 52.78%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 53.12%   [EVAL] batch:   10 | acc: 6.25%,  total acc: 48.86%   [EVAL] batch:   11 | acc: 6.25%,  total acc: 45.31%   [EVAL] batch:   12 | acc: 6.25%,  total acc: 42.31%   [EVAL] batch:   13 | acc: 6.25%,  total acc: 39.73%   [EVAL] batch:   14 | acc: 6.25%,  total acc: 37.50%   
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 16.67%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 15.62%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 15.00%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 12.50%   [EVAL] batch:    6 | acc: 6.25%,  total acc: 11.61%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 10.94%   [EVAL] batch:    8 | acc: 25.00%,  total acc: 12.50%   [EVAL] batch:    9 | acc: 6.25%,  total acc: 11.88%   [EVAL] batch:   10 | acc: 18.75%,  total acc: 12.50%   [EVAL] batch:   11 | acc: 18.75%,  total acc: 13.02%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 12.98%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 15.62%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 19.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 21.48%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 24.63%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 26.74%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 28.62%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 31.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 34.82%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 37.78%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 40.49%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 42.97%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 45.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 47.36%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 48.15%   [EVAL] batch:   27 | acc: 81.25%,  total acc: 49.33%   [EVAL] batch:   28 | acc: 81.25%,  total acc: 50.43%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 51.04%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 52.02%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 52.93%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 53.41%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 54.04%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 53.57%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 52.43%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 51.01%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 49.67%   [EVAL] batch:   38 | acc: 0.00%,  total acc: 48.40%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 48.44%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 49.39%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 50.45%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 50.58%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 50.57%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 50.42%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 50.41%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 51.20%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 50.91%   [EVAL] batch:   48 | acc: 6.25%,  total acc: 50.00%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 49.00%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 49.14%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 49.76%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:   53 | acc: 25.00%,  total acc: 49.54%   [EVAL] batch:   54 | acc: 43.75%,  total acc: 49.43%   [EVAL] batch:   55 | acc: 12.50%,  total acc: 48.77%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 48.46%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 47.95%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 47.78%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 47.81%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 48.46%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 47.98%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 47.22%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 46.48%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 46.92%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 47.73%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 48.41%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 49.17%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 49.91%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 50.62%   [EVAL] batch:   70 | acc: 43.75%,  total acc: 50.53%   [EVAL] batch:   71 | acc: 37.50%,  total acc: 50.35%   [EVAL] batch:   72 | acc: 62.50%,  total acc: 50.51%   [EVAL] batch:   73 | acc: 37.50%,  total acc: 50.34%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 50.58%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 50.82%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 51.06%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 51.04%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 50.40%   [EVAL] batch:   79 | acc: 6.25%,  total acc: 49.84%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 49.23%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 48.93%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 48.80%   [EVAL] batch:   83 | acc: 43.75%,  total acc: 48.74%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 48.31%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 47.89%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 47.41%   [EVAL] batch:   87 | acc: 0.00%,  total acc: 46.88%   [EVAL] batch:   88 | acc: 12.50%,  total acc: 46.49%   [EVAL] batch:   89 | acc: 37.50%,  total acc: 46.39%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 46.70%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 47.21%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 47.51%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 47.94%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 47.96%   [EVAL] batch:   95 | acc: 75.00%,  total acc: 48.24%   [EVAL] batch:   96 | acc: 6.25%,  total acc: 47.81%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 47.83%   [EVAL] batch:   98 | acc: 50.00%,  total acc: 47.85%   [EVAL] batch:   99 | acc: 12.50%,  total acc: 47.50%   [EVAL] batch:  100 | acc: 6.25%,  total acc: 47.09%   [EVAL] batch:  101 | acc: 18.75%,  total acc: 46.81%   [EVAL] batch:  102 | acc: 0.00%,  total acc: 46.36%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 46.39%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 46.55%   [EVAL] batch:  105 | acc: 37.50%,  total acc: 46.46%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 46.61%   [EVAL] batch:  107 | acc: 50.00%,  total acc: 46.64%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 46.90%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 46.76%   [EVAL] batch:  110 | acc: 6.25%,  total acc: 46.40%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 46.15%   [EVAL] batch:  112 | acc: 6.25%,  total acc: 45.80%   [EVAL] batch:  113 | acc: 43.75%,  total acc: 45.78%   [EVAL] batch:  114 | acc: 12.50%,  total acc: 45.49%   [EVAL] batch:  115 | acc: 56.25%,  total acc: 45.58%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 45.73%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 46.13%   [EVAL] batch:  118 | acc: 31.25%,  total acc: 46.01%   [EVAL] batch:  119 | acc: 25.00%,  total acc: 45.83%   [EVAL] batch:  120 | acc: 18.75%,  total acc: 45.61%   [EVAL] batch:  121 | acc: 31.25%,  total acc: 45.49%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 45.58%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 45.82%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 46.10%   [EVAL] batch:  125 | acc: 93.75%,  total acc: 46.48%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 46.56%   [EVAL] batch:  127 | acc: 75.00%,  total acc: 46.78%   [EVAL] batch:  128 | acc: 6.25%,  total acc: 46.46%   [EVAL] batch:  129 | acc: 6.25%,  total acc: 46.15%   [EVAL] batch:  130 | acc: 0.00%,  total acc: 45.80%   [EVAL] batch:  131 | acc: 6.25%,  total acc: 45.50%   [EVAL] batch:  132 | acc: 12.50%,  total acc: 45.25%   
cur_acc:  ['0.8693', '0.2500', '0.5179', '0.7639', '0.5096', '0.6607', '0.3494', '0.3750']
his_acc:  ['0.8693', '0.6234', '0.5938', '0.6426', '0.6027', '0.6140', '0.5068', '0.4525']
--------Round  4
seed:  500
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 5 6 4 2 1 3 0]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 1 1 0]
Losses:  14.084006309509277 13.508081436157227 0.5759246349334717
CurrentTrain: epoch  0, batch     0 | loss: 14.0840063Losses:  9.693352699279785 9.053647994995117 0.639704704284668
CurrentTrain: epoch  0, batch     1 | loss: 9.6933527Losses:  8.749811172485352 8.109371185302734 0.6404403448104858
CurrentTrain: epoch  0, batch     2 | loss: 8.7498112Losses:  9.136332511901855 8.502599716186523 0.633732795715332
CurrentTrain: epoch  0, batch     3 | loss: 9.1363325Losses:  9.670095443725586 9.031734466552734 0.6383613348007202
CurrentTrain: epoch  0, batch     4 | loss: 9.6700954Losses:  9.261054039001465 8.626192092895508 0.634861946105957
CurrentTrain: epoch  0, batch     5 | loss: 9.2610540Losses:  12.13790512084961 11.528895378112793 0.6090092658996582
CurrentTrain: epoch  0, batch     6 | loss: 12.1379051Losses:  10.11473274230957 9.507020950317383 0.6077115535736084
CurrentTrain: epoch  0, batch     7 | loss: 10.1147327Losses:  11.520594596862793 10.906097412109375 0.6144975423812866
CurrentTrain: epoch  0, batch     8 | loss: 11.5205946Losses:  12.992450714111328 12.417156219482422 0.5752947330474854
CurrentTrain: epoch  0, batch     9 | loss: 12.9924507Losses:  9.563554763793945 8.980541229248047 0.5830131769180298
CurrentTrain: epoch  0, batch    10 | loss: 9.5635548Losses:  9.90185832977295 9.335664749145508 0.566193699836731
CurrentTrain: epoch  0, batch    11 | loss: 9.9018583Losses:  7.685028553009033 7.1502227783203125 0.5348057150840759
CurrentTrain: epoch  0, batch    12 | loss: 7.6850286Losses:  8.679858207702637 8.13775634765625 0.5421015024185181
CurrentTrain: epoch  0, batch    13 | loss: 8.6798582Losses:  9.583856582641602 9.044527053833008 0.539330005645752
CurrentTrain: epoch  0, batch    14 | loss: 9.5838566Losses:  11.555606842041016 11.009848594665527 0.5457579493522644
CurrentTrain: epoch  0, batch    15 | loss: 11.5556068Losses:  11.034344673156738 10.49285888671875 0.5414861440658569
CurrentTrain: epoch  0, batch    16 | loss: 11.0343447Losses:  12.994956016540527 12.51466178894043 0.48029422760009766
CurrentTrain: epoch  0, batch    17 | loss: 12.9949560Losses:  10.317473411560059 9.831413269042969 0.4860602617263794
CurrentTrain: epoch  0, batch    18 | loss: 10.3174734Losses:  9.191044807434082 8.670309066772461 0.520735502243042
CurrentTrain: epoch  0, batch    19 | loss: 9.1910448Losses:  9.024641036987305 8.573034286499023 0.4516068696975708
CurrentTrain: epoch  0, batch    20 | loss: 9.0246410Losses:  9.915558815002441 9.488383293151855 0.4271758794784546
CurrentTrain: epoch  0, batch    21 | loss: 9.9155588Losses:  8.036253929138184 7.568036079406738 0.4682174026966095
CurrentTrain: epoch  0, batch    22 | loss: 8.0362539Losses:  8.961555480957031 8.523298263549805 0.43825680017471313
CurrentTrain: epoch  0, batch    23 | loss: 8.9615555Losses:  8.96448040008545 8.538562774658203 0.42591750621795654
CurrentTrain: epoch  0, batch    24 | loss: 8.9644804Losses:  8.625566482543945 8.260215759277344 0.3653508722782135
CurrentTrain: epoch  0, batch    25 | loss: 8.6255665Losses:  7.997488498687744 7.597433090209961 0.40005558729171753
CurrentTrain: epoch  0, batch    26 | loss: 7.9974885Losses:  8.675030708312988 8.250265121459961 0.4247658848762512
CurrentTrain: epoch  0, batch    27 | loss: 8.6750307Losses:  7.580151557922363 7.220664978027344 0.35948646068573
CurrentTrain: epoch  0, batch    28 | loss: 7.5801516Losses:  10.564891815185547 10.216268539428711 0.34862369298934937
CurrentTrain: epoch  0, batch    29 | loss: 10.5648918Losses:  8.078911781311035 7.772200584411621 0.30671149492263794
CurrentTrain: epoch  0, batch    30 | loss: 8.0789118Losses:  9.693232536315918 9.365493774414062 0.3277389407157898
CurrentTrain: epoch  0, batch    31 | loss: 9.6932325Losses:  9.581567764282227 9.234304428100586 0.3472636342048645
CurrentTrain: epoch  0, batch    32 | loss: 9.5815678Losses:  7.005077838897705 6.728431701660156 0.2766460180282593
CurrentTrain: epoch  0, batch    33 | loss: 7.0050778Losses:  10.18915843963623 9.92198371887207 0.26717430353164673
CurrentTrain: epoch  0, batch    34 | loss: 10.1891584Losses:  6.661771297454834 6.401266098022461 0.26050513982772827
CurrentTrain: epoch  0, batch    35 | loss: 6.6617713Losses:  6.680276393890381 6.459303379058838 0.22097307443618774
CurrentTrain: epoch  0, batch    36 | loss: 6.6802764Losses:  7.089560508728027 6.830501556396484 0.25905895233154297
CurrentTrain: epoch  0, batch    37 | loss: 7.0895605Losses:  11.293583869934082 11.094003677368164 0.1995800882577896
CurrentTrain: epoch  1, batch     0 | loss: 11.2935839Losses:  9.280797004699707 9.093666076660156 0.18713124096393585
CurrentTrain: epoch  1, batch     1 | loss: 9.2807970Losses:  9.002837181091309 8.850757598876953 0.15207962691783905
CurrentTrain: epoch  1, batch     2 | loss: 9.0028372Losses:  9.219327926635742 9.041803359985352 0.1775244027376175
CurrentTrain: epoch  1, batch     3 | loss: 9.2193279Losses:  9.182013511657715 9.009771347045898 0.17224249243736267
CurrentTrain: epoch  1, batch     4 | loss: 9.1820135Losses:  8.756941795349121 8.520601272583008 0.23634034395217896
CurrentTrain: epoch  1, batch     5 | loss: 8.7569418Losses:  8.799327850341797 8.655294418334961 0.144033744931221
CurrentTrain: epoch  1, batch     6 | loss: 8.7993279Losses:  6.55071496963501 6.386085033416748 0.16463014483451843
CurrentTrain: epoch  1, batch     7 | loss: 6.5507150Losses:  6.420611381530762 6.258100509643555 0.16251106560230255
CurrentTrain: epoch  1, batch     8 | loss: 6.4206114Losses:  10.744359016418457 10.573514938354492 0.17084386944770813
CurrentTrain: epoch  1, batch     9 | loss: 10.7443590Losses:  5.151101589202881 5.033506393432617 0.11759530007839203
CurrentTrain: epoch  1, batch    10 | loss: 5.1511016Losses:  7.0583720207214355 6.944675445556641 0.11369667947292328
CurrentTrain: epoch  1, batch    11 | loss: 7.0583720Losses:  7.057604789733887 6.925909996032715 0.1316947042942047
CurrentTrain: epoch  1, batch    12 | loss: 7.0576048Losses:  7.3013081550598145 7.19692325592041 0.10438472777605057
CurrentTrain: epoch  1, batch    13 | loss: 7.3013082Losses:  6.908024787902832 6.802555561065674 0.10546940565109253
CurrentTrain: epoch  1, batch    14 | loss: 6.9080248Losses:  10.338459014892578 10.153593063354492 0.18486562371253967
CurrentTrain: epoch  1, batch    15 | loss: 10.3384590Losses:  6.768831253051758 6.675354480743408 0.09347687661647797
CurrentTrain: epoch  1, batch    16 | loss: 6.7688313Losses:  6.617391109466553 6.518957138061523 0.09843401610851288
CurrentTrain: epoch  1, batch    17 | loss: 6.6173911Losses:  5.558051109313965 5.45927619934082 0.09877480566501617
CurrentTrain: epoch  1, batch    18 | loss: 5.5580511Losses:  9.456538200378418 9.355195045471191 0.10134357213973999
CurrentTrain: epoch  1, batch    19 | loss: 9.4565382Losses:  7.007055282592773 6.92210578918457 0.0849495530128479
CurrentTrain: epoch  1, batch    20 | loss: 7.0070553Losses:  8.22972297668457 8.142500877380371 0.08722195774316788
CurrentTrain: epoch  1, batch    21 | loss: 8.2297230Losses:  6.792013645172119 6.698936462402344 0.09307698160409927
CurrentTrain: epoch  1, batch    22 | loss: 6.7920136Losses:  6.858670711517334 6.793382167816162 0.06528858840465546
CurrentTrain: epoch  1, batch    23 | loss: 6.8586707Losses:  7.363104343414307 7.292242050170898 0.0708620697259903
CurrentTrain: epoch  1, batch    24 | loss: 7.3631043Losses:  8.403234481811523 8.329507827758789 0.07372699677944183
CurrentTrain: epoch  1, batch    25 | loss: 8.4032345Losses:  6.459448337554932 6.393521308898926 0.06592687964439392
CurrentTrain: epoch  1, batch    26 | loss: 6.4594483Losses:  6.599506378173828 6.53432035446167 0.06518584489822388
CurrentTrain: epoch  1, batch    27 | loss: 6.5995064Losses:  6.813978672027588 6.754998207092285 0.05898065119981766
CurrentTrain: epoch  1, batch    28 | loss: 6.8139787Losses:  5.411911964416504 5.348674774169922 0.06323741376399994
CurrentTrain: epoch  1, batch    29 | loss: 5.4119120Losses:  6.2814531326293945 6.212460517883301 0.06899261474609375
CurrentTrain: epoch  1, batch    30 | loss: 6.2814531Losses:  6.070960998535156 6.0142598152160645 0.05670120567083359
CurrentTrain: epoch  1, batch    31 | loss: 6.0709610Losses:  5.051991939544678 4.998371124267578 0.053620826452970505
CurrentTrain: epoch  1, batch    32 | loss: 5.0519919Losses:  10.105134010314941 10.037731170654297 0.0674024224281311
CurrentTrain: epoch  1, batch    33 | loss: 10.1051340Losses:  8.358861923217773 8.30003547668457 0.05882628262042999
CurrentTrain: epoch  1, batch    34 | loss: 8.3588619Losses:  6.992851257324219 6.945080757141113 0.04777030274271965
CurrentTrain: epoch  1, batch    35 | loss: 6.9928513Losses:  6.415453910827637 6.351040840148926 0.06441306322813034
CurrentTrain: epoch  1, batch    36 | loss: 6.4154539Losses:  1.0145663022994995 0.9636248350143433 0.05094151571393013
CurrentTrain: epoch  1, batch    37 | loss: 1.0145663Losses:  7.619312763214111 7.566730499267578 0.05258217826485634
CurrentTrain: epoch  2, batch     0 | loss: 7.6193128Losses:  5.8920745849609375 5.837583065032959 0.05449162423610687
CurrentTrain: epoch  2, batch     1 | loss: 5.8920746Losses:  5.513443470001221 5.459870338439941 0.05357297509908676
CurrentTrain: epoch  2, batch     2 | loss: 5.5134435Losses:  7.807499408721924 7.7562994956970215 0.05119989439845085
CurrentTrain: epoch  2, batch     3 | loss: 7.8074994Losses:  9.823046684265137 9.768317222595215 0.05472927913069725
CurrentTrain: epoch  2, batch     4 | loss: 9.8230467Losses:  5.695587158203125 5.6457672119140625 0.04981989786028862
CurrentTrain: epoch  2, batch     5 | loss: 5.6955872Losses:  6.505383014678955 6.449292182922363 0.056090857833623886
CurrentTrain: epoch  2, batch     6 | loss: 6.5053830Losses:  14.552660942077637 14.50606918334961 0.04659198224544525
CurrentTrain: epoch  2, batch     7 | loss: 14.5526609Losses:  6.627177715301514 6.5820488929748535 0.04512897878885269
CurrentTrain: epoch  2, batch     8 | loss: 6.6271777Losses:  6.785830020904541 6.7358174324035645 0.05001240223646164
CurrentTrain: epoch  2, batch     9 | loss: 6.7858300Losses:  5.525965213775635 5.4789276123046875 0.04703756794333458
CurrentTrain: epoch  2, batch    10 | loss: 5.5259652Losses:  6.221782684326172 6.177918434143066 0.04386436939239502
CurrentTrain: epoch  2, batch    11 | loss: 6.2217827Losses:  7.064682960510254 7.0165605545043945 0.04812249541282654
CurrentTrain: epoch  2, batch    12 | loss: 7.0646830Losses:  6.731986045837402 6.6818952560424805 0.050090640783309937
CurrentTrain: epoch  2, batch    13 | loss: 6.7319860Losses:  5.49833869934082 5.45857048034668 0.03976843133568764
CurrentTrain: epoch  2, batch    14 | loss: 5.4983387Losses:  5.890709400177002 5.8465375900268555 0.04417180269956589
CurrentTrain: epoch  2, batch    15 | loss: 5.8907094Losses:  6.504897594451904 6.4639739990234375 0.04092351719737053
CurrentTrain: epoch  2, batch    16 | loss: 6.5048976Losses:  6.8075714111328125 6.767662048339844 0.03990950435400009
CurrentTrain: epoch  2, batch    17 | loss: 6.8075714Losses:  5.898390769958496 5.84827184677124 0.050119102001190186
CurrentTrain: epoch  2, batch    18 | loss: 5.8983908Losses:  8.152942657470703 8.109148025512695 0.043794430792331696
CurrentTrain: epoch  2, batch    19 | loss: 8.1529427Losses:  9.194063186645508 9.152315139770508 0.041748154908418655
CurrentTrain: epoch  2, batch    20 | loss: 9.1940632Losses:  4.915881633758545 4.87989616394043 0.035985395312309265
CurrentTrain: epoch  2, batch    21 | loss: 4.9158816Losses:  7.844690322875977 7.792407989501953 0.0522821843624115
CurrentTrain: epoch  2, batch    22 | loss: 7.8446903Losses:  7.222499370574951 7.175421714782715 0.0470774881541729
CurrentTrain: epoch  2, batch    23 | loss: 7.2224994Losses:  7.844651222229004 7.796661376953125 0.04798979312181473
CurrentTrain: epoch  2, batch    24 | loss: 7.8446512Losses:  5.854127407073975 5.815001487731934 0.039125822484493256
CurrentTrain: epoch  2, batch    25 | loss: 5.8541274Losses:  6.286558628082275 6.247014045715332 0.03954443708062172
CurrentTrain: epoch  2, batch    26 | loss: 6.2865586Losses:  5.032980442047119 4.994946479797363 0.03803412616252899
CurrentTrain: epoch  2, batch    27 | loss: 5.0329804Losses:  8.358083724975586 8.32044506072998 0.0376383513212204
CurrentTrain: epoch  2, batch    28 | loss: 8.3580837Losses:  5.299858570098877 5.263340950012207 0.03651756793260574
CurrentTrain: epoch  2, batch    29 | loss: 5.2998586Losses:  5.011507034301758 4.978119373321533 0.03338777646422386
CurrentTrain: epoch  2, batch    30 | loss: 5.0115070Losses:  7.224015712738037 7.180934906005859 0.04308098554611206
CurrentTrain: epoch  2, batch    31 | loss: 7.2240157Losses:  10.00802230834961 9.959172248840332 0.04884989187121391
CurrentTrain: epoch  2, batch    32 | loss: 10.0080223Losses:  5.254712104797363 5.216109275817871 0.03860266134142876
CurrentTrain: epoch  2, batch    33 | loss: 5.2547121Losses:  4.695406436920166 4.658463001251221 0.03694342076778412
CurrentTrain: epoch  2, batch    34 | loss: 4.6954064Losses:  3.9586689472198486 3.92806339263916 0.030605532228946686
CurrentTrain: epoch  2, batch    35 | loss: 3.9586689Losses:  4.702106475830078 4.6678290367126465 0.03427756950259209
CurrentTrain: epoch  2, batch    36 | loss: 4.7021065Losses:  0.706309974193573 0.6701810359954834 0.03612895309925079
CurrentTrain: epoch  2, batch    37 | loss: 0.7063100Losses:  4.524392127990723 4.493470668792725 0.030921634286642075
CurrentTrain: epoch  3, batch     0 | loss: 4.5243921Losses:  8.45083236694336 8.415279388427734 0.03555331751704216
CurrentTrain: epoch  3, batch     1 | loss: 8.4508324Losses:  6.48984956741333 6.4550862312316895 0.03476344794034958
CurrentTrain: epoch  3, batch     2 | loss: 6.4898496Losses:  5.485758304595947 5.447910308837891 0.03784819692373276
CurrentTrain: epoch  3, batch     3 | loss: 5.4857583Losses:  13.31178092956543 13.258993148803711 0.05278800427913666
CurrentTrain: epoch  3, batch     4 | loss: 13.3117809Losses:  7.94851016998291 7.91799783706665 0.030512167140841484
CurrentTrain: epoch  3, batch     5 | loss: 7.9485102Losses:  7.197927951812744 7.162632465362549 0.035295698791742325
CurrentTrain: epoch  3, batch     6 | loss: 7.1979280Losses:  6.51401948928833 6.473837852478027 0.04018156975507736
CurrentTrain: epoch  3, batch     7 | loss: 6.5140195Losses:  7.43370246887207 7.378034591674805 0.055667947977781296
CurrentTrain: epoch  3, batch     8 | loss: 7.4337025Losses:  6.110022068023682 6.077142238616943 0.032879941165447235
CurrentTrain: epoch  3, batch     9 | loss: 6.1100221Losses:  5.961419582366943 5.926585674285889 0.03483373671770096
CurrentTrain: epoch  3, batch    10 | loss: 5.9614196Losses:  6.031048774719238 5.988829612731934 0.04221933335065842
CurrentTrain: epoch  3, batch    11 | loss: 6.0310488Losses:  7.662386894226074 7.629985809326172 0.032401226460933685
CurrentTrain: epoch  3, batch    12 | loss: 7.6623869Losses:  10.708322525024414 10.658973693847656 0.049349166452884674
CurrentTrain: epoch  3, batch    13 | loss: 10.7083225Losses:  4.833486557006836 4.800956726074219 0.032529957592487335
CurrentTrain: epoch  3, batch    14 | loss: 4.8334866Losses:  5.010128974914551 4.978826522827148 0.03130251169204712
CurrentTrain: epoch  3, batch    15 | loss: 5.0101290Losses:  5.5064592361450195 5.4761576652526855 0.030301762744784355
CurrentTrain: epoch  3, batch    16 | loss: 5.5064592Losses:  5.857484340667725 5.830385208129883 0.027099113911390305
CurrentTrain: epoch  3, batch    17 | loss: 5.8574843Losses:  5.8104095458984375 5.77797269821167 0.03243677318096161
CurrentTrain: epoch  3, batch    18 | loss: 5.8104095Losses:  4.376388072967529 4.346158027648926 0.030230171978473663
CurrentTrain: epoch  3, batch    19 | loss: 4.3763881Losses:  4.363914966583252 4.336508274078369 0.027406584471464157
CurrentTrain: epoch  3, batch    20 | loss: 4.3639150Losses:  4.361928462982178 4.333715438842773 0.028213119134306908
CurrentTrain: epoch  3, batch    21 | loss: 4.3619285Losses:  6.2819366455078125 6.24149751663208 0.040439192205667496
CurrentTrain: epoch  3, batch    22 | loss: 6.2819366Losses:  6.300294399261475 6.266550064086914 0.033744316548109055
CurrentTrain: epoch  3, batch    23 | loss: 6.3002944Losses:  6.447497367858887 6.411572456359863 0.03592495992779732
CurrentTrain: epoch  3, batch    24 | loss: 6.4474974Losses:  9.323486328125 9.292810440063477 0.03067570924758911
CurrentTrain: epoch  3, batch    25 | loss: 9.3234863Losses:  6.093695640563965 6.062266826629639 0.03142901510000229
CurrentTrain: epoch  3, batch    26 | loss: 6.0936956Losses:  8.307144165039062 8.264184951782227 0.04295891523361206
CurrentTrain: epoch  3, batch    27 | loss: 8.3071442Losses:  5.165048599243164 5.136184215545654 0.02886422537267208
CurrentTrain: epoch  3, batch    28 | loss: 5.1650486Losses:  5.7502946853637695 5.721077919006348 0.0292168278247118
CurrentTrain: epoch  3, batch    29 | loss: 5.7502947Losses:  6.780087947845459 6.740853309631348 0.039234600961208344
CurrentTrain: epoch  3, batch    30 | loss: 6.7800879Losses:  5.623341083526611 5.591054916381836 0.03228621929883957
CurrentTrain: epoch  3, batch    31 | loss: 5.6233411Losses:  5.033538818359375 4.99990177154541 0.03363689407706261
CurrentTrain: epoch  3, batch    32 | loss: 5.0335388Losses:  6.134976863861084 6.10423469543457 0.030742181465029716
CurrentTrain: epoch  3, batch    33 | loss: 6.1349769Losses:  6.982342720031738 6.947085857391357 0.03525703027844429
CurrentTrain: epoch  3, batch    34 | loss: 6.9823427Losses:  4.574554920196533 4.545340538024902 0.029214339330792427
CurrentTrain: epoch  3, batch    35 | loss: 4.5745549Losses:  8.334699630737305 8.294048309326172 0.04065131023526192
CurrentTrain: epoch  3, batch    36 | loss: 8.3346996Losses:  1.25936758518219 1.2312393188476562 0.028128281235694885
CurrentTrain: epoch  3, batch    37 | loss: 1.2593676Losses:  5.884902477264404 5.851632118225098 0.03327016532421112
CurrentTrain: epoch  4, batch     0 | loss: 5.8849025Losses:  7.99281644821167 7.952367305755615 0.04044903814792633
CurrentTrain: epoch  4, batch     1 | loss: 7.9928164Losses:  7.898947238922119 7.860422134399414 0.03852512314915657
CurrentTrain: epoch  4, batch     2 | loss: 7.8989472Losses:  7.0741987228393555 7.037945747375488 0.03625277802348137
CurrentTrain: epoch  4, batch     3 | loss: 7.0741987Losses:  4.2666449546813965 4.239130020141602 0.027514735236763954
CurrentTrain: epoch  4, batch     4 | loss: 4.2666450Losses:  4.955835342407227 4.92626953125 0.0295657180249691
CurrentTrain: epoch  4, batch     5 | loss: 4.9558353Losses:  7.923789978027344 7.891962051391602 0.03182800114154816
CurrentTrain: epoch  4, batch     6 | loss: 7.9237900Losses:  9.437122344970703 9.399845123291016 0.037277571856975555
CurrentTrain: epoch  4, batch     7 | loss: 9.4371223Losses:  7.071903228759766 7.03300666809082 0.03889675438404083
CurrentTrain: epoch  4, batch     8 | loss: 7.0719032Losses:  6.38812255859375 6.355434417724609 0.03268797695636749
CurrentTrain: epoch  4, batch     9 | loss: 6.3881226Losses:  11.345659255981445 11.283038139343262 0.06262119114398956
CurrentTrain: epoch  4, batch    10 | loss: 11.3456593Losses:  4.662855625152588 4.63821268081665 0.024642810225486755
CurrentTrain: epoch  4, batch    11 | loss: 4.6628556Losses:  7.339019298553467 7.306264877319336 0.03275448828935623
CurrentTrain: epoch  4, batch    12 | loss: 7.3390193Losses:  8.957029342651367 8.927849769592285 0.029179895296692848
CurrentTrain: epoch  4, batch    13 | loss: 8.9570293Losses:  8.485139846801758 8.452380180358887 0.03275968134403229
CurrentTrain: epoch  4, batch    14 | loss: 8.4851398Losses:  4.29457950592041 4.268185138702393 0.026394322514533997
CurrentTrain: epoch  4, batch    15 | loss: 4.2945795Losses:  4.99912166595459 4.970304489135742 0.02881697192788124
CurrentTrain: epoch  4, batch    16 | loss: 4.9991217Losses:  5.930728912353516 5.899246692657471 0.03148209676146507
CurrentTrain: epoch  4, batch    17 | loss: 5.9307289Losses:  8.372962951660156 8.333407402038574 0.03955601155757904
CurrentTrain: epoch  4, batch    18 | loss: 8.3729630Losses:  7.126695156097412 7.094952583312988 0.031742777675390244
CurrentTrain: epoch  4, batch    19 | loss: 7.1266952Losses:  13.166755676269531 13.130537033081055 0.03621840476989746
CurrentTrain: epoch  4, batch    20 | loss: 13.1667557Losses:  4.8855767250061035 4.857091903686523 0.028484851121902466
CurrentTrain: epoch  4, batch    21 | loss: 4.8855767Losses:  7.146660327911377 7.116166114807129 0.03049430251121521
CurrentTrain: epoch  4, batch    22 | loss: 7.1466603Losses:  9.172754287719727 9.146880149841309 0.02587432973086834
CurrentTrain: epoch  4, batch    23 | loss: 9.1727543Losses:  5.1797308921813965 5.150766372680664 0.02896440401673317
CurrentTrain: epoch  4, batch    24 | loss: 5.1797309Losses:  4.415462017059326 4.3903279304504395 0.025134090334177017
CurrentTrain: epoch  4, batch    25 | loss: 4.4154620Losses:  8.256531715393066 8.221617698669434 0.03491374850273132
CurrentTrain: epoch  4, batch    26 | loss: 8.2565317Losses:  7.186929225921631 7.155416965484619 0.03151236101984978
CurrentTrain: epoch  4, batch    27 | loss: 7.1869292Losses:  6.571608066558838 6.543312072753906 0.028295978903770447
CurrentTrain: epoch  4, batch    28 | loss: 6.5716081Losses:  6.702908992767334 6.673876762390137 0.029032031074166298
CurrentTrain: epoch  4, batch    29 | loss: 6.7029090Losses:  4.4916253089904785 4.466672420501709 0.024953071027994156
CurrentTrain: epoch  4, batch    30 | loss: 4.4916253Losses:  4.060044765472412 4.036215782165527 0.02382914163172245
CurrentTrain: epoch  4, batch    31 | loss: 4.0600448Losses:  7.67445707321167 7.644083023071289 0.030374158173799515
CurrentTrain: epoch  4, batch    32 | loss: 7.6744571Losses:  4.270201206207275 4.241222858428955 0.028978532180190086
CurrentTrain: epoch  4, batch    33 | loss: 4.2702012Losses:  6.053598403930664 6.023530006408691 0.03006858192384243
CurrentTrain: epoch  4, batch    34 | loss: 6.0535984Losses:  8.169527053833008 8.123169898986816 0.04635699465870857
CurrentTrain: epoch  4, batch    35 | loss: 8.1695271Losses:  9.585542678833008 9.553033828735352 0.03250861167907715
CurrentTrain: epoch  4, batch    36 | loss: 9.5855427Losses:  1.727054238319397 1.6731451749801636 0.05390901118516922
CurrentTrain: epoch  4, batch    37 | loss: 1.7270542Losses:  6.056290626525879 6.026122570037842 0.030167829245328903
CurrentTrain: epoch  5, batch     0 | loss: 6.0562906Losses:  6.955475807189941 6.9272308349609375 0.028244758024811745
CurrentTrain: epoch  5, batch     1 | loss: 6.9554758Losses:  5.36399507522583 5.335721969604492 0.028273150324821472
CurrentTrain: epoch  5, batch     2 | loss: 5.3639951Losses:  4.256328105926514 4.231950283050537 0.024377593770623207
CurrentTrain: epoch  5, batch     3 | loss: 4.2563281Losses:  7.66054105758667 7.626784324645996 0.03375688195228577
CurrentTrain: epoch  5, batch     4 | loss: 7.6605411Losses:  4.844526290893555 4.814349174499512 0.03017714060842991
CurrentTrain: epoch  5, batch     5 | loss: 4.8445263Losses:  6.048008441925049 6.019222736358643 0.028785813599824905
CurrentTrain: epoch  5, batch     6 | loss: 6.0480084Losses:  9.259007453918457 9.220186233520508 0.038821473717689514
CurrentTrain: epoch  5, batch     7 | loss: 9.2590075Losses:  5.959397315979004 5.933827877044678 0.025569584220647812
CurrentTrain: epoch  5, batch     8 | loss: 5.9593973Losses:  4.563741207122803 4.539894104003906 0.02384706772863865
CurrentTrain: epoch  5, batch     9 | loss: 4.5637412Losses:  8.484797477722168 8.440918922424316 0.04387817531824112
CurrentTrain: epoch  5, batch    10 | loss: 8.4847975Losses:  7.171308994293213 7.134568691253662 0.03674022853374481
CurrentTrain: epoch  5, batch    11 | loss: 7.1713090Losses:  8.268202781677246 8.232261657714844 0.03594140708446503
CurrentTrain: epoch  5, batch    12 | loss: 8.2682028Losses:  8.700907707214355 8.667388916015625 0.03351885452866554
CurrentTrain: epoch  5, batch    13 | loss: 8.7009077Losses:  4.915741443634033 4.8876800537109375 0.028061402961611748
CurrentTrain: epoch  5, batch    14 | loss: 4.9157414Losses:  5.528187274932861 5.500638961791992 0.02754819393157959
CurrentTrain: epoch  5, batch    15 | loss: 5.5281873Losses:  4.4732489585876465 4.448489665985107 0.024759460240602493
CurrentTrain: epoch  5, batch    16 | loss: 4.4732490Losses:  7.1373419761657715 7.10977840423584 0.027563443407416344
CurrentTrain: epoch  5, batch    17 | loss: 7.1373420Losses:  7.098896503448486 7.0722808837890625 0.026615416631102562
CurrentTrain: epoch  5, batch    18 | loss: 7.0988965Losses:  5.175048351287842 5.145397186279297 0.02965136058628559
CurrentTrain: epoch  5, batch    19 | loss: 5.1750484Losses:  4.25806188583374 4.233381271362305 0.02468061074614525
CurrentTrain: epoch  5, batch    20 | loss: 4.2580619Losses:  5.263867378234863 5.23423957824707 0.029627636075019836
CurrentTrain: epoch  5, batch    21 | loss: 5.2638674Losses:  4.635643005371094 4.612148284912109 0.023494545370340347
CurrentTrain: epoch  5, batch    22 | loss: 4.6356430Losses:  6.6641764640808105 6.623939514160156 0.040236763656139374
CurrentTrain: epoch  5, batch    23 | loss: 6.6641765Losses:  4.64447546005249 4.6171441078186035 0.02733113244175911
CurrentTrain: epoch  5, batch    24 | loss: 4.6444755Losses:  4.536849021911621 4.50940465927124 0.027444221079349518
CurrentTrain: epoch  5, batch    25 | loss: 4.5368490Losses:  5.754214286804199 5.717369079589844 0.0368451327085495
CurrentTrain: epoch  5, batch    26 | loss: 5.7542143Losses:  4.75368070602417 4.721848011016846 0.031832505017519
CurrentTrain: epoch  5, batch    27 | loss: 4.7536807Losses:  5.591949462890625 5.560946464538574 0.031002767384052277
CurrentTrain: epoch  5, batch    28 | loss: 5.5919495Losses:  4.9381256103515625 4.907893180847168 0.030232543125748634
CurrentTrain: epoch  5, batch    29 | loss: 4.9381256Losses:  9.201044082641602 9.15987777709961 0.04116647690534592
CurrentTrain: epoch  5, batch    30 | loss: 9.2010441Losses:  6.81051778793335 6.770638465881348 0.0398792065680027
CurrentTrain: epoch  5, batch    31 | loss: 6.8105178Losses:  5.8728346824646 5.841728687286377 0.03110608272254467
CurrentTrain: epoch  5, batch    32 | loss: 5.8728347Losses:  4.831262588500977 4.801292896270752 0.029969731345772743
CurrentTrain: epoch  5, batch    33 | loss: 4.8312626Losses:  4.566858768463135 4.540301322937012 0.026557259261608124
CurrentTrain: epoch  5, batch    34 | loss: 4.5668588Losses:  6.549816608428955 6.517151832580566 0.03266473114490509
CurrentTrain: epoch  5, batch    35 | loss: 6.5498166Losses:  5.781939506530762 5.750547409057617 0.031392164528369904
CurrentTrain: epoch  5, batch    36 | loss: 5.7819395Losses:  1.8433500528335571 1.805004358291626 0.03834564983844757
CurrentTrain: epoch  5, batch    37 | loss: 1.8433501Losses:  5.029698848724365 5.005422592163086 0.024276142939925194
CurrentTrain: epoch  6, batch     0 | loss: 5.0296988Losses:  6.831620216369629 6.793067932128906 0.03855207934975624
CurrentTrain: epoch  6, batch     1 | loss: 6.8316202Losses:  5.2103166580200195 5.181333541870117 0.028983162716031075
CurrentTrain: epoch  6, batch     2 | loss: 5.2103167Losses:  7.550568103790283 7.5228800773620605 0.02768786996603012
CurrentTrain: epoch  6, batch     3 | loss: 7.5505681Losses:  5.796365737915039 5.773873805999756 0.022491950541734695
CurrentTrain: epoch  6, batch     4 | loss: 5.7963657Losses:  10.42116928100586 10.374467849731445 0.04670172184705734
CurrentTrain: epoch  6, batch     5 | loss: 10.4211693Losses:  6.823591709136963 6.798666954040527 0.02492452599108219
CurrentTrain: epoch  6, batch     6 | loss: 6.8235917Losses:  9.555931091308594 9.516990661621094 0.038940779864788055
CurrentTrain: epoch  6, batch     7 | loss: 9.5559311Losses:  6.045313835144043 6.015235900878906 0.030077721923589706
CurrentTrain: epoch  6, batch     8 | loss: 6.0453138Losses:  3.83811354637146 3.8142614364624023 0.023852072656154633
CurrentTrain: epoch  6, batch     9 | loss: 3.8381135Losses:  4.525912284851074 4.502108573913574 0.02380388230085373
CurrentTrain: epoch  6, batch    10 | loss: 4.5259123Losses:  4.468209266662598 4.444631576538086 0.0235777385532856
CurrentTrain: epoch  6, batch    11 | loss: 4.4682093Losses:  8.280673027038574 8.249862670898438 0.030810672789812088
CurrentTrain: epoch  6, batch    12 | loss: 8.2806730Losses:  7.337000846862793 7.31198787689209 0.025013182312250137
CurrentTrain: epoch  6, batch    13 | loss: 7.3370008Losses:  5.640441417694092 5.615599632263184 0.024841677397489548
CurrentTrain: epoch  6, batch    14 | loss: 5.6404414Losses:  8.125179290771484 8.081298828125 0.04388005658984184
CurrentTrain: epoch  6, batch    15 | loss: 8.1251793Losses:  5.724562168121338 5.696468353271484 0.028093626722693443
CurrentTrain: epoch  6, batch    16 | loss: 5.7245622Losses:  5.2321319580078125 5.2021331787109375 0.029998652637004852
CurrentTrain: epoch  6, batch    17 | loss: 5.2321320Losses:  4.779311180114746 4.753122329711914 0.026188679039478302
CurrentTrain: epoch  6, batch    18 | loss: 4.7793112Losses:  4.630072593688965 4.605251312255859 0.024821443483233452
CurrentTrain: epoch  6, batch    19 | loss: 4.6300726Losses:  5.854957103729248 5.828635215759277 0.026321696117520332
CurrentTrain: epoch  6, batch    20 | loss: 5.8549571Losses:  10.157620429992676 10.116743087768555 0.04087723046541214
CurrentTrain: epoch  6, batch    21 | loss: 10.1576204Losses:  5.571190357208252 5.540870666503906 0.0303195733577013
CurrentTrain: epoch  6, batch    22 | loss: 5.5711904Losses:  7.140658378601074 7.10344934463501 0.037208832800388336
CurrentTrain: epoch  6, batch    23 | loss: 7.1406584Losses:  4.826927661895752 4.800264835357666 0.026662660762667656
CurrentTrain: epoch  6, batch    24 | loss: 4.8269277Losses:  4.673542499542236 4.650564193725586 0.02297811210155487
CurrentTrain: epoch  6, batch    25 | loss: 4.6735425Losses:  6.983975410461426 6.947363376617432 0.03661181032657623
CurrentTrain: epoch  6, batch    26 | loss: 6.9839754Losses:  6.029829978942871 6.007021903991699 0.022808121517300606
CurrentTrain: epoch  6, batch    27 | loss: 6.0298300Losses:  4.598395824432373 4.5733184814453125 0.02507735788822174
CurrentTrain: epoch  6, batch    28 | loss: 4.5983958Losses:  4.54645299911499 4.52500057220459 0.021452296525239944
CurrentTrain: epoch  6, batch    29 | loss: 4.5464530Losses:  5.426342010498047 5.401021480560303 0.02532065473496914
CurrentTrain: epoch  6, batch    30 | loss: 5.4263420Losses:  4.119901657104492 4.097297668457031 0.022604072466492653
CurrentTrain: epoch  6, batch    31 | loss: 4.1199017Losses:  5.468387603759766 5.43997859954834 0.028409074991941452
CurrentTrain: epoch  6, batch    32 | loss: 5.4683876Losses:  6.882048606872559 6.846030235290527 0.036018338054418564
CurrentTrain: epoch  6, batch    33 | loss: 6.8820486Losses:  9.013364791870117 8.981878280639648 0.031486254185438156
CurrentTrain: epoch  6, batch    34 | loss: 9.0133648Losses:  4.753631114959717 4.726435661315918 0.02719563990831375
CurrentTrain: epoch  6, batch    35 | loss: 4.7536311Losses:  4.256021499633789 4.231783866882324 0.02423739619553089
CurrentTrain: epoch  6, batch    36 | loss: 4.2560215Losses:  0.686943531036377 0.6462674140930176 0.04067614674568176
CurrentTrain: epoch  6, batch    37 | loss: 0.6869435Losses:  4.739988803863525 4.715917587280273 0.024071170017123222
CurrentTrain: epoch  7, batch     0 | loss: 4.7399888Losses:  9.424029350280762 9.392818450927734 0.031211061403155327
CurrentTrain: epoch  7, batch     1 | loss: 9.4240294Losses:  6.655185222625732 6.616933822631836 0.03825128823518753
CurrentTrain: epoch  7, batch     2 | loss: 6.6551852Losses:  5.558164119720459 5.524384498596191 0.033779583871364594
CurrentTrain: epoch  7, batch     3 | loss: 5.5581641Losses:  7.061463356018066 7.034862995147705 0.02660050056874752
CurrentTrain: epoch  7, batch     4 | loss: 7.0614634Losses:  4.1871724128723145 4.16331672668457 0.02385556511580944
CurrentTrain: epoch  7, batch     5 | loss: 4.1871724Losses:  5.407529830932617 5.374896049499512 0.03263392299413681
CurrentTrain: epoch  7, batch     6 | loss: 5.4075298Losses:  6.891441345214844 6.850375175476074 0.041066087782382965
CurrentTrain: epoch  7, batch     7 | loss: 6.8914413Losses:  3.9459164142608643 3.921638011932373 0.02427835389971733
CurrentTrain: epoch  7, batch     8 | loss: 3.9459164Losses:  3.9059948921203613 3.882309913635254 0.02368508279323578
CurrentTrain: epoch  7, batch     9 | loss: 3.9059949Losses:  4.4934210777282715 4.468287467956543 0.02513362467288971
CurrentTrain: epoch  7, batch    10 | loss: 4.4934211Losses:  6.471662998199463 6.431870460510254 0.039792489260435104
CurrentTrain: epoch  7, batch    11 | loss: 6.4716630Losses:  6.857576370239258 6.83001184463501 0.027564603835344315
CurrentTrain: epoch  7, batch    12 | loss: 6.8575764Losses:  4.500476837158203 4.475309371948242 0.025167569518089294
CurrentTrain: epoch  7, batch    13 | loss: 4.5004768Losses:  7.283356189727783 7.242548942565918 0.040807079523801804
CurrentTrain: epoch  7, batch    14 | loss: 7.2833562Losses:  4.531716823577881 4.5100274085998535 0.021689359098672867
CurrentTrain: epoch  7, batch    15 | loss: 4.5317168Losses:  5.062259674072266 5.031952381134033 0.030307255685329437
CurrentTrain: epoch  7, batch    16 | loss: 5.0622597Losses:  6.658609867095947 6.624893665313721 0.03371642529964447
CurrentTrain: epoch  7, batch    17 | loss: 6.6586099Losses:  7.475871562957764 7.441478252410889 0.03439344838261604
CurrentTrain: epoch  7, batch    18 | loss: 7.4758716Losses:  4.1434245109558105 4.119951248168945 0.023473119363188744
CurrentTrain: epoch  7, batch    19 | loss: 4.1434245Losses:  4.153107643127441 4.126291275024414 0.02681632898747921
CurrentTrain: epoch  7, batch    20 | loss: 4.1531076Losses:  5.923990249633789 5.893409252166748 0.030580848455429077
CurrentTrain: epoch  7, batch    21 | loss: 5.9239902Losses:  9.714606285095215 9.676803588867188 0.03780297189950943
CurrentTrain: epoch  7, batch    22 | loss: 9.7146063Losses:  4.205743789672852 4.182513236999512 0.02323053777217865
CurrentTrain: epoch  7, batch    23 | loss: 4.2057438Losses:  5.674624443054199 5.642931938171387 0.031692370772361755
CurrentTrain: epoch  7, batch    24 | loss: 5.6746244Losses:  5.779590129852295 5.742602348327637 0.03698771074414253
CurrentTrain: epoch  7, batch    25 | loss: 5.7795901Losses:  9.129417419433594 9.085925102233887 0.04349275678396225
CurrentTrain: epoch  7, batch    26 | loss: 9.1294174Losses:  5.3105244636535645 5.28176736831665 0.028756972402334213
CurrentTrain: epoch  7, batch    27 | loss: 5.3105245Losses:  6.074512958526611 6.039032936096191 0.035479914397001266
CurrentTrain: epoch  7, batch    28 | loss: 6.0745130Losses:  8.677746772766113 8.64239501953125 0.035351622849702835
CurrentTrain: epoch  7, batch    29 | loss: 8.6777468Losses:  7.819180488586426 7.7852396965026855 0.03394092246890068
CurrentTrain: epoch  7, batch    30 | loss: 7.8191805Losses:  5.118099689483643 5.087762832641602 0.030336689203977585
CurrentTrain: epoch  7, batch    31 | loss: 5.1180997Losses:  4.553186416625977 4.52912712097168 0.024059534072875977
CurrentTrain: epoch  7, batch    32 | loss: 4.5531864Losses:  7.940394878387451 7.912149429321289 0.02824556827545166
CurrentTrain: epoch  7, batch    33 | loss: 7.9403949Losses:  6.135371208190918 6.107490539550781 0.027880623936653137
CurrentTrain: epoch  7, batch    34 | loss: 6.1353712Losses:  3.9243955612182617 3.9018712043762207 0.022524353116750717
CurrentTrain: epoch  7, batch    35 | loss: 3.9243956Losses:  4.076925277709961 4.054579257965088 0.022346241399645805
CurrentTrain: epoch  7, batch    36 | loss: 4.0769253Losses:  1.3769454956054688 1.3349738121032715 0.04197165369987488
CurrentTrain: epoch  7, batch    37 | loss: 1.3769455Losses:  5.675516605377197 5.641862869262695 0.033653609454631805
CurrentTrain: epoch  8, batch     0 | loss: 5.6755166Losses:  4.417364120483398 4.390056133270264 0.027307933196425438
CurrentTrain: epoch  8, batch     1 | loss: 4.4173641Losses:  6.278628349304199 6.248859405517578 0.029769103974103928
CurrentTrain: epoch  8, batch     2 | loss: 6.2786283Losses:  7.669195175170898 7.631831645965576 0.03736364468932152
CurrentTrain: epoch  8, batch     3 | loss: 7.6691952Losses:  6.948421478271484 6.9134368896484375 0.03498471528291702
CurrentTrain: epoch  8, batch     4 | loss: 6.9484215Losses:  5.672544956207275 5.637579917907715 0.0349649153649807
CurrentTrain: epoch  8, batch     5 | loss: 5.6725450Losses:  12.343910217285156 12.306379318237305 0.03753094747662544
CurrentTrain: epoch  8, batch     6 | loss: 12.3439102Losses:  6.046213150024414 6.014901638031006 0.0313115268945694
CurrentTrain: epoch  8, batch     7 | loss: 6.0462132Losses:  8.555688858032227 8.514413833618164 0.041275497525930405
CurrentTrain: epoch  8, batch     8 | loss: 8.5556889Losses:  5.359808444976807 5.335672378540039 0.024136047810316086
CurrentTrain: epoch  8, batch     9 | loss: 5.3598084Losses:  6.163870334625244 6.140928268432617 0.022941995412111282
CurrentTrain: epoch  8, batch    10 | loss: 6.1638703Losses:  3.4728808403015137 3.451403856277466 0.02147691510617733
CurrentTrain: epoch  8, batch    11 | loss: 3.4728808Losses:  5.413911819458008 5.391925811767578 0.02198619395494461
CurrentTrain: epoch  8, batch    12 | loss: 5.4139118Losses:  4.469017028808594 4.438408374786377 0.030608827248215675
CurrentTrain: epoch  8, batch    13 | loss: 4.4690170Losses:  4.038290500640869 4.0169572830200195 0.021333275362849236
CurrentTrain: epoch  8, batch    14 | loss: 4.0382905Losses:  7.492595195770264 7.453592300415039 0.03900269418954849
CurrentTrain: epoch  8, batch    15 | loss: 7.4925952Losses:  4.609053134918213 4.584473609924316 0.024579305201768875
CurrentTrain: epoch  8, batch    16 | loss: 4.6090531Losses:  6.053178310394287 6.025779724121094 0.027398638427257538
CurrentTrain: epoch  8, batch    17 | loss: 6.0531783Losses:  4.80452299118042 4.77689266204834 0.02763018012046814
CurrentTrain: epoch  8, batch    18 | loss: 4.8045230Losses:  4.306362628936768 4.280959129333496 0.025403697043657303
CurrentTrain: epoch  8, batch    19 | loss: 4.3063626Losses:  5.1265668869018555 5.091966152191162 0.03460051864385605
CurrentTrain: epoch  8, batch    20 | loss: 5.1265669Losses:  7.197359085083008 7.165721893310547 0.0316372811794281
CurrentTrain: epoch  8, batch    21 | loss: 7.1973591Losses:  5.076114177703857 5.044291973114014 0.03182217478752136
CurrentTrain: epoch  8, batch    22 | loss: 5.0761142Losses:  7.070202827453613 7.0434770584106445 0.026725858449935913
CurrentTrain: epoch  8, batch    23 | loss: 7.0702028Losses:  4.292944431304932 4.265735149383545 0.027209213003516197
CurrentTrain: epoch  8, batch    24 | loss: 4.2929444Losses:  5.0359673500061035 5.00930118560791 0.026666374877095222
CurrentTrain: epoch  8, batch    25 | loss: 5.0359674Losses:  7.261318206787109 7.229742527008057 0.031575772911310196
CurrentTrain: epoch  8, batch    26 | loss: 7.2613182Losses:  5.8500776290893555 5.8128814697265625 0.037196312099695206
CurrentTrain: epoch  8, batch    27 | loss: 5.8500776Losses:  5.981050491333008 5.946206569671631 0.034843869507312775
CurrentTrain: epoch  8, batch    28 | loss: 5.9810505Losses:  4.452248573303223 4.428225517272949 0.02402322366833687
CurrentTrain: epoch  8, batch    29 | loss: 4.4522486Losses:  4.1861724853515625 4.160233497619629 0.025938915088772774
CurrentTrain: epoch  8, batch    30 | loss: 4.1861725Losses:  6.510958671569824 6.46942138671875 0.04153729975223541
CurrentTrain: epoch  8, batch    31 | loss: 6.5109587Losses:  6.701935768127441 6.668717384338379 0.0332186222076416
CurrentTrain: epoch  8, batch    32 | loss: 6.7019358Losses:  4.098893165588379 4.076426029205322 0.022467005997896194
CurrentTrain: epoch  8, batch    33 | loss: 4.0988932Losses:  5.39418888092041 5.367063999176025 0.02712487056851387
CurrentTrain: epoch  8, batch    34 | loss: 5.3941889Losses:  9.328415870666504 9.297021865844727 0.03139383718371391
CurrentTrain: epoch  8, batch    35 | loss: 9.3284159Losses:  4.1479172706604 4.121184825897217 0.02673264965415001
CurrentTrain: epoch  8, batch    36 | loss: 4.1479173Losses:  3.5683412551879883 3.496056318283081 0.072284996509552
CurrentTrain: epoch  8, batch    37 | loss: 3.5683413Losses:  6.18540096282959 6.148842811584473 0.036557964980602264
CurrentTrain: epoch  9, batch     0 | loss: 6.1854010Losses:  5.397349834442139 5.372340202331543 0.025009460747241974
CurrentTrain: epoch  9, batch     1 | loss: 5.3973498Losses:  10.396671295166016 10.365771293640137 0.030899615958333015
CurrentTrain: epoch  9, batch     2 | loss: 10.3966713Losses:  5.9071760177612305 5.874852180480957 0.03232372924685478
CurrentTrain: epoch  9, batch     3 | loss: 5.9071760Losses:  7.245157241821289 7.212158203125 0.03299897909164429
CurrentTrain: epoch  9, batch     4 | loss: 7.2451572Losses:  4.696663856506348 4.667667388916016 0.028996596112847328
CurrentTrain: epoch  9, batch     5 | loss: 4.6966639Losses:  5.4133830070495605 5.379952430725098 0.03343077749013901
CurrentTrain: epoch  9, batch     6 | loss: 5.4133830Losses:  10.169684410095215 10.130081176757812 0.0396033339202404
CurrentTrain: epoch  9, batch     7 | loss: 10.1696844Losses:  9.05484390258789 9.01395320892334 0.04089050367474556
CurrentTrain: epoch  9, batch     8 | loss: 9.0548439Losses:  4.610448837280273 4.582239151000977 0.028209758922457695
CurrentTrain: epoch  9, batch     9 | loss: 4.6104488Losses:  7.852989673614502 7.8187055587768555 0.034284137189388275
CurrentTrain: epoch  9, batch    10 | loss: 7.8529897Losses:  5.466849327087402 5.4320969581604 0.03475227952003479
CurrentTrain: epoch  9, batch    11 | loss: 5.4668493Losses:  6.883886814117432 6.853516578674316 0.030370110645890236
CurrentTrain: epoch  9, batch    12 | loss: 6.8838868Losses:  4.465616703033447 4.4405388832092285 0.025078054517507553
CurrentTrain: epoch  9, batch    13 | loss: 4.4656167Losses:  6.545473575592041 6.509113311767578 0.03636026754975319
CurrentTrain: epoch  9, batch    14 | loss: 6.5454736Losses:  4.9733381271362305 4.948215484619141 0.02512269653379917
CurrentTrain: epoch  9, batch    15 | loss: 4.9733381Losses:  6.941740036010742 6.908066272735596 0.03367378190159798
CurrentTrain: epoch  9, batch    16 | loss: 6.9417400Losses:  6.454089641571045 6.42003059387207 0.03405896574258804
CurrentTrain: epoch  9, batch    17 | loss: 6.4540896Losses:  5.485984802246094 5.453069686889648 0.032915301620960236
CurrentTrain: epoch  9, batch    18 | loss: 5.4859848Losses:  4.730972766876221 4.7044997215271 0.02647286280989647
CurrentTrain: epoch  9, batch    19 | loss: 4.7309728Losses:  3.457808017730713 3.436458110809326 0.021349962800741196
CurrentTrain: epoch  9, batch    20 | loss: 3.4578080Losses:  6.618113040924072 6.579906463623047 0.038206424564123154
CurrentTrain: epoch  9, batch    21 | loss: 6.6181130Losses:  5.4357733726501465 5.40447473526001 0.031298525631427765
CurrentTrain: epoch  9, batch    22 | loss: 5.4357734Losses:  6.64091157913208 6.608649730682373 0.03226180374622345
CurrentTrain: epoch  9, batch    23 | loss: 6.6409116Losses:  6.276346206665039 6.242254257202148 0.034092094749212265
CurrentTrain: epoch  9, batch    24 | loss: 6.2763462Losses:  4.450647830963135 4.427485466003418 0.023162515833973885
CurrentTrain: epoch  9, batch    25 | loss: 4.4506478Losses:  4.479536533355713 4.451871395111084 0.027665141969919205
CurrentTrain: epoch  9, batch    26 | loss: 4.4795365Losses:  5.303238391876221 5.27590799331665 0.027330435812473297
CurrentTrain: epoch  9, batch    27 | loss: 5.3032384Losses:  6.225754737854004 6.191474437713623 0.03428050875663757
CurrentTrain: epoch  9, batch    28 | loss: 6.2257547Losses:  6.532042026519775 6.496760368347168 0.03528154268860817
CurrentTrain: epoch  9, batch    29 | loss: 6.5320420Losses:  4.960372447967529 4.932965278625488 0.027406932786107063
CurrentTrain: epoch  9, batch    30 | loss: 4.9603724Losses:  4.255651473999023 4.234805107116699 0.020846547558903694
CurrentTrain: epoch  9, batch    31 | loss: 4.2556515Losses:  10.275654792785645 10.223176956176758 0.05247828736901283
CurrentTrain: epoch  9, batch    32 | loss: 10.2756548Losses:  4.407093524932861 4.38052225112915 0.0265713632106781
CurrentTrain: epoch  9, batch    33 | loss: 4.4070935Losses:  6.435123443603516 6.399637222290039 0.035486359149217606
CurrentTrain: epoch  9, batch    34 | loss: 6.4351234Losses:  5.280357837677002 5.255725383758545 0.02463245950639248
CurrentTrain: epoch  9, batch    35 | loss: 5.2803578Losses:  5.832641124725342 5.804553985595703 0.028087368234992027
CurrentTrain: epoch  9, batch    36 | loss: 5.8326411Losses:  0.5148500204086304 0.4873889088630676 0.02746109664440155
CurrentTrain: epoch  9, batch    37 | loss: 0.5148500
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 85.29%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 84.03%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 83.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.33%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 85.68%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.78%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.04%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.93%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.74%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 85.29%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 84.03%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 83.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.33%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 85.68%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.78%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.04%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.93%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.74%   
cur_acc:  ['0.8674']
his_acc:  ['0.8674']
Clustering into  4  clusters
Clusters:  [0 3 3 0 0 2 1 3 2 0 1]
Losses:  7.54945182800293 7.189511299133301 0.359940767288208
CurrentTrain: epoch  0, batch     0 | loss: 7.5494518Losses:  4.821890354156494 4.472379207611084 0.3495110869407654
CurrentTrain: epoch  0, batch     1 | loss: 4.8218904Losses:  6.476064205169678 6.179208755493164 0.2968553900718689
CurrentTrain: epoch  1, batch     0 | loss: 6.4760642Losses:  2.7875351905822754 2.4717326164245605 0.3158026933670044
CurrentTrain: epoch  1, batch     1 | loss: 2.7875352Losses:  7.502277374267578 7.221423149108887 0.28085410594940186
CurrentTrain: epoch  2, batch     0 | loss: 7.5022774Losses:  3.741335391998291 3.49039626121521 0.2509390711784363
CurrentTrain: epoch  2, batch     1 | loss: 3.7413354Losses:  7.394453525543213 7.1214189529418945 0.27303457260131836
CurrentTrain: epoch  3, batch     0 | loss: 7.3944535Losses:  1.8847287893295288 1.6347243785858154 0.2500044107437134
CurrentTrain: epoch  3, batch     1 | loss: 1.8847288Losses:  6.120066165924072 5.876016139984131 0.24405016005039215
CurrentTrain: epoch  4, batch     0 | loss: 6.1200662Losses:  2.5988073348999023 2.324486255645752 0.274321049451828
CurrentTrain: epoch  4, batch     1 | loss: 2.5988073Losses:  6.577295303344727 6.339210510253906 0.23808501660823822
CurrentTrain: epoch  5, batch     0 | loss: 6.5772953Losses:  1.8436719179153442 1.5817244052886963 0.26194751262664795
CurrentTrain: epoch  5, batch     1 | loss: 1.8436719Losses:  6.266379356384277 6.040851593017578 0.22552798688411713
CurrentTrain: epoch  6, batch     0 | loss: 6.2663794Losses:  3.658320665359497 3.3790371417999268 0.2792835533618927
CurrentTrain: epoch  6, batch     1 | loss: 3.6583207Losses:  6.630651950836182 6.402009010314941 0.2286427915096283
CurrentTrain: epoch  7, batch     0 | loss: 6.6306520Losses:  2.808307647705078 2.520798921585083 0.28750884532928467
CurrentTrain: epoch  7, batch     1 | loss: 2.8083076Losses:  6.926075458526611 6.701375961303711 0.22469967603683472
CurrentTrain: epoch  8, batch     0 | loss: 6.9260755Losses:  1.8460279703140259 1.6998010873794556 0.1462269127368927
CurrentTrain: epoch  8, batch     1 | loss: 1.8460280Losses:  5.371512413024902 5.151759624481201 0.21975280344486237
CurrentTrain: epoch  9, batch     0 | loss: 5.3715124Losses:  2.2452681064605713 2.0020532608032227 0.24321477115154266
CurrentTrain: epoch  9, batch     1 | loss: 2.2452681
Losses:  0.24822655320167542 -0.0 0.24822655320167542
MemoryTrain:  epoch  0, batch     0 | loss: 0.2482266Losses:  0.2344444990158081 -0.0 0.2344444990158081
MemoryTrain:  epoch  1, batch     0 | loss: 0.2344445Losses:  0.22726142406463623 -0.0 0.22726142406463623
MemoryTrain:  epoch  2, batch     0 | loss: 0.2272614Losses:  0.22515282034873962 -0.0 0.22515282034873962
MemoryTrain:  epoch  3, batch     0 | loss: 0.2251528Losses:  0.21495012938976288 -0.0 0.21495012938976288
MemoryTrain:  epoch  4, batch     0 | loss: 0.2149501Losses:  0.21596360206604004 -0.0 0.21596360206604004
MemoryTrain:  epoch  5, batch     0 | loss: 0.2159636Losses:  0.21227259933948517 -0.0 0.21227259933948517
MemoryTrain:  epoch  6, batch     0 | loss: 0.2122726Losses:  0.21196340024471283 -0.0 0.21196340024471283
MemoryTrain:  epoch  7, batch     0 | loss: 0.2119634Losses:  0.21033699810504913 -0.0 0.21033699810504913
MemoryTrain:  epoch  8, batch     0 | loss: 0.2103370Losses:  0.2076912224292755 -0.0 0.2076912224292755
MemoryTrain:  epoch  9, batch     0 | loss: 0.2076912
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 65.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 66.07%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 67.19%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 66.88%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 67.05%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 68.23%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 67.31%   
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 22.92%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 25.00%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 23.96%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 32.14%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 40.62%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 47.22%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 52.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 56.82%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 60.42%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 61.54%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 58.04%   [EVAL] batch:   14 | acc: 12.50%,  total acc: 55.00%   [EVAL] batch:   15 | acc: 0.00%,  total acc: 51.56%   [EVAL] batch:   16 | acc: 0.00%,  total acc: 48.53%   [EVAL] batch:   17 | acc: 0.00%,  total acc: 45.83%   [EVAL] batch:   18 | acc: 6.25%,  total acc: 43.75%   [EVAL] batch:   19 | acc: 12.50%,  total acc: 42.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 44.94%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 47.44%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 49.73%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 51.82%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 53.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 55.53%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 56.94%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 58.48%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 59.91%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 60.62%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 61.29%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 62.30%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 62.67%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 62.84%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 63.32%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 63.14%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 63.44%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 63.57%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 63.54%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 63.66%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 64.06%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 64.58%   
cur_acc:  ['0.8674', '0.6731']
his_acc:  ['0.8674', '0.6458']
Clustering into  6  clusters
Clusters:  [3 1 1 3 4 2 0 1 2 4 0 3 0 5 4 4]
Losses:  7.688419342041016 7.336448669433594 0.35197073221206665
CurrentTrain: epoch  0, batch     0 | loss: 7.6884193Losses:  3.02662992477417 2.6456990242004395 0.38093101978302
CurrentTrain: epoch  0, batch     1 | loss: 3.0266299Losses:  6.79540491104126 6.5053629875183105 0.2900419235229492
CurrentTrain: epoch  1, batch     0 | loss: 6.7954049Losses:  2.7569665908813477 2.436824321746826 0.3201421797275543
CurrentTrain: epoch  1, batch     1 | loss: 2.7569666Losses:  7.241987705230713 6.9464030265808105 0.295584499835968
CurrentTrain: epoch  2, batch     0 | loss: 7.2419877Losses:  1.929021954536438 1.5821865797042847 0.3468353748321533
CurrentTrain: epoch  2, batch     1 | loss: 1.9290220Losses:  6.443446636199951 6.159804344177246 0.28364208340644836
CurrentTrain: epoch  3, batch     0 | loss: 6.4434466Losses:  2.246309280395508 1.9698469638824463 0.27646222710609436
CurrentTrain: epoch  3, batch     1 | loss: 2.2463093Losses:  5.83159065246582 5.571746826171875 0.2598438858985901
CurrentTrain: epoch  4, batch     0 | loss: 5.8315907Losses:  1.5613422393798828 1.2749521732330322 0.2863900065422058
CurrentTrain: epoch  4, batch     1 | loss: 1.5613422Losses:  6.044422626495361 5.7855072021484375 0.2589154541492462
CurrentTrain: epoch  5, batch     0 | loss: 6.0444226Losses:  2.3806731700897217 2.1319127082824707 0.24876056611537933
CurrentTrain: epoch  5, batch     1 | loss: 2.3806732Losses:  6.12001895904541 5.872582912445068 0.24743583798408508
CurrentTrain: epoch  6, batch     0 | loss: 6.1200190Losses:  1.968204379081726 1.7226241827011108 0.24558015167713165
CurrentTrain: epoch  6, batch     1 | loss: 1.9682044Losses:  5.7704877853393555 5.531970500946045 0.23851710557937622
CurrentTrain: epoch  7, batch     0 | loss: 5.7704878Losses:  2.272866725921631 2.0327954292297363 0.24007141590118408
CurrentTrain: epoch  7, batch     1 | loss: 2.2728667Losses:  5.54364538192749 5.304229259490967 0.23941633105278015
CurrentTrain: epoch  8, batch     0 | loss: 5.5436454Losses:  1.316893219947815 1.0820131301879883 0.23488011956214905
CurrentTrain: epoch  8, batch     1 | loss: 1.3168932Losses:  7.711320400238037 7.477725028991699 0.2335955798625946
CurrentTrain: epoch  9, batch     0 | loss: 7.7113204Losses:  3.825871467590332 3.6629273891448975 0.16294413805007935
CurrentTrain: epoch  9, batch     1 | loss: 3.8258715
Losses:  0.4997299611568451 -0.0 0.4997299611568451
MemoryTrain:  epoch  0, batch     0 | loss: 0.4997300Losses:  0.45383429527282715 -0.0 0.45383429527282715
MemoryTrain:  epoch  1, batch     0 | loss: 0.4538343Losses:  0.4707374572753906 -0.0 0.4707374572753906
MemoryTrain:  epoch  2, batch     0 | loss: 0.4707375Losses:  0.4314473271369934 -0.0 0.4314473271369934
MemoryTrain:  epoch  3, batch     0 | loss: 0.4314473Losses:  0.4213331639766693 -0.0 0.4213331639766693
MemoryTrain:  epoch  4, batch     0 | loss: 0.4213332Losses:  0.4231153726577759 -0.0 0.4231153726577759
MemoryTrain:  epoch  5, batch     0 | loss: 0.4231154Losses:  0.42801913619041443 -0.0 0.42801913619041443
MemoryTrain:  epoch  6, batch     0 | loss: 0.4280191Losses:  0.4123072624206543 -0.0 0.4123072624206543
MemoryTrain:  epoch  7, batch     0 | loss: 0.4123073Losses:  0.40619736909866333 -0.0 0.40619736909866333
MemoryTrain:  epoch  8, batch     0 | loss: 0.4061974Losses:  0.4055939316749573 -0.0 0.4055939316749573
MemoryTrain:  epoch  9, batch     0 | loss: 0.4055939
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 79.38%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 78.41%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 79.69%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 79.33%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 79.46%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 77.08%   
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 8.33%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 9.38%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 8.75%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 9.38%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 19.64%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 29.69%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 36.81%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 43.12%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 47.73%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 51.04%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 53.85%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 54.46%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 54.17%   [EVAL] batch:   15 | acc: 37.50%,  total acc: 53.12%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 52.94%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 53.12%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 52.96%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 53.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 55.65%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 57.67%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 59.51%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 60.94%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 63.46%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 64.35%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 66.81%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 67.08%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 67.74%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 68.36%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 68.56%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 68.57%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   35 | acc: 31.25%,  total acc: 67.71%   [EVAL] batch:   36 | acc: 25.00%,  total acc: 66.55%   [EVAL] batch:   37 | acc: 37.50%,  total acc: 65.79%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 64.58%   [EVAL] batch:   39 | acc: 12.50%,  total acc: 63.28%   [EVAL] batch:   40 | acc: 0.00%,  total acc: 61.74%   [EVAL] batch:   41 | acc: 0.00%,  total acc: 60.27%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 58.87%   [EVAL] batch:   43 | acc: 31.25%,  total acc: 58.24%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 58.47%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 58.56%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 58.64%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 59.24%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 59.82%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 60.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 61.03%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 61.66%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 62.03%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 62.27%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 62.16%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 62.28%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 62.94%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 63.04%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 63.45%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 63.02%   
cur_acc:  ['0.8674', '0.6731', '0.7708']
his_acc:  ['0.8674', '0.6458', '0.6302']
Clustering into  9  clusters
Clusters:  [2 0 0 2 1 7 4 8 7 3 6 2 4 5 1 3 0 0 0 7 3]
Losses:  7.975326061248779 7.764899253845215 0.21042685210704803
CurrentTrain: epoch  0, batch     0 | loss: 7.9753261Losses:  5.175912857055664 5.102721214294434 0.07319177687168121
CurrentTrain: epoch  0, batch     1 | loss: 5.1759129Losses:  8.57642936706543 8.423368453979492 0.15306052565574646
CurrentTrain: epoch  1, batch     0 | loss: 8.5764294Losses:  2.5465564727783203 2.40702748298645 0.13952893018722534
CurrentTrain: epoch  1, batch     1 | loss: 2.5465565Losses:  7.007534980773926 6.863592147827148 0.14394286274909973
CurrentTrain: epoch  2, batch     0 | loss: 7.0075350Losses:  1.4778631925582886 1.3502066135406494 0.12765660881996155
CurrentTrain: epoch  2, batch     1 | loss: 1.4778632Losses:  7.563507556915283 7.434258460998535 0.12924930453300476
CurrentTrain: epoch  3, batch     0 | loss: 7.5635076Losses:  3.246490716934204 3.1768980026245117 0.06959263980388641
CurrentTrain: epoch  3, batch     1 | loss: 3.2464907Losses:  8.057051658630371 7.932369232177734 0.12468200922012329
CurrentTrain: epoch  4, batch     0 | loss: 8.0570517Losses:  4.201497554779053 4.076169013977051 0.12532860040664673
CurrentTrain: epoch  4, batch     1 | loss: 4.2014976Losses:  6.504992961883545 6.383954048156738 0.12103910744190216
CurrentTrain: epoch  5, batch     0 | loss: 6.5049930Losses:  3.2323155403137207 3.172722816467285 0.05959272012114525
CurrentTrain: epoch  5, batch     1 | loss: 3.2323155Losses:  7.514090538024902 7.390732765197754 0.1233576163649559
CurrentTrain: epoch  6, batch     0 | loss: 7.5140905Losses:  1.615999698638916 1.4944454431533813 0.12155421078205109
CurrentTrain: epoch  6, batch     1 | loss: 1.6159997Losses:  6.906475067138672 6.788402557373047 0.11807261407375336
CurrentTrain: epoch  7, batch     0 | loss: 6.9064751Losses:  2.048985481262207 1.925402283668518 0.12358327955007553
CurrentTrain: epoch  7, batch     1 | loss: 2.0489855Losses:  7.657462120056152 7.543107986450195 0.11435391008853912
CurrentTrain: epoch  8, batch     0 | loss: 7.6574621Losses:  4.130468368530273 4.011133193969727 0.11933523416519165
CurrentTrain: epoch  8, batch     1 | loss: 4.1304684Losses:  8.358975410461426 8.24094009399414 0.11803501844406128
CurrentTrain: epoch  9, batch     0 | loss: 8.3589754Losses:  2.418229579925537 2.302948474884033 0.11528118699789047
CurrentTrain: epoch  9, batch     1 | loss: 2.4182296
Losses:  0.6018136739730835 -0.0 0.6018136739730835
MemoryTrain:  epoch  0, batch     0 | loss: 0.6018137Losses:  0.507231593132019 -0.0 0.507231593132019
MemoryTrain:  epoch  0, batch     1 | loss: 0.5072316Losses:  0.7574923634529114 -0.0 0.7574923634529114
MemoryTrain:  epoch  1, batch     0 | loss: 0.7574924Losses:  0.33649325370788574 -0.0 0.33649325370788574
MemoryTrain:  epoch  1, batch     1 | loss: 0.3364933Losses:  0.7143690586090088 -0.0 0.7143690586090088
MemoryTrain:  epoch  2, batch     0 | loss: 0.7143691Losses:  0.536718487739563 -0.0 0.536718487739563
MemoryTrain:  epoch  2, batch     1 | loss: 0.5367185Losses:  0.6275543570518494 -0.0 0.6275543570518494
MemoryTrain:  epoch  3, batch     0 | loss: 0.6275544Losses:  0.2849624752998352 -0.0 0.2849624752998352
MemoryTrain:  epoch  3, batch     1 | loss: 0.2849625Losses:  0.5675359964370728 -0.0 0.5675359964370728
MemoryTrain:  epoch  4, batch     0 | loss: 0.5675360Losses:  0.4548063278198242 -0.0 0.4548063278198242
MemoryTrain:  epoch  4, batch     1 | loss: 0.4548063Losses:  0.5906482934951782 -0.0 0.5906482934951782
MemoryTrain:  epoch  5, batch     0 | loss: 0.5906483Losses:  0.2708079218864441 -0.0 0.2708079218864441
MemoryTrain:  epoch  5, batch     1 | loss: 0.2708079Losses:  0.6411287784576416 -0.0 0.6411287784576416
MemoryTrain:  epoch  6, batch     0 | loss: 0.6411288Losses:  0.3666037321090698 -0.0 0.3666037321090698
MemoryTrain:  epoch  6, batch     1 | loss: 0.3666037Losses:  0.6675342321395874 -0.0 0.6675342321395874
MemoryTrain:  epoch  7, batch     0 | loss: 0.6675342Losses:  0.3751274049282074 -0.0 0.3751274049282074
MemoryTrain:  epoch  7, batch     1 | loss: 0.3751274Losses:  0.6119930148124695 -0.0 0.6119930148124695
MemoryTrain:  epoch  8, batch     0 | loss: 0.6119930Losses:  0.3819684684276581 -0.0 0.3819684684276581
MemoryTrain:  epoch  8, batch     1 | loss: 0.3819685Losses:  0.6064211130142212 -0.0 0.6064211130142212
MemoryTrain:  epoch  9, batch     0 | loss: 0.6064211Losses:  0.35384654998779297 -0.0 0.35384654998779297
MemoryTrain:  epoch  9, batch     1 | loss: 0.3538465
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 35.42%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 29.69%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 26.25%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 23.96%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 23.21%   [EVAL] batch:    7 | acc: 31.25%,  total acc: 24.22%   [EVAL] batch:    8 | acc: 18.75%,  total acc: 23.61%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 24.38%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 25.00%   [EVAL] batch:   11 | acc: 18.75%,  total acc: 24.48%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 25.48%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 27.23%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 29.17%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 31.25%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 33.82%   [EVAL] batch:   17 | acc: 37.50%,  total acc: 34.03%   [EVAL] batch:   18 | acc: 6.25%,  total acc: 32.57%   [EVAL] batch:   19 | acc: 0.00%,  total acc: 30.94%   [EVAL] batch:   20 | acc: 0.00%,  total acc: 29.46%   [EVAL] batch:   21 | acc: 0.00%,  total acc: 28.12%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 3.12%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 2.08%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 3.12%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 3.75%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 5.21%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 15.18%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 25.78%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 33.33%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 38.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 43.18%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 46.88%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 48.08%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 48.66%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 48.33%   [EVAL] batch:   15 | acc: 31.25%,  total acc: 47.27%   [EVAL] batch:   16 | acc: 37.50%,  total acc: 46.69%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 46.88%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 46.38%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 46.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 49.11%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 51.42%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 53.53%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 55.21%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 57.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 58.65%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 59.72%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 60.71%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 61.64%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 62.29%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 62.90%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 63.48%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 64.39%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 64.52%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 65.18%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 64.58%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 62.84%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 62.01%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 61.06%   [EVAL] batch:   39 | acc: 6.25%,  total acc: 59.69%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 59.15%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 58.78%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 58.72%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 58.95%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 59.72%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 59.24%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 58.38%   [EVAL] batch:   47 | acc: 25.00%,  total acc: 57.68%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 57.65%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 57.50%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 58.09%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 58.77%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 59.20%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 59.61%   [EVAL] batch:   54 | acc: 43.75%,  total acc: 59.32%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 59.49%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 59.76%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 60.24%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 60.59%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 60.42%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 60.04%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 59.78%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 59.13%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 58.30%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 57.69%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 56.91%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 56.62%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 56.16%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 55.80%   [EVAL] batch:   69 | acc: 18.75%,  total acc: 55.27%   [EVAL] batch:   70 | acc: 25.00%,  total acc: 54.84%   [EVAL] batch:   71 | acc: 31.25%,  total acc: 54.51%   [EVAL] batch:   72 | acc: 43.75%,  total acc: 54.37%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 54.22%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 54.42%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 54.61%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 54.63%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 54.17%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 53.48%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 52.81%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 52.16%   
cur_acc:  ['0.8674', '0.6731', '0.7708', '0.2812']
his_acc:  ['0.8674', '0.6458', '0.6302', '0.5216']
Clustering into  12  clusters
Clusters:  [ 6  0  0  8  1  4  2  9  4  3  5  6  2  7 10  3  0  0  0  4  3  6 11  5
  3  1]
Losses:  9.4042387008667 8.683511734008789 0.7207273840904236
CurrentTrain: epoch  0, batch     0 | loss: 9.4042387Losses:  3.185577869415283 2.555443525314331 0.6301342844963074
CurrentTrain: epoch  0, batch     1 | loss: 3.1855779Losses:  7.906379699707031 7.3631720542907715 0.5432075262069702
CurrentTrain: epoch  1, batch     0 | loss: 7.9063797Losses:  4.340073108673096 3.6386826038360596 0.7013904452323914
CurrentTrain: epoch  1, batch     1 | loss: 4.3400731Losses:  8.758013725280762 8.190540313720703 0.5674734115600586
CurrentTrain: epoch  2, batch     0 | loss: 8.7580137Losses:  3.0003483295440674 2.650404691696167 0.3499436676502228
CurrentTrain: epoch  2, batch     1 | loss: 3.0003483Losses:  7.457051753997803 6.977715015411377 0.4793366491794586
CurrentTrain: epoch  3, batch     0 | loss: 7.4570518Losses:  2.9614977836608887 2.5806262493133545 0.38087165355682373
CurrentTrain: epoch  3, batch     1 | loss: 2.9614978Losses:  6.148139476776123 5.685367107391357 0.4627721905708313
CurrentTrain: epoch  4, batch     0 | loss: 6.1481395Losses:  1.7326416969299316 1.2832509279251099 0.44939079880714417
CurrentTrain: epoch  4, batch     1 | loss: 1.7326417Losses:  6.806371688842773 6.3568830490112305 0.4494886100292206
CurrentTrain: epoch  5, batch     0 | loss: 6.8063717Losses:  2.4969255924224854 2.169825792312622 0.3270997703075409
CurrentTrain: epoch  5, batch     1 | loss: 2.4969256Losses:  6.182415008544922 5.73275089263916 0.4496641755104065
CurrentTrain: epoch  6, batch     0 | loss: 6.1824150Losses:  1.8990390300750732 1.5478743314743042 0.35116469860076904
CurrentTrain: epoch  6, batch     1 | loss: 1.8990390Losses:  6.063917636871338 5.658974647521973 0.4049430787563324
CurrentTrain: epoch  7, batch     0 | loss: 6.0639176Losses:  2.0498509407043457 1.6571722030639648 0.39267873764038086
CurrentTrain: epoch  7, batch     1 | loss: 2.0498509Losses:  5.449357509613037 5.051645278930664 0.39771202206611633
CurrentTrain: epoch  8, batch     0 | loss: 5.4493575Losses:  1.3284533023834229 0.9781439304351807 0.35030943155288696
CurrentTrain: epoch  8, batch     1 | loss: 1.3284533Losses:  5.907588958740234 5.537169456481934 0.37041962146759033
CurrentTrain: epoch  9, batch     0 | loss: 5.9075890Losses:  1.9550492763519287 1.6029071807861328 0.3521420657634735
CurrentTrain: epoch  9, batch     1 | loss: 1.9550493
Losses:  0.7038257122039795 -0.0 0.7038257122039795
MemoryTrain:  epoch  0, batch     0 | loss: 0.7038257Losses:  0.7702491283416748 -0.0 0.7702491283416748
MemoryTrain:  epoch  0, batch     1 | loss: 0.7702491Losses:  0.583987832069397 -0.0 0.583987832069397
MemoryTrain:  epoch  1, batch     0 | loss: 0.5839878Losses:  0.8432440161705017 -0.0 0.8432440161705017
MemoryTrain:  epoch  1, batch     1 | loss: 0.8432440Losses:  0.7900111079216003 -0.0 0.7900111079216003
MemoryTrain:  epoch  2, batch     0 | loss: 0.7900111Losses:  0.5796600580215454 -0.0 0.5796600580215454
MemoryTrain:  epoch  2, batch     1 | loss: 0.5796601Losses:  0.8381273150444031 -0.0 0.8381273150444031
MemoryTrain:  epoch  3, batch     0 | loss: 0.8381273Losses:  0.22979998588562012 -0.0 0.22979998588562012
MemoryTrain:  epoch  3, batch     1 | loss: 0.2298000Losses:  0.6142929196357727 -0.0 0.6142929196357727
MemoryTrain:  epoch  4, batch     0 | loss: 0.6142929Losses:  0.6780087947845459 -0.0 0.6780087947845459
MemoryTrain:  epoch  4, batch     1 | loss: 0.6780088Losses:  0.6133156418800354 -0.0 0.6133156418800354
MemoryTrain:  epoch  5, batch     0 | loss: 0.6133156Losses:  0.5597565174102783 -0.0 0.5597565174102783
MemoryTrain:  epoch  5, batch     1 | loss: 0.5597565Losses:  0.8657704591751099 -0.0 0.8657704591751099
MemoryTrain:  epoch  6, batch     0 | loss: 0.8657705Losses:  0.4826100766658783 -0.0 0.4826100766658783
MemoryTrain:  epoch  6, batch     1 | loss: 0.4826101Losses:  0.8654582500457764 -0.0 0.8654582500457764
MemoryTrain:  epoch  7, batch     0 | loss: 0.8654583Losses:  0.40525633096694946 -0.0 0.40525633096694946
MemoryTrain:  epoch  7, batch     1 | loss: 0.4052563Losses:  0.7721387147903442 -0.0 0.7721387147903442
MemoryTrain:  epoch  8, batch     0 | loss: 0.7721387Losses:  0.550107479095459 -0.0 0.550107479095459
MemoryTrain:  epoch  8, batch     1 | loss: 0.5501075Losses:  0.8641431331634521 -0.0 0.8641431331634521
MemoryTrain:  epoch  9, batch     0 | loss: 0.8641431Losses:  0.5669528245925903 -0.0 0.5669528245925903
MemoryTrain:  epoch  9, batch     1 | loss: 0.5669528
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 60.00%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 59.38%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 63.39%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 65.28%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 63.12%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 60.80%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 66.83%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 69.20%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 71.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 73.05%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 74.63%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 71.88%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 3.12%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 2.08%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 3.12%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 3.75%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 5.21%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 15.18%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 25.78%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 33.33%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 39.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 44.89%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 48.44%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 50.00%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 50.45%   [EVAL] batch:   14 | acc: 37.50%,  total acc: 49.58%   [EVAL] batch:   15 | acc: 31.25%,  total acc: 48.44%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 48.53%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 48.61%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 48.03%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 48.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 50.60%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 52.84%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 54.89%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 56.77%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 58.50%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 59.86%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 60.88%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 61.83%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 63.15%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 63.75%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 64.72%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 65.43%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 65.34%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 64.71%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 64.82%   [EVAL] batch:   35 | acc: 31.25%,  total acc: 63.89%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 62.16%   [EVAL] batch:   37 | acc: 18.75%,  total acc: 61.02%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 60.42%   [EVAL] batch:   39 | acc: 6.25%,  total acc: 59.06%   [EVAL] batch:   40 | acc: 6.25%,  total acc: 57.77%   [EVAL] batch:   41 | acc: 6.25%,  total acc: 56.55%   [EVAL] batch:   42 | acc: 31.25%,  total acc: 55.96%   [EVAL] batch:   43 | acc: 6.25%,  total acc: 54.83%   [EVAL] batch:   44 | acc: 12.50%,  total acc: 53.89%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 54.35%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 54.92%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 55.60%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 56.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 57.48%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 58.29%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 58.96%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 59.49%   [EVAL] batch:   54 | acc: 43.75%,  total acc: 59.20%   [EVAL] batch:   55 | acc: 31.25%,  total acc: 58.71%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 58.33%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 57.87%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 57.63%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 57.29%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 57.07%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 56.85%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 56.25%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 55.37%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 54.81%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 54.07%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 54.10%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 53.68%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 53.53%   [EVAL] batch:   69 | acc: 62.50%,  total acc: 53.66%   [EVAL] batch:   70 | acc: 43.75%,  total acc: 53.52%   [EVAL] batch:   71 | acc: 37.50%,  total acc: 53.30%   [EVAL] batch:   72 | acc: 37.50%,  total acc: 53.08%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 53.04%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 53.25%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 53.37%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 53.49%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 53.04%   [EVAL] batch:   78 | acc: 6.25%,  total acc: 52.45%   [EVAL] batch:   79 | acc: 6.25%,  total acc: 51.88%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 51.47%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 51.52%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 51.66%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 51.86%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 51.91%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 51.89%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 52.01%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 52.49%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 52.74%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 52.78%   [EVAL] batch:   90 | acc: 43.75%,  total acc: 52.68%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 52.65%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 53.16%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 53.66%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 54.14%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 54.62%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 55.09%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 55.55%   [EVAL] batch:   98 | acc: 6.25%,  total acc: 55.05%   
cur_acc:  ['0.8674', '0.6731', '0.7708', '0.2812', '0.7188']
his_acc:  ['0.8674', '0.6458', '0.6302', '0.5216', '0.5505']
Clustering into  14  clusters
Clusters:  [ 1  0  0 12  2 10  3 11 10  4  1  1  3  7  2  4  0  0  0 10  4  1  5  1
  4 13  6  8  1  0  9]
Losses:  6.594672203063965 6.179257392883301 0.4154147207736969
CurrentTrain: epoch  0, batch     0 | loss: 6.5946722Losses:  1.3860818147659302 1.0027748346328735 0.38330695033073425
CurrentTrain: epoch  0, batch     1 | loss: 1.3860818Losses:  6.140873432159424 5.768640995025635 0.37223243713378906
CurrentTrain: epoch  1, batch     0 | loss: 6.1408734Losses:  2.507935047149658 2.131739377975464 0.3761957287788391
CurrentTrain: epoch  1, batch     1 | loss: 2.5079350Losses:  5.5165839195251465 5.1593017578125 0.35728195309638977
CurrentTrain: epoch  2, batch     0 | loss: 5.5165839Losses:  1.5184954404830933 1.165765643119812 0.35272979736328125
CurrentTrain: epoch  2, batch     1 | loss: 1.5184954Losses:  7.846597671508789 7.498118877410889 0.34847867488861084
CurrentTrain: epoch  3, batch     0 | loss: 7.8465977Losses:  4.250617027282715 4.071420669555664 0.17919640243053436
CurrentTrain: epoch  3, batch     1 | loss: 4.2506170Losses:  6.200115203857422 5.864749908447266 0.3353651463985443
CurrentTrain: epoch  4, batch     0 | loss: 6.2001152Losses:  1.9585754871368408 1.6890743970870972 0.26950106024742126
CurrentTrain: epoch  4, batch     1 | loss: 1.9585755Losses:  5.623058319091797 5.291733741760254 0.33132442831993103
CurrentTrain: epoch  5, batch     0 | loss: 5.6230583Losses:  1.744698166847229 1.4155222177505493 0.3291759788990021
CurrentTrain: epoch  5, batch     1 | loss: 1.7446982Losses:  6.336932182312012 6.012959957122803 0.3239721953868866
CurrentTrain: epoch  6, batch     0 | loss: 6.3369322Losses:  2.503347396850586 2.2441017627716064 0.2592456042766571
CurrentTrain: epoch  6, batch     1 | loss: 2.5033474Losses:  5.450265407562256 5.131198406219482 0.3190671503543854
CurrentTrain: epoch  7, batch     0 | loss: 5.4502654Losses:  1.6772379875183105 1.36129629611969 0.315941721200943
CurrentTrain: epoch  7, batch     1 | loss: 1.6772380Losses:  5.299988269805908 4.985236644744873 0.31475183367729187
CurrentTrain: epoch  8, batch     0 | loss: 5.2999883Losses:  1.783204436302185 1.4596648216247559 0.3235395848751068
CurrentTrain: epoch  8, batch     1 | loss: 1.7832044Losses:  5.308966159820557 5.000352382659912 0.3086138963699341
CurrentTrain: epoch  9, batch     0 | loss: 5.3089662Losses:  1.7477028369903564 1.4350839853286743 0.31261879205703735
CurrentTrain: epoch  9, batch     1 | loss: 1.7477028
Losses:  0.793641984462738 -0.0 0.793641984462738
MemoryTrain:  epoch  0, batch     0 | loss: 0.7936420Losses:  0.7963942885398865 -0.0 0.7963942885398865
MemoryTrain:  epoch  0, batch     1 | loss: 0.7963943Losses:  0.7799006700515747 -0.0 0.7799006700515747
MemoryTrain:  epoch  1, batch     0 | loss: 0.7799007Losses:  0.7034555077552795 -0.0 0.7034555077552795
MemoryTrain:  epoch  1, batch     1 | loss: 0.7034555Losses:  0.7805969715118408 -0.0 0.7805969715118408
MemoryTrain:  epoch  2, batch     0 | loss: 0.7805970Losses:  0.6072636246681213 -0.0 0.6072636246681213
MemoryTrain:  epoch  2, batch     1 | loss: 0.6072636Losses:  0.5952409505844116 -0.0 0.5952409505844116
MemoryTrain:  epoch  3, batch     0 | loss: 0.5952410Losses:  0.9107059836387634 -0.0 0.9107059836387634
MemoryTrain:  epoch  3, batch     1 | loss: 0.9107060Losses:  0.782418429851532 -0.0 0.782418429851532
MemoryTrain:  epoch  4, batch     0 | loss: 0.7824184Losses:  0.6260399222373962 -0.0 0.6260399222373962
MemoryTrain:  epoch  4, batch     1 | loss: 0.6260399Losses:  0.7151563167572021 -0.0 0.7151563167572021
MemoryTrain:  epoch  5, batch     0 | loss: 0.7151563Losses:  0.8311368823051453 -0.0 0.8311368823051453
MemoryTrain:  epoch  5, batch     1 | loss: 0.8311369Losses:  0.8444684147834778 -0.0 0.8444684147834778
MemoryTrain:  epoch  6, batch     0 | loss: 0.8444684Losses:  0.6005194187164307 -0.0 0.6005194187164307
MemoryTrain:  epoch  6, batch     1 | loss: 0.6005194Losses:  0.6910148859024048 -0.0 0.6910148859024048
MemoryTrain:  epoch  7, batch     0 | loss: 0.6910149Losses:  0.7688471674919128 -0.0 0.7688471674919128
MemoryTrain:  epoch  7, batch     1 | loss: 0.7688472Losses:  0.7863032817840576 -0.0 0.7863032817840576
MemoryTrain:  epoch  8, batch     0 | loss: 0.7863033Losses:  0.7212430238723755 -0.0 0.7212430238723755
MemoryTrain:  epoch  8, batch     1 | loss: 0.7212430Losses:  0.7251141667366028 -0.0 0.7251141667366028
MemoryTrain:  epoch  9, batch     0 | loss: 0.7251142Losses:  0.8440221548080444 -0.0 0.8440221548080444
MemoryTrain:  epoch  9, batch     1 | loss: 0.8440222
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 99.11%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 98.44%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 95.14%   [EVAL] batch:    9 | acc: 25.00%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 81.25%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 77.40%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 74.11%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 4.17%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 7.81%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 7.50%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 8.33%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 18.75%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 28.91%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 36.11%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 41.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 47.16%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 50.52%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 50.96%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 50.45%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 50.74%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 51.39%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 51.64%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 52.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 54.46%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 56.53%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 58.42%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 59.90%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 61.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 62.98%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 63.89%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 64.73%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 65.52%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 65.93%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 66.21%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 66.10%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 65.81%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 66.07%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 65.28%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 63.68%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 62.83%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 62.02%   [EVAL] batch:   39 | acc: 18.75%,  total acc: 60.94%   [EVAL] batch:   40 | acc: 25.00%,  total acc: 60.06%   [EVAL] batch:   41 | acc: 31.25%,  total acc: 59.38%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 59.16%   [EVAL] batch:   43 | acc: 6.25%,  total acc: 57.95%   [EVAL] batch:   44 | acc: 18.75%,  total acc: 57.08%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 57.20%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 57.31%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 57.55%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 57.78%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 58.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 58.95%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 59.74%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 60.38%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 60.76%   [EVAL] batch:   54 | acc: 43.75%,  total acc: 60.45%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 60.16%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 59.98%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 59.91%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 59.75%   [EVAL] batch:   59 | acc: 25.00%,  total acc: 59.17%   [EVAL] batch:   60 | acc: 25.00%,  total acc: 58.61%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 57.96%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 57.24%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 56.45%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 55.87%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 55.11%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 55.04%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 54.60%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 54.35%   [EVAL] batch:   69 | acc: 56.25%,  total acc: 54.37%   [EVAL] batch:   70 | acc: 43.75%,  total acc: 54.23%   [EVAL] batch:   71 | acc: 43.75%,  total acc: 54.08%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 53.68%   [EVAL] batch:   73 | acc: 25.00%,  total acc: 53.29%   [EVAL] batch:   74 | acc: 43.75%,  total acc: 53.17%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 53.21%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 53.17%   [EVAL] batch:   77 | acc: 12.50%,  total acc: 52.64%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 51.98%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 51.33%   [EVAL] batch:   80 | acc: 12.50%,  total acc: 50.85%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 50.69%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 50.23%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 49.85%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 49.49%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 49.35%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 49.35%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 49.86%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 50.14%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 49.93%   [EVAL] batch:   90 | acc: 37.50%,  total acc: 49.79%   [EVAL] batch:   91 | acc: 31.25%,  total acc: 49.59%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 50.13%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 50.66%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 51.18%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 51.69%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 52.19%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 52.68%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 53.16%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 53.62%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 54.08%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 54.53%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 54.98%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 55.41%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 55.77%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 56.13%   [EVAL] batch:  106 | acc: 75.00%,  total acc: 56.31%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 56.02%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 55.85%   [EVAL] batch:  109 | acc: 50.00%,  total acc: 55.80%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 55.57%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 55.41%   
cur_acc:  ['0.8674', '0.6731', '0.7708', '0.2812', '0.7188', '0.7411']
his_acc:  ['0.8674', '0.6458', '0.6302', '0.5216', '0.5505', '0.5541']
Clustering into  17  clusters
Clusters:  [ 0 10  9 11  1  7  3 13  7  4  0  0  3 12 15  4  9  9 16  7  4  0 14  0
  4  1  5  8  0  9  6  2  0  4 10  0]
Losses:  7.732476711273193 7.382425785064697 0.35005077719688416
CurrentTrain: epoch  0, batch     0 | loss: 7.7324767Losses:  2.954850435256958 2.714921236038208 0.23992925882339478
CurrentTrain: epoch  0, batch     1 | loss: 2.9548504Losses:  7.000844955444336 6.700281620025635 0.3005632162094116
CurrentTrain: epoch  1, batch     0 | loss: 7.0008450Losses:  2.078727960586548 1.79420006275177 0.28452786803245544
CurrentTrain: epoch  1, batch     1 | loss: 2.0787280Losses:  7.140256881713867 6.863530158996582 0.27672672271728516
CurrentTrain: epoch  2, batch     0 | loss: 7.1402569Losses:  2.1701197624206543 1.8955059051513672 0.27461397647857666
CurrentTrain: epoch  2, batch     1 | loss: 2.1701198Losses:  6.087935924530029 5.824458122253418 0.2634778320789337
CurrentTrain: epoch  3, batch     0 | loss: 6.0879359Losses:  2.7375030517578125 2.4702847003936768 0.26721829175949097
CurrentTrain: epoch  3, batch     1 | loss: 2.7375031Losses:  6.5630621910095215 6.292078018188477 0.2709841728210449
CurrentTrain: epoch  4, batch     0 | loss: 6.5630622Losses:  1.8113973140716553 1.5561681985855103 0.2552291452884674
CurrentTrain: epoch  4, batch     1 | loss: 1.8113973Losses:  7.05438756942749 6.806181907653809 0.24820585548877716
CurrentTrain: epoch  5, batch     0 | loss: 7.0543876Losses:  3.1199951171875 2.963231086730957 0.156763955950737
CurrentTrain: epoch  5, batch     1 | loss: 3.1199951Losses:  7.23255729675293 6.990823745727539 0.24173365533351898
CurrentTrain: epoch  6, batch     0 | loss: 7.2325573Losses:  4.188755035400391 4.032010078430176 0.15674500167369843
CurrentTrain: epoch  6, batch     1 | loss: 4.1887550Losses:  7.664204120635986 7.417237281799316 0.24696679413318634
CurrentTrain: epoch  7, batch     0 | loss: 7.6642041Losses:  3.189696788787842 3.036417245864868 0.1532796025276184
CurrentTrain: epoch  7, batch     1 | loss: 3.1896968Losses:  6.248558521270752 6.002145767211914 0.24641266465187073
CurrentTrain: epoch  8, batch     0 | loss: 6.2485585Losses:  1.7792201042175293 1.5465062856674194 0.23271380364894867
CurrentTrain: epoch  8, batch     1 | loss: 1.7792201Losses:  8.433540344238281 8.201086044311523 0.23245394229888916
CurrentTrain: epoch  9, batch     0 | loss: 8.4335403Losses:  4.163331031799316 4.007308006286621 0.15602320432662964
CurrentTrain: epoch  9, batch     1 | loss: 4.1633310
Losses:  0.8817344307899475 -0.0 0.8817344307899475
MemoryTrain:  epoch  0, batch     0 | loss: 0.8817344Losses:  0.8357962965965271 -0.0 0.8357962965965271
MemoryTrain:  epoch  0, batch     1 | loss: 0.8357963Losses:  0.4597591757774353 -0.0 0.4597591757774353
MemoryTrain:  epoch  0, batch     2 | loss: 0.4597592Losses:  0.7808090448379517 -0.0 0.7808090448379517
MemoryTrain:  epoch  1, batch     0 | loss: 0.7808090Losses:  0.7596052289009094 -0.0 0.7596052289009094
MemoryTrain:  epoch  1, batch     1 | loss: 0.7596052Losses:  0.39056843519210815 -0.0 0.39056843519210815
MemoryTrain:  epoch  1, batch     2 | loss: 0.3905684Losses:  0.789006233215332 -0.0 0.789006233215332
MemoryTrain:  epoch  2, batch     0 | loss: 0.7890062Losses:  0.9220134019851685 -0.0 0.9220134019851685
MemoryTrain:  epoch  2, batch     1 | loss: 0.9220134Losses:  0.2978802025318146 -0.0 0.2978802025318146
MemoryTrain:  epoch  2, batch     2 | loss: 0.2978802Losses:  0.8544860482215881 -0.0 0.8544860482215881
MemoryTrain:  epoch  3, batch     0 | loss: 0.8544860Losses:  0.7843777537345886 -0.0 0.7843777537345886
MemoryTrain:  epoch  3, batch     1 | loss: 0.7843778Losses:  0.3389686644077301 -0.0 0.3389686644077301
MemoryTrain:  epoch  3, batch     2 | loss: 0.3389687Losses:  0.8308995962142944 -0.0 0.8308995962142944
MemoryTrain:  epoch  4, batch     0 | loss: 0.8308996Losses:  0.7610971331596375 -0.0 0.7610971331596375
MemoryTrain:  epoch  4, batch     1 | loss: 0.7610971Losses:  0.3993780016899109 -0.0 0.3993780016899109
MemoryTrain:  epoch  4, batch     2 | loss: 0.3993780Losses:  0.8938996195793152 -0.0 0.8938996195793152
MemoryTrain:  epoch  5, batch     0 | loss: 0.8938996Losses:  0.7610287070274353 -0.0 0.7610287070274353
MemoryTrain:  epoch  5, batch     1 | loss: 0.7610287Losses:  0.5255475044250488 -0.0 0.5255475044250488
MemoryTrain:  epoch  5, batch     2 | loss: 0.5255475Losses:  0.8212287425994873 -0.0 0.8212287425994873
MemoryTrain:  epoch  6, batch     0 | loss: 0.8212287Losses:  0.7214903235435486 -0.0 0.7214903235435486
MemoryTrain:  epoch  6, batch     1 | loss: 0.7214903Losses:  0.36364617943763733 -0.0 0.36364617943763733
MemoryTrain:  epoch  6, batch     2 | loss: 0.3636462Losses:  0.9182605147361755 -0.0 0.9182605147361755
MemoryTrain:  epoch  7, batch     0 | loss: 0.9182605Losses:  0.7938334345817566 -0.0 0.7938334345817566
MemoryTrain:  epoch  7, batch     1 | loss: 0.7938334Losses:  0.09392017126083374 -0.0 0.09392017126083374
MemoryTrain:  epoch  7, batch     2 | loss: 0.0939202Losses:  0.648378849029541 -0.0 0.648378849029541
MemoryTrain:  epoch  8, batch     0 | loss: 0.6483788Losses:  0.7479697465896606 -0.0 0.7479697465896606
MemoryTrain:  epoch  8, batch     1 | loss: 0.7479697Losses:  0.22222408652305603 -0.0 0.22222408652305603
MemoryTrain:  epoch  8, batch     2 | loss: 0.2222241Losses:  0.7886191606521606 -0.0 0.7886191606521606
MemoryTrain:  epoch  9, batch     0 | loss: 0.7886192Losses:  0.9229292869567871 -0.0 0.9229292869567871
MemoryTrain:  epoch  9, batch     1 | loss: 0.9229293Losses:  0.32843926548957825 -0.0 0.32843926548957825
MemoryTrain:  epoch  9, batch     2 | loss: 0.3284393
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 51.56%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 47.92%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 47.32%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 52.34%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 59.09%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 58.33%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 59.13%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 55.80%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 3.12%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 2.08%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 3.12%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 2.50%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 4.17%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 14.29%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 25.00%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 32.64%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 38.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 44.32%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 47.92%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 48.08%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 46.43%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 47.50%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 47.66%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 48.90%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 49.65%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 50.33%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 51.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 53.87%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 55.97%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 57.88%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 59.38%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 61.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 63.43%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 64.73%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 65.95%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 66.04%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 66.94%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 67.38%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 67.42%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 66.91%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 67.14%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 66.32%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 64.70%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 63.82%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 62.82%   [EVAL] batch:   39 | acc: 25.00%,  total acc: 61.88%   [EVAL] batch:   40 | acc: 0.00%,  total acc: 60.37%   [EVAL] batch:   41 | acc: 6.25%,  total acc: 59.08%   [EVAL] batch:   42 | acc: 6.25%,  total acc: 57.85%   [EVAL] batch:   43 | acc: 6.25%,  total acc: 56.68%   [EVAL] batch:   44 | acc: 12.50%,  total acc: 55.69%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 55.30%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 54.92%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 54.95%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 54.85%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 54.75%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 55.02%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 55.53%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 55.90%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 56.25%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:   55 | acc: 12.50%,  total acc: 55.47%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 55.15%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 54.53%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 54.13%   [EVAL] batch:   59 | acc: 12.50%,  total acc: 53.44%   [EVAL] batch:   60 | acc: 0.00%,  total acc: 52.56%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 51.71%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 51.09%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 50.49%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 50.00%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 49.34%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 48.79%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 48.07%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 47.74%   [EVAL] batch:   69 | acc: 25.00%,  total acc: 47.41%   [EVAL] batch:   70 | acc: 12.50%,  total acc: 46.92%   [EVAL] batch:   71 | acc: 31.25%,  total acc: 46.70%   [EVAL] batch:   72 | acc: 0.00%,  total acc: 46.06%   [EVAL] batch:   73 | acc: 6.25%,  total acc: 45.52%   [EVAL] batch:   74 | acc: 25.00%,  total acc: 45.25%   [EVAL] batch:   75 | acc: 12.50%,  total acc: 44.82%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 44.56%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 44.47%   [EVAL] batch:   78 | acc: 12.50%,  total acc: 44.07%   [EVAL] batch:   79 | acc: 12.50%,  total acc: 43.67%   [EVAL] batch:   80 | acc: 12.50%,  total acc: 43.29%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 42.99%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 42.77%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 42.56%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 42.28%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 42.08%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 42.24%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 42.68%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 43.05%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 42.57%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 42.10%   [EVAL] batch:   91 | acc: 25.00%,  total acc: 41.92%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 42.54%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 43.15%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 43.75%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 44.34%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 44.91%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 45.47%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 46.02%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 46.56%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 47.09%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 47.55%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 48.00%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 48.50%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 48.93%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 49.41%   [EVAL] batch:  106 | acc: 75.00%,  total acc: 49.65%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 49.54%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 49.54%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 49.26%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 48.93%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 48.88%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 48.89%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 48.96%   [EVAL] batch:  114 | acc: 50.00%,  total acc: 48.97%   [EVAL] batch:  115 | acc: 31.25%,  total acc: 48.81%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 48.93%   [EVAL] batch:  117 | acc: 25.00%,  total acc: 48.73%   [EVAL] batch:  118 | acc: 50.00%,  total acc: 48.74%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 49.11%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 49.28%   [EVAL] batch:  121 | acc: 75.00%,  total acc: 49.49%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 49.64%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 49.75%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 49.80%   [EVAL] batch:  125 | acc: 12.50%,  total acc: 49.50%   
cur_acc:  ['0.8674', '0.6731', '0.7708', '0.2812', '0.7188', '0.7411', '0.5580']
his_acc:  ['0.8674', '0.6458', '0.6302', '0.5216', '0.5505', '0.5541', '0.4950']
Clustering into  19  clusters
Clusters:  [ 0  4  5  0  2 11  3 14 11  1  7  0  3 15 18  1  5  5 17 11  1  0  9  7
  1  2  6  8  0  5 12 13  0  1  4  7  0 10  5  6 16]
Losses:  8.365497589111328 7.838565826416016 0.526931881904602
CurrentTrain: epoch  0, batch     0 | loss: 8.3654976Losses:  3.7828383445739746 3.2814602851867676 0.5013779997825623
CurrentTrain: epoch  0, batch     1 | loss: 3.7828383Losses:  6.9925737380981445 6.5133867263793945 0.4791872501373291
CurrentTrain: epoch  1, batch     0 | loss: 6.9925737Losses:  2.7411115169525146 2.291865110397339 0.4492464065551758
CurrentTrain: epoch  1, batch     1 | loss: 2.7411115Losses:  7.372223854064941 6.874436378479004 0.4977876842021942
CurrentTrain: epoch  2, batch     0 | loss: 7.3722239Losses:  2.310492992401123 1.9527647495269775 0.35772836208343506
CurrentTrain: epoch  2, batch     1 | loss: 2.3104930Losses:  7.852083206176758 7.386300086975098 0.4657832086086273
CurrentTrain: epoch  3, batch     0 | loss: 7.8520832Losses:  2.5602316856384277 2.0751631259918213 0.48506855964660645
CurrentTrain: epoch  3, batch     1 | loss: 2.5602317Losses:  6.863687515258789 6.45835018157959 0.40533730387687683
CurrentTrain: epoch  4, batch     0 | loss: 6.8636875Losses:  3.3405189514160156 2.9059088230133057 0.4346102178096771
CurrentTrain: epoch  4, batch     1 | loss: 3.3405190Losses:  7.401066780090332 6.994322776794434 0.4067440629005432
CurrentTrain: epoch  5, batch     0 | loss: 7.4010668Losses:  1.8064820766448975 1.4363102912902832 0.37017175555229187
CurrentTrain: epoch  5, batch     1 | loss: 1.8064821Losses:  6.694915771484375 6.353508949279785 0.34140685200691223
CurrentTrain: epoch  6, batch     0 | loss: 6.6949158Losses:  2.779014825820923 2.3629908561706543 0.4160239100456238
CurrentTrain: epoch  6, batch     1 | loss: 2.7790148Losses:  6.346916198730469 5.973724842071533 0.37319156527519226
CurrentTrain: epoch  7, batch     0 | loss: 6.3469162Losses:  3.778170108795166 3.4213883876800537 0.35678163170814514
CurrentTrain: epoch  7, batch     1 | loss: 3.7781701Losses:  5.950716495513916 5.610313415527344 0.3404029309749603
CurrentTrain: epoch  8, batch     0 | loss: 5.9507165Losses:  1.3494465351104736 0.9262295961380005 0.42321696877479553
CurrentTrain: epoch  8, batch     1 | loss: 1.3494465Losses:  7.57611608505249 7.216151237487793 0.35996484756469727
CurrentTrain: epoch  9, batch     0 | loss: 7.5761161Losses:  3.2768726348876953 2.9338409900665283 0.343031644821167
CurrentTrain: epoch  9, batch     1 | loss: 3.2768726
Losses:  0.7718551158905029 -0.0 0.7718551158905029
MemoryTrain:  epoch  0, batch     0 | loss: 0.7718551Losses:  0.8163905143737793 -0.0 0.8163905143737793
MemoryTrain:  epoch  0, batch     1 | loss: 0.8163905Losses:  0.5899472236633301 -0.0 0.5899472236633301
MemoryTrain:  epoch  0, batch     2 | loss: 0.5899472Losses:  0.7702807188034058 -0.0 0.7702807188034058
MemoryTrain:  epoch  1, batch     0 | loss: 0.7702807Losses:  1.0268741846084595 -0.0 1.0268741846084595
MemoryTrain:  epoch  1, batch     1 | loss: 1.0268742Losses:  0.7238863706588745 -0.0 0.7238863706588745
MemoryTrain:  epoch  1, batch     2 | loss: 0.7238864Losses:  0.8773190379142761 -0.0 0.8773190379142761
MemoryTrain:  epoch  2, batch     0 | loss: 0.8773190Losses:  0.9001460075378418 -0.0 0.9001460075378418
MemoryTrain:  epoch  2, batch     1 | loss: 0.9001460Losses:  0.6569908261299133 -0.0 0.6569908261299133
MemoryTrain:  epoch  2, batch     2 | loss: 0.6569908Losses:  0.7922614812850952 -0.0 0.7922614812850952
MemoryTrain:  epoch  3, batch     0 | loss: 0.7922615Losses:  0.7625526785850525 -0.0 0.7625526785850525
MemoryTrain:  epoch  3, batch     1 | loss: 0.7625527Losses:  0.6129600405693054 -0.0 0.6129600405693054
MemoryTrain:  epoch  3, batch     2 | loss: 0.6129600Losses:  0.8402726054191589 -0.0 0.8402726054191589
MemoryTrain:  epoch  4, batch     0 | loss: 0.8402726Losses:  0.9859122037887573 -0.0 0.9859122037887573
MemoryTrain:  epoch  4, batch     1 | loss: 0.9859122Losses:  0.5082604289054871 -0.0 0.5082604289054871
MemoryTrain:  epoch  4, batch     2 | loss: 0.5082604Losses:  0.9044492244720459 -0.0 0.9044492244720459
MemoryTrain:  epoch  5, batch     0 | loss: 0.9044492Losses:  0.818175733089447 -0.0 0.818175733089447
MemoryTrain:  epoch  5, batch     1 | loss: 0.8181757Losses:  0.5815957188606262 -0.0 0.5815957188606262
MemoryTrain:  epoch  5, batch     2 | loss: 0.5815957Losses:  0.8608519434928894 -0.0 0.8608519434928894
MemoryTrain:  epoch  6, batch     0 | loss: 0.8608519Losses:  0.8046517968177795 -0.0 0.8046517968177795
MemoryTrain:  epoch  6, batch     1 | loss: 0.8046518Losses:  0.45599430799484253 -0.0 0.45599430799484253
MemoryTrain:  epoch  6, batch     2 | loss: 0.4559943Losses:  0.8247424364089966 -0.0 0.8247424364089966
MemoryTrain:  epoch  7, batch     0 | loss: 0.8247424Losses:  0.7051218748092651 -0.0 0.7051218748092651
MemoryTrain:  epoch  7, batch     1 | loss: 0.7051219Losses:  0.750978946685791 -0.0 0.750978946685791
MemoryTrain:  epoch  7, batch     2 | loss: 0.7509789Losses:  0.9414645433425903 -0.0 0.9414645433425903
MemoryTrain:  epoch  8, batch     0 | loss: 0.9414645Losses:  0.8212029337882996 -0.0 0.8212029337882996
MemoryTrain:  epoch  8, batch     1 | loss: 0.8212029Losses:  0.7125875353813171 -0.0 0.7125875353813171
MemoryTrain:  epoch  8, batch     2 | loss: 0.7125875Losses:  0.8754274249076843 -0.0 0.8754274249076843
MemoryTrain:  epoch  9, batch     0 | loss: 0.8754274Losses:  0.9005028009414673 -0.0 0.9005028009414673
MemoryTrain:  epoch  9, batch     1 | loss: 0.9005028Losses:  0.46975380182266235 -0.0 0.46975380182266235
MemoryTrain:  epoch  9, batch     2 | loss: 0.4697538
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 56.25%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 55.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 57.29%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 58.04%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 52.34%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 3.12%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 2.08%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 3.12%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 2.50%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 4.17%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 9.82%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 17.97%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 24.31%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 26.88%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 28.98%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 30.73%   [EVAL] batch:   12 | acc: 25.00%,  total acc: 30.29%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 30.36%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 32.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 33.98%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 36.03%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 37.50%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 38.82%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 40.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 43.45%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 46.02%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 48.37%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 50.26%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 52.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 54.09%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 55.56%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 57.14%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 58.41%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 58.96%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 59.88%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 60.74%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 60.98%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 60.85%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 61.07%   [EVAL] batch:   35 | acc: 31.25%,  total acc: 60.24%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 59.46%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 59.87%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 59.78%   [EVAL] batch:   39 | acc: 31.25%,  total acc: 59.06%   [EVAL] batch:   40 | acc: 0.00%,  total acc: 57.62%   [EVAL] batch:   41 | acc: 0.00%,  total acc: 56.25%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 54.94%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 53.69%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 52.50%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 51.49%   [EVAL] batch:   46 | acc: 6.25%,  total acc: 50.53%   [EVAL] batch:   47 | acc: 0.00%,  total acc: 49.48%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 48.72%   [EVAL] batch:   49 | acc: 25.00%,  total acc: 48.25%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 48.53%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 48.92%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 49.17%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 49.54%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 49.66%   [EVAL] batch:   55 | acc: 12.50%,  total acc: 49.00%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 48.90%   [EVAL] batch:   57 | acc: 12.50%,  total acc: 48.28%   [EVAL] batch:   58 | acc: 25.00%,  total acc: 47.88%   [EVAL] batch:   59 | acc: 12.50%,  total acc: 47.29%   [EVAL] batch:   60 | acc: 0.00%,  total acc: 46.52%   [EVAL] batch:   61 | acc: 6.25%,  total acc: 45.87%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 45.24%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 44.82%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 44.33%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 43.75%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 43.47%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 42.83%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 42.66%   [EVAL] batch:   69 | acc: 37.50%,  total acc: 42.59%   [EVAL] batch:   70 | acc: 12.50%,  total acc: 42.17%   [EVAL] batch:   71 | acc: 43.75%,  total acc: 42.19%   [EVAL] batch:   72 | acc: 62.50%,  total acc: 42.47%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 42.82%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 43.42%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 44.00%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 44.32%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 44.39%   [EVAL] batch:   78 | acc: 12.50%,  total acc: 43.99%   [EVAL] batch:   79 | acc: 12.50%,  total acc: 43.59%   [EVAL] batch:   80 | acc: 12.50%,  total acc: 43.21%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 43.45%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 43.67%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 43.60%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 43.68%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 43.90%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 44.11%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 44.67%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 45.01%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 44.51%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 44.02%   [EVAL] batch:   91 | acc: 25.00%,  total acc: 43.82%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 44.42%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 45.01%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 45.59%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 46.16%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 46.71%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 47.26%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 47.54%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 47.75%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 48.14%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 48.65%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 49.15%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 49.64%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 50.06%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 50.53%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 50.70%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 50.35%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 50.00%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 49.94%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 49.66%   [EVAL] batch:  111 | acc: 56.25%,  total acc: 49.72%   [EVAL] batch:  112 | acc: 37.50%,  total acc: 49.61%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 49.67%   [EVAL] batch:  114 | acc: 56.25%,  total acc: 49.73%   [EVAL] batch:  115 | acc: 31.25%,  total acc: 49.57%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 49.79%   [EVAL] batch:  117 | acc: 37.50%,  total acc: 49.68%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 49.79%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 50.16%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 50.46%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 50.72%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 50.91%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 51.01%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 51.05%   [EVAL] batch:  125 | acc: 18.75%,  total acc: 50.79%   [EVAL] batch:  126 | acc: 75.00%,  total acc: 50.98%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 50.93%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 51.02%   [EVAL] batch:  129 | acc: 37.50%,  total acc: 50.91%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 51.00%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 51.23%   [EVAL] batch:  132 | acc: 37.50%,  total acc: 51.13%   
cur_acc:  ['0.8674', '0.6731', '0.7708', '0.2812', '0.7188', '0.7411', '0.5580', '0.5234']
his_acc:  ['0.8674', '0.6458', '0.6302', '0.5216', '0.5505', '0.5541', '0.4950', '0.5113']
--------Round  5
seed:  600
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 2 0 1 6 3 4 5]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 1 1 0]
Losses:  10.785268783569336 10.134879112243652 0.650389552116394
CurrentTrain: epoch  0, batch     0 | loss: 10.7852688Losses:  14.288479804992676 13.665024757385254 0.6234546899795532
CurrentTrain: epoch  0, batch     1 | loss: 14.2884798Losses:  9.190496444702148 8.53818130493164 0.6523154973983765
CurrentTrain: epoch  0, batch     2 | loss: 9.1904964Losses:  12.667445182800293 12.029891967773438 0.6375532746315002
CurrentTrain: epoch  0, batch     3 | loss: 12.6674452Losses:  12.263717651367188 11.633077621459961 0.6306400895118713
CurrentTrain: epoch  0, batch     4 | loss: 12.2637177Losses:  8.58492660522461 7.956124782562256 0.6288019418716431
CurrentTrain: epoch  0, batch     5 | loss: 8.5849266Losses:  11.611119270324707 11.002336502075195 0.6087826490402222
CurrentTrain: epoch  0, batch     6 | loss: 11.6111193Losses:  10.577921867370605 9.97850513458252 0.5994168519973755
CurrentTrain: epoch  0, batch     7 | loss: 10.5779219Losses:  17.541032791137695 16.955062866210938 0.5859690308570862
CurrentTrain: epoch  0, batch     8 | loss: 17.5410328Losses:  8.510539054870605 7.9359331130981445 0.5746062397956848
CurrentTrain: epoch  0, batch     9 | loss: 8.5105391Losses:  9.052230834960938 8.43642520904541 0.6158052682876587
CurrentTrain: epoch  0, batch    10 | loss: 9.0522308Losses:  12.788538932800293 12.174768447875977 0.6137704253196716
CurrentTrain: epoch  0, batch    11 | loss: 12.7885389Losses:  10.04140853881836 9.465950012207031 0.5754584670066833
CurrentTrain: epoch  0, batch    12 | loss: 10.0414085Losses:  8.357552528381348 7.779045581817627 0.5785070657730103
CurrentTrain: epoch  0, batch    13 | loss: 8.3575525Losses:  15.607765197753906 15.07746696472168 0.5302983522415161
CurrentTrain: epoch  0, batch    14 | loss: 15.6077652Losses:  9.647150039672852 9.098569869995117 0.5485806465148926
CurrentTrain: epoch  0, batch    15 | loss: 9.6471500Losses:  8.816802978515625 8.277135848999023 0.5396669507026672
CurrentTrain: epoch  0, batch    16 | loss: 8.8168030Losses:  10.59194564819336 10.092012405395508 0.4999331533908844
CurrentTrain: epoch  0, batch    17 | loss: 10.5919456Losses:  10.491935729980469 9.973275184631348 0.5186601281166077
CurrentTrain: epoch  0, batch    18 | loss: 10.4919357Losses:  9.273873329162598 8.76788330078125 0.5059903860092163
CurrentTrain: epoch  0, batch    19 | loss: 9.2738733Losses:  10.612421989440918 10.126378059387207 0.4860435724258423
CurrentTrain: epoch  0, batch    20 | loss: 10.6124220Losses:  9.08333683013916 8.620601654052734 0.46273472905158997
CurrentTrain: epoch  0, batch    21 | loss: 9.0833368Losses:  7.450130462646484 7.007667541503906 0.4424631595611572
CurrentTrain: epoch  0, batch    22 | loss: 7.4501305Losses:  7.546306610107422 7.095764636993408 0.45054203271865845
CurrentTrain: epoch  0, batch    23 | loss: 7.5463066Losses:  7.222298622131348 6.812709331512451 0.40958914160728455
CurrentTrain: epoch  0, batch    24 | loss: 7.2222986Losses:  7.958165645599365 7.521498680114746 0.4366667866706848
CurrentTrain: epoch  0, batch    25 | loss: 7.9581656Losses:  11.192337989807129 10.839801788330078 0.3525363802909851
CurrentTrain: epoch  0, batch    26 | loss: 11.1923380Losses:  10.489753723144531 10.097917556762695 0.39183661341667175
CurrentTrain: epoch  0, batch    27 | loss: 10.4897537Losses:  8.623922348022461 8.244654655456543 0.3792673647403717
CurrentTrain: epoch  0, batch    28 | loss: 8.6239223Losses:  7.852923393249512 7.550086975097656 0.30283650755882263
CurrentTrain: epoch  0, batch    29 | loss: 7.8529234Losses:  10.522791862487793 10.14530086517334 0.37749066948890686
CurrentTrain: epoch  0, batch    30 | loss: 10.5227919Losses:  8.624655723571777 8.338739395141602 0.28591662645339966
CurrentTrain: epoch  0, batch    31 | loss: 8.6246557Losses:  7.671838760375977 7.3381667137146 0.3336719870567322
CurrentTrain: epoch  0, batch    32 | loss: 7.6718388Losses:  9.31893539428711 9.046167373657227 0.27276843786239624
CurrentTrain: epoch  0, batch    33 | loss: 9.3189354Losses:  8.067663192749023 7.837878227233887 0.22978505492210388
CurrentTrain: epoch  0, batch    34 | loss: 8.0676632Losses:  7.684335708618164 7.448483467102051 0.23585212230682373
CurrentTrain: epoch  0, batch    35 | loss: 7.6843357Losses:  8.353482246398926 8.134480476379395 0.21900171041488647
CurrentTrain: epoch  0, batch    36 | loss: 8.3534822Losses:  2.4519622325897217 2.2124223709106445 0.23953986167907715
CurrentTrain: epoch  0, batch    37 | loss: 2.4519622Losses:  7.966601848602295 7.754211902618408 0.21238990128040314
CurrentTrain: epoch  1, batch     0 | loss: 7.9666018Losses:  13.123913764953613 12.923248291015625 0.20066551864147186
CurrentTrain: epoch  1, batch     1 | loss: 13.1239138Losses:  9.10932731628418 8.94229793548584 0.16702917218208313
CurrentTrain: epoch  1, batch     2 | loss: 9.1093273Losses:  6.746330738067627 6.54047155380249 0.20585918426513672
CurrentTrain: epoch  1, batch     3 | loss: 6.7463307Losses:  7.968621730804443 7.814486503601074 0.15413512289524078
CurrentTrain: epoch  1, batch     4 | loss: 7.9686217Losses:  7.583409786224365 7.418516159057617 0.1648937463760376
CurrentTrain: epoch  1, batch     5 | loss: 7.5834098Losses:  9.118851661682129 8.96889877319336 0.14995285868644714
CurrentTrain: epoch  1, batch     6 | loss: 9.1188517Losses:  7.505756378173828 7.374021530151367 0.13173478841781616
CurrentTrain: epoch  1, batch     7 | loss: 7.5057564Losses:  7.790922164916992 7.64713716506958 0.14378485083580017
CurrentTrain: epoch  1, batch     8 | loss: 7.7909222Losses:  11.228042602539062 11.090473175048828 0.13756921887397766
CurrentTrain: epoch  1, batch     9 | loss: 11.2280426Losses:  7.177173137664795 7.064970016479492 0.11220318078994751
CurrentTrain: epoch  1, batch    10 | loss: 7.1771731Losses:  7.972476959228516 7.866766452789307 0.10571052879095078
CurrentTrain: epoch  1, batch    11 | loss: 7.9724770Losses:  8.032642364501953 7.9216532707214355 0.11098913103342056
CurrentTrain: epoch  1, batch    12 | loss: 8.0326424Losses:  7.403188705444336 7.312261581420898 0.09092714637517929
CurrentTrain: epoch  1, batch    13 | loss: 7.4031887Losses:  6.044252395629883 5.951780319213867 0.0924721360206604
CurrentTrain: epoch  1, batch    14 | loss: 6.0442524Losses:  6.094376564025879 5.996564865112305 0.09781165421009064
CurrentTrain: epoch  1, batch    15 | loss: 6.0943766Losses:  6.7780022621154785 6.691286563873291 0.08671560883522034
CurrentTrain: epoch  1, batch    16 | loss: 6.7780023Losses:  16.05809783935547 15.962133407592773 0.09596499055624008
CurrentTrain: epoch  1, batch    17 | loss: 16.0580978Losses:  6.627941131591797 6.521513938903809 0.10642729699611664
CurrentTrain: epoch  1, batch    18 | loss: 6.6279411Losses:  10.975357055664062 10.885043144226074 0.09031365811824799
CurrentTrain: epoch  1, batch    19 | loss: 10.9753571Losses:  6.322147846221924 6.241325378417969 0.08082261681556702
CurrentTrain: epoch  1, batch    20 | loss: 6.3221478Losses:  9.98621654510498 9.88895034790039 0.09726597368717194
CurrentTrain: epoch  1, batch    21 | loss: 9.9862165Losses:  7.91851806640625 7.822732925415039 0.0957852452993393
CurrentTrain: epoch  1, batch    22 | loss: 7.9185181Losses:  5.260924339294434 5.192647933959961 0.06827639043331146
CurrentTrain: epoch  1, batch    23 | loss: 5.2609243Losses:  8.124215126037598 8.053601264953613 0.07061411440372467
CurrentTrain: epoch  1, batch    24 | loss: 8.1242151Losses:  7.074313640594482 6.999006271362305 0.07530750334262848
CurrentTrain: epoch  1, batch    25 | loss: 7.0743136Losses:  12.465779304504395 12.390420913696289 0.07535848021507263
CurrentTrain: epoch  1, batch    26 | loss: 12.4657793Losses:  9.572026252746582 9.505462646484375 0.06656404584646225
CurrentTrain: epoch  1, batch    27 | loss: 9.5720263Losses:  7.399116516113281 7.341701030731201 0.057415470480918884
CurrentTrain: epoch  1, batch    28 | loss: 7.3991165Losses:  8.608983993530273 8.54743766784668 0.06154634431004524
CurrentTrain: epoch  1, batch    29 | loss: 8.6089840Losses:  7.2017645835876465 7.131032943725586 0.070731520652771
CurrentTrain: epoch  1, batch    30 | loss: 7.2017646Losses:  5.298324108123779 5.241199493408203 0.05712468922138214
CurrentTrain: epoch  1, batch    31 | loss: 5.2983241Losses:  6.965390682220459 6.907378196716309 0.0580124631524086
CurrentTrain: epoch  1, batch    32 | loss: 6.9653907Losses:  9.176077842712402 9.125808715820312 0.05026875436306
CurrentTrain: epoch  1, batch    33 | loss: 9.1760778Losses:  6.54626989364624 6.494129657745361 0.052140071988105774
CurrentTrain: epoch  1, batch    34 | loss: 6.5462699Losses:  7.516037940979004 7.451133728027344 0.06490418314933777
CurrentTrain: epoch  1, batch    35 | loss: 7.5160379Losses:  5.328276634216309 5.279281139373779 0.04899559170007706
CurrentTrain: epoch  1, batch    36 | loss: 5.3282766Losses:  3.7845590114593506 3.728875160217285 0.055683791637420654
CurrentTrain: epoch  1, batch    37 | loss: 3.7845590Losses:  7.464786529541016 7.417268753051758 0.047517914324998856
CurrentTrain: epoch  2, batch     0 | loss: 7.4647865Losses:  6.349567413330078 6.302748680114746 0.04681895300745964
CurrentTrain: epoch  2, batch     1 | loss: 6.3495674Losses:  6.692576885223389 6.638190269470215 0.0543866902589798
CurrentTrain: epoch  2, batch     2 | loss: 6.6925769Losses:  12.016414642333984 11.973421096801758 0.04299328848719597
CurrentTrain: epoch  2, batch     3 | loss: 12.0164146Losses:  5.730161666870117 5.685348033905029 0.04481368884444237
CurrentTrain: epoch  2, batch     4 | loss: 5.7301617Losses:  7.508485317230225 7.4577741622924805 0.0507110059261322
CurrentTrain: epoch  2, batch     5 | loss: 7.5084853Losses:  6.216205596923828 6.17282772064209 0.04337792843580246
CurrentTrain: epoch  2, batch     6 | loss: 6.2162056Losses:  5.41922664642334 5.373432159423828 0.04579439014196396
CurrentTrain: epoch  2, batch     7 | loss: 5.4192266Losses:  6.2392425537109375 6.190846920013428 0.04839585721492767
CurrentTrain: epoch  2, batch     8 | loss: 6.2392426Losses:  4.9956583976745605 4.951440811157227 0.04421748220920563
CurrentTrain: epoch  2, batch     9 | loss: 4.9956584Losses:  5.556153297424316 5.512866973876953 0.043286263942718506
CurrentTrain: epoch  2, batch    10 | loss: 5.5561533Losses:  7.940186977386475 7.891902923583984 0.04828420281410217
CurrentTrain: epoch  2, batch    11 | loss: 7.9401870Losses:  4.887039661407471 4.840540885925293 0.04649856314063072
CurrentTrain: epoch  2, batch    12 | loss: 4.8870397Losses:  5.168137073516846 5.128575325012207 0.039561912417411804
CurrentTrain: epoch  2, batch    13 | loss: 5.1681371Losses:  10.125357627868652 10.054444313049316 0.07091332972049713
CurrentTrain: epoch  2, batch    14 | loss: 10.1253576Losses:  7.227145195007324 7.181069850921631 0.046075448393821716
CurrentTrain: epoch  2, batch    15 | loss: 7.2271452Losses:  5.106482028961182 5.063712120056152 0.042769674211740494
CurrentTrain: epoch  2, batch    16 | loss: 5.1064820Losses:  6.615433216094971 6.575607776641846 0.03982548043131828
CurrentTrain: epoch  2, batch    17 | loss: 6.6154332Losses:  5.864818572998047 5.824617385864258 0.04020138084888458
CurrentTrain: epoch  2, batch    18 | loss: 5.8648186Losses:  11.646066665649414 11.601405143737793 0.04466139152646065
CurrentTrain: epoch  2, batch    19 | loss: 11.6460667Losses:  7.148460865020752 7.1071391105651855 0.04132182523608208
CurrentTrain: epoch  2, batch    20 | loss: 7.1484609Losses:  8.63752555847168 8.588312149047852 0.04921305179595947
CurrentTrain: epoch  2, batch    21 | loss: 8.6375256Losses:  6.684576511383057 6.6316328048706055 0.05294392630457878
CurrentTrain: epoch  2, batch    22 | loss: 6.6845765Losses:  4.784094333648682 4.745739459991455 0.03835465759038925
CurrentTrain: epoch  2, batch    23 | loss: 4.7840943Losses:  5.280876636505127 5.237330436706543 0.04354628548026085
CurrentTrain: epoch  2, batch    24 | loss: 5.2808766Losses:  6.793496608734131 6.753159046173096 0.040337640792131424
CurrentTrain: epoch  2, batch    25 | loss: 6.7934966Losses:  6.4014410972595215 6.364290237426758 0.037150826305150986
CurrentTrain: epoch  2, batch    26 | loss: 6.4014411Losses:  7.468345642089844 7.43345832824707 0.03488750010728836
CurrentTrain: epoch  2, batch    27 | loss: 7.4683456Losses:  5.467926025390625 5.436159133911133 0.031766798347234726
CurrentTrain: epoch  2, batch    28 | loss: 5.4679260Losses:  6.703872203826904 6.668698310852051 0.03517405688762665
CurrentTrain: epoch  2, batch    29 | loss: 6.7038722Losses:  8.846648216247559 8.801896095275879 0.04475196450948715
CurrentTrain: epoch  2, batch    30 | loss: 8.8466482Losses:  5.808373928070068 5.770575046539307 0.03779906779527664
CurrentTrain: epoch  2, batch    31 | loss: 5.8083739Losses:  7.515902519226074 7.4741387367248535 0.04176381975412369
CurrentTrain: epoch  2, batch    32 | loss: 7.5159025Losses:  6.508803844451904 6.469579696655273 0.03922399878501892
CurrentTrain: epoch  2, batch    33 | loss: 6.5088038Losses:  5.019691467285156 4.9887542724609375 0.030937064439058304
CurrentTrain: epoch  2, batch    34 | loss: 5.0196915Losses:  6.9311065673828125 6.885697364807129 0.0454094260931015
CurrentTrain: epoch  2, batch    35 | loss: 6.9311066Losses:  5.945313453674316 5.905634880065918 0.03967837989330292
CurrentTrain: epoch  2, batch    36 | loss: 5.9453135Losses:  1.322864294052124 1.282626986503601 0.04023733735084534
CurrentTrain: epoch  2, batch    37 | loss: 1.3228643Losses:  5.641699314117432 5.610396385192871 0.03130291774868965
CurrentTrain: epoch  3, batch     0 | loss: 5.6416993Losses:  7.160829544067383 7.1255974769592285 0.035232290625572205
CurrentTrain: epoch  3, batch     1 | loss: 7.1608295Losses:  7.127469539642334 7.080345153808594 0.04712434485554695
CurrentTrain: epoch  3, batch     2 | loss: 7.1274695Losses:  7.665975570678711 7.6238250732421875 0.04215053468942642
CurrentTrain: epoch  3, batch     3 | loss: 7.6659756Losses:  5.976959705352783 5.948312759399414 0.028646929189562798
CurrentTrain: epoch  3, batch     4 | loss: 5.9769597Losses:  4.960310459136963 4.9302754402160645 0.03003503568470478
CurrentTrain: epoch  3, batch     5 | loss: 4.9603105Losses:  6.251377105712891 6.222072601318359 0.029304642230272293
CurrentTrain: epoch  3, batch     6 | loss: 6.2513771Losses:  6.869113922119141 6.8319315910339355 0.03718212991952896
CurrentTrain: epoch  3, batch     7 | loss: 6.8691139Losses:  7.648583889007568 7.61486291885376 0.03372080624103546
CurrentTrain: epoch  3, batch     8 | loss: 7.6485839Losses:  4.984272003173828 4.949835777282715 0.0344361774623394
CurrentTrain: epoch  3, batch     9 | loss: 4.9842720Losses:  5.726171970367432 5.694231986999512 0.03194000571966171
CurrentTrain: epoch  3, batch    10 | loss: 5.7261720Losses:  6.702731132507324 6.663650989532471 0.039080023765563965
CurrentTrain: epoch  3, batch    11 | loss: 6.7027311Losses:  8.172975540161133 8.144407272338867 0.028567861765623093
CurrentTrain: epoch  3, batch    12 | loss: 8.1729755Losses:  8.40304183959961 8.367072105407715 0.03596935048699379
CurrentTrain: epoch  3, batch    13 | loss: 8.4030418Losses:  6.560824394226074 6.524410247802734 0.036413948982954025
CurrentTrain: epoch  3, batch    14 | loss: 6.5608244Losses:  5.475697994232178 5.441105842590332 0.03459228575229645
CurrentTrain: epoch  3, batch    15 | loss: 5.4756980Losses:  5.229426383972168 5.199718952178955 0.02970760129392147
CurrentTrain: epoch  3, batch    16 | loss: 5.2294264Losses:  8.038330078125 7.998369216918945 0.03996133431792259
CurrentTrain: epoch  3, batch    17 | loss: 8.0383301Losses:  10.791877746582031 10.744087219238281 0.047790929675102234
CurrentTrain: epoch  3, batch    18 | loss: 10.7918777Losses:  7.1974897384643555 7.161163330078125 0.03632663935422897
CurrentTrain: epoch  3, batch    19 | loss: 7.1974897Losses:  9.344799995422363 9.293149948120117 0.05165034160017967
CurrentTrain: epoch  3, batch    20 | loss: 9.3448000Losses:  4.3234333992004395 4.294926643371582 0.028506632894277573
CurrentTrain: epoch  3, batch    21 | loss: 4.3234334Losses:  5.8598480224609375 5.830533981323242 0.029313843697309494
CurrentTrain: epoch  3, batch    22 | loss: 5.8598480Losses:  6.444211006164551 6.411736488342285 0.03247427940368652
CurrentTrain: epoch  3, batch    23 | loss: 6.4442110Losses:  6.465849876403809 6.427987098693848 0.03786265850067139
CurrentTrain: epoch  3, batch    24 | loss: 6.4658499Losses:  5.1642255783081055 5.135149002075195 0.02907661348581314
CurrentTrain: epoch  3, batch    25 | loss: 5.1642256Losses:  5.554666519165039 5.527624607086182 0.02704191394150257
CurrentTrain: epoch  3, batch    26 | loss: 5.5546665Losses:  4.62004280090332 4.593913555145264 0.026129279285669327
CurrentTrain: epoch  3, batch    27 | loss: 4.6200428Losses:  5.8401875495910645 5.808896064758301 0.03129170835018158
CurrentTrain: epoch  3, batch    28 | loss: 5.8401875Losses:  6.8584208488464355 6.832016468048096 0.02640453912317753
CurrentTrain: epoch  3, batch    29 | loss: 6.8584208Losses:  5.22168493270874 5.192104339599609 0.029580358415842056
CurrentTrain: epoch  3, batch    30 | loss: 5.2216849Losses:  5.210547924041748 5.177993297576904 0.03255453705787659
CurrentTrain: epoch  3, batch    31 | loss: 5.2105479Losses:  5.205451965332031 5.175875663757324 0.0295761339366436
CurrentTrain: epoch  3, batch    32 | loss: 5.2054520Losses:  4.2656636238098145 4.240535736083984 0.02512792870402336
CurrentTrain: epoch  3, batch    33 | loss: 4.2656636Losses:  6.753250598907471 6.72685432434082 0.02639647014439106
CurrentTrain: epoch  3, batch    34 | loss: 6.7532506Losses:  5.175827980041504 5.147219657897949 0.028608178719878197
CurrentTrain: epoch  3, batch    35 | loss: 5.1758280Losses:  7.180654048919678 7.142618656158447 0.0380355603992939
CurrentTrain: epoch  3, batch    36 | loss: 7.1806540Losses:  1.5092530250549316 1.4740803241729736 0.03517267480492592
CurrentTrain: epoch  3, batch    37 | loss: 1.5092530Losses:  6.358177661895752 6.323133945465088 0.035043537616729736
CurrentTrain: epoch  4, batch     0 | loss: 6.3581777Losses:  5.458471298217773 5.432055473327637 0.026415742933750153
CurrentTrain: epoch  4, batch     1 | loss: 5.4584713Losses:  5.947970867156982 5.921929359436035 0.026041477918624878
CurrentTrain: epoch  4, batch     2 | loss: 5.9479709Losses:  9.136324882507324 9.109024047851562 0.027300741523504257
CurrentTrain: epoch  4, batch     3 | loss: 9.1363249Losses:  6.084842681884766 6.046576499938965 0.0382661446928978
CurrentTrain: epoch  4, batch     4 | loss: 6.0848427Losses:  4.922475814819336 4.897655487060547 0.024820396676659584
CurrentTrain: epoch  4, batch     5 | loss: 4.9224758Losses:  5.263103008270264 5.238367080688477 0.024736085906624794
CurrentTrain: epoch  4, batch     6 | loss: 5.2631030Losses:  12.615653038024902 12.56307601928711 0.05257657915353775
CurrentTrain: epoch  4, batch     7 | loss: 12.6156530Losses:  5.749027729034424 5.721645832061768 0.02738170512020588
CurrentTrain: epoch  4, batch     8 | loss: 5.7490277Losses:  6.287075519561768 6.251844882965088 0.03523079305887222
CurrentTrain: epoch  4, batch     9 | loss: 6.2870755Losses:  9.529115676879883 9.496284484863281 0.032830860465765
CurrentTrain: epoch  4, batch    10 | loss: 9.5291157Losses:  7.341405391693115 7.307519912719727 0.033885322511196136
CurrentTrain: epoch  4, batch    11 | loss: 7.3414054Losses:  6.110006332397461 6.074043273925781 0.03596308082342148
CurrentTrain: epoch  4, batch    12 | loss: 6.1100063Losses:  8.542497634887695 8.509779930114746 0.03271745517849922
CurrentTrain: epoch  4, batch    13 | loss: 8.5424976Losses:  7.447436332702637 7.419922828674316 0.02751334384083748
CurrentTrain: epoch  4, batch    14 | loss: 7.4474363Losses:  5.430727958679199 5.404648780822754 0.02607910707592964
CurrentTrain: epoch  4, batch    15 | loss: 5.4307280Losses:  6.943129062652588 6.918108940124512 0.025020157918334007
CurrentTrain: epoch  4, batch    16 | loss: 6.9431291Losses:  8.133443832397461 8.09726619720459 0.03617747500538826
CurrentTrain: epoch  4, batch    17 | loss: 8.1334438Losses:  7.533108234405518 7.50325345993042 0.029854901134967804
CurrentTrain: epoch  4, batch    18 | loss: 7.5331082Losses:  5.820391654968262 5.790619850158691 0.029771938920021057
CurrentTrain: epoch  4, batch    19 | loss: 5.8203917Losses:  6.9088006019592285 6.876714706420898 0.032085709273815155
CurrentTrain: epoch  4, batch    20 | loss: 6.9088006Losses:  6.769891738891602 6.742345333099365 0.02754628099501133
CurrentTrain: epoch  4, batch    21 | loss: 6.7698917Losses:  9.224714279174805 9.188949584960938 0.03576446324586868
CurrentTrain: epoch  4, batch    22 | loss: 9.2247143Losses:  4.97435188293457 4.945638656616211 0.028713129460811615
CurrentTrain: epoch  4, batch    23 | loss: 4.9743519Losses:  3.9274067878723145 3.90289044380188 0.024516260251402855
CurrentTrain: epoch  4, batch    24 | loss: 3.9274068Losses:  8.054974555969238 8.010763168334961 0.04421178251504898
CurrentTrain: epoch  4, batch    25 | loss: 8.0549746Losses:  5.9640421867370605 5.933934211730957 0.030108144506812096
CurrentTrain: epoch  4, batch    26 | loss: 5.9640422Losses:  5.025546073913574 4.995584011077881 0.029962267726659775
CurrentTrain: epoch  4, batch    27 | loss: 5.0255461Losses:  5.069974422454834 5.04585075378418 0.024123478680849075
CurrentTrain: epoch  4, batch    28 | loss: 5.0699744Losses:  6.328647136688232 6.295295715332031 0.033351339399814606
CurrentTrain: epoch  4, batch    29 | loss: 6.3286471Losses:  4.7266845703125 4.699988842010498 0.02669556625187397
CurrentTrain: epoch  4, batch    30 | loss: 4.7266846Losses:  5.179974555969238 5.155137538909912 0.024837026372551918
CurrentTrain: epoch  4, batch    31 | loss: 5.1799746Losses:  5.371935844421387 5.348987579345703 0.022948287427425385
CurrentTrain: epoch  4, batch    32 | loss: 5.3719358Losses:  12.227590560913086 12.177282333374023 0.05030786246061325
CurrentTrain: epoch  4, batch    33 | loss: 12.2275906Losses:  8.266701698303223 8.239726066589355 0.026975717395544052
CurrentTrain: epoch  4, batch    34 | loss: 8.2667017Losses:  6.9307966232299805 6.898411750793457 0.03238501399755478
CurrentTrain: epoch  4, batch    35 | loss: 6.9307966Losses:  7.2085394859313965 7.175612926483154 0.032926544547080994
CurrentTrain: epoch  4, batch    36 | loss: 7.2085395Losses:  2.973595380783081 2.9292850494384766 0.044310376048088074
CurrentTrain: epoch  4, batch    37 | loss: 2.9735954Losses:  9.889037132263184 9.855432510375977 0.033604852855205536
CurrentTrain: epoch  5, batch     0 | loss: 9.8890371Losses:  6.597428321838379 6.559104919433594 0.038323283195495605
CurrentTrain: epoch  5, batch     1 | loss: 6.5974283Losses:  5.789220333099365 5.760162830352783 0.02905750647187233
CurrentTrain: epoch  5, batch     2 | loss: 5.7892203Losses:  6.2440385818481445 6.216734886169434 0.02730381488800049
CurrentTrain: epoch  5, batch     3 | loss: 6.2440386Losses:  5.034447193145752 5.004352569580078 0.030094726011157036
CurrentTrain: epoch  5, batch     4 | loss: 5.0344472Losses:  4.827828884124756 4.8027663230896 0.025062650442123413
CurrentTrain: epoch  5, batch     5 | loss: 4.8278289Losses:  7.083069801330566 7.050699234008789 0.03237035498023033
CurrentTrain: epoch  5, batch     6 | loss: 7.0830698Losses:  7.748231410980225 7.717010974884033 0.031220536679029465
CurrentTrain: epoch  5, batch     7 | loss: 7.7482314Losses:  4.189448356628418 4.165783882141113 0.023664535954594612
CurrentTrain: epoch  5, batch     8 | loss: 4.1894484Losses:  3.9760453701019287 3.952423095703125 0.02362227253615856
CurrentTrain: epoch  5, batch     9 | loss: 3.9760454Losses:  6.12864875793457 6.098066329956055 0.030582640320062637
CurrentTrain: epoch  5, batch    10 | loss: 6.1286488Losses:  7.975604057312012 7.940442085266113 0.03516214340925217
CurrentTrain: epoch  5, batch    11 | loss: 7.9756041Losses:  5.548846244812012 5.523650169372559 0.025196164846420288
CurrentTrain: epoch  5, batch    12 | loss: 5.5488462Losses:  9.755186080932617 9.715215682983398 0.039970725774765015
CurrentTrain: epoch  5, batch    13 | loss: 9.7551861Losses:  4.313923358917236 4.290358066558838 0.023565106093883514
CurrentTrain: epoch  5, batch    14 | loss: 4.3139234Losses:  6.617805004119873 6.586545467376709 0.03125958889722824
CurrentTrain: epoch  5, batch    15 | loss: 6.6178050Losses:  4.244056701660156 4.217137336730957 0.02691926434636116
CurrentTrain: epoch  5, batch    16 | loss: 4.2440567Losses:  3.933297872543335 3.9077062606811523 0.025591662153601646
CurrentTrain: epoch  5, batch    17 | loss: 3.9332979Losses:  4.821063041687012 4.791729927062988 0.02933306060731411
CurrentTrain: epoch  5, batch    18 | loss: 4.8210630Losses:  4.064627170562744 4.040001392364502 0.024625830352306366
CurrentTrain: epoch  5, batch    19 | loss: 4.0646272Losses:  5.912683010101318 5.883395195007324 0.029287854209542274
CurrentTrain: epoch  5, batch    20 | loss: 5.9126830Losses:  5.917358875274658 5.894965171813965 0.022393804043531418
CurrentTrain: epoch  5, batch    21 | loss: 5.9173589Losses:  5.943393230438232 5.907550811767578 0.03584256023168564
CurrentTrain: epoch  5, batch    22 | loss: 5.9433932Losses:  4.032402515411377 4.008761405944824 0.023641018196940422
CurrentTrain: epoch  5, batch    23 | loss: 4.0324025Losses:  5.47212553024292 5.440308094024658 0.031817324459552765
CurrentTrain: epoch  5, batch    24 | loss: 5.4721255Losses:  6.567968845367432 6.536651611328125 0.03131701052188873
CurrentTrain: epoch  5, batch    25 | loss: 6.5679688Losses:  4.589212417602539 4.559595584869385 0.02961675636470318
CurrentTrain: epoch  5, batch    26 | loss: 4.5892124Losses:  4.215038299560547 4.18958854675293 0.0254496019333601
CurrentTrain: epoch  5, batch    27 | loss: 4.2150383Losses:  7.740700721740723 7.6949872970581055 0.04571319743990898
CurrentTrain: epoch  5, batch    28 | loss: 7.7407007Losses:  10.044719696044922 10.009804725646973 0.03491472825407982
CurrentTrain: epoch  5, batch    29 | loss: 10.0447197Losses:  5.513189315795898 5.483362197875977 0.029826901853084564
CurrentTrain: epoch  5, batch    30 | loss: 5.5131893Losses:  4.396492958068848 4.370600700378418 0.0258921030908823
CurrentTrain: epoch  5, batch    31 | loss: 4.3964930Losses:  7.534260272979736 7.494653701782227 0.039606425911188126
CurrentTrain: epoch  5, batch    32 | loss: 7.5342603Losses:  5.226271152496338 5.19462776184082 0.03164345771074295
CurrentTrain: epoch  5, batch    33 | loss: 5.2262712Losses:  4.846473693847656 4.820775985717773 0.025697629898786545
CurrentTrain: epoch  5, batch    34 | loss: 4.8464737Losses:  7.744400978088379 7.707221984863281 0.037178851664066315
CurrentTrain: epoch  5, batch    35 | loss: 7.7444010Losses:  7.395822525024414 7.368801116943359 0.027021491900086403
CurrentTrain: epoch  5, batch    36 | loss: 7.3958225Losses:  1.4485712051391602 1.4073917865753174 0.04117940738797188
CurrentTrain: epoch  5, batch    37 | loss: 1.4485712Losses:  5.560084819793701 5.533949375152588 0.02613566629588604
CurrentTrain: epoch  6, batch     0 | loss: 5.5600848Losses:  5.391093730926514 5.361731052398682 0.029362579807639122
CurrentTrain: epoch  6, batch     1 | loss: 5.3910937Losses:  9.363099098205566 9.322436332702637 0.040662411600351334
CurrentTrain: epoch  6, batch     2 | loss: 9.3630991Losses:  4.462833404541016 4.437168598175049 0.0256650373339653
CurrentTrain: epoch  6, batch     3 | loss: 4.4628334Losses:  4.9820098876953125 4.955557823181152 0.026452139019966125
CurrentTrain: epoch  6, batch     4 | loss: 4.9820099Losses:  5.5744452476501465 5.548234939575195 0.026210453361272812
CurrentTrain: epoch  6, batch     5 | loss: 5.5744452Losses:  7.1090898513793945 7.071633815765381 0.0374559685587883
CurrentTrain: epoch  6, batch     6 | loss: 7.1090899Losses:  6.267412185668945 6.245574951171875 0.021837089210748672
CurrentTrain: epoch  6, batch     7 | loss: 6.2674122Losses:  6.789389133453369 6.765478610992432 0.023910386487841606
CurrentTrain: epoch  6, batch     8 | loss: 6.7893891Losses:  6.665946006774902 6.6402764320373535 0.025669511407613754
CurrentTrain: epoch  6, batch     9 | loss: 6.6659460Losses:  5.590312480926514 5.562606334686279 0.027706190943717957
CurrentTrain: epoch  6, batch    10 | loss: 5.5903125Losses:  6.282969951629639 6.247992038726807 0.034977808594703674
CurrentTrain: epoch  6, batch    11 | loss: 6.2829700Losses:  4.8934550285339355 4.870059967041016 0.02339492365717888
CurrentTrain: epoch  6, batch    12 | loss: 4.8934550Losses:  4.385326385498047 4.361985683441162 0.02334071509540081
CurrentTrain: epoch  6, batch    13 | loss: 4.3853264Losses:  7.119143486022949 7.091320991516113 0.027822483330965042
CurrentTrain: epoch  6, batch    14 | loss: 7.1191435Losses:  4.846553802490234 4.821773529052734 0.02478039264678955
CurrentTrain: epoch  6, batch    15 | loss: 4.8465538Losses:  3.5595295429229736 3.537992000579834 0.02153758518397808
CurrentTrain: epoch  6, batch    16 | loss: 3.5595295Losses:  6.457807540893555 6.42416524887085 0.03364242985844612
CurrentTrain: epoch  6, batch    17 | loss: 6.4578075Losses:  3.5490598678588867 3.52754282951355 0.02151705138385296
CurrentTrain: epoch  6, batch    18 | loss: 3.5490599Losses:  4.446812629699707 4.425467491149902 0.021345119923353195
CurrentTrain: epoch  6, batch    19 | loss: 4.4468126Losses:  8.552535057067871 8.51791000366211 0.03462504595518112
CurrentTrain: epoch  6, batch    20 | loss: 8.5525351Losses:  4.478763580322266 4.456096649169922 0.022666888311505318
CurrentTrain: epoch  6, batch    21 | loss: 4.4787636Losses:  7.2059173583984375 7.173037528991699 0.03287965804338455
CurrentTrain: epoch  6, batch    22 | loss: 7.2059174Losses:  6.611235618591309 6.577130317687988 0.03410511091351509
CurrentTrain: epoch  6, batch    23 | loss: 6.6112356Losses:  5.13772439956665 5.109002113342285 0.02872222289443016
CurrentTrain: epoch  6, batch    24 | loss: 5.1377244Losses:  4.020516872406006 3.9972877502441406 0.023228907957673073
CurrentTrain: epoch  6, batch    25 | loss: 4.0205169Losses:  4.580936431884766 4.550899982452393 0.030036423355340958
CurrentTrain: epoch  6, batch    26 | loss: 4.5809364Losses:  6.705068111419678 6.678073406219482 0.026994921267032623
CurrentTrain: epoch  6, batch    27 | loss: 6.7050681Losses:  4.3249382972717285 4.2999396324157715 0.024998648092150688
CurrentTrain: epoch  6, batch    28 | loss: 4.3249383Losses:  6.71224308013916 6.68535041809082 0.026892689988017082
CurrentTrain: epoch  6, batch    29 | loss: 6.7122431Losses:  5.813962459564209 5.789226531982422 0.024736028164625168
CurrentTrain: epoch  6, batch    30 | loss: 5.8139625Losses:  7.613597393035889 7.579992294311523 0.03360496833920479
CurrentTrain: epoch  6, batch    31 | loss: 7.6135974Losses:  9.4277982711792 9.396380424499512 0.031417522579431534
CurrentTrain: epoch  6, batch    32 | loss: 9.4277983Losses:  7.199260711669922 7.172106742858887 0.027153875678777695
CurrentTrain: epoch  6, batch    33 | loss: 7.1992607Losses:  6.614589214324951 6.582296848297119 0.032292336225509644
CurrentTrain: epoch  6, batch    34 | loss: 6.6145892Losses:  11.609291076660156 11.556693077087402 0.05259789898991585
CurrentTrain: epoch  6, batch    35 | loss: 11.6092911Losses:  4.581839084625244 4.553328514099121 0.028510572388768196
CurrentTrain: epoch  6, batch    36 | loss: 4.5818391Losses:  3.1238315105438232 3.077928066253662 0.045903533697128296
CurrentTrain: epoch  6, batch    37 | loss: 3.1238315Losses:  4.129638195037842 4.106754302978516 0.022883858531713486
CurrentTrain: epoch  7, batch     0 | loss: 4.1296382Losses:  7.017942428588867 6.990964889526367 0.02697763219475746
CurrentTrain: epoch  7, batch     1 | loss: 7.0179424Losses:  6.340427398681641 6.299424171447754 0.04100321605801582
CurrentTrain: epoch  7, batch     2 | loss: 6.3404274Losses:  7.724294185638428 7.687345027923584 0.03694910928606987
CurrentTrain: epoch  7, batch     3 | loss: 7.7242942Losses:  11.426623344421387 11.36380386352539 0.06281907856464386
CurrentTrain: epoch  7, batch     4 | loss: 11.4266233Losses:  4.466122627258301 4.43585729598999 0.03026556223630905
CurrentTrain: epoch  7, batch     5 | loss: 4.4661226Losses:  5.517696857452393 5.482001304626465 0.03569573536515236
CurrentTrain: epoch  7, batch     6 | loss: 5.5176969Losses:  7.572254657745361 7.535079002380371 0.037175752222537994
CurrentTrain: epoch  7, batch     7 | loss: 7.5722547Losses:  5.942119121551514 5.900324821472168 0.04179411754012108
CurrentTrain: epoch  7, batch     8 | loss: 5.9421191Losses:  6.811183929443359 6.772469520568848 0.03871442377567291
CurrentTrain: epoch  7, batch     9 | loss: 6.8111839Losses:  7.326884746551514 7.295827865600586 0.031057026237249374
CurrentTrain: epoch  7, batch    10 | loss: 7.3268847Losses:  8.543593406677246 8.50450611114502 0.039086900651454926
CurrentTrain: epoch  7, batch    11 | loss: 8.5435934Losses:  7.211297512054443 7.167479991912842 0.04381764680147171
CurrentTrain: epoch  7, batch    12 | loss: 7.2112975Losses:  4.006464958190918 3.9818358421325684 0.02462908625602722
CurrentTrain: epoch  7, batch    13 | loss: 4.0064650Losses:  5.196178436279297 5.160216808319092 0.03596153110265732
CurrentTrain: epoch  7, batch    14 | loss: 5.1961784Losses:  6.054048538208008 6.024203300476074 0.02984500862658024
CurrentTrain: epoch  7, batch    15 | loss: 6.0540485Losses:  6.0927252769470215 6.067414283752441 0.025311123579740524
CurrentTrain: epoch  7, batch    16 | loss: 6.0927253Losses:  3.8289496898651123 3.807218551635742 0.021731236949563026
CurrentTrain: epoch  7, batch    17 | loss: 3.8289497Losses:  5.705835342407227 5.675777435302734 0.030058113858103752
CurrentTrain: epoch  7, batch    18 | loss: 5.7058353Losses:  7.235454082489014 7.199387550354004 0.03606651723384857
CurrentTrain: epoch  7, batch    19 | loss: 7.2354541Losses:  7.190898418426514 7.161501884460449 0.029396498575806618
CurrentTrain: epoch  7, batch    20 | loss: 7.1908984Losses:  3.6159210205078125 3.591904640197754 0.02401641570031643
CurrentTrain: epoch  7, batch    21 | loss: 3.6159210Losses:  6.263373374938965 6.230964660644531 0.03240878880023956
CurrentTrain: epoch  7, batch    22 | loss: 6.2633734Losses:  5.858590602874756 5.820425987243652 0.0381646566092968
CurrentTrain: epoch  7, batch    23 | loss: 5.8585906Losses:  4.900081634521484 4.866197109222412 0.03388462960720062
CurrentTrain: epoch  7, batch    24 | loss: 4.9000816Losses:  4.900327682495117 4.874114990234375 0.026212776079773903
CurrentTrain: epoch  7, batch    25 | loss: 4.9003277Losses:  4.628157615661621 4.596797943115234 0.03135981410741806
CurrentTrain: epoch  7, batch    26 | loss: 4.6281576Losses:  3.461298704147339 3.439439058303833 0.021859539672732353
CurrentTrain: epoch  7, batch    27 | loss: 3.4612987Losses:  6.428108215332031 6.398303031921387 0.02980538085103035
CurrentTrain: epoch  7, batch    28 | loss: 6.4281082Losses:  6.838011264801025 6.7977471351623535 0.040264129638671875
CurrentTrain: epoch  7, batch    29 | loss: 6.8380113Losses:  5.669042110443115 5.642361164093018 0.026680998504161835
CurrentTrain: epoch  7, batch    30 | loss: 5.6690421Losses:  4.408113956451416 4.383005142211914 0.025108862668275833
CurrentTrain: epoch  7, batch    31 | loss: 4.4081140Losses:  4.3786845207214355 4.354326248168945 0.02435828186571598
CurrentTrain: epoch  7, batch    32 | loss: 4.3786845Losses:  5.48116397857666 5.451756477355957 0.02940732054412365
CurrentTrain: epoch  7, batch    33 | loss: 5.4811640Losses:  6.648884296417236 6.6184163093566895 0.030467938631772995
CurrentTrain: epoch  7, batch    34 | loss: 6.6488843Losses:  6.987856864929199 6.956902980804443 0.030953938141465187
CurrentTrain: epoch  7, batch    35 | loss: 6.9878569Losses:  9.012383460998535 8.968327522277832 0.04405613988637924
CurrentTrain: epoch  7, batch    36 | loss: 9.0123835Losses:  2.1385319232940674 2.099419593811035 0.03911229223012924
CurrentTrain: epoch  7, batch    37 | loss: 2.1385319Losses:  9.555054664611816 9.485847473144531 0.06920710951089859
CurrentTrain: epoch  8, batch     0 | loss: 9.5550547Losses:  3.8677618503570557 3.846508502960205 0.02125333808362484
CurrentTrain: epoch  8, batch     1 | loss: 3.8677619Losses:  9.145371437072754 9.107776641845703 0.03759438544511795
CurrentTrain: epoch  8, batch     2 | loss: 9.1453714Losses:  3.818061590194702 3.796741008758545 0.02132049761712551
CurrentTrain: epoch  8, batch     3 | loss: 3.8180616Losses:  4.030466556549072 4.007234573364258 0.023232052102684975
CurrentTrain: epoch  8, batch     4 | loss: 4.0304666Losses:  4.120161533355713 4.094310760498047 0.025850633159279823
CurrentTrain: epoch  8, batch     5 | loss: 4.1201615Losses:  5.119021892547607 5.093357086181641 0.0256650373339653
CurrentTrain: epoch  8, batch     6 | loss: 5.1190219Losses:  16.081087112426758 16.036544799804688 0.044542595744132996
CurrentTrain: epoch  8, batch     7 | loss: 16.0810871Losses:  4.829207897186279 4.800029277801514 0.029178621247410774
CurrentTrain: epoch  8, batch     8 | loss: 4.8292079Losses:  5.348114490509033 5.31342887878418 0.034685440361499786
CurrentTrain: epoch  8, batch     9 | loss: 5.3481145Losses:  3.957442283630371 3.9316186904907227 0.0258236825466156
CurrentTrain: epoch  8, batch    10 | loss: 3.9574423Losses:  3.4854366779327393 3.4640536308288574 0.02138298563659191
CurrentTrain: epoch  8, batch    11 | loss: 3.4854367Losses:  6.470944404602051 6.43746280670166 0.033481620252132416
CurrentTrain: epoch  8, batch    12 | loss: 6.4709444Losses:  5.3187737464904785 5.287472724914551 0.031301237642765045
CurrentTrain: epoch  8, batch    13 | loss: 5.3187737Losses:  4.2286834716796875 4.20770788192749 0.020975489169359207
CurrentTrain: epoch  8, batch    14 | loss: 4.2286835Losses:  7.297641754150391 7.2733306884765625 0.02431092970073223
CurrentTrain: epoch  8, batch    15 | loss: 7.2976418Losses:  6.955493927001953 6.9305877685546875 0.024906329810619354
CurrentTrain: epoch  8, batch    16 | loss: 6.9554939Losses:  7.776023864746094 7.729684829711914 0.0463390126824379
CurrentTrain: epoch  8, batch    17 | loss: 7.7760239Losses:  6.909561634063721 6.882706165313721 0.026855438947677612
CurrentTrain: epoch  8, batch    18 | loss: 6.9095616Losses:  6.03336238861084 6.007501602172852 0.025860970839858055
CurrentTrain: epoch  8, batch    19 | loss: 6.0333624Losses:  6.576408863067627 6.549592971801758 0.026816096156835556
CurrentTrain: epoch  8, batch    20 | loss: 6.5764089Losses:  4.015406608581543 3.994511604309082 0.020894939079880714
CurrentTrain: epoch  8, batch    21 | loss: 4.0154066Losses:  7.188826560974121 7.1522297859191895 0.036596640944480896
CurrentTrain: epoch  8, batch    22 | loss: 7.1888266Losses:  4.973850250244141 4.948724269866943 0.02512606792151928
CurrentTrain: epoch  8, batch    23 | loss: 4.9738503Losses:  7.020567417144775 6.983877182006836 0.036690469831228256
CurrentTrain: epoch  8, batch    24 | loss: 7.0205674Losses:  6.07405424118042 6.046062469482422 0.027991971001029015
CurrentTrain: epoch  8, batch    25 | loss: 6.0740542Losses:  6.3224334716796875 6.285818099975586 0.03661554306745529
CurrentTrain: epoch  8, batch    26 | loss: 6.3224335Losses:  5.036392688751221 5.002110481262207 0.03428240865468979
CurrentTrain: epoch  8, batch    27 | loss: 5.0363927Losses:  5.1882147789001465 5.156634330749512 0.03158062696456909
CurrentTrain: epoch  8, batch    28 | loss: 5.1882148Losses:  3.8016843795776367 3.7785425186157227 0.023141900077462196
CurrentTrain: epoch  8, batch    29 | loss: 3.8016844Losses:  4.893553256988525 4.869452953338623 0.024100322276353836
CurrentTrain: epoch  8, batch    30 | loss: 4.8935533Losses:  8.903424263000488 8.859699249267578 0.04372516646981239
CurrentTrain: epoch  8, batch    31 | loss: 8.9034243Losses:  5.855093955993652 5.816571235656738 0.03852282837033272
CurrentTrain: epoch  8, batch    32 | loss: 5.8550940Losses:  7.122491359710693 7.075366497039795 0.04712508246302605
CurrentTrain: epoch  8, batch    33 | loss: 7.1224914Losses:  7.658517360687256 7.622183799743652 0.03633348271250725
CurrentTrain: epoch  8, batch    34 | loss: 7.6585174Losses:  10.929280281066895 10.890482902526855 0.03879711031913757
CurrentTrain: epoch  8, batch    35 | loss: 10.9292803Losses:  5.808256149291992 5.778652191162109 0.02960379607975483
CurrentTrain: epoch  8, batch    36 | loss: 5.8082561Losses:  2.210728168487549 2.160041570663452 0.050686586648225784
CurrentTrain: epoch  8, batch    37 | loss: 2.2107282Losses:  4.8025736808776855 4.77357292175293 0.02900070697069168
CurrentTrain: epoch  9, batch     0 | loss: 4.8025737Losses:  4.027595520019531 4.004694938659668 0.02290034294128418
CurrentTrain: epoch  9, batch     1 | loss: 4.0275955Losses:  6.415065288543701 6.385215759277344 0.029849637299776077
CurrentTrain: epoch  9, batch     2 | loss: 6.4150653Losses:  5.4269633293151855 5.402904987335205 0.02405819483101368
CurrentTrain: epoch  9, batch     3 | loss: 5.4269633Losses:  7.31913423538208 7.28884744644165 0.030286842957139015
CurrentTrain: epoch  9, batch     4 | loss: 7.3191342Losses:  4.988128662109375 4.959208011627197 0.028920743614435196
CurrentTrain: epoch  9, batch     5 | loss: 4.9881287Losses:  4.699374198913574 4.665627479553223 0.0337466225028038
CurrentTrain: epoch  9, batch     6 | loss: 4.6993742Losses:  6.514882564544678 6.480813980102539 0.03406870365142822
CurrentTrain: epoch  9, batch     7 | loss: 6.5148826Losses:  6.217018127441406 6.185914039611816 0.03110400214791298
CurrentTrain: epoch  9, batch     8 | loss: 6.2170181Losses:  4.776302814483643 4.747501373291016 0.02880156598985195
CurrentTrain: epoch  9, batch     9 | loss: 4.7763028Losses:  4.291757106781006 4.270429611206055 0.02132749930024147
CurrentTrain: epoch  9, batch    10 | loss: 4.2917571Losses:  7.677022457122803 7.650242805480957 0.026779770851135254
CurrentTrain: epoch  9, batch    11 | loss: 7.6770225Losses:  3.343933582305908 3.3226990699768066 0.021234549582004547
CurrentTrain: epoch  9, batch    12 | loss: 3.3439336Losses:  5.105813026428223 5.077215194702148 0.028597693890333176
CurrentTrain: epoch  9, batch    13 | loss: 5.1058130Losses:  4.697355270385742 4.670965194702148 0.026389852166175842
CurrentTrain: epoch  9, batch    14 | loss: 4.6973553Losses:  9.134878158569336 9.093076705932617 0.04180154949426651
CurrentTrain: epoch  9, batch    15 | loss: 9.1348782Losses:  7.472724914550781 7.436961650848389 0.035763196647167206
CurrentTrain: epoch  9, batch    16 | loss: 7.4727249Losses:  15.306655883789062 15.247511863708496 0.05914432182908058
CurrentTrain: epoch  9, batch    17 | loss: 15.3066559Losses:  6.234975337982178 6.206096649169922 0.02887858636677265
CurrentTrain: epoch  9, batch    18 | loss: 6.2349753Losses:  3.405508041381836 3.3831520080566406 0.022355977445840836
CurrentTrain: epoch  9, batch    19 | loss: 3.4055080Losses:  6.936900615692139 6.8945722579956055 0.042328521609306335
CurrentTrain: epoch  9, batch    20 | loss: 6.9369006Losses:  4.542621612548828 4.520289897918701 0.022331787273287773
CurrentTrain: epoch  9, batch    21 | loss: 4.5426216Losses:  4.619576454162598 4.586408615112305 0.0331679992377758
CurrentTrain: epoch  9, batch    22 | loss: 4.6195765Losses:  6.293849945068359 6.266440391540527 0.02740948274731636
CurrentTrain: epoch  9, batch    23 | loss: 6.2938499Losses:  5.9422831535339355 5.909762859344482 0.03252049535512924
CurrentTrain: epoch  9, batch    24 | loss: 5.9422832Losses:  3.722623586654663 3.700206756591797 0.02241688407957554
CurrentTrain: epoch  9, batch    25 | loss: 3.7226236Losses:  5.849979400634766 5.821391582489014 0.028587903827428818
CurrentTrain: epoch  9, batch    26 | loss: 5.8499794Losses:  5.245424747467041 5.2191362380981445 0.026288719847798347
CurrentTrain: epoch  9, batch    27 | loss: 5.2454247Losses:  5.006162166595459 4.980236053466797 0.025926079601049423
CurrentTrain: epoch  9, batch    28 | loss: 5.0061622Losses:  4.025839805603027 4.00026273727417 0.02557702176272869
CurrentTrain: epoch  9, batch    29 | loss: 4.0258398Losses:  3.8173065185546875 3.7928719520568848 0.02443448267877102
CurrentTrain: epoch  9, batch    30 | loss: 3.8173065Losses:  4.404621601104736 4.38242244720459 0.022199172526597977
CurrentTrain: epoch  9, batch    31 | loss: 4.4046216Losses:  5.842167854309082 5.806292533874512 0.035875506699085236
CurrentTrain: epoch  9, batch    32 | loss: 5.8421679Losses:  6.530564785003662 6.503489017486572 0.02707556262612343
CurrentTrain: epoch  9, batch    33 | loss: 6.5305648Losses:  5.3445210456848145 5.312299728393555 0.03222151845693588
CurrentTrain: epoch  9, batch    34 | loss: 5.3445210Losses:  5.0435075759887695 5.021978378295898 0.021529417484998703
CurrentTrain: epoch  9, batch    35 | loss: 5.0435076Losses:  7.608689308166504 7.580835342407227 0.027854017913341522
CurrentTrain: epoch  9, batch    36 | loss: 7.6086893Losses:  1.6013636589050293 1.570020318031311 0.03134335204958916
CurrentTrain: epoch  9, batch    37 | loss: 1.6013637
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 84.93%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 83.68%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 86.20%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.26%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.93%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 87.70%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.89%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.36%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 84.93%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 83.68%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 86.20%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.26%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.93%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 87.70%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.89%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.36%   
cur_acc:  ['0.8636']
his_acc:  ['0.8636']
Clustering into  4  clusters
Clusters:  [0 2 2 0 0 3 0 1 0 0 0]
Losses:  8.117416381835938 7.583930015563965 0.533486008644104
CurrentTrain: epoch  0, batch     0 | loss: 8.1174164Losses:  2.2758820056915283 1.8452203273773193 0.43066170811653137
CurrentTrain: epoch  0, batch     1 | loss: 2.2758820Losses:  8.162261962890625 7.9177703857421875 0.24449202418327332
CurrentTrain: epoch  1, batch     0 | loss: 8.1622620Losses:  3.6697628498077393 3.6697628498077393 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 3.6697628Losses:  7.087067604064941 6.937967777252197 0.14909975230693817
CurrentTrain: epoch  2, batch     0 | loss: 7.0870676Losses:  2.3456320762634277 2.1469123363494873 0.19871973991394043
CurrentTrain: epoch  2, batch     1 | loss: 2.3456321Losses:  6.556976795196533 6.379830360412598 0.17714622616767883
CurrentTrain: epoch  3, batch     0 | loss: 6.5569768Losses:  1.727677822113037 1.5688035488128662 0.1588743031024933
CurrentTrain: epoch  3, batch     1 | loss: 1.7276778Losses:  6.644861698150635 6.534055709838867 0.11080609261989594
CurrentTrain: epoch  4, batch     0 | loss: 6.6448617Losses:  2.081987142562866 1.892138123512268 0.18984901905059814
CurrentTrain: epoch  4, batch     1 | loss: 2.0819871Losses:  6.561824321746826 6.466254234313965 0.0955701544880867
CurrentTrain: epoch  5, batch     0 | loss: 6.5618243Losses:  1.7655268907546997 1.6891427040100098 0.07638418674468994
CurrentTrain: epoch  5, batch     1 | loss: 1.7655269Losses:  6.947914123535156 6.855747222900391 0.09216702729463577
CurrentTrain: epoch  6, batch     0 | loss: 6.9479141Losses:  3.5372745990753174 3.474684715270996 0.06258983910083771
CurrentTrain: epoch  6, batch     1 | loss: 3.5372746Losses:  5.90214204788208 5.836042404174805 0.06609946489334106
CurrentTrain: epoch  7, batch     0 | loss: 5.9021420Losses:  1.9226272106170654 1.86832594871521 0.054301220923662186
CurrentTrain: epoch  7, batch     1 | loss: 1.9226272Losses:  6.563634395599365 6.494747638702393 0.06888680160045624
CurrentTrain: epoch  8, batch     0 | loss: 6.5636344Losses:  3.518690347671509 3.518690347671509 -0.0
CurrentTrain: epoch  8, batch     1 | loss: 3.5186903Losses:  6.995674133300781 6.938571453094482 0.057102736085653305
CurrentTrain: epoch  9, batch     0 | loss: 6.9956741Losses:  4.136003494262695 4.076187610626221 0.05981580168008804
CurrentTrain: epoch  9, batch     1 | loss: 4.1360035
Losses:  0.35712119936943054 -0.0 0.35712119936943054
MemoryTrain:  epoch  0, batch     0 | loss: 0.3571212Losses:  0.33721864223480225 -0.0 0.33721864223480225
MemoryTrain:  epoch  1, batch     0 | loss: 0.3372186Losses:  0.3102155923843384 -0.0 0.3102155923843384
MemoryTrain:  epoch  2, batch     0 | loss: 0.3102156Losses:  0.29178178310394287 -0.0 0.29178178310394287
MemoryTrain:  epoch  3, batch     0 | loss: 0.2917818Losses:  0.2777394652366638 -0.0 0.2777394652366638
MemoryTrain:  epoch  4, batch     0 | loss: 0.2777395Losses:  0.24879343807697296 -0.0 0.24879343807697296
MemoryTrain:  epoch  5, batch     0 | loss: 0.2487934Losses:  0.25674739480018616 -0.0 0.25674739480018616
MemoryTrain:  epoch  6, batch     0 | loss: 0.2567474Losses:  0.23756609857082367 -0.0 0.23756609857082367
MemoryTrain:  epoch  7, batch     0 | loss: 0.2375661Losses:  0.23916251957416534 -0.0 0.23916251957416534
MemoryTrain:  epoch  8, batch     0 | loss: 0.2391625Losses:  0.23608087003231049 -0.0 0.23608087003231049
MemoryTrain:  epoch  9, batch     0 | loss: 0.2360809
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 94.79%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 92.19%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 83.52%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 86.06%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 88.67%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 89.34%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 85.76%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.95%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.29%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 80.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 81.85%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 82.67%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.42%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.11%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.34%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.65%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.16%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.64%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 86.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.10%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.30%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.69%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 87.87%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 88.04%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 88.19%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 88.34%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 88.49%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 88.46%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 87.35%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 86.90%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 86.48%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 86.65%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 86.94%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 87.23%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 87.76%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.01%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 87.50%   
cur_acc:  ['0.8636', '0.8576']
his_acc:  ['0.8636', '0.8750']
Clustering into  7  clusters
Clusters:  [4 1 1 4 0 3 4 5 4 0 0 4 6 1 2 1]
Losses:  7.050572395324707 6.677656173706055 0.3729160726070404
CurrentTrain: epoch  0, batch     0 | loss: 7.0505724Losses:  3.6625020503997803 3.2504189014434814 0.41208308935165405
CurrentTrain: epoch  0, batch     1 | loss: 3.6625021Losses:  7.005319595336914 6.7159318923950195 0.28938794136047363
CurrentTrain: epoch  1, batch     0 | loss: 7.0053196Losses:  5.557586669921875 5.133901119232178 0.4236854314804077
CurrentTrain: epoch  1, batch     1 | loss: 5.5575867Losses:  7.2708940505981445 6.937182426452637 0.33371177315711975
CurrentTrain: epoch  2, batch     0 | loss: 7.2708941Losses:  2.7201218605041504 2.444931745529175 0.2751901149749756
CurrentTrain: epoch  2, batch     1 | loss: 2.7201219Losses:  9.398645401000977 9.120981216430664 0.2776640057563782
CurrentTrain: epoch  3, batch     0 | loss: 9.3986454Losses:  2.5394809246063232 2.274737596511841 0.26474329829216003
CurrentTrain: epoch  3, batch     1 | loss: 2.5394809Losses:  6.844245433807373 6.582784652709961 0.2614608705043793
CurrentTrain: epoch  4, batch     0 | loss: 6.8442454Losses:  1.772973656654358 1.4618079662322998 0.3111656904220581
CurrentTrain: epoch  4, batch     1 | loss: 1.7729737Losses:  6.566102981567383 6.291635513305664 0.27446722984313965
CurrentTrain: epoch  5, batch     0 | loss: 6.5661030Losses:  2.893522024154663 2.642672300338745 0.25084975361824036
CurrentTrain: epoch  5, batch     1 | loss: 2.8935220Losses:  7.549202919006348 7.292824745178223 0.2563783526420593
CurrentTrain: epoch  6, batch     0 | loss: 7.5492029Losses:  3.966747999191284 3.852708578109741 0.1140393540263176
CurrentTrain: epoch  6, batch     1 | loss: 3.9667480Losses:  7.378451347351074 7.133116722106934 0.2453347146511078
CurrentTrain: epoch  7, batch     0 | loss: 7.3784513Losses:  1.6774247884750366 1.4291366338729858 0.2482881397008896
CurrentTrain: epoch  7, batch     1 | loss: 1.6774248Losses:  7.569011211395264 7.324754238128662 0.2442571222782135
CurrentTrain: epoch  8, batch     0 | loss: 7.5690112Losses:  2.331617832183838 2.090911388397217 0.24070635437965393
CurrentTrain: epoch  8, batch     1 | loss: 2.3316178Losses:  6.745671272277832 6.5036773681640625 0.24199390411376953
CurrentTrain: epoch  9, batch     0 | loss: 6.7456713Losses:  1.1450400352478027 0.8961979746818542 0.24884212017059326
CurrentTrain: epoch  9, batch     1 | loss: 1.1450400
Losses:  0.5694373846054077 -0.0 0.5694373846054077
MemoryTrain:  epoch  0, batch     0 | loss: 0.5694374Losses:  0.5407989025115967 -0.0 0.5407989025115967
MemoryTrain:  epoch  1, batch     0 | loss: 0.5407989Losses:  0.5313490033149719 -0.0 0.5313490033149719
MemoryTrain:  epoch  2, batch     0 | loss: 0.5313490Losses:  0.5261729955673218 -0.0 0.5261729955673218
MemoryTrain:  epoch  3, batch     0 | loss: 0.5261730Losses:  0.5204211473464966 -0.0 0.5204211473464966
MemoryTrain:  epoch  4, batch     0 | loss: 0.5204211Losses:  0.5138209462165833 -0.0 0.5138209462165833
MemoryTrain:  epoch  5, batch     0 | loss: 0.5138209Losses:  0.5092569589614868 -0.0 0.5092569589614868
MemoryTrain:  epoch  6, batch     0 | loss: 0.5092570Losses:  0.5013447999954224 -0.0 0.5013447999954224
MemoryTrain:  epoch  7, batch     0 | loss: 0.5013448Losses:  0.4994271397590637 -0.0 0.4994271397590637
MemoryTrain:  epoch  8, batch     0 | loss: 0.4994271Losses:  0.5026626586914062 -0.0 0.5026626586914062
MemoryTrain:  epoch  9, batch     0 | loss: 0.5026627
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 56.25%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 48.75%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 42.71%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 42.86%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 38.28%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 16.67%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 17.19%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 18.75%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 23.21%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 25.00%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 26.39%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 28.75%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 30.68%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 32.81%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 35.58%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 37.95%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 40.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 41.41%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 42.65%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 43.75%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 44.74%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 46.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 49.11%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 51.42%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 53.53%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 55.47%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 57.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 58.89%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 60.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 61.61%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 62.93%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 63.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 64.92%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 65.82%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 67.46%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 68.04%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 68.58%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 69.26%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 69.74%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 70.03%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 70.62%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 69.66%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 69.94%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 70.20%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 70.60%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 71.25%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 72.47%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 73.05%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 73.60%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 73.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 74.26%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 74.04%   [EVAL] batch:   52 | acc: 31.25%,  total acc: 73.23%   [EVAL] batch:   53 | acc: 25.00%,  total acc: 72.34%   [EVAL] batch:   54 | acc: 12.50%,  total acc: 71.25%   [EVAL] batch:   55 | acc: 18.75%,  total acc: 70.31%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 69.74%   
cur_acc:  ['0.8636', '0.8576', '0.3828']
his_acc:  ['0.8636', '0.8750', '0.6974']
Clustering into  9  clusters
Clusters:  [4 1 1 4 0 7 4 5 4 0 0 4 8 1 2 1 2 6 4 1 3]
Losses:  8.53752326965332 8.005731582641602 0.5317912697792053
CurrentTrain: epoch  0, batch     0 | loss: 8.5375233Losses:  4.012720584869385 3.6655197143554688 0.34720075130462646
CurrentTrain: epoch  0, batch     1 | loss: 4.0127206Losses:  8.489431381225586 8.154677391052246 0.3347536623477936
CurrentTrain: epoch  1, batch     0 | loss: 8.4894314Losses:  6.11847448348999 5.695749759674072 0.4227246940135956
CurrentTrain: epoch  1, batch     1 | loss: 6.1184745Losses:  7.202724933624268 6.763111114501953 0.4396139085292816
CurrentTrain: epoch  2, batch     0 | loss: 7.2027249Losses:  2.531917095184326 2.127394914627075 0.40452226996421814
CurrentTrain: epoch  2, batch     1 | loss: 2.5319171Losses:  6.128082275390625 5.708744525909424 0.41933751106262207
CurrentTrain: epoch  3, batch     0 | loss: 6.1280823Losses:  2.0568039417266846 1.6633894443511963 0.39341455698013306
CurrentTrain: epoch  3, batch     1 | loss: 2.0568039Losses:  5.796210289001465 5.401381015777588 0.39482927322387695
CurrentTrain: epoch  4, batch     0 | loss: 5.7962103Losses:  2.0564801692962646 1.6657578945159912 0.3907223641872406
CurrentTrain: epoch  4, batch     1 | loss: 2.0564802Losses:  5.803075313568115 5.4142842292785645 0.38879111409187317
CurrentTrain: epoch  5, batch     0 | loss: 5.8030753Losses:  1.9778666496276855 1.6018685102462769 0.3759981393814087
CurrentTrain: epoch  5, batch     1 | loss: 1.9778666Losses:  5.188492298126221 4.821907043457031 0.3665854036808014
CurrentTrain: epoch  6, batch     0 | loss: 5.1884923Losses:  1.4346895217895508 1.0504262447357178 0.3842632472515106
CurrentTrain: epoch  6, batch     1 | loss: 1.4346895Losses:  6.510142803192139 6.163933753967285 0.3462088406085968
CurrentTrain: epoch  7, batch     0 | loss: 6.5101428Losses:  2.5344531536102295 2.249485492706299 0.2849676311016083
CurrentTrain: epoch  7, batch     1 | loss: 2.5344532Losses:  6.535857677459717 6.196840763092041 0.33901676535606384
CurrentTrain: epoch  8, batch     0 | loss: 6.5358577Losses:  2.6168622970581055 2.342015504837036 0.2748467028141022
CurrentTrain: epoch  8, batch     1 | loss: 2.6168623Losses:  6.119836330413818 5.788393974304199 0.3314422070980072
CurrentTrain: epoch  9, batch     0 | loss: 6.1198363Losses:  3.1519744396209717 2.819486379623413 0.33248814940452576
CurrentTrain: epoch  9, batch     1 | loss: 3.1519744
Losses:  0.47614583373069763 -0.0 0.47614583373069763
MemoryTrain:  epoch  0, batch     0 | loss: 0.4761458Losses:  0.39691147208213806 -0.0 0.39691147208213806
MemoryTrain:  epoch  0, batch     1 | loss: 0.3969115Losses:  0.6909186244010925 -0.0 0.6909186244010925
MemoryTrain:  epoch  1, batch     0 | loss: 0.6909186Losses:  0.1450432538986206 -0.0 0.1450432538986206
MemoryTrain:  epoch  1, batch     1 | loss: 0.1450433Losses:  0.5257468223571777 -0.0 0.5257468223571777
MemoryTrain:  epoch  2, batch     0 | loss: 0.5257468Losses:  0.3332764506340027 -0.0 0.3332764506340027
MemoryTrain:  epoch  2, batch     1 | loss: 0.3332765Losses:  0.6975954174995422 -0.0 0.6975954174995422
MemoryTrain:  epoch  3, batch     0 | loss: 0.6975954Losses:  0.2654324173927307 -0.0 0.2654324173927307
MemoryTrain:  epoch  3, batch     1 | loss: 0.2654324Losses:  0.5651404857635498 -0.0 0.5651404857635498
MemoryTrain:  epoch  4, batch     0 | loss: 0.5651405Losses:  0.29906731843948364 -0.0 0.29906731843948364
MemoryTrain:  epoch  4, batch     1 | loss: 0.2990673Losses:  0.6069948077201843 -0.0 0.6069948077201843
MemoryTrain:  epoch  5, batch     0 | loss: 0.6069948Losses:  0.23563095927238464 -0.0 0.23563095927238464
MemoryTrain:  epoch  5, batch     1 | loss: 0.2356310Losses:  0.5075080990791321 -0.0 0.5075080990791321
MemoryTrain:  epoch  6, batch     0 | loss: 0.5075081Losses:  0.3809746205806732 -0.0 0.3809746205806732
MemoryTrain:  epoch  6, batch     1 | loss: 0.3809746Losses:  0.5838754177093506 -0.0 0.5838754177093506
MemoryTrain:  epoch  7, batch     0 | loss: 0.5838754Losses:  0.3243451714515686 -0.0 0.3243451714515686
MemoryTrain:  epoch  7, batch     1 | loss: 0.3243452Losses:  0.628858745098114 -0.0 0.628858745098114
MemoryTrain:  epoch  8, batch     0 | loss: 0.6288587Losses:  0.292236328125 -0.0 0.292236328125
MemoryTrain:  epoch  8, batch     1 | loss: 0.2922363Losses:  0.5977679491043091 -0.0 0.5977679491043091
MemoryTrain:  epoch  9, batch     0 | loss: 0.5977679Losses:  0.3639182448387146 -0.0 0.3639182448387146
MemoryTrain:  epoch  9, batch     1 | loss: 0.3639182
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 80.56%   [EVAL] batch:    9 | acc: 0.00%,  total acc: 72.50%   [EVAL] batch:   10 | acc: 25.00%,  total acc: 68.18%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 68.27%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 66.96%   
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 12.50%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 15.62%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 17.50%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 16.67%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 24.11%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 31.25%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 36.81%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 41.88%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 45.45%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 47.92%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 50.00%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 50.45%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 50.42%   [EVAL] batch:   15 | acc: 37.50%,  total acc: 49.61%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 49.26%   [EVAL] batch:   17 | acc: 31.25%,  total acc: 48.26%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 47.70%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 48.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 51.19%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 53.41%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 55.16%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 56.77%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 58.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 60.10%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 61.34%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 62.72%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 64.01%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 64.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 65.93%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 66.80%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 67.05%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 66.73%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 65.89%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 65.28%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 64.53%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 64.31%   [EVAL] batch:   38 | acc: 43.75%,  total acc: 63.78%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 64.53%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 63.72%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 63.99%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 64.10%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 64.63%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 65.42%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 66.17%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 66.89%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 68.24%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 68.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 69.00%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 67.69%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 66.67%   [EVAL] batch:   54 | acc: 6.25%,  total acc: 65.57%   [EVAL] batch:   55 | acc: 12.50%,  total acc: 64.62%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 64.04%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 64.66%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 65.25%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 65.52%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 65.57%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 65.73%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 65.87%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 66.21%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 66.15%   [EVAL] batch:   65 | acc: 50.00%,  total acc: 65.91%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 64.93%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 64.52%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 64.67%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 64.82%   [EVAL] batch:   70 | acc: 25.00%,  total acc: 64.26%   
cur_acc:  ['0.8636', '0.8576', '0.3828', '0.6696']
his_acc:  ['0.8636', '0.8750', '0.6974', '0.6426']
Clustering into  12  clusters
Clusters:  [ 2  0  0  2  1  9  2 11  2  1  5  2 10  0  6  0  6  8  2  0  4  2  7  3
  1  1]
Losses:  7.243007659912109 6.97684383392334 0.26616358757019043
CurrentTrain: epoch  0, batch     0 | loss: 7.2430077Losses:  2.2169718742370605 1.8802410364151 0.3367307782173157
CurrentTrain: epoch  0, batch     1 | loss: 2.2169719Losses:  7.072665691375732 6.805358409881592 0.2673071324825287
CurrentTrain: epoch  1, batch     0 | loss: 7.0726657Losses:  2.7465100288391113 2.4613888263702393 0.2851211428642273
CurrentTrain: epoch  1, batch     1 | loss: 2.7465100Losses:  8.50578784942627 8.251842498779297 0.25394535064697266
CurrentTrain: epoch  2, batch     0 | loss: 8.5057878Losses:  2.8217086791992188 2.5633082389831543 0.25840049982070923
CurrentTrain: epoch  2, batch     1 | loss: 2.8217087Losses:  7.214484691619873 6.969710350036621 0.24477426707744598
CurrentTrain: epoch  3, batch     0 | loss: 7.2144847Losses:  4.005825996398926 3.7532382011413574 0.2525879144668579
CurrentTrain: epoch  3, batch     1 | loss: 4.0058260Losses:  6.217555999755859 5.964615821838379 0.2529403567314148
CurrentTrain: epoch  4, batch     0 | loss: 6.2175560Losses:  2.286846160888672 2.0559306144714355 0.23091542720794678
CurrentTrain: epoch  4, batch     1 | loss: 2.2868462Losses:  6.967190265655518 6.7365288734436035 0.23066140711307526
CurrentTrain: epoch  5, batch     0 | loss: 6.9671903Losses:  4.300704479217529 4.10730504989624 0.1933993548154831
CurrentTrain: epoch  5, batch     1 | loss: 4.3007045Losses:  5.655187129974365 5.428654193878174 0.22653314471244812
CurrentTrain: epoch  6, batch     0 | loss: 5.6551871Losses:  2.288569450378418 2.0549228191375732 0.2336467206478119
CurrentTrain: epoch  6, batch     1 | loss: 2.2885695Losses:  6.855373859405518 6.628819465637207 0.22655442357063293
CurrentTrain: epoch  7, batch     0 | loss: 6.8553739Losses:  2.3066163063049316 2.0803048610687256 0.22631143033504486
CurrentTrain: epoch  7, batch     1 | loss: 2.3066163Losses:  7.229640483856201 7.006837844848633 0.22280247509479523
CurrentTrain: epoch  8, batch     0 | loss: 7.2296405Losses:  2.337754011154175 2.2084853649139404 0.12926867604255676
CurrentTrain: epoch  8, batch     1 | loss: 2.3377540Losses:  5.742326259613037 5.521878242492676 0.22044822573661804
CurrentTrain: epoch  9, batch     0 | loss: 5.7423263Losses:  1.7406988143920898 1.5161056518554688 0.22459320724010468
CurrentTrain: epoch  9, batch     1 | loss: 1.7406988
Losses:  0.5561082363128662 -0.0 0.5561082363128662
MemoryTrain:  epoch  0, batch     0 | loss: 0.5561082Losses:  0.6321886777877808 -0.0 0.6321886777877808
MemoryTrain:  epoch  0, batch     1 | loss: 0.6321887Losses:  0.7515528202056885 -0.0 0.7515528202056885
MemoryTrain:  epoch  1, batch     0 | loss: 0.7515528Losses:  0.45462989807128906 -0.0 0.45462989807128906
MemoryTrain:  epoch  1, batch     1 | loss: 0.4546299Losses:  0.4690423905849457 -0.0 0.4690423905849457
MemoryTrain:  epoch  2, batch     0 | loss: 0.4690424Losses:  0.6821731328964233 -0.0 0.6821731328964233
MemoryTrain:  epoch  2, batch     1 | loss: 0.6821731Losses:  0.6988590359687805 -0.0 0.6988590359687805
MemoryTrain:  epoch  3, batch     0 | loss: 0.6988590Losses:  0.4444971978664398 -0.0 0.4444971978664398
MemoryTrain:  epoch  3, batch     1 | loss: 0.4444972Losses:  0.8035432696342468 -0.0 0.8035432696342468
MemoryTrain:  epoch  4, batch     0 | loss: 0.8035433Losses:  0.4816708564758301 -0.0 0.4816708564758301
MemoryTrain:  epoch  4, batch     1 | loss: 0.4816709Losses:  0.533991813659668 -0.0 0.533991813659668
MemoryTrain:  epoch  5, batch     0 | loss: 0.5339918Losses:  0.5568892955780029 -0.0 0.5568892955780029
MemoryTrain:  epoch  5, batch     1 | loss: 0.5568893Losses:  0.5112015604972839 -0.0 0.5112015604972839
MemoryTrain:  epoch  6, batch     0 | loss: 0.5112016Losses:  0.650770902633667 -0.0 0.650770902633667
MemoryTrain:  epoch  6, batch     1 | loss: 0.6507709Losses:  0.547020673751831 -0.0 0.547020673751831
MemoryTrain:  epoch  7, batch     0 | loss: 0.5470207Losses:  0.7084625363349915 -0.0 0.7084625363349915
MemoryTrain:  epoch  7, batch     1 | loss: 0.7084625Losses:  0.6260206699371338 -0.0 0.6260206699371338
MemoryTrain:  epoch  8, batch     0 | loss: 0.6260207Losses:  0.5307913422584534 -0.0 0.5307913422584534
MemoryTrain:  epoch  8, batch     1 | loss: 0.5307913Losses:  0.5303232669830322 -0.0 0.5303232669830322
MemoryTrain:  epoch  9, batch     0 | loss: 0.5303233Losses:  0.6598387360572815 -0.0 0.6598387360572815
MemoryTrain:  epoch  9, batch     1 | loss: 0.6598387
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 41.67%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 45.31%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 48.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 55.21%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 61.61%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 64.84%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 66.67%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 66.88%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 68.18%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 67.71%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 68.27%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 69.20%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 67.50%   
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 22.92%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 28.12%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 32.50%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 30.21%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 36.61%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 42.19%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 44.44%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 49.38%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 52.27%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 53.12%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 54.81%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 54.02%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 53.33%   [EVAL] batch:   15 | acc: 37.50%,  total acc: 52.34%   [EVAL] batch:   16 | acc: 25.00%,  total acc: 50.74%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 49.31%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 48.68%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 49.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 51.79%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 53.98%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 55.98%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 57.81%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 59.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 61.06%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 61.81%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 63.17%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 64.44%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 64.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 65.93%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 66.80%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 66.18%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 65.00%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 64.24%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 63.34%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 62.83%   [EVAL] batch:   38 | acc: 43.75%,  total acc: 62.34%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 63.28%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 62.65%   [EVAL] batch:   41 | acc: 0.00%,  total acc: 61.16%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 59.74%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 59.94%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 60.83%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 61.68%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 63.28%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 64.03%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 64.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 64.83%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 64.54%   [EVAL] batch:   52 | acc: 18.75%,  total acc: 63.68%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 62.73%   [EVAL] batch:   54 | acc: 6.25%,  total acc: 61.70%   [EVAL] batch:   55 | acc: 6.25%,  total acc: 60.71%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 60.31%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 60.99%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 61.44%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 61.46%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 61.48%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 60.89%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 60.71%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 61.04%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 60.58%   [EVAL] batch:   65 | acc: 43.75%,  total acc: 60.32%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 59.70%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 59.47%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 59.69%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 59.91%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 59.95%   [EVAL] batch:   71 | acc: 43.75%,  total acc: 59.72%   [EVAL] batch:   72 | acc: 37.50%,  total acc: 59.42%   [EVAL] batch:   73 | acc: 37.50%,  total acc: 59.12%   [EVAL] batch:   74 | acc: 43.75%,  total acc: 58.92%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 59.38%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 59.82%   [EVAL] batch:   77 | acc: 93.75%,  total acc: 60.26%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 60.60%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 60.78%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 60.88%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 61.13%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 61.22%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 61.31%   [EVAL] batch:   84 | acc: 87.50%,  total acc: 61.62%   
cur_acc:  ['0.8636', '0.8576', '0.3828', '0.6696', '0.6750']
his_acc:  ['0.8636', '0.8750', '0.6974', '0.6426', '0.6162']
Clustering into  14  clusters
Clusters:  [ 1  9  0  1  4 11  1 13  1 10  6  1 12  0  2  0  2  8  1  0  5  1  3  7
  4 10  3  1 10  9  1]
Losses:  8.306682586669922 7.888810157775879 0.41787266731262207
CurrentTrain: epoch  0, batch     0 | loss: 8.3066826Losses:  2.391716957092285 2.088472843170166 0.3032441735267639
CurrentTrain: epoch  0, batch     1 | loss: 2.3917170Losses:  8.355903625488281 8.056890487670898 0.2990126609802246
CurrentTrain: epoch  1, batch     0 | loss: 8.3559036Losses:  3.5481505393981934 3.233130693435669 0.315019816160202
CurrentTrain: epoch  1, batch     1 | loss: 3.5481505Losses:  7.793730735778809 7.517810344696045 0.2759203612804413
CurrentTrain: epoch  2, batch     0 | loss: 7.7937307Losses:  3.1980960369110107 2.954686403274536 0.24340961873531342
CurrentTrain: epoch  2, batch     1 | loss: 3.1980960Losses:  6.841678619384766 6.565610885620117 0.27606749534606934
CurrentTrain: epoch  3, batch     0 | loss: 6.8416786Losses:  2.5619914531707764 2.389695644378662 0.17229583859443665
CurrentTrain: epoch  3, batch     1 | loss: 2.5619915Losses:  6.248593330383301 5.97274112701416 0.2758523225784302
CurrentTrain: epoch  4, batch     0 | loss: 6.2485933Losses:  2.146988868713379 1.9072990417480469 0.239689901471138
CurrentTrain: epoch  4, batch     1 | loss: 2.1469889Losses:  9.640544891357422 9.411630630493164 0.2289140522480011
CurrentTrain: epoch  5, batch     0 | loss: 9.6405449Losses:  4.847264289855957 4.654435157775879 0.19282889366149902
CurrentTrain: epoch  5, batch     1 | loss: 4.8472643Losses:  6.764434337615967 6.522002220153809 0.24243228137493134
CurrentTrain: epoch  6, batch     0 | loss: 6.7644343Losses:  1.8109674453735352 1.5849895477294922 0.22597789764404297
CurrentTrain: epoch  6, batch     1 | loss: 1.8109674Losses:  6.221607208251953 5.993597030639648 0.228010356426239
CurrentTrain: epoch  7, batch     0 | loss: 6.2216072Losses:  1.8007038831710815 1.5713059902191162 0.2293979376554489
CurrentTrain: epoch  7, batch     1 | loss: 1.8007039Losses:  5.820798397064209 5.596809387207031 0.22398895025253296
CurrentTrain: epoch  8, batch     0 | loss: 5.8207984Losses:  1.8866082429885864 1.6622700691223145 0.22433820366859436
CurrentTrain: epoch  8, batch     1 | loss: 1.8866082Losses:  6.104615688323975 5.8812055587768555 0.22341015934944153
CurrentTrain: epoch  9, batch     0 | loss: 6.1046157Losses:  2.463817596435547 2.317157030105591 0.14666049182415009
CurrentTrain: epoch  9, batch     1 | loss: 2.4638176
Losses:  0.8355507850646973 -0.0 0.8355507850646973
MemoryTrain:  epoch  0, batch     0 | loss: 0.8355508Losses:  0.8639000654220581 -0.0 0.8639000654220581
MemoryTrain:  epoch  0, batch     1 | loss: 0.8639001Losses:  0.7411234974861145 -0.0 0.7411234974861145
MemoryTrain:  epoch  1, batch     0 | loss: 0.7411235Losses:  0.848845899105072 -0.0 0.848845899105072
MemoryTrain:  epoch  1, batch     1 | loss: 0.8488459Losses:  0.8343193531036377 -0.0 0.8343193531036377
MemoryTrain:  epoch  2, batch     0 | loss: 0.8343194Losses:  0.7814850807189941 -0.0 0.7814850807189941
MemoryTrain:  epoch  2, batch     1 | loss: 0.7814851Losses:  0.750446617603302 -0.0 0.750446617603302
MemoryTrain:  epoch  3, batch     0 | loss: 0.7504466Losses:  0.9414834380149841 -0.0 0.9414834380149841
MemoryTrain:  epoch  3, batch     1 | loss: 0.9414834Losses:  0.6957505345344543 -0.0 0.6957505345344543
MemoryTrain:  epoch  4, batch     0 | loss: 0.6957505Losses:  0.6066654920578003 -0.0 0.6066654920578003
MemoryTrain:  epoch  4, batch     1 | loss: 0.6066655Losses:  0.7472688555717468 -0.0 0.7472688555717468
MemoryTrain:  epoch  5, batch     0 | loss: 0.7472689Losses:  0.728614091873169 -0.0 0.728614091873169
MemoryTrain:  epoch  5, batch     1 | loss: 0.7286141Losses:  0.7736348509788513 -0.0 0.7736348509788513
MemoryTrain:  epoch  6, batch     0 | loss: 0.7736349Losses:  0.7924240827560425 -0.0 0.7924240827560425
MemoryTrain:  epoch  6, batch     1 | loss: 0.7924241Losses:  0.6053915023803711 -0.0 0.6053915023803711
MemoryTrain:  epoch  7, batch     0 | loss: 0.6053915Losses:  0.767073929309845 -0.0 0.767073929309845
MemoryTrain:  epoch  7, batch     1 | loss: 0.7670739Losses:  0.7733316421508789 -0.0 0.7733316421508789
MemoryTrain:  epoch  8, batch     0 | loss: 0.7733316Losses:  0.5630940198898315 -0.0 0.5630940198898315
MemoryTrain:  epoch  8, batch     1 | loss: 0.5630940Losses:  0.849725604057312 -0.0 0.849725604057312
MemoryTrain:  epoch  9, batch     0 | loss: 0.8497256Losses:  0.7347327470779419 -0.0 0.7347327470779419
MemoryTrain:  epoch  9, batch     1 | loss: 0.7347327
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 81.82%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 80.21%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 79.81%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 75.00%   
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 39.58%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 42.19%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 45.00%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 42.71%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 47.32%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 51.56%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 53.47%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 56.88%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 59.09%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 61.54%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 61.61%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 61.25%   [EVAL] batch:   15 | acc: 37.50%,  total acc: 59.77%   [EVAL] batch:   16 | acc: 37.50%,  total acc: 58.46%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 56.60%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 55.59%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 55.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.04%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 59.94%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 61.68%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 63.02%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 64.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 65.87%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 66.20%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 67.41%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 68.53%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 69.76%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 70.51%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 70.08%   [EVAL] batch:   33 | acc: 37.50%,  total acc: 69.12%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 67.50%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 66.32%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 65.37%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 65.13%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 64.42%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 65.31%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 64.48%   [EVAL] batch:   41 | acc: 0.00%,  total acc: 62.95%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 61.48%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 61.65%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 63.32%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 64.10%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 64.84%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 65.56%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 65.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 66.30%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 66.11%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 65.09%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 64.12%   [EVAL] batch:   54 | acc: 6.25%,  total acc: 63.07%   [EVAL] batch:   55 | acc: 6.25%,  total acc: 62.05%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 61.51%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 61.64%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 61.65%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 61.77%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 61.58%   [EVAL] batch:   61 | acc: 37.50%,  total acc: 61.19%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 60.81%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 61.13%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 60.77%   [EVAL] batch:   65 | acc: 31.25%,  total acc: 60.32%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 59.42%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 59.19%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 59.33%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 59.46%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 59.33%   [EVAL] batch:   71 | acc: 25.00%,  total acc: 58.85%   [EVAL] batch:   72 | acc: 56.25%,  total acc: 58.82%   [EVAL] batch:   73 | acc: 31.25%,  total acc: 58.45%   [EVAL] batch:   74 | acc: 31.25%,  total acc: 58.08%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 58.14%   [EVAL] batch:   76 | acc: 43.75%,  total acc: 57.95%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 58.17%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 57.99%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 58.36%   [EVAL] batch:   80 | acc: 12.50%,  total acc: 57.79%   [EVAL] batch:   81 | acc: 6.25%,  total acc: 57.16%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 56.48%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 56.03%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 55.44%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 55.67%   [EVAL] batch:   86 | acc: 81.25%,  total acc: 55.96%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 56.11%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 56.32%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 56.81%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 57.14%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 57.34%   [EVAL] batch:   92 | acc: 87.50%,  total acc: 57.66%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 57.98%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 58.16%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 58.46%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 58.51%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 58.61%   [EVAL] batch:   98 | acc: 12.50%,  total acc: 58.14%   
cur_acc:  ['0.8636', '0.8576', '0.3828', '0.6696', '0.6750', '0.7500']
his_acc:  ['0.8636', '0.8750', '0.6974', '0.6426', '0.6162', '0.5814']
Clustering into  17  clusters
Clusters:  [ 1  9 10 16  0  2  1 11  1  4  0  1 12 10  6 13  6  8  1 10  5  1  3 15
 14  4  3  1  4  9  1 10 10  7  2  4]
Losses:  9.214988708496094 8.730146408081055 0.4848421812057495
CurrentTrain: epoch  0, batch     0 | loss: 9.2149887Losses:  2.625081777572632 2.1281650066375732 0.49691686034202576
CurrentTrain: epoch  0, batch     1 | loss: 2.6250818Losses:  10.021438598632812 9.531286239624023 0.49015262722969055
CurrentTrain: epoch  1, batch     0 | loss: 10.0214386Losses:  2.663635015487671 2.299771308898926 0.36386364698410034
CurrentTrain: epoch  1, batch     1 | loss: 2.6636350Losses:  8.520017623901367 8.081037521362305 0.4389801621437073
CurrentTrain: epoch  2, batch     0 | loss: 8.5200176Losses:  5.698770523071289 5.214178562164307 0.48459213972091675
CurrentTrain: epoch  2, batch     1 | loss: 5.6987705Losses:  6.991028785705566 6.496801376342773 0.4942275881767273
CurrentTrain: epoch  3, batch     0 | loss: 6.9910288Losses:  3.1991629600524902 2.739017963409424 0.4601450562477112
CurrentTrain: epoch  3, batch     1 | loss: 3.1991630Losses:  8.481489181518555 8.01153564453125 0.46995314955711365
CurrentTrain: epoch  4, batch     0 | loss: 8.4814892Losses:  1.9931297302246094 1.6790319681167603 0.31409770250320435
CurrentTrain: epoch  4, batch     1 | loss: 1.9931297Losses:  7.145578384399414 6.854037284851074 0.29154103994369507
CurrentTrain: epoch  5, batch     0 | loss: 7.1455784Losses:  5.289462089538574 4.882445335388184 0.4070167541503906
CurrentTrain: epoch  5, batch     1 | loss: 5.2894621Losses:  7.370455265045166 6.933937072753906 0.4365180730819702
CurrentTrain: epoch  6, batch     0 | loss: 7.3704553Losses:  3.793910264968872 3.392347574234009 0.40156275033950806
CurrentTrain: epoch  6, batch     1 | loss: 3.7939103Losses:  6.854568004608154 6.539671897888184 0.31489598751068115
CurrentTrain: epoch  7, batch     0 | loss: 6.8545680Losses:  5.158806324005127 4.719679832458496 0.4391264319419861
CurrentTrain: epoch  7, batch     1 | loss: 5.1588063Losses:  7.0670623779296875 6.7691264152526855 0.2979359030723572
CurrentTrain: epoch  8, batch     0 | loss: 7.0670624Losses:  2.4957387447357178 2.065426826477051 0.430311918258667
CurrentTrain: epoch  8, batch     1 | loss: 2.4957387Losses:  7.570511341094971 7.2659010887146 0.3046104311943054
CurrentTrain: epoch  9, batch     0 | loss: 7.5705113Losses:  3.678334951400757 3.2159640789031982 0.46237096190452576
CurrentTrain: epoch  9, batch     1 | loss: 3.6783350
Losses:  0.7873373627662659 -0.0 0.7873373627662659
MemoryTrain:  epoch  0, batch     0 | loss: 0.7873374Losses:  0.8691283464431763 -0.0 0.8691283464431763
MemoryTrain:  epoch  0, batch     1 | loss: 0.8691283Losses:  0.5146489143371582 -0.0 0.5146489143371582
MemoryTrain:  epoch  0, batch     2 | loss: 0.5146489Losses:  0.8830384016036987 -0.0 0.8830384016036987
MemoryTrain:  epoch  1, batch     0 | loss: 0.8830384Losses:  0.6923010349273682 -0.0 0.6923010349273682
MemoryTrain:  epoch  1, batch     1 | loss: 0.6923010Losses:  0.23563829064369202 -0.0 0.23563829064369202
MemoryTrain:  epoch  1, batch     2 | loss: 0.2356383Losses:  0.7865835428237915 -0.0 0.7865835428237915
MemoryTrain:  epoch  2, batch     0 | loss: 0.7865835Losses:  0.8986031413078308 -0.0 0.8986031413078308
MemoryTrain:  epoch  2, batch     1 | loss: 0.8986031Losses:  0.26267701387405396 -0.0 0.26267701387405396
MemoryTrain:  epoch  2, batch     2 | loss: 0.2626770Losses:  0.8426790237426758 -0.0 0.8426790237426758
MemoryTrain:  epoch  3, batch     0 | loss: 0.8426790Losses:  0.907562255859375 -0.0 0.907562255859375
MemoryTrain:  epoch  3, batch     1 | loss: 0.9075623Losses:  0.26321858167648315 -0.0 0.26321858167648315
MemoryTrain:  epoch  3, batch     2 | loss: 0.2632186Losses:  0.8036565780639648 -0.0 0.8036565780639648
MemoryTrain:  epoch  4, batch     0 | loss: 0.8036566Losses:  0.8149499893188477 -0.0 0.8149499893188477
MemoryTrain:  epoch  4, batch     1 | loss: 0.8149500Losses:  0.0983857810497284 -0.0 0.0983857810497284
MemoryTrain:  epoch  4, batch     2 | loss: 0.0983858Losses:  0.8900452852249146 -0.0 0.8900452852249146
MemoryTrain:  epoch  5, batch     0 | loss: 0.8900453Losses:  0.8357338309288025 -0.0 0.8357338309288025
MemoryTrain:  epoch  5, batch     1 | loss: 0.8357338Losses:  0.28308403491973877 -0.0 0.28308403491973877
MemoryTrain:  epoch  5, batch     2 | loss: 0.2830840Losses:  0.7436097860336304 -0.0 0.7436097860336304
MemoryTrain:  epoch  6, batch     0 | loss: 0.7436098Losses:  0.7422589063644409 -0.0 0.7422589063644409
MemoryTrain:  epoch  6, batch     1 | loss: 0.7422589Losses:  0.4311508536338806 -0.0 0.4311508536338806
MemoryTrain:  epoch  6, batch     2 | loss: 0.4311509Losses:  0.7702898979187012 -0.0 0.7702898979187012
MemoryTrain:  epoch  7, batch     0 | loss: 0.7702899Losses:  0.9273875951766968 -0.0 0.9273875951766968
MemoryTrain:  epoch  7, batch     1 | loss: 0.9273876Losses:  0.25149813294410706 -0.0 0.25149813294410706
MemoryTrain:  epoch  7, batch     2 | loss: 0.2514981Losses:  1.0353271961212158 -0.0 1.0353271961212158
MemoryTrain:  epoch  8, batch     0 | loss: 1.0353272Losses:  0.7935782074928284 -0.0 0.7935782074928284
MemoryTrain:  epoch  8, batch     1 | loss: 0.7935782Losses:  0.29045480489730835 -0.0 0.29045480489730835
MemoryTrain:  epoch  8, batch     2 | loss: 0.2904548Losses:  0.6503576040267944 -0.0 0.6503576040267944
MemoryTrain:  epoch  9, batch     0 | loss: 0.6503576Losses:  0.9577755928039551 -0.0 0.9577755928039551
MemoryTrain:  epoch  9, batch     1 | loss: 0.9577756Losses:  0.36346596479415894 -0.0 0.36346596479415894
MemoryTrain:  epoch  9, batch     2 | loss: 0.3634660
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 18.75%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 16.67%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 14.06%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 12.50%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 10.42%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 11.61%   [EVAL] batch:    7 | acc: 31.25%,  total acc: 14.06%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 16.67%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 19.38%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 21.59%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 22.92%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 25.96%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 30.36%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 33.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 37.89%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 41.18%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 43.40%   [EVAL] batch:   18 | acc: 12.50%,  total acc: 41.78%   [EVAL] batch:   19 | acc: 18.75%,  total acc: 40.62%   [EVAL] batch:   20 | acc: 12.50%,  total acc: 39.29%   [EVAL] batch:   21 | acc: 6.25%,  total acc: 37.78%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 48.75%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 46.88%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 48.21%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 48.44%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 47.22%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 47.50%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 46.59%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 46.35%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 48.08%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 46.88%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 48.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 49.22%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 50.74%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 51.39%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 51.97%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 53.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 55.95%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 57.95%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 59.78%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 61.46%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 63.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 64.42%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 65.51%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 66.74%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 67.89%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 68.33%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 68.95%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 69.73%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 69.32%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 67.83%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 66.07%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 64.93%   [EVAL] batch:   36 | acc: 25.00%,  total acc: 63.85%   [EVAL] batch:   37 | acc: 37.50%,  total acc: 63.16%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 62.50%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 62.97%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 62.20%   [EVAL] batch:   41 | acc: 0.00%,  total acc: 60.71%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 59.30%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 59.52%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 60.42%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 61.28%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 62.10%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 62.89%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 63.65%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 63.88%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 63.85%   [EVAL] batch:   51 | acc: 25.00%,  total acc: 63.10%   [EVAL] batch:   52 | acc: 25.00%,  total acc: 62.38%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 62.04%   [EVAL] batch:   54 | acc: 18.75%,  total acc: 61.25%   [EVAL] batch:   55 | acc: 18.75%,  total acc: 60.49%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 60.42%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 60.78%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 61.12%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 61.15%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 60.96%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 60.38%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 60.02%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 60.55%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 60.19%   [EVAL] batch:   65 | acc: 43.75%,  total acc: 59.94%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 59.05%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 58.73%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 58.70%   [EVAL] batch:   69 | acc: 62.50%,  total acc: 58.75%   [EVAL] batch:   70 | acc: 37.50%,  total acc: 58.45%   [EVAL] batch:   71 | acc: 18.75%,  total acc: 57.90%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 57.79%   [EVAL] batch:   73 | acc: 31.25%,  total acc: 57.43%   [EVAL] batch:   74 | acc: 18.75%,  total acc: 56.92%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 57.15%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 57.47%   [EVAL] batch:   77 | acc: 93.75%,  total acc: 57.93%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 58.07%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 58.52%   [EVAL] batch:   80 | acc: 12.50%,  total acc: 57.95%   [EVAL] batch:   81 | acc: 6.25%,  total acc: 57.32%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 56.63%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 56.03%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 55.37%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 55.31%   [EVAL] batch:   86 | acc: 75.00%,  total acc: 55.53%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 55.68%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 55.76%   [EVAL] batch:   89 | acc: 87.50%,  total acc: 56.11%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 56.39%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 56.59%   [EVAL] batch:   92 | acc: 87.50%,  total acc: 56.92%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 57.31%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 57.57%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 57.88%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 57.86%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 57.91%   [EVAL] batch:   98 | acc: 12.50%,  total acc: 57.45%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 57.06%   [EVAL] batch:  100 | acc: 25.00%,  total acc: 56.75%   [EVAL] batch:  101 | acc: 0.00%,  total acc: 56.19%   [EVAL] batch:  102 | acc: 6.25%,  total acc: 55.70%   [EVAL] batch:  103 | acc: 6.25%,  total acc: 55.23%   [EVAL] batch:  104 | acc: 0.00%,  total acc: 54.70%   [EVAL] batch:  105 | acc: 31.25%,  total acc: 54.48%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 54.26%   [EVAL] batch:  107 | acc: 43.75%,  total acc: 54.17%   [EVAL] batch:  108 | acc: 43.75%,  total acc: 54.07%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 53.92%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 53.94%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 54.02%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 54.37%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 54.61%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 55.00%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 55.23%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 55.29%   [EVAL] batch:  117 | acc: 12.50%,  total acc: 54.93%   [EVAL] batch:  118 | acc: 12.50%,  total acc: 54.57%   [EVAL] batch:  119 | acc: 18.75%,  total acc: 54.27%   [EVAL] batch:  120 | acc: 0.00%,  total acc: 53.82%   
cur_acc:  ['0.8636', '0.8576', '0.3828', '0.6696', '0.6750', '0.7500', '0.3778']
his_acc:  ['0.8636', '0.8750', '0.6974', '0.6426', '0.6162', '0.5814', '0.5382']
Clustering into  18  clusters
Clusters:  [ 2  4  5  2  0 11  2 15  7  1  0  2 14  5  6 16  6 17  2  5 12  2  3  9
  0  1 13  2  1  4  7  5  5  8 11  1  3 10 11  1  7]
Losses:  6.944992542266846 6.524724960327148 0.4202677309513092
CurrentTrain: epoch  0, batch     0 | loss: 6.9449925Losses:  3.13441801071167 2.670156717300415 0.46426135301589966
CurrentTrain: epoch  0, batch     1 | loss: 3.1344180Losses:  6.709178924560547 6.3246307373046875 0.38454821705818176
CurrentTrain: epoch  1, batch     0 | loss: 6.7091789Losses:  2.4570107460021973 2.1881866455078125 0.26882404088974
CurrentTrain: epoch  1, batch     1 | loss: 2.4570107Losses:  6.464971542358398 6.09293794631958 0.37203362584114075
CurrentTrain: epoch  2, batch     0 | loss: 6.4649715Losses:  1.949464201927185 1.5796796083450317 0.3697845935821533
CurrentTrain: epoch  2, batch     1 | loss: 1.9494642Losses:  6.9765191078186035 6.605648040771484 0.3708709180355072
CurrentTrain: epoch  3, batch     0 | loss: 6.9765191Losses:  2.846708297729492 2.543212890625 0.3034953474998474
CurrentTrain: epoch  3, batch     1 | loss: 2.8467083Losses:  6.7501606941223145 6.394903182983398 0.3552576005458832
CurrentTrain: epoch  4, batch     0 | loss: 6.7501607Losses:  3.010180950164795 2.7074666023254395 0.3027142286300659
CurrentTrain: epoch  4, batch     1 | loss: 3.0101810Losses:  5.950301170349121 5.602946758270264 0.3473546504974365
CurrentTrain: epoch  5, batch     0 | loss: 5.9503012Losses:  1.8130545616149902 1.4562103748321533 0.3568441569805145
CurrentTrain: epoch  5, batch     1 | loss: 1.8130546Losses:  5.377203941345215 5.033737659454346 0.34346649050712585
CurrentTrain: epoch  6, batch     0 | loss: 5.3772039Losses:  1.2093446254730225 0.8665557503700256 0.34278884530067444
CurrentTrain: epoch  6, batch     1 | loss: 1.2093446Losses:  7.478452682495117 7.137561798095703 0.34089067578315735
CurrentTrain: epoch  7, batch     0 | loss: 7.4784527Losses:  3.8502254486083984 3.588075876235962 0.2621495723724365
CurrentTrain: epoch  7, batch     1 | loss: 3.8502254Losses:  5.927286148071289 5.595959663391113 0.3313267230987549
CurrentTrain: epoch  8, batch     0 | loss: 5.9272861Losses:  2.446868419647217 2.1084675788879395 0.33840084075927734
CurrentTrain: epoch  8, batch     1 | loss: 2.4468684Losses:  5.028003215789795 4.701337814331055 0.3266652226448059
CurrentTrain: epoch  9, batch     0 | loss: 5.0280032Losses:  1.2323077917099 0.9051641821861267 0.3271436393260956
CurrentTrain: epoch  9, batch     1 | loss: 1.2323078
Losses:  0.826938807964325 -0.0 0.826938807964325
MemoryTrain:  epoch  0, batch     0 | loss: 0.8269388Losses:  1.0050852298736572 -0.0 1.0050852298736572
MemoryTrain:  epoch  0, batch     1 | loss: 1.0050852Losses:  0.7254815101623535 -0.0 0.7254815101623535
MemoryTrain:  epoch  0, batch     2 | loss: 0.7254815Losses:  0.9329380989074707 -0.0 0.9329380989074707
MemoryTrain:  epoch  1, batch     0 | loss: 0.9329381Losses:  0.8729456663131714 -0.0 0.8729456663131714
MemoryTrain:  epoch  1, batch     1 | loss: 0.8729457Losses:  0.6913273334503174 -0.0 0.6913273334503174
MemoryTrain:  epoch  1, batch     2 | loss: 0.6913273Losses:  0.8183040618896484 -0.0 0.8183040618896484
MemoryTrain:  epoch  2, batch     0 | loss: 0.8183041Losses:  0.7743568420410156 -0.0 0.7743568420410156
MemoryTrain:  epoch  2, batch     1 | loss: 0.7743568Losses:  0.6386810541152954 -0.0 0.6386810541152954
MemoryTrain:  epoch  2, batch     2 | loss: 0.6386811Losses:  0.8417660593986511 -0.0 0.8417660593986511
MemoryTrain:  epoch  3, batch     0 | loss: 0.8417661Losses:  0.6771656274795532 -0.0 0.6771656274795532
MemoryTrain:  epoch  3, batch     1 | loss: 0.6771656Losses:  0.6197330951690674 -0.0 0.6197330951690674
MemoryTrain:  epoch  3, batch     2 | loss: 0.6197331Losses:  0.8555036187171936 -0.0 0.8555036187171936
MemoryTrain:  epoch  4, batch     0 | loss: 0.8555036Losses:  0.8717584013938904 -0.0 0.8717584013938904
MemoryTrain:  epoch  4, batch     1 | loss: 0.8717584Losses:  0.6075493097305298 -0.0 0.6075493097305298
MemoryTrain:  epoch  4, batch     2 | loss: 0.6075493Losses:  0.9046582579612732 -0.0 0.9046582579612732
MemoryTrain:  epoch  5, batch     0 | loss: 0.9046583Losses:  0.7873188257217407 -0.0 0.7873188257217407
MemoryTrain:  epoch  5, batch     1 | loss: 0.7873188Losses:  0.5421146750450134 -0.0 0.5421146750450134
MemoryTrain:  epoch  5, batch     2 | loss: 0.5421147Losses:  0.871593177318573 -0.0 0.871593177318573
MemoryTrain:  epoch  6, batch     0 | loss: 0.8715932Losses:  0.8467813730239868 -0.0 0.8467813730239868
MemoryTrain:  epoch  6, batch     1 | loss: 0.8467814Losses:  0.6914364099502563 -0.0 0.6914364099502563
MemoryTrain:  epoch  6, batch     2 | loss: 0.6914364Losses:  0.7887063026428223 -0.0 0.7887063026428223
MemoryTrain:  epoch  7, batch     0 | loss: 0.7887063Losses:  0.9260221719741821 -0.0 0.9260221719741821
MemoryTrain:  epoch  7, batch     1 | loss: 0.9260222Losses:  0.7391184568405151 -0.0 0.7391184568405151
MemoryTrain:  epoch  7, batch     2 | loss: 0.7391185Losses:  0.8479273915290833 -0.0 0.8479273915290833
MemoryTrain:  epoch  8, batch     0 | loss: 0.8479274Losses:  0.8205398917198181 -0.0 0.8205398917198181
MemoryTrain:  epoch  8, batch     1 | loss: 0.8205399Losses:  0.6271792650222778 -0.0 0.6271792650222778
MemoryTrain:  epoch  8, batch     2 | loss: 0.6271793Losses:  0.7442520260810852 -0.0 0.7442520260810852
MemoryTrain:  epoch  9, batch     0 | loss: 0.7442520Losses:  0.8826861381530762 -0.0 0.8826861381530762
MemoryTrain:  epoch  9, batch     1 | loss: 0.8826861Losses:  0.7332561016082764 -0.0 0.7332561016082764
MemoryTrain:  epoch  9, batch     2 | loss: 0.7332561
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 53.12%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 67.71%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 64.58%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 63.12%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 61.93%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 59.38%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 55.77%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 47.92%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 45.31%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 50.00%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 46.88%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 50.89%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 54.69%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 58.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 61.36%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 63.02%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 59.82%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 60.83%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 60.16%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 60.66%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 60.76%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 60.86%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 62.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 63.99%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 67.12%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 68.49%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 70.91%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 71.06%   [EVAL] batch:   27 | acc: 81.25%,  total acc: 71.43%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 72.20%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 72.29%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 72.18%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 72.46%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 71.78%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 70.59%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 68.75%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 67.19%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 65.88%   [EVAL] batch:   37 | acc: 37.50%,  total acc: 65.13%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 64.26%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 64.69%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 63.87%   [EVAL] batch:   41 | acc: 0.00%,  total acc: 62.35%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 60.90%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 60.94%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 61.67%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 63.30%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 64.80%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 65.12%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 65.20%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 64.78%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 63.80%   [EVAL] batch:   53 | acc: 6.25%,  total acc: 62.73%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 61.59%   [EVAL] batch:   55 | acc: 6.25%,  total acc: 60.60%   [EVAL] batch:   56 | acc: 12.50%,  total acc: 59.76%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 59.38%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 58.90%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 59.06%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 58.91%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 58.47%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 58.13%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 58.69%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 58.37%   [EVAL] batch:   65 | acc: 25.00%,  total acc: 57.86%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 57.00%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 56.71%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 56.97%   [EVAL] batch:   69 | acc: 56.25%,  total acc: 56.96%   [EVAL] batch:   70 | acc: 37.50%,  total acc: 56.69%   [EVAL] batch:   71 | acc: 37.50%,  total acc: 56.42%   [EVAL] batch:   72 | acc: 62.50%,  total acc: 56.51%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 56.50%   [EVAL] batch:   74 | acc: 37.50%,  total acc: 56.25%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 56.66%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 57.22%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 57.77%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 58.07%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 58.44%   [EVAL] batch:   80 | acc: 12.50%,  total acc: 57.87%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 57.16%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 56.48%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 55.88%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 55.22%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 54.72%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 54.38%   [EVAL] batch:   87 | acc: 25.00%,  total acc: 54.05%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 53.44%   [EVAL] batch:   89 | acc: 6.25%,  total acc: 52.92%   [EVAL] batch:   90 | acc: 31.25%,  total acc: 52.68%   [EVAL] batch:   91 | acc: 31.25%,  total acc: 52.45%   [EVAL] batch:   92 | acc: 81.25%,  total acc: 52.76%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 53.19%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 53.36%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 53.71%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 53.74%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 53.83%   [EVAL] batch:   98 | acc: 25.00%,  total acc: 53.54%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 53.37%   [EVAL] batch:  100 | acc: 43.75%,  total acc: 53.28%   [EVAL] batch:  101 | acc: 6.25%,  total acc: 52.82%   [EVAL] batch:  102 | acc: 12.50%,  total acc: 52.43%   [EVAL] batch:  103 | acc: 18.75%,  total acc: 52.10%   [EVAL] batch:  104 | acc: 6.25%,  total acc: 51.67%   [EVAL] batch:  105 | acc: 37.50%,  total acc: 51.53%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 51.69%   [EVAL] batch:  107 | acc: 68.75%,  total acc: 51.85%   [EVAL] batch:  108 | acc: 68.75%,  total acc: 52.01%   [EVAL] batch:  109 | acc: 62.50%,  total acc: 52.10%   [EVAL] batch:  110 | acc: 81.25%,  total acc: 52.36%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 52.23%   [EVAL] batch:  112 | acc: 12.50%,  total acc: 51.88%   [EVAL] batch:  113 | acc: 18.75%,  total acc: 51.59%   [EVAL] batch:  114 | acc: 6.25%,  total acc: 51.20%   [EVAL] batch:  115 | acc: 18.75%,  total acc: 50.92%   [EVAL] batch:  116 | acc: 12.50%,  total acc: 50.59%   [EVAL] batch:  117 | acc: 37.50%,  total acc: 50.48%   [EVAL] batch:  118 | acc: 25.00%,  total acc: 50.26%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 50.26%   [EVAL] batch:  120 | acc: 37.50%,  total acc: 50.15%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 50.15%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 50.25%   [EVAL] batch:  123 | acc: 56.25%,  total acc: 50.30%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 50.70%   [EVAL] batch:  125 | acc: 93.75%,  total acc: 51.04%   [EVAL] batch:  126 | acc: 93.75%,  total acc: 51.38%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 51.37%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 51.26%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 51.20%   [EVAL] batch:  130 | acc: 56.25%,  total acc: 51.24%   [EVAL] batch:  131 | acc: 31.25%,  total acc: 51.09%   [EVAL] batch:  132 | acc: 12.50%,  total acc: 50.80%   
cur_acc:  ['0.8636', '0.8576', '0.3828', '0.6696', '0.6750', '0.7500', '0.3778', '0.5577']
his_acc:  ['0.8636', '0.8750', '0.6974', '0.6426', '0.6162', '0.5814', '0.5382', '0.5080']
----------END
his_acc mean:  [0.8658 0.7528 0.6382 0.6097 0.5673 0.566  0.5104 0.4911]
