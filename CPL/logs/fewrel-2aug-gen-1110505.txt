#############params############
cuda:0
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
CurrentTrain: epoch 15, batch     0 | loss: 21.4923599CurrentTrain: epoch 15, batch     1 | loss: 27.8141725CurrentTrain: epoch 15, batch     2 | loss: 22.7208211CurrentTrain: epoch 15, batch     3 | loss: 23.6630891CurrentTrain: epoch 15, batch     4 | loss: 23.8657355CurrentTrain: epoch 15, batch     5 | loss: 29.9012723CurrentTrain: epoch 15, batch     6 | loss: 33.4217641CurrentTrain: epoch 15, batch     7 | loss: 24.7915912CurrentTrain: epoch 15, batch     8 | loss: 18.0229866CurrentTrain: epoch 15, batch     9 | loss: 16.5160186CurrentTrain: epoch 15, batch    10 | loss: 15.5438218CurrentTrain: epoch 15, batch    11 | loss: 21.5772781CurrentTrain: epoch 15, batch    12 | loss: 20.4852350CurrentTrain: epoch 15, batch    13 | loss: 18.6908028CurrentTrain: epoch 15, batch    14 | loss: 19.6507548CurrentTrain: epoch 15, batch    15 | loss: 22.7019423CurrentTrain: epoch 15, batch    16 | loss: 33.8537808CurrentTrain: epoch 15, batch    17 | loss: 26.3478492CurrentTrain: epoch 15, batch    18 | loss: 15.1308014CurrentTrain: epoch 15, batch    19 | loss: 20.3217347CurrentTrain: epoch 15, batch    20 | loss: 22.9666435CurrentTrain: epoch 15, batch    21 | loss: 19.6703250CurrentTrain: epoch 15, batch    22 | loss: 22.4350433CurrentTrain: epoch 15, batch    23 | loss: 20.9290674CurrentTrain: epoch 15, batch    24 | loss: 19.3856198CurrentTrain: epoch 15, batch    25 | loss: 16.3025373CurrentTrain: epoch 15, batch    26 | loss: 21.6231511CurrentTrain: epoch 15, batch    27 | loss: 15.7144253CurrentTrain: epoch 15, batch    28 | loss: 17.4373657CurrentTrain: epoch 15, batch    29 | loss: 25.6066352CurrentTrain: epoch 15, batch    30 | loss: 25.7744819CurrentTrain: epoch 15, batch    31 | loss: 27.0765850CurrentTrain: epoch 15, batch    32 | loss: 15.9495988CurrentTrain: epoch 15, batch    33 | loss: 16.9035865CurrentTrain: epoch 15, batch    34 | loss: 25.1353050CurrentTrain: epoch 15, batch    35 | loss: 15.1701407CurrentTrain: epoch 15, batch    36 | loss: 15.3169238CurrentTrain: epoch 15, batch    37 | loss: 19.0653523CurrentTrain: epoch 15, batch    38 | loss: 18.8382737CurrentTrain: epoch 15, batch    39 | loss: 18.8810943CurrentTrain: epoch 15, batch    40 | loss: 23.0857771CurrentTrain: epoch 15, batch    41 | loss: 20.2676021CurrentTrain: epoch 15, batch    42 | loss: 17.8767131CurrentTrain: epoch 15, batch    43 | loss: 27.5236810CurrentTrain: epoch 15, batch    44 | loss: 16.6398621CurrentTrain: epoch 15, batch    45 | loss: 20.0258154CurrentTrain: epoch 15, batch    46 | loss: 12.0571297CurrentTrain: epoch 15, batch    47 | loss: 15.0252975CurrentTrain: epoch 15, batch    48 | loss: 13.3528550CurrentTrain: epoch 15, batch    49 | loss: 14.0507466CurrentTrain: epoch 15, batch    50 | loss: 15.1002125CurrentTrain: epoch 15, batch    51 | loss: 19.4380465CurrentTrain: epoch 15, batch    52 | loss: 14.4231271CurrentTrain: epoch 15, batch    53 | loss: 25.2441389CurrentTrain: epoch 15, batch    54 | loss: 12.7141878CurrentTrain: epoch 15, batch    55 | loss: 15.1041664CurrentTrain: epoch 15, batch    56 | loss: 17.0616809CurrentTrain: epoch 15, batch    57 | loss: 10.9651003CurrentTrain: epoch 15, batch    58 | loss: 12.1764395CurrentTrain: epoch 15, batch    59 | loss: 31.1135897CurrentTrain: epoch 15, batch    60 | loss: 16.3880207CurrentTrain: epoch 15, batch    61 | loss: 13.1661689CurrentTrain: epoch  7, batch    62 | loss: 19.8918980CurrentTrain: epoch 15, batch     0 | loss: 13.7245686CurrentTrain: epoch 15, batch     1 | loss: 21.8742130CurrentTrain: epoch 15, batch     2 | loss: 14.1646950CurrentTrain: epoch 15, batch     3 | loss: 12.1056341CurrentTrain: epoch 15, batch     4 | loss: 13.5456285CurrentTrain: epoch 15, batch     5 | loss: 12.7809260CurrentTrain: epoch 15, batch     6 | loss: 12.7365797CurrentTrain: epoch 15, batch     7 | loss: 13.5034916CurrentTrain: epoch 15, batch     8 | loss: 18.5605704CurrentTrain: epoch 15, batch     9 | loss: 14.8921467CurrentTrain: epoch 15, batch    10 | loss: 12.6412415CurrentTrain: epoch 15, batch    11 | loss: 13.1985217CurrentTrain: epoch 15, batch    12 | loss: 23.4475359CurrentTrain: epoch 15, batch    13 | loss: 15.0244400CurrentTrain: epoch 15, batch    14 | loss: 19.8997422CurrentTrain: epoch 15, batch    15 | loss: 11.8967719CurrentTrain: epoch 15, batch    16 | loss: 12.8393356CurrentTrain: epoch 15, batch    17 | loss: 19.8640584CurrentTrain: epoch 15, batch    18 | loss: 13.6901834CurrentTrain: epoch 15, batch    19 | loss: 19.2292738CurrentTrain: epoch 15, batch    20 | loss: 15.1826205CurrentTrain: epoch 15, batch    21 | loss: 16.3413352CurrentTrain: epoch 15, batch    22 | loss: 14.1326409CurrentTrain: epoch 15, batch    23 | loss: 14.6557446CurrentTrain: epoch 15, batch    24 | loss: 18.9186036CurrentTrain: epoch 15, batch    25 | loss: 12.7963986CurrentTrain: epoch 15, batch    26 | loss: 11.3125590CurrentTrain: epoch 15, batch    27 | loss: 15.7034859CurrentTrain: epoch 15, batch    28 | loss: 13.7542182CurrentTrain: epoch 15, batch    29 | loss: 13.7581583CurrentTrain: epoch 15, batch    30 | loss: 14.3438735CurrentTrain: epoch 15, batch    31 | loss: 18.0905114CurrentTrain: epoch 15, batch    32 | loss: 12.6852419CurrentTrain: epoch 15, batch    33 | loss: 18.6620506CurrentTrain: epoch 15, batch    34 | loss: 14.2756508CurrentTrain: epoch 15, batch    35 | loss: 15.8435155CurrentTrain: epoch 15, batch    36 | loss: 36.1022009CurrentTrain: epoch 15, batch    37 | loss: 15.1917951CurrentTrain: epoch 15, batch    38 | loss: 11.1886924CurrentTrain: epoch 15, batch    39 | loss: 10.7148004CurrentTrain: epoch 15, batch    40 | loss: 19.0516473CurrentTrain: epoch 15, batch    41 | loss: 12.0480269CurrentTrain: epoch 15, batch    42 | loss: 11.1087630CurrentTrain: epoch 15, batch    43 | loss: 9.7164629CurrentTrain: epoch 15, batch    44 | loss: 14.1105927CurrentTrain: epoch 15, batch    45 | loss: 10.9251925CurrentTrain: epoch 15, batch    46 | loss: 18.2012064CurrentTrain: epoch 15, batch    47 | loss: 16.3972510CurrentTrain: epoch 15, batch    48 | loss: 16.1965295CurrentTrain: epoch 15, batch    49 | loss: 11.5655355CurrentTrain: epoch 15, batch    50 | loss: 20.3112988CurrentTrain: epoch 15, batch    51 | loss: 16.8372011CurrentTrain: epoch 15, batch    52 | loss: 12.7648752CurrentTrain: epoch 15, batch    53 | loss: 18.2289991CurrentTrain: epoch 15, batch    54 | loss: 17.5117168CurrentTrain: epoch 15, batch    55 | loss: 12.1831596CurrentTrain: epoch 15, batch    56 | loss: 13.2611383CurrentTrain: epoch 15, batch    57 | loss: 10.6544262CurrentTrain: epoch 15, batch    58 | loss: 16.4988466CurrentTrain: epoch 15, batch    59 | loss: 15.4718608CurrentTrain: epoch 15, batch    60 | loss: 12.2224764CurrentTrain: epoch 15, batch    61 | loss: 15.5902713CurrentTrain: epoch  7, batch    62 | loss: 16.0350569CurrentTrain: epoch 15, batch     0 | loss: 16.0657554CurrentTrain: epoch 15, batch     1 | loss: 8.9455415CurrentTrain: epoch 15, batch     2 | loss: 19.1696067CurrentTrain: epoch 15, batch     3 | loss: 12.2867052CurrentTrain: epoch 15, batch     4 | loss: 13.2427762CurrentTrain: epoch 15, batch     5 | loss: 17.9976725CurrentTrain: epoch 15, batch     6 | loss: 28.7444942CurrentTrain: epoch 15, batch     7 | loss: 18.4137638CurrentTrain: epoch 15, batch     8 | loss: 11.4884069CurrentTrain: epoch 15, batch     9 | loss: 10.6982687CurrentTrain: epoch 15, batch    10 | loss: 9.0560086CurrentTrain: epoch 15, batch    11 | loss: 16.3723652CurrentTrain: epoch 15, batch    12 | loss: 14.0866345CurrentTrain: epoch 15, batch    13 | loss: 11.3787377CurrentTrain: epoch 15, batch    14 | loss: 16.8535984CurrentTrain: epoch 15, batch    15 | loss: 11.8632949CurrentTrain: epoch 15, batch    16 | loss: 16.5172941CurrentTrain: epoch 15, batch    17 | loss: 9.9234907CurrentTrain: epoch 15, batch    18 | loss: 9.6602499CurrentTrain: epoch 15, batch    19 | loss: 9.9730522CurrentTrain: epoch 15, batch    20 | loss: 9.5969402CurrentTrain: epoch 15, batch    21 | loss: 13.3361954CurrentTrain: epoch 15, batch    22 | loss: 20.7899096CurrentTrain: epoch 15, batch    23 | loss: 11.2388412CurrentTrain: epoch 15, batch    24 | loss: 14.7790303CurrentTrain: epoch 15, batch    25 | loss: 19.6807656CurrentTrain: epoch 15, batch    26 | loss: 11.0712596CurrentTrain: epoch 15, batch    27 | loss: 11.2228550CurrentTrain: epoch 15, batch    28 | loss: 9.9371370CurrentTrain: epoch 15, batch    29 | loss: 12.1135691CurrentTrain: epoch 15, batch    30 | loss: 13.5051110CurrentTrain: epoch 15, batch    31 | loss: 18.1399429CurrentTrain: epoch 15, batch    32 | loss: 13.0140370CurrentTrain: epoch 15, batch    33 | loss: 9.4963644CurrentTrain: epoch 15, batch    34 | loss: 12.2303562CurrentTrain: epoch 15, batch    35 | loss: 11.9545421CurrentTrain: epoch 15, batch    36 | loss: 16.3539486CurrentTrain: epoch 15, batch    37 | loss: 12.0415949CurrentTrain: epoch 15, batch    38 | loss: 12.4587554CurrentTrain: epoch 15, batch    39 | loss: 16.6757643CurrentTrain: epoch 15, batch    40 | loss: 15.3109428CurrentTrain: epoch 15, batch    41 | loss: 15.2977717CurrentTrain: epoch 15, batch    42 | loss: 11.9946461CurrentTrain: epoch 15, batch    43 | loss: 11.4447749CurrentTrain: epoch 15, batch    44 | loss: 13.9286559CurrentTrain: epoch 15, batch    45 | loss: 15.2805403CurrentTrain: epoch 15, batch    46 | loss: 12.4579200CurrentTrain: epoch 15, batch    47 | loss: 8.1003655CurrentTrain: epoch 15, batch    48 | loss: 8.7777078CurrentTrain: epoch 15, batch    49 | loss: 13.9078819CurrentTrain: epoch 15, batch    50 | loss: 15.6120722CurrentTrain: epoch 15, batch    51 | loss: 13.2607353CurrentTrain: epoch 15, batch    52 | loss: 13.0171006CurrentTrain: epoch 15, batch    53 | loss: 16.1171605CurrentTrain: epoch 15, batch    54 | loss: 15.2648791CurrentTrain: epoch 15, batch    55 | loss: 13.0488314CurrentTrain: epoch 15, batch    56 | loss: 11.4982472CurrentTrain: epoch 15, batch    57 | loss: 16.6099684CurrentTrain: epoch 15, batch    58 | loss: 15.1744539CurrentTrain: epoch 15, batch    59 | loss: 14.1322955CurrentTrain: epoch 15, batch    60 | loss: 8.9904719CurrentTrain: epoch 15, batch    61 | loss: 10.5082145CurrentTrain: epoch  7, batch    62 | loss: 15.2381908CurrentTrain: epoch 15, batch     0 | loss: 9.0150177CurrentTrain: epoch 15, batch     1 | loss: 13.8875925CurrentTrain: epoch 15, batch     2 | loss: 36.0360817CurrentTrain: epoch 15, batch     3 | loss: 17.4579701CurrentTrain: epoch 15, batch     4 | loss: 16.0510769CurrentTrain: epoch 15, batch     5 | loss: 16.6953660CurrentTrain: epoch 15, batch     6 | loss: 16.2178573CurrentTrain: epoch 15, batch     7 | loss: 8.3208013CurrentTrain: epoch 15, batch     8 | loss: 11.5697543CurrentTrain: epoch 15, batch     9 | loss: 10.3915469CurrentTrain: epoch 15, batch    10 | loss: 11.3219004CurrentTrain: epoch 15, batch    11 | loss: 11.5309602CurrentTrain: epoch 15, batch    12 | loss: 11.1452986CurrentTrain: epoch 15, batch    13 | loss: 14.2034448CurrentTrain: epoch 15, batch    14 | loss: 9.8868189CurrentTrain: epoch 15, batch    15 | loss: 8.6799417CurrentTrain: epoch 15, batch    16 | loss: 11.4241017CurrentTrain: epoch 15, batch    17 | loss: 10.7677968CurrentTrain: epoch 15, batch    18 | loss: 15.6196905CurrentTrain: epoch 15, batch    19 | loss: 10.6868388CurrentTrain: epoch 15, batch    20 | loss: 9.4843309CurrentTrain: epoch 15, batch    21 | loss: 13.3494565CurrentTrain: epoch 15, batch    22 | loss: 16.7313862CurrentTrain: epoch 15, batch    23 | loss: 10.6689208CurrentTrain: epoch 15, batch    24 | loss: 10.6098535CurrentTrain: epoch 15, batch    25 | loss: 16.9475209CurrentTrain: epoch 15, batch    26 | loss: 10.9483459CurrentTrain: epoch 15, batch    27 | loss: 10.7203912CurrentTrain: epoch 15, batch    28 | loss: 11.3259487CurrentTrain: epoch 15, batch    29 | loss: 12.1743668CurrentTrain: epoch 15, batch    30 | loss: 15.1481808CurrentTrain: epoch 15, batch    31 | loss: 18.1440182CurrentTrain: epoch 15, batch    32 | loss: 10.7549173CurrentTrain: epoch 15, batch    33 | loss: 11.1314613CurrentTrain: epoch 15, batch    34 | loss: 19.3107063CurrentTrain: epoch 15, batch    35 | loss: 11.9260537CurrentTrain: epoch 15, batch    36 | loss: 20.5062255CurrentTrain: epoch 15, batch    37 | loss: 33.9695468CurrentTrain: epoch 15, batch    38 | loss: 18.6420349CurrentTrain: epoch 15, batch    39 | loss: 11.6331871CurrentTrain: epoch 15, batch    40 | loss: 9.2250553CurrentTrain: epoch 15, batch    41 | loss: 17.7034633CurrentTrain: epoch 15, batch    42 | loss: 19.1011247CurrentTrain: epoch 15, batch    43 | loss: 12.5159761CurrentTrain: epoch 15, batch    44 | loss: 10.5235910CurrentTrain: epoch 15, batch    45 | loss: 10.5569166CurrentTrain: epoch 15, batch    46 | loss: 20.1080536CurrentTrain: epoch 15, batch    47 | loss: 8.7544609CurrentTrain: epoch 15, batch    48 | loss: 22.4255769CurrentTrain: epoch 15, batch    49 | loss: 13.7176021CurrentTrain: epoch 15, batch    50 | loss: 9.1174580CurrentTrain: epoch 15, batch    51 | loss: 11.1262095CurrentTrain: epoch 15, batch    52 | loss: 9.2399076CurrentTrain: epoch 15, batch    53 | loss: 21.2239454CurrentTrain: epoch 15, batch    54 | loss: 8.3822751CurrentTrain: epoch 15, batch    55 | loss: 9.6413212CurrentTrain: epoch 15, batch    56 | loss: 11.2049546CurrentTrain: epoch 15, batch    57 | loss: 11.2493732CurrentTrain: epoch 15, batch    58 | loss: 18.4466005CurrentTrain: epoch 15, batch    59 | loss: 8.5067304CurrentTrain: epoch 15, batch    60 | loss: 12.8959317CurrentTrain: epoch 15, batch    61 | loss: 9.1223704CurrentTrain: epoch  7, batch    62 | loss: 13.3678417CurrentTrain: epoch 15, batch     0 | loss: 12.2387929CurrentTrain: epoch 15, batch     1 | loss: 10.5267000CurrentTrain: epoch 15, batch     2 | loss: 11.6523971CurrentTrain: epoch 15, batch     3 | loss: 10.0613637CurrentTrain: epoch 15, batch     4 | loss: 13.0613610CurrentTrain: epoch 15, batch     5 | loss: 18.4087060CurrentTrain: epoch 15, batch     6 | loss: 12.9570789CurrentTrain: epoch 15, batch     7 | loss: 7.9450520CurrentTrain: epoch 15, batch     8 | loss: 11.6395955CurrentTrain: epoch 15, batch     9 | loss: 9.7396913CurrentTrain: epoch 15, batch    10 | loss: 10.6054232CurrentTrain: epoch 15, batch    11 | loss: 15.0280264CurrentTrain: epoch 15, batch    12 | loss: 11.0735673CurrentTrain: epoch 15, batch    13 | loss: 10.8975490CurrentTrain: epoch 15, batch    14 | loss: 9.8358127CurrentTrain: epoch 15, batch    15 | loss: 11.4626822CurrentTrain: epoch 15, batch    16 | loss: 8.8969805CurrentTrain: epoch 15, batch    17 | loss: 13.4586024CurrentTrain: epoch 15, batch    18 | loss: 9.2626596CurrentTrain: epoch 15, batch    19 | loss: 9.0775767CurrentTrain: epoch 15, batch    20 | loss: 9.5218324CurrentTrain: epoch 15, batch    21 | loss: 11.1706107CurrentTrain: epoch 15, batch    22 | loss: 8.5788793CurrentTrain: epoch 15, batch    23 | loss: 15.4882209CurrentTrain: epoch 15, batch    24 | loss: 10.4526444CurrentTrain: epoch 15, batch    25 | loss: 11.9221251CurrentTrain: epoch 15, batch    26 | loss: 10.4143800CurrentTrain: epoch 15, batch    27 | loss: 8.6333722CurrentTrain: epoch 15, batch    28 | loss: 11.9004701CurrentTrain: epoch 15, batch    29 | loss: 15.3006965CurrentTrain: epoch 15, batch    30 | loss: 22.0462661CurrentTrain: epoch 15, batch    31 | loss: 15.5637679CurrentTrain: epoch 15, batch    32 | loss: 9.5717513CurrentTrain: epoch 15, batch    33 | loss: 12.3946936CurrentTrain: epoch 15, batch    34 | loss: 10.8544514CurrentTrain: epoch 15, batch    35 | loss: 11.7603852CurrentTrain: epoch 15, batch    36 | loss: 9.1421077CurrentTrain: epoch 15, batch    37 | loss: 21.0697980CurrentTrain: epoch 15, batch    38 | loss: 13.5871846CurrentTrain: epoch 15, batch    39 | loss: 10.9919854CurrentTrain: epoch 15, batch    40 | loss: 9.5983950CurrentTrain: epoch 15, batch    41 | loss: 7.9101753CurrentTrain: epoch 15, batch    42 | loss: 12.2618684CurrentTrain: epoch 15, batch    43 | loss: 17.1699158CurrentTrain: epoch 15, batch    44 | loss: 9.6869492CurrentTrain: epoch 15, batch    45 | loss: 12.5230893CurrentTrain: epoch 15, batch    46 | loss: 8.0361038CurrentTrain: epoch 15, batch    47 | loss: 16.2606140CurrentTrain: epoch 15, batch    48 | loss: 11.5182046CurrentTrain: epoch 15, batch    49 | loss: 12.9156465CurrentTrain: epoch 15, batch    50 | loss: 12.0136651CurrentTrain: epoch 15, batch    51 | loss: 9.9040290CurrentTrain: epoch 15, batch    52 | loss: 11.0411317CurrentTrain: epoch 15, batch    53 | loss: 17.4990304CurrentTrain: epoch 15, batch    54 | loss: 12.9737841CurrentTrain: epoch 15, batch    55 | loss: 9.1459197CurrentTrain: epoch 15, batch    56 | loss: 11.4744175CurrentTrain: epoch 15, batch    57 | loss: 10.7532628CurrentTrain: epoch 15, batch    58 | loss: 10.4110770CurrentTrain: epoch 15, batch    59 | loss: 11.6524193CurrentTrain: epoch 15, batch    60 | loss: 13.1081037CurrentTrain: epoch 15, batch    61 | loss: 12.9937525CurrentTrain: epoch  7, batch    62 | loss: 8.0010455CurrentTrain: epoch 15, batch     0 | loss: 7.0826918CurrentTrain: epoch 15, batch     1 | loss: 9.0985305CurrentTrain: epoch 15, batch     2 | loss: 10.6567694CurrentTrain: epoch 15, batch     3 | loss: 11.2639328CurrentTrain: epoch 15, batch     4 | loss: 18.8805377CurrentTrain: epoch 15, batch     5 | loss: 13.6208670CurrentTrain: epoch 15, batch     6 | loss: 7.7206749CurrentTrain: epoch 15, batch     7 | loss: 12.0687154CurrentTrain: epoch 15, batch     8 | loss: 9.4887984CurrentTrain: epoch 15, batch     9 | loss: 8.1923616CurrentTrain: epoch 15, batch    10 | loss: 11.1337283CurrentTrain: epoch 15, batch    11 | loss: 16.7676054CurrentTrain: epoch 15, batch    12 | loss: 27.8437511CurrentTrain: epoch 15, batch    13 | loss: 12.9007384CurrentTrain: epoch 15, batch    14 | loss: 15.1660349CurrentTrain: epoch 15, batch    15 | loss: 16.3033765CurrentTrain: epoch 15, batch    16 | loss: 10.7432014CurrentTrain: epoch 15, batch    17 | loss: 17.0434950CurrentTrain: epoch 15, batch    18 | loss: 14.2044830CurrentTrain: epoch 15, batch    19 | loss: 10.5090970CurrentTrain: epoch 15, batch    20 | loss: 10.6019272CurrentTrain: epoch 15, batch    21 | loss: 16.8096353CurrentTrain: epoch 15, batch    22 | loss: 17.9170376CurrentTrain: epoch 15, batch    23 | loss: 12.8204235CurrentTrain: epoch 15, batch    24 | loss: 14.3894755CurrentTrain: epoch 15, batch    25 | loss: 21.9122363CurrentTrain: epoch 15, batch    26 | loss: 11.3995370CurrentTrain: epoch 15, batch    27 | loss: 8.2546943CurrentTrain: epoch 15, batch    28 | loss: 18.0198421CurrentTrain: epoch 15, batch    29 | loss: 6.9642209CurrentTrain: epoch 15, batch    30 | loss: 14.6936084CurrentTrain: epoch 15, batch    31 | loss: 17.2253618CurrentTrain: epoch 15, batch    32 | loss: 11.4610850CurrentTrain: epoch 15, batch    33 | loss: 11.6233915CurrentTrain: epoch 15, batch    34 | loss: 11.9010190CurrentTrain: epoch 15, batch    35 | loss: 10.5340665CurrentTrain: epoch 15, batch    36 | loss: 7.1184809CurrentTrain: epoch 15, batch    37 | loss: 13.3593116CurrentTrain: epoch 15, batch    38 | loss: 12.6489312CurrentTrain: epoch 15, batch    39 | loss: 10.6533224CurrentTrain: epoch 15, batch    40 | loss: 15.4160588CurrentTrain: epoch 15, batch    41 | loss: 12.0987952CurrentTrain: epoch 15, batch    42 | loss: 11.3386058CurrentTrain: epoch 15, batch    43 | loss: 9.1996521CurrentTrain: epoch 15, batch    44 | loss: 10.5193063CurrentTrain: epoch 15, batch    45 | loss: 8.4606247CurrentTrain: epoch 15, batch    46 | loss: 15.3015666CurrentTrain: epoch 15, batch    47 | loss: 9.3884433CurrentTrain: epoch 15, batch    48 | loss: 16.5347602CurrentTrain: epoch 15, batch    49 | loss: 8.9688007CurrentTrain: epoch 15, batch    50 | loss: 11.7599426CurrentTrain: epoch 15, batch    51 | loss: 10.2974064CurrentTrain: epoch 15, batch    52 | loss: 10.7971984CurrentTrain: epoch 15, batch    53 | loss: 11.3620207CurrentTrain: epoch 15, batch    54 | loss: 15.4101974CurrentTrain: epoch 15, batch    55 | loss: 15.1132682CurrentTrain: epoch 15, batch    56 | loss: 20.8954209CurrentTrain: epoch 15, batch    57 | loss: 13.4473109CurrentTrain: epoch 15, batch    58 | loss: 11.0750839CurrentTrain: epoch 15, batch    59 | loss: 11.1918958CurrentTrain: epoch 15, batch    60 | loss: 10.0818684CurrentTrain: epoch 15, batch    61 | loss: 9.5038316CurrentTrain: epoch  7, batch    62 | loss: 4.5863813CurrentTrain: epoch 15, batch     0 | loss: 10.8335717CurrentTrain: epoch 15, batch     1 | loss: 10.3095961CurrentTrain: epoch 15, batch     2 | loss: 24.5908643CurrentTrain: epoch 15, batch     3 | loss: 13.1922186CurrentTrain: epoch 15, batch     4 | loss: 19.9735774CurrentTrain: epoch 15, batch     5 | loss: 12.1087400CurrentTrain: epoch 15, batch     6 | loss: 18.7422557CurrentTrain: epoch 15, batch     7 | loss: 8.9663066CurrentTrain: epoch 15, batch     8 | loss: 11.2417626CurrentTrain: epoch 15, batch     9 | loss: 6.7501572CurrentTrain: epoch 15, batch    10 | loss: 8.9286264CurrentTrain: epoch 15, batch    11 | loss: 14.7640015CurrentTrain: epoch 15, batch    12 | loss: 8.3529554CurrentTrain: epoch 15, batch    13 | loss: 21.7084460CurrentTrain: epoch 15, batch    14 | loss: 9.1134906CurrentTrain: epoch 15, batch    15 | loss: 14.9190546CurrentTrain: epoch 15, batch    16 | loss: 25.0552466CurrentTrain: epoch 15, batch    17 | loss: 14.5241495CurrentTrain: epoch 15, batch    18 | loss: 14.4592837CurrentTrain: epoch 15, batch    19 | loss: 13.5663134CurrentTrain: epoch 15, batch    20 | loss: 10.9144583CurrentTrain: epoch 15, batch    21 | loss: 8.1100948CurrentTrain: epoch 15, batch    22 | loss: 14.2935893CurrentTrain: epoch 15, batch    23 | loss: 9.4099845CurrentTrain: epoch 15, batch    24 | loss: 10.0501960CurrentTrain: epoch 15, batch    25 | loss: 19.1210875CurrentTrain: epoch 15, batch    26 | loss: 11.0044471CurrentTrain: epoch 15, batch    27 | loss: 10.6947737CurrentTrain: epoch 15, batch    28 | loss: 10.9858610CurrentTrain: epoch 15, batch    29 | loss: 12.1759600CurrentTrain: epoch 15, batch    30 | loss: 12.0472417CurrentTrain: epoch 15, batch    31 | loss: 9.4164595CurrentTrain: epoch 15, batch    32 | loss: 9.6963155CurrentTrain: epoch 15, batch    33 | loss: 8.2312926CurrentTrain: epoch 15, batch    34 | loss: 15.0598601CurrentTrain: epoch 15, batch    35 | loss: 7.5467262CurrentTrain: epoch 15, batch    36 | loss: 7.7004600CurrentTrain: epoch 15, batch    37 | loss: 11.3566501CurrentTrain: epoch 15, batch    38 | loss: 8.7023410CurrentTrain: epoch 15, batch    39 | loss: 8.9204351CurrentTrain: epoch 15, batch    40 | loss: 13.7651825CurrentTrain: epoch 15, batch    41 | loss: 9.6282827CurrentTrain: epoch 15, batch    42 | loss: 11.8551333CurrentTrain: epoch 15, batch    43 | loss: 6.9865724CurrentTrain: epoch 15, batch    44 | loss: 13.0431577CurrentTrain: epoch 15, batch    45 | loss: 12.8500923CurrentTrain: epoch 15, batch    46 | loss: 8.7194039CurrentTrain: epoch 15, batch    47 | loss: 9.0785334CurrentTrain: epoch 15, batch    48 | loss: 8.8367540CurrentTrain: epoch 15, batch    49 | loss: 10.0356098CurrentTrain: epoch 15, batch    50 | loss: 6.4866332CurrentTrain: epoch 15, batch    51 | loss: 15.3534962CurrentTrain: epoch 15, batch    52 | loss: 8.4001416CurrentTrain: epoch 15, batch    53 | loss: 24.1752319CurrentTrain: epoch 15, batch    54 | loss: 10.3705192CurrentTrain: epoch 15, batch    55 | loss: 8.3095832CurrentTrain: epoch 15, batch    56 | loss: 18.7243164CurrentTrain: epoch 15, batch    57 | loss: 17.2492755CurrentTrain: epoch 15, batch    58 | loss: 7.5030080CurrentTrain: epoch 15, batch    59 | loss: 8.3337136CurrentTrain: epoch 15, batch    60 | loss: 11.9917114CurrentTrain: epoch 15, batch    61 | loss: 10.1881060CurrentTrain: epoch  7, batch    62 | loss: 10.6524606CurrentTrain: epoch 15, batch     0 | loss: 15.5728446CurrentTrain: epoch 15, batch     1 | loss: 12.8332949CurrentTrain: epoch 15, batch     2 | loss: 8.1444065CurrentTrain: epoch 15, batch     3 | loss: 11.2826278CurrentTrain: epoch 15, batch     4 | loss: 16.6601553CurrentTrain: epoch 15, batch     5 | loss: 11.6427577CurrentTrain: epoch 15, batch     6 | loss: 10.0168827CurrentTrain: epoch 15, batch     7 | loss: 7.4779572CurrentTrain: epoch 15, batch     8 | loss: 26.9943592CurrentTrain: epoch 15, batch     9 | loss: 17.3476743CurrentTrain: epoch 15, batch    10 | loss: 17.2359482CurrentTrain: epoch 15, batch    11 | loss: 7.1600244CurrentTrain: epoch 15, batch    12 | loss: 12.4993919CurrentTrain: epoch 15, batch    13 | loss: 9.0255393CurrentTrain: epoch 15, batch    14 | loss: 11.7277982CurrentTrain: epoch 15, batch    15 | loss: 8.3290939CurrentTrain: epoch 15, batch    16 | loss: 14.7720288CurrentTrain: epoch 15, batch    17 | loss: 14.4361370CurrentTrain: epoch 15, batch    18 | loss: 16.3081901CurrentTrain: epoch 15, batch    19 | loss: 11.8896588CurrentTrain: epoch 15, batch    20 | loss: 9.9419276CurrentTrain: epoch 15, batch    21 | loss: 15.1232868CurrentTrain: epoch 15, batch    22 | loss: 12.8551124CurrentTrain: epoch 15, batch    23 | loss: 13.0687326CurrentTrain: epoch 15, batch    24 | loss: 10.8939797CurrentTrain: epoch 15, batch    25 | loss: 9.4119937CurrentTrain: epoch 15, batch    26 | loss: 10.4106182CurrentTrain: epoch 15, batch    27 | loss: 11.6149823CurrentTrain: epoch 15, batch    28 | loss: 12.6968580CurrentTrain: epoch 15, batch    29 | loss: 17.1941360CurrentTrain: epoch 15, batch    30 | loss: 11.2749711CurrentTrain: epoch 15, batch    31 | loss: 8.9037612CurrentTrain: epoch 15, batch    32 | loss: 12.5696729CurrentTrain: epoch 15, batch    33 | loss: 9.3369295CurrentTrain: epoch 15, batch    34 | loss: 20.2274008CurrentTrain: epoch 15, batch    35 | loss: 8.6702281CurrentTrain: epoch 15, batch    36 | loss: 9.2892657CurrentTrain: epoch 15, batch    37 | loss: 11.9166294CurrentTrain: epoch 15, batch    38 | loss: 12.6490166CurrentTrain: epoch 15, batch    39 | loss: 9.5767936CurrentTrain: epoch 15, batch    40 | loss: 6.7754404CurrentTrain: epoch 15, batch    41 | loss: 16.2020545CurrentTrain: epoch 15, batch    42 | loss: 14.8373926CurrentTrain: epoch 15, batch    43 | loss: 10.9680477CurrentTrain: epoch 15, batch    44 | loss: 13.3875580CurrentTrain: epoch 15, batch    45 | loss: 12.2629754CurrentTrain: epoch 15, batch    46 | loss: 13.1874680CurrentTrain: epoch 15, batch    47 | loss: 8.0313438CurrentTrain: epoch 15, batch    48 | loss: 11.0695529CurrentTrain: epoch 15, batch    49 | loss: 7.1062508CurrentTrain: epoch 15, batch    50 | loss: 11.3356171CurrentTrain: epoch 15, batch    51 | loss: 14.6229099CurrentTrain: epoch 15, batch    52 | loss: 7.8104063CurrentTrain: epoch 15, batch    53 | loss: 13.6223753CurrentTrain: epoch 15, batch    54 | loss: 13.5676174CurrentTrain: epoch 15, batch    55 | loss: 10.0969497CurrentTrain: epoch 15, batch    56 | loss: 9.4776880CurrentTrain: epoch 15, batch    57 | loss: 15.2148960CurrentTrain: epoch 15, batch    58 | loss: 11.9836128CurrentTrain: epoch 15, batch    59 | loss: 7.1612419CurrentTrain: epoch 15, batch    60 | loss: 10.8921679CurrentTrain: epoch 15, batch    61 | loss: 7.1707702CurrentTrain: epoch  7, batch    62 | loss: 11.6803592CurrentTrain: epoch 15, batch     0 | loss: 8.1930147CurrentTrain: epoch 15, batch     1 | loss: 11.4749638CurrentTrain: epoch 15, batch     2 | loss: 19.1028790CurrentTrain: epoch 15, batch     3 | loss: 6.5360797CurrentTrain: epoch 15, batch     4 | loss: 10.8441475CurrentTrain: epoch 15, batch     5 | loss: 11.6178085CurrentTrain: epoch 15, batch     6 | loss: 6.0425258CurrentTrain: epoch 15, batch     7 | loss: 9.2059731CurrentTrain: epoch 15, batch     8 | loss: 16.2934711CurrentTrain: epoch 15, batch     9 | loss: 13.1514744CurrentTrain: epoch 15, batch    10 | loss: 9.0820481CurrentTrain: epoch 15, batch    11 | loss: 18.9000302CurrentTrain: epoch 15, batch    12 | loss: 14.4744257CurrentTrain: epoch 15, batch    13 | loss: 9.1764280CurrentTrain: epoch 15, batch    14 | loss: 18.5675457CurrentTrain: epoch 15, batch    15 | loss: 8.6032335CurrentTrain: epoch 15, batch    16 | loss: 11.1036367CurrentTrain: epoch 15, batch    17 | loss: 11.0302625CurrentTrain: epoch 15, batch    18 | loss: 7.6264590CurrentTrain: epoch 15, batch    19 | loss: 10.0775167CurrentTrain: epoch 15, batch    20 | loss: 19.4731456CurrentTrain: epoch 15, batch    21 | loss: 10.1132506CurrentTrain: epoch 15, batch    22 | loss: 16.4074103CurrentTrain: epoch 15, batch    23 | loss: 11.7423269CurrentTrain: epoch 15, batch    24 | loss: 15.3383540CurrentTrain: epoch 15, batch    25 | loss: 10.1148714CurrentTrain: epoch 15, batch    26 | loss: 10.9380932CurrentTrain: epoch 15, batch    27 | loss: 11.0327941CurrentTrain: epoch 15, batch    28 | loss: 8.7550441CurrentTrain: epoch 15, batch    29 | loss: 14.0343169CurrentTrain: epoch 15, batch    30 | loss: 9.4976944CurrentTrain: epoch 15, batch    31 | loss: 10.3391436CurrentTrain: epoch 15, batch    32 | loss: 12.0357061CurrentTrain: epoch 15, batch    33 | loss: 6.8577283CurrentTrain: epoch 15, batch    34 | loss: 14.9629276CurrentTrain: epoch 15, batch    35 | loss: 13.8629310CurrentTrain: epoch 15, batch    36 | loss: 13.6852790CurrentTrain: epoch 15, batch    37 | loss: 8.7582060CurrentTrain: epoch 15, batch    38 | loss: 8.0809621CurrentTrain: epoch 15, batch    39 | loss: 21.9344503CurrentTrain: epoch 15, batch    40 | loss: 12.0308884CurrentTrain: epoch 15, batch    41 | loss: 8.5825889CurrentTrain: epoch 15, batch    42 | loss: 15.6101643CurrentTrain: epoch 15, batch    43 | loss: 8.1771755CurrentTrain: epoch 15, batch    44 | loss: 17.2507012CurrentTrain: epoch 15, batch    45 | loss: 7.9912066CurrentTrain: epoch 15, batch    46 | loss: 10.4154469CurrentTrain: epoch 15, batch    47 | loss: 15.7316902CurrentTrain: epoch 15, batch    48 | loss: 14.1245367CurrentTrain: epoch 15, batch    49 | loss: 7.4729887CurrentTrain: epoch 15, batch    50 | loss: 7.2780548CurrentTrain: epoch 15, batch    51 | loss: 10.0283126CurrentTrain: epoch 15, batch    52 | loss: 15.5837862CurrentTrain: epoch 15, batch    53 | loss: 20.6476678CurrentTrain: epoch 15, batch    54 | loss: 13.8170238CurrentTrain: epoch 15, batch    55 | loss: 9.5802440CurrentTrain: epoch 15, batch    56 | loss: 6.8186884CurrentTrain: epoch 15, batch    57 | loss: 15.0268212CurrentTrain: epoch 15, batch    58 | loss: 8.5734275CurrentTrain: epoch 15, batch    59 | loss: 9.3284390CurrentTrain: epoch 15, batch    60 | loss: 8.4647254CurrentTrain: epoch 15, batch    61 | loss: 11.2955384CurrentTrain: epoch  7, batch    62 | loss: 23.2623348CurrentTrain: epoch 15, batch     0 | loss: 10.5858149CurrentTrain: epoch 15, batch     1 | loss: 10.3318269CurrentTrain: epoch 15, batch     2 | loss: 10.2479946CurrentTrain: epoch 15, batch     3 | loss: 10.2376131CurrentTrain: epoch 15, batch     4 | loss: 8.6640556CurrentTrain: epoch 15, batch     5 | loss: 10.0317263CurrentTrain: epoch 15, batch     6 | loss: 15.5878219CurrentTrain: epoch 15, batch     7 | loss: 14.6477402CurrentTrain: epoch 15, batch     8 | loss: 8.4295586CurrentTrain: epoch 15, batch     9 | loss: 7.2047380CurrentTrain: epoch 15, batch    10 | loss: 11.8592529CurrentTrain: epoch 15, batch    11 | loss: 10.4533070CurrentTrain: epoch 15, batch    12 | loss: 7.6862167CurrentTrain: epoch 15, batch    13 | loss: 9.3593488CurrentTrain: epoch 15, batch    14 | loss: 9.6258851CurrentTrain: epoch 15, batch    15 | loss: 8.7869438CurrentTrain: epoch 15, batch    16 | loss: 13.7395494CurrentTrain: epoch 15, batch    17 | loss: 8.3761595CurrentTrain: epoch 15, batch    18 | loss: 6.9476330CurrentTrain: epoch 15, batch    19 | loss: 13.1864442CurrentTrain: epoch 15, batch    20 | loss: 17.1866558CurrentTrain: epoch 15, batch    21 | loss: 10.4547452CurrentTrain: epoch 15, batch    22 | loss: 14.9420433CurrentTrain: epoch 15, batch    23 | loss: 6.0871123CurrentTrain: epoch 15, batch    24 | loss: 11.7006181CurrentTrain: epoch 15, batch    25 | loss: 9.0848825CurrentTrain: epoch 15, batch    26 | loss: 9.6485029CurrentTrain: epoch 15, batch    27 | loss: 16.6558121CurrentTrain: epoch 15, batch    28 | loss: 30.8304135CurrentTrain: epoch 15, batch    29 | loss: 8.1588511CurrentTrain: epoch 15, batch    30 | loss: 8.9629336CurrentTrain: epoch 15, batch    31 | loss: 12.9104638CurrentTrain: epoch 15, batch    32 | loss: 11.0285142CurrentTrain: epoch 15, batch    33 | loss: 6.4248376CurrentTrain: epoch 15, batch    34 | loss: 8.4515181CurrentTrain: epoch 15, batch    35 | loss: 9.2651423CurrentTrain: epoch 15, batch    36 | loss: 10.1831280CurrentTrain: epoch 15, batch    37 | loss: 11.0271277CurrentTrain: epoch 15, batch    38 | loss: 8.3644365CurrentTrain: epoch 15, batch    39 | loss: 16.7456035CurrentTrain: epoch 15, batch    40 | loss: 32.4118775CurrentTrain: epoch 15, batch    41 | loss: 9.3822908CurrentTrain: epoch 15, batch    42 | loss: 13.0189598CurrentTrain: epoch 15, batch    43 | loss: 7.1095471CurrentTrain: epoch 15, batch    44 | loss: 14.8665140CurrentTrain: epoch 15, batch    45 | loss: 18.9933799CurrentTrain: epoch 15, batch    46 | loss: 9.2171050CurrentTrain: epoch 15, batch    47 | loss: 7.0458069CurrentTrain: epoch 15, batch    48 | loss: 9.6384538CurrentTrain: epoch 15, batch    49 | loss: 10.2057671CurrentTrain: epoch 15, batch    50 | loss: 13.3234219CurrentTrain: epoch 15, batch    51 | loss: 8.3891888CurrentTrain: epoch 15, batch    52 | loss: 14.9175906CurrentTrain: epoch 15, batch    53 | loss: 20.3165154CurrentTrain: epoch 15, batch    54 | loss: 10.8980108CurrentTrain: epoch 15, batch    55 | loss: 10.8086347CurrentTrain: epoch 15, batch    56 | loss: 15.8999412CurrentTrain: epoch 15, batch    57 | loss: 8.9245969CurrentTrain: epoch 15, batch    58 | loss: 14.3118339CurrentTrain: epoch 15, batch    59 | loss: 10.7397672CurrentTrain: epoch 15, batch    60 | loss: 11.0807088CurrentTrain: epoch 15, batch    61 | loss: 10.7155057CurrentTrain: epoch  7, batch    62 | loss: 7.5457087
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: richard mudge ( born 1718 in bideford ; died april 1763 in bedworth ) was an english clergyman and composer of the late baroque period .
Head Entity: richard mudge
Tail Entity: baroque
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: the impressionist movement, which emerged in the late 19th century, was characterized by a focus on capturing light and natural forms, and it included notable artists such as claude monet and edgar degas.  
Head Entity: impressionist movement  
Tail Entity: impressionism  

Relation: movement  
Context: the existentialist movement gained prominence in the 20th century, with philosophers like jean-paul sartre and simone de beauvoir exploring themes of freedom, choice, and the absurd.  
Head Entity: existentialist movement  
Tail Entity: existentialism  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: port alberta is a joint venture between the edmonton economic development corporation ( eedc ) and edmonton international airport ( eia ) based in the edmonton capital region in alberta , canada .
Head Entity: edmonton capital region
Tail Entity: alberta
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: the city of san francisco is known for its iconic golden gate bridge and is part of the larger san francisco bay area in california, united states.  
Head Entity: san francisco bay area  
Tail Entity: california  

Relation: located in the administrative territorial entity  
Context: the university of toronto is one of the leading educational institutions in canada and is situated in the city of toronto, which is part of the province of ontario.  
Head Entity: city of toronto  
Tail Entity: ontario  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the nation as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: hank henshaw became a member of the sinestro corps during the sinestro corps war .
Head Entity: hank henshaw
Tail Entity: sinestro corps
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: after years of dedication, she was proud to be a member of the national art society.  
Head Entity: she  
Tail Entity: national art society  

Relation: member of  
Context: the young musician was thrilled to announce that he is now a member of the local jazz band.  
Head Entity: the young musician  
Tail Entity: local jazz band  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: the bosideng bridge ( ) is a highway bridge over the yangtze river in hejiang county , sichuan , china .
Head Entity: bosideng bridge
Tail Entity: yangtze river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: the golden gate bridge ( ) spans the golden gate strait, connecting san francisco to marin county in california.  
Head Entity: golden gate bridge  
Tail Entity: golden gate strait  

Relation: crosses  
Context: the channel tunnel ( ) runs beneath the english channel, linking the united kingdom with mainland europe.  
Head Entity: channel tunnel  
Tail Entity: english channel  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: it is on the panj river just downstream from the mouth of the bartang river at the point where the panj briefly turns west before resuming its northerly course .
Head Entity: panj river
Tail Entity: bartang river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The Missouri River, which is a major tributary of the Mississippi River, flows through several states before joining the larger watercourse.  
Head Entity: Missouri River  
Tail Entity: Mississippi River  

Relation: tributary  
Context: The Rio Grande serves as a natural border between the United States and Mexico, and it has several tributaries, including the Pecos River.  
Head Entity: Rio Grande  
Tail Entity: Pecos River  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: on 3 july 1937 , the count married rocafort - atuzarrra in a lavish wedding in havana , cuba attended by the president of cuba , federico laredo brú .
Head Entity: federico laredo brú
Tail Entity: president of cuba
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: After serving as the mayor of New York City for eight years, Bill de Blasio announced his candidacy for governor of New York in 2022.  
Head Entity: Bill de Blasio  
Tail Entity: mayor of New York City  

Relation: position held  
Context: In 2015, Angela Merkel was recognized for her leadership as the Chancellor of Germany during a critical period in European politics.  
Head Entity: Angela Merkel  
Tail Entity: Chancellor of Germany  
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: a headcrab is a fictional alien parasitoid found in the " half - life " video game series created by valve software .
Head Entity: half - life
Tail Entity: valve software
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: The popular game "The Witcher 3: Wild Hunt" was developed by CD Projekt Red, a well-known Polish video game developer.  
Head Entity: The Witcher 3: Wild Hunt  
Tail Entity: CD Projekt Red  

Relation: developer  
Context: The innovative mobile game "Angry Birds" was created by Rovio Entertainment, a Finnish company that specializes in game development.  
Head Entity: Angry Birds  
Tail Entity: Rovio Entertainment  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: dean reis , meanwhile , performs with the heartaches , while rubalcaba plays in earthless and is part - owner of the independent record store thirsty moon records in the hillcrest area of san diego .
Head Entity: earthless
Tail Entity: san diego
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the tech startup was founded in a small garage in palo alto, where a group of innovative engineers came together to create groundbreaking software.  
Head Entity: tech startup  
Tail Entity: palo alto  

Relation: location of formation  
Context: the famous rock band originated in a vibrant music scene in nashville, where they played their first gigs and built a loyal fanbase.  
Head Entity: rock band  
Tail Entity: nashville  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: during the same month , the band supported kings of leon 's tour of the united states , and coldplay 's tour of the united kingdom .
Head Entity: kings of leon
Tail Entity: united states
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous Italian dish, pizza, has gained popularity worldwide, but its roots can be traced back to Naples, Italy.  
Head Entity: pizza  
Tail Entity: Italy  

Relation: country of origin  
Context: The iconic brand, Rolex, is renowned for its luxury watches, which are crafted in Switzerland, known for its precision and quality in watchmaking.  
Head Entity: Rolex  
Tail Entity: Switzerland  
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 93.06%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 93.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 92.61%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 93.23%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 92.79%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 94.08%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 94.03%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.29%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.47%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 94.68%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.61%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.96%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.40%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.61%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 95.72%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.94%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 96.04%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 96.13%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 96.22%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.31%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 96.39%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.47%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.41%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 96.35%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.43%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.45%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 96.39%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 96.34%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.41%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 96.14%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 96.09%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 96.05%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 96.12%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 96.19%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 96.15%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 96.21%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 96.17%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.44%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 93.06%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 93.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 92.61%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 93.23%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 92.79%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 94.08%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 94.03%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.29%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.47%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 94.68%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.61%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.96%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.40%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.61%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 95.72%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.94%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 96.04%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 96.13%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 96.22%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.31%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 96.39%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.47%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.41%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 96.35%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.43%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.45%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 96.39%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 96.34%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.41%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 96.14%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 96.09%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 96.05%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 96.12%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 96.19%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 96.15%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 96.21%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 96.17%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.44%   
cur_acc:  ['0.9544']
his_acc:  ['0.9544']
CurrentTrain: epoch 15, batch     0 | loss: 15.1844393CurrentTrain: epoch 15, batch     1 | loss: 15.4169671CurrentTrain: epoch 15, batch     2 | loss: 17.2606502CurrentTrain: epoch  1, batch     3 | loss: 9.3947593CurrentTrain: epoch 15, batch     0 | loss: 15.2059101CurrentTrain: epoch 15, batch     1 | loss: 9.9336612CurrentTrain: epoch 15, batch     2 | loss: 11.6501838CurrentTrain: epoch  1, batch     3 | loss: 8.1364556CurrentTrain: epoch 15, batch     0 | loss: 16.0520402CurrentTrain: epoch 15, batch     1 | loss: 12.4519198CurrentTrain: epoch 15, batch     2 | loss: 11.8768194CurrentTrain: epoch  1, batch     3 | loss: 8.2660386CurrentTrain: epoch 15, batch     0 | loss: 10.8775975CurrentTrain: epoch 15, batch     1 | loss: 10.9203141CurrentTrain: epoch 15, batch     2 | loss: 19.3109496CurrentTrain: epoch  1, batch     3 | loss: 6.8853363CurrentTrain: epoch 15, batch     0 | loss: 7.9381931CurrentTrain: epoch 15, batch     1 | loss: 14.5452630CurrentTrain: epoch 15, batch     2 | loss: 10.7757114CurrentTrain: epoch  1, batch     3 | loss: 8.2963893CurrentTrain: epoch 15, batch     0 | loss: 11.2638834CurrentTrain: epoch 15, batch     1 | loss: 7.8237821CurrentTrain: epoch 15, batch     2 | loss: 15.1044016CurrentTrain: epoch  1, batch     3 | loss: 7.6117847CurrentTrain: epoch 15, batch     0 | loss: 7.9856933CurrentTrain: epoch 15, batch     1 | loss: 10.7627013CurrentTrain: epoch 15, batch     2 | loss: 9.5379552CurrentTrain: epoch  1, batch     3 | loss: 13.2576603CurrentTrain: epoch 15, batch     0 | loss: 9.2783236CurrentTrain: epoch 15, batch     1 | loss: 8.4136784CurrentTrain: epoch 15, batch     2 | loss: 6.8572798CurrentTrain: epoch  1, batch     3 | loss: 6.7091550CurrentTrain: epoch 15, batch     0 | loss: 5.7775895CurrentTrain: epoch 15, batch     1 | loss: 20.2082989CurrentTrain: epoch 15, batch     2 | loss: 6.9182961CurrentTrain: epoch  1, batch     3 | loss: 6.1473978CurrentTrain: epoch 15, batch     0 | loss: 4.6594964CurrentTrain: epoch 15, batch     1 | loss: 8.7733540CurrentTrain: epoch 15, batch     2 | loss: 8.2816182CurrentTrain: epoch  1, batch     3 | loss: 6.0313827
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: rohan is the son of narayana murthy , founder of infosys , and his wife sudha murthy , an engineer and author of note .
Head Entity: narayana murthy
Tail Entity: sudha murthy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: elon musk, the CEO of spacex and tesla, was previously married to talulah riley, a talented actress and writer.  
Head Entity: elon musk  
Tail Entity: talulah riley  

Relation: spouse  
Context: barack obama, the 44th president of the united states, has been married to michelle obama since 1992, and they have two daughters together.  
Head Entity: barack obama  
Tail Entity: michelle obama  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: they briefly reformed in 1992 , when sub pop re - released " primal rock therapy " on cd with five additional unreleased tracks .
Head Entity: primal rock therapy
Tail Entity: sub pop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: The album "Future Nostalgia" was released under the Warner Records label, showcasing Dua Lipa's unique sound and style.  
Head Entity: Future Nostalgia  
Tail Entity: Warner Records  

Relation: record label  
Context: After signing with Atlantic Records, the band released their highly anticipated debut album, which quickly climbed the charts.  
Head Entity: debut album  
Tail Entity: Atlantic Records  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: distributed by paramount pictures , " the italian job " was theatrically released in the united states on may 30 , 2003 , and grossed over $ 176 million worldwide .
Head Entity: the italian job
Tail Entity: paramount pictures
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: the film was distributed by universal pictures and received critical acclaim upon its release in 2015.  
Head Entity: the film  
Tail Entity: universal pictures  

Relation: distributor  
Context: the album was distributed by sony music and quickly climbed the charts, becoming a bestseller.  
Head Entity: the album  
Tail Entity: sony music  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: it then crossed the indian ocean , passing st. pierre island , providence atoll and farquhar atoll of seychelles before making landfall in southeastern australia .
Head Entity: farquhar atoll
Tail Entity: indian ocean
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: The city of Venice is famous for its canals, which are situated right next to the Adriatic Sea, providing a unique maritime experience.  
Head Entity: Venice  
Tail Entity: Adriatic Sea  

Relation: located in or next to body of water  
Context: The village of Hvar is nestled on the shores of the sparkling blue waters of the Adriatic, making it a popular destination for tourists.  
Head Entity: Hvar  
Tail Entity: Adriatic
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: following the sale of amstrad plc to rupert murdoch 's bskyb , viglen is now lord sugar 's sole it establishment .
Head Entity: bskyb
Tail Entity: amstrad
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Relation: subsidiary  
Context: In 2018, Facebook acquired WhatsApp, making it a subsidiary of the social media giant.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Relation: subsidiary  
Context: After the merger, the new company announced that its subsidiary, XYZ Corp, would continue to operate independently.  
Head Entity: new company  
Tail Entity: XYZ Corp  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: he challenged seven other swimmers in heat four including ukraine 's rostyslav svanidze , a top 16 finalist in atlanta four years earlier .
Head Entity: rostyslav svanidze
Tail Entity: ukraine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: after years of living in the united states, the renowned artist decided to apply for dual citizenship, allowing him to maintain his ties to his homeland, italy.  
Head Entity: the renowned artist  
Tail Entity: italy  

Relation: country of citizenship  
Context: during the international conference, the delegate from canada proudly spoke about the rights and privileges of being a citizen of his country, emphasizing the importance of citizenship in shaping national identity.  
Head Entity: the delegate from canada  
Tail Entity: canada  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: nina cites " duffy , muse , and other pop and alternative artists " as her major influences .
Head Entity: duffy
Tail Entity: pop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the film "inception" is often categorized as a science fiction thriller that explores complex themes of dreams and reality.  
Head Entity: inception  
Tail Entity: science fiction  

Relation: genre  
Context: the band "coldplay" is known for their melodic rock sound, blending elements of pop and alternative music in their albums.  
Head Entity: coldplay  
Tail Entity: melodic rock  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: san lázaro is 660 km far from asunción and is located at the exact site of the confluence of the apa river with paraguay river , bordering brazil - chaco .
Head Entity: apa river
Tail Entity: paraguay river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: the mississippi river flows southward through the united states and empties into the gulf of mexico, making it one of the largest river systems in north america.  
Head Entity: mississippi river  
Tail Entity: gulf of mexico  

Relation: mouth of the watercourse  
Context: the rhine river, which originates in the swiss alps, eventually reaches the north sea, serving as a vital waterway for trade in europe.  
Head Entity: rhine river  
Tail Entity: north sea  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: bradley halliday ( born 10 july 1995 ) is an english professional footballer who plays as a defender or a midfielder for league two club cambridge united .
Head Entity: bradley halliday
Tail Entity: defender
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: alex morgan ( born 2 july 1989 ) is an american soccer player who plays as a forward for the national team and club orlando pride in the national women's soccer league.  
Head Entity: alex morgan  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: lebron james ( born 30 december 1984 ) is an american professional basketball player who plays as a small forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: small forward  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: the stade louis ii in monaco was the venue for the uefa super cup every year since 1998 .
Head Entity: 1998
Tail Entity: uefa super cup
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2020 NBA season was significantly impacted by the COVID-19 pandemic, leading to a unique playoff format.  
Head Entity: 2020  
Tail Entity: NBA  

Relation: sports season of league or competition  
Context: The 2019 Rugby World Cup took place in Japan, showcasing teams from around the globe.  
Head Entity: 2019  
Tail Entity: Rugby World Cup  
MemoryTrain:  epoch 15, batch     0 | loss: 8.0548777MemoryTrain:  epoch 15, batch     1 | loss: 7.5053369MemoryTrain:  epoch 15, batch     2 | loss: 7.6069060MemoryTrain:  epoch 11, batch     3 | loss: 8.0242429MemoryTrain:  epoch 15, batch     0 | loss: 6.1344457MemoryTrain:  epoch 15, batch     1 | loss: 7.5438967MemoryTrain:  epoch 15, batch     2 | loss: 5.0483785MemoryTrain:  epoch 11, batch     3 | loss: 8.1365606MemoryTrain:  epoch 15, batch     0 | loss: 5.5829647MemoryTrain:  epoch 15, batch     1 | loss: 4.7935522MemoryTrain:  epoch 15, batch     2 | loss: 6.9211824MemoryTrain:  epoch 11, batch     3 | loss: 4.4819679MemoryTrain:  epoch 15, batch     0 | loss: 6.2299031MemoryTrain:  epoch 15, batch     1 | loss: 9.9134228MemoryTrain:  epoch 15, batch     2 | loss: 6.2386769MemoryTrain:  epoch 11, batch     3 | loss: 12.0247953MemoryTrain:  epoch 15, batch     0 | loss: 4.7391058MemoryTrain:  epoch 15, batch     1 | loss: 4.6464219MemoryTrain:  epoch 15, batch     2 | loss: 5.0923168MemoryTrain:  epoch 11, batch     3 | loss: 7.6728561MemoryTrain:  epoch 15, batch     0 | loss: 3.8761913MemoryTrain:  epoch 15, batch     1 | loss: 7.5593758MemoryTrain:  epoch 15, batch     2 | loss: 6.4568131MemoryTrain:  epoch 11, batch     3 | loss: 2.2301878MemoryTrain:  epoch 15, batch     0 | loss: 6.8420577MemoryTrain:  epoch 15, batch     1 | loss: 3.3018876MemoryTrain:  epoch 15, batch     2 | loss: 5.2966851MemoryTrain:  epoch 11, batch     3 | loss: 3.4010892MemoryTrain:  epoch 15, batch     0 | loss: 2.9781175MemoryTrain:  epoch 15, batch     1 | loss: 3.3256290MemoryTrain:  epoch 15, batch     2 | loss: 4.6221405MemoryTrain:  epoch 11, batch     3 | loss: 4.8348418MemoryTrain:  epoch 15, batch     0 | loss: 3.7224511MemoryTrain:  epoch 15, batch     1 | loss: 10.8363286MemoryTrain:  epoch 15, batch     2 | loss: 2.9446708MemoryTrain:  epoch 11, batch     3 | loss: 1.9191874MemoryTrain:  epoch 15, batch     0 | loss: 3.6337805MemoryTrain:  epoch 15, batch     1 | loss: 6.5135076MemoryTrain:  epoch 15, batch     2 | loss: 4.8296371MemoryTrain:  epoch 11, batch     3 | loss: 3.4234280
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 97.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 97.32%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 96.09%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 95.14%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 89.58%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 88.24%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 87.85%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 88.49%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 88.10%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 88.35%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 88.59%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 88.50%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 88.46%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 87.96%   [EVAL] batch:   27 | acc: 81.25%,  total acc: 87.72%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 87.72%   [EVAL] batch:   29 | acc: 56.25%,  total acc: 86.67%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 86.29%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 85.55%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 85.04%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 84.38%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 83.57%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 82.81%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 82.26%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 81.58%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 80.93%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 80.78%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 80.03%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 79.46%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 79.22%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 78.98%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 77.78%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 76.77%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 75.80%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 75.39%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 74.36%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 73.50%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 74.02%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 74.52%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 75.46%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 75.91%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 76.34%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 76.75%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 77.16%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 77.54%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 77.92%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 78.28%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 78.63%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 78.17%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 84.13%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 85.27%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 87.85%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 88.49%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 88.92%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 88.86%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 88.80%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.18%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 89.96%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 90.30%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 90.93%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 91.02%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.29%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 90.44%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 90.00%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 89.76%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 89.70%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 89.64%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.90%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.40%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 90.84%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 91.05%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 90.97%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 91.17%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 91.09%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 91.02%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.20%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 91.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.42%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 91.35%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 91.39%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 91.55%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 91.25%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.29%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 91.34%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 91.16%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 91.31%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 91.39%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 91.33%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 91.47%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 91.50%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 91.63%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 91.76%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 91.82%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 91.94%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 91.99%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 91.67%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 91.52%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 91.30%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 91.28%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 91.23%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 91.11%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 90.90%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 90.62%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 90.66%   [EVAL] batch:   81 | acc: 93.75%,  total acc: 90.70%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 90.51%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   84 | acc: 87.50%,  total acc: 90.59%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 90.70%   [EVAL] batch:   86 | acc: 75.00%,  total acc: 90.52%   [EVAL] batch:   87 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 90.45%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 90.28%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 90.25%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 90.08%   [EVAL] batch:   92 | acc: 62.50%,  total acc: 89.78%   [EVAL] batch:   93 | acc: 62.50%,  total acc: 89.49%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 89.28%   [EVAL] batch:   95 | acc: 62.50%,  total acc: 89.00%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 88.72%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 88.39%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 88.13%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 87.88%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 87.50%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 87.32%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 86.95%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 86.72%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 86.55%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 86.38%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 86.04%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 85.47%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 84.92%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 84.55%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 84.12%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 83.65%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 83.52%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 83.66%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 83.80%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 83.94%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 84.08%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 84.22%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 84.35%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 84.48%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 84.61%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 84.73%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 84.86%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 84.98%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 85.10%   
cur_acc:  ['0.9544', '0.7817']
his_acc:  ['0.9544', '0.8510']
CurrentTrain: epoch 15, batch     0 | loss: 13.6179868CurrentTrain: epoch 15, batch     1 | loss: 22.0894384CurrentTrain: epoch 15, batch     2 | loss: 16.5023482CurrentTrain: epoch  1, batch     3 | loss: 12.1529835CurrentTrain: epoch 15, batch     0 | loss: 15.0670296CurrentTrain: epoch 15, batch     1 | loss: 20.3296442CurrentTrain: epoch 15, batch     2 | loss: 18.3339138CurrentTrain: epoch  1, batch     3 | loss: 7.1161401CurrentTrain: epoch 15, batch     0 | loss: 13.5945496CurrentTrain: epoch 15, batch     1 | loss: 11.7639737CurrentTrain: epoch 15, batch     2 | loss: 12.4388122CurrentTrain: epoch  1, batch     3 | loss: 8.1610202CurrentTrain: epoch 15, batch     0 | loss: 15.6936601CurrentTrain: epoch 15, batch     1 | loss: 15.9363923CurrentTrain: epoch 15, batch     2 | loss: 9.2869334CurrentTrain: epoch  1, batch     3 | loss: 10.1526182CurrentTrain: epoch 15, batch     0 | loss: 8.8986567CurrentTrain: epoch 15, batch     1 | loss: 9.3830132CurrentTrain: epoch 15, batch     2 | loss: 12.0765404CurrentTrain: epoch  1, batch     3 | loss: 10.7716733CurrentTrain: epoch 15, batch     0 | loss: 12.1264702CurrentTrain: epoch 15, batch     1 | loss: 8.2575095CurrentTrain: epoch 15, batch     2 | loss: 16.2839139CurrentTrain: epoch  1, batch     3 | loss: 12.8108659CurrentTrain: epoch 15, batch     0 | loss: 15.5344611CurrentTrain: epoch 15, batch     1 | loss: 9.0589307CurrentTrain: epoch 15, batch     2 | loss: 11.4741159CurrentTrain: epoch  1, batch     3 | loss: 21.0153451CurrentTrain: epoch 15, batch     0 | loss: 10.4383938CurrentTrain: epoch 15, batch     1 | loss: 12.8543301CurrentTrain: epoch 15, batch     2 | loss: 7.9302463CurrentTrain: epoch  1, batch     3 | loss: 8.2069559CurrentTrain: epoch 15, batch     0 | loss: 10.2150714CurrentTrain: epoch 15, batch     1 | loss: 8.2777217CurrentTrain: epoch 15, batch     2 | loss: 6.6566584CurrentTrain: epoch  1, batch     3 | loss: 6.6107863CurrentTrain: epoch 15, batch     0 | loss: 9.4863764CurrentTrain: epoch 15, batch     1 | loss: 22.6921747CurrentTrain: epoch 15, batch     2 | loss: 12.4026105CurrentTrain: epoch  1, batch     3 | loss: 7.7278993
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: for more information on the grand mosque seizure itself , see " the siege of mecca " by yaroslav trofimov .
Head Entity: the siege of mecca
Tail Entity: grand mosque seizure
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: in her book "the power of habit," charles duhigg explores the science behind why habits exist and how they can be changed.  
Head Entity: the power of habit  
Tail Entity: science of habits  

Relation: main subject  
Context: "sapiens: a brief history of humankind" by yuval noah harari provides a compelling overview of the history and impact of our species.  
Head Entity: sapiens: a brief history of humankind  
Tail Entity: history of humankind  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: he represented romania at euro 2000 , his first major tournament where they beat england 3–2 , and drew 1–1 against germany .
Head Entity: euro 2000
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: The United States women's national soccer team has consistently performed well in international competitions, winning the FIFA Women's World Cup multiple times, including in 2019, where they faced the Netherlands in the final.  
Head Entity: FIFA Women's World Cup  
Tail Entity: Netherlands  

Relation: participating team  
Context: During the 2022 Winter Olympics, the Canadian ice hockey team showcased their skills, ultimately defeating the Russian Olympic Committee in a thrilling final match to secure the gold medal.  
Head Entity: 2022 Winter Olympics  
Tail Entity: Russian Olympic Committee  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: among the competing artists was previous eurovision song contest entrant robin juhkental who represented estonia in 2010 as part of the group malcolm lincoln .
Head Entity: malcolm lincoln
Tail Entity: robin juhkental
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The famous painting "The Last Supper" features several apostles, including Peter, who is depicted prominently in the scene.  
Head Entity: The Last Supper  
Tail Entity: Peter  

Relation: has part  
Context: The smartphone model Galaxy S21 comes with a high-resolution camera that includes a wide-angle lens, enhancing photography capabilities.  
Head Entity: Galaxy S21  
Tail Entity: wide-angle lens  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: for her performance in the musical - drama " crazy heart " ( 2009 ) , she was nominated for the academy award for best supporting actress .
Head Entity: crazy heart
Tail Entity: academy award for best supporting actress
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film "The Shape of Water" received critical acclaim and was nominated for several prestigious awards, including the Academy Award for Best Picture.  
Head Entity: The Shape of Water  
Tail Entity: Academy Award for Best Picture  

Relation: nominated for  
Context: In 2021, the popular series "The Crown" was nominated for multiple Emmy Awards, showcasing its excellence in television production.  
Head Entity: The Crown  
Tail Entity: Emmy Awards  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: in 2007 , the county municipality created the transit authority called " skyss " that would market public transport while is would be operated by private companies based on public service obligation contracts .
Head Entity: skyss
Tail Entity: transit authority
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone known as the iPhone was first released by Apple Inc. in 2007, revolutionizing the mobile phone industry and setting new standards for design and functionality.  
Head Entity: iPhone  
Tail Entity: smartphone  

Relation: instance of  
Context: The Great Wall of China is a series of fortifications made of various materials, built to protect the Chinese states and empires from invasions and raids.  
Head Entity: Great Wall of China  
Tail Entity: fortification
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: brand peak is a sharp snow - covered peak located east - southeast of the eternity range and northwest of mount duemler , in palmer land .
Head Entity: mount duemler
Tail Entity: eternity range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the rocky mountains extend from Canada through the United States and into Mexico, providing a stunning backdrop for outdoor enthusiasts.  
Head Entity: rocky mountains  
Tail Entity: Canada  

Relation: mountain range  
Context: the appalachian mountains are known for their rolling hills and rich biodiversity, stretching from Georgia to Maine.  
Head Entity: appalachian mountains  
Tail Entity: Maine  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: it would also be the last episode to feature a grounding , until " the marge - ian chronicles " in season 27 , six years later ( also written by brian kelley ) .
Head Entity: the marge - ian chronicles
Tail Entity: brian kelley
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: The film "Inception" was a groundbreaking project that showcased the visionary talents of its creator, with the screenplay crafted by the brilliant Christopher Nolan.  
Head Entity: Inception  
Tail Entity: Christopher Nolan  

Relation: screenwriter  
Context: The beloved animated feature "Toy Story" was brought to life through the imaginative script penned by the talented Andrew Stanton.  
Head Entity: Toy Story  
Tail Entity: Andrew Stanton  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the publisher found success in 1993 with tarō gomi 's " everyone poops " , a japanese language import that sold over one million copies and is now their best - known title .
Head Entity: everyone poops
Tail Entity: japanese language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The animated series "Avatar: The Last Airbender" features a rich narrative and is primarily presented in English, appealing to a wide audience across various age groups.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  

Relation: language of work or name  
Context: The famous novel "Cien años de soledad" by Gabriel García Márquez is celebrated for its intricate storytelling and is originally written in Spanish, reflecting the culture of Latin America.  
Head Entity: Cien años de soledad  
Tail Entity: Spanish  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: the aircraft was a boeing 707 - 321c that first entered service with pan american world airways ( pan am ) in 1963 .
Head Entity: boeing 707 - 321c
Tail Entity: pan american world airways
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: the new electric bus fleet is managed by the city transportation authority, which ensures efficient public transit services.  
Head Entity: electric bus fleet  
Tail Entity: city transportation authority  

Relation: operator  
Context: the state-of-the-art research facility is operated by the national science foundation, providing resources for groundbreaking studies.  
Head Entity: research facility  
Tail Entity: national science foundation  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: the most important lama of this series was the third changkya , rolpai dorje , who was preceptor to the qianlong emperor of china , and chief representative of tibetan buddhism at the qing court .
Head Entity: rolpai dorje
Tail Entity: tibetan buddhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the cathedral of notre-dame is a famous example of french gothic architecture and serves as the seat of the archbishop of paris, representing the roman catholic faith in the heart of the city.  
Head Entity: cathedral of notre-dame  
Tail Entity: roman catholic faith  

Relation: religion  
Context: the dalai lama is a prominent figure in the world, known for his teachings on compassion and non-violence, and is the spiritual leader of tibetan buddhism.  
Head Entity: dalai lama  
Tail Entity: tibetan buddhism  
MemoryTrain:  epoch 15, batch     0 | loss: 3.6821932MemoryTrain:  epoch 15, batch     1 | loss: 6.2719749MemoryTrain:  epoch 15, batch     2 | loss: 4.5394196MemoryTrain:  epoch 15, batch     3 | loss: 4.6817071MemoryTrain:  epoch 15, batch     4 | loss: 5.2679994MemoryTrain:  epoch  9, batch     5 | loss: 5.1303794MemoryTrain:  epoch 15, batch     0 | loss: 3.3566349MemoryTrain:  epoch 15, batch     1 | loss: 4.0660544MemoryTrain:  epoch 15, batch     2 | loss: 4.8558920MemoryTrain:  epoch 15, batch     3 | loss: 6.9566622MemoryTrain:  epoch 15, batch     4 | loss: 3.7025950MemoryTrain:  epoch  9, batch     5 | loss: 2.9136939MemoryTrain:  epoch 15, batch     0 | loss: 5.1298291MemoryTrain:  epoch 15, batch     1 | loss: 4.7333452MemoryTrain:  epoch 15, batch     2 | loss: 3.6513295MemoryTrain:  epoch 15, batch     3 | loss: 6.0233727MemoryTrain:  epoch 15, batch     4 | loss: 3.5874382MemoryTrain:  epoch  9, batch     5 | loss: 2.5260827MemoryTrain:  epoch 15, batch     0 | loss: 7.8356906MemoryTrain:  epoch 15, batch     1 | loss: 7.8542344MemoryTrain:  epoch 15, batch     2 | loss: 5.5689967MemoryTrain:  epoch 15, batch     3 | loss: 3.4618697MemoryTrain:  epoch 15, batch     4 | loss: 3.4353658MemoryTrain:  epoch  9, batch     5 | loss: 4.7778268MemoryTrain:  epoch 15, batch     0 | loss: 5.4067502MemoryTrain:  epoch 15, batch     1 | loss: 4.5129349MemoryTrain:  epoch 15, batch     2 | loss: 4.9848239MemoryTrain:  epoch 15, batch     3 | loss: 5.0935492MemoryTrain:  epoch 15, batch     4 | loss: 2.7408689MemoryTrain:  epoch  9, batch     5 | loss: 3.4819046MemoryTrain:  epoch 15, batch     0 | loss: 3.6547685MemoryTrain:  epoch 15, batch     1 | loss: 2.3101822MemoryTrain:  epoch 15, batch     2 | loss: 2.5202631MemoryTrain:  epoch 15, batch     3 | loss: 4.4147180MemoryTrain:  epoch 15, batch     4 | loss: 1.9906739MemoryTrain:  epoch  9, batch     5 | loss: 4.8665250MemoryTrain:  epoch 15, batch     0 | loss: 2.0448302MemoryTrain:  epoch 15, batch     1 | loss: 3.9318395MemoryTrain:  epoch 15, batch     2 | loss: 4.0802396MemoryTrain:  epoch 15, batch     3 | loss: 2.5483533MemoryTrain:  epoch 15, batch     4 | loss: 2.0798112MemoryTrain:  epoch  9, batch     5 | loss: 4.1947454MemoryTrain:  epoch 15, batch     0 | loss: 9.7833251MemoryTrain:  epoch 15, batch     1 | loss: 5.6905423MemoryTrain:  epoch 15, batch     2 | loss: 2.9557784MemoryTrain:  epoch 15, batch     3 | loss: 4.8023470MemoryTrain:  epoch 15, batch     4 | loss: 4.1050673MemoryTrain:  epoch  9, batch     5 | loss: 2.0412668MemoryTrain:  epoch 15, batch     0 | loss: 4.1135159MemoryTrain:  epoch 15, batch     1 | loss: 2.1127634MemoryTrain:  epoch 15, batch     2 | loss: 4.1098052MemoryTrain:  epoch 15, batch     3 | loss: 3.1076499MemoryTrain:  epoch 15, batch     4 | loss: 2.3867111MemoryTrain:  epoch  9, batch     5 | loss: 2.7932911MemoryTrain:  epoch 15, batch     0 | loss: 2.0074256MemoryTrain:  epoch 15, batch     1 | loss: 2.1307942MemoryTrain:  epoch 15, batch     2 | loss: 1.7219826MemoryTrain:  epoch 15, batch     3 | loss: 4.2335903MemoryTrain:  epoch 15, batch     4 | loss: 2.4007378MemoryTrain:  epoch  9, batch     5 | loss: 1.7774342
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 54.69%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 57.50%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 58.33%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 64.29%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 76.70%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 77.88%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 74.11%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 71.25%   [EVAL] batch:   15 | acc: 25.00%,  total acc: 68.36%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 67.65%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 66.32%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 64.80%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 66.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 68.15%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 69.32%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 70.65%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 72.75%   [EVAL] batch:   25 | acc: 31.25%,  total acc: 71.15%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 68.98%   [EVAL] batch:   27 | acc: 25.00%,  total acc: 67.41%   [EVAL] batch:   28 | acc: 50.00%,  total acc: 66.81%   [EVAL] batch:   29 | acc: 25.00%,  total acc: 65.42%   [EVAL] batch:   30 | acc: 31.25%,  total acc: 64.31%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 64.65%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 65.53%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 66.54%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 67.32%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 67.88%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 70.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 71.80%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 72.47%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 73.11%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 73.72%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 74.03%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 74.46%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 75.13%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 75.51%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 75.88%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 75.86%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 75.84%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 75.83%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 75.81%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 76.14%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 76.23%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 76.21%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 76.29%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 76.59%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 76.84%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 76.81%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 76.39%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 80.00%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 78.65%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 78.37%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 79.46%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 80.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 81.64%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 82.72%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 83.88%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 84.06%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 84.66%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 84.24%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 84.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.86%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.42%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.88%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 87.32%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 86.98%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 86.82%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 86.84%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.18%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.80%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 88.10%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 88.37%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 88.61%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 88.72%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 88.56%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.78%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 88.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 88.73%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 88.82%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 88.92%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 89.12%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 89.09%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.17%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 89.14%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 88.47%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 88.03%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 87.92%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 87.70%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 87.30%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 87.30%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 87.40%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 87.60%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 87.78%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 87.97%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 87.96%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 88.13%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 88.21%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 88.38%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 88.28%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 88.36%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 88.26%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 88.25%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 88.32%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 88.23%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 88.22%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 88.13%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 87.97%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 88.04%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 88.03%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 87.73%   [EVAL] batch:   83 | acc: 93.75%,  total acc: 87.80%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 87.65%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 87.65%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 87.28%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 87.29%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 87.15%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 87.01%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 87.09%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 87.02%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 86.90%   [EVAL] batch:   93 | acc: 62.50%,  total acc: 86.64%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 86.64%   [EVAL] batch:   95 | acc: 56.25%,  total acc: 86.33%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 86.15%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 85.97%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 85.80%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 85.62%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 85.40%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 85.23%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 84.95%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 84.86%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 84.76%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 84.73%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 84.46%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 83.97%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 83.49%   [EVAL] batch:  109 | acc: 50.00%,  total acc: 83.18%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 82.71%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 82.42%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 82.25%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 82.40%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 82.55%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 82.70%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 82.85%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 83.00%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 83.14%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 83.28%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 83.42%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 83.56%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 83.69%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 83.82%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 83.95%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 83.73%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 83.46%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 83.20%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 83.04%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 82.93%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 82.78%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 82.91%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 83.04%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 83.16%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 83.24%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 83.36%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 83.44%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 83.38%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 82.96%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 82.59%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 82.18%   [EVAL] batch:  141 | acc: 56.25%,  total acc: 82.00%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 81.73%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 81.42%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 81.55%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 81.68%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 81.76%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 81.88%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 82.01%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 82.08%   [EVAL] batch:  150 | acc: 31.25%,  total acc: 81.75%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 81.29%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 80.92%   [EVAL] batch:  153 | acc: 50.00%,  total acc: 80.72%   [EVAL] batch:  154 | acc: 25.00%,  total acc: 80.36%   [EVAL] batch:  155 | acc: 31.25%,  total acc: 80.05%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 80.02%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 80.10%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 80.23%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 80.31%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 80.36%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 80.48%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 80.60%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 80.72%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 80.83%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 80.95%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 81.06%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 81.18%   [EVAL] batch:  168 | acc: 100.00%,  total acc: 81.29%   [EVAL] batch:  169 | acc: 87.50%,  total acc: 81.32%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 81.40%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 81.50%   [EVAL] batch:  172 | acc: 81.25%,  total acc: 81.50%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 81.57%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 81.64%   [EVAL] batch:  175 | acc: 75.00%,  total acc: 81.61%   [EVAL] batch:  176 | acc: 75.00%,  total acc: 81.57%   [EVAL] batch:  177 | acc: 75.00%,  total acc: 81.53%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 81.49%   [EVAL] batch:  179 | acc: 93.75%,  total acc: 81.56%   [EVAL] batch:  180 | acc: 81.25%,  total acc: 81.56%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 81.52%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 81.52%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 81.59%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 81.55%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 81.62%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 81.58%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 81.42%   
cur_acc:  ['0.9544', '0.7817', '0.7639']
his_acc:  ['0.9544', '0.8510', '0.8142']
CurrentTrain: epoch 15, batch     0 | loss: 8.4939902CurrentTrain: epoch 15, batch     1 | loss: 13.7993839CurrentTrain: epoch 15, batch     2 | loss: 12.2555382CurrentTrain: epoch  1, batch     3 | loss: 9.7914326CurrentTrain: epoch 15, batch     0 | loss: 9.8607745CurrentTrain: epoch 15, batch     1 | loss: 10.8710702CurrentTrain: epoch 15, batch     2 | loss: 10.9968645CurrentTrain: epoch  1, batch     3 | loss: 5.8874493CurrentTrain: epoch 15, batch     0 | loss: 6.2722130CurrentTrain: epoch 15, batch     1 | loss: 14.2247056CurrentTrain: epoch 15, batch     2 | loss: 18.5951710CurrentTrain: epoch  1, batch     3 | loss: 7.2902904CurrentTrain: epoch 15, batch     0 | loss: 8.1064043CurrentTrain: epoch 15, batch     1 | loss: 11.4361922CurrentTrain: epoch 15, batch     2 | loss: 7.2929458CurrentTrain: epoch  1, batch     3 | loss: 9.4819339CurrentTrain: epoch 15, batch     0 | loss: 7.8124338CurrentTrain: epoch 15, batch     1 | loss: 11.2003010CurrentTrain: epoch 15, batch     2 | loss: 7.8209607CurrentTrain: epoch  1, batch     3 | loss: 6.6442635CurrentTrain: epoch 15, batch     0 | loss: 6.7060282CurrentTrain: epoch 15, batch     1 | loss: 13.2075013CurrentTrain: epoch 15, batch     2 | loss: 7.7539989CurrentTrain: epoch  1, batch     3 | loss: 7.2163105CurrentTrain: epoch 15, batch     0 | loss: 6.4456938CurrentTrain: epoch 15, batch     1 | loss: 5.7804294CurrentTrain: epoch 15, batch     2 | loss: 5.0045016CurrentTrain: epoch  1, batch     3 | loss: 6.6222536CurrentTrain: epoch 15, batch     0 | loss: 8.1855365CurrentTrain: epoch 15, batch     1 | loss: 13.6888792CurrentTrain: epoch 15, batch     2 | loss: 8.4350715CurrentTrain: epoch  1, batch     3 | loss: 6.6878698CurrentTrain: epoch 15, batch     0 | loss: 6.9733524CurrentTrain: epoch 15, batch     1 | loss: 6.5088269CurrentTrain: epoch 15, batch     2 | loss: 19.0319769CurrentTrain: epoch  1, batch     3 | loss: 5.4742515CurrentTrain: epoch 15, batch     0 | loss: 9.8372092CurrentTrain: epoch 15, batch     1 | loss: 7.0465419CurrentTrain: epoch 15, batch     2 | loss: 5.4763926CurrentTrain: epoch  1, batch     3 | loss: 5.8539793
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: alongside evan durbin and hugh gaitskell , he brought the thinking of john maynard keynes to the labour party , especially in relation to price determination .
Head Entity: hugh gaitskell
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: After serving as the mayor of Chicago, he became a prominent figure in the Democratic Party, advocating for social justice and economic reform.  
Head Entity: mayor of Chicago  
Tail Entity: Democratic Party  

Relation: member of political party  
Context: Throughout her career, she has been a staunch advocate for environmental policies as a member of the Green Party, pushing for sustainable practices at both local and national levels.  
Head Entity: she  
Tail Entity: Green Party  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: it is a partly fictional biography , adapted by shane connaughton and jim sheridan from the book " my left foot " by christy brown .
Head Entity: my left foot
Tail Entity: christy brown
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film is based on the novel "the great gatsby" by f. scott fitzgerald, capturing the essence of the roaring twenties.  
Head Entity: the great gatsby  
Tail Entity: f. scott fitzgerald  

Relation: after a work by  
Context: this painting draws inspiration from the famous artwork "starry night" created by vincent van gogh, showcasing swirling skies and vibrant colors.  
Head Entity: starry night  
Tail Entity: vincent van gogh  
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: the influential victorian art critic john ruskin expounded theories about social justice in " unto this last " ( 1860 ) .
Head Entity: john ruskin
Tail Entity: unto this last
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: the renowned physicist albert einstein published his groundbreaking paper on the theory of relativity in "on the electrodynamics of moving bodies" (1905).  
Head Entity: albert einstein  
Tail Entity: on the electrodynamics of moving bodies  

Relation: notable work  
Context: the celebrated author j.k. rowling captivated readers with her first book in the harry potter series, "harry potter and the philosopher's stone" (1997).  
Head Entity: j.k. rowling  
Tail Entity: harry potter and the philosopher's stone  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: at the request of the stockholm institute for bible translation , mirza khazar translated the bible – the new testament and old testament – into the azerbaijani language .
Head Entity: institute for bible translation
Tail Entity: stockholm
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: the global technology company, headquartered in san francisco, has been a leader in software development for over two decades.  
Head Entity: global technology company  
Tail Entity: san francisco  

Relation: headquarters location  
Context: after relocating its main office to new york city, the startup saw a significant increase in investment and visibility.  
Head Entity: startup  
Tail Entity: new york city  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most recognized as the wolf, which is classified under the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of flowering plants, the family Rosaceae encompasses a variety of species, including roses and strawberries, which are categorized under the order Rosales.  
Head Entity: Rosaceae  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: he later collaborated with vito volterra and explored laplace transforms and other parts of functional analysis .
Head Entity: vito volterra
Tail Entity: functional analysis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: Marie Curie was renowned for her groundbreaking research in radioactivity, which laid the foundation for advancements in nuclear physics.  
Head Entity: Marie Curie  
Tail Entity: nuclear physics  

Relation: field of work  
Context: The company specializes in artificial intelligence and machine learning, providing innovative solutions for various industries.  
Head Entity: The company  
Tail Entity: artificial intelligence
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: this early involvement in radio led him to buy many small , under performing radio stations beginning with wcos in columbia , south carolina , and turn them around with new formats and improved management .
Head Entity: wcos
Tail Entity: columbia , south carolina
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: The new television station, KXYZ, has been granted permission to broadcast to the entire metropolitan area of San Francisco, ensuring that residents can access a variety of programming.  
Head Entity: KXYZ  
Tail Entity: San Francisco  

Relation: licensed to broadcast to  
Context: After acquiring the rights, the radio station WABC was officially licensed to broadcast to the listeners in the greater New York City area, expanding its reach significantly.  
Head Entity: WABC  
Tail Entity: greater New York City area  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: tau² eridani ( τ² eridani , abbreviated tau² eri , τ² eri ) , also named angetenar , is a star in the constellation of eridanus .
Head Entity: angetenar
Tail Entity: eridanus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: betelgeuse ( α orionis , also known as alpha orionis ) is a red supergiant star located in the constellation of orion.  
Head Entity: betelgeuse  
Tail Entity: orion  

Relation: constellation  
Context: the star vega, also designated alpha lyrae, is the brightest star in the constellation of lyra.  
Head Entity: vega  
Tail Entity: lyra  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: aéroport is the place of gustaf iii airport , in " quartier " saint - jean of saint barthélemy in the caribbean .
Head Entity: gustaf iii airport
Tail Entity: saint barthélemy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The central train station in the city of Zurich connects various regions and serves as a major hub for travelers heading to the picturesque Lake Zurich.  
Head Entity: central train station  
Tail Entity: Lake Zurich  

Relation: place served by transport hub  
Context: The international airport in Tokyo is a key gateway for flights to and from the bustling city of Yokohama, making it a vital transport hub in the region.  
Head Entity: international airport  
Tail Entity: Yokohama  
MemoryTrain:  epoch 15, batch     0 | loss: 3.2517579MemoryTrain:  epoch 15, batch     1 | loss: 2.6059975MemoryTrain:  epoch 15, batch     2 | loss: 3.8033346MemoryTrain:  epoch 15, batch     3 | loss: 4.0024437MemoryTrain:  epoch 15, batch     4 | loss: 3.0560896MemoryTrain:  epoch 15, batch     5 | loss: 11.2709423MemoryTrain:  epoch 15, batch     6 | loss: 5.3330524MemoryTrain:  epoch  7, batch     7 | loss: 3.9556938MemoryTrain:  epoch 15, batch     0 | loss: 3.2187763MemoryTrain:  epoch 15, batch     1 | loss: 3.1713603MemoryTrain:  epoch 15, batch     2 | loss: 2.7483693MemoryTrain:  epoch 15, batch     3 | loss: 7.3037827MemoryTrain:  epoch 15, batch     4 | loss: 3.3880732MemoryTrain:  epoch 15, batch     5 | loss: 2.8775360MemoryTrain:  epoch 15, batch     6 | loss: 5.5672275MemoryTrain:  epoch  7, batch     7 | loss: 3.2094561MemoryTrain:  epoch 15, batch     0 | loss: 2.3497162MemoryTrain:  epoch 15, batch     1 | loss: 3.2546283MemoryTrain:  epoch 15, batch     2 | loss: 5.0304880MemoryTrain:  epoch 15, batch     3 | loss: 2.9046826MemoryTrain:  epoch 15, batch     4 | loss: 4.4808245MemoryTrain:  epoch 15, batch     5 | loss: 2.8429249MemoryTrain:  epoch 15, batch     6 | loss: 4.3799429MemoryTrain:  epoch  7, batch     7 | loss: 4.8449313MemoryTrain:  epoch 15, batch     0 | loss: 2.9988998MemoryTrain:  epoch 15, batch     1 | loss: 5.8782002MemoryTrain:  epoch 15, batch     2 | loss: 2.1664450MemoryTrain:  epoch 15, batch     3 | loss: 4.7210315MemoryTrain:  epoch 15, batch     4 | loss: 3.1651616MemoryTrain:  epoch 15, batch     5 | loss: 2.0889655MemoryTrain:  epoch 15, batch     6 | loss: 2.0923705MemoryTrain:  epoch  7, batch     7 | loss: 3.0581613MemoryTrain:  epoch 15, batch     0 | loss: 4.1510475MemoryTrain:  epoch 15, batch     1 | loss: 2.7843857MemoryTrain:  epoch 15, batch     2 | loss: 2.7902242MemoryTrain:  epoch 15, batch     3 | loss: 4.3076612MemoryTrain:  epoch 15, batch     4 | loss: 2.0424904MemoryTrain:  epoch 15, batch     5 | loss: 2.6387396MemoryTrain:  epoch 15, batch     6 | loss: 3.1805449MemoryTrain:  epoch  7, batch     7 | loss: 2.2318689MemoryTrain:  epoch 15, batch     0 | loss: 3.1860834MemoryTrain:  epoch 15, batch     1 | loss: 1.9978049MemoryTrain:  epoch 15, batch     2 | loss: 4.0990418MemoryTrain:  epoch 15, batch     3 | loss: 8.9334135MemoryTrain:  epoch 15, batch     4 | loss: 2.9542831MemoryTrain:  epoch 15, batch     5 | loss: 1.6126206MemoryTrain:  epoch 15, batch     6 | loss: 2.1093803MemoryTrain:  epoch  7, batch     7 | loss: 1.8200349MemoryTrain:  epoch 15, batch     0 | loss: 3.4821983MemoryTrain:  epoch 15, batch     1 | loss: 1.7993055MemoryTrain:  epoch 15, batch     2 | loss: 2.6993026MemoryTrain:  epoch 15, batch     3 | loss: 2.5762965MemoryTrain:  epoch 15, batch     4 | loss: 3.4052063MemoryTrain:  epoch 15, batch     5 | loss: 1.6991141MemoryTrain:  epoch 15, batch     6 | loss: 2.5354267MemoryTrain:  epoch  7, batch     7 | loss: 2.0589669MemoryTrain:  epoch 15, batch     0 | loss: 3.1963474MemoryTrain:  epoch 15, batch     1 | loss: 1.9895499MemoryTrain:  epoch 15, batch     2 | loss: 1.5209057MemoryTrain:  epoch 15, batch     3 | loss: 1.8098251MemoryTrain:  epoch 15, batch     4 | loss: 1.7765253MemoryTrain:  epoch 15, batch     5 | loss: 2.5752775MemoryTrain:  epoch 15, batch     6 | loss: 1.7339303MemoryTrain:  epoch  7, batch     7 | loss: 1.6958774MemoryTrain:  epoch 15, batch     0 | loss: 1.5714175MemoryTrain:  epoch 15, batch     1 | loss: 3.8564795MemoryTrain:  epoch 15, batch     2 | loss: 1.6450789MemoryTrain:  epoch 15, batch     3 | loss: 2.6950623MemoryTrain:  epoch 15, batch     4 | loss: 2.6084503MemoryTrain:  epoch 15, batch     5 | loss: 1.8800532MemoryTrain:  epoch 15, batch     6 | loss: 1.8703543MemoryTrain:  epoch  7, batch     7 | loss: 1.2909072MemoryTrain:  epoch 15, batch     0 | loss: 1.6788350MemoryTrain:  epoch 15, batch     1 | loss: 6.8103379MemoryTrain:  epoch 15, batch     2 | loss: 1.5547047MemoryTrain:  epoch 15, batch     3 | loss: 1.5403766MemoryTrain:  epoch 15, batch     4 | loss: 1.6224857MemoryTrain:  epoch 15, batch     5 | loss: 2.2702447MemoryTrain:  epoch 15, batch     6 | loss: 2.0500204MemoryTrain:  epoch  7, batch     7 | loss: 1.3680500
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 81.82%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 79.69%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 78.85%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 78.57%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 78.33%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 77.73%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 77.57%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 76.64%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 75.94%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 76.19%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 76.14%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 75.54%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 75.52%   [EVAL] batch:   24 | acc: 37.50%,  total acc: 74.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 75.93%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 76.79%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 77.59%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.03%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 79.30%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 79.55%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 79.60%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 79.82%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 79.51%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 79.39%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 79.77%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 80.29%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 80.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 81.70%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 82.12%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 82.39%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 82.78%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 83.15%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 83.51%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 83.85%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 84.18%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 84.50%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 84.44%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 84.62%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 84.79%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 85.07%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 85.11%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 85.38%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 85.53%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 85.67%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 85.70%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 86.17%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 86.19%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 85.62%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 74.31%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 72.16%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 70.83%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 71.15%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 72.77%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 74.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 77.21%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 79.28%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 79.38%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 79.46%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 79.55%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 79.62%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 79.95%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.77%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.48%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 82.54%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.67%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 83.98%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 84.47%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 84.01%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 83.51%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 83.45%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 83.55%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.97%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.76%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 85.12%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 85.47%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 85.80%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 85.83%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 85.73%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 85.51%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 85.46%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 85.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 85.54%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 85.58%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 85.73%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 85.53%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 85.11%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 85.16%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 85.20%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 84.59%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 84.22%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 84.17%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 84.12%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 83.87%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 84.03%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 84.18%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 84.42%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 84.89%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 84.93%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 85.14%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 85.27%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 85.48%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 85.33%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 85.45%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 85.56%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 85.67%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 85.86%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 85.88%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 85.90%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 85.84%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 85.70%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 85.80%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 85.75%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 85.39%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 85.37%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 85.39%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 85.06%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 84.94%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 84.76%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 84.65%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 84.62%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 84.44%   [EVAL] batch:   92 | acc: 68.75%,  total acc: 84.27%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 83.84%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:   95 | acc: 56.25%,  total acc: 83.46%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 83.31%   [EVAL] batch:   97 | acc: 75.00%,  total acc: 83.23%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 83.21%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 83.06%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 82.86%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 82.78%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 82.71%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 82.75%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 82.62%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 82.72%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 82.54%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 82.06%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 81.59%   [EVAL] batch:  109 | acc: 50.00%,  total acc: 81.31%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 80.86%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 80.58%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 80.48%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 80.65%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 80.82%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 80.98%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 81.14%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 81.30%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 81.46%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 81.61%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 81.71%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 81.86%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 82.01%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 82.16%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 82.30%   [EVAL] batch:  125 | acc: 50.00%,  total acc: 82.04%   [EVAL] batch:  126 | acc: 62.50%,  total acc: 81.89%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 81.74%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 81.54%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 81.39%   [EVAL] batch:  130 | acc: 56.25%,  total acc: 81.20%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 81.34%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 81.39%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 81.48%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 81.53%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 81.66%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 81.75%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 81.66%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 81.25%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 80.89%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 80.54%   [EVAL] batch:  141 | acc: 50.00%,  total acc: 80.33%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 80.07%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 79.77%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 79.91%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 80.05%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 80.14%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 80.28%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 80.41%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 80.50%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 80.05%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 79.61%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 79.13%   [EVAL] batch:  153 | acc: 43.75%,  total acc: 78.90%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 78.47%   [EVAL] batch:  155 | acc: 31.25%,  total acc: 78.17%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 78.14%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 78.24%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 78.38%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 78.48%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 78.53%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 78.67%   [EVAL] batch:  162 | acc: 75.00%,  total acc: 78.64%   [EVAL] batch:  163 | acc: 81.25%,  total acc: 78.66%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 78.67%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 78.58%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 78.63%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 78.72%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 78.81%   [EVAL] batch:  169 | acc: 93.75%,  total acc: 78.90%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 78.98%   [EVAL] batch:  171 | acc: 93.75%,  total acc: 79.07%   [EVAL] batch:  172 | acc: 81.25%,  total acc: 79.08%   [EVAL] batch:  173 | acc: 100.00%,  total acc: 79.20%   [EVAL] batch:  174 | acc: 100.00%,  total acc: 79.32%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 79.26%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 79.17%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 79.07%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 79.05%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 79.10%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 79.04%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 79.02%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 79.06%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 79.14%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 79.12%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 79.20%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 79.21%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 79.29%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 79.37%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 79.47%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 79.58%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 79.59%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 79.70%   [EVAL] batch:  193 | acc: 87.50%,  total acc: 79.74%   [EVAL] batch:  194 | acc: 62.50%,  total acc: 79.65%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 79.59%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 79.60%   [EVAL] batch:  197 | acc: 56.25%,  total acc: 79.48%   [EVAL] batch:  198 | acc: 56.25%,  total acc: 79.37%   [EVAL] batch:  199 | acc: 50.00%,  total acc: 79.22%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 79.20%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 79.24%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 79.22%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 79.14%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 79.05%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 79.04%   [EVAL] batch:  206 | acc: 68.75%,  total acc: 78.99%   [EVAL] batch:  207 | acc: 75.00%,  total acc: 78.97%   [EVAL] batch:  208 | acc: 75.00%,  total acc: 78.95%   [EVAL] batch:  209 | acc: 68.75%,  total acc: 78.90%   [EVAL] batch:  210 | acc: 68.75%,  total acc: 78.85%   [EVAL] batch:  211 | acc: 43.75%,  total acc: 78.69%   [EVAL] batch:  212 | acc: 81.25%,  total acc: 78.70%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 78.80%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 78.90%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 78.99%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 79.09%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 79.19%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 79.28%   [EVAL] batch:  219 | acc: 75.00%,  total acc: 79.26%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 79.33%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 79.34%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 79.29%   [EVAL] batch:  223 | acc: 87.50%,  total acc: 79.32%   [EVAL] batch:  224 | acc: 75.00%,  total acc: 79.31%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 79.40%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 79.49%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 79.58%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 79.67%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 79.76%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 79.82%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 79.90%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 79.99%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 80.07%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 80.16%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 80.24%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 80.33%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 80.38%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 80.39%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 80.44%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 80.52%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 80.58%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 80.63%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 80.69%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 80.77%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 80.79%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 80.85%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 80.92%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 80.97%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 81.03%   
cur_acc:  ['0.9544', '0.7817', '0.7639', '0.8562']
his_acc:  ['0.9544', '0.8510', '0.8142', '0.8103']
CurrentTrain: epoch 15, batch     0 | loss: 14.0263959CurrentTrain: epoch 15, batch     1 | loss: 12.0184833CurrentTrain: epoch 15, batch     2 | loss: 20.3932919CurrentTrain: epoch  1, batch     3 | loss: 9.4838814CurrentTrain: epoch 15, batch     0 | loss: 9.1372343CurrentTrain: epoch 15, batch     1 | loss: 10.9426477CurrentTrain: epoch 15, batch     2 | loss: 16.1267793CurrentTrain: epoch  1, batch     3 | loss: 9.7482809CurrentTrain: epoch 15, batch     0 | loss: 10.2011169CurrentTrain: epoch 15, batch     1 | loss: 10.4589859CurrentTrain: epoch 15, batch     2 | loss: 8.4627480CurrentTrain: epoch  1, batch     3 | loss: 6.8072685CurrentTrain: epoch 15, batch     0 | loss: 10.8577029CurrentTrain: epoch 15, batch     1 | loss: 11.8139573CurrentTrain: epoch 15, batch     2 | loss: 8.2819620CurrentTrain: epoch  1, batch     3 | loss: 7.9053857CurrentTrain: epoch 15, batch     0 | loss: 6.8360483CurrentTrain: epoch 15, batch     1 | loss: 16.4573514CurrentTrain: epoch 15, batch     2 | loss: 11.3547929CurrentTrain: epoch  1, batch     3 | loss: 6.6862490CurrentTrain: epoch 15, batch     0 | loss: 8.8071459CurrentTrain: epoch 15, batch     1 | loss: 6.0981391CurrentTrain: epoch 15, batch     2 | loss: 5.5959977CurrentTrain: epoch  1, batch     3 | loss: 6.4154036CurrentTrain: epoch 15, batch     0 | loss: 4.9174833CurrentTrain: epoch 15, batch     1 | loss: 17.7287970CurrentTrain: epoch 15, batch     2 | loss: 11.1688824CurrentTrain: epoch  1, batch     3 | loss: 7.7993971CurrentTrain: epoch 15, batch     0 | loss: 5.1865340CurrentTrain: epoch 15, batch     1 | loss: 8.8734039CurrentTrain: epoch 15, batch     2 | loss: 6.9019036CurrentTrain: epoch  1, batch     3 | loss: 7.4110664CurrentTrain: epoch 15, batch     0 | loss: 6.6448846CurrentTrain: epoch 15, batch     1 | loss: 6.6551367CurrentTrain: epoch 15, batch     2 | loss: 6.3048036CurrentTrain: epoch  1, batch     3 | loss: 6.3094204CurrentTrain: epoch 15, batch     0 | loss: 6.8193020CurrentTrain: epoch 15, batch     1 | loss: 6.0266167CurrentTrain: epoch 15, batch     2 | loss: 6.9455542CurrentTrain: epoch  1, batch     3 | loss: 6.1067315
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the cantons of créteil are administrative divisions of the val - de - marne department , île - de - france region , northern france .
Head Entity: île - de - france
Tail Entity: val - de - marne
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The provinces of Canada are the primary administrative divisions, with Ontario being one of the largest provinces in the country.  
Head Entity: Canada  
Tail Entity: Ontario  

Relation: contains administrative territorial entity  
Context: The state of California is divided into several counties, with Los Angeles County being the most populous.  
Head Entity: California  
Tail Entity: Los Angeles County  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: enzo is an italian given name derivative of heinz , a form of henry .
Head Entity: henry
Tail Entity: heinz
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as each other due to their close genetic relationship.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: Many historians argue that the ancient city of Byzantium is said to be the same as modern-day Istanbul, although the transition involved significant changes over time.  
Head Entity: Byzantium  
Tail Entity: Istanbul  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: he considers his father to be biggest influence on his career as his brother javier castellano recipient of four eclipse award for outstanding jockey in the row ( 2013 , 2014,2015 and 2016 ) .
Head Entity: eclipse award for outstanding jockey
Tail Entity: javier castellano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: In the thrilling finale of the championship, Sarah Thompson emerged victorious, claiming the title of best player in the tournament, while her teammate, Mark Johnson, was awarded the runner-up position.  
Head Entity: best player in the tournament  
Tail Entity: Sarah Thompson  

Relation: winner  
Context: The annual science fair concluded with Emily Chen taking home the grand prize for her innovative project on renewable energy, while her classmate, David Lee, received an honorable mention for his work on robotics.  
Head Entity: grand prize  
Tail Entity: Emily Chen  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: the calais region was defended by the 15th army under " generaloberst " ( colonel general ) hans von salmuth , and normandy by the 7th army commanded by " generaloberst " friedrich dollmann .
Head Entity: hans von salmuth
Tail Entity: generaloberst
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: during the battle of stalingrad, the soviet forces were led by general vasily chuikov, who held the rank of major general, while the german troops were commanded by general friedrich paulus.  
Head Entity: vasily chuikov  
Tail Entity: major general  

Relation: military rank  
Context: admiral horatio nelson was a key figure in the british navy, famously known for his rank of vice admiral during the napoleonic wars.  
Head Entity: horatio nelson  
Tail Entity: vice admiral  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: he also worked on activision games like " gun " , " " and " the amazing spider - man " .
Head Entity: gun
Tail Entity: activision
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: The latest novel by J.K. Rowling was released by Bloomsbury Publishing, captivating readers worldwide.  
Head Entity: J.K. Rowling  
Tail Entity: Bloomsbury Publishing  

Relation: publisher  
Context: The popular science magazine was launched by National Geographic, providing insights into nature and exploration.  
Head Entity: science magazine  
Tail Entity: National Geographic  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " muzai moratorium " and " shōso strip " were released on vinyl in 2008 , as a part of her 10-year anniversary celebrations .
Head Entity: muzai moratorium
Tail Entity: shōso strip
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first chapter of the book introduces the main character, while the second chapter follows with a detailed backstory.  
Head Entity: first chapter  
Tail Entity: second chapter  

Relation: followed by  
Context: In the concert lineup, the opening act performed a lively set, followed by the headliner who captivated the audience with their greatest hits.  
Head Entity: opening act  
Tail Entity: headliner  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: there is a memoir of his life and work by gustav queck ( jena , 1852 ) .
Head Entity: gustav queck
Tail Entity: jena
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: the headquarters of the company is situated in san francisco, where they have been operating since 2010.  
Head Entity: the company  
Tail Entity: san francisco  

Relation: work location  
Context: during her time at the university, she conducted research in various labs located in boston.  
Head Entity: she  
Tail Entity: boston  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in enzyme activity.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a talented software engineer, has been working on innovative artificial intelligence projects for over a decade.  
Head Entity: john smith  
Tail Entity: software engineer  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan was designated as a World Heritage Site in 1985, attracting millions of tourists each year.  
Head Entity: Petra  
Tail Entity: World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: beverly bower ( september 30 , 1925 – march 24 , 2002 ) was an american operatic soprano who had an active international opera career from the mid-1950s through the early 1970s .
Head Entity: beverly bower
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti ( october 12 , 1935 – september 6 , 2007 ) was an italian tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: aretha franklin ( march 25 , 1942 – august 16 , 2018 ) was an american singer, songwriter, and civil rights activist, often referred to as the "Queen of Soul" for her powerful and emotive voice.  
Head Entity: aretha franklin  
Tail Entity: soul
MemoryTrain:  epoch 15, batch     0 | loss: 4.8541837MemoryTrain:  epoch 15, batch     1 | loss: 5.0588386MemoryTrain:  epoch 15, batch     2 | loss: 4.8602679MemoryTrain:  epoch 15, batch     3 | loss: 6.0782568MemoryTrain:  epoch 15, batch     4 | loss: 3.6111094MemoryTrain:  epoch 15, batch     5 | loss: 3.6102586MemoryTrain:  epoch 15, batch     6 | loss: 3.9833369MemoryTrain:  epoch 15, batch     7 | loss: 3.1777408MemoryTrain:  epoch 15, batch     8 | loss: 4.0053589MemoryTrain:  epoch  5, batch     9 | loss: 8.3117451MemoryTrain:  epoch 15, batch     0 | loss: 6.5750599MemoryTrain:  epoch 15, batch     1 | loss: 4.4506704MemoryTrain:  epoch 15, batch     2 | loss: 3.1814794MemoryTrain:  epoch 15, batch     3 | loss: 2.4074197MemoryTrain:  epoch 15, batch     4 | loss: 1.8734833MemoryTrain:  epoch 15, batch     5 | loss: 2.1920619MemoryTrain:  epoch 15, batch     6 | loss: 11.1240579MemoryTrain:  epoch 15, batch     7 | loss: 2.6288654MemoryTrain:  epoch 15, batch     8 | loss: 2.5619554MemoryTrain:  epoch  5, batch     9 | loss: 11.2728201MemoryTrain:  epoch 15, batch     0 | loss: 2.3593805MemoryTrain:  epoch 15, batch     1 | loss: 2.4885504MemoryTrain:  epoch 15, batch     2 | loss: 5.0996356MemoryTrain:  epoch 15, batch     3 | loss: 2.3190857MemoryTrain:  epoch 15, batch     4 | loss: 2.5183883MemoryTrain:  epoch 15, batch     5 | loss: 1.8350155MemoryTrain:  epoch 15, batch     6 | loss: 9.0025752MemoryTrain:  epoch 15, batch     7 | loss: 2.5754707MemoryTrain:  epoch 15, batch     8 | loss: 3.5188723MemoryTrain:  epoch  5, batch     9 | loss: 8.4212430MemoryTrain:  epoch 15, batch     0 | loss: 2.5403925MemoryTrain:  epoch 15, batch     1 | loss: 2.1290232MemoryTrain:  epoch 15, batch     2 | loss: 2.7519601MemoryTrain:  epoch 15, batch     3 | loss: 2.6428035MemoryTrain:  epoch 15, batch     4 | loss: 4.1902812MemoryTrain:  epoch 15, batch     5 | loss: 4.3493545MemoryTrain:  epoch 15, batch     6 | loss: 1.7796106MemoryTrain:  epoch 15, batch     7 | loss: 1.9171574MemoryTrain:  epoch 15, batch     8 | loss: 4.7047194MemoryTrain:  epoch  5, batch     9 | loss: 8.0416344MemoryTrain:  epoch 15, batch     0 | loss: 1.8810460MemoryTrain:  epoch 15, batch     1 | loss: 2.9349952MemoryTrain:  epoch 15, batch     2 | loss: 2.1588754MemoryTrain:  epoch 15, batch     3 | loss: 2.4315462MemoryTrain:  epoch 15, batch     4 | loss: 2.4546491MemoryTrain:  epoch 15, batch     5 | loss: 2.1247526MemoryTrain:  epoch 15, batch     6 | loss: 1.9066475MemoryTrain:  epoch 15, batch     7 | loss: 1.8194741MemoryTrain:  epoch 15, batch     8 | loss: 2.4954582MemoryTrain:  epoch  5, batch     9 | loss: 8.0923810MemoryTrain:  epoch 15, batch     0 | loss: 1.6940535MemoryTrain:  epoch 15, batch     1 | loss: 2.8352506MemoryTrain:  epoch 15, batch     2 | loss: 1.9330499MemoryTrain:  epoch 15, batch     3 | loss: 2.1905498MemoryTrain:  epoch 15, batch     4 | loss: 2.0479811MemoryTrain:  epoch 15, batch     5 | loss: 2.8914800MemoryTrain:  epoch 15, batch     6 | loss: 1.5387827MemoryTrain:  epoch 15, batch     7 | loss: 2.0815004MemoryTrain:  epoch 15, batch     8 | loss: 4.3010421MemoryTrain:  epoch  5, batch     9 | loss: 8.2045224MemoryTrain:  epoch 15, batch     0 | loss: 2.7647624MemoryTrain:  epoch 15, batch     1 | loss: 2.6544895MemoryTrain:  epoch 15, batch     2 | loss: 1.5447113MemoryTrain:  epoch 15, batch     3 | loss: 1.4925299MemoryTrain:  epoch 15, batch     4 | loss: 2.0172717MemoryTrain:  epoch 15, batch     5 | loss: 1.7807682MemoryTrain:  epoch 15, batch     6 | loss: 2.0906841MemoryTrain:  epoch 15, batch     7 | loss: 1.8826809MemoryTrain:  epoch 15, batch     8 | loss: 1.8730119MemoryTrain:  epoch  5, batch     9 | loss: 8.2456405MemoryTrain:  epoch 15, batch     0 | loss: 2.4291684MemoryTrain:  epoch 15, batch     1 | loss: 1.9983317MemoryTrain:  epoch 15, batch     2 | loss: 3.9551664MemoryTrain:  epoch 15, batch     3 | loss: 2.4686552MemoryTrain:  epoch 15, batch     4 | loss: 1.9684552MemoryTrain:  epoch 15, batch     5 | loss: 2.0828265MemoryTrain:  epoch 15, batch     6 | loss: 2.5727523MemoryTrain:  epoch 15, batch     7 | loss: 1.4647853MemoryTrain:  epoch 15, batch     8 | loss: 1.8673196MemoryTrain:  epoch  5, batch     9 | loss: 8.0653746MemoryTrain:  epoch 15, batch     0 | loss: 1.5535929MemoryTrain:  epoch 15, batch     1 | loss: 2.0157478MemoryTrain:  epoch 15, batch     2 | loss: 1.4242501MemoryTrain:  epoch 15, batch     3 | loss: 1.4343475MemoryTrain:  epoch 15, batch     4 | loss: 1.4719384MemoryTrain:  epoch 15, batch     5 | loss: 1.7665622MemoryTrain:  epoch 15, batch     6 | loss: 1.8984813MemoryTrain:  epoch 15, batch     7 | loss: 2.5499953MemoryTrain:  epoch 15, batch     8 | loss: 1.7215378MemoryTrain:  epoch  5, batch     9 | loss: 7.8372696MemoryTrain:  epoch 15, batch     0 | loss: 2.0074761MemoryTrain:  epoch 15, batch     1 | loss: 1.6246097MemoryTrain:  epoch 15, batch     2 | loss: 1.7385709MemoryTrain:  epoch 15, batch     3 | loss: 1.5831685MemoryTrain:  epoch 15, batch     4 | loss: 1.9969026MemoryTrain:  epoch 15, batch     5 | loss: 3.9362553MemoryTrain:  epoch 15, batch     6 | loss: 1.6610455MemoryTrain:  epoch 15, batch     7 | loss: 1.7112258MemoryTrain:  epoch 15, batch     8 | loss: 1.4697826MemoryTrain:  epoch  5, batch     9 | loss: 7.9163912
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 29.69%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 32.50%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 33.33%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 36.61%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 42.19%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 45.83%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 47.50%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 47.16%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 48.96%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 52.40%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 53.12%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 54.17%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 55.47%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 56.62%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 57.99%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 59.54%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 60.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 62.80%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 64.49%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 65.76%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 66.67%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 67.75%   [EVAL] batch:   25 | acc: 56.25%,  total acc: 67.31%   [EVAL] batch:   26 | acc: 31.25%,  total acc: 65.97%   [EVAL] batch:   27 | acc: 43.75%,  total acc: 65.18%   [EVAL] batch:   28 | acc: 62.50%,  total acc: 65.09%   [EVAL] batch:   29 | acc: 37.50%,  total acc: 64.17%   [EVAL] batch:   30 | acc: 50.00%,  total acc: 63.71%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 63.67%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 63.83%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 62.87%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 62.32%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 61.28%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 60.81%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 61.18%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 61.86%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 62.96%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 63.69%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 64.10%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 64.77%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 65.14%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 65.49%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 65.69%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 65.76%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 66.20%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 66.62%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 67.28%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 67.91%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 68.51%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 69.10%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 69.66%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 70.20%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 70.72%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 71.23%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 71.72%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 72.19%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 72.64%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 73.08%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 72.72%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 74.11%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 69.44%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 67.61%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 65.10%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 65.38%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 67.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 69.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 72.79%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 73.96%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 75.33%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 75.62%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 76.19%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 75.85%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 75.54%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 76.30%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 76.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.40%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.24%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.02%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 79.53%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.85%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.82%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 81.62%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 81.61%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 81.77%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 82.09%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 82.40%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 82.85%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.28%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 83.69%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 84.16%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 84.23%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 84.17%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 83.83%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 83.51%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 83.46%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 83.55%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 83.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 83.58%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 83.53%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 83.73%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 83.22%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 82.73%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 82.70%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 82.79%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 82.33%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 82.20%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 82.19%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 82.17%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 81.96%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 82.42%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 82.69%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 82.95%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 83.21%   [EVAL] batch:   67 | acc: 81.25%,  total acc: 83.18%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 83.42%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 83.57%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 83.80%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 83.77%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 83.90%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 84.12%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 84.25%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 84.29%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 83.93%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 83.73%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 83.31%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 82.97%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 82.79%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 82.39%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 82.00%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 81.99%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 81.76%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 81.61%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 81.25%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 81.04%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 80.83%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 80.76%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 80.77%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 80.57%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 80.31%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 79.79%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 79.67%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 79.36%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 79.25%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 79.02%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 78.91%   [EVAL] batch:   99 | acc: 56.25%,  total acc: 78.69%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 78.53%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 78.49%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 78.40%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 78.43%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 78.33%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 78.48%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 78.33%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 77.89%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 77.47%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 77.16%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 76.75%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 76.51%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 76.38%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 76.59%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 76.79%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 76.99%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 77.19%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 77.38%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 77.57%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 77.76%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 77.89%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 78.07%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 78.25%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 78.43%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 78.60%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 78.42%   [EVAL] batch:  126 | acc: 62.50%,  total acc: 78.30%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 78.12%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 77.96%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 77.88%   [EVAL] batch:  130 | acc: 56.25%,  total acc: 77.72%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 77.89%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 77.91%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 77.94%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 77.96%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 78.08%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 78.15%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 78.08%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 77.65%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 77.32%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 76.95%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 76.67%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 76.44%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 76.17%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 76.29%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 76.46%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 76.57%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 76.73%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 76.89%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 77.00%   [EVAL] batch:  150 | acc: 25.00%,  total acc: 76.66%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 76.23%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 75.78%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 75.45%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 75.04%   [EVAL] batch:  155 | acc: 12.50%,  total acc: 74.64%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 74.64%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 74.76%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 74.92%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 75.04%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 75.12%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 75.27%   [EVAL] batch:  162 | acc: 75.00%,  total acc: 75.27%   [EVAL] batch:  163 | acc: 81.25%,  total acc: 75.30%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 75.30%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 75.19%   [EVAL] batch:  166 | acc: 81.25%,  total acc: 75.22%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 75.33%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 75.44%   [EVAL] batch:  169 | acc: 93.75%,  total acc: 75.55%   [EVAL] batch:  170 | acc: 100.00%,  total acc: 75.69%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 75.84%   [EVAL] batch:  172 | acc: 81.25%,  total acc: 75.87%   [EVAL] batch:  173 | acc: 100.00%,  total acc: 76.01%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 76.11%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 76.07%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 75.99%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 75.95%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 75.91%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 75.97%   [EVAL] batch:  180 | acc: 43.75%,  total acc: 75.79%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 75.79%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 75.85%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 75.95%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 76.01%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 76.11%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 76.14%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 76.23%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 76.32%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 76.41%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 76.54%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 76.56%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 76.68%   [EVAL] batch:  193 | acc: 93.75%,  total acc: 76.77%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 76.79%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 76.79%   [EVAL] batch:  196 | acc: 87.50%,  total acc: 76.84%   [EVAL] batch:  197 | acc: 68.75%,  total acc: 76.80%   [EVAL] batch:  198 | acc: 56.25%,  total acc: 76.70%   [EVAL] batch:  199 | acc: 68.75%,  total acc: 76.66%   [EVAL] batch:  200 | acc: 87.50%,  total acc: 76.71%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 76.73%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 76.75%   [EVAL] batch:  203 | acc: 75.00%,  total acc: 76.75%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 76.71%   [EVAL] batch:  205 | acc: 81.25%,  total acc: 76.73%   [EVAL] batch:  206 | acc: 43.75%,  total acc: 76.57%   [EVAL] batch:  207 | acc: 43.75%,  total acc: 76.41%   [EVAL] batch:  208 | acc: 56.25%,  total acc: 76.32%   [EVAL] batch:  209 | acc: 43.75%,  total acc: 76.16%   [EVAL] batch:  210 | acc: 50.00%,  total acc: 76.04%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 75.83%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 75.82%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 75.93%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 76.05%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 76.16%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 76.27%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 76.38%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 76.46%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 76.39%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 76.41%   [EVAL] batch:  221 | acc: 56.25%,  total acc: 76.32%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 76.26%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 76.28%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 76.17%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 76.27%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 76.38%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 76.48%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 76.58%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 76.68%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 76.76%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 76.86%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 76.96%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 77.06%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 77.15%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 77.25%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 77.32%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 77.42%   [EVAL] batch:  238 | acc: 93.75%,  total acc: 77.48%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 77.55%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 77.65%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 77.71%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 77.80%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 77.87%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 77.96%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 78.00%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 78.06%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 78.15%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 78.21%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 78.27%   [EVAL] batch:  250 | acc: 25.00%,  total acc: 78.06%   [EVAL] batch:  251 | acc: 37.50%,  total acc: 77.90%   [EVAL] batch:  252 | acc: 31.25%,  total acc: 77.72%   [EVAL] batch:  253 | acc: 25.00%,  total acc: 77.51%   [EVAL] batch:  254 | acc: 43.75%,  total acc: 77.38%   [EVAL] batch:  255 | acc: 37.50%,  total acc: 77.22%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 77.14%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 77.16%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 77.15%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 77.09%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 76.96%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 76.93%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 77.00%   [EVAL] batch:  263 | acc: 62.50%,  total acc: 76.94%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 76.91%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 76.90%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 76.90%   [EVAL] batch:  267 | acc: 81.25%,  total acc: 76.91%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 76.95%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 76.99%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 77.16%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 77.22%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 77.26%   [EVAL] batch:  274 | acc: 93.75%,  total acc: 77.32%   [EVAL] batch:  275 | acc: 56.25%,  total acc: 77.24%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 77.08%   [EVAL] batch:  277 | acc: 43.75%,  total acc: 76.96%   [EVAL] batch:  278 | acc: 62.50%,  total acc: 76.90%   [EVAL] batch:  279 | acc: 37.50%,  total acc: 76.76%   [EVAL] batch:  280 | acc: 50.00%,  total acc: 76.67%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 76.62%   [EVAL] batch:  282 | acc: 68.75%,  total acc: 76.59%   [EVAL] batch:  283 | acc: 31.25%,  total acc: 76.43%   [EVAL] batch:  284 | acc: 43.75%,  total acc: 76.32%   [EVAL] batch:  285 | acc: 25.00%,  total acc: 76.14%   [EVAL] batch:  286 | acc: 43.75%,  total acc: 76.02%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 76.02%   [EVAL] batch:  288 | acc: 87.50%,  total acc: 76.06%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 76.10%   [EVAL] batch:  290 | acc: 81.25%,  total acc: 76.12%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 76.18%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 76.19%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 76.25%   [EVAL] batch:  294 | acc: 81.25%,  total acc: 76.27%   [EVAL] batch:  295 | acc: 81.25%,  total acc: 76.29%   [EVAL] batch:  296 | acc: 75.00%,  total acc: 76.28%   [EVAL] batch:  297 | acc: 68.75%,  total acc: 76.26%   [EVAL] batch:  298 | acc: 87.50%,  total acc: 76.30%   [EVAL] batch:  299 | acc: 87.50%,  total acc: 76.33%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 76.41%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 76.49%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 76.57%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 76.64%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 76.72%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 76.80%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 76.87%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 76.95%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 77.02%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 77.10%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 77.17%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 77.24%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 77.16%   
cur_acc:  ['0.9544', '0.7817', '0.7639', '0.8562', '0.7272']
his_acc:  ['0.9544', '0.8510', '0.8142', '0.8103', '0.7716']
CurrentTrain: epoch 15, batch     0 | loss: 12.3593870CurrentTrain: epoch 15, batch     1 | loss: 11.6528357CurrentTrain: epoch 15, batch     2 | loss: 15.7186724CurrentTrain: epoch  1, batch     3 | loss: 9.4675092CurrentTrain: epoch 15, batch     0 | loss: 8.3717066CurrentTrain: epoch 15, batch     1 | loss: 9.2735021CurrentTrain: epoch 15, batch     2 | loss: 13.0445843CurrentTrain: epoch  1, batch     3 | loss: 10.2217325CurrentTrain: epoch 15, batch     0 | loss: 7.0598173CurrentTrain: epoch 15, batch     1 | loss: 15.9583354CurrentTrain: epoch 15, batch     2 | loss: 9.7003523CurrentTrain: epoch  1, batch     3 | loss: 9.6615232CurrentTrain: epoch 15, batch     0 | loss: 9.7966785CurrentTrain: epoch 15, batch     1 | loss: 6.6663289CurrentTrain: epoch 15, batch     2 | loss: 11.3877216CurrentTrain: epoch  1, batch     3 | loss: 6.3695986CurrentTrain: epoch 15, batch     0 | loss: 9.1090270CurrentTrain: epoch 15, batch     1 | loss: 5.8996429CurrentTrain: epoch 15, batch     2 | loss: 12.8234261CurrentTrain: epoch  1, batch     3 | loss: 17.9384157CurrentTrain: epoch 15, batch     0 | loss: 7.7785525CurrentTrain: epoch 15, batch     1 | loss: 7.2101778CurrentTrain: epoch 15, batch     2 | loss: 10.5390850CurrentTrain: epoch  1, batch     3 | loss: 6.4545305CurrentTrain: epoch 15, batch     0 | loss: 7.2798609CurrentTrain: epoch 15, batch     1 | loss: 8.0186053CurrentTrain: epoch 15, batch     2 | loss: 7.8999234CurrentTrain: epoch  1, batch     3 | loss: 9.9984088CurrentTrain: epoch 15, batch     0 | loss: 12.5231286CurrentTrain: epoch 15, batch     1 | loss: 8.8075242CurrentTrain: epoch 15, batch     2 | loss: 6.5903533CurrentTrain: epoch  1, batch     3 | loss: 7.0124770CurrentTrain: epoch 15, batch     0 | loss: 12.2154916CurrentTrain: epoch 15, batch     1 | loss: 13.0407202CurrentTrain: epoch 15, batch     2 | loss: 11.2143905CurrentTrain: epoch  1, batch     3 | loss: 6.7104376CurrentTrain: epoch 15, batch     0 | loss: 8.1822630CurrentTrain: epoch 15, batch     1 | loss: 10.7662369CurrentTrain: epoch 15, batch     2 | loss: 6.7797897CurrentTrain: epoch  1, batch     3 | loss: 12.3430714
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: ( 1988 ) , " night watch " ( 2004 ) , " day watch " ( 2006 ) and " legend № 17 " ( 2013 ) .
Head Entity: day watch
Tail Entity: night watch
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: ( 1995 ) , " The Phantom Menace " ( 1999 ) , " Attack of the Clones " ( 2002 ) and " Revenge of the Sith " ( 2005 ) .  
Head Entity: Attack of the Clones  
Tail Entity: The Phantom Menace  

Relation: follows  
Context: ( 2010 ) , " The Hunger Games " ( 2012 ) , " Catching Fire " ( 2013 ) and " Mockingjay " ( 2014 ) .  
Head Entity: Catching Fire  
Tail Entity: The Hunger Games  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: it is primarily used for baseball and serves as the home stadium for yaquis de obregón .
Head Entity: yaquis de obregón
Tail Entity: baseball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: she has been a professional athlete in tennis since she was a teenager and has won several championships.  
Head Entity: she  
Tail Entity: tennis  

Relation: sport  
Context: the city is known for its vibrant soccer culture and is home to several professional teams.  
Head Entity: the city  
Tail Entity: soccer  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael was the father of lucas, who had recently graduated from university.  
Head Entity: lucas  
Tail Entity: michael  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: hana yori dango returns ( ) is a japanese television drama series , broadcast on tbs in 2007 .
Head Entity: hana yori dango returns
Tail Entity: tbs
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: the popular series breaking bad was originally aired on amc from 2008 to 2013.  
Head Entity: breaking bad  
Tail Entity: amc  

Relation: original network  
Context: the animated show avatar: the last airbender premiered on nickelodeon in 2005.  
Head Entity: avatar: the last airbender  
Tail Entity: nickelodeon  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: the 1954 film , directed by kadri venkata reddy , " peddamanushulu " was honoured with the first president 's silver medal for best feature film in telugu .
Head Entity: peddamanushulu
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: the 1994 animated film "The Lion King," produced by Walt Disney, was originally created in English before being dubbed into several other languages.  
Head Entity: The Lion King  
Tail Entity: English  

Relation: original language of film or TV show  
Context: the critically acclaimed 2001 film "Amélie," directed by Jean-Pierre Jeunet, was filmed in French and has since gained a global audience.  
Head Entity: Amélie  
Tail Entity: French  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: in the summer of 2013 he returned to montenegro and joined with fk rudar pljevlja , playing with the club for two seasons in the montenegrin first league .
Head Entity: fk rudar pljevlja
Tail Entity: montenegrin first league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: After a successful stint in the English Premier League, the player signed with FC Barcelona, competing in La Liga for the next three seasons.  
Head Entity: FC Barcelona  
Tail Entity: La Liga  

Relation: league  
Context: The young athlete made headlines when he joined the New York Yankees, showcasing his talent in Major League Baseball.  
Head Entity: New York Yankees  
Tail Entity: Major League Baseball  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: the first stewart king of scotland and son of marjorie bruce and walter stewart , robert ii , is believed to have been born in the abbey .
Head Entity: robert ii
Tail Entity: marjorie bruce
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: in the historical records, it is noted that queen elizabeth i was the daughter of anne boleyn, who served as her mother during her early years.  
Head Entity: queen elizabeth i  
Tail Entity: anne boleyn  

Relation: mother  
Context: the famous artist pablo picasso often credited his mother, maría ruiz, as a significant influence on his early artistic development.  
Head Entity: pablo picasso  
Tail Entity: maría ruiz  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres. he is particularly famous for his interpretations of classical cello concertos.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: fastjet tanzania maintains a head office in samora avenue , dar es salaam , tanzania .
Head Entity: fastjet
Tail Entity: tanzania
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the headquarters of the international organization is located in geneva, switzerland.  
Head Entity: international organization  
Tail Entity: switzerland  

Relation: country  
Context: the famous car manufacturer has its production facilities in munich, germany.  
Head Entity: car manufacturer  
Tail Entity: germany  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: klaus kinski , who would play dracula himself nine years later in " nosferatu the vampyre " , is also featured in the film as renfield .
Head Entity: nosferatu the vampyre
Tail Entity: renfield
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: in the animated series "avatar: the last airbender," zuko is a complex character who struggles with his identity and family legacy.  
Head Entity: avatar: the last airbender  
Tail Entity: zuko  

Relation: characters  
Context: the novel "pride and prejudice" features elizabeth bennet, a strong-willed protagonist who navigates societal expectations and personal relationships.  
Head Entity: pride and prejudice  
Tail Entity: elizabeth bennet  
MemoryTrain:  epoch 15, batch     0 | loss: 5.0514588MemoryTrain:  epoch 15, batch     1 | loss: 3.6060500MemoryTrain:  epoch 15, batch     2 | loss: 3.8445062MemoryTrain:  epoch 15, batch     3 | loss: 2.7406182MemoryTrain:  epoch 15, batch     4 | loss: 2.9719315MemoryTrain:  epoch 15, batch     5 | loss: 4.1121611MemoryTrain:  epoch 15, batch     6 | loss: 3.1543712MemoryTrain:  epoch 15, batch     7 | loss: 2.6200247MemoryTrain:  epoch 15, batch     8 | loss: 2.8154833MemoryTrain:  epoch 15, batch     9 | loss: 4.1386519MemoryTrain:  epoch 15, batch    10 | loss: 3.1086040MemoryTrain:  epoch  3, batch    11 | loss: 12.5658044MemoryTrain:  epoch 15, batch     0 | loss: 3.3881354MemoryTrain:  epoch 15, batch     1 | loss: 3.4304086MemoryTrain:  epoch 15, batch     2 | loss: 2.7140411MemoryTrain:  epoch 15, batch     3 | loss: 4.3014241MemoryTrain:  epoch 15, batch     4 | loss: 2.6459297MemoryTrain:  epoch 15, batch     5 | loss: 2.0192669MemoryTrain:  epoch 15, batch     6 | loss: 6.1348113MemoryTrain:  epoch 15, batch     7 | loss: 2.7596674MemoryTrain:  epoch 15, batch     8 | loss: 3.5481796MemoryTrain:  epoch 15, batch     9 | loss: 6.5853491MemoryTrain:  epoch 15, batch    10 | loss: 4.5311069MemoryTrain:  epoch  3, batch    11 | loss: 10.5445772MemoryTrain:  epoch 15, batch     0 | loss: 2.2945193MemoryTrain:  epoch 15, batch     1 | loss: 4.5326622MemoryTrain:  epoch 15, batch     2 | loss: 3.1156015MemoryTrain:  epoch 15, batch     3 | loss: 2.6659123MemoryTrain:  epoch 15, batch     4 | loss: 3.0198715MemoryTrain:  epoch 15, batch     5 | loss: 2.2517342MemoryTrain:  epoch 15, batch     6 | loss: 2.5687831MemoryTrain:  epoch 15, batch     7 | loss: 4.6663484MemoryTrain:  epoch 15, batch     8 | loss: 3.9493448MemoryTrain:  epoch 15, batch     9 | loss: 1.7029017MemoryTrain:  epoch 15, batch    10 | loss: 2.4984069MemoryTrain:  epoch  3, batch    11 | loss: 10.4358964MemoryTrain:  epoch 15, batch     0 | loss: 3.6330597MemoryTrain:  epoch 15, batch     1 | loss: 2.4852280MemoryTrain:  epoch 15, batch     2 | loss: 3.3662585MemoryTrain:  epoch 15, batch     3 | loss: 1.7805354MemoryTrain:  epoch 15, batch     4 | loss: 1.7704821MemoryTrain:  epoch 15, batch     5 | loss: 1.7523999MemoryTrain:  epoch 15, batch     6 | loss: 2.3036698MemoryTrain:  epoch 15, batch     7 | loss: 3.7555251MemoryTrain:  epoch 15, batch     8 | loss: 2.0359585MemoryTrain:  epoch 15, batch     9 | loss: 3.4686960MemoryTrain:  epoch 15, batch    10 | loss: 1.7436831MemoryTrain:  epoch  3, batch    11 | loss: 27.6580387MemoryTrain:  epoch 15, batch     0 | loss: 2.1253054MemoryTrain:  epoch 15, batch     1 | loss: 4.5964897MemoryTrain:  epoch 15, batch     2 | loss: 1.9102873MemoryTrain:  epoch 15, batch     3 | loss: 1.7762122MemoryTrain:  epoch 15, batch     4 | loss: 2.1247930MemoryTrain:  epoch 15, batch     5 | loss: 1.9339865MemoryTrain:  epoch 15, batch     6 | loss: 3.9185270MemoryTrain:  epoch 15, batch     7 | loss: 1.4032713MemoryTrain:  epoch 15, batch     8 | loss: 2.5480181MemoryTrain:  epoch 15, batch     9 | loss: 1.4497353MemoryTrain:  epoch 15, batch    10 | loss: 1.7155809MemoryTrain:  epoch  3, batch    11 | loss: 11.6000291MemoryTrain:  epoch 15, batch     0 | loss: 1.7975325MemoryTrain:  epoch 15, batch     1 | loss: 1.8982487MemoryTrain:  epoch 15, batch     2 | loss: 4.1493076MemoryTrain:  epoch 15, batch     3 | loss: 1.4009276MemoryTrain:  epoch 15, batch     4 | loss: 1.6809270MemoryTrain:  epoch 15, batch     5 | loss: 2.9560826MemoryTrain:  epoch 15, batch     6 | loss: 7.1085869MemoryTrain:  epoch 15, batch     7 | loss: 1.4855359MemoryTrain:  epoch 15, batch     8 | loss: 4.3070740MemoryTrain:  epoch 15, batch     9 | loss: 2.7285528MemoryTrain:  epoch 15, batch    10 | loss: 1.8446288MemoryTrain:  epoch  3, batch    11 | loss: 10.0724852MemoryTrain:  epoch 15, batch     0 | loss: 5.7789314MemoryTrain:  epoch 15, batch     1 | loss: 1.6355690MemoryTrain:  epoch 15, batch     2 | loss: 1.5015547MemoryTrain:  epoch 15, batch     3 | loss: 1.8967352MemoryTrain:  epoch 15, batch     4 | loss: 4.1264683MemoryTrain:  epoch 15, batch     5 | loss: 2.0100422MemoryTrain:  epoch 15, batch     6 | loss: 3.4720811MemoryTrain:  epoch 15, batch     7 | loss: 1.7133389MemoryTrain:  epoch 15, batch     8 | loss: 1.3598491MemoryTrain:  epoch 15, batch     9 | loss: 1.5791957MemoryTrain:  epoch 15, batch    10 | loss: 4.7905846MemoryTrain:  epoch  3, batch    11 | loss: 9.7011170MemoryTrain:  epoch 15, batch     0 | loss: 1.7147290MemoryTrain:  epoch 15, batch     1 | loss: 1.6763289MemoryTrain:  epoch 15, batch     2 | loss: 8.6013306MemoryTrain:  epoch 15, batch     3 | loss: 4.1571345MemoryTrain:  epoch 15, batch     4 | loss: 1.8501976MemoryTrain:  epoch 15, batch     5 | loss: 4.0794538MemoryTrain:  epoch 15, batch     6 | loss: 2.2106408MemoryTrain:  epoch 15, batch     7 | loss: 4.1412764MemoryTrain:  epoch 15, batch     8 | loss: 1.5198351MemoryTrain:  epoch 15, batch     9 | loss: 1.5927203MemoryTrain:  epoch 15, batch    10 | loss: 1.7121956MemoryTrain:  epoch  3, batch    11 | loss: 10.2741108MemoryTrain:  epoch 15, batch     0 | loss: 1.7389696MemoryTrain:  epoch 15, batch     1 | loss: 1.6192128MemoryTrain:  epoch 15, batch     2 | loss: 3.7543181MemoryTrain:  epoch 15, batch     3 | loss: 1.4825237MemoryTrain:  epoch 15, batch     4 | loss: 2.7070910MemoryTrain:  epoch 15, batch     5 | loss: 1.4679873MemoryTrain:  epoch 15, batch     6 | loss: 1.6263892MemoryTrain:  epoch 15, batch     7 | loss: 2.2103147MemoryTrain:  epoch 15, batch     8 | loss: 2.2445104MemoryTrain:  epoch 15, batch     9 | loss: 1.7536819MemoryTrain:  epoch 15, batch    10 | loss: 1.9859053MemoryTrain:  epoch  3, batch    11 | loss: 26.2946102MemoryTrain:  epoch 15, batch     0 | loss: 1.6807217MemoryTrain:  epoch 15, batch     1 | loss: 1.6882317MemoryTrain:  epoch 15, batch     2 | loss: 1.5692714MemoryTrain:  epoch 15, batch     3 | loss: 1.5002610MemoryTrain:  epoch 15, batch     4 | loss: 1.5573071MemoryTrain:  epoch 15, batch     5 | loss: 1.6149625MemoryTrain:  epoch 15, batch     6 | loss: 4.1664210MemoryTrain:  epoch 15, batch     7 | loss: 1.4890403MemoryTrain:  epoch 15, batch     8 | loss: 4.0507585MemoryTrain:  epoch 15, batch     9 | loss: 1.4532744MemoryTrain:  epoch 15, batch    10 | loss: 2.0674150MemoryTrain:  epoch  3, batch    11 | loss: 10.6092878
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 16.67%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 12.50%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 15.00%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 13.54%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 20.54%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 30.47%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 38.19%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 44.38%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 48.86%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 53.12%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 54.81%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 54.02%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 53.75%   [EVAL] batch:   15 | acc: 43.75%,  total acc: 53.12%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 54.41%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 55.21%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 55.26%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 57.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 59.23%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 60.80%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 65.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 66.11%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 66.90%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 67.63%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 68.53%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 68.96%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 69.56%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 70.51%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 71.40%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 72.06%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 72.68%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 73.26%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 73.99%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 74.01%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 74.20%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 73.91%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 73.02%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 72.77%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 72.24%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 72.16%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 72.78%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 73.23%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 73.80%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 74.35%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 74.74%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 74.39%   [EVAL] batch:   51 | acc: 12.50%,  total acc: 73.20%   [EVAL] batch:   52 | acc: 31.25%,  total acc: 72.41%   [EVAL] batch:   53 | acc: 6.25%,  total acc: 71.18%   [EVAL] batch:   54 | acc: 31.25%,  total acc: 70.45%   [EVAL] batch:   55 | acc: 31.25%,  total acc: 69.75%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 69.19%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 69.40%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 69.39%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 69.48%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 69.57%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 70.06%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 69.44%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 68.18%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 65.62%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 65.38%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 67.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 69.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 72.79%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 73.96%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 75.33%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 75.62%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 76.19%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 75.57%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 75.82%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 76.30%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 76.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.16%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.01%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 79.31%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.65%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 81.05%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.63%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 81.42%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 81.59%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 81.91%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 82.37%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 83.23%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 83.48%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 83.87%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 83.81%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 83.89%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 83.83%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 83.51%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 83.46%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 83.55%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 83.38%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 83.29%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 83.49%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 83.22%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 82.73%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 82.68%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 82.00%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 81.46%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 81.05%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 80.75%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 80.26%   [EVAL] batch:   63 | acc: 62.50%,  total acc: 79.98%   [EVAL] batch:   64 | acc: 50.00%,  total acc: 79.52%   [EVAL] batch:   65 | acc: 31.25%,  total acc: 78.79%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 78.17%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 77.67%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 77.45%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 77.77%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 78.08%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 78.25%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 78.55%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 78.45%   [EVAL] batch:   76 | acc: 18.75%,  total acc: 77.68%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 77.16%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 76.74%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 76.41%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 76.08%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 75.76%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 75.45%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 75.52%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 75.37%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 75.36%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 75.14%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 75.07%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 74.93%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 74.93%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 74.93%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 74.73%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 74.34%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 74.28%   [EVAL] batch:   95 | acc: 56.25%,  total acc: 74.09%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 73.97%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 73.92%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 73.93%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 73.81%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 73.70%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 73.71%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 73.73%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 73.86%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 73.87%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 74.06%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 73.95%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 73.55%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 73.17%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 72.90%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 72.52%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 72.32%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 72.29%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 72.53%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 72.77%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 73.01%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 73.24%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 73.46%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 73.58%   [EVAL] batch:  119 | acc: 75.00%,  total acc: 73.59%   [EVAL] batch:  120 | acc: 62.50%,  total acc: 73.50%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 73.57%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 73.53%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 73.54%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 73.60%   [EVAL] batch:  125 | acc: 43.75%,  total acc: 73.36%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 73.18%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 72.85%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 72.77%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 72.64%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 72.47%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 72.59%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 72.65%   [EVAL] batch:  133 | acc: 68.75%,  total acc: 72.62%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 72.73%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 72.84%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 72.95%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 72.53%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 72.23%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 71.85%   [EVAL] batch:  141 | acc: 31.25%,  total acc: 71.57%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 71.33%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 71.05%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 71.21%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 71.40%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 71.56%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 71.75%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 71.94%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 72.08%   [EVAL] batch:  150 | acc: 18.75%,  total acc: 71.73%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 71.34%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 70.92%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 70.62%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 70.24%   [EVAL] batch:  155 | acc: 12.50%,  total acc: 69.87%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 69.90%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 70.06%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 70.24%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 70.39%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 70.50%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 70.68%   [EVAL] batch:  162 | acc: 81.25%,  total acc: 70.74%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 70.73%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 70.76%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 70.67%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 70.77%   [EVAL] batch:  167 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 70.93%   [EVAL] batch:  169 | acc: 56.25%,  total acc: 70.85%   [EVAL] batch:  170 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:  171 | acc: 62.50%,  total acc: 70.78%   [EVAL] batch:  172 | acc: 62.50%,  total acc: 70.74%   [EVAL] batch:  173 | acc: 81.25%,  total acc: 70.80%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 70.86%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 70.85%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 70.80%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 70.79%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 70.81%   [EVAL] batch:  179 | acc: 81.25%,  total acc: 70.87%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 70.79%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 70.81%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 70.90%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 71.03%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 71.11%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 71.24%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 71.29%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 71.41%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 71.53%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 71.83%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:  193 | acc: 87.50%,  total acc: 72.10%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 72.12%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 72.00%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 72.02%   [EVAL] batch:  197 | acc: 37.50%,  total acc: 71.84%   [EVAL] batch:  198 | acc: 56.25%,  total acc: 71.77%   [EVAL] batch:  199 | acc: 56.25%,  total acc: 71.69%   [EVAL] batch:  200 | acc: 87.50%,  total acc: 71.77%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 71.84%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 71.89%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:  204 | acc: 50.00%,  total acc: 71.77%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 71.78%   [EVAL] batch:  206 | acc: 43.75%,  total acc: 71.65%   [EVAL] batch:  207 | acc: 50.00%,  total acc: 71.54%   [EVAL] batch:  208 | acc: 62.50%,  total acc: 71.50%   [EVAL] batch:  209 | acc: 43.75%,  total acc: 71.37%   [EVAL] batch:  210 | acc: 50.00%,  total acc: 71.27%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 71.08%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 71.07%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 71.20%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 71.34%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 71.47%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 71.60%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 71.73%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 71.83%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 71.82%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 71.86%   [EVAL] batch:  221 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 71.86%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 71.90%   [EVAL] batch:  224 | acc: 43.75%,  total acc: 71.78%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 71.90%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 72.03%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 72.15%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 72.39%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 72.48%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 72.60%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 72.72%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 72.84%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 72.95%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 73.07%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 73.15%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 73.27%   [EVAL] batch:  238 | acc: 93.75%,  total acc: 73.35%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 73.44%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 73.55%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 73.61%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 73.69%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 73.77%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 73.88%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 73.93%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 74.01%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 74.12%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 74.20%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 74.28%   [EVAL] batch:  250 | acc: 50.00%,  total acc: 74.18%   [EVAL] batch:  251 | acc: 37.50%,  total acc: 74.03%   [EVAL] batch:  252 | acc: 50.00%,  total acc: 73.94%   [EVAL] batch:  253 | acc: 31.25%,  total acc: 73.77%   [EVAL] batch:  254 | acc: 50.00%,  total acc: 73.68%   [EVAL] batch:  255 | acc: 43.75%,  total acc: 73.56%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 73.47%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 73.45%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 73.43%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 73.41%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 73.30%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 73.28%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 73.36%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 73.30%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 73.25%   [EVAL] batch:  265 | acc: 68.75%,  total acc: 73.24%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 73.24%   [EVAL] batch:  267 | acc: 75.00%,  total acc: 73.25%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 73.30%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 73.38%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 73.48%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 73.58%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 73.65%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 73.72%   [EVAL] batch:  274 | acc: 93.75%,  total acc: 73.80%   [EVAL] batch:  275 | acc: 56.25%,  total acc: 73.73%   [EVAL] batch:  276 | acc: 37.50%,  total acc: 73.60%   [EVAL] batch:  277 | acc: 62.50%,  total acc: 73.56%   [EVAL] batch:  278 | acc: 56.25%,  total acc: 73.50%   [EVAL] batch:  279 | acc: 43.75%,  total acc: 73.39%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 73.29%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 73.23%   [EVAL] batch:  282 | acc: 50.00%,  total acc: 73.14%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 72.91%   [EVAL] batch:  284 | acc: 18.75%,  total acc: 72.72%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 72.51%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 72.28%   [EVAL] batch:  287 | acc: 56.25%,  total acc: 72.22%   [EVAL] batch:  288 | acc: 87.50%,  total acc: 72.28%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 72.33%   [EVAL] batch:  290 | acc: 81.25%,  total acc: 72.36%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 72.43%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 72.46%   [EVAL] batch:  293 | acc: 100.00%,  total acc: 72.56%   [EVAL] batch:  294 | acc: 81.25%,  total acc: 72.58%   [EVAL] batch:  295 | acc: 62.50%,  total acc: 72.55%   [EVAL] batch:  296 | acc: 43.75%,  total acc: 72.45%   [EVAL] batch:  297 | acc: 62.50%,  total acc: 72.42%   [EVAL] batch:  298 | acc: 56.25%,  total acc: 72.37%   [EVAL] batch:  299 | acc: 62.50%,  total acc: 72.33%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 72.43%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 72.52%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 72.61%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 72.70%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 72.79%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 72.88%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 72.94%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 72.99%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 73.04%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 73.12%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 73.19%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 73.28%   [EVAL] batch:  312 | acc: 62.50%,  total acc: 73.24%   [EVAL] batch:  313 | acc: 12.50%,  total acc: 73.05%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 72.88%   [EVAL] batch:  315 | acc: 6.25%,  total acc: 72.67%   [EVAL] batch:  316 | acc: 12.50%,  total acc: 72.48%   [EVAL] batch:  317 | acc: 18.75%,  total acc: 72.31%   [EVAL] batch:  318 | acc: 18.75%,  total acc: 72.14%   [EVAL] batch:  319 | acc: 93.75%,  total acc: 72.21%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 72.29%   [EVAL] batch:  321 | acc: 100.00%,  total acc: 72.38%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 72.45%   [EVAL] batch:  323 | acc: 100.00%,  total acc: 72.53%   [EVAL] batch:  324 | acc: 100.00%,  total acc: 72.62%   [EVAL] batch:  325 | acc: 56.25%,  total acc: 72.57%   [EVAL] batch:  326 | acc: 31.25%,  total acc: 72.44%   [EVAL] batch:  327 | acc: 43.75%,  total acc: 72.35%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 72.36%   [EVAL] batch:  329 | acc: 75.00%,  total acc: 72.37%   [EVAL] batch:  330 | acc: 43.75%,  total acc: 72.28%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 72.33%   [EVAL] batch:  332 | acc: 93.75%,  total acc: 72.39%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 72.47%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 72.54%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 72.62%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 72.70%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 72.74%   [EVAL] batch:  338 | acc: 81.25%,  total acc: 72.77%   [EVAL] batch:  339 | acc: 87.50%,  total acc: 72.81%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 72.89%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 72.90%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 72.98%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 73.02%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 73.10%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 73.18%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 73.22%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 73.28%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 73.35%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 73.43%   [EVAL] batch:  350 | acc: 62.50%,  total acc: 73.40%   [EVAL] batch:  351 | acc: 68.75%,  total acc: 73.38%   [EVAL] batch:  352 | acc: 56.25%,  total acc: 73.34%   [EVAL] batch:  353 | acc: 50.00%,  total acc: 73.27%   [EVAL] batch:  354 | acc: 50.00%,  total acc: 73.20%   [EVAL] batch:  355 | acc: 62.50%,  total acc: 73.17%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 73.21%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 73.27%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 73.35%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 73.42%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 73.49%   [EVAL] batch:  361 | acc: 81.25%,  total acc: 73.52%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 73.48%   [EVAL] batch:  363 | acc: 37.50%,  total acc: 73.39%   [EVAL] batch:  364 | acc: 25.00%,  total acc: 73.25%   [EVAL] batch:  365 | acc: 12.50%,  total acc: 73.09%   [EVAL] batch:  366 | acc: 18.75%,  total acc: 72.94%   [EVAL] batch:  367 | acc: 37.50%,  total acc: 72.84%   [EVAL] batch:  368 | acc: 25.00%,  total acc: 72.71%   [EVAL] batch:  369 | acc: 75.00%,  total acc: 72.72%   [EVAL] batch:  370 | acc: 68.75%,  total acc: 72.71%   [EVAL] batch:  371 | acc: 62.50%,  total acc: 72.68%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 72.70%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 72.74%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 72.77%   
cur_acc:  ['0.9544', '0.7817', '0.7639', '0.8562', '0.7272', '0.6944']
his_acc:  ['0.9544', '0.8510', '0.8142', '0.8103', '0.7716', '0.7277']
CurrentTrain: epoch 15, batch     0 | loss: 12.3230578CurrentTrain: epoch 15, batch     1 | loss: 13.9383841CurrentTrain: epoch 15, batch     2 | loss: 14.0221587CurrentTrain: epoch  1, batch     3 | loss: 12.1362957CurrentTrain: epoch 15, batch     0 | loss: 18.7099302CurrentTrain: epoch 15, batch     1 | loss: 10.6791847CurrentTrain: epoch 15, batch     2 | loss: 14.8039061CurrentTrain: epoch  1, batch     3 | loss: 18.2541669CurrentTrain: epoch 15, batch     0 | loss: 10.2352670CurrentTrain: epoch 15, batch     1 | loss: 10.7470147CurrentTrain: epoch 15, batch     2 | loss: 8.6580977CurrentTrain: epoch  1, batch     3 | loss: 7.3821343CurrentTrain: epoch 15, batch     0 | loss: 9.9995181CurrentTrain: epoch 15, batch     1 | loss: 9.0303944CurrentTrain: epoch 15, batch     2 | loss: 6.8547860CurrentTrain: epoch  1, batch     3 | loss: 9.0177031CurrentTrain: epoch 15, batch     0 | loss: 10.4008639CurrentTrain: epoch 15, batch     1 | loss: 7.2024726CurrentTrain: epoch 15, batch     2 | loss: 6.8776190CurrentTrain: epoch  1, batch     3 | loss: 9.3386682CurrentTrain: epoch 15, batch     0 | loss: 10.0917171CurrentTrain: epoch 15, batch     1 | loss: 11.1916094CurrentTrain: epoch 15, batch     2 | loss: 9.7846592CurrentTrain: epoch  1, batch     3 | loss: 8.6897440CurrentTrain: epoch 15, batch     0 | loss: 15.8193369CurrentTrain: epoch 15, batch     1 | loss: 7.9726234CurrentTrain: epoch 15, batch     2 | loss: 13.1686026CurrentTrain: epoch  1, batch     3 | loss: 7.7491736CurrentTrain: epoch 15, batch     0 | loss: 7.3069549CurrentTrain: epoch 15, batch     1 | loss: 7.5668065CurrentTrain: epoch 15, batch     2 | loss: 13.2369202CurrentTrain: epoch  1, batch     3 | loss: 7.6634913CurrentTrain: epoch 15, batch     0 | loss: 14.4627553CurrentTrain: epoch 15, batch     1 | loss: 7.9825236CurrentTrain: epoch 15, batch     2 | loss: 13.6766777CurrentTrain: epoch  1, batch     3 | loss: 5.9863151CurrentTrain: epoch 15, batch     0 | loss: 7.4090472CurrentTrain: epoch 15, batch     1 | loss: 6.1771677CurrentTrain: epoch 15, batch     2 | loss: 9.4631079CurrentTrain: epoch  1, batch     3 | loss: 9.0890738
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: it is situated in the historic lower lusatia region , about south of beeskow , and north of cottbus .
Head Entity: cottbus
Tail Entity: lower lusatia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The ancient ruins are found near the banks of the river Nile, surrounded by vast deserts and fertile lands.  
Head Entity: Nile  
Tail Entity: deserts  

Relation: located on terrain feature  
Context: The village is nestled in the foothills of the Rocky Mountains, offering breathtaking views of the surrounding landscape.  
Head Entity: village  
Tail Entity: Rocky Mountains  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, also known as cop26, took place in glasgow, scotland, where numerous world leaders gathered to discuss climate action.  
Head Entity: united nations climate change conference  
Tail Entity: cop26  

Relation: participant of  
Context: elon musk, the ceo of spacex, was a key participant in the 2021 met gala, showcasing his vision for sustainable energy and space exploration.  
Head Entity: elon musk  
Tail Entity: 2021 met gala  
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: the festival opened with " whiplash " directed by damien chazelle and closed with musical drama " rudderless " directed by william h. macy .
Head Entity: whiplash
Tail Entity: damien chazelle
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: the critically acclaimed series "breaking bad" was brought to life by director vince gilligan, while the spin-off "better call saul" was directed by peter gould.  
Head Entity: breaking bad  
Tail Entity: vince gilligan  

Relation: director  
Context: the animated feature "spider-man: into the spider-verse" was directed by bob persichetti, peter ramsey, and rodney rothman, showcasing a unique visual style.  
Head Entity: spider-man: into the spider-verse  
Tail Entity: bob persichetti  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: tata prima is a range of heavy trucks produced by tata daewoo , a wholly owned subsidiary of tata motors of india .
Head Entity: tata daewoo
Tail Entity: tata motors
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: The famous luxury brand Gucci is owned by the French conglomerate Kering, which also owns several other high-end fashion labels.  
Head Entity: Gucci  
Tail Entity: Kering  

Relation: owned by  
Context: The popular social media platform Instagram is owned by Meta Platforms, Inc., which also owns Facebook and WhatsApp.  
Head Entity: Instagram  
Tail Entity: Meta Platforms, Inc.  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: thessaly was home to extensive neolithic and chalcolithic cultures around 6000 bc–2500 bc ( see cardium pottery , dimini and sesklo ) .
Head Entity: cardium pottery
Tail Entity: neolithic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The engine is a crucial part of the car's overall functionality, providing the necessary power to drive.  
Head Entity: engine  
Tail Entity: car  

Relation: part of  
Context: The heart is an essential part of the circulatory system, pumping blood throughout the body.  
Head Entity: heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: it served as the home of k league side gyeongnam fc and national league side changwon city fc prior to the construction of the changwon football center in 2009 .
Head Entity: changwon football center
Tail Entity: gyeongnam fc
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was once the residence of the famous artist, who transformed it into a vibrant studio before it became a gallery for local artists.  
Head Entity: historic building  
Tail Entity: famous artist  

Relation: occupant  
Context: After the renovation, the old factory was repurposed into a modern office space, housing several tech startups and creative agencies.  
Head Entity: old factory  
Tail Entity: tech startups  
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: he also commissioned renowned architect richard neutra to design the iconic desert kaufmann house ( 1946 ) in palm springs , california .
Head Entity: kaufmann house
Tail Entity: richard neutra
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: the innovative design of the new city library was created by the famous architect zaha hadid, known for her futuristic structures.  
Head Entity: city library  
Tail Entity: zaha hadid  

Relation: architect  
Context: after years of planning, the historic renovation of the old courthouse was finally completed, thanks to the talented architect frank gehry.  
Head Entity: old courthouse  
Tail Entity: frank gehry  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: in 2001 brian haw set up the parliament square peace campaign outside the houses of parliament in london .
Head Entity: brian haw
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: After moving to New York City in 2010, Sarah found her dream apartment in Brooklyn.  
Head Entity: Sarah  
Tail Entity: Brooklyn  

Relation: residence  
Context: The famous author lived in a quaint cottage in the countryside for many years before relocating to the city.  
Head Entity: The famous author  
Tail Entity: the city  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: he also took part in recordings of several gilbert and sullivan operattas as well as edward german 's " merrie england " under the direction of joe batten .
Head Entity: merrie england
Tail Entity: edward german
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: Ludwig van Beethoven is widely regarded as one of the greatest composers in the history of Western music, known for his symphonies and sonatas.  
Head Entity: Ludwig van Beethoven  
Tail Entity: symphonies  

Relation: composer  
Context: The famous opera "Carmen" was composed by Georges Bizet, who infused the work with rich melodies and dramatic flair.  
Head Entity: Carmen  
Tail Entity: Georges Bizet  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: one polish archer qualified for the women 's individual recurve by obtaining one of the eight olympic places available from the 2015 world archery championships in copenhagen , denmark .
Head Entity: 2015 world archery championships
Tail Entity: copenhagen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: The annual music festival will take place in the vibrant city of Austin, Texas, attracting thousands of fans from around the world.  
Head Entity: annual music festival  
Tail Entity: Austin, Texas  

Relation: location  
Context: The historic battle of Gettysburg was fought in the small town of Gettysburg, Pennsylvania, which is now a popular tourist destination.  
Head Entity: battle of Gettysburg  
Tail Entity: Gettysburg, Pennsylvania  
MemoryTrain:  epoch 15, batch     0 | loss: 3.7524971MemoryTrain:  epoch 15, batch     1 | loss: 2.1910637MemoryTrain:  epoch 15, batch     2 | loss: 2.9198087MemoryTrain:  epoch 15, batch     3 | loss: 2.4874598MemoryTrain:  epoch 15, batch     4 | loss: 3.3147094MemoryTrain:  epoch 15, batch     5 | loss: 2.9028802MemoryTrain:  epoch 15, batch     6 | loss: 2.6906592MemoryTrain:  epoch 15, batch     7 | loss: 4.9819530MemoryTrain:  epoch 15, batch     8 | loss: 4.1066251MemoryTrain:  epoch 15, batch     9 | loss: 2.9687954MemoryTrain:  epoch 15, batch    10 | loss: 5.1960667MemoryTrain:  epoch 15, batch    11 | loss: 5.4495339MemoryTrain:  epoch 15, batch    12 | loss: 2.7292258MemoryTrain:  epoch  1, batch    13 | loss: 6.4929179MemoryTrain:  epoch 15, batch     0 | loss: 2.4681140MemoryTrain:  epoch 15, batch     1 | loss: 2.7352136MemoryTrain:  epoch 15, batch     2 | loss: 2.8021186MemoryTrain:  epoch 15, batch     3 | loss: 1.9927769MemoryTrain:  epoch 15, batch     4 | loss: 2.7616036MemoryTrain:  epoch 15, batch     5 | loss: 2.2957547MemoryTrain:  epoch 15, batch     6 | loss: 4.4938014MemoryTrain:  epoch 15, batch     7 | loss: 3.3679608MemoryTrain:  epoch 15, batch     8 | loss: 3.6886033MemoryTrain:  epoch 15, batch     9 | loss: 1.5925928MemoryTrain:  epoch 15, batch    10 | loss: 3.8516678MemoryTrain:  epoch 15, batch    11 | loss: 5.2515541MemoryTrain:  epoch 15, batch    12 | loss: 2.3516177MemoryTrain:  epoch  1, batch    13 | loss: 6.6883568MemoryTrain:  epoch 15, batch     0 | loss: 2.5999035MemoryTrain:  epoch 15, batch     1 | loss: 2.2671878MemoryTrain:  epoch 15, batch     2 | loss: 2.5923554MemoryTrain:  epoch 15, batch     3 | loss: 2.0832957MemoryTrain:  epoch 15, batch     4 | loss: 1.9012804MemoryTrain:  epoch 15, batch     5 | loss: 2.4786384MemoryTrain:  epoch 15, batch     6 | loss: 2.5264190MemoryTrain:  epoch 15, batch     7 | loss: 2.0653342MemoryTrain:  epoch 15, batch     8 | loss: 2.2387438MemoryTrain:  epoch 15, batch     9 | loss: 2.4468345MemoryTrain:  epoch 15, batch    10 | loss: 1.5075378MemoryTrain:  epoch 15, batch    11 | loss: 1.8516726MemoryTrain:  epoch 15, batch    12 | loss: 1.7373345MemoryTrain:  epoch  1, batch    13 | loss: 9.3833563MemoryTrain:  epoch 15, batch     0 | loss: 2.7341411MemoryTrain:  epoch 15, batch     1 | loss: 2.1544843MemoryTrain:  epoch 15, batch     2 | loss: 1.8283453MemoryTrain:  epoch 15, batch     3 | loss: 2.1261375MemoryTrain:  epoch 15, batch     4 | loss: 3.7864252MemoryTrain:  epoch 15, batch     5 | loss: 4.0844435MemoryTrain:  epoch 15, batch     6 | loss: 1.9473782MemoryTrain:  epoch 15, batch     7 | loss: 3.0273406MemoryTrain:  epoch 15, batch     8 | loss: 4.5131179MemoryTrain:  epoch 15, batch     9 | loss: 2.7862327MemoryTrain:  epoch 15, batch    10 | loss: 1.8572634MemoryTrain:  epoch 15, batch    11 | loss: 2.5492184MemoryTrain:  epoch 15, batch    12 | loss: 1.5885615MemoryTrain:  epoch  1, batch    13 | loss: 5.7108600MemoryTrain:  epoch 15, batch     0 | loss: 4.0579943MemoryTrain:  epoch 15, batch     1 | loss: 4.3120043MemoryTrain:  epoch 15, batch     2 | loss: 1.7508989MemoryTrain:  epoch 15, batch     3 | loss: 2.0693967MemoryTrain:  epoch 15, batch     4 | loss: 1.5629930MemoryTrain:  epoch 15, batch     5 | loss: 2.0656548MemoryTrain:  epoch 15, batch     6 | loss: 2.2204342MemoryTrain:  epoch 15, batch     7 | loss: 1.8535070MemoryTrain:  epoch 15, batch     8 | loss: 2.2103951MemoryTrain:  epoch 15, batch     9 | loss: 3.8591546MemoryTrain:  epoch 15, batch    10 | loss: 2.7348573MemoryTrain:  epoch 15, batch    11 | loss: 1.7468097MemoryTrain:  epoch 15, batch    12 | loss: 1.5893311MemoryTrain:  epoch  1, batch    13 | loss: 5.6998488MemoryTrain:  epoch 15, batch     0 | loss: 2.2625188MemoryTrain:  epoch 15, batch     1 | loss: 1.8910026MemoryTrain:  epoch 15, batch     2 | loss: 2.6398019MemoryTrain:  epoch 15, batch     3 | loss: 1.7850267MemoryTrain:  epoch 15, batch     4 | loss: 1.6369256MemoryTrain:  epoch 15, batch     5 | loss: 2.2308861MemoryTrain:  epoch 15, batch     6 | loss: 1.6935073MemoryTrain:  epoch 15, batch     7 | loss: 1.4491843MemoryTrain:  epoch 15, batch     8 | loss: 2.0429387MemoryTrain:  epoch 15, batch     9 | loss: 1.7371372MemoryTrain:  epoch 15, batch    10 | loss: 2.5933009MemoryTrain:  epoch 15, batch    11 | loss: 1.8761095MemoryTrain:  epoch 15, batch    12 | loss: 1.8597518MemoryTrain:  epoch  1, batch    13 | loss: 7.5004265MemoryTrain:  epoch 15, batch     0 | loss: 1.8842856MemoryTrain:  epoch 15, batch     1 | loss: 3.8499987MemoryTrain:  epoch 15, batch     2 | loss: 1.7021741MemoryTrain:  epoch 15, batch     3 | loss: 2.1714282MemoryTrain:  epoch 15, batch     4 | loss: 1.4263836MemoryTrain:  epoch 15, batch     5 | loss: 2.0595742MemoryTrain:  epoch 15, batch     6 | loss: 1.7317761MemoryTrain:  epoch 15, batch     7 | loss: 1.6200314MemoryTrain:  epoch 15, batch     8 | loss: 1.5292829MemoryTrain:  epoch 15, batch     9 | loss: 1.9360437MemoryTrain:  epoch 15, batch    10 | loss: 1.7529918MemoryTrain:  epoch 15, batch    11 | loss: 1.9431003MemoryTrain:  epoch 15, batch    12 | loss: 3.7629475MemoryTrain:  epoch  1, batch    13 | loss: 5.6148141MemoryTrain:  epoch 15, batch     0 | loss: 1.6786648MemoryTrain:  epoch 15, batch     1 | loss: 2.9784202MemoryTrain:  epoch 15, batch     2 | loss: 1.5335038MemoryTrain:  epoch 15, batch     3 | loss: 1.9305205MemoryTrain:  epoch 15, batch     4 | loss: 2.6846837MemoryTrain:  epoch 15, batch     5 | loss: 1.5579014MemoryTrain:  epoch 15, batch     6 | loss: 3.9286825MemoryTrain:  epoch 15, batch     7 | loss: 4.4595685MemoryTrain:  epoch 15, batch     8 | loss: 2.4608929MemoryTrain:  epoch 15, batch     9 | loss: 4.0109827MemoryTrain:  epoch 15, batch    10 | loss: 1.4295257MemoryTrain:  epoch 15, batch    11 | loss: 1.3337382MemoryTrain:  epoch 15, batch    12 | loss: 1.5513615MemoryTrain:  epoch  1, batch    13 | loss: 7.1097420MemoryTrain:  epoch 15, batch     0 | loss: 1.6634932MemoryTrain:  epoch 15, batch     1 | loss: 1.5170952MemoryTrain:  epoch 15, batch     2 | loss: 1.7379664MemoryTrain:  epoch 15, batch     3 | loss: 1.8633495MemoryTrain:  epoch 15, batch     4 | loss: 1.5196318MemoryTrain:  epoch 15, batch     5 | loss: 1.4975182MemoryTrain:  epoch 15, batch     6 | loss: 1.4869524MemoryTrain:  epoch 15, batch     7 | loss: 1.6697506MemoryTrain:  epoch 15, batch     8 | loss: 1.3627856MemoryTrain:  epoch 15, batch     9 | loss: 1.4737491MemoryTrain:  epoch 15, batch    10 | loss: 2.2856217MemoryTrain:  epoch 15, batch    11 | loss: 3.8956723MemoryTrain:  epoch 15, batch    12 | loss: 2.3638424MemoryTrain:  epoch  1, batch    13 | loss: 5.8622331MemoryTrain:  epoch 15, batch     0 | loss: 3.5833352MemoryTrain:  epoch 15, batch     1 | loss: 2.1995440MemoryTrain:  epoch 15, batch     2 | loss: 1.6518796MemoryTrain:  epoch 15, batch     3 | loss: 1.5702198MemoryTrain:  epoch 15, batch     4 | loss: 3.7248569MemoryTrain:  epoch 15, batch     5 | loss: 1.5174667MemoryTrain:  epoch 15, batch     6 | loss: 1.4482958MemoryTrain:  epoch 15, batch     7 | loss: 2.3652894MemoryTrain:  epoch 15, batch     8 | loss: 1.5687905MemoryTrain:  epoch 15, batch     9 | loss: 2.0807274MemoryTrain:  epoch 15, batch    10 | loss: 3.8929097MemoryTrain:  epoch 15, batch    11 | loss: 1.9084552MemoryTrain:  epoch 15, batch    12 | loss: 2.6079348MemoryTrain:  epoch  1, batch    13 | loss: 5.2675613
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 18.75%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 17.19%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 17.50%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 17.71%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 25.00%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 33.59%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 40.28%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 45.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 49.43%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 52.08%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 54.33%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 55.80%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 57.50%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 59.38%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 60.66%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 62.15%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 61.84%   [EVAL] batch:   19 | acc: 18.75%,  total acc: 59.69%   [EVAL] batch:   20 | acc: 18.75%,  total acc: 57.74%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 56.82%   [EVAL] batch:   22 | acc: 37.50%,  total acc: 55.98%   [EVAL] batch:   23 | acc: 37.50%,  total acc: 55.21%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 54.00%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 52.16%   [EVAL] batch:   26 | acc: 18.75%,  total acc: 50.93%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 49.11%   [EVAL] batch:   28 | acc: 12.50%,  total acc: 47.84%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 46.46%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 45.16%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 45.90%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 47.16%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 47.98%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 49.11%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 49.83%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 50.34%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 51.32%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 52.24%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 53.28%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 54.27%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 55.06%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 56.10%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 56.82%   [EVAL] batch:   44 | acc: 18.75%,  total acc: 55.97%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 55.57%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 55.05%   [EVAL] batch:   47 | acc: 12.50%,  total acc: 54.17%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 53.70%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 52.88%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 53.06%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 53.85%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 54.01%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 54.51%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 55.11%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 55.58%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 55.48%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 55.17%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 55.08%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 55.21%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 55.02%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 55.14%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 54.56%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 67.36%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 64.38%   [EVAL] batch:   10 | acc: 18.75%,  total acc: 60.23%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 57.81%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 58.17%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 60.71%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 62.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 65.23%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 67.28%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 70.39%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 70.62%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 71.43%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 71.02%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 70.65%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 71.09%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 71.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.12%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 73.15%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 74.78%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 75.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 76.41%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 76.95%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 77.65%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 77.21%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 77.32%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 77.60%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 77.87%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 78.29%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 78.85%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 79.88%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 80.52%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 80.54%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 80.56%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 80.43%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 80.32%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 80.08%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 80.23%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 80.25%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 80.27%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 80.29%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 80.54%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 80.32%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 79.89%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 79.91%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 79.93%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 79.31%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 79.03%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 78.75%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 78.69%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 78.43%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 77.98%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 77.83%   [EVAL] batch:   64 | acc: 50.00%,  total acc: 77.40%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 76.80%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 76.31%   [EVAL] batch:   67 | acc: 62.50%,  total acc: 76.10%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 75.91%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 76.25%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 76.58%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 76.65%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 76.97%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 77.28%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 77.50%   [EVAL] batch:   75 | acc: 43.75%,  total acc: 77.06%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 76.54%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 76.04%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 75.63%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 75.39%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 75.08%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 74.92%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 74.55%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 74.70%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 74.56%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 74.49%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 74.35%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 74.22%   [EVAL] batch:   88 | acc: 43.75%,  total acc: 73.88%   [EVAL] batch:   89 | acc: 43.75%,  total acc: 73.54%   [EVAL] batch:   90 | acc: 50.00%,  total acc: 73.28%   [EVAL] batch:   91 | acc: 31.25%,  total acc: 72.83%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 72.65%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 72.21%   [EVAL] batch:   94 | acc: 62.50%,  total acc: 72.11%   [EVAL] batch:   95 | acc: 56.25%,  total acc: 71.94%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 71.84%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 71.75%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 71.78%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 71.56%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 71.47%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 71.45%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 71.48%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 71.63%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 71.61%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 71.82%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 71.73%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 71.35%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 70.99%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 70.74%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 70.38%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 70.20%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 70.19%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 70.45%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 70.71%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 70.96%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 71.21%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 71.45%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 71.59%   [EVAL] batch:  119 | acc: 75.00%,  total acc: 71.61%   [EVAL] batch:  120 | acc: 56.25%,  total acc: 71.49%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 71.57%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 71.54%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 71.57%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 71.55%   [EVAL] batch:  125 | acc: 43.75%,  total acc: 71.33%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 71.01%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 70.70%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 70.59%   [EVAL] batch:  129 | acc: 37.50%,  total acc: 70.34%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 70.09%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 70.17%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 70.30%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 70.38%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 70.51%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 70.63%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 70.76%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 70.70%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 70.32%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 70.00%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 69.55%   [EVAL] batch:  141 | acc: 12.50%,  total acc: 69.15%   [EVAL] batch:  142 | acc: 18.75%,  total acc: 68.79%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 68.53%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 68.71%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 68.92%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 69.09%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 69.30%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 69.51%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 69.67%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 69.29%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 68.87%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 68.42%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 68.10%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 67.78%   [EVAL] batch:  155 | acc: 12.50%,  total acc: 67.43%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 67.44%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 67.60%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 67.77%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 67.93%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 68.05%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 68.21%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 68.14%   [EVAL] batch:  163 | acc: 18.75%,  total acc: 67.84%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 67.77%   [EVAL] batch:  165 | acc: 18.75%,  total acc: 67.47%   [EVAL] batch:  166 | acc: 31.25%,  total acc: 67.25%   [EVAL] batch:  167 | acc: 37.50%,  total acc: 67.08%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 67.01%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 66.99%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 67.03%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 67.04%   [EVAL] batch:  172 | acc: 62.50%,  total acc: 67.02%   [EVAL] batch:  173 | acc: 87.50%,  total acc: 67.13%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 67.25%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 67.26%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 67.20%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 67.10%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 67.07%   [EVAL] batch:  179 | acc: 81.25%,  total acc: 67.15%   [EVAL] batch:  180 | acc: 31.25%,  total acc: 66.95%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 67.00%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 67.11%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 67.26%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 67.40%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 67.57%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 67.65%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 67.79%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 67.92%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 68.09%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 68.26%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 68.33%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 68.49%   [EVAL] batch:  193 | acc: 87.50%,  total acc: 68.59%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 68.59%   [EVAL] batch:  195 | acc: 56.25%,  total acc: 68.53%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 68.50%   [EVAL] batch:  197 | acc: 37.50%,  total acc: 68.34%   [EVAL] batch:  198 | acc: 62.50%,  total acc: 68.31%   [EVAL] batch:  199 | acc: 50.00%,  total acc: 68.22%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 68.19%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 68.25%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 68.23%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 68.17%   [EVAL] batch:  204 | acc: 50.00%,  total acc: 68.08%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 68.02%   [EVAL] batch:  206 | acc: 50.00%,  total acc: 67.93%   [EVAL] batch:  207 | acc: 43.75%,  total acc: 67.82%   [EVAL] batch:  208 | acc: 56.25%,  total acc: 67.76%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 67.56%   [EVAL] batch:  210 | acc: 43.75%,  total acc: 67.45%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 67.28%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 67.25%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 67.41%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 67.56%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 67.71%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 67.86%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 68.00%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 68.12%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 68.10%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 68.16%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 68.22%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 68.22%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 68.28%   [EVAL] batch:  224 | acc: 43.75%,  total acc: 68.17%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 68.31%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 68.59%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 68.72%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 68.86%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 68.97%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 69.10%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 69.23%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 69.36%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 69.49%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 69.62%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 69.73%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 69.85%   [EVAL] batch:  238 | acc: 93.75%,  total acc: 69.95%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 70.05%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 70.18%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 70.25%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 70.34%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 70.44%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 70.56%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 70.63%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 70.72%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 70.84%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 70.93%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 71.03%   [EVAL] batch:  250 | acc: 25.00%,  total acc: 70.84%   [EVAL] batch:  251 | acc: 37.50%,  total acc: 70.71%   [EVAL] batch:  252 | acc: 43.75%,  total acc: 70.60%   [EVAL] batch:  253 | acc: 31.25%,  total acc: 70.45%   [EVAL] batch:  254 | acc: 50.00%,  total acc: 70.37%   [EVAL] batch:  255 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 70.23%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 70.23%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 70.22%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 70.22%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 70.11%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 70.11%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 70.20%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 70.12%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 70.09%   [EVAL] batch:  265 | acc: 68.75%,  total acc: 70.09%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 70.08%   [EVAL] batch:  267 | acc: 56.25%,  total acc: 70.03%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 70.10%   [EVAL] batch:  269 | acc: 81.25%,  total acc: 70.14%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 70.25%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 70.36%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 70.47%   [EVAL] batch:  273 | acc: 100.00%,  total acc: 70.57%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 70.68%   [EVAL] batch:  275 | acc: 56.25%,  total acc: 70.63%   [EVAL] batch:  276 | acc: 43.75%,  total acc: 70.53%   [EVAL] batch:  277 | acc: 68.75%,  total acc: 70.53%   [EVAL] batch:  278 | acc: 56.25%,  total acc: 70.47%   [EVAL] batch:  279 | acc: 68.75%,  total acc: 70.47%   [EVAL] batch:  280 | acc: 50.00%,  total acc: 70.40%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 70.35%   [EVAL] batch:  282 | acc: 43.75%,  total acc: 70.25%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 70.03%   [EVAL] batch:  284 | acc: 18.75%,  total acc: 69.85%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 69.65%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 69.40%   [EVAL] batch:  287 | acc: 56.25%,  total acc: 69.36%   [EVAL] batch:  288 | acc: 68.75%,  total acc: 69.36%   [EVAL] batch:  289 | acc: 75.00%,  total acc: 69.38%   [EVAL] batch:  290 | acc: 75.00%,  total acc: 69.39%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 69.43%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 69.48%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 69.52%   [EVAL] batch:  294 | acc: 68.75%,  total acc: 69.51%   [EVAL] batch:  295 | acc: 56.25%,  total acc: 69.47%   [EVAL] batch:  296 | acc: 50.00%,  total acc: 69.40%   [EVAL] batch:  297 | acc: 62.50%,  total acc: 69.38%   [EVAL] batch:  298 | acc: 56.25%,  total acc: 69.34%   [EVAL] batch:  299 | acc: 68.75%,  total acc: 69.33%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 69.54%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 69.64%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 69.74%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 69.84%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 69.93%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 70.01%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 70.07%   [EVAL] batch:  308 | acc: 81.25%,  total acc: 70.11%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 70.20%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 70.26%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 70.35%   [EVAL] batch:  312 | acc: 62.50%,  total acc: 70.33%   [EVAL] batch:  313 | acc: 18.75%,  total acc: 70.16%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 70.00%   [EVAL] batch:  315 | acc: 12.50%,  total acc: 69.82%   [EVAL] batch:  316 | acc: 6.25%,  total acc: 69.62%   [EVAL] batch:  317 | acc: 6.25%,  total acc: 69.42%   [EVAL] batch:  318 | acc: 25.00%,  total acc: 69.28%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 69.38%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 69.47%   [EVAL] batch:  321 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 69.66%   [EVAL] batch:  323 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:  324 | acc: 100.00%,  total acc: 69.85%   [EVAL] batch:  325 | acc: 56.25%,  total acc: 69.80%   [EVAL] batch:  326 | acc: 37.50%,  total acc: 69.71%   [EVAL] batch:  327 | acc: 43.75%,  total acc: 69.63%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 69.64%   [EVAL] batch:  329 | acc: 75.00%,  total acc: 69.66%   [EVAL] batch:  330 | acc: 43.75%,  total acc: 69.58%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 69.63%   [EVAL] batch:  332 | acc: 93.75%,  total acc: 69.71%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 69.80%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 69.87%   [EVAL] batch:  335 | acc: 93.75%,  total acc: 69.94%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 70.03%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 70.08%   [EVAL] batch:  338 | acc: 81.25%,  total acc: 70.11%   [EVAL] batch:  339 | acc: 81.25%,  total acc: 70.15%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 70.23%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 70.25%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 70.34%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 70.39%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 70.47%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 70.56%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 70.61%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 70.67%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 70.76%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 70.84%   [EVAL] batch:  350 | acc: 75.00%,  total acc: 70.85%   [EVAL] batch:  351 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:  352 | acc: 62.50%,  total acc: 70.80%   [EVAL] batch:  353 | acc: 56.25%,  total acc: 70.76%   [EVAL] batch:  354 | acc: 56.25%,  total acc: 70.72%   [EVAL] batch:  355 | acc: 62.50%,  total acc: 70.70%   [EVAL] batch:  356 | acc: 93.75%,  total acc: 70.76%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 70.89%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 70.97%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 71.05%   [EVAL] batch:  361 | acc: 81.25%,  total acc: 71.08%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 71.07%   [EVAL] batch:  363 | acc: 25.00%,  total acc: 70.95%   [EVAL] batch:  364 | acc: 31.25%,  total acc: 70.84%   [EVAL] batch:  365 | acc: 25.00%,  total acc: 70.71%   [EVAL] batch:  366 | acc: 12.50%,  total acc: 70.56%   [EVAL] batch:  367 | acc: 31.25%,  total acc: 70.45%   [EVAL] batch:  368 | acc: 25.00%,  total acc: 70.33%   [EVAL] batch:  369 | acc: 68.75%,  total acc: 70.32%   [EVAL] batch:  370 | acc: 68.75%,  total acc: 70.32%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 70.26%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 70.29%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 70.34%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 70.35%   [EVAL] batch:  375 | acc: 18.75%,  total acc: 70.21%   [EVAL] batch:  376 | acc: 31.25%,  total acc: 70.11%   [EVAL] batch:  377 | acc: 6.25%,  total acc: 69.94%   [EVAL] batch:  378 | acc: 12.50%,  total acc: 69.79%   [EVAL] batch:  379 | acc: 18.75%,  total acc: 69.65%   [EVAL] batch:  380 | acc: 18.75%,  total acc: 69.52%   [EVAL] batch:  381 | acc: 68.75%,  total acc: 69.52%   [EVAL] batch:  382 | acc: 93.75%,  total acc: 69.58%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 69.65%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 69.69%   [EVAL] batch:  385 | acc: 93.75%,  total acc: 69.75%   [EVAL] batch:  386 | acc: 81.25%,  total acc: 69.78%   [EVAL] batch:  387 | acc: 81.25%,  total acc: 69.81%   [EVAL] batch:  388 | acc: 75.00%,  total acc: 69.83%   [EVAL] batch:  389 | acc: 81.25%,  total acc: 69.86%   [EVAL] batch:  390 | acc: 87.50%,  total acc: 69.90%   [EVAL] batch:  391 | acc: 81.25%,  total acc: 69.93%   [EVAL] batch:  392 | acc: 87.50%,  total acc: 69.97%   [EVAL] batch:  393 | acc: 56.25%,  total acc: 69.94%   [EVAL] batch:  394 | acc: 18.75%,  total acc: 69.81%   [EVAL] batch:  395 | acc: 18.75%,  total acc: 69.68%   [EVAL] batch:  396 | acc: 37.50%,  total acc: 69.60%   [EVAL] batch:  397 | acc: 37.50%,  total acc: 69.52%   [EVAL] batch:  398 | acc: 37.50%,  total acc: 69.44%   [EVAL] batch:  399 | acc: 25.00%,  total acc: 69.33%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 69.17%   [EVAL] batch:  401 | acc: 18.75%,  total acc: 69.05%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 68.87%   [EVAL] batch:  403 | acc: 12.50%,  total acc: 68.73%   [EVAL] batch:  404 | acc: 6.25%,  total acc: 68.58%   [EVAL] batch:  405 | acc: 6.25%,  total acc: 68.43%   [EVAL] batch:  406 | acc: 68.75%,  total acc: 68.43%   [EVAL] batch:  407 | acc: 87.50%,  total acc: 68.47%   [EVAL] batch:  408 | acc: 75.00%,  total acc: 68.49%   [EVAL] batch:  409 | acc: 87.50%,  total acc: 68.54%   [EVAL] batch:  410 | acc: 75.00%,  total acc: 68.55%   [EVAL] batch:  411 | acc: 68.75%,  total acc: 68.55%   [EVAL] batch:  412 | acc: 87.50%,  total acc: 68.60%   [EVAL] batch:  413 | acc: 87.50%,  total acc: 68.64%   [EVAL] batch:  414 | acc: 93.75%,  total acc: 68.70%   [EVAL] batch:  415 | acc: 93.75%,  total acc: 68.77%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 68.81%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 68.88%   [EVAL] batch:  418 | acc: 87.50%,  total acc: 68.93%   [EVAL] batch:  419 | acc: 18.75%,  total acc: 68.81%   [EVAL] batch:  420 | acc: 37.50%,  total acc: 68.74%   [EVAL] batch:  421 | acc: 31.25%,  total acc: 68.65%   [EVAL] batch:  422 | acc: 12.50%,  total acc: 68.51%   [EVAL] batch:  423 | acc: 31.25%,  total acc: 68.43%   [EVAL] batch:  424 | acc: 12.50%,  total acc: 68.29%   [EVAL] batch:  425 | acc: 62.50%,  total acc: 68.28%   [EVAL] batch:  426 | acc: 93.75%,  total acc: 68.34%   [EVAL] batch:  427 | acc: 62.50%,  total acc: 68.33%   [EVAL] batch:  428 | acc: 81.25%,  total acc: 68.36%   [EVAL] batch:  429 | acc: 87.50%,  total acc: 68.40%   [EVAL] batch:  430 | acc: 81.25%,  total acc: 68.43%   [EVAL] batch:  431 | acc: 50.00%,  total acc: 68.39%   [EVAL] batch:  432 | acc: 37.50%,  total acc: 68.32%   [EVAL] batch:  433 | acc: 50.00%,  total acc: 68.27%   [EVAL] batch:  434 | acc: 62.50%,  total acc: 68.26%   [EVAL] batch:  435 | acc: 43.75%,  total acc: 68.21%   [EVAL] batch:  436 | acc: 62.50%,  total acc: 68.19%   [EVAL] batch:  437 | acc: 18.75%,  total acc: 68.08%   
cur_acc:  ['0.9544', '0.7817', '0.7639', '0.8562', '0.7272', '0.6944', '0.5456']
his_acc:  ['0.9544', '0.8510', '0.8142', '0.8103', '0.7716', '0.7277', '0.6808']
CurrentTrain: epoch 15, batch     0 | loss: 13.8741379CurrentTrain: epoch 15, batch     1 | loss: 9.2564225CurrentTrain: epoch 15, batch     2 | loss: 15.4298439CurrentTrain: epoch  1, batch     3 | loss: 15.2844615CurrentTrain: epoch 15, batch     0 | loss: 11.6302248CurrentTrain: epoch 15, batch     1 | loss: 10.1184814CurrentTrain: epoch 15, batch     2 | loss: 11.1394406CurrentTrain: epoch  1, batch     3 | loss: 9.8513107CurrentTrain: epoch 15, batch     0 | loss: 25.2401790CurrentTrain: epoch 15, batch     1 | loss: 10.1528900CurrentTrain: epoch 15, batch     2 | loss: 12.0367699CurrentTrain: epoch  1, batch     3 | loss: 7.6610599CurrentTrain: epoch 15, batch     0 | loss: 8.4244779CurrentTrain: epoch 15, batch     1 | loss: 16.0318421CurrentTrain: epoch 15, batch     2 | loss: 9.7448147CurrentTrain: epoch  1, batch     3 | loss: 7.1800110CurrentTrain: epoch 15, batch     0 | loss: 13.6900004CurrentTrain: epoch 15, batch     1 | loss: 9.1943471CurrentTrain: epoch 15, batch     2 | loss: 9.3713630CurrentTrain: epoch  1, batch     3 | loss: 9.6768940CurrentTrain: epoch 15, batch     0 | loss: 9.7655880CurrentTrain: epoch 15, batch     1 | loss: 10.3609374CurrentTrain: epoch 15, batch     2 | loss: 9.0037356CurrentTrain: epoch  1, batch     3 | loss: 9.6085877CurrentTrain: epoch 15, batch     0 | loss: 12.5337729CurrentTrain: epoch 15, batch     1 | loss: 8.6375481CurrentTrain: epoch 15, batch     2 | loss: 7.4324226CurrentTrain: epoch  1, batch     3 | loss: 6.2424843CurrentTrain: epoch 15, batch     0 | loss: 15.3937365CurrentTrain: epoch 15, batch     1 | loss: 9.0536175CurrentTrain: epoch 15, batch     2 | loss: 10.9133932CurrentTrain: epoch  1, batch     3 | loss: 6.3816404CurrentTrain: epoch 15, batch     0 | loss: 15.9204335CurrentTrain: epoch 15, batch     1 | loss: 7.4712416CurrentTrain: epoch 15, batch     2 | loss: 9.5377432CurrentTrain: epoch  1, batch     3 | loss: 6.2719613CurrentTrain: epoch 15, batch     0 | loss: 16.4088624CurrentTrain: epoch 15, batch     1 | loss: 13.6591083CurrentTrain: epoch 15, batch     2 | loss: 12.6742253CurrentTrain: epoch  1, batch     3 | loss: 6.5749511
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: george lawson sheldon ( may 31 , 1870 – april 4 , 1960 ) was 14th governor of nebraska with service from 1907 until 1909 .
Head Entity: governor
Tail Entity: nebraska
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The Environmental Protection Agency (EPA) is responsible for enforcing regulations related to environmental protection in the United States.  
Head Entity: Environmental Protection Agency  
Tail Entity: United States  

Relation: applies to jurisdiction  
Context: The California Civil Code governs various aspects of civil law within the state of California, including contracts and property rights.  
Head Entity: California Civil Code  
Tail Entity: California  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: " tenth avenue freeze - out " is the second song on bruce springsteen 's breakthrough album " born to run " , released in 1975 .
Head Entity: tenth avenue freeze - out
Tail Entity: bruce springsteen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: "Shape of You" is a popular song by Ed Sheeran that topped charts worldwide after its release in 2017.  
Head Entity: Shape of You  
Tail Entity: Ed Sheeran  

Relation: performer  
Context: The Beatles are widely regarded as one of the most influential bands in the history of music, with hits like "Hey Jude" and "Let It Be."  
Head Entity: The Beatles  
Tail Entity: music
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the glasflügel h-201 standard " libelle " ( german : " dragonfly " ) is an early composite standard class single - seat sailplane produced by glasflügel from 1967 .
Head Entity: dragonfly
Tail Entity: glasflügel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The iPhone 13, a smartphone developed by Apple, was released in September 2021 and features advanced camera technology and improved battery life.  
Head Entity: iPhone 13  
Tail Entity: Apple  

Relation: manufacturer  
Context: The Tesla Model S is an all-electric luxury sedan that has been produced by Tesla, Inc. since 2012, showcasing cutting-edge technology and performance.  
Head Entity: Tesla Model S  
Tail Entity: Tesla, Inc.  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: at the 2016 summer olympics , heath and schofield won the silver k-2 200 event , finishing behind spain 's saúl craviotto and cristian toro .
Head Entity: 2016 summer olympics
Tail Entity: spain
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: during the 2020 presidential election, the democratic party nominated joe biden as their candidate, while the republican party supported donald trump.  
Head Entity: 2020 presidential election  
Tail Entity: democratic party  

Relation: participant  
Context: in the 2022 world cup, france emerged victorious, defeating argentina in a thrilling final match that captivated millions of fans worldwide.  
Head Entity: 2022 world cup  
Tail Entity: france  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the application is compatible with various operating systems, including macOS, Windows, and Ubuntu.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: This game runs smoothly on multiple operating systems, such as Linux, Windows 10, and older versions of Windows.  
Head Entity: game  
Tail Entity: Linux  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in his next bout , he won the wbc light welterweight title by defeating roger mayweather for a second time .
Head Entity: roger mayweather
Tail Entity: light welterweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: The team secured their place in the tournament by qualifying under the UEFA Champions League regulations.  
Head Entity: UEFA Champions League  
Tail Entity: tournament  

Relation: competition class  
Context: The athlete was classified under the FIS Alpine Skiing World Cup, allowing him to compete at the highest level.  
Head Entity: FIS Alpine Skiing World Cup  
Tail Entity: highest level  
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: the qup gained 3 seats from its previous election result , and won a 10 % swing to it in primary votes , but failed to defeat the incumbent labor government led by ted theodore .
Head Entity: previous election
Tail Entity: ted theodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: After a closely contested campaign, the voters chose Sarah Johnson as their representative, marking a significant shift in the political landscape of the district.  
Head Entity: closely contested campaign  
Tail Entity: Sarah Johnson  

Relation: successful candidate  
Context: In the recent mayoral election, the community rallied behind Mark Thompson, who promised to bring new initiatives to the city, ultimately leading to his victory.  
Head Entity: recent mayoral election  
Tail Entity: Mark Thompson  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: although the latter 's eldest son , lǫgmaðr , appears to have succeeded to the kingship , he was soon forced to contend with factions supporting his younger brothers : haraldr , and óláfr .
Head Entity: lǫgmaðr
Tail Entity: óláfr
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: In the royal family of the ancient kingdom, the two brothers, Eirik and Bjorn, often competed for their father's favor, but they shared a strong bond as siblings.  
Head Entity: Eirik  
Tail Entity: Bjorn  

Relation: sibling  
Context: During the family reunion, Anna and her brother, Mark, reminisced about their childhood adventures, highlighting the unique connection they had as siblings.  
Head Entity: Anna  
Tail Entity: Mark  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: dennis chalker is a retired navy seal , inventor and author who has written six books about the united states navy seals .
Head Entity: dennis chalker
Tail Entity: united states navy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: general john smith served in the air force for over twenty years, leading numerous missions and earning several commendations.  
Head Entity: general john smith  
Tail Entity: air force  

Relation: military branch  
Context: the distinguished service cross was awarded to captain elizabeth jones for her bravery while serving in the army during a critical operation.  
Head Entity: captain elizabeth jones  
Tail Entity: army  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: ziad rahbani is the son of the lebanese famous composer assi rahbani and nouhad haddad , the famous lebanese female singer known as fairuz .
Head Entity: assi rahbani
Tail Entity: ziad rahbani
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: in the animated series, the character of simon is the son of the adventurous couple, marie and tom.  
Head Entity: marie  
Tail Entity: simon  

Relation: child  
Context: the renowned physicist albert einstein had a daughter named lieserl, who was born in 1902.  
Head Entity: albert einstein  
Tail Entity: lieserl  
MemoryTrain:  epoch 15, batch     0 | loss: 3.1208266MemoryTrain:  epoch 15, batch     1 | loss: 4.0727175MemoryTrain:  epoch 15, batch     2 | loss: 2.4901670MemoryTrain:  epoch 15, batch     3 | loss: 5.6819283MemoryTrain:  epoch 15, batch     4 | loss: 3.2901110MemoryTrain:  epoch 15, batch     5 | loss: 3.5610614MemoryTrain:  epoch 15, batch     6 | loss: 3.2902653MemoryTrain:  epoch 15, batch     7 | loss: 2.4386300MemoryTrain:  epoch 15, batch     8 | loss: 3.1291960MemoryTrain:  epoch 15, batch     9 | loss: 3.3724813MemoryTrain:  epoch 15, batch    10 | loss: 2.1513943MemoryTrain:  epoch 15, batch    11 | loss: 4.1076622MemoryTrain:  epoch 15, batch    12 | loss: 3.4067958MemoryTrain:  epoch 15, batch    13 | loss: 2.3577729MemoryTrain:  epoch 15, batch    14 | loss: 2.5933496MemoryTrain:  epoch 15, batch     0 | loss: 2.4487417MemoryTrain:  epoch 15, batch     1 | loss: 2.1865023MemoryTrain:  epoch 15, batch     2 | loss: 2.4009171MemoryTrain:  epoch 15, batch     3 | loss: 2.0587403MemoryTrain:  epoch 15, batch     4 | loss: 2.4791120MemoryTrain:  epoch 15, batch     5 | loss: 2.7890350MemoryTrain:  epoch 15, batch     6 | loss: 4.6709762MemoryTrain:  epoch 15, batch     7 | loss: 2.6202618MemoryTrain:  epoch 15, batch     8 | loss: 2.7954120MemoryTrain:  epoch 15, batch     9 | loss: 4.7501928MemoryTrain:  epoch 15, batch    10 | loss: 1.7803100MemoryTrain:  epoch 15, batch    11 | loss: 2.0911540MemoryTrain:  epoch 15, batch    12 | loss: 1.6965493MemoryTrain:  epoch 15, batch    13 | loss: 4.1982653MemoryTrain:  epoch 15, batch    14 | loss: 2.2019045MemoryTrain:  epoch 15, batch     0 | loss: 1.7204269MemoryTrain:  epoch 15, batch     1 | loss: 1.6568739MemoryTrain:  epoch 15, batch     2 | loss: 3.8065788MemoryTrain:  epoch 15, batch     3 | loss: 4.0646814MemoryTrain:  epoch 15, batch     4 | loss: 3.0076370MemoryTrain:  epoch 15, batch     5 | loss: 2.3349029MemoryTrain:  epoch 15, batch     6 | loss: 3.4615538MemoryTrain:  epoch 15, batch     7 | loss: 1.6742505MemoryTrain:  epoch 15, batch     8 | loss: 3.9796236MemoryTrain:  epoch 15, batch     9 | loss: 2.0207641MemoryTrain:  epoch 15, batch    10 | loss: 2.7832736MemoryTrain:  epoch 15, batch    11 | loss: 2.0427528MemoryTrain:  epoch 15, batch    12 | loss: 1.8973190MemoryTrain:  epoch 15, batch    13 | loss: 1.5389034MemoryTrain:  epoch 15, batch    14 | loss: 2.9654188MemoryTrain:  epoch 15, batch     0 | loss: 1.7074848MemoryTrain:  epoch 15, batch     1 | loss: 1.4982393MemoryTrain:  epoch 15, batch     2 | loss: 1.5757570MemoryTrain:  epoch 15, batch     3 | loss: 4.0415441MemoryTrain:  epoch 15, batch     4 | loss: 3.8198864MemoryTrain:  epoch 15, batch     5 | loss: 2.0152051MemoryTrain:  epoch 15, batch     6 | loss: 1.3766231MemoryTrain:  epoch 15, batch     7 | loss: 1.7789200MemoryTrain:  epoch 15, batch     8 | loss: 1.7174078MemoryTrain:  epoch 15, batch     9 | loss: 2.4090217MemoryTrain:  epoch 15, batch    10 | loss: 2.3176543MemoryTrain:  epoch 15, batch    11 | loss: 2.5893485MemoryTrain:  epoch 15, batch    12 | loss: 2.7057517MemoryTrain:  epoch 15, batch    13 | loss: 1.6550998MemoryTrain:  epoch 15, batch    14 | loss: 2.1834718MemoryTrain:  epoch 15, batch     0 | loss: 1.7468888MemoryTrain:  epoch 15, batch     1 | loss: 1.5544542MemoryTrain:  epoch 15, batch     2 | loss: 2.9227620MemoryTrain:  epoch 15, batch     3 | loss: 1.5333193MemoryTrain:  epoch 15, batch     4 | loss: 1.9683682MemoryTrain:  epoch 15, batch     5 | loss: 2.0650260MemoryTrain:  epoch 15, batch     6 | loss: 1.9426150MemoryTrain:  epoch 15, batch     7 | loss: 4.0405048MemoryTrain:  epoch 15, batch     8 | loss: 1.6279177MemoryTrain:  epoch 15, batch     9 | loss: 2.1593458MemoryTrain:  epoch 15, batch    10 | loss: 2.0924680MemoryTrain:  epoch 15, batch    11 | loss: 1.6402729MemoryTrain:  epoch 15, batch    12 | loss: 2.8485038MemoryTrain:  epoch 15, batch    13 | loss: 1.6157842MemoryTrain:  epoch 15, batch    14 | loss: 1.6961442MemoryTrain:  epoch 15, batch     0 | loss: 1.6897766MemoryTrain:  epoch 15, batch     1 | loss: 1.5824538MemoryTrain:  epoch 15, batch     2 | loss: 2.1455605MemoryTrain:  epoch 15, batch     3 | loss: 1.5870936MemoryTrain:  epoch 15, batch     4 | loss: 2.8513530MemoryTrain:  epoch 15, batch     5 | loss: 3.7355433MemoryTrain:  epoch 15, batch     6 | loss: 1.8443512MemoryTrain:  epoch 15, batch     7 | loss: 1.9714453MemoryTrain:  epoch 15, batch     8 | loss: 1.9055396MemoryTrain:  epoch 15, batch     9 | loss: 2.8359693MemoryTrain:  epoch 15, batch    10 | loss: 1.8533799MemoryTrain:  epoch 15, batch    11 | loss: 3.9898200MemoryTrain:  epoch 15, batch    12 | loss: 1.7476048MemoryTrain:  epoch 15, batch    13 | loss: 2.0227109MemoryTrain:  epoch 15, batch    14 | loss: 2.3673928MemoryTrain:  epoch 15, batch     0 | loss: 2.4427601MemoryTrain:  epoch 15, batch     1 | loss: 4.2231406MemoryTrain:  epoch 15, batch     2 | loss: 1.5528929MemoryTrain:  epoch 15, batch     3 | loss: 2.0722043MemoryTrain:  epoch 15, batch     4 | loss: 6.4643487MemoryTrain:  epoch 15, batch     5 | loss: 1.5312207MemoryTrain:  epoch 15, batch     6 | loss: 1.6232620MemoryTrain:  epoch 15, batch     7 | loss: 1.9932815MemoryTrain:  epoch 15, batch     8 | loss: 1.7717089MemoryTrain:  epoch 15, batch     9 | loss: 4.2255506MemoryTrain:  epoch 15, batch    10 | loss: 1.9327094MemoryTrain:  epoch 15, batch    11 | loss: 1.6368750MemoryTrain:  epoch 15, batch    12 | loss: 1.3360229MemoryTrain:  epoch 15, batch    13 | loss: 1.6792601MemoryTrain:  epoch 15, batch    14 | loss: 2.1219005MemoryTrain:  epoch 15, batch     0 | loss: 1.7394171MemoryTrain:  epoch 15, batch     1 | loss: 1.5512893MemoryTrain:  epoch 15, batch     2 | loss: 1.7212546MemoryTrain:  epoch 15, batch     3 | loss: 1.3694850MemoryTrain:  epoch 15, batch     4 | loss: 1.5308333MemoryTrain:  epoch 15, batch     5 | loss: 1.3401841MemoryTrain:  epoch 15, batch     6 | loss: 1.8292887MemoryTrain:  epoch 15, batch     7 | loss: 1.7367992MemoryTrain:  epoch 15, batch     8 | loss: 1.5270826MemoryTrain:  epoch 15, batch     9 | loss: 1.8993849MemoryTrain:  epoch 15, batch    10 | loss: 1.7133399MemoryTrain:  epoch 15, batch    11 | loss: 1.6250858MemoryTrain:  epoch 15, batch    12 | loss: 1.6360065MemoryTrain:  epoch 15, batch    13 | loss: 1.7484741MemoryTrain:  epoch 15, batch    14 | loss: 1.4847019MemoryTrain:  epoch 15, batch     0 | loss: 1.4976569MemoryTrain:  epoch 15, batch     1 | loss: 1.3795095MemoryTrain:  epoch 15, batch     2 | loss: 1.2720772MemoryTrain:  epoch 15, batch     3 | loss: 1.7104159MemoryTrain:  epoch 15, batch     4 | loss: 1.4380062MemoryTrain:  epoch 15, batch     5 | loss: 1.3751845MemoryTrain:  epoch 15, batch     6 | loss: 1.9842919MemoryTrain:  epoch 15, batch     7 | loss: 1.5993368MemoryTrain:  epoch 15, batch     8 | loss: 1.7099588MemoryTrain:  epoch 15, batch     9 | loss: 1.5584854MemoryTrain:  epoch 15, batch    10 | loss: 3.9332600MemoryTrain:  epoch 15, batch    11 | loss: 4.0425062MemoryTrain:  epoch 15, batch    12 | loss: 1.7648705MemoryTrain:  epoch 15, batch    13 | loss: 1.6964112MemoryTrain:  epoch 15, batch    14 | loss: 1.5430693MemoryTrain:  epoch 15, batch     0 | loss: 1.6035752MemoryTrain:  epoch 15, batch     1 | loss: 1.5608967MemoryTrain:  epoch 15, batch     2 | loss: 1.3235413MemoryTrain:  epoch 15, batch     3 | loss: 1.7472354MemoryTrain:  epoch 15, batch     4 | loss: 1.4440373MemoryTrain:  epoch 15, batch     5 | loss: 1.4808179MemoryTrain:  epoch 15, batch     6 | loss: 1.4883960MemoryTrain:  epoch 15, batch     7 | loss: 1.7190782MemoryTrain:  epoch 15, batch     8 | loss: 1.3819532MemoryTrain:  epoch 15, batch     9 | loss: 2.0436499MemoryTrain:  epoch 15, batch    10 | loss: 1.3458462MemoryTrain:  epoch 15, batch    11 | loss: 1.4816666MemoryTrain:  epoch 15, batch    12 | loss: 3.9331228MemoryTrain:  epoch 15, batch    13 | loss: 1.7227207MemoryTrain:  epoch 15, batch    14 | loss: 4.0662323
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 77.78%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 76.88%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 77.27%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 76.04%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 76.44%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 74.58%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 74.22%   [EVAL] batch:   16 | acc: 37.50%,  total acc: 72.06%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 71.71%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 69.38%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 67.86%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 66.76%   [EVAL] batch:   22 | acc: 18.75%,  total acc: 64.67%   [EVAL] batch:   23 | acc: 62.50%,  total acc: 64.58%   [EVAL] batch:   24 | acc: 31.25%,  total acc: 63.25%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 64.18%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 65.28%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 66.29%   [EVAL] batch:   28 | acc: 81.25%,  total acc: 66.81%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 67.92%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 68.55%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 70.45%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 71.32%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 72.14%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 73.65%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 74.34%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 74.84%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 75.15%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 75.60%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 76.02%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 76.14%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 75.28%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 74.05%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 73.14%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 72.66%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 72.19%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 71.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 71.45%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 71.93%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 72.11%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 72.50%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 72.77%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 72.48%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 72.41%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 72.14%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 71.77%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 71.93%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 71.47%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 71.03%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 67.97%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 65.97%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 63.12%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 60.23%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 58.33%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 58.65%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 60.71%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 62.92%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 64.84%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 66.54%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 68.06%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 69.41%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 69.69%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 70.24%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 69.89%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 70.11%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 70.83%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 70.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 73.88%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 74.57%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 75.42%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 76.21%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 76.76%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 77.46%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 77.21%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 77.14%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 77.26%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 77.70%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 78.69%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.22%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 79.73%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 80.06%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 80.38%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 80.40%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 80.56%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 80.43%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 80.19%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 79.69%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 79.85%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 79.62%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 79.78%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 79.81%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 80.07%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 79.86%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 79.43%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 79.58%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 79.39%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 78.56%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 78.07%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 77.92%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 77.77%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 77.32%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 76.69%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 75.68%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 74.90%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 73.96%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 73.13%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 72.43%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 72.01%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 72.41%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 72.80%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 73.29%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 73.65%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 73.92%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 73.85%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 73.62%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 73.56%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 73.34%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 73.28%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 73.07%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 73.02%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 72.67%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 72.69%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 72.50%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 72.46%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 72.27%   [EVAL] batch:   87 | acc: 37.50%,  total acc: 71.88%   [EVAL] batch:   88 | acc: 37.50%,  total acc: 71.49%   [EVAL] batch:   89 | acc: 25.00%,  total acc: 70.97%   [EVAL] batch:   90 | acc: 25.00%,  total acc: 70.47%   [EVAL] batch:   91 | acc: 31.25%,  total acc: 70.04%   [EVAL] batch:   92 | acc: 31.25%,  total acc: 69.62%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 69.22%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 69.28%   [EVAL] batch:   95 | acc: 56.25%,  total acc: 69.14%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 69.01%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 68.94%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 69.00%   [EVAL] batch:   99 | acc: 56.25%,  total acc: 68.88%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 68.81%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 68.81%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 68.81%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 68.99%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 68.99%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 69.22%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 69.16%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 68.81%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 68.46%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 68.24%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 67.96%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 67.80%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 67.81%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 68.09%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 68.37%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 68.64%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 68.91%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 69.17%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 69.33%   [EVAL] batch:  119 | acc: 75.00%,  total acc: 69.38%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 69.21%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 69.31%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 69.41%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 69.46%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 69.45%   [EVAL] batch:  125 | acc: 43.75%,  total acc: 69.25%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 68.90%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 68.65%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 68.56%   [EVAL] batch:  129 | acc: 37.50%,  total acc: 68.32%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 68.08%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 68.18%   [EVAL] batch:  132 | acc: 62.50%,  total acc: 68.14%   [EVAL] batch:  133 | acc: 62.50%,  total acc: 68.10%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 68.15%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 68.24%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 68.34%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 68.34%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 67.94%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 67.68%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 67.29%   [EVAL] batch:  141 | acc: 18.75%,  total acc: 66.95%   [EVAL] batch:  142 | acc: 18.75%,  total acc: 66.61%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 66.36%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 66.55%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 66.78%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 67.01%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 67.23%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 67.45%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 67.62%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 67.18%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 66.74%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 66.30%   [EVAL] batch:  153 | acc: 12.50%,  total acc: 65.95%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 65.56%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 65.14%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 65.17%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 65.35%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 65.53%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 65.70%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 65.84%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 66.01%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 65.99%   [EVAL] batch:  163 | acc: 18.75%,  total acc: 65.70%   [EVAL] batch:  164 | acc: 43.75%,  total acc: 65.57%   [EVAL] batch:  165 | acc: 25.00%,  total acc: 65.32%   [EVAL] batch:  166 | acc: 25.00%,  total acc: 65.08%   [EVAL] batch:  167 | acc: 43.75%,  total acc: 64.96%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 64.90%   [EVAL] batch:  169 | acc: 56.25%,  total acc: 64.85%   [EVAL] batch:  170 | acc: 68.75%,  total acc: 64.88%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 64.90%   [EVAL] batch:  172 | acc: 62.50%,  total acc: 64.88%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 65.05%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 65.14%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 65.13%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 65.01%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 64.92%   [EVAL] batch:  178 | acc: 37.50%,  total acc: 64.77%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 64.83%   [EVAL] batch:  180 | acc: 50.00%,  total acc: 64.74%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 64.80%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 64.92%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 65.08%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 65.20%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 65.36%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 65.44%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 65.59%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 65.74%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 65.89%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 66.07%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 66.15%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 66.32%   [EVAL] batch:  193 | acc: 87.50%,  total acc: 66.43%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 66.44%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 66.45%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 66.50%   [EVAL] batch:  197 | acc: 43.75%,  total acc: 66.38%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 66.39%   [EVAL] batch:  199 | acc: 56.25%,  total acc: 66.34%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 66.32%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 66.37%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 66.35%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 66.30%   [EVAL] batch:  204 | acc: 50.00%,  total acc: 66.22%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 66.17%   [EVAL] batch:  206 | acc: 50.00%,  total acc: 66.09%   [EVAL] batch:  207 | acc: 50.00%,  total acc: 66.02%   [EVAL] batch:  208 | acc: 62.50%,  total acc: 66.00%   [EVAL] batch:  209 | acc: 31.25%,  total acc: 65.83%   [EVAL] batch:  210 | acc: 43.75%,  total acc: 65.73%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 65.57%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 65.61%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 65.77%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 65.93%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 66.09%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 66.24%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 66.40%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 66.52%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 66.51%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 66.57%   [EVAL] batch:  221 | acc: 68.75%,  total acc: 66.58%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 66.56%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 66.63%   [EVAL] batch:  224 | acc: 43.75%,  total acc: 66.53%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 66.68%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 66.82%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 66.97%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 67.11%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 67.26%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 67.37%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 67.51%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 67.65%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 67.79%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 68.06%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 68.17%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 68.30%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 68.38%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 68.49%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 68.62%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 68.70%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 68.80%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 68.88%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 69.01%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 69.08%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 69.18%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 69.30%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 69.40%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 69.50%   [EVAL] batch:  250 | acc: 18.75%,  total acc: 69.30%   [EVAL] batch:  251 | acc: 37.50%,  total acc: 69.17%   [EVAL] batch:  252 | acc: 25.00%,  total acc: 69.00%   [EVAL] batch:  253 | acc: 25.00%,  total acc: 68.82%   [EVAL] batch:  254 | acc: 25.00%,  total acc: 68.65%   [EVAL] batch:  255 | acc: 43.75%,  total acc: 68.55%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 68.48%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 68.48%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 68.48%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 68.49%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 68.39%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 68.39%   [EVAL] batch:  262 | acc: 75.00%,  total acc: 68.42%   [EVAL] batch:  263 | acc: 62.50%,  total acc: 68.39%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 68.37%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 68.33%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 68.28%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 68.26%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 68.31%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 68.38%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 68.50%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 68.61%   [EVAL] batch:  272 | acc: 87.50%,  total acc: 68.68%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 68.86%   [EVAL] batch:  275 | acc: 56.25%,  total acc: 68.82%   [EVAL] batch:  276 | acc: 56.25%,  total acc: 68.77%   [EVAL] batch:  277 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:  278 | acc: 56.25%,  total acc: 68.71%   [EVAL] batch:  279 | acc: 43.75%,  total acc: 68.62%   [EVAL] batch:  280 | acc: 50.00%,  total acc: 68.55%   [EVAL] batch:  281 | acc: 43.75%,  total acc: 68.46%   [EVAL] batch:  282 | acc: 43.75%,  total acc: 68.37%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 68.16%   [EVAL] batch:  284 | acc: 12.50%,  total acc: 67.96%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 67.77%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 67.53%   [EVAL] batch:  287 | acc: 43.75%,  total acc: 67.45%   [EVAL] batch:  288 | acc: 56.25%,  total acc: 67.41%   [EVAL] batch:  289 | acc: 68.75%,  total acc: 67.41%   [EVAL] batch:  290 | acc: 50.00%,  total acc: 67.35%   [EVAL] batch:  291 | acc: 68.75%,  total acc: 67.36%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 67.38%   [EVAL] batch:  293 | acc: 62.50%,  total acc: 67.37%   [EVAL] batch:  294 | acc: 62.50%,  total acc: 67.35%   [EVAL] batch:  295 | acc: 62.50%,  total acc: 67.34%   [EVAL] batch:  296 | acc: 43.75%,  total acc: 67.26%   [EVAL] batch:  297 | acc: 62.50%,  total acc: 67.24%   [EVAL] batch:  298 | acc: 56.25%,  total acc: 67.20%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 67.23%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 67.34%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 67.45%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 67.55%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 67.66%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 67.77%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 67.87%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 67.96%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 68.02%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 68.08%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 68.19%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 68.25%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 68.35%   [EVAL] batch:  312 | acc: 68.75%,  total acc: 68.35%   [EVAL] batch:  313 | acc: 18.75%,  total acc: 68.19%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 68.04%   [EVAL] batch:  315 | acc: 12.50%,  total acc: 67.86%   [EVAL] batch:  316 | acc: 6.25%,  total acc: 67.67%   [EVAL] batch:  317 | acc: 18.75%,  total acc: 67.51%   [EVAL] batch:  318 | acc: 18.75%,  total acc: 67.36%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 67.42%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 67.52%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 67.60%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 67.69%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 67.75%   [EVAL] batch:  324 | acc: 100.00%,  total acc: 67.85%   [EVAL] batch:  325 | acc: 50.00%,  total acc: 67.79%   [EVAL] batch:  326 | acc: 12.50%,  total acc: 67.62%   [EVAL] batch:  327 | acc: 18.75%,  total acc: 67.47%   [EVAL] batch:  328 | acc: 37.50%,  total acc: 67.38%   [EVAL] batch:  329 | acc: 31.25%,  total acc: 67.27%   [EVAL] batch:  330 | acc: 25.00%,  total acc: 67.15%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 67.21%   [EVAL] batch:  332 | acc: 93.75%,  total acc: 67.29%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 67.38%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 67.46%   [EVAL] batch:  335 | acc: 93.75%,  total acc: 67.54%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:  337 | acc: 81.25%,  total acc: 67.68%   [EVAL] batch:  338 | acc: 81.25%,  total acc: 67.72%   [EVAL] batch:  339 | acc: 87.50%,  total acc: 67.78%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 67.87%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 67.89%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 67.98%   [EVAL] batch:  343 | acc: 93.75%,  total acc: 68.06%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 68.15%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 68.24%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 68.30%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 68.37%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 68.45%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 68.54%   [EVAL] batch:  350 | acc: 18.75%,  total acc: 68.39%   [EVAL] batch:  351 | acc: 12.50%,  total acc: 68.24%   [EVAL] batch:  352 | acc: 12.50%,  total acc: 68.08%   [EVAL] batch:  353 | acc: 6.25%,  total acc: 67.90%   [EVAL] batch:  354 | acc: 6.25%,  total acc: 67.73%   [EVAL] batch:  355 | acc: 18.75%,  total acc: 67.59%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 67.65%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 67.72%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 67.79%   [EVAL] batch:  359 | acc: 93.75%,  total acc: 67.86%   [EVAL] batch:  360 | acc: 87.50%,  total acc: 67.92%   [EVAL] batch:  361 | acc: 81.25%,  total acc: 67.96%   [EVAL] batch:  362 | acc: 56.25%,  total acc: 67.92%   [EVAL] batch:  363 | acc: 0.00%,  total acc: 67.74%   [EVAL] batch:  364 | acc: 18.75%,  total acc: 67.60%   [EVAL] batch:  365 | acc: 0.00%,  total acc: 67.42%   [EVAL] batch:  366 | acc: 12.50%,  total acc: 67.27%   [EVAL] batch:  367 | acc: 18.75%,  total acc: 67.14%   [EVAL] batch:  368 | acc: 12.50%,  total acc: 66.99%   [EVAL] batch:  369 | acc: 62.50%,  total acc: 66.98%   [EVAL] batch:  370 | acc: 56.25%,  total acc: 66.95%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 66.89%   [EVAL] batch:  372 | acc: 68.75%,  total acc: 66.89%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 66.93%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 66.95%   [EVAL] batch:  375 | acc: 12.50%,  total acc: 66.81%   [EVAL] batch:  376 | acc: 31.25%,  total acc: 66.71%   [EVAL] batch:  377 | acc: 6.25%,  total acc: 66.55%   [EVAL] batch:  378 | acc: 12.50%,  total acc: 66.41%   [EVAL] batch:  379 | acc: 18.75%,  total acc: 66.28%   [EVAL] batch:  380 | acc: 18.75%,  total acc: 66.16%   [EVAL] batch:  381 | acc: 68.75%,  total acc: 66.16%   [EVAL] batch:  382 | acc: 81.25%,  total acc: 66.20%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 66.28%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 66.33%   [EVAL] batch:  385 | acc: 100.00%,  total acc: 66.42%   [EVAL] batch:  386 | acc: 87.50%,  total acc: 66.47%   [EVAL] batch:  387 | acc: 81.25%,  total acc: 66.51%   [EVAL] batch:  388 | acc: 68.75%,  total acc: 66.52%   [EVAL] batch:  389 | acc: 93.75%,  total acc: 66.59%   [EVAL] batch:  390 | acc: 93.75%,  total acc: 66.66%   [EVAL] batch:  391 | acc: 81.25%,  total acc: 66.69%   [EVAL] batch:  392 | acc: 93.75%,  total acc: 66.76%   [EVAL] batch:  393 | acc: 56.25%,  total acc: 66.74%   [EVAL] batch:  394 | acc: 31.25%,  total acc: 66.65%   [EVAL] batch:  395 | acc: 25.00%,  total acc: 66.54%   [EVAL] batch:  396 | acc: 62.50%,  total acc: 66.53%   [EVAL] batch:  397 | acc: 50.00%,  total acc: 66.49%   [EVAL] batch:  398 | acc: 37.50%,  total acc: 66.42%   [EVAL] batch:  399 | acc: 37.50%,  total acc: 66.34%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 66.19%   [EVAL] batch:  401 | acc: 6.25%,  total acc: 66.04%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 65.88%   [EVAL] batch:  403 | acc: 6.25%,  total acc: 65.73%   [EVAL] batch:  404 | acc: 0.00%,  total acc: 65.57%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 65.41%   [EVAL] batch:  406 | acc: 68.75%,  total acc: 65.42%   [EVAL] batch:  407 | acc: 87.50%,  total acc: 65.47%   [EVAL] batch:  408 | acc: 75.00%,  total acc: 65.50%   [EVAL] batch:  409 | acc: 87.50%,  total acc: 65.55%   [EVAL] batch:  410 | acc: 81.25%,  total acc: 65.59%   [EVAL] batch:  411 | acc: 68.75%,  total acc: 65.59%   [EVAL] batch:  412 | acc: 81.25%,  total acc: 65.63%   [EVAL] batch:  413 | acc: 87.50%,  total acc: 65.69%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 65.77%   [EVAL] batch:  415 | acc: 93.75%,  total acc: 65.84%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 65.89%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 65.97%   [EVAL] batch:  418 | acc: 87.50%,  total acc: 66.02%   [EVAL] batch:  419 | acc: 25.00%,  total acc: 65.92%   [EVAL] batch:  420 | acc: 37.50%,  total acc: 65.86%   [EVAL] batch:  421 | acc: 37.50%,  total acc: 65.79%   [EVAL] batch:  422 | acc: 25.00%,  total acc: 65.69%   [EVAL] batch:  423 | acc: 43.75%,  total acc: 65.64%   [EVAL] batch:  424 | acc: 31.25%,  total acc: 65.56%   [EVAL] batch:  425 | acc: 50.00%,  total acc: 65.52%   [EVAL] batch:  426 | acc: 75.00%,  total acc: 65.54%   [EVAL] batch:  427 | acc: 62.50%,  total acc: 65.54%   [EVAL] batch:  428 | acc: 75.00%,  total acc: 65.56%   [EVAL] batch:  429 | acc: 75.00%,  total acc: 65.58%   [EVAL] batch:  430 | acc: 75.00%,  total acc: 65.60%   [EVAL] batch:  431 | acc: 37.50%,  total acc: 65.54%   [EVAL] batch:  432 | acc: 43.75%,  total acc: 65.49%   [EVAL] batch:  433 | acc: 37.50%,  total acc: 65.42%   [EVAL] batch:  434 | acc: 56.25%,  total acc: 65.40%   [EVAL] batch:  435 | acc: 43.75%,  total acc: 65.35%   [EVAL] batch:  436 | acc: 75.00%,  total acc: 65.37%   [EVAL] batch:  437 | acc: 68.75%,  total acc: 65.38%   [EVAL] batch:  438 | acc: 81.25%,  total acc: 65.42%   [EVAL] batch:  439 | acc: 68.75%,  total acc: 65.43%   [EVAL] batch:  440 | acc: 93.75%,  total acc: 65.49%   [EVAL] batch:  441 | acc: 87.50%,  total acc: 65.54%   [EVAL] batch:  442 | acc: 87.50%,  total acc: 65.59%   [EVAL] batch:  443 | acc: 75.00%,  total acc: 65.61%   [EVAL] batch:  444 | acc: 75.00%,  total acc: 65.63%   [EVAL] batch:  445 | acc: 56.25%,  total acc: 65.61%   [EVAL] batch:  446 | acc: 56.25%,  total acc: 65.59%   [EVAL] batch:  447 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:  448 | acc: 75.00%,  total acc: 65.65%   [EVAL] batch:  449 | acc: 81.25%,  total acc: 65.68%   [EVAL] batch:  450 | acc: 62.50%,  total acc: 65.67%   [EVAL] batch:  451 | acc: 68.75%,  total acc: 65.68%   [EVAL] batch:  452 | acc: 56.25%,  total acc: 65.66%   [EVAL] batch:  453 | acc: 56.25%,  total acc: 65.64%   [EVAL] batch:  454 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:  455 | acc: 62.50%,  total acc: 65.61%   [EVAL] batch:  456 | acc: 56.25%,  total acc: 65.59%   [EVAL] batch:  457 | acc: 25.00%,  total acc: 65.50%   [EVAL] batch:  458 | acc: 37.50%,  total acc: 65.44%   [EVAL] batch:  459 | acc: 37.50%,  total acc: 65.38%   [EVAL] batch:  460 | acc: 37.50%,  total acc: 65.32%   [EVAL] batch:  461 | acc: 43.75%,  total acc: 65.27%   [EVAL] batch:  462 | acc: 56.25%,  total acc: 65.25%   [EVAL] batch:  463 | acc: 93.75%,  total acc: 65.32%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 65.39%   [EVAL] batch:  465 | acc: 75.00%,  total acc: 65.41%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 65.48%   [EVAL] batch:  467 | acc: 93.75%,  total acc: 65.54%   [EVAL] batch:  468 | acc: 93.75%,  total acc: 65.61%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 65.68%   [EVAL] batch:  470 | acc: 100.00%,  total acc: 65.75%   [EVAL] batch:  471 | acc: 100.00%,  total acc: 65.82%   [EVAL] batch:  472 | acc: 100.00%,  total acc: 65.90%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 65.97%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 66.04%   [EVAL] batch:  475 | acc: 93.75%,  total acc: 66.10%   [EVAL] batch:  476 | acc: 87.50%,  total acc: 66.14%   [EVAL] batch:  477 | acc: 87.50%,  total acc: 66.19%   [EVAL] batch:  478 | acc: 81.25%,  total acc: 66.22%   [EVAL] batch:  479 | acc: 100.00%,  total acc: 66.29%   [EVAL] batch:  480 | acc: 93.75%,  total acc: 66.35%   [EVAL] batch:  481 | acc: 50.00%,  total acc: 66.31%   [EVAL] batch:  482 | acc: 25.00%,  total acc: 66.23%   [EVAL] batch:  483 | acc: 37.50%,  total acc: 66.17%   [EVAL] batch:  484 | acc: 25.00%,  total acc: 66.08%   [EVAL] batch:  485 | acc: 56.25%,  total acc: 66.06%   [EVAL] batch:  486 | acc: 31.25%,  total acc: 65.99%   [EVAL] batch:  487 | acc: 56.25%,  total acc: 65.97%   [EVAL] batch:  488 | acc: 87.50%,  total acc: 66.01%   [EVAL] batch:  489 | acc: 93.75%,  total acc: 66.07%   [EVAL] batch:  490 | acc: 75.00%,  total acc: 66.09%   [EVAL] batch:  491 | acc: 87.50%,  total acc: 66.13%   [EVAL] batch:  492 | acc: 93.75%,  total acc: 66.19%   [EVAL] batch:  493 | acc: 87.50%,  total acc: 66.23%   [EVAL] batch:  494 | acc: 37.50%,  total acc: 66.17%   [EVAL] batch:  495 | acc: 75.00%,  total acc: 66.19%   [EVAL] batch:  496 | acc: 37.50%,  total acc: 66.13%   [EVAL] batch:  497 | acc: 68.75%,  total acc: 66.14%   [EVAL] batch:  498 | acc: 62.50%,  total acc: 66.13%   [EVAL] batch:  499 | acc: 68.75%,  total acc: 66.14%   
cur_acc:  ['0.9544', '0.7817', '0.7639', '0.8562', '0.7272', '0.6944', '0.5456', '0.7103']
his_acc:  ['0.9544', '0.8510', '0.8142', '0.8103', '0.7716', '0.7277', '0.6808', '0.6614']
--------Round  1
seed:  200
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
CurrentTrain: epoch 15, batch     0 | loss: 21.6055198CurrentTrain: epoch 15, batch     1 | loss: 20.8811001CurrentTrain: epoch 15, batch     2 | loss: 31.0684138CurrentTrain: epoch 15, batch     3 | loss: 20.6886676CurrentTrain: epoch 15, batch     4 | loss: 24.6541408CurrentTrain: epoch 15, batch     5 | loss: 20.1348199CurrentTrain: epoch 15, batch     6 | loss: 21.1915551CurrentTrain: epoch 15, batch     7 | loss: 18.8819427CurrentTrain: epoch 15, batch     8 | loss: 15.2533920CurrentTrain: epoch 15, batch     9 | loss: 16.1184226CurrentTrain: epoch 15, batch    10 | loss: 20.3631994CurrentTrain: epoch 15, batch    11 | loss: 32.8165126CurrentTrain: epoch 15, batch    12 | loss: 19.2694636CurrentTrain: epoch 15, batch    13 | loss: 20.5412020CurrentTrain: epoch 15, batch    14 | loss: 19.7061687CurrentTrain: epoch 15, batch    15 | loss: 22.7301862CurrentTrain: epoch 15, batch    16 | loss: 20.2816302CurrentTrain: epoch 15, batch    17 | loss: 20.2501305CurrentTrain: epoch 15, batch    18 | loss: 15.9121106CurrentTrain: epoch 15, batch    19 | loss: 16.9767608CurrentTrain: epoch 15, batch    20 | loss: 17.2394167CurrentTrain: epoch 15, batch    21 | loss: 16.9570349CurrentTrain: epoch 15, batch    22 | loss: 22.2677295CurrentTrain: epoch 15, batch    23 | loss: 16.5954838CurrentTrain: epoch 15, batch    24 | loss: 24.2604986CurrentTrain: epoch 15, batch    25 | loss: 16.1638636CurrentTrain: epoch 15, batch    26 | loss: 32.1653012CurrentTrain: epoch 15, batch    27 | loss: 18.9248177CurrentTrain: epoch 15, batch    28 | loss: 17.8228427CurrentTrain: epoch 15, batch    29 | loss: 13.2308711CurrentTrain: epoch 15, batch    30 | loss: 23.4796315CurrentTrain: epoch 15, batch    31 | loss: 15.7841345CurrentTrain: epoch 15, batch    32 | loss: 23.0538932CurrentTrain: epoch 15, batch    33 | loss: 17.7913554CurrentTrain: epoch 15, batch    34 | loss: 14.8046981CurrentTrain: epoch 15, batch    35 | loss: 32.2397026CurrentTrain: epoch 15, batch    36 | loss: 15.8792291CurrentTrain: epoch 15, batch    37 | loss: 16.3817403CurrentTrain: epoch 15, batch    38 | loss: 19.5376218CurrentTrain: epoch 15, batch    39 | loss: 20.3255912CurrentTrain: epoch 15, batch    40 | loss: 16.5697194CurrentTrain: epoch 15, batch    41 | loss: 18.0475094CurrentTrain: epoch 15, batch    42 | loss: 14.5560593CurrentTrain: epoch 15, batch    43 | loss: 18.9431248CurrentTrain: epoch 15, batch    44 | loss: 11.8865287CurrentTrain: epoch 15, batch    45 | loss: 21.8188879CurrentTrain: epoch 15, batch    46 | loss: 12.6012943CurrentTrain: epoch 15, batch    47 | loss: 20.1989667CurrentTrain: epoch 15, batch    48 | loss: 14.7787673CurrentTrain: epoch 15, batch    49 | loss: 13.0190167CurrentTrain: epoch 15, batch    50 | loss: 19.7561559CurrentTrain: epoch 15, batch    51 | loss: 13.7291511CurrentTrain: epoch 15, batch    52 | loss: 14.7836816CurrentTrain: epoch 15, batch    53 | loss: 19.3416874CurrentTrain: epoch 15, batch    54 | loss: 19.6706053CurrentTrain: epoch 15, batch    55 | loss: 28.1395844CurrentTrain: epoch 15, batch    56 | loss: 15.7727385CurrentTrain: epoch 15, batch    57 | loss: 16.7225078CurrentTrain: epoch 15, batch    58 | loss: 16.7147863CurrentTrain: epoch 15, batch    59 | loss: 23.0690677CurrentTrain: epoch 15, batch    60 | loss: 12.9936543CurrentTrain: epoch 15, batch    61 | loss: 14.4917221CurrentTrain: epoch  7, batch    62 | loss: 16.0378770CurrentTrain: epoch 15, batch     0 | loss: 14.7964313CurrentTrain: epoch 15, batch     1 | loss: 11.8823316CurrentTrain: epoch 15, batch     2 | loss: 16.1651193CurrentTrain: epoch 15, batch     3 | loss: 11.6225577CurrentTrain: epoch 15, batch     4 | loss: 13.4289100CurrentTrain: epoch 15, batch     5 | loss: 27.8752078CurrentTrain: epoch 15, batch     6 | loss: 19.9774185CurrentTrain: epoch 15, batch     7 | loss: 13.3573958CurrentTrain: epoch 15, batch     8 | loss: 13.1317054CurrentTrain: epoch 15, batch     9 | loss: 11.3760649CurrentTrain: epoch 15, batch    10 | loss: 18.5925027CurrentTrain: epoch 15, batch    11 | loss: 12.7581493CurrentTrain: epoch 15, batch    12 | loss: 22.4548292CurrentTrain: epoch 15, batch    13 | loss: 15.8211997CurrentTrain: epoch 15, batch    14 | loss: 16.9081160CurrentTrain: epoch 15, batch    15 | loss: 20.1989586CurrentTrain: epoch 15, batch    16 | loss: 13.5727658CurrentTrain: epoch 15, batch    17 | loss: 11.5574421CurrentTrain: epoch 15, batch    18 | loss: 28.0426611CurrentTrain: epoch 15, batch    19 | loss: 18.0409579CurrentTrain: epoch 15, batch    20 | loss: 12.7092497CurrentTrain: epoch 15, batch    21 | loss: 17.2625210CurrentTrain: epoch 15, batch    22 | loss: 15.4423352CurrentTrain: epoch 15, batch    23 | loss: 19.9274059CurrentTrain: epoch 15, batch    24 | loss: 12.9613621CurrentTrain: epoch 15, batch    25 | loss: 11.1819028CurrentTrain: epoch 15, batch    26 | loss: 13.5711017CurrentTrain: epoch 15, batch    27 | loss: 12.7287798CurrentTrain: epoch 15, batch    28 | loss: 23.8586567CurrentTrain: epoch 15, batch    29 | loss: 16.9491586CurrentTrain: epoch 15, batch    30 | loss: 13.8708441CurrentTrain: epoch 15, batch    31 | loss: 14.1217166CurrentTrain: epoch 15, batch    32 | loss: 12.1022278CurrentTrain: epoch 15, batch    33 | loss: 14.5083488CurrentTrain: epoch 15, batch    34 | loss: 12.5646049CurrentTrain: epoch 15, batch    35 | loss: 11.2013442CurrentTrain: epoch 15, batch    36 | loss: 11.3345741CurrentTrain: epoch 15, batch    37 | loss: 16.7204001CurrentTrain: epoch 15, batch    38 | loss: 15.7096623CurrentTrain: epoch 15, batch    39 | loss: 25.5925707CurrentTrain: epoch 15, batch    40 | loss: 10.2694929CurrentTrain: epoch 15, batch    41 | loss: 13.0147323CurrentTrain: epoch 15, batch    42 | loss: 24.2457858CurrentTrain: epoch 15, batch    43 | loss: 14.4567422CurrentTrain: epoch 15, batch    44 | loss: 11.3312142CurrentTrain: epoch 15, batch    45 | loss: 26.8843698CurrentTrain: epoch 15, batch    46 | loss: 13.5758729CurrentTrain: epoch 15, batch    47 | loss: 13.1212179CurrentTrain: epoch 15, batch    48 | loss: 18.5532746CurrentTrain: epoch 15, batch    49 | loss: 16.5637410CurrentTrain: epoch 15, batch    50 | loss: 9.9851404CurrentTrain: epoch 15, batch    51 | loss: 16.7777546CurrentTrain: epoch 15, batch    52 | loss: 10.6662769CurrentTrain: epoch 15, batch    53 | loss: 24.2068017CurrentTrain: epoch 15, batch    54 | loss: 13.9882047CurrentTrain: epoch 15, batch    55 | loss: 17.2813335CurrentTrain: epoch 15, batch    56 | loss: 12.9354509CurrentTrain: epoch 15, batch    57 | loss: 9.6584588CurrentTrain: epoch 15, batch    58 | loss: 18.4736106CurrentTrain: epoch 15, batch    59 | loss: 12.8432339CurrentTrain: epoch 15, batch    60 | loss: 13.1256984CurrentTrain: epoch 15, batch    61 | loss: 14.8920267CurrentTrain: epoch  7, batch    62 | loss: 9.5980325CurrentTrain: epoch 15, batch     0 | loss: 11.5300686CurrentTrain: epoch 15, batch     1 | loss: 10.1699970CurrentTrain: epoch 15, batch     2 | loss: 10.5226421CurrentTrain: epoch 15, batch     3 | loss: 16.3044553CurrentTrain: epoch 15, batch     4 | loss: 15.2801062CurrentTrain: epoch 15, batch     5 | loss: 10.2196533CurrentTrain: epoch 15, batch     6 | loss: 12.7390764CurrentTrain: epoch 15, batch     7 | loss: 11.1372090CurrentTrain: epoch 15, batch     8 | loss: 19.0477685CurrentTrain: epoch 15, batch     9 | loss: 14.6195753CurrentTrain: epoch 15, batch    10 | loss: 19.1819851CurrentTrain: epoch 15, batch    11 | loss: 9.7980350CurrentTrain: epoch 15, batch    12 | loss: 10.5659059CurrentTrain: epoch 15, batch    13 | loss: 14.7200088CurrentTrain: epoch 15, batch    14 | loss: 19.5504449CurrentTrain: epoch 15, batch    15 | loss: 19.4967371CurrentTrain: epoch 15, batch    16 | loss: 10.5604687CurrentTrain: epoch 15, batch    17 | loss: 16.1804820CurrentTrain: epoch 15, batch    18 | loss: 26.7441232CurrentTrain: epoch 15, batch    19 | loss: 13.0511914CurrentTrain: epoch 15, batch    20 | loss: 10.7523769CurrentTrain: epoch 15, batch    21 | loss: 16.3800410CurrentTrain: epoch 15, batch    22 | loss: 16.0625967CurrentTrain: epoch 15, batch    23 | loss: 26.3540776CurrentTrain: epoch 15, batch    24 | loss: 11.4480828CurrentTrain: epoch 15, batch    25 | loss: 30.2598578CurrentTrain: epoch 15, batch    26 | loss: 8.5355552CurrentTrain: epoch 15, batch    27 | loss: 10.2622137CurrentTrain: epoch 15, batch    28 | loss: 9.8929394CurrentTrain: epoch 15, batch    29 | loss: 13.1639394CurrentTrain: epoch 15, batch    30 | loss: 24.4336183CurrentTrain: epoch 15, batch    31 | loss: 15.2034516CurrentTrain: epoch 15, batch    32 | loss: 18.7819304CurrentTrain: epoch 15, batch    33 | loss: 16.4350215CurrentTrain: epoch 15, batch    34 | loss: 11.5522655CurrentTrain: epoch 15, batch    35 | loss: 12.1466936CurrentTrain: epoch 15, batch    36 | loss: 13.4369459CurrentTrain: epoch 15, batch    37 | loss: 11.2726855CurrentTrain: epoch 15, batch    38 | loss: 16.9292004CurrentTrain: epoch 15, batch    39 | loss: 16.9121662CurrentTrain: epoch 15, batch    40 | loss: 23.9339921CurrentTrain: epoch 15, batch    41 | loss: 29.9197823CurrentTrain: epoch 15, batch    42 | loss: 11.1639874CurrentTrain: epoch 15, batch    43 | loss: 17.0594184CurrentTrain: epoch 15, batch    44 | loss: 13.2248358CurrentTrain: epoch 15, batch    45 | loss: 17.1346162CurrentTrain: epoch 15, batch    46 | loss: 10.8051359CurrentTrain: epoch 15, batch    47 | loss: 8.8299192CurrentTrain: epoch 15, batch    48 | loss: 8.5345470CurrentTrain: epoch 15, batch    49 | loss: 11.7113773CurrentTrain: epoch 15, batch    50 | loss: 8.0582576CurrentTrain: epoch 15, batch    51 | loss: 12.9993457CurrentTrain: epoch 15, batch    52 | loss: 12.4820065CurrentTrain: epoch 15, batch    53 | loss: 11.7121176CurrentTrain: epoch 15, batch    54 | loss: 9.3154674CurrentTrain: epoch 15, batch    55 | loss: 17.2091048CurrentTrain: epoch 15, batch    56 | loss: 15.7766107CurrentTrain: epoch 15, batch    57 | loss: 9.4731616CurrentTrain: epoch 15, batch    58 | loss: 25.2723201CurrentTrain: epoch 15, batch    59 | loss: 12.6524791CurrentTrain: epoch 15, batch    60 | loss: 19.9598295CurrentTrain: epoch 15, batch    61 | loss: 18.8015870CurrentTrain: epoch  7, batch    62 | loss: 7.5477463CurrentTrain: epoch 15, batch     0 | loss: 28.9225132CurrentTrain: epoch 15, batch     1 | loss: 27.8830884CurrentTrain: epoch 15, batch     2 | loss: 13.3206663CurrentTrain: epoch 15, batch     3 | loss: 14.4183202CurrentTrain: epoch 15, batch     4 | loss: 11.5101475CurrentTrain: epoch 15, batch     5 | loss: 10.8266785CurrentTrain: epoch 15, batch     6 | loss: 9.6022919CurrentTrain: epoch 15, batch     7 | loss: 17.6953958CurrentTrain: epoch 15, batch     8 | loss: 10.7244200CurrentTrain: epoch 15, batch     9 | loss: 10.1080922CurrentTrain: epoch 15, batch    10 | loss: 28.5590498CurrentTrain: epoch 15, batch    11 | loss: 10.2627950CurrentTrain: epoch 15, batch    12 | loss: 10.6150508CurrentTrain: epoch 15, batch    13 | loss: 9.8841671CurrentTrain: epoch 15, batch    14 | loss: 13.7040899CurrentTrain: epoch 15, batch    15 | loss: 8.0898671CurrentTrain: epoch 15, batch    16 | loss: 26.9780681CurrentTrain: epoch 15, batch    17 | loss: 12.8125187CurrentTrain: epoch 15, batch    18 | loss: 10.4738736CurrentTrain: epoch 15, batch    19 | loss: 9.1553468CurrentTrain: epoch 15, batch    20 | loss: 16.3863451CurrentTrain: epoch 15, batch    21 | loss: 11.9800335CurrentTrain: epoch 15, batch    22 | loss: 10.6336512CurrentTrain: epoch 15, batch    23 | loss: 24.1149140CurrentTrain: epoch 15, batch    24 | loss: 9.8136967CurrentTrain: epoch 15, batch    25 | loss: 12.2192218CurrentTrain: epoch 15, batch    26 | loss: 10.4745609CurrentTrain: epoch 15, batch    27 | loss: 10.2582719CurrentTrain: epoch 15, batch    28 | loss: 9.9439258CurrentTrain: epoch 15, batch    29 | loss: 17.4486168CurrentTrain: epoch 15, batch    30 | loss: 16.9790432CurrentTrain: epoch 15, batch    31 | loss: 12.4689689CurrentTrain: epoch 15, batch    32 | loss: 7.5151327CurrentTrain: epoch 15, batch    33 | loss: 7.9495640CurrentTrain: epoch 15, batch    34 | loss: 7.3542879CurrentTrain: epoch 15, batch    35 | loss: 12.4547046CurrentTrain: epoch 15, batch    36 | loss: 10.4196454CurrentTrain: epoch 15, batch    37 | loss: 18.6785400CurrentTrain: epoch 15, batch    38 | loss: 20.4889788CurrentTrain: epoch 15, batch    39 | loss: 10.2174892CurrentTrain: epoch 15, batch    40 | loss: 18.6729443CurrentTrain: epoch 15, batch    41 | loss: 11.4660154CurrentTrain: epoch 15, batch    42 | loss: 10.9639615CurrentTrain: epoch 15, batch    43 | loss: 14.0212067CurrentTrain: epoch 15, batch    44 | loss: 9.8598459CurrentTrain: epoch 15, batch    45 | loss: 14.3285418CurrentTrain: epoch 15, batch    46 | loss: 10.9594622CurrentTrain: epoch 15, batch    47 | loss: 16.4862954CurrentTrain: epoch 15, batch    48 | loss: 15.1737253CurrentTrain: epoch 15, batch    49 | loss: 12.0937175CurrentTrain: epoch 15, batch    50 | loss: 7.8583260CurrentTrain: epoch 15, batch    51 | loss: 16.1880234CurrentTrain: epoch 15, batch    52 | loss: 9.0241025CurrentTrain: epoch 15, batch    53 | loss: 7.9712077CurrentTrain: epoch 15, batch    54 | loss: 11.9719515CurrentTrain: epoch 15, batch    55 | loss: 11.4304223CurrentTrain: epoch 15, batch    56 | loss: 11.7644261CurrentTrain: epoch 15, batch    57 | loss: 9.0173218CurrentTrain: epoch 15, batch    58 | loss: 10.5138565CurrentTrain: epoch 15, batch    59 | loss: 9.0256538CurrentTrain: epoch 15, batch    60 | loss: 10.9464653CurrentTrain: epoch 15, batch    61 | loss: 9.0704718CurrentTrain: epoch  7, batch    62 | loss: 9.2700584CurrentTrain: epoch 15, batch     0 | loss: 9.9565687CurrentTrain: epoch 15, batch     1 | loss: 12.0886375CurrentTrain: epoch 15, batch     2 | loss: 11.5279305CurrentTrain: epoch 15, batch     3 | loss: 7.3574951CurrentTrain: epoch 15, batch     4 | loss: 14.0924476CurrentTrain: epoch 15, batch     5 | loss: 14.0565579CurrentTrain: epoch 15, batch     6 | loss: 26.5545484CurrentTrain: epoch 15, batch     7 | loss: 12.3867309CurrentTrain: epoch 15, batch     8 | loss: 10.3967995CurrentTrain: epoch 15, batch     9 | loss: 13.3854217CurrentTrain: epoch 15, batch    10 | loss: 10.9837087CurrentTrain: epoch 15, batch    11 | loss: 13.5761400CurrentTrain: epoch 15, batch    12 | loss: 11.6216668CurrentTrain: epoch 15, batch    13 | loss: 9.1256064CurrentTrain: epoch 15, batch    14 | loss: 10.5551792CurrentTrain: epoch 15, batch    15 | loss: 17.2922212CurrentTrain: epoch 15, batch    16 | loss: 22.3116524CurrentTrain: epoch 15, batch    17 | loss: 11.4078169CurrentTrain: epoch 15, batch    18 | loss: 9.1924376CurrentTrain: epoch 15, batch    19 | loss: 18.3912596CurrentTrain: epoch 15, batch    20 | loss: 12.2939077CurrentTrain: epoch 15, batch    21 | loss: 18.0127302CurrentTrain: epoch 15, batch    22 | loss: 7.2153438CurrentTrain: epoch 15, batch    23 | loss: 10.5080099CurrentTrain: epoch 15, batch    24 | loss: 12.3180506CurrentTrain: epoch 15, batch    25 | loss: 15.2883328CurrentTrain: epoch 15, batch    26 | loss: 7.6014502CurrentTrain: epoch 15, batch    27 | loss: 14.0623418CurrentTrain: epoch 15, batch    28 | loss: 9.0139364CurrentTrain: epoch 15, batch    29 | loss: 7.3769539CurrentTrain: epoch 15, batch    30 | loss: 11.3717711CurrentTrain: epoch 15, batch    31 | loss: 24.7270245CurrentTrain: epoch 15, batch    32 | loss: 16.2813528CurrentTrain: epoch 15, batch    33 | loss: 8.9599935CurrentTrain: epoch 15, batch    34 | loss: 12.4879715CurrentTrain: epoch 15, batch    35 | loss: 13.0988988CurrentTrain: epoch 15, batch    36 | loss: 15.1543494CurrentTrain: epoch 15, batch    37 | loss: 15.8578420CurrentTrain: epoch 15, batch    38 | loss: 13.8455630CurrentTrain: epoch 15, batch    39 | loss: 11.4452334CurrentTrain: epoch 15, batch    40 | loss: 14.1602593CurrentTrain: epoch 15, batch    41 | loss: 22.2339907CurrentTrain: epoch 15, batch    42 | loss: 13.1145043CurrentTrain: epoch 15, batch    43 | loss: 14.5065165CurrentTrain: epoch 15, batch    44 | loss: 10.6750524CurrentTrain: epoch 15, batch    45 | loss: 8.8991097CurrentTrain: epoch 15, batch    46 | loss: 13.5153249CurrentTrain: epoch 15, batch    47 | loss: 21.5081007CurrentTrain: epoch 15, batch    48 | loss: 7.8413979CurrentTrain: epoch 15, batch    49 | loss: 8.7249392CurrentTrain: epoch 15, batch    50 | loss: 13.5639769CurrentTrain: epoch 15, batch    51 | loss: 9.0423865CurrentTrain: epoch 15, batch    52 | loss: 10.0514140CurrentTrain: epoch 15, batch    53 | loss: 21.7578956CurrentTrain: epoch 15, batch    54 | loss: 10.8069569CurrentTrain: epoch 15, batch    55 | loss: 16.3514191CurrentTrain: epoch 15, batch    56 | loss: 12.1136591CurrentTrain: epoch 15, batch    57 | loss: 11.1206225CurrentTrain: epoch 15, batch    58 | loss: 9.7985619CurrentTrain: epoch 15, batch    59 | loss: 9.8521034CurrentTrain: epoch 15, batch    60 | loss: 8.9512005CurrentTrain: epoch 15, batch    61 | loss: 9.9136398CurrentTrain: epoch  7, batch    62 | loss: 13.9750861CurrentTrain: epoch 15, batch     0 | loss: 12.7173847CurrentTrain: epoch 15, batch     1 | loss: 7.4801742CurrentTrain: epoch 15, batch     2 | loss: 9.9690398CurrentTrain: epoch 15, batch     3 | loss: 10.0308443CurrentTrain: epoch 15, batch     4 | loss: 14.3584627CurrentTrain: epoch 15, batch     5 | loss: 8.8550869CurrentTrain: epoch 15, batch     6 | loss: 15.9422147CurrentTrain: epoch 15, batch     7 | loss: 15.4928202CurrentTrain: epoch 15, batch     8 | loss: 14.6652326CurrentTrain: epoch 15, batch     9 | loss: 10.1680413CurrentTrain: epoch 15, batch    10 | loss: 9.5335291CurrentTrain: epoch 15, batch    11 | loss: 15.2263294CurrentTrain: epoch 15, batch    12 | loss: 8.8917642CurrentTrain: epoch 15, batch    13 | loss: 16.3699367CurrentTrain: epoch 15, batch    14 | loss: 16.8130976CurrentTrain: epoch 15, batch    15 | loss: 9.5299365CurrentTrain: epoch 15, batch    16 | loss: 16.7830575CurrentTrain: epoch 15, batch    17 | loss: 17.5852329CurrentTrain: epoch 15, batch    18 | loss: 9.6972681CurrentTrain: epoch 15, batch    19 | loss: 8.3543070CurrentTrain: epoch 15, batch    20 | loss: 18.0183206CurrentTrain: epoch 15, batch    21 | loss: 16.6665230CurrentTrain: epoch 15, batch    22 | loss: 8.9892063CurrentTrain: epoch 15, batch    23 | loss: 7.7622090CurrentTrain: epoch 15, batch    24 | loss: 13.0236393CurrentTrain: epoch 15, batch    25 | loss: 13.4794165CurrentTrain: epoch 15, batch    26 | loss: 12.4830079CurrentTrain: epoch 15, batch    27 | loss: 13.3537870CurrentTrain: epoch 15, batch    28 | loss: 12.5590096CurrentTrain: epoch 15, batch    29 | loss: 9.2345893CurrentTrain: epoch 15, batch    30 | loss: 10.9236639CurrentTrain: epoch 15, batch    31 | loss: 13.3210962CurrentTrain: epoch 15, batch    32 | loss: 7.9749725CurrentTrain: epoch 15, batch    33 | loss: 9.2592807CurrentTrain: epoch 15, batch    34 | loss: 14.0000161CurrentTrain: epoch 15, batch    35 | loss: 12.4997290CurrentTrain: epoch 15, batch    36 | loss: 9.9053093CurrentTrain: epoch 15, batch    37 | loss: 9.5106412CurrentTrain: epoch 15, batch    38 | loss: 10.4735535CurrentTrain: epoch 15, batch    39 | loss: 33.3735360CurrentTrain: epoch 15, batch    40 | loss: 8.7860604CurrentTrain: epoch 15, batch    41 | loss: 10.1450762CurrentTrain: epoch 15, batch    42 | loss: 11.0721106CurrentTrain: epoch 15, batch    43 | loss: 13.1883783CurrentTrain: epoch 15, batch    44 | loss: 16.1356477CurrentTrain: epoch 15, batch    45 | loss: 10.3523852CurrentTrain: epoch 15, batch    46 | loss: 9.4713936CurrentTrain: epoch 15, batch    47 | loss: 11.7392879CurrentTrain: epoch 15, batch    48 | loss: 14.0409760CurrentTrain: epoch 15, batch    49 | loss: 33.1297982CurrentTrain: epoch 15, batch    50 | loss: 11.1160735CurrentTrain: epoch 15, batch    51 | loss: 12.4125991CurrentTrain: epoch 15, batch    52 | loss: 7.8137673CurrentTrain: epoch 15, batch    53 | loss: 11.0361406CurrentTrain: epoch 15, batch    54 | loss: 11.7094655CurrentTrain: epoch 15, batch    55 | loss: 12.8517195CurrentTrain: epoch 15, batch    56 | loss: 12.2688784CurrentTrain: epoch 15, batch    57 | loss: 12.2974715CurrentTrain: epoch 15, batch    58 | loss: 16.0368946CurrentTrain: epoch 15, batch    59 | loss: 9.3038193CurrentTrain: epoch 15, batch    60 | loss: 8.1012632CurrentTrain: epoch 15, batch    61 | loss: 7.8028560CurrentTrain: epoch  7, batch    62 | loss: 8.8807785CurrentTrain: epoch 15, batch     0 | loss: 10.0046196CurrentTrain: epoch 15, batch     1 | loss: 9.7307351CurrentTrain: epoch 15, batch     2 | loss: 16.9304315CurrentTrain: epoch 15, batch     3 | loss: 16.0128181CurrentTrain: epoch 15, batch     4 | loss: 15.8118044CurrentTrain: epoch 15, batch     5 | loss: 10.1467952CurrentTrain: epoch 15, batch     6 | loss: 13.9251215CurrentTrain: epoch 15, batch     7 | loss: 7.3930726CurrentTrain: epoch 15, batch     8 | loss: 8.6987703CurrentTrain: epoch 15, batch     9 | loss: 9.2589906CurrentTrain: epoch 15, batch    10 | loss: 9.1750555CurrentTrain: epoch 15, batch    11 | loss: 8.7631480CurrentTrain: epoch 15, batch    12 | loss: 17.3223119CurrentTrain: epoch 15, batch    13 | loss: 10.2997061CurrentTrain: epoch 15, batch    14 | loss: 7.0935299CurrentTrain: epoch 15, batch    15 | loss: 10.5360659CurrentTrain: epoch 15, batch    16 | loss: 21.6625971CurrentTrain: epoch 15, batch    17 | loss: 12.3970637CurrentTrain: epoch 15, batch    18 | loss: 12.3346122CurrentTrain: epoch 15, batch    19 | loss: 17.1937392CurrentTrain: epoch 15, batch    20 | loss: 11.5583023CurrentTrain: epoch 15, batch    21 | loss: 8.9027094CurrentTrain: epoch 15, batch    22 | loss: 21.3232997CurrentTrain: epoch 15, batch    23 | loss: 10.8099165CurrentTrain: epoch 15, batch    24 | loss: 14.3510238CurrentTrain: epoch 15, batch    25 | loss: 12.1904901CurrentTrain: epoch 15, batch    26 | loss: 7.0031053CurrentTrain: epoch 15, batch    27 | loss: 17.1383722CurrentTrain: epoch 15, batch    28 | loss: 6.7919411CurrentTrain: epoch 15, batch    29 | loss: 9.4338786CurrentTrain: epoch 15, batch    30 | loss: 10.3479407CurrentTrain: epoch 15, batch    31 | loss: 9.8655104CurrentTrain: epoch 15, batch    32 | loss: 9.7476835CurrentTrain: epoch 15, batch    33 | loss: 7.6970974CurrentTrain: epoch 15, batch    34 | loss: 8.3984699CurrentTrain: epoch 15, batch    35 | loss: 9.7379021CurrentTrain: epoch 15, batch    36 | loss: 9.6138518CurrentTrain: epoch 15, batch    37 | loss: 9.6225335CurrentTrain: epoch 15, batch    38 | loss: 9.4348707CurrentTrain: epoch 15, batch    39 | loss: 9.1996807CurrentTrain: epoch 15, batch    40 | loss: 16.0368003CurrentTrain: epoch 15, batch    41 | loss: 9.1564651CurrentTrain: epoch 15, batch    42 | loss: 11.8090075CurrentTrain: epoch 15, batch    43 | loss: 25.2777617CurrentTrain: epoch 15, batch    44 | loss: 15.8287647CurrentTrain: epoch 15, batch    45 | loss: 11.5545590CurrentTrain: epoch 15, batch    46 | loss: 9.6640944CurrentTrain: epoch 15, batch    47 | loss: 18.1339828CurrentTrain: epoch 15, batch    48 | loss: 9.4530622CurrentTrain: epoch 15, batch    49 | loss: 9.3430793CurrentTrain: epoch 15, batch    50 | loss: 10.5575185CurrentTrain: epoch 15, batch    51 | loss: 11.8265748CurrentTrain: epoch 15, batch    52 | loss: 10.5989948CurrentTrain: epoch 15, batch    53 | loss: 10.4720578CurrentTrain: epoch 15, batch    54 | loss: 9.9620766CurrentTrain: epoch 15, batch    55 | loss: 10.9610750CurrentTrain: epoch 15, batch    56 | loss: 11.2282098CurrentTrain: epoch 15, batch    57 | loss: 14.2263198CurrentTrain: epoch 15, batch    58 | loss: 20.4269366CurrentTrain: epoch 15, batch    59 | loss: 11.6923365CurrentTrain: epoch 15, batch    60 | loss: 12.0743640CurrentTrain: epoch 15, batch    61 | loss: 9.1627080CurrentTrain: epoch  7, batch    62 | loss: 20.6381680CurrentTrain: epoch 15, batch     0 | loss: 9.6416938CurrentTrain: epoch 15, batch     1 | loss: 12.2645120CurrentTrain: epoch 15, batch     2 | loss: 11.4328213CurrentTrain: epoch 15, batch     3 | loss: 12.7914269CurrentTrain: epoch 15, batch     4 | loss: 13.9904605CurrentTrain: epoch 15, batch     5 | loss: 18.1582789CurrentTrain: epoch 15, batch     6 | loss: 9.3518874CurrentTrain: epoch 15, batch     7 | loss: 9.7597817CurrentTrain: epoch 15, batch     8 | loss: 17.9130715CurrentTrain: epoch 15, batch     9 | loss: 8.4993537CurrentTrain: epoch 15, batch    10 | loss: 13.0091950CurrentTrain: epoch 15, batch    11 | loss: 10.2741298CurrentTrain: epoch 15, batch    12 | loss: 13.2360336CurrentTrain: epoch 15, batch    13 | loss: 9.8906233CurrentTrain: epoch 15, batch    14 | loss: 16.8850530CurrentTrain: epoch 15, batch    15 | loss: 11.2260702CurrentTrain: epoch 15, batch    16 | loss: 9.6858789CurrentTrain: epoch 15, batch    17 | loss: 8.4566961CurrentTrain: epoch 15, batch    18 | loss: 7.8991468CurrentTrain: epoch 15, batch    19 | loss: 8.5567025CurrentTrain: epoch 15, batch    20 | loss: 12.6695124CurrentTrain: epoch 15, batch    21 | loss: 9.8679547CurrentTrain: epoch 15, batch    22 | loss: 12.9463281CurrentTrain: epoch 15, batch    23 | loss: 10.3838557CurrentTrain: epoch 15, batch    24 | loss: 10.2745539CurrentTrain: epoch 15, batch    25 | loss: 17.0046048CurrentTrain: epoch 15, batch    26 | loss: 13.2422337CurrentTrain: epoch 15, batch    27 | loss: 8.9407895CurrentTrain: epoch 15, batch    28 | loss: 8.1379872CurrentTrain: epoch 15, batch    29 | loss: 12.5076149CurrentTrain: epoch 15, batch    30 | loss: 8.2604637CurrentTrain: epoch 15, batch    31 | loss: 7.9177743CurrentTrain: epoch 15, batch    32 | loss: 16.7083527CurrentTrain: epoch 15, batch    33 | loss: 10.4511443CurrentTrain: epoch 15, batch    34 | loss: 9.7251314CurrentTrain: epoch 15, batch    35 | loss: 8.2839960CurrentTrain: epoch 15, batch    36 | loss: 15.9314171CurrentTrain: epoch 15, batch    37 | loss: 10.9428267CurrentTrain: epoch 15, batch    38 | loss: 6.9570685CurrentTrain: epoch 15, batch    39 | loss: 9.6745655CurrentTrain: epoch 15, batch    40 | loss: 7.0829979CurrentTrain: epoch 15, batch    41 | loss: 16.4128931CurrentTrain: epoch 15, batch    42 | loss: 8.7079570CurrentTrain: epoch 15, batch    43 | loss: 8.9688800CurrentTrain: epoch 15, batch    44 | loss: 13.9244988CurrentTrain: epoch 15, batch    45 | loss: 8.9594814CurrentTrain: epoch 15, batch    46 | loss: 15.6664574CurrentTrain: epoch 15, batch    47 | loss: 6.7873928CurrentTrain: epoch 15, batch    48 | loss: 13.0577601CurrentTrain: epoch 15, batch    49 | loss: 9.1753016CurrentTrain: epoch 15, batch    50 | loss: 15.4956272CurrentTrain: epoch 15, batch    51 | loss: 10.4553941CurrentTrain: epoch 15, batch    52 | loss: 20.9831987CurrentTrain: epoch 15, batch    53 | loss: 13.9386473CurrentTrain: epoch 15, batch    54 | loss: 10.4546977CurrentTrain: epoch 15, batch    55 | loss: 7.3888760CurrentTrain: epoch 15, batch    56 | loss: 6.7603150CurrentTrain: epoch 15, batch    57 | loss: 13.3839266CurrentTrain: epoch 15, batch    58 | loss: 10.6969557CurrentTrain: epoch 15, batch    59 | loss: 8.1848678CurrentTrain: epoch 15, batch    60 | loss: 6.1151033CurrentTrain: epoch 15, batch    61 | loss: 6.7046441CurrentTrain: epoch  7, batch    62 | loss: 19.6085624CurrentTrain: epoch 15, batch     0 | loss: 15.0718259CurrentTrain: epoch 15, batch     1 | loss: 16.1462585CurrentTrain: epoch 15, batch     2 | loss: 12.1687016CurrentTrain: epoch 15, batch     3 | loss: 14.0969516CurrentTrain: epoch 15, batch     4 | loss: 20.6623638CurrentTrain: epoch 15, batch     5 | loss: 7.9082179CurrentTrain: epoch 15, batch     6 | loss: 10.3397646CurrentTrain: epoch 15, batch     7 | loss: 20.5765713CurrentTrain: epoch 15, batch     8 | loss: 8.3752539CurrentTrain: epoch 15, batch     9 | loss: 12.5647716CurrentTrain: epoch 15, batch    10 | loss: 8.8184268CurrentTrain: epoch 15, batch    11 | loss: 16.2722069CurrentTrain: epoch 15, batch    12 | loss: 12.9268665CurrentTrain: epoch 15, batch    13 | loss: 12.7108017CurrentTrain: epoch 15, batch    14 | loss: 10.5662458CurrentTrain: epoch 15, batch    15 | loss: 14.9751998CurrentTrain: epoch 15, batch    16 | loss: 8.9307687CurrentTrain: epoch 15, batch    17 | loss: 11.5837097CurrentTrain: epoch 15, batch    18 | loss: 15.4372169CurrentTrain: epoch 15, batch    19 | loss: 15.1553758CurrentTrain: epoch 15, batch    20 | loss: 7.0515478CurrentTrain: epoch 15, batch    21 | loss: 8.0249304CurrentTrain: epoch 15, batch    22 | loss: 8.1543459CurrentTrain: epoch 15, batch    23 | loss: 9.5661515CurrentTrain: epoch 15, batch    24 | loss: 8.8508322CurrentTrain: epoch 15, batch    25 | loss: 8.5845813CurrentTrain: epoch 15, batch    26 | loss: 8.5749358CurrentTrain: epoch 15, batch    27 | loss: 9.7820316CurrentTrain: epoch 15, batch    28 | loss: 14.0672627CurrentTrain: epoch 15, batch    29 | loss: 10.7899261CurrentTrain: epoch 15, batch    30 | loss: 13.8453034CurrentTrain: epoch 15, batch    31 | loss: 10.6525340CurrentTrain: epoch 15, batch    32 | loss: 5.8042624CurrentTrain: epoch 15, batch    33 | loss: 14.6741254CurrentTrain: epoch 15, batch    34 | loss: 9.5152808CurrentTrain: epoch 15, batch    35 | loss: 10.6055048CurrentTrain: epoch 15, batch    36 | loss: 7.9734306CurrentTrain: epoch 15, batch    37 | loss: 12.4008032CurrentTrain: epoch 15, batch    38 | loss: 14.6077241CurrentTrain: epoch 15, batch    39 | loss: 9.4640357CurrentTrain: epoch 15, batch    40 | loss: 8.7562849CurrentTrain: epoch 15, batch    41 | loss: 11.9201156CurrentTrain: epoch 15, batch    42 | loss: 8.9880820CurrentTrain: epoch 15, batch    43 | loss: 10.7450841CurrentTrain: epoch 15, batch    44 | loss: 15.0131215CurrentTrain: epoch 15, batch    45 | loss: 12.3428329CurrentTrain: epoch 15, batch    46 | loss: 18.0476940CurrentTrain: epoch 15, batch    47 | loss: 10.1123473CurrentTrain: epoch 15, batch    48 | loss: 19.9360255CurrentTrain: epoch 15, batch    49 | loss: 6.9056527CurrentTrain: epoch 15, batch    50 | loss: 13.4928758CurrentTrain: epoch 15, batch    51 | loss: 14.1914903CurrentTrain: epoch 15, batch    52 | loss: 9.1381645CurrentTrain: epoch 15, batch    53 | loss: 7.1555969CurrentTrain: epoch 15, batch    54 | loss: 13.0358180CurrentTrain: epoch 15, batch    55 | loss: 21.7378018CurrentTrain: epoch 15, batch    56 | loss: 11.2601511CurrentTrain: epoch 15, batch    57 | loss: 6.7121646CurrentTrain: epoch 15, batch    58 | loss: 10.0706416CurrentTrain: epoch 15, batch    59 | loss: 9.5851857CurrentTrain: epoch 15, batch    60 | loss: 7.4234725CurrentTrain: epoch 15, batch    61 | loss: 7.5604925CurrentTrain: epoch  7, batch    62 | loss: 15.4754112CurrentTrain: epoch 15, batch     0 | loss: 7.1376823CurrentTrain: epoch 15, batch     1 | loss: 7.6163596CurrentTrain: epoch 15, batch     2 | loss: 14.3488874CurrentTrain: epoch 15, batch     3 | loss: 8.9842660CurrentTrain: epoch 15, batch     4 | loss: 33.0724227CurrentTrain: epoch 15, batch     5 | loss: 17.9938001CurrentTrain: epoch 15, batch     6 | loss: 13.5910108CurrentTrain: epoch 15, batch     7 | loss: 17.5551101CurrentTrain: epoch 15, batch     8 | loss: 12.5595731CurrentTrain: epoch 15, batch     9 | loss: 16.9781529CurrentTrain: epoch 15, batch    10 | loss: 7.0835238CurrentTrain: epoch 15, batch    11 | loss: 9.1209576CurrentTrain: epoch 15, batch    12 | loss: 8.5084298CurrentTrain: epoch 15, batch    13 | loss: 8.0999422CurrentTrain: epoch 15, batch    14 | loss: 15.7572361CurrentTrain: epoch 15, batch    15 | loss: 16.5523888CurrentTrain: epoch 15, batch    16 | loss: 15.7581857CurrentTrain: epoch 15, batch    17 | loss: 9.6695870CurrentTrain: epoch 15, batch    18 | loss: 12.9462629CurrentTrain: epoch 15, batch    19 | loss: 13.9000964CurrentTrain: epoch 15, batch    20 | loss: 19.3780626CurrentTrain: epoch 15, batch    21 | loss: 12.3752680CurrentTrain: epoch 15, batch    22 | loss: 10.4876797CurrentTrain: epoch 15, batch    23 | loss: 21.3882564CurrentTrain: epoch 15, batch    24 | loss: 13.8109513CurrentTrain: epoch 15, batch    25 | loss: 9.2839795CurrentTrain: epoch 15, batch    26 | loss: 14.9381424CurrentTrain: epoch 15, batch    27 | loss: 14.8729886CurrentTrain: epoch 15, batch    28 | loss: 10.9402973CurrentTrain: epoch 15, batch    29 | loss: 15.1536962CurrentTrain: epoch 15, batch    30 | loss: 12.4127151CurrentTrain: epoch 15, batch    31 | loss: 13.8760616CurrentTrain: epoch 15, batch    32 | loss: 15.7679189CurrentTrain: epoch 15, batch    33 | loss: 10.4178414CurrentTrain: epoch 15, batch    34 | loss: 11.2431930CurrentTrain: epoch 15, batch    35 | loss: 9.2748637CurrentTrain: epoch 15, batch    36 | loss: 9.7452850CurrentTrain: epoch 15, batch    37 | loss: 10.5860887CurrentTrain: epoch 15, batch    38 | loss: 6.1195130CurrentTrain: epoch 15, batch    39 | loss: 7.2384374CurrentTrain: epoch 15, batch    40 | loss: 24.1718154CurrentTrain: epoch 15, batch    41 | loss: 9.0547835CurrentTrain: epoch 15, batch    42 | loss: 10.7185957CurrentTrain: epoch 15, batch    43 | loss: 12.9803667CurrentTrain: epoch 15, batch    44 | loss: 10.3019603CurrentTrain: epoch 15, batch    45 | loss: 20.6051670CurrentTrain: epoch 15, batch    46 | loss: 10.2303772CurrentTrain: epoch 15, batch    47 | loss: 6.5834617CurrentTrain: epoch 15, batch    48 | loss: 10.3429618CurrentTrain: epoch 15, batch    49 | loss: 17.2419509CurrentTrain: epoch 15, batch    50 | loss: 8.5127860CurrentTrain: epoch 15, batch    51 | loss: 20.8645779CurrentTrain: epoch 15, batch    52 | loss: 10.7025467CurrentTrain: epoch 15, batch    53 | loss: 13.4711953CurrentTrain: epoch 15, batch    54 | loss: 10.9050537CurrentTrain: epoch 15, batch    55 | loss: 11.5188749CurrentTrain: epoch 15, batch    56 | loss: 13.0504987CurrentTrain: epoch 15, batch    57 | loss: 6.6527436CurrentTrain: epoch 15, batch    58 | loss: 6.5474402CurrentTrain: epoch 15, batch    59 | loss: 21.6741118CurrentTrain: epoch 15, batch    60 | loss: 8.7579522CurrentTrain: epoch 15, batch    61 | loss: 13.4136537CurrentTrain: epoch  7, batch    62 | loss: 6.5911018
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: richard mudge ( born 1718 in bideford ; died april 1763 in bedworth ) was an english clergyman and composer of the late baroque period .
Head Entity: richard mudge
Tail Entity: baroque
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: the impressionist movement, which emerged in the late 19th century, was characterized by a focus on capturing light and natural forms, and it included notable artists such as claude monet and edgar degas.  
Head Entity: impressionist movement  
Tail Entity: impressionism  

Relation: movement  
Context: the existentialist movement gained prominence in the 20th century, with philosophers like jean-paul sartre and simone de beauvoir exploring themes of freedom, choice, and the absurd.  
Head Entity: existentialist movement  
Tail Entity: existentialism  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: lyons hill is a restored village , and former parish with church , now part of the community of ardclough in north county kildare .
Head Entity: ardclough
Tail Entity: county kildare
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: The city of San Francisco is known for its iconic Golden Gate Bridge and is part of the larger San Francisco County in California.  
Head Entity: San Francisco  
Tail Entity: San Francisco County  

Relation: located in the administrative territorial entity  
Context: The historic town of Bath is famous for its Roman baths and is situated within the unitary authority of Bath and North East Somerset in England.  
Head Entity: Bath  
Tail Entity: Bath and North East Somerset  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the nation as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: he also went on a road trip from san antonio to oakland for the magnum project “ postcards from america ” with jim goldberg , alec soth , mikhael subotzky and susan meiselas .
Head Entity: jim goldberg
Tail Entity: magnum
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: After years of dedication and hard work, she was finally recognized as a key member of the prestigious art collective known as the "Creative Minds."  
Head Entity: she  
Tail Entity: Creative Minds  

Relation: member of  
Context: The renowned guitarist joined the band last year, bringing a fresh sound to the group and solidifying his status as a member of the legendary rock ensemble, "The Echoes."  
Head Entity: guitarist  
Tail Entity: The Echoes  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: us 30 currently passes through pittsburgh on the penn - lincoln parkway , crossing the monongahela river on the fort pitt bridge .
Head Entity: fort pitt bridge
Tail Entity: monongahela river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: the new highway will traverse the city, crossing the grand river at several points, including the newly constructed bridge.  
Head Entity: newly constructed bridge  
Tail Entity: grand river  

Relation: crosses  
Context: during the hike, we encountered a beautiful trail that crosses the crystal clear stream, providing stunning views of the surrounding nature.  
Head Entity: trail  
Tail Entity: crystal clear stream  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: it is on the panj river just downstream from the mouth of the bartang river at the point where the panj briefly turns west before resuming its northerly course .
Head Entity: panj river
Tail Entity: bartang river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The Missouri River, which is a major tributary of the Mississippi River, flows through several states before joining the larger watercourse.  
Head Entity: Missouri River  
Tail Entity: Mississippi River  

Relation: tributary  
Context: The Rio Grande serves as a natural border and has several tributaries, including the Pecos River, which contributes to its flow.  
Head Entity: Pecos River  
Tail Entity: Rio Grande  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: on 3 july 1937 , the count married rocafort - atuzarrra in a lavish wedding in havana , cuba attended by the president of cuba , federico laredo brú .
Head Entity: federico laredo brú
Tail Entity: president of cuba
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: After serving as the mayor of New York City for eight years, Bill de Blasio announced his candidacy for the presidency in 2020.  
Head Entity: Bill de Blasio  
Tail Entity: mayor of New York City  

Relation: position held  
Context: Angela Merkel served as the Chancellor of Germany for 16 years, becoming one of the longest-serving leaders in the country's history.  
Head Entity: Angela Merkel  
Tail Entity: Chancellor of Germany  
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: a headcrab is a fictional alien parasitoid found in the " half - life " video game series created by valve software .
Head Entity: half - life
Tail Entity: valve software
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: The popular game "The Witcher 3: Wild Hunt" was developed by CD Projekt Red, a well-known Polish video game developer.  
Head Entity: The Witcher 3: Wild Hunt  
Tail Entity: CD Projekt Red  

Relation: developer  
Context: The innovative mobile game "Angry Birds" was created by Rovio Entertainment, a Finnish company that specializes in game development.  
Head Entity: Angry Birds  
Tail Entity: Rovio Entertainment  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: holy molar are a san diego based band formed in 2001 .
Head Entity: holy molar
Tail Entity: san diego
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the tech startup was established in silicon valley in 2015.  
Head Entity: the tech startup  
Tail Entity: silicon valley  

Relation: location of formation  
Context: the famous rock band was created in new york city during the summer of 1999.  
Head Entity: the famous rock band  
Tail Entity: new york city  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: she has appeared in some american television shows , beginning with her regular role as snow white 's stepmother , evil queen lillian " lily " white in the series " the charmings " .
Head Entity: the charmings
Tail Entity: american
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous dish sushi is traditionally made with vinegared rice and often includes seafood, vegetables, and occasionally tropical fruits, originating from Japan.  
Head Entity: sushi  
Tail Entity: Japan  

Relation: country of origin  
Context: The iconic painting "Mona Lisa," known for its captivating smile, was created by the renowned artist Leonardo da Vinci in Italy during the Renaissance period.  
Head Entity: Mona Lisa  
Tail Entity: Italy  
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 94.27%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 94.14%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 94.44%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 94.74%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 94.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.94%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 94.32%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.57%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.53%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.47%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.44%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.42%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.40%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.92%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.08%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.22%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.39%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.51%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.62%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.73%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.93%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.02%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 96.11%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.20%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.14%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 96.22%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.30%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.32%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 96.15%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 96.11%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.18%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.91%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.91%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 96.04%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 96.11%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 96.07%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.34%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 94.27%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 94.14%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 94.44%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 94.74%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 94.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.94%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 94.32%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.57%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.53%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.47%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.44%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.42%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.40%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.92%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.08%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.22%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.39%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.51%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.62%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.73%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.93%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.02%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 96.11%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.20%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.14%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 96.22%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.30%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.32%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 96.15%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 96.11%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.18%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.91%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.91%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 96.04%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 96.11%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 96.07%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.34%   
cur_acc:  ['0.9534']
his_acc:  ['0.9534']
CurrentTrain: epoch 15, batch     0 | loss: 38.9960294CurrentTrain: epoch 15, batch     1 | loss: 17.0370382CurrentTrain: epoch 15, batch     2 | loss: 15.8284986CurrentTrain: epoch  1, batch     3 | loss: 9.8943993CurrentTrain: epoch 15, batch     0 | loss: 16.2577835CurrentTrain: epoch 15, batch     1 | loss: 20.4142962CurrentTrain: epoch 15, batch     2 | loss: 13.2033134CurrentTrain: epoch  1, batch     3 | loss: 10.8208407CurrentTrain: epoch 15, batch     0 | loss: 17.5995800CurrentTrain: epoch 15, batch     1 | loss: 13.0567064CurrentTrain: epoch 15, batch     2 | loss: 12.9113078CurrentTrain: epoch  1, batch     3 | loss: 8.4025195CurrentTrain: epoch 15, batch     0 | loss: 14.9936732CurrentTrain: epoch 15, batch     1 | loss: 16.9717269CurrentTrain: epoch 15, batch     2 | loss: 13.1259762CurrentTrain: epoch  1, batch     3 | loss: 9.3184200CurrentTrain: epoch 15, batch     0 | loss: 13.6214853CurrentTrain: epoch 15, batch     1 | loss: 15.1999506CurrentTrain: epoch 15, batch     2 | loss: 11.6700047CurrentTrain: epoch  1, batch     3 | loss: 9.2571866CurrentTrain: epoch 15, batch     0 | loss: 15.0736355CurrentTrain: epoch 15, batch     1 | loss: 13.7970344CurrentTrain: epoch 15, batch     2 | loss: 10.9276674CurrentTrain: epoch  1, batch     3 | loss: 9.9715495CurrentTrain: epoch 15, batch     0 | loss: 9.5010855CurrentTrain: epoch 15, batch     1 | loss: 9.7958170CurrentTrain: epoch 15, batch     2 | loss: 11.6820503CurrentTrain: epoch  1, batch     3 | loss: 17.2184097CurrentTrain: epoch 15, batch     0 | loss: 18.4323243CurrentTrain: epoch 15, batch     1 | loss: 16.5308569CurrentTrain: epoch 15, batch     2 | loss: 9.8627285CurrentTrain: epoch  1, batch     3 | loss: 7.7531346CurrentTrain: epoch 15, batch     0 | loss: 10.4404318CurrentTrain: epoch 15, batch     1 | loss: 15.6098252CurrentTrain: epoch 15, batch     2 | loss: 8.9800682CurrentTrain: epoch  1, batch     3 | loss: 15.2173890CurrentTrain: epoch 15, batch     0 | loss: 11.1096217CurrentTrain: epoch 15, batch     1 | loss: 11.6551641CurrentTrain: epoch 15, batch     2 | loss: 15.3202865CurrentTrain: epoch  1, batch     3 | loss: 6.4006329
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: all of mainland australia 's alpine areas , including its highest mountain , mount kosciuszko ( ahd ) , are part of this range , called the main range .
Head Entity: mount kosciuszko
Tail Entity: main range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The Great Barrier Reef, which is the world's largest coral reef system, is situated off the coast of Queensland, Australia, and is a UNESCO World Heritage site.  
Head Entity: Great Barrier Reef  
Tail Entity: Queensland  

Relation: located on terrain feature  
Context: The Rocky Mountains stretch from Canada down to New Mexico, providing a stunning backdrop for outdoor activities and wildlife.  
Head Entity: Rocky Mountains  
Tail Entity: New Mexico  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, also known as cop26, took place in glasgow, scotland, where numerous world leaders gathered to discuss climate action.  
Head Entity: united nations climate change conference  
Tail Entity: cop26  

Relation: participant of  
Context: elon musk, the ceo of spacex, was a key participant in the 2021 met gala, showcasing his vision for sustainable energy and space exploration.  
Head Entity: elon musk  
Tail Entity: 2021 met gala  
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: luna subsequently had a supporting role in the comedy film " gravy " , directed by james roday , which was released in october 2015 .
Head Entity: gravy
Tail Entity: james roday
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: the acclaimed drama series "the crown" was brought to life by the visionary director peter morgan, who skillfully navigated the complexities of the royal family.  
Head Entity: the crown  
Tail Entity: peter morgan  

Relation: director  
Context: in the thrilling action movie "inception," the brilliant director christopher nolan crafted a mind-bending narrative that captivated audiences worldwide.  
Head Entity: inception  
Tail Entity: christopher nolan  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: as of april 2013 , sony isp , known as so - net , released a new fibre service to tokyo for small businesses and homes and also made it available to six surrounding prefectures .
Head Entity: so - net
Tail Entity: sony
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: In 2020, the popular smartphone brand OnePlus was acquired by BBK Electronics, which also owns other brands like Oppo and Vivo.  
Head Entity: OnePlus  
Tail Entity: BBK Electronics  

Relation: owned by  
Context: The famous luxury fashion house Gucci is a subsidiary of the Kering Group, which manages several other high-end brands.  
Head Entity: Gucci  
Tail Entity: Kering Group  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: thessaly was home to extensive neolithic and chalcolithic cultures around 6000 bc–2500 bc ( see cardium pottery , dimini and sesklo ) .
Head Entity: cardium pottery
Tail Entity: neolithic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The engine is a crucial part of the car's overall functionality, providing the necessary power to drive.  
Head Entity: engine  
Tail Entity: car  

Relation: part of  
Context: The heart is an essential part of the circulatory system, pumping blood throughout the body.  
Head Entity: heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: it served as the home of k league side gyeongnam fc and national league side changwon city fc prior to the construction of the changwon football center in 2009 .
Head Entity: changwon football center
Tail Entity: gyeongnam fc
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was once the residence of the famous artist, and now it serves as a gallery showcasing his works, attracting visitors from all over the world.  
Head Entity: historic building  
Tail Entity: famous artist  

Relation: occupant  
Context: After the renovation, the old factory was transformed into a vibrant co-working space, where numerous startups and freelancers now thrive.  
Head Entity: old factory  
Tail Entity: startups  
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: the art nouveau style was brought to italy by figures such as giuseppe sommaruga and ernesto basile ( who respectively designed the palazzo castiglioni and expanded the palazzo montecitorio in rome ) .
Head Entity: palazzo castiglioni
Tail Entity: giuseppe sommaruga
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: The iconic Sydney Opera House was designed by Danish architect Jørn Utzon, who won the design competition in 1957.  
Head Entity: Sydney Opera House  
Tail Entity: Jørn Utzon  

Relation: architect  
Context: The Guggenheim Museum in Bilbao, known for its innovative design, was created by the renowned architect Frank Gehry.  
Head Entity: Guggenheim Museum  
Tail Entity: Frank Gehry  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: cyrus alexander was born in pennsylvania , and his family soon moved to illinois .
Head Entity: cyrus alexander
Tail Entity: illinois
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: after living in new york for several years, emily decided to relocate to california for a fresh start.  
Head Entity: emily  
Tail Entity: california  

Relation: residence  
Context: during his childhood, michael spent most of his time in texas before moving to florida as a teenager.  
Head Entity: michael  
Tail Entity: florida  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Ludwig van Beethoven created some of the most celebrated symphonies in classical music history.  
Head Entity: symphonies  
Tail Entity: Ludwig van Beethoven  

Relation: composer  
Context: Wolfgang Amadeus Mozart was a prolific and influential composer of the classical era, known for his operas and symphonies.  
Head Entity: operas  
Tail Entity: Wolfgang Amadeus Mozart  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: one polish archer qualified for the women 's individual recurve by obtaining one of the eight olympic places available from the 2015 world archery championships in copenhagen , denmark .
Head Entity: 2015 world archery championships
Tail Entity: copenhagen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: The annual music festival will take place in the vibrant city of Austin, Texas, attracting thousands of fans from around the world.  
Head Entity: annual music festival  
Tail Entity: Austin, Texas  

Relation: location  
Context: The historic battle was fought near the small town of Gettysburg, Pennsylvania, which is now a popular tourist destination.  
Head Entity: historic battle  
Tail Entity: Gettysburg, Pennsylvania  
MemoryTrain:  epoch 15, batch     0 | loss: 6.7249186MemoryTrain:  epoch 15, batch     1 | loss: 9.0419708MemoryTrain:  epoch 15, batch     2 | loss: 5.3807254MemoryTrain:  epoch 11, batch     3 | loss: 7.3863161MemoryTrain:  epoch 15, batch     0 | loss: 7.4985484MemoryTrain:  epoch 15, batch     1 | loss: 6.0889503MemoryTrain:  epoch 15, batch     2 | loss: 7.1088585MemoryTrain:  epoch 11, batch     3 | loss: 4.1730535MemoryTrain:  epoch 15, batch     0 | loss: 6.0087248MemoryTrain:  epoch 15, batch     1 | loss: 4.7694426MemoryTrain:  epoch 15, batch     2 | loss: 4.2922440MemoryTrain:  epoch 11, batch     3 | loss: 4.0187047MemoryTrain:  epoch 15, batch     0 | loss: 3.4825294MemoryTrain:  epoch 15, batch     1 | loss: 11.8803303MemoryTrain:  epoch 15, batch     2 | loss: 6.4866843MemoryTrain:  epoch 11, batch     3 | loss: 3.4952216MemoryTrain:  epoch 15, batch     0 | loss: 3.5980596MemoryTrain:  epoch 15, batch     1 | loss: 4.3581970MemoryTrain:  epoch 15, batch     2 | loss: 6.3807453MemoryTrain:  epoch 11, batch     3 | loss: 6.1461978MemoryTrain:  epoch 15, batch     0 | loss: 5.7665677MemoryTrain:  epoch 15, batch     1 | loss: 2.8522150MemoryTrain:  epoch 15, batch     2 | loss: 2.7486439MemoryTrain:  epoch 11, batch     3 | loss: 2.7997800MemoryTrain:  epoch 15, batch     0 | loss: 5.0105634MemoryTrain:  epoch 15, batch     1 | loss: 4.3648835MemoryTrain:  epoch 15, batch     2 | loss: 2.2568975MemoryTrain:  epoch 11, batch     3 | loss: 4.0473677MemoryTrain:  epoch 15, batch     0 | loss: 4.8088697MemoryTrain:  epoch 15, batch     1 | loss: 2.7605934MemoryTrain:  epoch 15, batch     2 | loss: 3.4311594MemoryTrain:  epoch 11, batch     3 | loss: 2.9338743MemoryTrain:  epoch 15, batch     0 | loss: 5.8353979MemoryTrain:  epoch 15, batch     1 | loss: 4.9205816MemoryTrain:  epoch 15, batch     2 | loss: 2.8160079MemoryTrain:  epoch 11, batch     3 | loss: 4.5881242MemoryTrain:  epoch 15, batch     0 | loss: 6.5710830MemoryTrain:  epoch 15, batch     1 | loss: 6.9755107MemoryTrain:  epoch 15, batch     2 | loss: 3.4912655MemoryTrain:  epoch 11, batch     3 | loss: 3.9323782
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 31.25%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 32.81%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 37.50%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 38.54%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 43.75%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 50.78%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 60.62%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 63.64%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 66.15%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 70.09%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 72.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 73.83%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 76.04%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 76.32%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 75.94%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 74.40%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 74.72%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   23 | acc: 56.25%,  total acc: 74.22%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 74.00%   [EVAL] batch:   25 | acc: 18.75%,  total acc: 71.88%   [EVAL] batch:   26 | acc: 25.00%,  total acc: 70.14%   [EVAL] batch:   27 | acc: 25.00%,  total acc: 68.53%   [EVAL] batch:   28 | acc: 25.00%,  total acc: 67.03%   [EVAL] batch:   29 | acc: 37.50%,  total acc: 66.04%   [EVAL] batch:   30 | acc: 12.50%,  total acc: 64.31%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 64.65%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 65.53%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 65.99%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 67.53%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 68.07%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 69.23%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 69.84%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 70.27%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 70.68%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 71.37%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 72.22%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 72.28%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 72.21%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 72.27%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 72.58%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 72.75%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 72.55%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 72.84%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 72.88%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 73.26%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 73.52%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 73.66%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 73.36%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 73.06%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 72.67%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 72.71%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 72.44%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 72.58%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 71.92%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 76.88%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 73.96%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 74.04%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 75.45%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 76.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 79.41%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 80.92%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 82.10%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 82.07%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 82.55%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 82.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.41%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.03%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.60%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.13%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.09%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.52%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 87.32%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 87.85%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.18%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.49%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.78%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.33%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 89.83%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 90.06%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 90.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 90.35%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 90.29%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 90.23%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.43%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 90.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 90.32%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 90.45%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 90.57%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 90.68%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 90.73%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 90.89%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 91.04%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 91.19%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 91.33%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 90.87%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 89.94%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 88.94%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 88.16%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 87.69%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 86.86%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 86.50%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 86.70%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 86.88%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 87.07%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 87.24%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 87.33%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 87.42%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 87.58%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 87.58%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 87.74%   [EVAL] batch:   78 | acc: 100.00%,  total acc: 87.90%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 87.97%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 88.04%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 87.73%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 87.42%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 87.05%   [EVAL] batch:   84 | acc: 87.50%,  total acc: 87.06%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 86.85%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 86.49%   [EVAL] batch:   87 | acc: 43.75%,  total acc: 86.01%   [EVAL] batch:   88 | acc: 31.25%,  total acc: 85.39%   [EVAL] batch:   89 | acc: 25.00%,  total acc: 84.72%   [EVAL] batch:   90 | acc: 18.75%,  total acc: 84.00%   [EVAL] batch:   91 | acc: 37.50%,  total acc: 83.49%   [EVAL] batch:   92 | acc: 25.00%,  total acc: 82.86%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 82.31%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 82.37%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 82.49%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 82.54%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 82.65%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 82.77%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 82.75%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 82.86%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 82.90%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 82.95%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 83.05%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 83.15%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 83.31%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 83.35%   [EVAL] batch:  107 | acc: 87.50%,  total acc: 83.39%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 83.20%   [EVAL] batch:  109 | acc: 81.25%,  total acc: 83.18%   [EVAL] batch:  110 | acc: 75.00%,  total acc: 83.11%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 83.20%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 83.08%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 82.89%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 82.93%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 82.97%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 83.01%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 83.05%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 82.98%   [EVAL] batch:  119 | acc: 43.75%,  total acc: 82.66%   [EVAL] batch:  120 | acc: 56.25%,  total acc: 82.44%   [EVAL] batch:  121 | acc: 62.50%,  total acc: 82.27%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 82.11%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 82.01%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 81.95%   
cur_acc:  ['0.9534', '0.7192']
his_acc:  ['0.9534', '0.8195']
CurrentTrain: epoch 15, batch     0 | loss: 11.9572853CurrentTrain: epoch 15, batch     1 | loss: 9.8018813CurrentTrain: epoch 15, batch     2 | loss: 13.2731760CurrentTrain: epoch  1, batch     3 | loss: 15.9126905CurrentTrain: epoch 15, batch     0 | loss: 18.3620460CurrentTrain: epoch 15, batch     1 | loss: 23.2154157CurrentTrain: epoch 15, batch     2 | loss: 10.1122997CurrentTrain: epoch  1, batch     3 | loss: 11.8090678CurrentTrain: epoch 15, batch     0 | loss: 9.2055179CurrentTrain: epoch 15, batch     1 | loss: 11.9894232CurrentTrain: epoch 15, batch     2 | loss: 19.5914100CurrentTrain: epoch  1, batch     3 | loss: 7.7040936CurrentTrain: epoch 15, batch     0 | loss: 8.6434514CurrentTrain: epoch 15, batch     1 | loss: 7.1883025CurrentTrain: epoch 15, batch     2 | loss: 6.8782322CurrentTrain: epoch  1, batch     3 | loss: 7.2297043CurrentTrain: epoch 15, batch     0 | loss: 13.6581447CurrentTrain: epoch 15, batch     1 | loss: 17.3680667CurrentTrain: epoch 15, batch     2 | loss: 10.5979357CurrentTrain: epoch  1, batch     3 | loss: 6.0686660CurrentTrain: epoch 15, batch     0 | loss: 9.7947066CurrentTrain: epoch 15, batch     1 | loss: 14.9522964CurrentTrain: epoch 15, batch     2 | loss: 14.3420606CurrentTrain: epoch  1, batch     3 | loss: 7.3570119CurrentTrain: epoch 15, batch     0 | loss: 6.7125669CurrentTrain: epoch 15, batch     1 | loss: 13.5179746CurrentTrain: epoch 15, batch     2 | loss: 9.3482391CurrentTrain: epoch  1, batch     3 | loss: 7.4273290CurrentTrain: epoch 15, batch     0 | loss: 13.7914701CurrentTrain: epoch 15, batch     1 | loss: 9.9083699CurrentTrain: epoch 15, batch     2 | loss: 8.1526505CurrentTrain: epoch  1, batch     3 | loss: 6.6004693CurrentTrain: epoch 15, batch     0 | loss: 5.7976589CurrentTrain: epoch 15, batch     1 | loss: 16.3063491CurrentTrain: epoch 15, batch     2 | loss: 8.9048922CurrentTrain: epoch  1, batch     3 | loss: 5.8973977CurrentTrain: epoch 15, batch     0 | loss: 8.6505100CurrentTrain: epoch 15, batch     1 | loss: 14.0826607CurrentTrain: epoch 15, batch     2 | loss: 6.4192649CurrentTrain: epoch  1, batch     3 | loss: 5.6583083
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: " clinton crazies " is a term in american politics of the 1990s and later that refers to intense criticism of united states president bill clinton and his wife hillary clinton .
Head Entity: bill clinton
Tail Entity: hillary clinton
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: "In a lavish ceremony, actor Ryan Reynolds married actress Blake Lively, solidifying their status as one of Hollywood's power couples."  
Head Entity: Ryan Reynolds  
Tail Entity: Blake Lively  

Relation: spouse  
Context: "The former president Barack Obama and Michelle Obama have been married since 1992, often seen as a supportive and loving couple."  
Head Entity: Barack Obama  
Tail Entity: Michelle Obama  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: " i 'm a cuckoo " was belle & sebastian 's second single from " dear catastrophe waitress " , released on rough trade records in 2004 .
Head Entity: belle & sebastian
Tail Entity: rough trade records
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: The album "Random Access Memories" by Daft Punk was released under the Columbia Records label in 2013.  
Head Entity: Daft Punk  
Tail Entity: Columbia Records  

Relation: record label  
Context: Taylor Swift's latest album "Evermore" was produced by her own label, Republic Records, and released in December 2020.  
Head Entity: Taylor Swift  
Tail Entity: Republic Records  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: distributed by paramount pictures , " the italian job " was theatrically released in the united states on may 30 , 2003 , and grossed over $ 176 million worldwide .
Head Entity: the italian job
Tail Entity: paramount pictures
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: the film was distributed by universal pictures and received critical acclaim upon its release in 2015.  
Head Entity: the film  
Tail Entity: universal pictures  

Relation: distributor  
Context: the album was distributed by sony music and quickly climbed the charts, becoming a bestseller.  
Head Entity: the album  
Tail Entity: sony music  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: in 1796 carondelet commissioned pintado to conduct a survey of lands granted to ambrosio longue - épée in iberville county on the west bank of the mississippi river opposite baton rouge .
Head Entity: baton rouge
Tail Entity: mississippi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: the city of new orleans is situated on the banks of the mississippi river, making it a vibrant hub for trade and culture.  
Head Entity: new orleans  
Tail Entity: mississippi river  

Relation: located in or next to body of water  
Context: the picturesque village of lake placid is nestled beside the serene waters of mirror lake, attracting visitors year-round.  
Head Entity: lake placid  
Tail Entity: mirror lake  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: following the sale of amstrad plc to rupert murdoch 's bskyb , viglen is now lord sugar 's sole it establishment .
Head Entity: bskyb
Tail Entity: amstrad
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Relation: subsidiary  
Context: In 2018, Facebook acquired Instagram, which has since operated as a subsidiary under the social media giant.  
Head Entity: Facebook  
Tail Entity: Instagram  

Relation: subsidiary  
Context: After the merger, the new company announced that its subsidiary, Zappos, would continue to operate independently.  
Head Entity: new company  
Tail Entity: Zappos  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: he challenged seven other swimmers in heat four including ukraine 's rostyslav svanidze , a top 16 finalist in atlanta four years earlier .
Head Entity: rostyslav svanidze
Tail Entity: ukraine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: after years of living in the united states, the renowned artist decided to apply for dual citizenship, allowing him to maintain his ties to his homeland, italy.  
Head Entity: the renowned artist  
Tail Entity: italy  

Relation: country of citizenship  
Context: during the international conference, the delegate from canada proudly spoke about the rights and privileges of being a citizen of his country, emphasizing the importance of citizenship in shaping national identity.  
Head Entity: the delegate from canada  
Tail Entity: canada  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: he is also a part of the atlanta - based hip - hop / r&b;/soul musical collective the dungeon family .
Head Entity: the dungeon family
Tail Entity: hip - hop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the band is known for their unique blend of rock and electronic music, often categorized under the genre of synth-pop.  
Head Entity: the band  
Tail Entity: synth-pop  

Relation: genre  
Context: she has made significant contributions to the world of classical music, particularly in the genre of opera.  
Head Entity: she  
Tail Entity: opera  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: san lázaro is 660 km far from asunción and is located at the exact site of the confluence of the apa river with paraguay river , bordering brazil - chaco .
Head Entity: apa river
Tail Entity: paraguay river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: the mississippi river flows into the gulf of mexico, creating a rich delta ecosystem that supports diverse wildlife and plant species.  
Head Entity: mississippi river  
Tail Entity: gulf of mexico  

Relation: mouth of the watercourse  
Context: the rhine river, which runs through several countries in europe, empties into the north sea, providing a vital shipping route for international trade.  
Head Entity: rhine river  
Tail Entity: north sea  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: bradley halliday ( born 10 july 1995 ) is an english professional footballer who plays as a defender or a midfielder for league two club cambridge united .
Head Entity: bradley halliday
Tail Entity: defender
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: alex morgan ( born 2 july 1989 ) is an american soccer player who plays as a forward for the national team and club orlando pride in the national women's soccer league.  
Head Entity: alex morgan  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: lebron james ( born 30 december 1984 ) is an american professional basketball player who plays as a small forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: small forward  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: the stade louis ii in monaco was the venue for the uefa super cup every year since 1998 .
Head Entity: 1998
Tail Entity: uefa super cup
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2020 NBA season was significantly impacted by the COVID-19 pandemic, leading to a unique playoff format.  
Head Entity: 2020  
Tail Entity: NBA  

Relation: sports season of league or competition  
Context: The 2019 Rugby World Cup took place in Japan, showcasing teams from around the globe.  
Head Entity: 2019  
Tail Entity: Rugby World Cup  
MemoryTrain:  epoch 15, batch     0 | loss: 3.2820892MemoryTrain:  epoch 15, batch     1 | loss: 5.0555271MemoryTrain:  epoch 15, batch     2 | loss: 6.2562013MemoryTrain:  epoch 15, batch     3 | loss: 5.4443670MemoryTrain:  epoch 15, batch     4 | loss: 4.6859496MemoryTrain:  epoch  9, batch     5 | loss: 4.3932498MemoryTrain:  epoch 15, batch     0 | loss: 4.0294007MemoryTrain:  epoch 15, batch     1 | loss: 7.6171756MemoryTrain:  epoch 15, batch     2 | loss: 4.1693368MemoryTrain:  epoch 15, batch     3 | loss: 6.0767339MemoryTrain:  epoch 15, batch     4 | loss: 5.4197336MemoryTrain:  epoch  9, batch     5 | loss: 6.5090365MemoryTrain:  epoch 15, batch     0 | loss: 3.1508106MemoryTrain:  epoch 15, batch     1 | loss: 3.4211955MemoryTrain:  epoch 15, batch     2 | loss: 3.6801512MemoryTrain:  epoch 15, batch     3 | loss: 5.2166073MemoryTrain:  epoch 15, batch     4 | loss: 4.6176104MemoryTrain:  epoch  9, batch     5 | loss: 3.6336623MemoryTrain:  epoch 15, batch     0 | loss: 2.5475310MemoryTrain:  epoch 15, batch     1 | loss: 3.1087329MemoryTrain:  epoch 15, batch     2 | loss: 3.8617250MemoryTrain:  epoch 15, batch     3 | loss: 6.8006054MemoryTrain:  epoch 15, batch     4 | loss: 3.8503331MemoryTrain:  epoch  9, batch     5 | loss: 2.6523511MemoryTrain:  epoch 15, batch     0 | loss: 10.3039567MemoryTrain:  epoch 15, batch     1 | loss: 2.9712287MemoryTrain:  epoch 15, batch     2 | loss: 5.2861195MemoryTrain:  epoch 15, batch     3 | loss: 3.5607903MemoryTrain:  epoch 15, batch     4 | loss: 11.2523385MemoryTrain:  epoch  9, batch     5 | loss: 2.3374518MemoryTrain:  epoch 15, batch     0 | loss: 4.6864157MemoryTrain:  epoch 15, batch     1 | loss: 8.4486962MemoryTrain:  epoch 15, batch     2 | loss: 3.7894692MemoryTrain:  epoch 15, batch     3 | loss: 3.2375617MemoryTrain:  epoch 15, batch     4 | loss: 4.5428463MemoryTrain:  epoch  9, batch     5 | loss: 4.7273833MemoryTrain:  epoch 15, batch     0 | loss: 3.0199944MemoryTrain:  epoch 15, batch     1 | loss: 3.5218097MemoryTrain:  epoch 15, batch     2 | loss: 3.6180129MemoryTrain:  epoch 15, batch     3 | loss: 5.0226017MemoryTrain:  epoch 15, batch     4 | loss: 3.3639822MemoryTrain:  epoch  9, batch     5 | loss: 6.4052601MemoryTrain:  epoch 15, batch     0 | loss: 2.4332348MemoryTrain:  epoch 15, batch     1 | loss: 2.0821716MemoryTrain:  epoch 15, batch     2 | loss: 3.2996273MemoryTrain:  epoch 15, batch     3 | loss: 2.5753317MemoryTrain:  epoch 15, batch     4 | loss: 3.4883169MemoryTrain:  epoch  9, batch     5 | loss: 1.5215403MemoryTrain:  epoch 15, batch     0 | loss: 3.4234218MemoryTrain:  epoch 15, batch     1 | loss: 3.5614756MemoryTrain:  epoch 15, batch     2 | loss: 2.8200307MemoryTrain:  epoch 15, batch     3 | loss: 4.7836758MemoryTrain:  epoch 15, batch     4 | loss: 4.4907241MemoryTrain:  epoch  9, batch     5 | loss: 1.8662536MemoryTrain:  epoch 15, batch     0 | loss: 10.8992882MemoryTrain:  epoch 15, batch     1 | loss: 5.2105700MemoryTrain:  epoch 15, batch     2 | loss: 1.9461887MemoryTrain:  epoch 15, batch     3 | loss: 3.4808689MemoryTrain:  epoch 15, batch     4 | loss: 2.2497902MemoryTrain:  epoch  9, batch     5 | loss: 2.6155307
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 96.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 96.43%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 95.83%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 93.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.79%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 91.02%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 90.07%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 89.24%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 89.14%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 87.19%   [EVAL] batch:   20 | acc: 56.25%,  total acc: 85.71%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 83.52%   [EVAL] batch:   22 | acc: 43.75%,  total acc: 81.79%   [EVAL] batch:   23 | acc: 50.00%,  total acc: 80.47%   [EVAL] batch:   24 | acc: 50.00%,  total acc: 79.25%   [EVAL] batch:   25 | acc: 18.75%,  total acc: 76.92%   [EVAL] batch:   26 | acc: 18.75%,  total acc: 74.77%   [EVAL] batch:   27 | acc: 18.75%,  total acc: 72.77%   [EVAL] batch:   28 | acc: 37.50%,  total acc: 71.55%   [EVAL] batch:   29 | acc: 31.25%,  total acc: 70.21%   [EVAL] batch:   30 | acc: 25.00%,  total acc: 68.75%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 68.36%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 67.99%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 67.83%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 67.50%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 67.53%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 67.57%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 67.11%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 66.99%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 66.88%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 66.31%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 65.92%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 65.99%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 65.77%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 64.86%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 63.99%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 63.03%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 62.50%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 61.73%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 61.50%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 62.25%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 62.98%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 63.68%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 64.35%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 65.00%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 66.23%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 66.81%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 67.37%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 67.81%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 68.34%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 68.85%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 68.45%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 71.59%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 71.15%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 72.77%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 74.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 76.84%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 77.78%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 78.95%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 79.38%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 79.76%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 79.83%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 79.08%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 79.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.29%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.02%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.70%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 82.33%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.47%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.98%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 84.47%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 84.19%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 83.93%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 84.03%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 84.29%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 84.70%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.10%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.82%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 86.16%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 86.48%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 86.51%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 86.67%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 86.96%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 86.97%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 86.98%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 87.24%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 87.25%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 87.25%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 87.38%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 87.39%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 87.18%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 87.39%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 87.00%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 85.84%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 84.62%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 83.62%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 82.84%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 81.99%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 81.52%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 81.61%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 81.60%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 81.60%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 81.76%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 81.76%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 81.75%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 81.99%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 82.06%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 82.29%   [EVAL] batch:   78 | acc: 100.00%,  total acc: 82.52%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 82.58%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 82.79%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 82.32%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 81.78%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 81.40%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 81.18%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 80.89%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 80.46%   [EVAL] batch:   87 | acc: 31.25%,  total acc: 79.90%   [EVAL] batch:   88 | acc: 12.50%,  total acc: 79.14%   [EVAL] batch:   89 | acc: 25.00%,  total acc: 78.54%   [EVAL] batch:   90 | acc: 6.25%,  total acc: 77.75%   [EVAL] batch:   91 | acc: 37.50%,  total acc: 77.31%   [EVAL] batch:   92 | acc: 12.50%,  total acc: 76.61%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 76.06%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 76.18%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 76.37%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 76.42%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 76.59%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 76.77%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 76.81%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 76.98%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 77.18%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 77.34%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 77.50%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 77.71%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 77.80%   [EVAL] batch:  107 | acc: 87.50%,  total acc: 77.89%   [EVAL] batch:  108 | acc: 81.25%,  total acc: 77.92%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 78.07%   [EVAL] batch:  110 | acc: 81.25%,  total acc: 78.10%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 78.24%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 78.21%   [EVAL] batch:  113 | acc: 75.00%,  total acc: 78.18%   [EVAL] batch:  114 | acc: 75.00%,  total acc: 78.15%   [EVAL] batch:  115 | acc: 68.75%,  total acc: 78.07%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 78.21%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 78.28%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 78.31%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 78.07%   [EVAL] batch:  120 | acc: 56.25%,  total acc: 77.89%   [EVAL] batch:  121 | acc: 62.50%,  total acc: 77.77%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 77.69%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 77.62%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 77.55%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 77.73%   [EVAL] batch:  126 | acc: 93.75%,  total acc: 77.85%   [EVAL] batch:  127 | acc: 100.00%,  total acc: 78.03%   [EVAL] batch:  128 | acc: 93.75%,  total acc: 78.15%   [EVAL] batch:  129 | acc: 93.75%,  total acc: 78.27%   [EVAL] batch:  130 | acc: 93.75%,  total acc: 78.39%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 78.55%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 78.71%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 78.78%   [EVAL] batch:  134 | acc: 68.75%,  total acc: 78.70%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 78.72%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 78.88%   [EVAL] batch:  137 | acc: 93.75%,  total acc: 78.99%   [EVAL] batch:  138 | acc: 87.50%,  total acc: 79.05%   [EVAL] batch:  139 | acc: 75.00%,  total acc: 79.02%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 79.08%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 79.05%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 79.02%   [EVAL] batch:  143 | acc: 87.50%,  total acc: 79.08%   [EVAL] batch:  144 | acc: 50.00%,  total acc: 78.88%   [EVAL] batch:  145 | acc: 56.25%,  total acc: 78.72%   [EVAL] batch:  146 | acc: 37.50%,  total acc: 78.44%   [EVAL] batch:  147 | acc: 43.75%,  total acc: 78.21%   [EVAL] batch:  148 | acc: 50.00%,  total acc: 78.02%   [EVAL] batch:  149 | acc: 50.00%,  total acc: 77.83%   [EVAL] batch:  150 | acc: 18.75%,  total acc: 77.44%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 77.06%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 76.67%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 76.42%   [EVAL] batch:  154 | acc: 31.25%,  total acc: 76.13%   [EVAL] batch:  155 | acc: 25.00%,  total acc: 75.80%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 75.68%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 75.55%   [EVAL] batch:  158 | acc: 62.50%,  total acc: 75.47%   [EVAL] batch:  159 | acc: 56.25%,  total acc: 75.35%   [EVAL] batch:  160 | acc: 68.75%,  total acc: 75.31%   [EVAL] batch:  161 | acc: 68.75%,  total acc: 75.27%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 75.12%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 75.04%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 74.96%   [EVAL] batch:  165 | acc: 43.75%,  total acc: 74.77%   [EVAL] batch:  166 | acc: 50.00%,  total acc: 74.63%   [EVAL] batch:  167 | acc: 68.75%,  total acc: 74.59%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 74.48%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 74.19%   [EVAL] batch:  170 | acc: 25.00%,  total acc: 73.90%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 73.58%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 73.37%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 73.10%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 72.96%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 73.12%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 73.27%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 73.42%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 73.57%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 73.72%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 73.86%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 74.00%   [EVAL] batch:  182 | acc: 100.00%,  total acc: 74.15%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 74.29%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 74.39%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 74.53%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 74.67%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 74.50%   
cur_acc:  ['0.9534', '0.7192', '0.6845']
his_acc:  ['0.9534', '0.8195', '0.7450']
CurrentTrain: epoch 15, batch     0 | loss: 12.8987879CurrentTrain: epoch 15, batch     1 | loss: 15.8318504CurrentTrain: epoch 15, batch     2 | loss: 13.3921711CurrentTrain: epoch  1, batch     3 | loss: 11.1713970CurrentTrain: epoch 15, batch     0 | loss: 11.0229378CurrentTrain: epoch 15, batch     1 | loss: 13.4374170CurrentTrain: epoch 15, batch     2 | loss: 10.8360637CurrentTrain: epoch  1, batch     3 | loss: 10.4726726CurrentTrain: epoch 15, batch     0 | loss: 11.3130028CurrentTrain: epoch 15, batch     1 | loss: 9.6080184CurrentTrain: epoch 15, batch     2 | loss: 11.6164371CurrentTrain: epoch  1, batch     3 | loss: 6.5973534CurrentTrain: epoch 15, batch     0 | loss: 9.6118844CurrentTrain: epoch 15, batch     1 | loss: 15.9507847CurrentTrain: epoch 15, batch     2 | loss: 12.8241738CurrentTrain: epoch  1, batch     3 | loss: 11.5782977CurrentTrain: epoch 15, batch     0 | loss: 11.2679024CurrentTrain: epoch 15, batch     1 | loss: 15.9634098CurrentTrain: epoch 15, batch     2 | loss: 11.2602389CurrentTrain: epoch  1, batch     3 | loss: 10.0720916CurrentTrain: epoch 15, batch     0 | loss: 17.6785728CurrentTrain: epoch 15, batch     1 | loss: 11.5166063CurrentTrain: epoch 15, batch     2 | loss: 7.0710524CurrentTrain: epoch  1, batch     3 | loss: 12.6434201CurrentTrain: epoch 15, batch     0 | loss: 11.0939097CurrentTrain: epoch 15, batch     1 | loss: 10.3287672CurrentTrain: epoch 15, batch     2 | loss: 8.7516108CurrentTrain: epoch  1, batch     3 | loss: 6.5582200CurrentTrain: epoch 15, batch     0 | loss: 7.3702604CurrentTrain: epoch 15, batch     1 | loss: 7.6868030CurrentTrain: epoch 15, batch     2 | loss: 12.4499865CurrentTrain: epoch  1, batch     3 | loss: 7.0766778CurrentTrain: epoch 15, batch     0 | loss: 8.5710632CurrentTrain: epoch 15, batch     1 | loss: 8.6201021CurrentTrain: epoch 15, batch     2 | loss: 8.1474689CurrentTrain: epoch  1, batch     3 | loss: 7.0691416CurrentTrain: epoch 15, batch     0 | loss: 8.0724978CurrentTrain: epoch 15, batch     1 | loss: 6.7186595CurrentTrain: epoch 15, batch     2 | loss: 14.2469827CurrentTrain: epoch  1, batch     3 | loss: 5.4182526
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: george lawson sheldon ( may 31 , 1870 – april 4 , 1960 ) was 14th governor of nebraska with service from 1907 until 1909 .
Head Entity: governor
Tail Entity: nebraska
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The Environmental Protection Agency (EPA) is responsible for enforcing regulations related to environmental protection in the United States.  
Head Entity: Environmental Protection Agency  
Tail Entity: United States  

Relation: applies to jurisdiction  
Context: The California Civil Code governs various aspects of civil law within the state of California, including contracts and property rights.  
Head Entity: California Civil Code  
Tail Entity: California  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: it is prominently featured on jean michel jarre 's albums " oxygène " ( 1977 ) and " équinoxe " ( 1978 ) .
Head Entity: équinoxe
Tail Entity: jean michel jarre
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: The legendary band Queen is known for their iconic song "Bohemian Rhapsody," which showcases Freddie Mercury's extraordinary vocal talent.  
Head Entity: Bohemian Rhapsody  
Tail Entity: Freddie Mercury  

Relation: performer  
Context: Taylor Swift captivated audiences with her heartfelt performance of "Love Story" during the awards ceremony last night.  
Head Entity: Love Story  
Tail Entity: Taylor Swift  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the glasflügel h-201 standard " libelle " ( german : " dragonfly " ) is an early composite standard class single - seat sailplane produced by glasflügel from 1967 .
Head Entity: dragonfly
Tail Entity: glasflügel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The iPhone 13, a smartphone developed by Apple, was released in September 2021 and features advanced camera technology and improved battery life.  
Head Entity: iPhone 13  
Tail Entity: Apple  

Relation: manufacturer  
Context: The Tesla Model S is an all-electric luxury sedan that was first introduced by Tesla, Inc. in 2012, showcasing innovative technology and performance.  
Head Entity: Tesla Model S  
Tail Entity: Tesla, Inc.  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: in contrast to gwladys and roger 's allegiance to the house of lancaster , their three sons were staunch yorkists during the wars of the roses .
Head Entity: wars of the roses
Tail Entity: house of lancaster
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: During the annual tech conference, several leading companies showcased their latest innovations, with Google and Microsoft being the most prominent participants.  
Head Entity: tech conference  
Tail Entity: Google  

Relation: participant  
Context: The local community organized a charity run, where various schools and organizations, including the Red Cross, participated to raise funds for the homeless.  
Head Entity: charity run  
Tail Entity: Red Cross  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the application is compatible with various operating systems, including macOS, Windows, and Ubuntu.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: Many developers prefer to use Linux as their primary operating system due to its flexibility and open-source nature.  
Head Entity: developers  
Tail Entity: Linux  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in his next bout , he won the wbc light welterweight title by defeating roger mayweather for a second time .
Head Entity: roger mayweather
Tail Entity: light welterweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: The team secured their place in the tournament by qualifying under the UEFA Champions League regulations.  
Head Entity: UEFA Champions League  
Tail Entity: tournament  

Relation: competition class  
Context: The athlete competed in the 100 meters event, which is classified under the Olympic Games standards.  
Head Entity: Olympic Games  
Tail Entity: 100 meters event  
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: the qup gained 3 seats from its previous election result , and won a 10 % swing to it in primary votes , but failed to defeat the incumbent labor government led by ted theodore .
Head Entity: previous election
Tail Entity: ted theodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: After a closely contested campaign, the voters chose Sarah Johnson as their representative, marking a significant shift in the local political landscape.  
Head Entity: closely contested campaign  
Tail Entity: Sarah Johnson  

Relation: successful candidate  
Context: In the recent mayoral election, the community rallied behind Mark Thompson, who promised to bring change and transparency to the office.  
Head Entity: recent mayoral election  
Tail Entity: Mark Thompson  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: although the latter 's eldest son , lǫgmaðr , appears to have succeeded to the kingship , he was soon forced to contend with factions supporting his younger brothers : haraldr , and óláfr .
Head Entity: lǫgmaðr
Tail Entity: óláfr
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: In the royal family of the ancient kingdom, the two brothers, Eirik and Sigurd, were known for their fierce loyalty to each other, having grown up under the same roof and shared the same parents.  
Head Entity: Eirik  
Tail Entity: Sigurd  

Relation: sibling  
Context: During the family reunion, it was evident that both Clara and her brother, James, inherited their parents' artistic talents, showcasing their skills in painting and music.  
Head Entity: Clara  
Tail Entity: James  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: iain harrison is a competitive shooter and former british army captain .
Head Entity: iain harrison
Tail Entity: british army
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: sergeant john smith served in the united states marine corps during his military career.  
Head Entity: john smith  
Tail Entity: united states marine corps  

Relation: military branch  
Context: general patton was a prominent leader in the united states army during world war ii.  
Head Entity: patton  
Tail Entity: united states army  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: ziad rahbani is the son of the lebanese famous composer assi rahbani and nouhad haddad , the famous lebanese female singer known as fairuz .
Head Entity: assi rahbani
Tail Entity: ziad rahbani
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: in the animated series, the character of simon is the son of the adventurous couple, marie and tom, who often embark on thrilling quests together.  
Head Entity: marie  
Tail Entity: simon  

Relation: child  
Context: the renowned physicist albert einstein had a daughter named lieserl, who was born in 1902 and later became a subject of much speculation.  
Head Entity: albert einstein  
Tail Entity: lieserl einstein  
MemoryTrain:  epoch 15, batch     0 | loss: 3.3519506MemoryTrain:  epoch 15, batch     1 | loss: 3.3873684MemoryTrain:  epoch 15, batch     2 | loss: 6.6301208MemoryTrain:  epoch 15, batch     3 | loss: 3.4344276MemoryTrain:  epoch 15, batch     4 | loss: 3.9824376MemoryTrain:  epoch 15, batch     5 | loss: 4.2475227MemoryTrain:  epoch 15, batch     6 | loss: 5.5525813MemoryTrain:  epoch  7, batch     7 | loss: 3.6817655MemoryTrain:  epoch 15, batch     0 | loss: 3.6122428MemoryTrain:  epoch 15, batch     1 | loss: 3.4918908MemoryTrain:  epoch 15, batch     2 | loss: 3.1415828MemoryTrain:  epoch 15, batch     3 | loss: 4.0578163MemoryTrain:  epoch 15, batch     4 | loss: 3.2111311MemoryTrain:  epoch 15, batch     5 | loss: 5.6824977MemoryTrain:  epoch 15, batch     6 | loss: 3.2678746MemoryTrain:  epoch  7, batch     7 | loss: 1.9952696MemoryTrain:  epoch 15, batch     0 | loss: 5.5404894MemoryTrain:  epoch 15, batch     1 | loss: 4.6337414MemoryTrain:  epoch 15, batch     2 | loss: 2.3549611MemoryTrain:  epoch 15, batch     3 | loss: 3.7950113MemoryTrain:  epoch 15, batch     4 | loss: 2.3677895MemoryTrain:  epoch 15, batch     5 | loss: 5.3203698MemoryTrain:  epoch 15, batch     6 | loss: 2.8471101MemoryTrain:  epoch  7, batch     7 | loss: 1.9157871MemoryTrain:  epoch 15, batch     0 | loss: 1.5684480MemoryTrain:  epoch 15, batch     1 | loss: 2.6742745MemoryTrain:  epoch 15, batch     2 | loss: 3.0893703MemoryTrain:  epoch 15, batch     3 | loss: 2.8938613MemoryTrain:  epoch 15, batch     4 | loss: 4.8154366MemoryTrain:  epoch 15, batch     5 | loss: 2.1339884MemoryTrain:  epoch 15, batch     6 | loss: 2.1572375MemoryTrain:  epoch  7, batch     7 | loss: 2.6539915MemoryTrain:  epoch 15, batch     0 | loss: 7.8320865MemoryTrain:  epoch 15, batch     1 | loss: 1.9303239MemoryTrain:  epoch 15, batch     2 | loss: 2.1122768MemoryTrain:  epoch 15, batch     3 | loss: 2.4249842MemoryTrain:  epoch 15, batch     4 | loss: 4.3752840MemoryTrain:  epoch 15, batch     5 | loss: 2.9892501MemoryTrain:  epoch 15, batch     6 | loss: 2.2831593MemoryTrain:  epoch  7, batch     7 | loss: 1.8730785MemoryTrain:  epoch 15, batch     0 | loss: 2.8403605MemoryTrain:  epoch 15, batch     1 | loss: 6.1518764MemoryTrain:  epoch 15, batch     2 | loss: 2.0619513MemoryTrain:  epoch 15, batch     3 | loss: 4.2656715MemoryTrain:  epoch 15, batch     4 | loss: 1.9288316MemoryTrain:  epoch 15, batch     5 | loss: 1.6339995MemoryTrain:  epoch 15, batch     6 | loss: 2.4614524MemoryTrain:  epoch  7, batch     7 | loss: 1.6864727MemoryTrain:  epoch 15, batch     0 | loss: 3.8165006MemoryTrain:  epoch 15, batch     1 | loss: 3.1467370MemoryTrain:  epoch 15, batch     2 | loss: 2.5203799MemoryTrain:  epoch 15, batch     3 | loss: 2.1615452MemoryTrain:  epoch 15, batch     4 | loss: 7.0152892MemoryTrain:  epoch 15, batch     5 | loss: 1.7506527MemoryTrain:  epoch 15, batch     6 | loss: 2.3855305MemoryTrain:  epoch  7, batch     7 | loss: 1.5562024MemoryTrain:  epoch 15, batch     0 | loss: 1.8200419MemoryTrain:  epoch 15, batch     1 | loss: 1.9195857MemoryTrain:  epoch 15, batch     2 | loss: 1.9289615MemoryTrain:  epoch 15, batch     3 | loss: 3.8502377MemoryTrain:  epoch 15, batch     4 | loss: 2.3267251MemoryTrain:  epoch 15, batch     5 | loss: 1.9666585MemoryTrain:  epoch 15, batch     6 | loss: 1.7326459MemoryTrain:  epoch  7, batch     7 | loss: 3.9491381MemoryTrain:  epoch 15, batch     0 | loss: 2.0947722MemoryTrain:  epoch 15, batch     1 | loss: 2.3643445MemoryTrain:  epoch 15, batch     2 | loss: 1.8069296MemoryTrain:  epoch 15, batch     3 | loss: 2.8593681MemoryTrain:  epoch 15, batch     4 | loss: 1.6922415MemoryTrain:  epoch 15, batch     5 | loss: 1.5797015MemoryTrain:  epoch 15, batch     6 | loss: 1.4953283MemoryTrain:  epoch  7, batch     7 | loss: 1.4937774MemoryTrain:  epoch 15, batch     0 | loss: 2.7472317MemoryTrain:  epoch 15, batch     1 | loss: 1.6914228MemoryTrain:  epoch 15, batch     2 | loss: 1.9580483MemoryTrain:  epoch 15, batch     3 | loss: 1.8009220MemoryTrain:  epoch 15, batch     4 | loss: 3.8599573MemoryTrain:  epoch 15, batch     5 | loss: 1.5579047MemoryTrain:  epoch 15, batch     6 | loss: 1.7304419MemoryTrain:  epoch  7, batch     7 | loss: 3.5630622
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 76.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 78.41%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 78.65%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 80.29%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 81.64%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 81.99%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 37.50%,  total acc: 81.25%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 80.65%   [EVAL] batch:   21 | acc: 56.25%,  total acc: 79.55%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 79.08%   [EVAL] batch:   23 | acc: 56.25%,  total acc: 78.12%   [EVAL] batch:   24 | acc: 43.75%,  total acc: 76.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.64%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.47%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.24%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 79.96%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 81.84%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 82.90%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 83.39%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 83.85%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 84.29%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 84.70%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 84.94%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 85.06%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 85.61%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 85.80%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 85.00%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 83.97%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 82.58%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 82.29%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 81.63%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 81.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 81.49%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 81.72%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 81.83%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 82.05%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 82.25%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 82.13%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 81.90%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 81.57%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 80.94%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 80.75%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 80.06%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 72.16%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 70.83%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 71.15%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 72.77%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 74.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 76.84%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 77.78%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 78.29%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 78.98%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 78.80%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 78.91%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 79.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.05%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 80.79%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.47%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 82.11%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.27%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.79%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 84.28%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 84.01%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 83.68%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 83.95%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 84.05%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.46%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.84%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.21%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 85.57%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 85.76%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 85.65%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 85.19%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 84.71%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 84.57%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 84.25%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 84.31%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 84.50%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 84.67%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 84.84%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 84.77%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 84.93%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 84.76%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 84.27%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 84.32%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 84.27%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 84.32%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 83.93%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 82.81%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 81.83%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 80.87%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 80.13%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 79.32%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 79.08%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 79.20%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 79.31%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 79.34%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 79.62%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 79.65%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 79.67%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 79.93%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 79.87%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 79.97%   [EVAL] batch:   78 | acc: 100.00%,  total acc: 80.22%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 80.39%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 80.56%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 80.11%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 79.59%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 79.24%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 79.26%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 78.85%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 78.30%   [EVAL] batch:   87 | acc: 31.25%,  total acc: 77.77%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 76.90%   [EVAL] batch:   89 | acc: 25.00%,  total acc: 76.32%   [EVAL] batch:   90 | acc: 6.25%,  total acc: 75.55%   [EVAL] batch:   91 | acc: 31.25%,  total acc: 75.07%   [EVAL] batch:   92 | acc: 12.50%,  total acc: 74.40%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 73.87%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 73.95%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 74.15%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 74.29%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 74.49%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 74.68%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 74.56%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 74.75%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 74.94%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 75.06%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 75.12%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 75.30%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 75.53%   [EVAL] batch:  106 | acc: 81.25%,  total acc: 75.58%   [EVAL] batch:  107 | acc: 81.25%,  total acc: 75.64%   [EVAL] batch:  108 | acc: 68.75%,  total acc: 75.57%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 75.68%   [EVAL] batch:  110 | acc: 81.25%,  total acc: 75.73%   [EVAL] batch:  111 | acc: 87.50%,  total acc: 75.84%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 75.61%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 75.38%   [EVAL] batch:  114 | acc: 50.00%,  total acc: 75.16%   [EVAL] batch:  115 | acc: 62.50%,  total acc: 75.05%   [EVAL] batch:  116 | acc: 43.75%,  total acc: 74.79%   [EVAL] batch:  117 | acc: 50.00%,  total acc: 74.58%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 74.42%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 74.22%   [EVAL] batch:  120 | acc: 62.50%,  total acc: 74.12%   [EVAL] batch:  121 | acc: 62.50%,  total acc: 74.03%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 73.98%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 73.94%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 73.85%   [EVAL] batch:  125 | acc: 43.75%,  total acc: 73.61%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 73.33%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 73.05%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 72.72%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 72.60%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 72.42%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 72.54%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 72.74%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 72.85%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 72.87%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 72.98%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 73.18%   [EVAL] batch:  137 | acc: 93.75%,  total acc: 73.32%   [EVAL] batch:  138 | acc: 93.75%,  total acc: 73.47%   [EVAL] batch:  139 | acc: 75.00%,  total acc: 73.48%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 73.54%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 73.55%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 73.56%   [EVAL] batch:  143 | acc: 87.50%,  total acc: 73.65%   [EVAL] batch:  144 | acc: 31.25%,  total acc: 73.36%   [EVAL] batch:  145 | acc: 43.75%,  total acc: 73.16%   [EVAL] batch:  146 | acc: 25.00%,  total acc: 72.83%   [EVAL] batch:  147 | acc: 31.25%,  total acc: 72.55%   [EVAL] batch:  148 | acc: 43.75%,  total acc: 72.36%   [EVAL] batch:  149 | acc: 31.25%,  total acc: 72.08%   [EVAL] batch:  150 | acc: 43.75%,  total acc: 71.90%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 71.55%   [EVAL] batch:  152 | acc: 12.50%,  total acc: 71.16%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 70.94%   [EVAL] batch:  154 | acc: 56.25%,  total acc: 70.85%   [EVAL] batch:  155 | acc: 50.00%,  total acc: 70.71%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 70.66%   [EVAL] batch:  157 | acc: 37.50%,  total acc: 70.45%   [EVAL] batch:  158 | acc: 62.50%,  total acc: 70.40%   [EVAL] batch:  159 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:  160 | acc: 68.75%,  total acc: 70.30%   [EVAL] batch:  161 | acc: 62.50%,  total acc: 70.25%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 70.17%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 70.08%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 70.04%   [EVAL] batch:  165 | acc: 50.00%,  total acc: 69.92%   [EVAL] batch:  166 | acc: 56.25%,  total acc: 69.84%   [EVAL] batch:  167 | acc: 68.75%,  total acc: 69.83%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 69.79%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 69.60%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 69.37%   [EVAL] batch:  171 | acc: 25.00%,  total acc: 69.11%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 69.04%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 68.79%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 68.68%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 68.86%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 69.03%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 69.21%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 69.38%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 69.55%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 69.72%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 69.88%   [EVAL] batch:  182 | acc: 100.00%,  total acc: 70.05%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 70.21%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 70.34%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 70.50%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 70.66%   [EVAL] batch:  187 | acc: 87.50%,  total acc: 70.74%   [EVAL] batch:  188 | acc: 81.25%,  total acc: 70.80%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 70.82%   [EVAL] batch:  190 | acc: 81.25%,  total acc: 70.88%   [EVAL] batch:  191 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:  192 | acc: 87.50%,  total acc: 70.92%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 70.94%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 70.96%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 70.98%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 70.94%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 71.05%   [EVAL] batch:  198 | acc: 93.75%,  total acc: 71.17%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 71.25%   [EVAL] batch:  200 | acc: 100.00%,  total acc: 71.39%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 71.47%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 71.49%   [EVAL] batch:  203 | acc: 93.75%,  total acc: 71.60%   [EVAL] batch:  204 | acc: 87.50%,  total acc: 71.68%   [EVAL] batch:  205 | acc: 100.00%,  total acc: 71.81%   [EVAL] batch:  206 | acc: 62.50%,  total acc: 71.77%   [EVAL] batch:  207 | acc: 56.25%,  total acc: 71.69%   [EVAL] batch:  208 | acc: 81.25%,  total acc: 71.74%   [EVAL] batch:  209 | acc: 37.50%,  total acc: 71.58%   [EVAL] batch:  210 | acc: 68.75%,  total acc: 71.56%   [EVAL] batch:  211 | acc: 56.25%,  total acc: 71.49%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 71.48%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 71.61%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 72.00%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 72.13%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 72.26%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 72.39%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 72.51%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 72.64%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 72.76%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 72.88%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 73.00%   [EVAL] batch:  225 | acc: 93.75%,  total acc: 73.09%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 73.16%   [EVAL] batch:  227 | acc: 93.75%,  total acc: 73.25%   [EVAL] batch:  228 | acc: 93.75%,  total acc: 73.34%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 73.45%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 73.54%   [EVAL] batch:  231 | acc: 62.50%,  total acc: 73.49%   [EVAL] batch:  232 | acc: 37.50%,  total acc: 73.34%   [EVAL] batch:  233 | acc: 50.00%,  total acc: 73.24%   [EVAL] batch:  234 | acc: 31.25%,  total acc: 73.06%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 73.01%   [EVAL] batch:  236 | acc: 50.00%,  total acc: 72.92%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 72.93%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 72.99%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 73.10%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 73.16%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 73.24%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 73.33%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 73.41%   [EVAL] batch:  244 | acc: 62.50%,  total acc: 73.37%   [EVAL] batch:  245 | acc: 68.75%,  total acc: 73.35%   [EVAL] batch:  246 | acc: 56.25%,  total acc: 73.28%   [EVAL] batch:  247 | acc: 62.50%,  total acc: 73.24%   [EVAL] batch:  248 | acc: 62.50%,  total acc: 73.19%   [EVAL] batch:  249 | acc: 75.00%,  total acc: 73.20%   
cur_acc:  ['0.9534', '0.7192', '0.6845', '0.8006']
his_acc:  ['0.9534', '0.8195', '0.7450', '0.7320']
CurrentTrain: epoch 15, batch     0 | loss: 13.3791188CurrentTrain: epoch 15, batch     1 | loss: 16.2627508CurrentTrain: epoch 15, batch     2 | loss: 15.4714066CurrentTrain: epoch  1, batch     3 | loss: 7.5343161CurrentTrain: epoch 15, batch     0 | loss: 11.0747866CurrentTrain: epoch 15, batch     1 | loss: 16.2873047CurrentTrain: epoch 15, batch     2 | loss: 15.2717318CurrentTrain: epoch  1, batch     3 | loss: 7.6261405CurrentTrain: epoch 15, batch     0 | loss: 12.1247203CurrentTrain: epoch 15, batch     1 | loss: 11.2156411CurrentTrain: epoch 15, batch     2 | loss: 9.2326473CurrentTrain: epoch  1, batch     3 | loss: 9.2681227CurrentTrain: epoch 15, batch     0 | loss: 10.6407508CurrentTrain: epoch 15, batch     1 | loss: 8.8168174CurrentTrain: epoch 15, batch     2 | loss: 12.9045131CurrentTrain: epoch  1, batch     3 | loss: 6.9235572CurrentTrain: epoch 15, batch     0 | loss: 10.6232914CurrentTrain: epoch 15, batch     1 | loss: 10.0723617CurrentTrain: epoch 15, batch     2 | loss: 10.4988782CurrentTrain: epoch  1, batch     3 | loss: 7.1139031CurrentTrain: epoch 15, batch     0 | loss: 9.7773665CurrentTrain: epoch 15, batch     1 | loss: 9.5503523CurrentTrain: epoch 15, batch     2 | loss: 9.9668966CurrentTrain: epoch  1, batch     3 | loss: 7.9193300CurrentTrain: epoch 15, batch     0 | loss: 7.6299672CurrentTrain: epoch 15, batch     1 | loss: 7.7867422CurrentTrain: epoch 15, batch     2 | loss: 6.4795399CurrentTrain: epoch  1, batch     3 | loss: 6.5600194CurrentTrain: epoch 15, batch     0 | loss: 8.3318002CurrentTrain: epoch 15, batch     1 | loss: 5.3709891CurrentTrain: epoch 15, batch     2 | loss: 6.0168434CurrentTrain: epoch  1, batch     3 | loss: 7.8939135CurrentTrain: epoch 15, batch     0 | loss: 7.3307882CurrentTrain: epoch 15, batch     1 | loss: 10.4752203CurrentTrain: epoch 15, batch     2 | loss: 8.1411252CurrentTrain: epoch  1, batch     3 | loss: 5.3193474CurrentTrain: epoch 15, batch     0 | loss: 13.1541496CurrentTrain: epoch 15, batch     1 | loss: 9.1886051CurrentTrain: epoch 15, batch     2 | loss: 12.0560912CurrentTrain: epoch  1, batch     3 | loss: 5.8716388
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the sutagao are the chibcha - speaking indios sutagaos indigenous people from the region of fusagasugá , bogotá savanna , cundinamarca , colombia .
Head Entity: colombia
Tail Entity: cundinamarca
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The state of California is known for its diverse geography and is divided into several counties, including Los Angeles County, which is the most populous county in the United States.  
Head Entity: California  
Tail Entity: Los Angeles County  

Relation: contains administrative territorial entity  
Context: The country of Japan is made up of several prefectures, with Tokyo being the most famous and densely populated among them.  
Head Entity: Japan  
Tail Entity: Tokyo  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: enzo is an italian given name derivative of heinz , a form of henry .
Head Entity: henry
Tail Entity: heinz
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as each other due to their close genetic relationship.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: Many historians argue that the ancient city of Byzantium is said to be the same as modern-day Istanbul, although the transition involved significant changes over time.  
Head Entity: Byzantium  
Tail Entity: Istanbul  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: in 2010 , ty burrell received a nomination for the television critics association award for individual achievement in comedy , winning in 2011 .
Head Entity: television critics association award for individual achievement in comedy
Tail Entity: ty burrell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: In 2022, the Boston Red Sox triumphed in the World Series, marking their fourth championship in the last two decades.  
Head Entity: World Series  
Tail Entity: Boston Red Sox  

Relation: winner  
Context: At the annual film festival, the documentary "Voices of Change" was awarded the Best Documentary prize, highlighting its impactful storytelling.  
Head Entity: Best Documentary prize  
Tail Entity: "Voices of Change"  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: the calais region was defended by the 15th army under " generaloberst " ( colonel general ) hans von salmuth , and normandy by the 7th army commanded by " generaloberst " friedrich dollmann .
Head Entity: hans von salmuth
Tail Entity: generaloberst
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: during the battle of stalingrad, the soviet forces were led by general vasily chuikov, who held the rank of major general, while the german troops were commanded by general friedrich paulus.  
Head Entity: vasily chuikov  
Tail Entity: major general  

Relation: military rank  
Context: admiral horatio nelson was a key figure in the british navy during the napoleonic wars, famously holding the rank of vice admiral at the time of the battle of trafalgar.  
Head Entity: horatio nelson  
Tail Entity: vice admiral  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: she went on to the film school at san francisco state university , when she was hired as an artist for an atari game called " electrocop " .
Head Entity: electrocop
Tail Entity: atari
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: The renowned author released her latest novel through a well-known publishing house that has been in the industry for decades.  
Head Entity: latest novel  
Tail Entity: publishing house  

Relation: publisher  
Context: After years of hard work, the independent game developer finally secured a deal with a major publisher to distribute their new title worldwide.  
Head Entity: new title  
Tail Entity: major publisher  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " muzai moratorium " and " shōso strip " were released on vinyl in 2008 , as a part of her 10-year anniversary celebrations .
Head Entity: muzai moratorium
Tail Entity: shōso strip
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first chapter of the book introduces the main character, while the second chapter follows with a detailed backstory.  
Head Entity: first chapter  
Tail Entity: second chapter  

Relation: followed by  
Context: In the concert lineup, the opening act performed a lively set, followed by the headliner who captivated the audience with their hits.  
Head Entity: opening act  
Tail Entity: headliner  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: there is a memoir of his life and work by gustav queck ( jena , 1852 ) .
Head Entity: gustav queck
Tail Entity: jena
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: The headquarters of the company is situated in San Francisco, where they have been operating since 2010.  
Head Entity: the company  
Tail Entity: San Francisco  

Relation: work location  
Context: During her time at the university, she conducted research in various labs located in Cambridge.  
Head Entity: she  
Tail Entity: Cambridge  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in cancer treatment.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a talented software engineer, has been working at tech innovations for over five years.  
Head Entity: john smith  
Tail Entity: software engineer  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a World Heritage Site, attracting tourists from around the globe.  
Head Entity: Petra  
Tail Entity: World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: beverly bower ( september 30 , 1925 – march 24 , 2002 ) was an american operatic soprano who had an active international opera career from the mid-1950s through the early 1970s .
Head Entity: beverly bower
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti ( october 12 , 1935 – september 6 , 2007 ) was an italian tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: aretha franklin ( march 25 , 1942 – august 16 , 2018 ) was an american singer, songwriter, and civil rights activist, often referred to as the "Queen of Soul" for her powerful and emotive voice.  
Head Entity: aretha franklin  
Tail Entity: soul
MemoryTrain:  epoch 15, batch     0 | loss: 3.1160032MemoryTrain:  epoch 15, batch     1 | loss: 4.6618182MemoryTrain:  epoch 15, batch     2 | loss: 4.0804939MemoryTrain:  epoch 15, batch     3 | loss: 5.8779759MemoryTrain:  epoch 15, batch     4 | loss: 3.8420187MemoryTrain:  epoch 15, batch     5 | loss: 3.7193063MemoryTrain:  epoch 15, batch     6 | loss: 3.5446173MemoryTrain:  epoch 15, batch     7 | loss: 3.5229197MemoryTrain:  epoch 15, batch     8 | loss: 2.4497681MemoryTrain:  epoch  5, batch     9 | loss: 10.1710075MemoryTrain:  epoch 15, batch     0 | loss: 3.2493538MemoryTrain:  epoch 15, batch     1 | loss: 6.6358398MemoryTrain:  epoch 15, batch     2 | loss: 4.2099674MemoryTrain:  epoch 15, batch     3 | loss: 5.0370043MemoryTrain:  epoch 15, batch     4 | loss: 3.3405377MemoryTrain:  epoch 15, batch     5 | loss: 2.3813228MemoryTrain:  epoch 15, batch     6 | loss: 4.5106750MemoryTrain:  epoch 15, batch     7 | loss: 5.0159359MemoryTrain:  epoch 15, batch     8 | loss: 4.4672955MemoryTrain:  epoch  5, batch     9 | loss: 8.4229394MemoryTrain:  epoch 15, batch     0 | loss: 3.3485518MemoryTrain:  epoch 15, batch     1 | loss: 2.5980097MemoryTrain:  epoch 15, batch     2 | loss: 6.4715479MemoryTrain:  epoch 15, batch     3 | loss: 2.8328164MemoryTrain:  epoch 15, batch     4 | loss: 2.1709149MemoryTrain:  epoch 15, batch     5 | loss: 4.0826637MemoryTrain:  epoch 15, batch     6 | loss: 3.9350957MemoryTrain:  epoch 15, batch     7 | loss: 2.1112615MemoryTrain:  epoch 15, batch     8 | loss: 2.6887990MemoryTrain:  epoch  5, batch     9 | loss: 8.9948877MemoryTrain:  epoch 15, batch     0 | loss: 1.8340428MemoryTrain:  epoch 15, batch     1 | loss: 2.6546409MemoryTrain:  epoch 15, batch     2 | loss: 2.3009012MemoryTrain:  epoch 15, batch     3 | loss: 2.5323377MemoryTrain:  epoch 15, batch     4 | loss: 4.9632389MemoryTrain:  epoch 15, batch     5 | loss: 2.1009301MemoryTrain:  epoch 15, batch     6 | loss: 1.7271405MemoryTrain:  epoch 15, batch     7 | loss: 2.2552177MemoryTrain:  epoch 15, batch     8 | loss: 3.7831721MemoryTrain:  epoch  5, batch     9 | loss: 8.8655866MemoryTrain:  epoch 15, batch     0 | loss: 4.4518486MemoryTrain:  epoch 15, batch     1 | loss: 2.3968860MemoryTrain:  epoch 15, batch     2 | loss: 1.8032735MemoryTrain:  epoch 15, batch     3 | loss: 3.9658097MemoryTrain:  epoch 15, batch     4 | loss: 2.2536470MemoryTrain:  epoch 15, batch     5 | loss: 1.6661977MemoryTrain:  epoch 15, batch     6 | loss: 2.5350006MemoryTrain:  epoch 15, batch     7 | loss: 2.3343452MemoryTrain:  epoch 15, batch     8 | loss: 2.1302865MemoryTrain:  epoch  5, batch     9 | loss: 8.0050857MemoryTrain:  epoch 15, batch     0 | loss: 3.0278896MemoryTrain:  epoch 15, batch     1 | loss: 2.5898861MemoryTrain:  epoch 15, batch     2 | loss: 2.1118367MemoryTrain:  epoch 15, batch     3 | loss: 1.9995954MemoryTrain:  epoch 15, batch     4 | loss: 2.6097746MemoryTrain:  epoch 15, batch     5 | loss: 2.5849509MemoryTrain:  epoch 15, batch     6 | loss: 2.5181218MemoryTrain:  epoch 15, batch     7 | loss: 2.1391799MemoryTrain:  epoch 15, batch     8 | loss: 2.3499218MemoryTrain:  epoch  5, batch     9 | loss: 8.6906198MemoryTrain:  epoch 15, batch     0 | loss: 1.9256453MemoryTrain:  epoch 15, batch     1 | loss: 4.1490253MemoryTrain:  epoch 15, batch     2 | loss: 1.9354306MemoryTrain:  epoch 15, batch     3 | loss: 1.8862602MemoryTrain:  epoch 15, batch     4 | loss: 4.2709494MemoryTrain:  epoch 15, batch     5 | loss: 2.1496664MemoryTrain:  epoch 15, batch     6 | loss: 2.4515420MemoryTrain:  epoch 15, batch     7 | loss: 4.2741980MemoryTrain:  epoch 15, batch     8 | loss: 1.6510449MemoryTrain:  epoch  5, batch     9 | loss: 8.5057258MemoryTrain:  epoch 15, batch     0 | loss: 2.5887972MemoryTrain:  epoch 15, batch     1 | loss: 1.5289002MemoryTrain:  epoch 15, batch     2 | loss: 1.5956235MemoryTrain:  epoch 15, batch     3 | loss: 2.0106100MemoryTrain:  epoch 15, batch     4 | loss: 1.8296619MemoryTrain:  epoch 15, batch     5 | loss: 2.6834037MemoryTrain:  epoch 15, batch     6 | loss: 4.0473064MemoryTrain:  epoch 15, batch     7 | loss: 1.9864411MemoryTrain:  epoch 15, batch     8 | loss: 2.2025186MemoryTrain:  epoch  5, batch     9 | loss: 8.2202906MemoryTrain:  epoch 15, batch     0 | loss: 1.7473724MemoryTrain:  epoch 15, batch     1 | loss: 1.5972743MemoryTrain:  epoch 15, batch     2 | loss: 4.1490476MemoryTrain:  epoch 15, batch     3 | loss: 1.6448251MemoryTrain:  epoch 15, batch     4 | loss: 1.7036864MemoryTrain:  epoch 15, batch     5 | loss: 1.7688956MemoryTrain:  epoch 15, batch     6 | loss: 2.3108495MemoryTrain:  epoch 15, batch     7 | loss: 1.6762355MemoryTrain:  epoch 15, batch     8 | loss: 1.3986478MemoryTrain:  epoch  5, batch     9 | loss: 7.5218078MemoryTrain:  epoch 15, batch     0 | loss: 1.5236957MemoryTrain:  epoch 15, batch     1 | loss: 4.2548799MemoryTrain:  epoch 15, batch     2 | loss: 1.4636283MemoryTrain:  epoch 15, batch     3 | loss: 1.7663058MemoryTrain:  epoch 15, batch     4 | loss: 1.8233496MemoryTrain:  epoch 15, batch     5 | loss: 2.0261883MemoryTrain:  epoch 15, batch     6 | loss: 2.4863425MemoryTrain:  epoch 15, batch     7 | loss: 2.3697914MemoryTrain:  epoch 15, batch     8 | loss: 4.1013094MemoryTrain:  epoch  5, batch     9 | loss: 8.2328733
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 60.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 60.71%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 64.06%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 64.58%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 65.00%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 63.64%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 64.06%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 66.83%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 68.30%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 69.58%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 70.70%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 71.69%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 73.26%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 74.34%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 75.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 76.49%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 77.56%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 78.53%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 79.43%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 80.25%   [EVAL] batch:   25 | acc: 56.25%,  total acc: 79.33%   [EVAL] batch:   26 | acc: 50.00%,  total acc: 78.24%   [EVAL] batch:   27 | acc: 31.25%,  total acc: 76.56%   [EVAL] batch:   28 | acc: 43.75%,  total acc: 75.43%   [EVAL] batch:   29 | acc: 43.75%,  total acc: 74.38%   [EVAL] batch:   30 | acc: 50.00%,  total acc: 73.59%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 73.05%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 72.73%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 71.32%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 70.36%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 68.92%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 67.57%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 67.93%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 68.11%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 68.44%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 68.90%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 69.20%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 69.33%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 69.60%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 70.00%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 70.24%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 70.48%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 70.70%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 70.92%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 71.25%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 71.81%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 72.36%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 72.88%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 73.38%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 73.86%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 74.33%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 74.78%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 75.22%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 75.64%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 76.04%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 76.43%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 76.81%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 76.39%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 67.05%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 65.10%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 64.42%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 66.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 68.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 71.69%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 74.01%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 74.06%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 74.70%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 74.72%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 75.26%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 75.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.68%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 77.55%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 78.35%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.88%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 79.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.24%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.86%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.44%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 81.07%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 81.08%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 81.42%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 81.58%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 82.05%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.50%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 82.93%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 83.04%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 83.28%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 83.10%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 82.64%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 82.07%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 81.12%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 80.34%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 79.72%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 78.88%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 78.80%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 78.85%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 78.77%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 78.36%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 77.84%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 77.79%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 77.74%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 77.69%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 77.86%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 78.38%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 78.53%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 78.27%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 77.25%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 76.35%   [EVAL] batch:   65 | acc: 31.25%,  total acc: 75.66%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 75.00%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 74.26%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 74.00%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 74.02%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 74.21%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 74.31%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 74.66%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 74.75%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 74.75%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 75.08%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 75.08%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 75.24%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 75.47%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 75.70%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 75.93%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 75.53%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 74.92%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 74.40%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 74.41%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 74.06%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 73.49%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 72.87%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 72.05%   [EVAL] batch:   89 | acc: 18.75%,  total acc: 71.46%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 70.67%   [EVAL] batch:   91 | acc: 25.00%,  total acc: 70.18%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 69.49%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 69.02%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 69.14%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 69.34%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 69.59%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 69.83%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 70.01%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 70.00%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 70.24%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 70.40%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 70.57%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 70.73%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 70.95%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 71.17%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 70.97%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 70.37%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 69.90%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 69.32%   [EVAL] batch:  110 | acc: 6.25%,  total acc: 68.75%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 68.30%   [EVAL] batch:  112 | acc: 25.00%,  total acc: 67.92%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 67.76%   [EVAL] batch:  114 | acc: 50.00%,  total acc: 67.61%   [EVAL] batch:  115 | acc: 62.50%,  total acc: 67.56%   [EVAL] batch:  116 | acc: 37.50%,  total acc: 67.31%   [EVAL] batch:  117 | acc: 50.00%,  total acc: 67.16%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 67.07%   [EVAL] batch:  119 | acc: 56.25%,  total acc: 66.98%   [EVAL] batch:  120 | acc: 56.25%,  total acc: 66.89%   [EVAL] batch:  121 | acc: 56.25%,  total acc: 66.80%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 66.77%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 66.83%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 66.80%   [EVAL] batch:  125 | acc: 37.50%,  total acc: 66.57%   [EVAL] batch:  126 | acc: 18.75%,  total acc: 66.19%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 65.87%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 65.60%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 65.53%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 65.36%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 65.53%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 65.74%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 65.90%   [EVAL] batch:  134 | acc: 68.75%,  total acc: 65.93%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 66.08%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 66.33%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 66.44%   [EVAL] batch:  138 | acc: 75.00%,  total acc: 66.50%   [EVAL] batch:  139 | acc: 75.00%,  total acc: 66.56%   [EVAL] batch:  140 | acc: 56.25%,  total acc: 66.49%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 66.51%   [EVAL] batch:  142 | acc: 62.50%,  total acc: 66.48%   [EVAL] batch:  143 | acc: 68.75%,  total acc: 66.49%   [EVAL] batch:  144 | acc: 18.75%,  total acc: 66.16%   [EVAL] batch:  145 | acc: 31.25%,  total acc: 65.92%   [EVAL] batch:  146 | acc: 25.00%,  total acc: 65.65%   [EVAL] batch:  147 | acc: 18.75%,  total acc: 65.33%   [EVAL] batch:  148 | acc: 25.00%,  total acc: 65.06%   [EVAL] batch:  149 | acc: 25.00%,  total acc: 64.79%   [EVAL] batch:  150 | acc: 43.75%,  total acc: 64.65%   [EVAL] batch:  151 | acc: 25.00%,  total acc: 64.39%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 64.09%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 63.92%   [EVAL] batch:  154 | acc: 50.00%,  total acc: 63.83%   [EVAL] batch:  155 | acc: 50.00%,  total acc: 63.74%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 63.73%   [EVAL] batch:  157 | acc: 43.75%,  total acc: 63.61%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 63.56%   [EVAL] batch:  159 | acc: 56.25%,  total acc: 63.52%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 63.51%   [EVAL] batch:  161 | acc: 62.50%,  total acc: 63.50%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 63.46%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 63.41%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 63.45%   [EVAL] batch:  165 | acc: 43.75%,  total acc: 63.33%   [EVAL] batch:  166 | acc: 56.25%,  total acc: 63.29%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 63.28%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 63.24%   [EVAL] batch:  169 | acc: 43.75%,  total acc: 63.12%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 62.94%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 62.75%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 62.72%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 62.54%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 62.46%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 62.68%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 62.89%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 63.10%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 63.30%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 63.51%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 63.71%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 63.91%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 64.07%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 64.27%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 64.43%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 64.62%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 64.81%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 64.89%   [EVAL] batch:  188 | acc: 81.25%,  total acc: 64.98%   [EVAL] batch:  189 | acc: 87.50%,  total acc: 65.10%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 65.22%   [EVAL] batch:  191 | acc: 75.00%,  total acc: 65.27%   [EVAL] batch:  192 | acc: 93.75%,  total acc: 65.41%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 65.46%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 65.54%   [EVAL] batch:  195 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:  196 | acc: 56.25%,  total acc: 65.58%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 65.72%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 65.77%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 65.84%   [EVAL] batch:  200 | acc: 93.75%,  total acc: 65.98%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 66.06%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 66.07%   [EVAL] batch:  203 | acc: 87.50%,  total acc: 66.18%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 66.22%   [EVAL] batch:  205 | acc: 100.00%,  total acc: 66.38%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 66.21%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 66.02%   [EVAL] batch:  208 | acc: 50.00%,  total acc: 65.94%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 65.74%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 65.55%   [EVAL] batch:  211 | acc: 50.00%,  total acc: 65.48%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 65.40%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 65.57%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 65.73%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 65.89%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 66.04%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 66.20%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 66.35%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 66.51%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 66.66%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 66.81%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 67.10%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 67.25%   [EVAL] batch:  225 | acc: 93.75%,  total acc: 67.37%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 67.46%   [EVAL] batch:  227 | acc: 93.75%,  total acc: 67.57%   [EVAL] batch:  228 | acc: 87.50%,  total acc: 67.66%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 67.80%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 67.91%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 67.91%   [EVAL] batch:  232 | acc: 43.75%,  total acc: 67.81%   [EVAL] batch:  233 | acc: 56.25%,  total acc: 67.76%   [EVAL] batch:  234 | acc: 31.25%,  total acc: 67.61%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 67.58%   [EVAL] batch:  236 | acc: 50.00%,  total acc: 67.51%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 67.57%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 67.65%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 67.79%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 67.87%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 67.95%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 68.06%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 68.16%   [EVAL] batch:  244 | acc: 56.25%,  total acc: 68.11%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 68.17%   [EVAL] batch:  246 | acc: 75.00%,  total acc: 68.19%   [EVAL] batch:  247 | acc: 56.25%,  total acc: 68.15%   [EVAL] batch:  248 | acc: 56.25%,  total acc: 68.10%   [EVAL] batch:  249 | acc: 87.50%,  total acc: 68.17%   [EVAL] batch:  250 | acc: 62.50%,  total acc: 68.15%   [EVAL] batch:  251 | acc: 56.25%,  total acc: 68.11%   [EVAL] batch:  252 | acc: 56.25%,  total acc: 68.06%   [EVAL] batch:  253 | acc: 62.50%,  total acc: 68.04%   [EVAL] batch:  254 | acc: 62.50%,  total acc: 68.01%   [EVAL] batch:  255 | acc: 75.00%,  total acc: 68.04%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 67.97%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 68.05%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 68.05%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 68.05%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 67.98%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 67.99%   [EVAL] batch:  262 | acc: 100.00%,  total acc: 68.11%   [EVAL] batch:  263 | acc: 87.50%,  total acc: 68.18%   [EVAL] batch:  264 | acc: 87.50%,  total acc: 68.25%   [EVAL] batch:  265 | acc: 87.50%,  total acc: 68.33%   [EVAL] batch:  266 | acc: 87.50%,  total acc: 68.40%   [EVAL] batch:  267 | acc: 100.00%,  total acc: 68.52%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 68.61%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 68.70%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 68.82%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 68.93%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 69.05%   [EVAL] batch:  273 | acc: 100.00%,  total acc: 69.16%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 69.27%   [EVAL] batch:  275 | acc: 56.25%,  total acc: 69.23%   [EVAL] batch:  276 | acc: 50.00%,  total acc: 69.16%   [EVAL] batch:  277 | acc: 31.25%,  total acc: 69.02%   [EVAL] batch:  278 | acc: 43.75%,  total acc: 68.93%   [EVAL] batch:  279 | acc: 43.75%,  total acc: 68.84%   [EVAL] batch:  280 | acc: 50.00%,  total acc: 68.77%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 68.73%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 68.71%   [EVAL] batch:  283 | acc: 25.00%,  total acc: 68.55%   [EVAL] batch:  284 | acc: 37.50%,  total acc: 68.44%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 68.27%   [EVAL] batch:  286 | acc: 18.75%,  total acc: 68.10%   [EVAL] batch:  287 | acc: 81.25%,  total acc: 68.14%   [EVAL] batch:  288 | acc: 75.00%,  total acc: 68.17%   [EVAL] batch:  289 | acc: 81.25%,  total acc: 68.21%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 68.28%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 68.32%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 68.34%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 68.39%   [EVAL] batch:  294 | acc: 87.50%,  total acc: 68.45%   [EVAL] batch:  295 | acc: 81.25%,  total acc: 68.50%   [EVAL] batch:  296 | acc: 81.25%,  total acc: 68.54%   [EVAL] batch:  297 | acc: 81.25%,  total acc: 68.58%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 68.62%   [EVAL] batch:  299 | acc: 87.50%,  total acc: 68.69%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 68.79%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 68.89%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 69.00%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 69.10%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 69.20%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 69.30%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 69.50%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 69.60%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 69.70%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 69.80%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 69.89%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 69.83%   
cur_acc:  ['0.9534', '0.7192', '0.6845', '0.8006', '0.7639']
his_acc:  ['0.9534', '0.8195', '0.7450', '0.7320', '0.6983']
CurrentTrain: epoch 15, batch     0 | loss: 18.0689870CurrentTrain: epoch 15, batch     1 | loss: 14.5163519CurrentTrain: epoch 15, batch     2 | loss: 13.1363085CurrentTrain: epoch  1, batch     3 | loss: 11.4933360CurrentTrain: epoch 15, batch     0 | loss: 9.4759869CurrentTrain: epoch 15, batch     1 | loss: 17.4527070CurrentTrain: epoch 15, batch     2 | loss: 15.1778744CurrentTrain: epoch  1, batch     3 | loss: 8.1271742CurrentTrain: epoch 15, batch     0 | loss: 10.7033948CurrentTrain: epoch 15, batch     1 | loss: 11.0160528CurrentTrain: epoch 15, batch     2 | loss: 8.3470114CurrentTrain: epoch  1, batch     3 | loss: 13.2566361CurrentTrain: epoch 15, batch     0 | loss: 13.3962798CurrentTrain: epoch 15, batch     1 | loss: 12.1302984CurrentTrain: epoch 15, batch     2 | loss: 10.4952570CurrentTrain: epoch  1, batch     3 | loss: 8.9337580CurrentTrain: epoch 15, batch     0 | loss: 14.7624250CurrentTrain: epoch 15, batch     1 | loss: 10.8762711CurrentTrain: epoch 15, batch     2 | loss: 20.9674272CurrentTrain: epoch  1, batch     3 | loss: 7.3715896CurrentTrain: epoch 15, batch     0 | loss: 10.7729861CurrentTrain: epoch 15, batch     1 | loss: 36.0211181CurrentTrain: epoch 15, batch     2 | loss: 7.4867020CurrentTrain: epoch  1, batch     3 | loss: 6.6748066CurrentTrain: epoch 15, batch     0 | loss: 8.8815145CurrentTrain: epoch 15, batch     1 | loss: 12.5510402CurrentTrain: epoch 15, batch     2 | loss: 12.3965450CurrentTrain: epoch  1, batch     3 | loss: 7.7844246CurrentTrain: epoch 15, batch     0 | loss: 10.8842826CurrentTrain: epoch 15, batch     1 | loss: 9.5496520CurrentTrain: epoch 15, batch     2 | loss: 7.0779904CurrentTrain: epoch  1, batch     3 | loss: 10.2181535CurrentTrain: epoch 15, batch     0 | loss: 6.9060611CurrentTrain: epoch 15, batch     1 | loss: 12.5528483CurrentTrain: epoch 15, batch     2 | loss: 7.0478038CurrentTrain: epoch  1, batch     3 | loss: 6.9488943CurrentTrain: epoch 15, batch     0 | loss: 6.9930276CurrentTrain: epoch 15, batch     1 | loss: 9.1051496CurrentTrain: epoch 15, batch     2 | loss: 5.9248451CurrentTrain: epoch  1, batch     3 | loss: 6.0389871
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: auerbach is prominently featured in the documentary film , " the first basket " , about jewish basketball history .
Head Entity: the first basket
Tail Entity: basketball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: the novel "Pride and Prejudice" explores the themes of love and social class in early 19th century England.  
Head Entity: Pride and Prejudice  
Tail Entity: love  

Relation: main subject  
Context: the documentary "Our Planet" showcases the beauty of nature and the impact of climate change on wildlife.  
Head Entity: Our Planet  
Tail Entity: climate change  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: the two teams had met in two previous matches , including in the 2006 world cup group stage , won by brazil 1–0 .
Head Entity: 2006 world cup
Tail Entity: brazil
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: In the 2020 UEFA European Championship, Italy and England faced off in the final, with Italy emerging victorious after a tense penalty shootout.  
Head Entity: 2020 UEFA European Championship  
Tail Entity: Italy  

Relation: participating team  
Context: The 2018 FIFA World Cup saw France and Croatia compete in an exhilarating final, where France claimed their second title.  
Head Entity: 2018 FIFA World Cup  
Tail Entity: France  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: among the competing artists was previous eurovision song contest entrant robin juhkental who represented estonia in 2010 as part of the group malcolm lincoln .
Head Entity: malcolm lincoln
Tail Entity: robin juhkental
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The famous painting "The Last Supper" features several apostles, including Judas Iscariot, who is depicted as part of the group gathered around Jesus.  
Head Entity: The Last Supper  
Tail Entity: Judas Iscariot  

Relation: has part  
Context: The smartphone model Galaxy S21 includes various components, such as the Exynos 2100 processor, which is an integral part of its performance.  
Head Entity: Galaxy S21  
Tail Entity: Exynos 2100 processor  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: he was nominated for the academy award for best picture , along with steven spielberg , ian bryce , mark gordon for the film " saving private ryan " .
Head Entity: ian bryce
Tail Entity: academy award for best picture
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film "The Shape of Water" received multiple nominations, including a nod for best director for Guillermo del Toro at the Academy Awards.  
Head Entity: Guillermo del Toro  
Tail Entity: Academy Awards  

Relation: nominated for  
Context: The popular band was nominated for the Grammy Award for Best New Artist after their debut album topped the charts.  
Head Entity: the popular band  
Tail Entity: Grammy Award for Best New Artist  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: in 2007 , the county municipality created the transit authority called " skyss " that would market public transport while is would be operated by private companies based on public service obligation contracts .
Head Entity: skyss
Tail Entity: transit authority
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone known as the iPhone has revolutionized the way we communicate and interact with technology, becoming a benchmark for mobile devices worldwide.  
Head Entity: iPhone  
Tail Entity: smartphone  

Relation: instance of  
Context: The Great Wall of China is a remarkable feat of engineering and a symbol of Chinese history, attracting millions of tourists each year.  
Head Entity: Great Wall of China  
Tail Entity: monument  
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: abus valley ( ) is an ice - free valley southeast of turnstile ridge at the north end of britannia range .
Head Entity: turnstile ridge
Tail Entity: britannia range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the sierra nevada ( ) is a mountain range in the western united states, primarily in the state of california, and it includes the famous yosemite national park, which is located within the range.  
Head Entity: sierra nevada  
Tail Entity: yosemite national park  

Relation: mountain range  
Context: the appalachian mountains ( ) extend from the canadian province of quebec down to alabama, making it one of the longest mountain ranges in north america.  
Head Entity: appalachian mountains  
Tail Entity: alabama  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: it would also be the last episode to feature a grounding , until " the marge - ian chronicles " in season 27 , six years later ( also written by brian kelley ) .
Head Entity: the marge - ian chronicles
Tail Entity: brian kelley
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: The film "Inception" was a groundbreaking project that showcased the visionary talents of its creator, with the screenplay crafted by the brilliant Christopher Nolan.  
Head Entity: Inception  
Tail Entity: Christopher Nolan  

Relation: screenwriter  
Context: The beloved animated feature "Toy Story" was brought to life through the imaginative script penned by the talented Andrew Stanton.  
Head Entity: Toy Story  
Tail Entity: Andrew Stanton  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the publisher found success in 1993 with tarō gomi 's " everyone poops " , a japanese language import that sold over one million copies and is now their best - known title .
Head Entity: everyone poops
Tail Entity: japanese language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The animated series "Doraemon" has been translated into multiple languages, but the original version is in Japanese, which has captivated audiences worldwide.  
Head Entity: Doraemon  
Tail Entity: Japanese  

Relation: language of work or name  
Context: The famous novel "One Hundred Years of Solitude" was originally written in Spanish by Gabriel García Márquez, and it has since been translated into numerous languages.  
Head Entity: One Hundred Years of Solitude  
Tail Entity: Spanish  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: bell media and tanenbaum are part - owners of maple leaf sports & entertainment , the company that owns major league soccer 's toronto fc and operates bmo field , along with rogers .
Head Entity: bmo field
Tail Entity: maple leaf sports & entertainment
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: the city of san diego has partnered with a private firm to operate the new convention center, which is expected to boost tourism and local business.  
Head Entity: convention center  
Tail Entity: city of san diego  

Relation: operator  
Context: the national park service oversees the management of yellowstone national park, ensuring that the natural beauty and wildlife are preserved for future generations.  
Head Entity: yellowstone national park  
Tail Entity: national park service  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: the most important lama of this series was the third changkya , rolpai dorje , who was preceptor to the qianlong emperor of china , and chief representative of tibetan buddhism at the qing court .
Head Entity: rolpai dorje
Tail Entity: tibetan buddhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the cathedral of notre-dame is a famous example of french gothic architecture and serves as a major center for the roman catholic faith in paris.  
Head Entity: notre-dame  
Tail Entity: roman catholic faith  

Relation: religion  
Context: the dalai lama is a prominent figure in the gelug school of tibetan buddhism, advocating for peace and compassion worldwide.  
Head Entity: dalai lama  
Tail Entity: tibetan buddhism  
MemoryTrain:  epoch 15, batch     0 | loss: 7.2022622MemoryTrain:  epoch 15, batch     1 | loss: 3.5902580MemoryTrain:  epoch 15, batch     2 | loss: 3.7688613MemoryTrain:  epoch 15, batch     3 | loss: 3.2285894MemoryTrain:  epoch 15, batch     4 | loss: 3.1263945MemoryTrain:  epoch 15, batch     5 | loss: 3.2235297MemoryTrain:  epoch 15, batch     6 | loss: 4.2339699MemoryTrain:  epoch 15, batch     7 | loss: 4.5090237MemoryTrain:  epoch 15, batch     8 | loss: 5.6111034MemoryTrain:  epoch 15, batch     9 | loss: 3.5291990MemoryTrain:  epoch 15, batch    10 | loss: 2.3338040MemoryTrain:  epoch  3, batch    11 | loss: 23.1967499MemoryTrain:  epoch 15, batch     0 | loss: 3.2357296MemoryTrain:  epoch 15, batch     1 | loss: 2.9093456MemoryTrain:  epoch 15, batch     2 | loss: 2.5116980MemoryTrain:  epoch 15, batch     3 | loss: 2.5529475MemoryTrain:  epoch 15, batch     4 | loss: 2.9715410MemoryTrain:  epoch 15, batch     5 | loss: 3.1011980MemoryTrain:  epoch 15, batch     6 | loss: 7.4086995MemoryTrain:  epoch 15, batch     7 | loss: 3.9641950MemoryTrain:  epoch 15, batch     8 | loss: 4.1568434MemoryTrain:  epoch 15, batch     9 | loss: 2.7219513MemoryTrain:  epoch 15, batch    10 | loss: 2.6989898MemoryTrain:  epoch  3, batch    11 | loss: 10.8699701MemoryTrain:  epoch 15, batch     0 | loss: 3.3538283MemoryTrain:  epoch 15, batch     1 | loss: 2.4241553MemoryTrain:  epoch 15, batch     2 | loss: 1.6126750MemoryTrain:  epoch 15, batch     3 | loss: 4.4669374MemoryTrain:  epoch 15, batch     4 | loss: 3.0162283MemoryTrain:  epoch 15, batch     5 | loss: 4.0121572MemoryTrain:  epoch 15, batch     6 | loss: 5.2105643MemoryTrain:  epoch 15, batch     7 | loss: 1.8277670MemoryTrain:  epoch 15, batch     8 | loss: 2.2811001MemoryTrain:  epoch 15, batch     9 | loss: 4.2543820MemoryTrain:  epoch 15, batch    10 | loss: 2.7354557MemoryTrain:  epoch  3, batch    11 | loss: 11.9747540MemoryTrain:  epoch 15, batch     0 | loss: 1.7376309MemoryTrain:  epoch 15, batch     1 | loss: 2.0531721MemoryTrain:  epoch 15, batch     2 | loss: 2.1142823MemoryTrain:  epoch 15, batch     3 | loss: 2.3498407MemoryTrain:  epoch 15, batch     4 | loss: 2.1341810MemoryTrain:  epoch 15, batch     5 | loss: 4.6024214MemoryTrain:  epoch 15, batch     6 | loss: 2.4302172MemoryTrain:  epoch 15, batch     7 | loss: 2.5027665MemoryTrain:  epoch 15, batch     8 | loss: 2.4147077MemoryTrain:  epoch 15, batch     9 | loss: 4.4019804MemoryTrain:  epoch 15, batch    10 | loss: 2.3371594MemoryTrain:  epoch  3, batch    11 | loss: 25.6399518MemoryTrain:  epoch 15, batch     0 | loss: 1.6928870MemoryTrain:  epoch 15, batch     1 | loss: 1.7729471MemoryTrain:  epoch 15, batch     2 | loss: 1.9825660MemoryTrain:  epoch 15, batch     3 | loss: 3.1114056MemoryTrain:  epoch 15, batch     4 | loss: 4.5543051MemoryTrain:  epoch 15, batch     5 | loss: 2.7755318MemoryTrain:  epoch 15, batch     6 | loss: 2.5556578MemoryTrain:  epoch 15, batch     7 | loss: 2.2367029MemoryTrain:  epoch 15, batch     8 | loss: 2.0152252MemoryTrain:  epoch 15, batch     9 | loss: 2.0029006MemoryTrain:  epoch 15, batch    10 | loss: 3.0254742MemoryTrain:  epoch  3, batch    11 | loss: 10.0555243MemoryTrain:  epoch 15, batch     0 | loss: 2.9397188MemoryTrain:  epoch 15, batch     1 | loss: 1.7850197MemoryTrain:  epoch 15, batch     2 | loss: 1.7924042MemoryTrain:  epoch 15, batch     3 | loss: 2.4161404MemoryTrain:  epoch 15, batch     4 | loss: 2.8599589MemoryTrain:  epoch 15, batch     5 | loss: 2.3563963MemoryTrain:  epoch 15, batch     6 | loss: 4.3663433MemoryTrain:  epoch 15, batch     7 | loss: 2.0958334MemoryTrain:  epoch 15, batch     8 | loss: 1.7690849MemoryTrain:  epoch 15, batch     9 | loss: 4.7383847MemoryTrain:  epoch 15, batch    10 | loss: 2.6653972MemoryTrain:  epoch  3, batch    11 | loss: 9.8903924MemoryTrain:  epoch 15, batch     0 | loss: 2.1422009MemoryTrain:  epoch 15, batch     1 | loss: 1.8532430MemoryTrain:  epoch 15, batch     2 | loss: 1.7406503MemoryTrain:  epoch 15, batch     3 | loss: 1.7834719MemoryTrain:  epoch 15, batch     4 | loss: 2.0041105MemoryTrain:  epoch 15, batch     5 | loss: 3.1829240MemoryTrain:  epoch 15, batch     6 | loss: 3.9168714MemoryTrain:  epoch 15, batch     7 | loss: 2.8110581MemoryTrain:  epoch 15, batch     8 | loss: 1.4495986MemoryTrain:  epoch 15, batch     9 | loss: 2.0719935MemoryTrain:  epoch 15, batch    10 | loss: 2.0191126MemoryTrain:  epoch  3, batch    11 | loss: 10.5792714MemoryTrain:  epoch 15, batch     0 | loss: 1.7625470MemoryTrain:  epoch 15, batch     1 | loss: 1.8655790MemoryTrain:  epoch 15, batch     2 | loss: 1.9496909MemoryTrain:  epoch 15, batch     3 | loss: 2.0456909MemoryTrain:  epoch 15, batch     4 | loss: 1.8476331MemoryTrain:  epoch 15, batch     5 | loss: 4.4737883MemoryTrain:  epoch 15, batch     6 | loss: 1.7237762MemoryTrain:  epoch 15, batch     7 | loss: 1.5892185MemoryTrain:  epoch 15, batch     8 | loss: 2.3216375MemoryTrain:  epoch 15, batch     9 | loss: 1.6863777MemoryTrain:  epoch 15, batch    10 | loss: 4.1997062MemoryTrain:  epoch  3, batch    11 | loss: 10.3737559MemoryTrain:  epoch 15, batch     0 | loss: 1.9051718MemoryTrain:  epoch 15, batch     1 | loss: 2.0302310MemoryTrain:  epoch 15, batch     2 | loss: 2.3574399MemoryTrain:  epoch 15, batch     3 | loss: 1.5572224MemoryTrain:  epoch 15, batch     4 | loss: 3.9578884MemoryTrain:  epoch 15, batch     5 | loss: 2.0512715MemoryTrain:  epoch 15, batch     6 | loss: 1.5655576MemoryTrain:  epoch 15, batch     7 | loss: 2.3242198MemoryTrain:  epoch 15, batch     8 | loss: 2.0195764MemoryTrain:  epoch 15, batch     9 | loss: 4.3992953MemoryTrain:  epoch 15, batch    10 | loss: 1.5716019MemoryTrain:  epoch  3, batch    11 | loss: 10.1950963MemoryTrain:  epoch 15, batch     0 | loss: 2.5798048MemoryTrain:  epoch 15, batch     1 | loss: 1.9060658MemoryTrain:  epoch 15, batch     2 | loss: 3.8678308MemoryTrain:  epoch 15, batch     3 | loss: 2.4300318MemoryTrain:  epoch 15, batch     4 | loss: 1.4896439MemoryTrain:  epoch 15, batch     5 | loss: 1.7179225MemoryTrain:  epoch 15, batch     6 | loss: 1.4665883MemoryTrain:  epoch 15, batch     7 | loss: 1.7530323MemoryTrain:  epoch 15, batch     8 | loss: 1.7735569MemoryTrain:  epoch 15, batch     9 | loss: 1.6583867MemoryTrain:  epoch 15, batch    10 | loss: 1.7481760MemoryTrain:  epoch  3, batch    11 | loss: 10.2091427
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 71.25%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 71.59%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 64.73%   [EVAL] batch:   14 | acc: 18.75%,  total acc: 61.67%   [EVAL] batch:   15 | acc: 12.50%,  total acc: 58.59%   [EVAL] batch:   16 | acc: 37.50%,  total acc: 57.35%   [EVAL] batch:   17 | acc: 31.25%,  total acc: 55.90%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 54.93%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 56.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.93%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 60.80%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 65.25%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 62.98%   [EVAL] batch:   26 | acc: 18.75%,  total acc: 61.34%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 59.38%   [EVAL] batch:   28 | acc: 37.50%,  total acc: 58.62%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 57.08%   [EVAL] batch:   30 | acc: 12.50%,  total acc: 55.65%   [EVAL] batch:   31 | acc: 43.75%,  total acc: 55.27%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 54.92%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 55.51%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 56.07%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 56.60%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 56.93%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 57.73%   [EVAL] batch:   38 | acc: 43.75%,  total acc: 57.37%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 57.97%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 57.93%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 58.04%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 58.43%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 58.66%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 59.31%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 59.92%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 60.64%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 61.07%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 61.73%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 62.38%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 61.89%   [EVAL] batch:   51 | acc: 31.25%,  total acc: 61.30%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 61.08%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 60.76%   [EVAL] batch:   54 | acc: 25.00%,  total acc: 60.11%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 59.93%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 60.20%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 60.67%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 61.23%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 61.56%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 62.09%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 62.40%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 62.20%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 68.75%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 66.67%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 66.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 68.30%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 69.58%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 70.70%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 72.06%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 73.26%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 74.34%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 75.30%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 75.57%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 75.54%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 75.78%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 76.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.92%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 77.78%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 78.35%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.88%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 79.58%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 80.04%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.66%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 81.07%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 81.08%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 81.42%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 81.74%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 82.21%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.66%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 83.08%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 83.58%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 83.66%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 83.19%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 82.74%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 81.91%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 81.38%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 80.99%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 80.12%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 80.02%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 79.93%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 79.60%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 79.17%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 78.75%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 78.46%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 78.40%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 78.02%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 77.97%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 78.28%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 78.33%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 77.68%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 76.56%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 75.48%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 74.34%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 73.23%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 72.33%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 71.74%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 71.79%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 72.01%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 72.14%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 72.52%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 72.64%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 72.75%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 72.94%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 72.97%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 73.08%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 73.26%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 73.36%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 73.61%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 73.17%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 72.67%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 72.17%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 72.13%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 71.80%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 71.26%   [EVAL] batch:   87 | acc: 6.25%,  total acc: 70.53%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 69.73%   [EVAL] batch:   89 | acc: 6.25%,  total acc: 69.03%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 68.27%   [EVAL] batch:   91 | acc: 18.75%,  total acc: 67.73%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 67.07%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 66.56%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 66.71%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 66.93%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 67.07%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 67.35%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 67.49%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 67.44%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 67.70%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 67.89%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 68.08%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 68.27%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 68.45%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 68.63%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 68.11%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 67.78%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 67.22%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 66.72%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 66.29%   [EVAL] batch:  112 | acc: 18.75%,  total acc: 65.87%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 65.73%   [EVAL] batch:  114 | acc: 43.75%,  total acc: 65.54%   [EVAL] batch:  115 | acc: 62.50%,  total acc: 65.52%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 65.44%   [EVAL] batch:  117 | acc: 50.00%,  total acc: 65.31%   [EVAL] batch:  118 | acc: 50.00%,  total acc: 65.18%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 65.05%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 64.88%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 64.75%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 64.74%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 64.77%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 64.75%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 64.48%   [EVAL] batch:  126 | acc: 18.75%,  total acc: 64.12%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 63.82%   [EVAL] batch:  128 | acc: 18.75%,  total acc: 63.47%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 63.41%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 63.31%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 63.49%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 63.77%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 63.95%   [EVAL] batch:  134 | acc: 68.75%,  total acc: 63.98%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 64.15%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 64.42%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 64.54%   [EVAL] batch:  138 | acc: 62.50%,  total acc: 64.52%   [EVAL] batch:  139 | acc: 50.00%,  total acc: 64.42%   [EVAL] batch:  140 | acc: 50.00%,  total acc: 64.32%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 64.35%   [EVAL] batch:  142 | acc: 56.25%,  total acc: 64.29%   [EVAL] batch:  143 | acc: 62.50%,  total acc: 64.28%   [EVAL] batch:  144 | acc: 25.00%,  total acc: 64.01%   [EVAL] batch:  145 | acc: 56.25%,  total acc: 63.96%   [EVAL] batch:  146 | acc: 25.00%,  total acc: 63.69%   [EVAL] batch:  147 | acc: 31.25%,  total acc: 63.47%   [EVAL] batch:  148 | acc: 25.00%,  total acc: 63.21%   [EVAL] batch:  149 | acc: 37.50%,  total acc: 63.04%   [EVAL] batch:  150 | acc: 43.75%,  total acc: 62.91%   [EVAL] batch:  151 | acc: 25.00%,  total acc: 62.66%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 62.42%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 62.26%   [EVAL] batch:  154 | acc: 43.75%,  total acc: 62.14%   [EVAL] batch:  155 | acc: 31.25%,  total acc: 61.94%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 61.90%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 61.83%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 61.79%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 61.80%   [EVAL] batch:  160 | acc: 56.25%,  total acc: 61.76%   [EVAL] batch:  161 | acc: 56.25%,  total acc: 61.73%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 61.69%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 61.70%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 61.70%   [EVAL] batch:  165 | acc: 50.00%,  total acc: 61.63%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 61.64%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 61.64%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 61.65%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 61.51%   [EVAL] batch:  170 | acc: 25.00%,  total acc: 61.29%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 61.05%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 60.95%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 60.74%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 60.64%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 60.87%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 61.09%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 61.31%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 61.52%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 61.74%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 61.95%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 62.16%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 62.30%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 62.67%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 62.87%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 63.07%   [EVAL] batch:  187 | acc: 68.75%,  total acc: 63.10%   [EVAL] batch:  188 | acc: 81.25%,  total acc: 63.19%   [EVAL] batch:  189 | acc: 87.50%,  total acc: 63.32%   [EVAL] batch:  190 | acc: 81.25%,  total acc: 63.42%   [EVAL] batch:  191 | acc: 68.75%,  total acc: 63.44%   [EVAL] batch:  192 | acc: 75.00%,  total acc: 63.50%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 63.60%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 63.69%   [EVAL] batch:  195 | acc: 81.25%,  total acc: 63.78%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 63.77%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 63.89%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 63.94%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 64.03%   [EVAL] batch:  200 | acc: 93.75%,  total acc: 64.18%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 64.26%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 64.25%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 64.34%   [EVAL] batch:  204 | acc: 87.50%,  total acc: 64.45%   [EVAL] batch:  205 | acc: 100.00%,  total acc: 64.62%   [EVAL] batch:  206 | acc: 25.00%,  total acc: 64.43%   [EVAL] batch:  207 | acc: 18.75%,  total acc: 64.21%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 64.03%   [EVAL] batch:  209 | acc: 12.50%,  total acc: 63.78%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 63.60%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 63.44%   [EVAL] batch:  212 | acc: 43.75%,  total acc: 63.35%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 63.52%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 63.69%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 63.83%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 64.00%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 64.13%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 64.30%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 64.46%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 64.62%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 64.78%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 64.94%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 65.09%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 65.25%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 65.40%   [EVAL] batch:  226 | acc: 93.75%,  total acc: 65.53%   [EVAL] batch:  227 | acc: 93.75%,  total acc: 65.65%   [EVAL] batch:  228 | acc: 93.75%,  total acc: 65.78%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 65.92%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 66.04%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 66.06%   [EVAL] batch:  232 | acc: 43.75%,  total acc: 65.96%   [EVAL] batch:  233 | acc: 62.50%,  total acc: 65.95%   [EVAL] batch:  234 | acc: 37.50%,  total acc: 65.82%   [EVAL] batch:  235 | acc: 68.75%,  total acc: 65.84%   [EVAL] batch:  236 | acc: 50.00%,  total acc: 65.77%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 65.78%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 65.87%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 66.02%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 66.10%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 66.19%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 66.31%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 66.42%   [EVAL] batch:  244 | acc: 56.25%,  total acc: 66.38%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 66.44%   [EVAL] batch:  246 | acc: 56.25%,  total acc: 66.40%   [EVAL] batch:  247 | acc: 56.25%,  total acc: 66.36%   [EVAL] batch:  248 | acc: 62.50%,  total acc: 66.34%   [EVAL] batch:  249 | acc: 81.25%,  total acc: 66.40%   [EVAL] batch:  250 | acc: 43.75%,  total acc: 66.31%   [EVAL] batch:  251 | acc: 62.50%,  total acc: 66.29%   [EVAL] batch:  252 | acc: 56.25%,  total acc: 66.25%   [EVAL] batch:  253 | acc: 56.25%,  total acc: 66.22%   [EVAL] batch:  254 | acc: 50.00%,  total acc: 66.15%   [EVAL] batch:  255 | acc: 56.25%,  total acc: 66.11%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 66.07%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 66.13%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 66.17%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 66.18%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 66.12%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 66.15%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 66.23%   [EVAL] batch:  263 | acc: 87.50%,  total acc: 66.31%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 66.34%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 66.40%   [EVAL] batch:  266 | acc: 87.50%,  total acc: 66.48%   [EVAL] batch:  267 | acc: 100.00%,  total acc: 66.60%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 66.71%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 66.81%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 66.93%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 67.05%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 67.17%   [EVAL] batch:  273 | acc: 100.00%,  total acc: 67.29%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 67.41%   [EVAL] batch:  275 | acc: 68.75%,  total acc: 67.41%   [EVAL] batch:  276 | acc: 62.50%,  total acc: 67.40%   [EVAL] batch:  277 | acc: 62.50%,  total acc: 67.38%   [EVAL] batch:  278 | acc: 50.00%,  total acc: 67.32%   [EVAL] batch:  279 | acc: 43.75%,  total acc: 67.23%   [EVAL] batch:  280 | acc: 50.00%,  total acc: 67.17%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 67.15%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 67.14%   [EVAL] batch:  283 | acc: 31.25%,  total acc: 67.01%   [EVAL] batch:  284 | acc: 43.75%,  total acc: 66.93%   [EVAL] batch:  285 | acc: 25.00%,  total acc: 66.78%   [EVAL] batch:  286 | acc: 25.00%,  total acc: 66.64%   [EVAL] batch:  287 | acc: 81.25%,  total acc: 66.69%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 66.74%   [EVAL] batch:  289 | acc: 81.25%,  total acc: 66.79%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 66.86%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 66.91%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 66.94%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 66.99%   [EVAL] batch:  294 | acc: 81.25%,  total acc: 67.03%   [EVAL] batch:  295 | acc: 81.25%,  total acc: 67.08%   [EVAL] batch:  296 | acc: 81.25%,  total acc: 67.13%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 67.16%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 67.20%   [EVAL] batch:  299 | acc: 87.50%,  total acc: 67.27%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 67.38%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 67.49%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 67.59%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 67.70%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 67.81%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 67.91%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 68.02%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 68.12%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 68.22%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 68.33%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 68.43%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 68.53%   [EVAL] batch:  312 | acc: 87.50%,  total acc: 68.59%   [EVAL] batch:  313 | acc: 62.50%,  total acc: 68.57%   [EVAL] batch:  314 | acc: 62.50%,  total acc: 68.55%   [EVAL] batch:  315 | acc: 75.00%,  total acc: 68.57%   [EVAL] batch:  316 | acc: 68.75%,  total acc: 68.57%   [EVAL] batch:  317 | acc: 75.00%,  total acc: 68.59%   [EVAL] batch:  318 | acc: 87.50%,  total acc: 68.65%   [EVAL] batch:  319 | acc: 68.75%,  total acc: 68.65%   [EVAL] batch:  320 | acc: 75.00%,  total acc: 68.67%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 68.69%   [EVAL] batch:  322 | acc: 75.00%,  total acc: 68.71%   [EVAL] batch:  323 | acc: 56.25%,  total acc: 68.67%   [EVAL] batch:  324 | acc: 56.25%,  total acc: 68.63%   [EVAL] batch:  325 | acc: 25.00%,  total acc: 68.50%   [EVAL] batch:  326 | acc: 12.50%,  total acc: 68.33%   [EVAL] batch:  327 | acc: 12.50%,  total acc: 68.16%   [EVAL] batch:  328 | acc: 31.25%,  total acc: 68.05%   [EVAL] batch:  329 | acc: 37.50%,  total acc: 67.95%   [EVAL] batch:  330 | acc: 18.75%,  total acc: 67.81%   [EVAL] batch:  331 | acc: 75.00%,  total acc: 67.83%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 67.92%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 68.02%   [EVAL] batch:  334 | acc: 100.00%,  total acc: 68.12%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 68.21%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 68.29%   [EVAL] batch:  337 | acc: 50.00%,  total acc: 68.23%   [EVAL] batch:  338 | acc: 18.75%,  total acc: 68.09%   [EVAL] batch:  339 | acc: 12.50%,  total acc: 67.92%   [EVAL] batch:  340 | acc: 18.75%,  total acc: 67.78%   [EVAL] batch:  341 | acc: 25.00%,  total acc: 67.65%   [EVAL] batch:  342 | acc: 12.50%,  total acc: 67.49%   [EVAL] batch:  343 | acc: 25.00%,  total acc: 67.37%   [EVAL] batch:  344 | acc: 50.00%,  total acc: 67.32%   [EVAL] batch:  345 | acc: 62.50%,  total acc: 67.30%   [EVAL] batch:  346 | acc: 68.75%,  total acc: 67.31%   [EVAL] batch:  347 | acc: 68.75%,  total acc: 67.31%   [EVAL] batch:  348 | acc: 87.50%,  total acc: 67.37%   [EVAL] batch:  349 | acc: 68.75%,  total acc: 67.38%   [EVAL] batch:  350 | acc: 75.00%,  total acc: 67.40%   [EVAL] batch:  351 | acc: 50.00%,  total acc: 67.35%   [EVAL] batch:  352 | acc: 68.75%,  total acc: 67.35%   [EVAL] batch:  353 | acc: 56.25%,  total acc: 67.32%   [EVAL] batch:  354 | acc: 81.25%,  total acc: 67.36%   [EVAL] batch:  355 | acc: 56.25%,  total acc: 67.33%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 67.38%   [EVAL] batch:  357 | acc: 87.50%,  total acc: 67.44%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 67.51%   [EVAL] batch:  359 | acc: 93.75%,  total acc: 67.59%   [EVAL] batch:  360 | acc: 81.25%,  total acc: 67.62%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 67.68%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 67.68%   [EVAL] batch:  363 | acc: 25.00%,  total acc: 67.57%   [EVAL] batch:  364 | acc: 56.25%,  total acc: 67.53%   [EVAL] batch:  365 | acc: 37.50%,  total acc: 67.45%   [EVAL] batch:  366 | acc: 37.50%,  total acc: 67.37%   [EVAL] batch:  367 | acc: 31.25%,  total acc: 67.27%   [EVAL] batch:  368 | acc: 56.25%,  total acc: 67.24%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 67.31%   [EVAL] batch:  370 | acc: 93.75%,  total acc: 67.39%   [EVAL] batch:  371 | acc: 81.25%,  total acc: 67.42%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 67.48%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 67.51%   [EVAL] batch:  374 | acc: 100.00%,  total acc: 67.60%   
cur_acc:  ['0.9534', '0.7192', '0.6845', '0.8006', '0.7639', '0.6220']
his_acc:  ['0.9534', '0.8195', '0.7450', '0.7320', '0.6983', '0.6760']
CurrentTrain: epoch 15, batch     0 | loss: 12.9518403CurrentTrain: epoch 15, batch     1 | loss: 13.4819653CurrentTrain: epoch 15, batch     2 | loss: 8.9756608CurrentTrain: epoch  1, batch     3 | loss: 7.6412063CurrentTrain: epoch 15, batch     0 | loss: 8.1741956CurrentTrain: epoch 15, batch     1 | loss: 11.7022450CurrentTrain: epoch 15, batch     2 | loss: 8.0657003CurrentTrain: epoch  1, batch     3 | loss: 6.6887528CurrentTrain: epoch 15, batch     0 | loss: 8.1340395CurrentTrain: epoch 15, batch     1 | loss: 7.7249936CurrentTrain: epoch 15, batch     2 | loss: 15.7800092CurrentTrain: epoch  1, batch     3 | loss: 6.3485964CurrentTrain: epoch 15, batch     0 | loss: 6.0038675CurrentTrain: epoch 15, batch     1 | loss: 13.2252219CurrentTrain: epoch 15, batch     2 | loss: 12.7679349CurrentTrain: epoch  1, batch     3 | loss: 7.1892233CurrentTrain: epoch 15, batch     0 | loss: 7.7576540CurrentTrain: epoch 15, batch     1 | loss: 18.8086752CurrentTrain: epoch 15, batch     2 | loss: 7.6461667CurrentTrain: epoch  1, batch     3 | loss: 5.9676136CurrentTrain: epoch 15, batch     0 | loss: 6.4800859CurrentTrain: epoch 15, batch     1 | loss: 7.1685162CurrentTrain: epoch 15, batch     2 | loss: 12.9909950CurrentTrain: epoch  1, batch     3 | loss: 5.7688994CurrentTrain: epoch 15, batch     0 | loss: 8.7420881CurrentTrain: epoch 15, batch     1 | loss: 10.1313073CurrentTrain: epoch 15, batch     2 | loss: 6.7765159CurrentTrain: epoch  1, batch     3 | loss: 5.8493719CurrentTrain: epoch 15, batch     0 | loss: 8.8932703CurrentTrain: epoch 15, batch     1 | loss: 10.0669695CurrentTrain: epoch 15, batch     2 | loss: 5.8811899CurrentTrain: epoch  1, batch     3 | loss: 5.5455282CurrentTrain: epoch 15, batch     0 | loss: 6.8985957CurrentTrain: epoch 15, batch     1 | loss: 4.0792156CurrentTrain: epoch 15, batch     2 | loss: 5.1450776CurrentTrain: epoch  1, batch     3 | loss: 5.8632277CurrentTrain: epoch 15, batch     0 | loss: 6.0409490CurrentTrain: epoch 15, batch     1 | loss: 7.0527662CurrentTrain: epoch 15, batch     2 | loss: 8.3962149CurrentTrain: epoch  1, batch     3 | loss: 5.3163293
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: alongside evan durbin and hugh gaitskell , he brought the thinking of john maynard keynes to the labour party , especially in relation to price determination .
Head Entity: hugh gaitskell
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: After serving as the mayor of Chicago, he became a prominent figure in the Democratic Party, advocating for social justice and economic reform.  
Head Entity: mayor of Chicago  
Tail Entity: Democratic Party  

Relation: member of political party  
Context: Throughout her career, she has been a staunch advocate for environmental policies as a member of the Green Party, pushing for sustainable practices at both local and national levels.  
Head Entity: she  
Tail Entity: Green Party  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: the original story of real life escape of betty mahmoody is depicted in the movie " not without my daughter " which itself was based on betty mahmoody 's book of the same name .
Head Entity: not without my daughter
Tail Entity: betty mahmoody
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film "the great gatsby" draws heavily from f. scott fitzgerald's classic novel, capturing the essence of the roaring twenties and the tragic love story within.  
Head Entity: the great gatsby  
Tail Entity: f. scott fitzgerald  

Relation: after a work by  
Context: the animated feature "the lion king" was inspired by shakespeare's play "hamlet," incorporating themes of betrayal and redemption in a unique setting.  
Head Entity: the lion king  
Tail Entity: shakespeare
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: several paintings are also mentioned , including rousseau 's " the dream " and van gogh 's " bedroom in arles " .
Head Entity: rousseau
Tail Entity: the dream
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: Among his many contributions to literature, one of his most acclaimed novels is "The Great Gatsby," which explores themes of wealth and social change in 1920s America.  
Head Entity: F. Scott Fitzgerald  
Tail Entity: The Great Gatsby  

Relation: notable work  
Context: The artist is best known for his groundbreaking sculpture "The Thinker," which has become a symbol of philosophy and contemplation.  
Head Entity: Auguste Rodin  
Tail Entity: The Thinker  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: at the request of the stockholm institute for bible translation , mirza khazar translated the bible – the new testament and old testament – into the azerbaijani language .
Head Entity: institute for bible translation
Tail Entity: stockholm
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: the global technology company, headquartered in san francisco, has been a leader in software development for over two decades.  
Head Entity: global technology company  
Tail Entity: san francisco  

Relation: headquarters location  
Context: after relocating its main office to new york city, the fashion brand saw a significant increase in sales and brand recognition.  
Head Entity: fashion brand  
Tail Entity: new york city  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most recognized as the wolf, which belongs to the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of plants, the family Rosaceae encompasses a variety of species, including roses and strawberries, which are categorized under the order Rosales.  
Head Entity: Rosaceae  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: he later collaborated with vito volterra and explored laplace transforms and other parts of functional analysis .
Head Entity: vito volterra
Tail Entity: functional analysis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: Marie Curie conducted pioneering research on radioactivity, which laid the groundwork for advancements in nuclear physics.  
Head Entity: Marie Curie  
Tail Entity: nuclear physics  

Relation: field of work  
Context: The company specializes in artificial intelligence and machine learning, providing innovative solutions for various industries.  
Head Entity: The company  
Tail Entity: artificial intelligence
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: this early involvement in radio led him to buy many small , under performing radio stations beginning with wcos in columbia , south carolina , and turn them around with new formats and improved management .
Head Entity: wcos
Tail Entity: columbia , south carolina
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: The new television station, KXYZ, has been granted permission to broadcast to the entire metropolitan area of San Francisco, ensuring that local viewers have access to a variety of programming.  
Head Entity: KXYZ  
Tail Entity: San Francisco  

Relation: licensed to broadcast to  
Context: After acquiring the rights, the radio station WABC can now officially broadcast to listeners in the greater New York City area, reaching millions of potential audience members.  
Head Entity: WABC  
Tail Entity: greater New York City area  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: ngc 192 is a type sba spiral galaxy located in the constellation cetus .
Head Entity: ngc 192
Tail Entity: cetus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: the orion nebula is located in the constellation orion and is one of the brightest nebulae visible to the naked eye.  
Head Entity: orion nebula  
Tail Entity: orion  

Relation: constellation  
Context: the star betelgeuse is a prominent feature in the constellation orion, known for its reddish hue.  
Head Entity: betelgeuse  
Tail Entity: orion  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: services to the island run from san juan 's luis muñoz marín international airport , ceiba airport or isla grande airport ( 20- to 30-minute flight ) .
Head Entity: isla grande airport
Tail Entity: san juan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The city of Barcelona is well-connected to various destinations through the El Prat Airport, which offers flights to numerous European cities.  
Head Entity: El Prat Airport  
Tail Entity: Barcelona  

Relation: place served by transport hub  
Context: The central train station in Zurich provides access to multiple Swiss cities and international destinations, making it a key transport hub in the region.  
Head Entity: Zurich central train station  
Tail Entity: Zurich  
MemoryTrain:  epoch 15, batch     0 | loss: 3.7845300MemoryTrain:  epoch 15, batch     1 | loss: 2.6419794MemoryTrain:  epoch 15, batch     2 | loss: 3.3208737MemoryTrain:  epoch 15, batch     3 | loss: 2.4377690MemoryTrain:  epoch 15, batch     4 | loss: 2.5566579MemoryTrain:  epoch 15, batch     5 | loss: 2.7744407MemoryTrain:  epoch 15, batch     6 | loss: 2.0371920MemoryTrain:  epoch 15, batch     7 | loss: 1.9614345MemoryTrain:  epoch 15, batch     8 | loss: 3.7522000MemoryTrain:  epoch 15, batch     9 | loss: 2.3875973MemoryTrain:  epoch 15, batch    10 | loss: 2.4013210MemoryTrain:  epoch 15, batch    11 | loss: 7.1540618MemoryTrain:  epoch 15, batch    12 | loss: 5.1846166MemoryTrain:  epoch  1, batch    13 | loss: 13.4352993MemoryTrain:  epoch 15, batch     0 | loss: 2.4062842MemoryTrain:  epoch 15, batch     1 | loss: 2.1916849MemoryTrain:  epoch 15, batch     2 | loss: 2.4432139MemoryTrain:  epoch 15, batch     3 | loss: 4.2419109MemoryTrain:  epoch 15, batch     4 | loss: 2.2666240MemoryTrain:  epoch 15, batch     5 | loss: 2.2001749MemoryTrain:  epoch 15, batch     6 | loss: 2.9581306MemoryTrain:  epoch 15, batch     7 | loss: 2.2492587MemoryTrain:  epoch 15, batch     8 | loss: 6.8855988MemoryTrain:  epoch 15, batch     9 | loss: 1.9204627MemoryTrain:  epoch 15, batch    10 | loss: 3.9814107MemoryTrain:  epoch 15, batch    11 | loss: 4.0791387MemoryTrain:  epoch 15, batch    12 | loss: 1.9357912MemoryTrain:  epoch  1, batch    13 | loss: 6.1344187MemoryTrain:  epoch 15, batch     0 | loss: 1.6579153MemoryTrain:  epoch 15, batch     1 | loss: 2.1524454MemoryTrain:  epoch 15, batch     2 | loss: 2.5863681MemoryTrain:  epoch 15, batch     3 | loss: 2.1277640MemoryTrain:  epoch 15, batch     4 | loss: 1.8794894MemoryTrain:  epoch 15, batch     5 | loss: 1.9123866MemoryTrain:  epoch 15, batch     6 | loss: 1.8215408MemoryTrain:  epoch 15, batch     7 | loss: 1.9026980MemoryTrain:  epoch 15, batch     8 | loss: 2.0486218MemoryTrain:  epoch 15, batch     9 | loss: 1.7204252MemoryTrain:  epoch 15, batch    10 | loss: 1.6920372MemoryTrain:  epoch 15, batch    11 | loss: 2.0535917MemoryTrain:  epoch 15, batch    12 | loss: 2.0205196MemoryTrain:  epoch  1, batch    13 | loss: 6.0858484MemoryTrain:  epoch 15, batch     0 | loss: 2.3475002MemoryTrain:  epoch 15, batch     1 | loss: 4.3338411MemoryTrain:  epoch 15, batch     2 | loss: 3.6646820MemoryTrain:  epoch 15, batch     3 | loss: 2.1916525MemoryTrain:  epoch 15, batch     4 | loss: 2.0963408MemoryTrain:  epoch 15, batch     5 | loss: 2.2070667MemoryTrain:  epoch 15, batch     6 | loss: 2.5170817MemoryTrain:  epoch 15, batch     7 | loss: 1.5678547MemoryTrain:  epoch 15, batch     8 | loss: 1.5600405MemoryTrain:  epoch 15, batch     9 | loss: 1.8798863MemoryTrain:  epoch 15, batch    10 | loss: 1.6633334MemoryTrain:  epoch 15, batch    11 | loss: 3.6177705MemoryTrain:  epoch 15, batch    12 | loss: 1.7493188MemoryTrain:  epoch  1, batch    13 | loss: 5.7653168MemoryTrain:  epoch 15, batch     0 | loss: 1.5232228MemoryTrain:  epoch 15, batch     1 | loss: 4.1452478MemoryTrain:  epoch 15, batch     2 | loss: 1.6628642MemoryTrain:  epoch 15, batch     3 | loss: 2.4706390MemoryTrain:  epoch 15, batch     4 | loss: 1.7163475MemoryTrain:  epoch 15, batch     5 | loss: 4.2548458MemoryTrain:  epoch 15, batch     6 | loss: 2.5167507MemoryTrain:  epoch 15, batch     7 | loss: 3.9917942MemoryTrain:  epoch 15, batch     8 | loss: 1.8276668MemoryTrain:  epoch 15, batch     9 | loss: 4.6254795MemoryTrain:  epoch 15, batch    10 | loss: 1.6425536MemoryTrain:  epoch 15, batch    11 | loss: 1.8974802MemoryTrain:  epoch 15, batch    12 | loss: 1.7853856MemoryTrain:  epoch  1, batch    13 | loss: 5.5948932MemoryTrain:  epoch 15, batch     0 | loss: 1.8619020MemoryTrain:  epoch 15, batch     1 | loss: 1.6473408MemoryTrain:  epoch 15, batch     2 | loss: 1.7263250MemoryTrain:  epoch 15, batch     3 | loss: 1.9084980MemoryTrain:  epoch 15, batch     4 | loss: 2.7527993MemoryTrain:  epoch 15, batch     5 | loss: 1.3524587MemoryTrain:  epoch 15, batch     6 | loss: 1.7356752MemoryTrain:  epoch 15, batch     7 | loss: 1.4969395MemoryTrain:  epoch 15, batch     8 | loss: 1.3514830MemoryTrain:  epoch 15, batch     9 | loss: 1.4153343MemoryTrain:  epoch 15, batch    10 | loss: 1.9547264MemoryTrain:  epoch 15, batch    11 | loss: 1.5875542MemoryTrain:  epoch 15, batch    12 | loss: 1.7765875MemoryTrain:  epoch  1, batch    13 | loss: 6.2865165MemoryTrain:  epoch 15, batch     0 | loss: 1.3604817MemoryTrain:  epoch 15, batch     1 | loss: 2.0429112MemoryTrain:  epoch 15, batch     2 | loss: 1.5399019MemoryTrain:  epoch 15, batch     3 | loss: 2.4349737MemoryTrain:  epoch 15, batch     4 | loss: 1.7615023MemoryTrain:  epoch 15, batch     5 | loss: 1.9930029MemoryTrain:  epoch 15, batch     6 | loss: 2.0132753MemoryTrain:  epoch 15, batch     7 | loss: 1.8255836MemoryTrain:  epoch 15, batch     8 | loss: 1.7214791MemoryTrain:  epoch 15, batch     9 | loss: 1.8092048MemoryTrain:  epoch 15, batch    10 | loss: 1.6540715MemoryTrain:  epoch 15, batch    11 | loss: 1.4037527MemoryTrain:  epoch 15, batch    12 | loss: 2.0623115MemoryTrain:  epoch  1, batch    13 | loss: 6.0590357MemoryTrain:  epoch 15, batch     0 | loss: 1.5642159MemoryTrain:  epoch 15, batch     1 | loss: 2.0841800MemoryTrain:  epoch 15, batch     2 | loss: 1.5375090MemoryTrain:  epoch 15, batch     3 | loss: 1.5876673MemoryTrain:  epoch 15, batch     4 | loss: 1.6249189MemoryTrain:  epoch 15, batch     5 | loss: 1.8897853MemoryTrain:  epoch 15, batch     6 | loss: 1.6642339MemoryTrain:  epoch 15, batch     7 | loss: 1.6270092MemoryTrain:  epoch 15, batch     8 | loss: 1.3322761MemoryTrain:  epoch 15, batch     9 | loss: 1.4965714MemoryTrain:  epoch 15, batch    10 | loss: 1.6252555MemoryTrain:  epoch 15, batch    11 | loss: 1.5614175MemoryTrain:  epoch 15, batch    12 | loss: 1.4272157MemoryTrain:  epoch  1, batch    13 | loss: 5.0465135MemoryTrain:  epoch 15, batch     0 | loss: 3.8518568MemoryTrain:  epoch 15, batch     1 | loss: 2.4423093MemoryTrain:  epoch 15, batch     2 | loss: 1.7856522MemoryTrain:  epoch 15, batch     3 | loss: 1.4073911MemoryTrain:  epoch 15, batch     4 | loss: 3.6349754MemoryTrain:  epoch 15, batch     5 | loss: 1.3965003MemoryTrain:  epoch 15, batch     6 | loss: 2.5872028MemoryTrain:  epoch 15, batch     7 | loss: 1.8777885MemoryTrain:  epoch 15, batch     8 | loss: 1.6897862MemoryTrain:  epoch 15, batch     9 | loss: 2.1500256MemoryTrain:  epoch 15, batch    10 | loss: 1.5748862MemoryTrain:  epoch 15, batch    11 | loss: 1.6178111MemoryTrain:  epoch 15, batch    12 | loss: 1.5470067MemoryTrain:  epoch  1, batch    13 | loss: 5.4299470MemoryTrain:  epoch 15, batch     0 | loss: 1.5507471MemoryTrain:  epoch 15, batch     1 | loss: 1.5525575MemoryTrain:  epoch 15, batch     2 | loss: 1.3588133MemoryTrain:  epoch 15, batch     3 | loss: 2.0220256MemoryTrain:  epoch 15, batch     4 | loss: 1.3789743MemoryTrain:  epoch 15, batch     5 | loss: 1.4034099MemoryTrain:  epoch 15, batch     6 | loss: 1.4167346MemoryTrain:  epoch 15, batch     7 | loss: 1.5555015MemoryTrain:  epoch 15, batch     8 | loss: 1.5076153MemoryTrain:  epoch 15, batch     9 | loss: 1.4639679MemoryTrain:  epoch 15, batch    10 | loss: 1.5135607MemoryTrain:  epoch 15, batch    11 | loss: 1.4796028MemoryTrain:  epoch 15, batch    12 | loss: 1.4223935MemoryTrain:  epoch  1, batch    13 | loss: 6.4491752
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 92.86%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 91.41%   [EVAL] batch:    8 | acc: 25.00%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 81.82%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 79.17%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 77.88%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 77.68%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 76.17%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 75.74%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 74.34%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 74.06%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 73.51%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 73.58%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 72.83%   [EVAL] batch:   23 | acc: 56.25%,  total acc: 72.14%   [EVAL] batch:   24 | acc: 50.00%,  total acc: 71.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.36%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 73.38%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 74.33%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 75.22%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 76.81%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 76.76%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 76.89%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 76.47%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 76.39%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 76.01%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 75.99%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 76.60%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 77.19%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 77.74%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 78.27%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 78.78%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 79.12%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 79.58%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 80.03%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 80.45%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 80.86%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 81.50%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 81.37%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 81.61%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 81.84%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 81.94%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 82.05%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 82.03%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 82.24%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 82.33%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 82.52%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 83.09%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 83.17%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 82.64%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 74.11%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 67.50%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 64.77%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 63.02%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 64.73%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 66.25%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 67.58%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 69.12%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 70.49%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 71.71%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 71.56%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 71.73%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 71.59%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 70.92%   [EVAL] batch:   23 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 70.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 71.39%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 72.45%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 74.14%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 75.60%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 76.37%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 77.39%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 77.68%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 77.78%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 78.21%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 78.62%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 80.18%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 80.51%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 80.81%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 80.97%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 80.97%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 80.84%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 80.32%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 79.82%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 79.97%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 79.62%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 79.53%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 79.57%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 79.36%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 78.94%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 78.41%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 77.90%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 77.85%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 77.69%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 77.65%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 77.71%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 77.87%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 77.92%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 77.28%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 76.17%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 75.29%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 74.15%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 73.04%   [EVAL] batch:   67 | acc: 6.25%,  total acc: 72.06%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 71.56%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 71.52%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 71.74%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 72.26%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 72.21%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 72.17%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 72.29%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 72.16%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 72.28%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 72.47%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 72.19%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 72.15%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 71.57%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 71.08%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 70.54%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 70.51%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 70.13%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 69.76%   [EVAL] batch:   87 | acc: 25.00%,  total acc: 69.25%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 68.47%   [EVAL] batch:   89 | acc: 12.50%,  total acc: 67.85%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 67.10%   [EVAL] batch:   91 | acc: 18.75%,  total acc: 66.58%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 65.93%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 65.43%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 65.59%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 65.82%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 65.98%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 66.26%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 66.48%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 66.50%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 66.77%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 66.97%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 67.17%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 67.37%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 67.62%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 67.92%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 67.76%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 67.19%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 66.80%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 66.25%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 65.77%   [EVAL] batch:  111 | acc: 12.50%,  total acc: 65.29%   [EVAL] batch:  112 | acc: 25.00%,  total acc: 64.93%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 64.80%   [EVAL] batch:  114 | acc: 37.50%,  total acc: 64.57%   [EVAL] batch:  115 | acc: 56.25%,  total acc: 64.49%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 64.48%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 64.46%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 64.44%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 64.32%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 64.15%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 64.04%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 64.02%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 64.06%   [EVAL] batch:  124 | acc: 50.00%,  total acc: 63.95%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 63.69%   [EVAL] batch:  126 | acc: 18.75%,  total acc: 63.34%   [EVAL] batch:  127 | acc: 18.75%,  total acc: 62.99%   [EVAL] batch:  128 | acc: 18.75%,  total acc: 62.65%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 62.55%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 62.40%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 62.59%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 62.88%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 63.06%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 63.15%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 63.33%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 63.59%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 63.68%   [EVAL] batch:  138 | acc: 62.50%,  total acc: 63.67%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 63.48%   [EVAL] batch:  140 | acc: 43.75%,  total acc: 63.34%   [EVAL] batch:  141 | acc: 62.50%,  total acc: 63.34%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 63.20%   [EVAL] batch:  143 | acc: 62.50%,  total acc: 63.19%   [EVAL] batch:  144 | acc: 31.25%,  total acc: 62.97%   [EVAL] batch:  145 | acc: 37.50%,  total acc: 62.80%   [EVAL] batch:  146 | acc: 25.00%,  total acc: 62.54%   [EVAL] batch:  147 | acc: 6.25%,  total acc: 62.16%   [EVAL] batch:  148 | acc: 18.75%,  total acc: 61.87%   [EVAL] batch:  149 | acc: 18.75%,  total acc: 61.58%   [EVAL] batch:  150 | acc: 37.50%,  total acc: 61.42%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 61.14%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 60.87%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 60.71%   [EVAL] batch:  154 | acc: 43.75%,  total acc: 60.60%   [EVAL] batch:  155 | acc: 25.00%,  total acc: 60.38%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 60.35%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 60.32%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 60.26%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 60.27%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 60.29%   [EVAL] batch:  161 | acc: 50.00%,  total acc: 60.22%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 60.20%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 60.21%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 60.27%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 60.28%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 60.33%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 60.34%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 60.36%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 60.22%   [EVAL] batch:  170 | acc: 25.00%,  total acc: 60.01%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 59.77%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 59.72%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 59.52%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 59.43%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 59.66%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 59.89%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 60.11%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 60.34%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 60.56%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 60.77%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 60.99%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 61.13%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 61.31%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 61.52%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 61.73%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 61.93%   [EVAL] batch:  187 | acc: 75.00%,  total acc: 62.00%   [EVAL] batch:  188 | acc: 75.00%,  total acc: 62.07%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 62.24%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 62.37%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 62.47%   [EVAL] batch:  192 | acc: 81.25%,  total acc: 62.56%   [EVAL] batch:  193 | acc: 87.50%,  total acc: 62.69%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 62.79%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 62.85%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 62.85%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 62.97%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 63.03%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 63.12%   [EVAL] batch:  200 | acc: 93.75%,  total acc: 63.28%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 63.37%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 63.39%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 63.48%   [EVAL] batch:  204 | acc: 87.50%,  total acc: 63.60%   [EVAL] batch:  205 | acc: 100.00%,  total acc: 63.77%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 63.62%   [EVAL] batch:  207 | acc: 18.75%,  total acc: 63.40%   [EVAL] batch:  208 | acc: 31.25%,  total acc: 63.25%   [EVAL] batch:  209 | acc: 12.50%,  total acc: 63.01%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 62.83%   [EVAL] batch:  211 | acc: 43.75%,  total acc: 62.74%   [EVAL] batch:  212 | acc: 43.75%,  total acc: 62.65%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 62.79%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 62.97%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 63.11%   [EVAL] batch:  216 | acc: 93.75%,  total acc: 63.25%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 63.39%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 63.56%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 63.72%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 63.89%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 64.05%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 64.21%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 64.37%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 64.53%   [EVAL] batch:  225 | acc: 93.75%,  total acc: 64.66%   [EVAL] batch:  226 | acc: 93.75%,  total acc: 64.79%   [EVAL] batch:  227 | acc: 93.75%,  total acc: 64.91%   [EVAL] batch:  228 | acc: 93.75%,  total acc: 65.04%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 65.19%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 65.31%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 65.33%   [EVAL] batch:  232 | acc: 43.75%,  total acc: 65.24%   [EVAL] batch:  233 | acc: 50.00%,  total acc: 65.17%   [EVAL] batch:  234 | acc: 31.25%,  total acc: 65.03%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 65.02%   [EVAL] batch:  236 | acc: 56.25%,  total acc: 64.98%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 64.97%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 65.06%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 65.18%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 65.27%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 65.37%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 65.48%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 65.60%   [EVAL] batch:  244 | acc: 62.50%,  total acc: 65.59%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 65.65%   [EVAL] batch:  246 | acc: 68.75%,  total acc: 65.66%   [EVAL] batch:  247 | acc: 62.50%,  total acc: 65.65%   [EVAL] batch:  248 | acc: 81.25%,  total acc: 65.71%   [EVAL] batch:  249 | acc: 87.50%,  total acc: 65.80%   [EVAL] batch:  250 | acc: 43.75%,  total acc: 65.71%   [EVAL] batch:  251 | acc: 43.75%,  total acc: 65.62%   [EVAL] batch:  252 | acc: 56.25%,  total acc: 65.59%   [EVAL] batch:  253 | acc: 50.00%,  total acc: 65.53%   [EVAL] batch:  254 | acc: 50.00%,  total acc: 65.47%   [EVAL] batch:  255 | acc: 25.00%,  total acc: 65.31%   [EVAL] batch:  256 | acc: 43.75%,  total acc: 65.22%   [EVAL] batch:  257 | acc: 62.50%,  total acc: 65.21%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 65.23%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 65.22%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 65.16%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 65.17%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 65.26%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 65.32%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 65.33%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 65.39%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 65.43%   [EVAL] batch:  267 | acc: 100.00%,  total acc: 65.56%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 65.66%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 65.74%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 65.87%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 65.99%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 66.12%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 66.22%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 66.34%   [EVAL] batch:  275 | acc: 68.75%,  total acc: 66.35%   [EVAL] batch:  276 | acc: 56.25%,  total acc: 66.31%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 66.28%   [EVAL] batch:  278 | acc: 62.50%,  total acc: 66.26%   [EVAL] batch:  279 | acc: 50.00%,  total acc: 66.21%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 66.13%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 66.07%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 66.06%   [EVAL] batch:  283 | acc: 18.75%,  total acc: 65.89%   [EVAL] batch:  284 | acc: 37.50%,  total acc: 65.79%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 65.62%   [EVAL] batch:  286 | acc: 25.00%,  total acc: 65.48%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 65.52%   [EVAL] batch:  288 | acc: 75.00%,  total acc: 65.55%   [EVAL] batch:  289 | acc: 81.25%,  total acc: 65.60%   [EVAL] batch:  290 | acc: 81.25%,  total acc: 65.66%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 65.71%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 65.76%   [EVAL] batch:  293 | acc: 75.00%,  total acc: 65.80%   [EVAL] batch:  294 | acc: 87.50%,  total acc: 65.87%   [EVAL] batch:  295 | acc: 81.25%,  total acc: 65.92%   [EVAL] batch:  296 | acc: 81.25%,  total acc: 65.97%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 66.00%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 66.05%   [EVAL] batch:  299 | acc: 87.50%,  total acc: 66.12%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 66.24%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 66.35%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 66.46%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 66.57%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 66.68%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 66.79%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 66.90%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 67.00%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 67.11%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 67.32%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 67.43%   [EVAL] batch:  312 | acc: 87.50%,  total acc: 67.49%   [EVAL] batch:  313 | acc: 56.25%,  total acc: 67.46%   [EVAL] batch:  314 | acc: 56.25%,  total acc: 67.42%   [EVAL] batch:  315 | acc: 75.00%,  total acc: 67.44%   [EVAL] batch:  316 | acc: 50.00%,  total acc: 67.39%   [EVAL] batch:  317 | acc: 68.75%,  total acc: 67.39%   [EVAL] batch:  318 | acc: 81.25%,  total acc: 67.44%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 67.48%   [EVAL] batch:  320 | acc: 81.25%,  total acc: 67.52%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 67.55%   [EVAL] batch:  322 | acc: 81.25%,  total acc: 67.59%   [EVAL] batch:  323 | acc: 68.75%,  total acc: 67.59%   [EVAL] batch:  324 | acc: 62.50%,  total acc: 67.58%   [EVAL] batch:  325 | acc: 25.00%,  total acc: 67.45%   [EVAL] batch:  326 | acc: 6.25%,  total acc: 67.26%   [EVAL] batch:  327 | acc: 12.50%,  total acc: 67.09%   [EVAL] batch:  328 | acc: 12.50%,  total acc: 66.93%   [EVAL] batch:  329 | acc: 25.00%,  total acc: 66.80%   [EVAL] batch:  330 | acc: 12.50%,  total acc: 66.64%   [EVAL] batch:  331 | acc: 75.00%,  total acc: 66.66%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 66.76%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 66.86%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 66.94%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 67.04%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 67.12%   [EVAL] batch:  337 | acc: 50.00%,  total acc: 67.07%   [EVAL] batch:  338 | acc: 0.00%,  total acc: 66.87%   [EVAL] batch:  339 | acc: 6.25%,  total acc: 66.69%   [EVAL] batch:  340 | acc: 18.75%,  total acc: 66.55%   [EVAL] batch:  341 | acc: 25.00%,  total acc: 66.43%   [EVAL] batch:  342 | acc: 6.25%,  total acc: 66.25%   [EVAL] batch:  343 | acc: 12.50%,  total acc: 66.10%   [EVAL] batch:  344 | acc: 56.25%,  total acc: 66.07%   [EVAL] batch:  345 | acc: 62.50%,  total acc: 66.06%   [EVAL] batch:  346 | acc: 68.75%,  total acc: 66.07%   [EVAL] batch:  347 | acc: 75.00%,  total acc: 66.09%   [EVAL] batch:  348 | acc: 87.50%,  total acc: 66.15%   [EVAL] batch:  349 | acc: 68.75%,  total acc: 66.16%   [EVAL] batch:  350 | acc: 25.00%,  total acc: 66.04%   [EVAL] batch:  351 | acc: 37.50%,  total acc: 65.96%   [EVAL] batch:  352 | acc: 37.50%,  total acc: 65.88%   [EVAL] batch:  353 | acc: 43.75%,  total acc: 65.82%   [EVAL] batch:  354 | acc: 68.75%,  total acc: 65.83%   [EVAL] batch:  355 | acc: 37.50%,  total acc: 65.75%   [EVAL] batch:  356 | acc: 81.25%,  total acc: 65.79%   [EVAL] batch:  357 | acc: 87.50%,  total acc: 65.85%   [EVAL] batch:  358 | acc: 87.50%,  total acc: 65.91%   [EVAL] batch:  359 | acc: 93.75%,  total acc: 65.99%   [EVAL] batch:  360 | acc: 75.00%,  total acc: 66.01%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 66.07%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 66.08%   [EVAL] batch:  363 | acc: 31.25%,  total acc: 65.99%   [EVAL] batch:  364 | acc: 37.50%,  total acc: 65.91%   [EVAL] batch:  365 | acc: 18.75%,  total acc: 65.78%   [EVAL] batch:  366 | acc: 37.50%,  total acc: 65.70%   [EVAL] batch:  367 | acc: 31.25%,  total acc: 65.61%   [EVAL] batch:  368 | acc: 43.75%,  total acc: 65.55%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:  370 | acc: 93.75%,  total acc: 65.70%   [EVAL] batch:  371 | acc: 81.25%,  total acc: 65.74%   [EVAL] batch:  372 | acc: 75.00%,  total acc: 65.77%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 65.81%   [EVAL] batch:  374 | acc: 87.50%,  total acc: 65.87%   [EVAL] batch:  375 | acc: 93.75%,  total acc: 65.94%   [EVAL] batch:  376 | acc: 93.75%,  total acc: 66.01%   [EVAL] batch:  377 | acc: 100.00%,  total acc: 66.10%   [EVAL] batch:  378 | acc: 87.50%,  total acc: 66.16%   [EVAL] batch:  379 | acc: 93.75%,  total acc: 66.23%   [EVAL] batch:  380 | acc: 100.00%,  total acc: 66.32%   [EVAL] batch:  381 | acc: 81.25%,  total acc: 66.36%   [EVAL] batch:  382 | acc: 81.25%,  total acc: 66.40%   [EVAL] batch:  383 | acc: 25.00%,  total acc: 66.29%   [EVAL] batch:  384 | acc: 62.50%,  total acc: 66.28%   [EVAL] batch:  385 | acc: 81.25%,  total acc: 66.32%   [EVAL] batch:  386 | acc: 50.00%,  total acc: 66.28%   [EVAL] batch:  387 | acc: 62.50%,  total acc: 66.27%   [EVAL] batch:  388 | acc: 75.00%,  total acc: 66.29%   [EVAL] batch:  389 | acc: 68.75%,  total acc: 66.30%   [EVAL] batch:  390 | acc: 62.50%,  total acc: 66.29%   [EVAL] batch:  391 | acc: 68.75%,  total acc: 66.29%   [EVAL] batch:  392 | acc: 62.50%,  total acc: 66.28%   [EVAL] batch:  393 | acc: 62.50%,  total acc: 66.28%   [EVAL] batch:  394 | acc: 68.75%,  total acc: 66.28%   [EVAL] batch:  395 | acc: 62.50%,  total acc: 66.27%   [EVAL] batch:  396 | acc: 75.00%,  total acc: 66.29%   [EVAL] batch:  397 | acc: 56.25%,  total acc: 66.27%   [EVAL] batch:  398 | acc: 56.25%,  total acc: 66.24%   [EVAL] batch:  399 | acc: 50.00%,  total acc: 66.20%   [EVAL] batch:  400 | acc: 100.00%,  total acc: 66.29%   [EVAL] batch:  401 | acc: 100.00%,  total acc: 66.37%   [EVAL] batch:  402 | acc: 100.00%,  total acc: 66.45%   [EVAL] batch:  403 | acc: 100.00%,  total acc: 66.54%   [EVAL] batch:  404 | acc: 100.00%,  total acc: 66.62%   [EVAL] batch:  405 | acc: 100.00%,  total acc: 66.70%   [EVAL] batch:  406 | acc: 75.00%,  total acc: 66.72%   [EVAL] batch:  407 | acc: 81.25%,  total acc: 66.76%   [EVAL] batch:  408 | acc: 62.50%,  total acc: 66.75%   [EVAL] batch:  409 | acc: 68.75%,  total acc: 66.75%   [EVAL] batch:  410 | acc: 81.25%,  total acc: 66.79%   [EVAL] batch:  411 | acc: 62.50%,  total acc: 66.78%   [EVAL] batch:  412 | acc: 75.00%,  total acc: 66.80%   [EVAL] batch:  413 | acc: 100.00%,  total acc: 66.88%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:  415 | acc: 100.00%,  total acc: 67.04%   [EVAL] batch:  416 | acc: 100.00%,  total acc: 67.12%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 67.26%   [EVAL] batch:  419 | acc: 100.00%,  total acc: 67.34%   [EVAL] batch:  420 | acc: 100.00%,  total acc: 67.41%   [EVAL] batch:  421 | acc: 100.00%,  total acc: 67.49%   [EVAL] batch:  422 | acc: 100.00%,  total acc: 67.57%   [EVAL] batch:  423 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:  424 | acc: 93.75%,  total acc: 67.71%   [EVAL] batch:  425 | acc: 75.00%,  total acc: 67.72%   [EVAL] batch:  426 | acc: 93.75%,  total acc: 67.78%   [EVAL] batch:  427 | acc: 93.75%,  total acc: 67.84%   [EVAL] batch:  428 | acc: 87.50%,  total acc: 67.89%   [EVAL] batch:  429 | acc: 87.50%,  total acc: 67.94%   [EVAL] batch:  430 | acc: 81.25%,  total acc: 67.97%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 68.03%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 68.07%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 68.13%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 68.20%   [EVAL] batch:  435 | acc: 100.00%,  total acc: 68.28%   [EVAL] batch:  436 | acc: 87.50%,  total acc: 68.32%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 68.28%   
cur_acc:  ['0.9534', '0.7192', '0.6845', '0.8006', '0.7639', '0.6220', '0.8264']
his_acc:  ['0.9534', '0.8195', '0.7450', '0.7320', '0.6983', '0.6760', '0.6828']
CurrentTrain: epoch 15, batch     0 | loss: 10.0488625CurrentTrain: epoch 15, batch     1 | loss: 10.6127349CurrentTrain: epoch 15, batch     2 | loss: 13.7184318CurrentTrain: epoch  1, batch     3 | loss: 11.1652266CurrentTrain: epoch 15, batch     0 | loss: 10.0533484CurrentTrain: epoch 15, batch     1 | loss: 10.2858991CurrentTrain: epoch 15, batch     2 | loss: 10.4581995CurrentTrain: epoch  1, batch     3 | loss: 9.6036337CurrentTrain: epoch 15, batch     0 | loss: 11.9426918CurrentTrain: epoch 15, batch     1 | loss: 9.4303491CurrentTrain: epoch 15, batch     2 | loss: 11.5084903CurrentTrain: epoch  1, batch     3 | loss: 9.3444426CurrentTrain: epoch 15, batch     0 | loss: 10.9538965CurrentTrain: epoch 15, batch     1 | loss: 10.7284139CurrentTrain: epoch 15, batch     2 | loss: 7.7612354CurrentTrain: epoch  1, batch     3 | loss: 8.5783192CurrentTrain: epoch 15, batch     0 | loss: 11.3371206CurrentTrain: epoch 15, batch     1 | loss: 7.1617401CurrentTrain: epoch 15, batch     2 | loss: 12.5875584CurrentTrain: epoch  1, batch     3 | loss: 6.1357538CurrentTrain: epoch 15, batch     0 | loss: 7.6027358CurrentTrain: epoch 15, batch     1 | loss: 17.8983601CurrentTrain: epoch 15, batch     2 | loss: 4.9982582CurrentTrain: epoch  1, batch     3 | loss: 6.1519000CurrentTrain: epoch 15, batch     0 | loss: 21.2953535CurrentTrain: epoch 15, batch     1 | loss: 15.8536669CurrentTrain: epoch 15, batch     2 | loss: 7.1080540CurrentTrain: epoch  1, batch     3 | loss: 6.7299872CurrentTrain: epoch 15, batch     0 | loss: 7.3571838CurrentTrain: epoch 15, batch     1 | loss: 6.6307352CurrentTrain: epoch 15, batch     2 | loss: 6.3996352CurrentTrain: epoch  1, batch     3 | loss: 6.3072164CurrentTrain: epoch 15, batch     0 | loss: 11.4985477CurrentTrain: epoch 15, batch     1 | loss: 7.1048680CurrentTrain: epoch 15, batch     2 | loss: 9.8471848CurrentTrain: epoch  1, batch     3 | loss: 5.4759145CurrentTrain: epoch 15, batch     0 | loss: 13.2257882CurrentTrain: epoch 15, batch     1 | loss: 6.5022511CurrentTrain: epoch 15, batch     2 | loss: 10.2963924CurrentTrain: epoch  1, batch     3 | loss: 5.9546421
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: one of the young authors discovered by nekrasov was leo tolstoy who debuted in " sovremennik " with his trilogy " childhood " , " boyhood " and " youth " .
Head Entity: youth
Tail Entity: boyhood
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: In the famous series of novels, the character Harry Potter first appears in "Harry Potter and the Philosopher's Stone," followed by his adventures in "Harry Potter and the Chamber of Secrets."  
Head Entity: Chamber of Secrets  
Tail Entity: Philosopher's Stone  

Relation: follows  
Context: The artist's latest album features a track titled "Eternal Sunshine," which comes right after the popular song "Summer Nights" in the playlist.  
Head Entity: Summer Nights  
Tail Entity: Eternal Sunshine  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: dick packer was a u.s. soccer center forward who was a member of the u.s. team at the 1956 summer olympics .
Head Entity: dick packer
Tail Entity: soccer
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: serena williams is a professional tennis player who has won numerous grand slam titles throughout her career.  
Head Entity: serena williams  
Tail Entity: tennis  

Relation: sport  
Context: lebron james is known for his exceptional skills in basketball and has played for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: basketball  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael was the father of lucas, who had recently graduated from university.  
Head Entity: lucas  
Tail Entity: michael  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: since 2009 , culshaw has starred in the bbc one comedy sketch show " the impressions show " alongside debra stephenson .
Head Entity: the impressions show
Tail Entity: bbc one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: The popular series "Breaking Bad" first aired on AMC, captivating audiences with its intense storytelling and character development.  
Head Entity: Breaking Bad  
Tail Entity: AMC  

Relation: original network  
Context: "Friends" became a cultural phenomenon after it premiered on NBC, showcasing the lives of six friends living in New York City.  
Head Entity: Friends  
Tail Entity: NBC  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: he was also nominated twice for the filmfare best telugu actor award , for the films bhale bhale magadivoy and " gentleman " .
Head Entity: gentleman
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: The movie "Parasite" received critical acclaim and was originally produced in Korean, showcasing the unique storytelling style of South Korean cinema.  
Head Entity: Parasite  
Tail Entity: Korean  

Relation: original language of film or TV show  
Context: The animated series "Naruto" was first aired in Japan and is primarily in Japanese, reflecting the cultural nuances of its origin.  
Head Entity: Naruto  
Tail Entity: Japanese  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: in the summer of 2013 he returned to montenegro and joined with fk rudar pljevlja , playing with the club for two seasons in the montenegrin first league .
Head Entity: fk rudar pljevlja
Tail Entity: montenegrin first league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: After a successful stint in the English Premier League, the player signed with FC Barcelona, competing in La Liga for the next three seasons.  
Head Entity: FC Barcelona  
Tail Entity: La Liga  

Relation: league  
Context: The young athlete made headlines when he joined the New York Yankees, showcasing his talent in Major League Baseball.  
Head Entity: New York Yankees  
Tail Entity: Major League Baseball  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: the first stewart king of scotland and son of marjorie bruce and walter stewart , robert ii , is believed to have been born in the abbey .
Head Entity: robert ii
Tail Entity: marjorie bruce
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: in the historical records, it is noted that queen elizabeth i was the daughter of anne boleyn, who served as her mother during her early years.  
Head Entity: queen elizabeth i  
Tail Entity: anne boleyn  

Relation: mother  
Context: the famous artist pablo picasso often spoke fondly of his mother, maría ruiz, who greatly influenced his early artistic development.  
Head Entity: pablo picasso  
Tail Entity: maría ruiz  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: his oldest son , sarod player brij narayan , was born on 25 april 1952 in udaipur , and his daughter aruna narayan was born in 1959 in mumbai .
Head Entity: brij narayan
Tail Entity: sarod
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: The renowned musician, John Smith, is known for his exceptional skills on the guitar, which he has been playing since childhood.  
Head Entity: John Smith  
Tail Entity: guitar  

Relation: instrument  
Context: During the concert, Emily Johnson captivated the audience with her beautiful melodies on the flute, showcasing her talent as a music teacher.  
Head Entity: Emily Johnson  
Tail Entity: flute  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: the type locality given is " fan - si - pan mountains , tonkin , indo - china " ( mount fansipan , sa pa district , nghệ an province , northwestern vietnam ) .
Head Entity: sa pa district
Tail Entity: vietnam
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the capital city of the country is located in the central region, known for its rich history and cultural heritage, particularly in the city of "hanoi" which is the heart of "vietnam".  
Head Entity: hanoi  
Tail Entity: vietnam  

Relation: country  
Context: the famous landmark "eiffel tower" is situated in the city of "paris", which is the capital of the beautiful country known for its art, fashion, and cuisine, "france".  
Head Entity: paris  
Tail Entity: france  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: hagrid suggests in " harry potter and the chamber of secrets " that " " they 're startin ' ter think the job 's jinxed .
Head Entity: harry potter and the chamber of secrets
Tail Entity: hagrid
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: In the animated series "Avatar: The Last Airbender," Aang, the last Airbender, teams up with Katara and Sokka to defeat the Fire Nation.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: Aang  

Relation: characters  
Context: The novel "Pride and Prejudice" features Elizabeth Bennet as she navigates societal expectations and her relationship with Mr. Darcy.  
Head Entity: Pride and Prejudice  
Tail Entity: Elizabeth Bennet  
MemoryTrain:  epoch 15, batch     0 | loss: 4.8265515MemoryTrain:  epoch 15, batch     1 | loss: 5.6386480MemoryTrain:  epoch 15, batch     2 | loss: 3.5902909MemoryTrain:  epoch 15, batch     3 | loss: 2.6931565MemoryTrain:  epoch 15, batch     4 | loss: 4.2811618MemoryTrain:  epoch 15, batch     5 | loss: 3.7887964MemoryTrain:  epoch 15, batch     6 | loss: 3.3906137MemoryTrain:  epoch 15, batch     7 | loss: 3.1308229MemoryTrain:  epoch 15, batch     8 | loss: 3.1539243MemoryTrain:  epoch 15, batch     9 | loss: 4.3845667MemoryTrain:  epoch 15, batch    10 | loss: 2.2614878MemoryTrain:  epoch 15, batch    11 | loss: 2.8704928MemoryTrain:  epoch 15, batch    12 | loss: 4.4350900MemoryTrain:  epoch 15, batch    13 | loss: 1.9936418MemoryTrain:  epoch 15, batch    14 | loss: 2.5715642MemoryTrain:  epoch 15, batch     0 | loss: 4.0221011MemoryTrain:  epoch 15, batch     1 | loss: 5.7861334MemoryTrain:  epoch 15, batch     2 | loss: 2.3954585MemoryTrain:  epoch 15, batch     3 | loss: 2.8757135MemoryTrain:  epoch 15, batch     4 | loss: 2.2511961MemoryTrain:  epoch 15, batch     5 | loss: 3.1084375MemoryTrain:  epoch 15, batch     6 | loss: 2.5836170MemoryTrain:  epoch 15, batch     7 | loss: 1.9003151MemoryTrain:  epoch 15, batch     8 | loss: 2.5330763MemoryTrain:  epoch 15, batch     9 | loss: 2.0621489MemoryTrain:  epoch 15, batch    10 | loss: 2.1961561MemoryTrain:  epoch 15, batch    11 | loss: 1.9069037MemoryTrain:  epoch 15, batch    12 | loss: 1.6093337MemoryTrain:  epoch 15, batch    13 | loss: 2.0937702MemoryTrain:  epoch 15, batch    14 | loss: 2.7935005MemoryTrain:  epoch 15, batch     0 | loss: 2.5235262MemoryTrain:  epoch 15, batch     1 | loss: 1.6581831MemoryTrain:  epoch 15, batch     2 | loss: 2.9780399MemoryTrain:  epoch 15, batch     3 | loss: 2.1722564MemoryTrain:  epoch 15, batch     4 | loss: 1.4824473MemoryTrain:  epoch 15, batch     5 | loss: 1.6162173MemoryTrain:  epoch 15, batch     6 | loss: 4.5952159MemoryTrain:  epoch 15, batch     7 | loss: 3.9123852MemoryTrain:  epoch 15, batch     8 | loss: 1.9232076MemoryTrain:  epoch 15, batch     9 | loss: 1.8004881MemoryTrain:  epoch 15, batch    10 | loss: 2.3080777MemoryTrain:  epoch 15, batch    11 | loss: 2.6059897MemoryTrain:  epoch 15, batch    12 | loss: 2.0121319MemoryTrain:  epoch 15, batch    13 | loss: 1.7752301MemoryTrain:  epoch 15, batch    14 | loss: 1.9683460MemoryTrain:  epoch 15, batch     0 | loss: 1.6323085MemoryTrain:  epoch 15, batch     1 | loss: 1.7554108MemoryTrain:  epoch 15, batch     2 | loss: 1.9059031MemoryTrain:  epoch 15, batch     3 | loss: 1.7701134MemoryTrain:  epoch 15, batch     4 | loss: 1.5600674MemoryTrain:  epoch 15, batch     5 | loss: 1.5259777MemoryTrain:  epoch 15, batch     6 | loss: 1.8675024MemoryTrain:  epoch 15, batch     7 | loss: 2.4962720MemoryTrain:  epoch 15, batch     8 | loss: 2.2354494MemoryTrain:  epoch 15, batch     9 | loss: 2.5229951MemoryTrain:  epoch 15, batch    10 | loss: 5.0053672MemoryTrain:  epoch 15, batch    11 | loss: 1.8077336MemoryTrain:  epoch 15, batch    12 | loss: 2.1375617MemoryTrain:  epoch 15, batch    13 | loss: 1.7929162MemoryTrain:  epoch 15, batch    14 | loss: 2.7370556MemoryTrain:  epoch 15, batch     0 | loss: 1.9038040MemoryTrain:  epoch 15, batch     1 | loss: 4.6752011MemoryTrain:  epoch 15, batch     2 | loss: 11.0389850MemoryTrain:  epoch 15, batch     3 | loss: 1.7441096MemoryTrain:  epoch 15, batch     4 | loss: 4.2864517MemoryTrain:  epoch 15, batch     5 | loss: 1.8716954MemoryTrain:  epoch 15, batch     6 | loss: 1.9509777MemoryTrain:  epoch 15, batch     7 | loss: 1.8372624MemoryTrain:  epoch 15, batch     8 | loss: 2.2752112MemoryTrain:  epoch 15, batch     9 | loss: 1.6656350MemoryTrain:  epoch 15, batch    10 | loss: 1.7611034MemoryTrain:  epoch 15, batch    11 | loss: 1.5091766MemoryTrain:  epoch 15, batch    12 | loss: 1.3650016MemoryTrain:  epoch 15, batch    13 | loss: 4.5403675MemoryTrain:  epoch 15, batch    14 | loss: 1.9823021MemoryTrain:  epoch 15, batch     0 | loss: 2.4197602MemoryTrain:  epoch 15, batch     1 | loss: 1.3503974MemoryTrain:  epoch 15, batch     2 | loss: 1.9483743MemoryTrain:  epoch 15, batch     3 | loss: 1.7827657MemoryTrain:  epoch 15, batch     4 | loss: 2.1577086MemoryTrain:  epoch 15, batch     5 | loss: 1.5267116MemoryTrain:  epoch 15, batch     6 | loss: 1.5288174MemoryTrain:  epoch 15, batch     7 | loss: 1.3528046MemoryTrain:  epoch 15, batch     8 | loss: 1.7678644MemoryTrain:  epoch 15, batch     9 | loss: 2.0708598MemoryTrain:  epoch 15, batch    10 | loss: 1.8219262MemoryTrain:  epoch 15, batch    11 | loss: 1.9232743MemoryTrain:  epoch 15, batch    12 | loss: 2.2539669MemoryTrain:  epoch 15, batch    13 | loss: 1.9884016MemoryTrain:  epoch 15, batch    14 | loss: 1.6499149MemoryTrain:  epoch 15, batch     0 | loss: 1.5659903MemoryTrain:  epoch 15, batch     1 | loss: 1.6864918MemoryTrain:  epoch 15, batch     2 | loss: 1.4004740MemoryTrain:  epoch 15, batch     3 | loss: 1.8495946MemoryTrain:  epoch 15, batch     4 | loss: 1.4420636MemoryTrain:  epoch 15, batch     5 | loss: 1.9600308MemoryTrain:  epoch 15, batch     6 | loss: 3.7278919MemoryTrain:  epoch 15, batch     7 | loss: 1.2810902MemoryTrain:  epoch 15, batch     8 | loss: 1.4982812MemoryTrain:  epoch 15, batch     9 | loss: 1.3496642MemoryTrain:  epoch 15, batch    10 | loss: 2.0458007MemoryTrain:  epoch 15, batch    11 | loss: 1.5227809MemoryTrain:  epoch 15, batch    12 | loss: 3.8043703MemoryTrain:  epoch 15, batch    13 | loss: 1.8126709MemoryTrain:  epoch 15, batch    14 | loss: 4.4385994MemoryTrain:  epoch 15, batch     0 | loss: 1.3479133MemoryTrain:  epoch 15, batch     1 | loss: 1.6761612MemoryTrain:  epoch 15, batch     2 | loss: 1.7474177MemoryTrain:  epoch 15, batch     3 | loss: 1.7427502MemoryTrain:  epoch 15, batch     4 | loss: 1.3627826MemoryTrain:  epoch 15, batch     5 | loss: 1.5115231MemoryTrain:  epoch 15, batch     6 | loss: 1.4015431MemoryTrain:  epoch 15, batch     7 | loss: 6.3919823MemoryTrain:  epoch 15, batch     8 | loss: 1.4715240MemoryTrain:  epoch 15, batch     9 | loss: 1.5581773MemoryTrain:  epoch 15, batch    10 | loss: 1.3752200MemoryTrain:  epoch 15, batch    11 | loss: 2.3558868MemoryTrain:  epoch 15, batch    12 | loss: 4.2465080MemoryTrain:  epoch 15, batch    13 | loss: 1.4691784MemoryTrain:  epoch 15, batch    14 | loss: 1.5704277MemoryTrain:  epoch 15, batch     0 | loss: 1.8433979MemoryTrain:  epoch 15, batch     1 | loss: 1.4854379MemoryTrain:  epoch 15, batch     2 | loss: 1.2675393MemoryTrain:  epoch 15, batch     3 | loss: 3.5551451MemoryTrain:  epoch 15, batch     4 | loss: 1.9232052MemoryTrain:  epoch 15, batch     5 | loss: 1.7080683MemoryTrain:  epoch 15, batch     6 | loss: 1.4956121MemoryTrain:  epoch 15, batch     7 | loss: 1.4097541MemoryTrain:  epoch 15, batch     8 | loss: 2.7762578MemoryTrain:  epoch 15, batch     9 | loss: 1.3839487MemoryTrain:  epoch 15, batch    10 | loss: 2.3583829MemoryTrain:  epoch 15, batch    11 | loss: 1.7194006MemoryTrain:  epoch 15, batch    12 | loss: 1.4399505MemoryTrain:  epoch 15, batch    13 | loss: 3.7046734MemoryTrain:  epoch 15, batch    14 | loss: 1.5895531MemoryTrain:  epoch 15, batch     0 | loss: 1.8359701MemoryTrain:  epoch 15, batch     1 | loss: 3.7355826MemoryTrain:  epoch 15, batch     2 | loss: 1.5351060MemoryTrain:  epoch 15, batch     3 | loss: 1.4005006MemoryTrain:  epoch 15, batch     4 | loss: 1.2683068MemoryTrain:  epoch 15, batch     5 | loss: 1.3660127MemoryTrain:  epoch 15, batch     6 | loss: 1.4029508MemoryTrain:  epoch 15, batch     7 | loss: 3.7412281MemoryTrain:  epoch 15, batch     8 | loss: 1.5308420MemoryTrain:  epoch 15, batch     9 | loss: 2.2653949MemoryTrain:  epoch 15, batch    10 | loss: 1.4528697MemoryTrain:  epoch 15, batch    11 | loss: 1.6610064MemoryTrain:  epoch 15, batch    12 | loss: 1.8853600MemoryTrain:  epoch 15, batch    13 | loss: 1.2847596MemoryTrain:  epoch 15, batch    14 | loss: 1.3581479
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 25.00%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 26.56%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 22.50%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 23.96%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 28.57%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 35.94%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 42.36%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 46.88%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 50.00%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 52.08%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 53.85%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 52.68%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 53.33%   [EVAL] batch:   15 | acc: 31.25%,  total acc: 51.95%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 52.21%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 53.12%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 53.29%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 55.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 57.74%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 59.38%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 61.14%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 64.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 65.14%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 66.20%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 66.96%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 67.67%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 68.54%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 69.35%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 70.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 71.02%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 72.68%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 73.26%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 73.99%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 74.18%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 74.04%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 73.59%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 72.87%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 72.17%   [EVAL] batch:   42 | acc: 43.75%,  total acc: 71.51%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 71.45%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 71.94%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 72.28%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 72.74%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 73.31%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 73.21%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 73.62%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 73.41%   [EVAL] batch:   51 | acc: 37.50%,  total acc: 72.72%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 72.29%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 71.76%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 71.70%   [EVAL] batch:   55 | acc: 25.00%,  total acc: 70.87%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 70.72%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 70.58%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 70.34%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 70.10%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 69.98%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 70.26%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 69.44%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 69.64%   [EVAL] batch:    7 | acc: 31.25%,  total acc: 64.84%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 63.19%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 60.23%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 58.33%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 58.17%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 60.27%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 62.08%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 63.67%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 65.44%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 67.01%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 68.42%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 68.12%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 68.15%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 67.90%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 67.39%   [EVAL] batch:   23 | acc: 50.00%,  total acc: 66.67%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 66.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 68.03%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 69.21%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 71.12%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 72.08%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 72.78%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 73.44%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 74.24%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 74.82%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 75.18%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 75.52%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 76.18%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 76.64%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 77.24%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 77.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 78.35%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 78.72%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 79.07%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 79.26%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 79.03%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 78.94%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 78.32%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 77.86%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 77.81%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 77.00%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 76.96%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 77.04%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 76.89%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 76.62%   [EVAL] batch:   54 | acc: 43.75%,  total acc: 76.02%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 75.89%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 75.55%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 74.58%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 74.58%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 74.69%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 74.60%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 73.81%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 72.75%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 71.83%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 70.74%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 69.68%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 68.84%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 68.21%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 68.30%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 68.57%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 69.18%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 69.17%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 69.17%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 69.33%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 69.32%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 69.47%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 69.70%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 69.53%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 69.52%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 68.98%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 68.45%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 67.93%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 67.87%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 67.51%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 67.10%   [EVAL] batch:   87 | acc: 6.25%,  total acc: 66.41%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 65.66%   [EVAL] batch:   89 | acc: 12.50%,  total acc: 65.07%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 64.35%   [EVAL] batch:   91 | acc: 12.50%,  total acc: 63.79%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 63.17%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 62.70%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 62.89%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 63.15%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 63.34%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 63.65%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 63.89%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 63.88%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 64.17%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 64.40%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 64.62%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 64.90%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 65.12%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 65.45%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 65.30%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 64.81%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 64.39%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 63.86%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 63.40%   [EVAL] batch:  111 | acc: 12.50%,  total acc: 62.95%   [EVAL] batch:  112 | acc: 25.00%,  total acc: 62.61%   [EVAL] batch:  113 | acc: 43.75%,  total acc: 62.45%   [EVAL] batch:  114 | acc: 37.50%,  total acc: 62.23%   [EVAL] batch:  115 | acc: 50.00%,  total acc: 62.12%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 62.13%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 62.13%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 62.08%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 61.98%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 61.88%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 61.78%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 61.79%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 61.84%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 61.85%   [EVAL] batch:  125 | acc: 43.75%,  total acc: 61.71%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 61.52%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 61.33%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 61.05%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 61.06%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 60.88%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 61.08%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 61.37%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 61.57%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 61.67%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 61.90%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 62.18%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 62.27%   [EVAL] batch:  138 | acc: 31.25%,  total acc: 62.05%   [EVAL] batch:  139 | acc: 43.75%,  total acc: 61.92%   [EVAL] batch:  140 | acc: 50.00%,  total acc: 61.84%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 61.88%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 61.71%   [EVAL] batch:  143 | acc: 56.25%,  total acc: 61.68%   [EVAL] batch:  144 | acc: 31.25%,  total acc: 61.47%   [EVAL] batch:  145 | acc: 37.50%,  total acc: 61.30%   [EVAL] batch:  146 | acc: 25.00%,  total acc: 61.05%   [EVAL] batch:  147 | acc: 18.75%,  total acc: 60.77%   [EVAL] batch:  148 | acc: 31.25%,  total acc: 60.57%   [EVAL] batch:  149 | acc: 31.25%,  total acc: 60.38%   [EVAL] batch:  150 | acc: 37.50%,  total acc: 60.22%   [EVAL] batch:  151 | acc: 25.00%,  total acc: 59.99%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 59.72%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 59.58%   [EVAL] batch:  154 | acc: 62.50%,  total acc: 59.60%   [EVAL] batch:  155 | acc: 37.50%,  total acc: 59.46%   [EVAL] batch:  156 | acc: 43.75%,  total acc: 59.36%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 59.30%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 59.24%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 59.26%   [EVAL] batch:  160 | acc: 68.75%,  total acc: 59.32%   [EVAL] batch:  161 | acc: 50.00%,  total acc: 59.26%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 59.20%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 59.22%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 59.24%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 59.26%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 59.32%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 59.34%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 59.36%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 59.23%   [EVAL] batch:  170 | acc: 25.00%,  total acc: 59.03%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 58.79%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 58.71%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 58.51%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 58.39%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 58.63%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 58.86%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 59.09%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 59.32%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 59.55%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 59.77%   [EVAL] batch:  181 | acc: 93.75%,  total acc: 59.96%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 59.94%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 60.12%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 60.30%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 60.38%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 60.46%   [EVAL] batch:  187 | acc: 75.00%,  total acc: 60.54%   [EVAL] batch:  188 | acc: 75.00%,  total acc: 60.62%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 60.79%   [EVAL] batch:  190 | acc: 81.25%,  total acc: 60.90%   [EVAL] batch:  191 | acc: 75.00%,  total acc: 60.97%   [EVAL] batch:  192 | acc: 87.50%,  total acc: 61.11%   [EVAL] batch:  193 | acc: 93.75%,  total acc: 61.28%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 61.35%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 61.42%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 61.42%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 61.55%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 61.59%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 61.69%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 61.78%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 61.88%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 61.92%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 62.01%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 62.10%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 62.26%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 62.11%   [EVAL] batch:  207 | acc: 18.75%,  total acc: 61.90%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 61.72%   [EVAL] batch:  209 | acc: 6.25%,  total acc: 61.46%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 61.29%   [EVAL] batch:  211 | acc: 37.50%,  total acc: 61.17%   [EVAL] batch:  212 | acc: 43.75%,  total acc: 61.09%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 61.24%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 61.42%   [EVAL] batch:  215 | acc: 75.00%,  total acc: 61.49%   [EVAL] batch:  216 | acc: 93.75%,  total acc: 61.64%   [EVAL] batch:  217 | acc: 75.00%,  total acc: 61.70%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 61.82%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 61.99%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 62.16%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 62.33%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 62.67%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 62.83%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 63.00%   [EVAL] batch:  226 | acc: 93.75%,  total acc: 63.13%   [EVAL] batch:  227 | acc: 93.75%,  total acc: 63.27%   [EVAL] batch:  228 | acc: 93.75%,  total acc: 63.40%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 63.56%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 63.69%   [EVAL] batch:  231 | acc: 50.00%,  total acc: 63.63%   [EVAL] batch:  232 | acc: 25.00%,  total acc: 63.47%   [EVAL] batch:  233 | acc: 31.25%,  total acc: 63.33%   [EVAL] batch:  234 | acc: 31.25%,  total acc: 63.19%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 63.19%   [EVAL] batch:  236 | acc: 31.25%,  total acc: 63.05%   [EVAL] batch:  237 | acc: 56.25%,  total acc: 63.03%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 63.10%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 63.26%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 63.36%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 63.46%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 63.58%   [EVAL] batch:  243 | acc: 75.00%,  total acc: 63.63%   [EVAL] batch:  244 | acc: 6.25%,  total acc: 63.39%   [EVAL] batch:  245 | acc: 18.75%,  total acc: 63.21%   [EVAL] batch:  246 | acc: 6.25%,  total acc: 62.98%   [EVAL] batch:  247 | acc: 6.25%,  total acc: 62.75%   [EVAL] batch:  248 | acc: 0.00%,  total acc: 62.50%   [EVAL] batch:  249 | acc: 12.50%,  total acc: 62.30%   [EVAL] batch:  250 | acc: 31.25%,  total acc: 62.18%   [EVAL] batch:  251 | acc: 43.75%,  total acc: 62.10%   [EVAL] batch:  252 | acc: 62.50%,  total acc: 62.10%   [EVAL] batch:  253 | acc: 43.75%,  total acc: 62.03%   [EVAL] batch:  254 | acc: 43.75%,  total acc: 61.96%   [EVAL] batch:  255 | acc: 62.50%,  total acc: 61.96%   [EVAL] batch:  256 | acc: 43.75%,  total acc: 61.89%   [EVAL] batch:  257 | acc: 62.50%,  total acc: 61.89%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 61.92%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 61.90%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 61.85%   [EVAL] batch:  261 | acc: 62.50%,  total acc: 61.86%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 61.93%   [EVAL] batch:  263 | acc: 68.75%,  total acc: 61.96%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 62.00%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 62.05%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 62.10%   [EVAL] batch:  267 | acc: 100.00%,  total acc: 62.24%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 62.36%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 62.45%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 62.59%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 62.73%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 62.84%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 62.96%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 63.09%   [EVAL] batch:  275 | acc: 68.75%,  total acc: 63.11%   [EVAL] batch:  276 | acc: 56.25%,  total acc: 63.09%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 63.06%   [EVAL] batch:  278 | acc: 62.50%,  total acc: 63.06%   [EVAL] batch:  279 | acc: 62.50%,  total acc: 63.06%   [EVAL] batch:  280 | acc: 56.25%,  total acc: 63.03%   [EVAL] batch:  281 | acc: 31.25%,  total acc: 62.92%   [EVAL] batch:  282 | acc: 18.75%,  total acc: 62.77%   [EVAL] batch:  283 | acc: 31.25%,  total acc: 62.65%   [EVAL] batch:  284 | acc: 12.50%,  total acc: 62.48%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 62.33%   [EVAL] batch:  286 | acc: 18.75%,  total acc: 62.17%   [EVAL] batch:  287 | acc: 62.50%,  total acc: 62.17%   [EVAL] batch:  288 | acc: 75.00%,  total acc: 62.22%   [EVAL] batch:  289 | acc: 75.00%,  total acc: 62.26%   [EVAL] batch:  290 | acc: 81.25%,  total acc: 62.33%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 62.41%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 62.48%   [EVAL] batch:  293 | acc: 75.00%,  total acc: 62.52%   [EVAL] batch:  294 | acc: 87.50%,  total acc: 62.61%   [EVAL] batch:  295 | acc: 75.00%,  total acc: 62.65%   [EVAL] batch:  296 | acc: 62.50%,  total acc: 62.65%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 62.63%   [EVAL] batch:  298 | acc: 75.00%,  total acc: 62.67%   [EVAL] batch:  299 | acc: 87.50%,  total acc: 62.75%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 62.87%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 63.00%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 63.12%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 63.24%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 63.36%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 63.48%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 63.58%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 63.66%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 63.73%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 63.83%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 63.91%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 64.02%   [EVAL] batch:  312 | acc: 87.50%,  total acc: 64.10%   [EVAL] batch:  313 | acc: 56.25%,  total acc: 64.07%   [EVAL] batch:  314 | acc: 43.75%,  total acc: 64.01%   [EVAL] batch:  315 | acc: 68.75%,  total acc: 64.02%   [EVAL] batch:  316 | acc: 43.75%,  total acc: 63.96%   [EVAL] batch:  317 | acc: 56.25%,  total acc: 63.93%   [EVAL] batch:  318 | acc: 75.00%,  total acc: 63.97%   [EVAL] batch:  319 | acc: 62.50%,  total acc: 63.96%   [EVAL] batch:  320 | acc: 62.50%,  total acc: 63.96%   [EVAL] batch:  321 | acc: 62.50%,  total acc: 63.96%   [EVAL] batch:  322 | acc: 81.25%,  total acc: 64.01%   [EVAL] batch:  323 | acc: 62.50%,  total acc: 64.00%   [EVAL] batch:  324 | acc: 50.00%,  total acc: 63.96%   [EVAL] batch:  325 | acc: 18.75%,  total acc: 63.82%   [EVAL] batch:  326 | acc: 6.25%,  total acc: 63.65%   [EVAL] batch:  327 | acc: 6.25%,  total acc: 63.47%   [EVAL] batch:  328 | acc: 6.25%,  total acc: 63.30%   [EVAL] batch:  329 | acc: 12.50%,  total acc: 63.14%   [EVAL] batch:  330 | acc: 6.25%,  total acc: 62.97%   [EVAL] batch:  331 | acc: 68.75%,  total acc: 62.99%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 63.10%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 63.21%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 63.30%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 63.41%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 63.50%   [EVAL] batch:  337 | acc: 50.00%,  total acc: 63.46%   [EVAL] batch:  338 | acc: 0.00%,  total acc: 63.27%   [EVAL] batch:  339 | acc: 6.25%,  total acc: 63.11%   [EVAL] batch:  340 | acc: 18.75%,  total acc: 62.98%   [EVAL] batch:  341 | acc: 18.75%,  total acc: 62.85%   [EVAL] batch:  342 | acc: 6.25%,  total acc: 62.68%   [EVAL] batch:  343 | acc: 12.50%,  total acc: 62.54%   [EVAL] batch:  344 | acc: 56.25%,  total acc: 62.52%   [EVAL] batch:  345 | acc: 62.50%,  total acc: 62.52%   [EVAL] batch:  346 | acc: 68.75%,  total acc: 62.54%   [EVAL] batch:  347 | acc: 68.75%,  total acc: 62.55%   [EVAL] batch:  348 | acc: 87.50%,  total acc: 62.63%   [EVAL] batch:  349 | acc: 68.75%,  total acc: 62.64%   [EVAL] batch:  350 | acc: 25.00%,  total acc: 62.54%   [EVAL] batch:  351 | acc: 37.50%,  total acc: 62.46%   [EVAL] batch:  352 | acc: 50.00%,  total acc: 62.43%   [EVAL] batch:  353 | acc: 31.25%,  total acc: 62.34%   [EVAL] batch:  354 | acc: 68.75%,  total acc: 62.36%   [EVAL] batch:  355 | acc: 43.75%,  total acc: 62.31%   [EVAL] batch:  356 | acc: 56.25%,  total acc: 62.29%   [EVAL] batch:  357 | acc: 56.25%,  total acc: 62.27%   [EVAL] batch:  358 | acc: 31.25%,  total acc: 62.19%   [EVAL] batch:  359 | acc: 43.75%,  total acc: 62.14%   [EVAL] batch:  360 | acc: 37.50%,  total acc: 62.07%   [EVAL] batch:  361 | acc: 37.50%,  total acc: 62.00%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 62.00%   [EVAL] batch:  363 | acc: 37.50%,  total acc: 61.93%   [EVAL] batch:  364 | acc: 43.75%,  total acc: 61.88%   [EVAL] batch:  365 | acc: 37.50%,  total acc: 61.82%   [EVAL] batch:  366 | acc: 43.75%,  total acc: 61.77%   [EVAL] batch:  367 | acc: 37.50%,  total acc: 61.70%   [EVAL] batch:  368 | acc: 43.75%,  total acc: 61.65%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 61.74%   [EVAL] batch:  370 | acc: 87.50%,  total acc: 61.81%   [EVAL] batch:  371 | acc: 87.50%,  total acc: 61.88%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 61.95%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 62.00%   [EVAL] batch:  374 | acc: 87.50%,  total acc: 62.07%   [EVAL] batch:  375 | acc: 100.00%,  total acc: 62.17%   [EVAL] batch:  376 | acc: 93.75%,  total acc: 62.25%   [EVAL] batch:  377 | acc: 100.00%,  total acc: 62.35%   [EVAL] batch:  378 | acc: 87.50%,  total acc: 62.42%   [EVAL] batch:  379 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:  380 | acc: 100.00%,  total acc: 62.60%   [EVAL] batch:  381 | acc: 75.00%,  total acc: 62.63%   [EVAL] batch:  382 | acc: 75.00%,  total acc: 62.66%   [EVAL] batch:  383 | acc: 37.50%,  total acc: 62.60%   [EVAL] batch:  384 | acc: 50.00%,  total acc: 62.56%   [EVAL] batch:  385 | acc: 75.00%,  total acc: 62.60%   [EVAL] batch:  386 | acc: 50.00%,  total acc: 62.56%   [EVAL] batch:  387 | acc: 50.00%,  total acc: 62.53%   [EVAL] batch:  388 | acc: 68.75%,  total acc: 62.55%   [EVAL] batch:  389 | acc: 68.75%,  total acc: 62.56%   [EVAL] batch:  390 | acc: 56.25%,  total acc: 62.55%   [EVAL] batch:  391 | acc: 68.75%,  total acc: 62.56%   [EVAL] batch:  392 | acc: 56.25%,  total acc: 62.55%   [EVAL] batch:  393 | acc: 62.50%,  total acc: 62.55%   [EVAL] batch:  394 | acc: 56.25%,  total acc: 62.53%   [EVAL] batch:  395 | acc: 75.00%,  total acc: 62.56%   [EVAL] batch:  396 | acc: 75.00%,  total acc: 62.59%   [EVAL] batch:  397 | acc: 50.00%,  total acc: 62.56%   [EVAL] batch:  398 | acc: 68.75%,  total acc: 62.58%   [EVAL] batch:  399 | acc: 50.00%,  total acc: 62.55%   [EVAL] batch:  400 | acc: 100.00%,  total acc: 62.64%   [EVAL] batch:  401 | acc: 100.00%,  total acc: 62.73%   [EVAL] batch:  402 | acc: 100.00%,  total acc: 62.83%   [EVAL] batch:  403 | acc: 100.00%,  total acc: 62.92%   [EVAL] batch:  404 | acc: 100.00%,  total acc: 63.01%   [EVAL] batch:  405 | acc: 100.00%,  total acc: 63.10%   [EVAL] batch:  406 | acc: 75.00%,  total acc: 63.13%   [EVAL] batch:  407 | acc: 81.25%,  total acc: 63.17%   [EVAL] batch:  408 | acc: 62.50%,  total acc: 63.17%   [EVAL] batch:  409 | acc: 68.75%,  total acc: 63.19%   [EVAL] batch:  410 | acc: 81.25%,  total acc: 63.23%   [EVAL] batch:  411 | acc: 62.50%,  total acc: 63.23%   [EVAL] batch:  412 | acc: 68.75%,  total acc: 63.24%   [EVAL] batch:  413 | acc: 100.00%,  total acc: 63.33%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 63.42%   [EVAL] batch:  415 | acc: 100.00%,  total acc: 63.51%   [EVAL] batch:  416 | acc: 100.00%,  total acc: 63.59%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 63.68%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 63.75%   [EVAL] batch:  419 | acc: 100.00%,  total acc: 63.84%   [EVAL] batch:  420 | acc: 100.00%,  total acc: 63.93%   [EVAL] batch:  421 | acc: 100.00%,  total acc: 64.01%   [EVAL] batch:  422 | acc: 100.00%,  total acc: 64.10%   [EVAL] batch:  423 | acc: 100.00%,  total acc: 64.18%   [EVAL] batch:  424 | acc: 100.00%,  total acc: 64.26%   [EVAL] batch:  425 | acc: 75.00%,  total acc: 64.29%   [EVAL] batch:  426 | acc: 93.75%,  total acc: 64.36%   [EVAL] batch:  427 | acc: 93.75%,  total acc: 64.43%   [EVAL] batch:  428 | acc: 87.50%,  total acc: 64.48%   [EVAL] batch:  429 | acc: 87.50%,  total acc: 64.53%   [EVAL] batch:  430 | acc: 93.75%,  total acc: 64.60%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 64.67%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 64.72%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 64.79%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 64.87%   [EVAL] batch:  435 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:  436 | acc: 87.50%,  total acc: 65.00%   [EVAL] batch:  437 | acc: 75.00%,  total acc: 65.03%   [EVAL] batch:  438 | acc: 12.50%,  total acc: 64.91%   [EVAL] batch:  439 | acc: 31.25%,  total acc: 64.83%   [EVAL] batch:  440 | acc: 25.00%,  total acc: 64.74%   [EVAL] batch:  441 | acc: 18.75%,  total acc: 64.64%   [EVAL] batch:  442 | acc: 31.25%,  total acc: 64.56%   [EVAL] batch:  443 | acc: 18.75%,  total acc: 64.46%   [EVAL] batch:  444 | acc: 81.25%,  total acc: 64.49%   [EVAL] batch:  445 | acc: 93.75%,  total acc: 64.56%   [EVAL] batch:  446 | acc: 87.50%,  total acc: 64.61%   [EVAL] batch:  447 | acc: 81.25%,  total acc: 64.65%   [EVAL] batch:  448 | acc: 81.25%,  total acc: 64.69%   [EVAL] batch:  449 | acc: 81.25%,  total acc: 64.72%   [EVAL] batch:  450 | acc: 62.50%,  total acc: 64.72%   [EVAL] batch:  451 | acc: 37.50%,  total acc: 64.66%   [EVAL] batch:  452 | acc: 37.50%,  total acc: 64.60%   [EVAL] batch:  453 | acc: 50.00%,  total acc: 64.56%   [EVAL] batch:  454 | acc: 68.75%,  total acc: 64.57%   [EVAL] batch:  455 | acc: 43.75%,  total acc: 64.53%   [EVAL] batch:  456 | acc: 93.75%,  total acc: 64.59%   [EVAL] batch:  457 | acc: 100.00%,  total acc: 64.67%   [EVAL] batch:  458 | acc: 100.00%,  total acc: 64.75%   [EVAL] batch:  459 | acc: 93.75%,  total acc: 64.81%   [EVAL] batch:  460 | acc: 93.75%,  total acc: 64.87%   [EVAL] batch:  461 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:  462 | acc: 100.00%,  total acc: 65.02%   [EVAL] batch:  463 | acc: 87.50%,  total acc: 65.07%   [EVAL] batch:  464 | acc: 87.50%,  total acc: 65.12%   [EVAL] batch:  465 | acc: 93.75%,  total acc: 65.18%   [EVAL] batch:  466 | acc: 87.50%,  total acc: 65.23%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 65.30%   [EVAL] batch:  468 | acc: 87.50%,  total acc: 65.35%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 65.43%   [EVAL] batch:  470 | acc: 100.00%,  total acc: 65.50%   [EVAL] batch:  471 | acc: 100.00%,  total acc: 65.57%   [EVAL] batch:  472 | acc: 93.75%,  total acc: 65.63%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 65.70%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 65.78%   [EVAL] batch:  475 | acc: 62.50%,  total acc: 65.77%   [EVAL] batch:  476 | acc: 62.50%,  total acc: 65.76%   [EVAL] batch:  477 | acc: 56.25%,  total acc: 65.74%   [EVAL] batch:  478 | acc: 43.75%,  total acc: 65.70%   [EVAL] batch:  479 | acc: 37.50%,  total acc: 65.64%   [EVAL] batch:  480 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:  481 | acc: 87.50%,  total acc: 65.66%   [EVAL] batch:  482 | acc: 87.50%,  total acc: 65.71%   [EVAL] batch:  483 | acc: 87.50%,  total acc: 65.75%   [EVAL] batch:  484 | acc: 100.00%,  total acc: 65.82%   [EVAL] batch:  485 | acc: 81.25%,  total acc: 65.86%   [EVAL] batch:  486 | acc: 81.25%,  total acc: 65.89%   [EVAL] batch:  487 | acc: 68.75%,  total acc: 65.89%   [EVAL] batch:  488 | acc: 68.75%,  total acc: 65.90%   [EVAL] batch:  489 | acc: 37.50%,  total acc: 65.84%   [EVAL] batch:  490 | acc: 43.75%,  total acc: 65.80%   [EVAL] batch:  491 | acc: 56.25%,  total acc: 65.78%   [EVAL] batch:  492 | acc: 50.00%,  total acc: 65.75%   [EVAL] batch:  493 | acc: 37.50%,  total acc: 65.69%   [EVAL] batch:  494 | acc: 81.25%,  total acc: 65.72%   [EVAL] batch:  495 | acc: 56.25%,  total acc: 65.70%   [EVAL] batch:  496 | acc: 43.75%,  total acc: 65.66%   [EVAL] batch:  497 | acc: 62.50%,  total acc: 65.65%   [EVAL] batch:  498 | acc: 81.25%,  total acc: 65.68%   [EVAL] batch:  499 | acc: 56.25%,  total acc: 65.66%   
cur_acc:  ['0.9534', '0.7192', '0.6845', '0.8006', '0.7639', '0.6220', '0.8264', '0.6944']
his_acc:  ['0.9534', '0.8195', '0.7450', '0.7320', '0.6983', '0.6760', '0.6828', '0.6566']
--------Round  2
seed:  300
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
CurrentTrain: epoch 15, batch     0 | loss: 22.9614546CurrentTrain: epoch 15, batch     1 | loss: 36.1960989CurrentTrain: epoch 15, batch     2 | loss: 25.9680181CurrentTrain: epoch 15, batch     3 | loss: 27.9078000CurrentTrain: epoch 15, batch     4 | loss: 25.4967186CurrentTrain: epoch 15, batch     5 | loss: 21.5596675CurrentTrain: epoch 15, batch     6 | loss: 34.4419760CurrentTrain: epoch 15, batch     7 | loss: 23.1268192CurrentTrain: epoch 15, batch     8 | loss: 22.2387505CurrentTrain: epoch 15, batch     9 | loss: 28.2080306CurrentTrain: epoch 15, batch    10 | loss: 22.1975564CurrentTrain: epoch 15, batch    11 | loss: 29.6385172CurrentTrain: epoch 15, batch    12 | loss: 23.5472917CurrentTrain: epoch 15, batch    13 | loss: 18.3661761CurrentTrain: epoch 15, batch    14 | loss: 21.8848369CurrentTrain: epoch 15, batch    15 | loss: 16.7701988CurrentTrain: epoch 15, batch    16 | loss: 15.5103970CurrentTrain: epoch 15, batch    17 | loss: 18.4105237CurrentTrain: epoch 15, batch    18 | loss: 20.0223302CurrentTrain: epoch 15, batch    19 | loss: 15.0792080CurrentTrain: epoch 15, batch    20 | loss: 19.1274169CurrentTrain: epoch 15, batch    21 | loss: 14.1284184CurrentTrain: epoch 15, batch    22 | loss: 17.0088118CurrentTrain: epoch 15, batch    23 | loss: 11.8737792CurrentTrain: epoch 15, batch    24 | loss: 12.6129693CurrentTrain: epoch 15, batch    25 | loss: 18.3804921CurrentTrain: epoch 15, batch    26 | loss: 17.4131962CurrentTrain: epoch 15, batch    27 | loss: 11.8827558CurrentTrain: epoch 15, batch    28 | loss: 17.3858547CurrentTrain: epoch 15, batch    29 | loss: 17.1058851CurrentTrain: epoch 15, batch    30 | loss: 14.8557704CurrentTrain: epoch 15, batch    31 | loss: 16.7048387CurrentTrain: epoch 15, batch    32 | loss: 20.8984557CurrentTrain: epoch 15, batch    33 | loss: 20.1804340CurrentTrain: epoch 15, batch    34 | loss: 17.9281810CurrentTrain: epoch 15, batch    35 | loss: 17.0869536CurrentTrain: epoch 15, batch    36 | loss: 15.6326224CurrentTrain: epoch 15, batch    37 | loss: 18.0998679CurrentTrain: epoch 15, batch    38 | loss: 20.4967907CurrentTrain: epoch 15, batch    39 | loss: 16.9198378CurrentTrain: epoch 15, batch    40 | loss: 17.8487648CurrentTrain: epoch 15, batch    41 | loss: 19.9530922CurrentTrain: epoch 15, batch    42 | loss: 25.9741255CurrentTrain: epoch 15, batch    43 | loss: 18.3302216CurrentTrain: epoch 15, batch    44 | loss: 15.0411878CurrentTrain: epoch 15, batch    45 | loss: 13.1022118CurrentTrain: epoch 15, batch    46 | loss: 13.6130565CurrentTrain: epoch 15, batch    47 | loss: 15.3356510CurrentTrain: epoch 15, batch    48 | loss: 17.3994976CurrentTrain: epoch 15, batch    49 | loss: 14.2991403CurrentTrain: epoch 15, batch    50 | loss: 27.3215461CurrentTrain: epoch 15, batch    51 | loss: 15.7080802CurrentTrain: epoch 15, batch    52 | loss: 17.9424243CurrentTrain: epoch 15, batch    53 | loss: 13.4864267CurrentTrain: epoch 15, batch    54 | loss: 23.4081381CurrentTrain: epoch 15, batch    55 | loss: 14.2371714CurrentTrain: epoch 15, batch    56 | loss: 12.8098000CurrentTrain: epoch 15, batch    57 | loss: 19.1042257CurrentTrain: epoch 15, batch    58 | loss: 20.6915510CurrentTrain: epoch 15, batch    59 | loss: 17.8173407CurrentTrain: epoch 15, batch    60 | loss: 14.7069067CurrentTrain: epoch 15, batch    61 | loss: 13.8610084CurrentTrain: epoch  7, batch    62 | loss: 20.3953688CurrentTrain: epoch 15, batch     0 | loss: 15.5781648CurrentTrain: epoch 15, batch     1 | loss: 13.9962863CurrentTrain: epoch 15, batch     2 | loss: 11.8967318CurrentTrain: epoch 15, batch     3 | loss: 13.3981119CurrentTrain: epoch 15, batch     4 | loss: 12.0516201CurrentTrain: epoch 15, batch     5 | loss: 16.6828324CurrentTrain: epoch 15, batch     6 | loss: 21.9424586CurrentTrain: epoch 15, batch     7 | loss: 15.1431985CurrentTrain: epoch 15, batch     8 | loss: 13.1832234CurrentTrain: epoch 15, batch     9 | loss: 16.9857130CurrentTrain: epoch 15, batch    10 | loss: 20.4789214CurrentTrain: epoch 15, batch    11 | loss: 13.6289285CurrentTrain: epoch 15, batch    12 | loss: 16.3228560CurrentTrain: epoch 15, batch    13 | loss: 14.4694632CurrentTrain: epoch 15, batch    14 | loss: 14.7613359CurrentTrain: epoch 15, batch    15 | loss: 13.4703938CurrentTrain: epoch 15, batch    16 | loss: 16.8518475CurrentTrain: epoch 15, batch    17 | loss: 21.7699750CurrentTrain: epoch 15, batch    18 | loss: 14.7667208CurrentTrain: epoch 15, batch    19 | loss: 16.8910381CurrentTrain: epoch 15, batch    20 | loss: 13.1815378CurrentTrain: epoch 15, batch    21 | loss: 28.2282415CurrentTrain: epoch 15, batch    22 | loss: 15.2401040CurrentTrain: epoch 15, batch    23 | loss: 17.2409228CurrentTrain: epoch 15, batch    24 | loss: 10.6165015CurrentTrain: epoch 15, batch    25 | loss: 15.0962402CurrentTrain: epoch 15, batch    26 | loss: 13.3579354CurrentTrain: epoch 15, batch    27 | loss: 19.3898874CurrentTrain: epoch 15, batch    28 | loss: 12.4191342CurrentTrain: epoch 15, batch    29 | loss: 20.5688574CurrentTrain: epoch 15, batch    30 | loss: 13.9622811CurrentTrain: epoch 15, batch    31 | loss: 11.6815828CurrentTrain: epoch 15, batch    32 | loss: 18.5004248CurrentTrain: epoch 15, batch    33 | loss: 12.2183298CurrentTrain: epoch 15, batch    34 | loss: 12.6518017CurrentTrain: epoch 15, batch    35 | loss: 13.4217389CurrentTrain: epoch 15, batch    36 | loss: 13.6867036CurrentTrain: epoch 15, batch    37 | loss: 20.4623017CurrentTrain: epoch 15, batch    38 | loss: 15.2809601CurrentTrain: epoch 15, batch    39 | loss: 11.1572282CurrentTrain: epoch 15, batch    40 | loss: 16.3167428CurrentTrain: epoch 15, batch    41 | loss: 22.1181912CurrentTrain: epoch 15, batch    42 | loss: 12.4902631CurrentTrain: epoch 15, batch    43 | loss: 10.7042274CurrentTrain: epoch 15, batch    44 | loss: 12.3562143CurrentTrain: epoch 15, batch    45 | loss: 13.6572934CurrentTrain: epoch 15, batch    46 | loss: 12.6667848CurrentTrain: epoch 15, batch    47 | loss: 18.9329851CurrentTrain: epoch 15, batch    48 | loss: 19.7600152CurrentTrain: epoch 15, batch    49 | loss: 11.6994731CurrentTrain: epoch 15, batch    50 | loss: 15.9088409CurrentTrain: epoch 15, batch    51 | loss: 12.6369243CurrentTrain: epoch 15, batch    52 | loss: 10.7384555CurrentTrain: epoch 15, batch    53 | loss: 12.0066702CurrentTrain: epoch 15, batch    54 | loss: 11.4534957CurrentTrain: epoch 15, batch    55 | loss: 17.0674285CurrentTrain: epoch 15, batch    56 | loss: 11.1322001CurrentTrain: epoch 15, batch    57 | loss: 12.1562491CurrentTrain: epoch 15, batch    58 | loss: 14.8808368CurrentTrain: epoch 15, batch    59 | loss: 15.4510498CurrentTrain: epoch 15, batch    60 | loss: 18.5881173CurrentTrain: epoch 15, batch    61 | loss: 11.4689487CurrentTrain: epoch  7, batch    62 | loss: 19.4057969CurrentTrain: epoch 15, batch     0 | loss: 8.2703075CurrentTrain: epoch 15, batch     1 | loss: 25.0590923CurrentTrain: epoch 15, batch     2 | loss: 11.8152014CurrentTrain: epoch 15, batch     3 | loss: 12.4528489CurrentTrain: epoch 15, batch     4 | loss: 17.8954975CurrentTrain: epoch 15, batch     5 | loss: 12.8894756CurrentTrain: epoch 15, batch     6 | loss: 15.8158140CurrentTrain: epoch 15, batch     7 | loss: 17.0845331CurrentTrain: epoch 15, batch     8 | loss: 12.4690435CurrentTrain: epoch 15, batch     9 | loss: 18.5015340CurrentTrain: epoch 15, batch    10 | loss: 10.2998629CurrentTrain: epoch 15, batch    11 | loss: 22.3895658CurrentTrain: epoch 15, batch    12 | loss: 14.4780454CurrentTrain: epoch 15, batch    13 | loss: 14.9832451CurrentTrain: epoch 15, batch    14 | loss: 26.8030208CurrentTrain: epoch 15, batch    15 | loss: 13.5479187CurrentTrain: epoch 15, batch    16 | loss: 11.8905336CurrentTrain: epoch 15, batch    17 | loss: 15.0188089CurrentTrain: epoch 15, batch    18 | loss: 15.7360735CurrentTrain: epoch 15, batch    19 | loss: 18.5894530CurrentTrain: epoch 15, batch    20 | loss: 14.8997283CurrentTrain: epoch 15, batch    21 | loss: 15.3459202CurrentTrain: epoch 15, batch    22 | loss: 12.2750428CurrentTrain: epoch 15, batch    23 | loss: 12.3455922CurrentTrain: epoch 15, batch    24 | loss: 15.2209105CurrentTrain: epoch 15, batch    25 | loss: 11.6760755CurrentTrain: epoch 15, batch    26 | loss: 13.5879261CurrentTrain: epoch 15, batch    27 | loss: 11.6435780CurrentTrain: epoch 15, batch    28 | loss: 10.5052152CurrentTrain: epoch 15, batch    29 | loss: 12.2534791CurrentTrain: epoch 15, batch    30 | loss: 13.1056205CurrentTrain: epoch 15, batch    31 | loss: 19.1217413CurrentTrain: epoch 15, batch    32 | loss: 13.2100082CurrentTrain: epoch 15, batch    33 | loss: 8.7646194CurrentTrain: epoch 15, batch    34 | loss: 14.6521552CurrentTrain: epoch 15, batch    35 | loss: 11.7443506CurrentTrain: epoch 15, batch    36 | loss: 17.6519662CurrentTrain: epoch 15, batch    37 | loss: 13.0824722CurrentTrain: epoch 15, batch    38 | loss: 11.4297414CurrentTrain: epoch 15, batch    39 | loss: 17.2054676CurrentTrain: epoch 15, batch    40 | loss: 10.3024979CurrentTrain: epoch 15, batch    41 | loss: 8.4538712CurrentTrain: epoch 15, batch    42 | loss: 26.8064649CurrentTrain: epoch 15, batch    43 | loss: 25.7413607CurrentTrain: epoch 15, batch    44 | loss: 17.2255043CurrentTrain: epoch 15, batch    45 | loss: 15.2219971CurrentTrain: epoch 15, batch    46 | loss: 10.5179823CurrentTrain: epoch 15, batch    47 | loss: 12.7311041CurrentTrain: epoch 15, batch    48 | loss: 11.6927631CurrentTrain: epoch 15, batch    49 | loss: 15.6437864CurrentTrain: epoch 15, batch    50 | loss: 8.2886433CurrentTrain: epoch 15, batch    51 | loss: 8.7295510CurrentTrain: epoch 15, batch    52 | loss: 8.2459087CurrentTrain: epoch 15, batch    53 | loss: 13.6278953CurrentTrain: epoch 15, batch    54 | loss: 10.2065733CurrentTrain: epoch 15, batch    55 | loss: 21.9949322CurrentTrain: epoch 15, batch    56 | loss: 14.6276890CurrentTrain: epoch 15, batch    57 | loss: 23.0858684CurrentTrain: epoch 15, batch    58 | loss: 17.7018310CurrentTrain: epoch 15, batch    59 | loss: 13.7320656CurrentTrain: epoch 15, batch    60 | loss: 16.0654986CurrentTrain: epoch 15, batch    61 | loss: 16.7571191CurrentTrain: epoch  7, batch    62 | loss: 6.1054369CurrentTrain: epoch 15, batch     0 | loss: 22.4237126CurrentTrain: epoch 15, batch     1 | loss: 10.8729269CurrentTrain: epoch 15, batch     2 | loss: 19.8056746CurrentTrain: epoch 15, batch     3 | loss: 16.0556041CurrentTrain: epoch 15, batch     4 | loss: 13.6353038CurrentTrain: epoch 15, batch     5 | loss: 11.1789696CurrentTrain: epoch 15, batch     6 | loss: 10.3789435CurrentTrain: epoch 15, batch     7 | loss: 11.2047129CurrentTrain: epoch 15, batch     8 | loss: 23.2807676CurrentTrain: epoch 15, batch     9 | loss: 9.5450270CurrentTrain: epoch 15, batch    10 | loss: 27.7774782CurrentTrain: epoch 15, batch    11 | loss: 12.1564313CurrentTrain: epoch 15, batch    12 | loss: 10.1924396CurrentTrain: epoch 15, batch    13 | loss: 11.2048723CurrentTrain: epoch 15, batch    14 | loss: 13.7325555CurrentTrain: epoch 15, batch    15 | loss: 38.8237362CurrentTrain: epoch 15, batch    16 | loss: 15.1821070CurrentTrain: epoch 15, batch    17 | loss: 8.2416566CurrentTrain: epoch 15, batch    18 | loss: 11.8554007CurrentTrain: epoch 15, batch    19 | loss: 10.1948310CurrentTrain: epoch 15, batch    20 | loss: 19.1554917CurrentTrain: epoch 15, batch    21 | loss: 11.6801082CurrentTrain: epoch 15, batch    22 | loss: 13.0812398CurrentTrain: epoch 15, batch    23 | loss: 15.0337233CurrentTrain: epoch 15, batch    24 | loss: 9.1017025CurrentTrain: epoch 15, batch    25 | loss: 17.4235121CurrentTrain: epoch 15, batch    26 | loss: 13.5042879CurrentTrain: epoch 15, batch    27 | loss: 23.2337009CurrentTrain: epoch 15, batch    28 | loss: 10.7531609CurrentTrain: epoch 15, batch    29 | loss: 9.3532360CurrentTrain: epoch 15, batch    30 | loss: 23.6366077CurrentTrain: epoch 15, batch    31 | loss: 12.0006044CurrentTrain: epoch 15, batch    32 | loss: 15.2733553CurrentTrain: epoch 15, batch    33 | loss: 10.8192037CurrentTrain: epoch 15, batch    34 | loss: 23.0494962CurrentTrain: epoch 15, batch    35 | loss: 17.3893055CurrentTrain: epoch 15, batch    36 | loss: 18.5579350CurrentTrain: epoch 15, batch    37 | loss: 9.8134243CurrentTrain: epoch 15, batch    38 | loss: 20.4551783CurrentTrain: epoch 15, batch    39 | loss: 17.9299454CurrentTrain: epoch 15, batch    40 | loss: 9.7206479CurrentTrain: epoch 15, batch    41 | loss: 19.3560704CurrentTrain: epoch 15, batch    42 | loss: 19.5166819CurrentTrain: epoch 15, batch    43 | loss: 9.1939406CurrentTrain: epoch 15, batch    44 | loss: 11.4446027CurrentTrain: epoch 15, batch    45 | loss: 26.3688823CurrentTrain: epoch 15, batch    46 | loss: 16.5187621CurrentTrain: epoch 15, batch    47 | loss: 14.0788913CurrentTrain: epoch 15, batch    48 | loss: 10.0226906CurrentTrain: epoch 15, batch    49 | loss: 10.0247378CurrentTrain: epoch 15, batch    50 | loss: 8.4077149CurrentTrain: epoch 15, batch    51 | loss: 15.0448964CurrentTrain: epoch 15, batch    52 | loss: 17.6047685CurrentTrain: epoch 15, batch    53 | loss: 10.2680352CurrentTrain: epoch 15, batch    54 | loss: 15.2148179CurrentTrain: epoch 15, batch    55 | loss: 15.6599932CurrentTrain: epoch 15, batch    56 | loss: 21.8852538CurrentTrain: epoch 15, batch    57 | loss: 14.6044345CurrentTrain: epoch 15, batch    58 | loss: 13.9135741CurrentTrain: epoch 15, batch    59 | loss: 19.0799209CurrentTrain: epoch 15, batch    60 | loss: 14.1295411CurrentTrain: epoch 15, batch    61 | loss: 13.7147454CurrentTrain: epoch  7, batch    62 | loss: 16.1180670CurrentTrain: epoch 15, batch     0 | loss: 8.8596043CurrentTrain: epoch 15, batch     1 | loss: 11.6424027CurrentTrain: epoch 15, batch     2 | loss: 9.9655941CurrentTrain: epoch 15, batch     3 | loss: 10.0831323CurrentTrain: epoch 15, batch     4 | loss: 11.3111939CurrentTrain: epoch 15, batch     5 | loss: 13.2673885CurrentTrain: epoch 15, batch     6 | loss: 12.5743036CurrentTrain: epoch 15, batch     7 | loss: 15.0254971CurrentTrain: epoch 15, batch     8 | loss: 9.5370325CurrentTrain: epoch 15, batch     9 | loss: 8.7516853CurrentTrain: epoch 15, batch    10 | loss: 12.6790692CurrentTrain: epoch 15, batch    11 | loss: 11.7957272CurrentTrain: epoch 15, batch    12 | loss: 17.5785766CurrentTrain: epoch 15, batch    13 | loss: 10.0141808CurrentTrain: epoch 15, batch    14 | loss: 11.8705209CurrentTrain: epoch 15, batch    15 | loss: 9.3525091CurrentTrain: epoch 15, batch    16 | loss: 10.2669818CurrentTrain: epoch 15, batch    17 | loss: 15.2072558CurrentTrain: epoch 15, batch    18 | loss: 14.6955689CurrentTrain: epoch 15, batch    19 | loss: 16.5558519CurrentTrain: epoch 15, batch    20 | loss: 13.5633505CurrentTrain: epoch 15, batch    21 | loss: 10.3483965CurrentTrain: epoch 15, batch    22 | loss: 7.4724293CurrentTrain: epoch 15, batch    23 | loss: 11.2227313CurrentTrain: epoch 15, batch    24 | loss: 7.0298660CurrentTrain: epoch 15, batch    25 | loss: 8.6839612CurrentTrain: epoch 15, batch    26 | loss: 11.1888895CurrentTrain: epoch 15, batch    27 | loss: 10.4412557CurrentTrain: epoch 15, batch    28 | loss: 17.4381364CurrentTrain: epoch 15, batch    29 | loss: 20.3392627CurrentTrain: epoch 15, batch    30 | loss: 17.0710713CurrentTrain: epoch 15, batch    31 | loss: 9.1576458CurrentTrain: epoch 15, batch    32 | loss: 21.2945894CurrentTrain: epoch 15, batch    33 | loss: 9.1613707CurrentTrain: epoch 15, batch    34 | loss: 8.7561097CurrentTrain: epoch 15, batch    35 | loss: 27.9955271CurrentTrain: epoch 15, batch    36 | loss: 7.7071429CurrentTrain: epoch 15, batch    37 | loss: 11.9759778CurrentTrain: epoch 15, batch    38 | loss: 8.9360581CurrentTrain: epoch 15, batch    39 | loss: 21.1899525CurrentTrain: epoch 15, batch    40 | loss: 9.4329915CurrentTrain: epoch 15, batch    41 | loss: 12.4846134CurrentTrain: epoch 15, batch    42 | loss: 25.0400914CurrentTrain: epoch 15, batch    43 | loss: 7.3069512CurrentTrain: epoch 15, batch    44 | loss: 15.3013396CurrentTrain: epoch 15, batch    45 | loss: 9.2797870CurrentTrain: epoch 15, batch    46 | loss: 11.0098374CurrentTrain: epoch 15, batch    47 | loss: 13.1619934CurrentTrain: epoch 15, batch    48 | loss: 9.7393178CurrentTrain: epoch 15, batch    49 | loss: 10.4450186CurrentTrain: epoch 15, batch    50 | loss: 27.3359384CurrentTrain: epoch 15, batch    51 | loss: 18.2544577CurrentTrain: epoch 15, batch    52 | loss: 13.5692630CurrentTrain: epoch 15, batch    53 | loss: 23.9761692CurrentTrain: epoch 15, batch    54 | loss: 11.1246096CurrentTrain: epoch 15, batch    55 | loss: 8.3377626CurrentTrain: epoch 15, batch    56 | loss: 18.0193013CurrentTrain: epoch 15, batch    57 | loss: 14.1051467CurrentTrain: epoch 15, batch    58 | loss: 8.1442702CurrentTrain: epoch 15, batch    59 | loss: 13.2733023CurrentTrain: epoch 15, batch    60 | loss: 16.4740853CurrentTrain: epoch 15, batch    61 | loss: 11.4118129CurrentTrain: epoch  7, batch    62 | loss: 19.0602809CurrentTrain: epoch 15, batch     0 | loss: 16.2480467CurrentTrain: epoch 15, batch     1 | loss: 10.0962296CurrentTrain: epoch 15, batch     2 | loss: 10.0013125CurrentTrain: epoch 15, batch     3 | loss: 16.3144800CurrentTrain: epoch 15, batch     4 | loss: 18.3604299CurrentTrain: epoch 15, batch     5 | loss: 6.5255664CurrentTrain: epoch 15, batch     6 | loss: 9.2547379CurrentTrain: epoch 15, batch     7 | loss: 10.5281684CurrentTrain: epoch 15, batch     8 | loss: 10.8571730CurrentTrain: epoch 15, batch     9 | loss: 11.4473164CurrentTrain: epoch 15, batch    10 | loss: 9.1244420CurrentTrain: epoch 15, batch    11 | loss: 11.4289607CurrentTrain: epoch 15, batch    12 | loss: 8.5661317CurrentTrain: epoch 15, batch    13 | loss: 9.1456315CurrentTrain: epoch 15, batch    14 | loss: 12.7125739CurrentTrain: epoch 15, batch    15 | loss: 9.2951496CurrentTrain: epoch 15, batch    16 | loss: 12.1532757CurrentTrain: epoch 15, batch    17 | loss: 16.4885620CurrentTrain: epoch 15, batch    18 | loss: 9.8861671CurrentTrain: epoch 15, batch    19 | loss: 9.9205022CurrentTrain: epoch 15, batch    20 | loss: 10.3863119CurrentTrain: epoch 15, batch    21 | loss: 12.7806962CurrentTrain: epoch 15, batch    22 | loss: 7.0720730CurrentTrain: epoch 15, batch    23 | loss: 9.4864033CurrentTrain: epoch 15, batch    24 | loss: 10.3751327CurrentTrain: epoch 15, batch    25 | loss: 8.3659223CurrentTrain: epoch 15, batch    26 | loss: 14.2980477CurrentTrain: epoch 15, batch    27 | loss: 10.3860621CurrentTrain: epoch 15, batch    28 | loss: 14.5717236CurrentTrain: epoch 15, batch    29 | loss: 11.5368287CurrentTrain: epoch 15, batch    30 | loss: 18.6052676CurrentTrain: epoch 15, batch    31 | loss: 16.6279969CurrentTrain: epoch 15, batch    32 | loss: 9.5467771CurrentTrain: epoch 15, batch    33 | loss: 8.7737837CurrentTrain: epoch 15, batch    34 | loss: 6.7418832CurrentTrain: epoch 15, batch    35 | loss: 24.9270027CurrentTrain: epoch 15, batch    36 | loss: 9.7327247CurrentTrain: epoch 15, batch    37 | loss: 12.0750514CurrentTrain: epoch 15, batch    38 | loss: 12.7142948CurrentTrain: epoch 15, batch    39 | loss: 10.0536233CurrentTrain: epoch 15, batch    40 | loss: 8.6136448CurrentTrain: epoch 15, batch    41 | loss: 11.4512829CurrentTrain: epoch 15, batch    42 | loss: 12.9003815CurrentTrain: epoch 15, batch    43 | loss: 21.3403468CurrentTrain: epoch 15, batch    44 | loss: 8.2799275CurrentTrain: epoch 15, batch    45 | loss: 9.9351078CurrentTrain: epoch 15, batch    46 | loss: 14.9917560CurrentTrain: epoch 15, batch    47 | loss: 11.8382951CurrentTrain: epoch 15, batch    48 | loss: 16.4000169CurrentTrain: epoch 15, batch    49 | loss: 7.5461884CurrentTrain: epoch 15, batch    50 | loss: 11.9040125CurrentTrain: epoch 15, batch    51 | loss: 23.7453447CurrentTrain: epoch 15, batch    52 | loss: 11.1012982CurrentTrain: epoch 15, batch    53 | loss: 14.2366107CurrentTrain: epoch 15, batch    54 | loss: 7.2705323CurrentTrain: epoch 15, batch    55 | loss: 10.7679111CurrentTrain: epoch 15, batch    56 | loss: 14.8283214CurrentTrain: epoch 15, batch    57 | loss: 11.6877784CurrentTrain: epoch 15, batch    58 | loss: 18.1814950CurrentTrain: epoch 15, batch    59 | loss: 10.9591348CurrentTrain: epoch 15, batch    60 | loss: 22.9010912CurrentTrain: epoch 15, batch    61 | loss: 6.3203674CurrentTrain: epoch  7, batch    62 | loss: 9.0921152CurrentTrain: epoch 15, batch     0 | loss: 8.4451600CurrentTrain: epoch 15, batch     1 | loss: 9.6421334CurrentTrain: epoch 15, batch     2 | loss: 8.8695576CurrentTrain: epoch 15, batch     3 | loss: 21.2300285CurrentTrain: epoch 15, batch     4 | loss: 12.9160733CurrentTrain: epoch 15, batch     5 | loss: 6.9529223CurrentTrain: epoch 15, batch     6 | loss: 7.0019800CurrentTrain: epoch 15, batch     7 | loss: 8.1061075CurrentTrain: epoch 15, batch     8 | loss: 11.5585503CurrentTrain: epoch 15, batch     9 | loss: 10.7067424CurrentTrain: epoch 15, batch    10 | loss: 11.7842545CurrentTrain: epoch 15, batch    11 | loss: 14.2746851CurrentTrain: epoch 15, batch    12 | loss: 16.3717402CurrentTrain: epoch 15, batch    13 | loss: 10.6151418CurrentTrain: epoch 15, batch    14 | loss: 9.0037378CurrentTrain: epoch 15, batch    15 | loss: 23.0636601CurrentTrain: epoch 15, batch    16 | loss: 12.4735810CurrentTrain: epoch 15, batch    17 | loss: 12.0981015CurrentTrain: epoch 15, batch    18 | loss: 14.3498988CurrentTrain: epoch 15, batch    19 | loss: 9.7751602CurrentTrain: epoch 15, batch    20 | loss: 15.6075929CurrentTrain: epoch 15, batch    21 | loss: 14.8841523CurrentTrain: epoch 15, batch    22 | loss: 13.2852532CurrentTrain: epoch 15, batch    23 | loss: 12.8007073CurrentTrain: epoch 15, batch    24 | loss: 13.1375030CurrentTrain: epoch 15, batch    25 | loss: 9.5303084CurrentTrain: epoch 15, batch    26 | loss: 19.1934003CurrentTrain: epoch 15, batch    27 | loss: 12.8042069CurrentTrain: epoch 15, batch    28 | loss: 7.8644346CurrentTrain: epoch 15, batch    29 | loss: 15.2210336CurrentTrain: epoch 15, batch    30 | loss: 8.4508396CurrentTrain: epoch 15, batch    31 | loss: 8.1516090CurrentTrain: epoch 15, batch    32 | loss: 16.3135606CurrentTrain: epoch 15, batch    33 | loss: 20.6371080CurrentTrain: epoch 15, batch    34 | loss: 12.2599861CurrentTrain: epoch 15, batch    35 | loss: 21.8487604CurrentTrain: epoch 15, batch    36 | loss: 26.3726289CurrentTrain: epoch 15, batch    37 | loss: 9.6422092CurrentTrain: epoch 15, batch    38 | loss: 15.4711863CurrentTrain: epoch 15, batch    39 | loss: 14.0386140CurrentTrain: epoch 15, batch    40 | loss: 10.0873147CurrentTrain: epoch 15, batch    41 | loss: 9.6371383CurrentTrain: epoch 15, batch    42 | loss: 13.0421973CurrentTrain: epoch 15, batch    43 | loss: 12.1956486CurrentTrain: epoch 15, batch    44 | loss: 22.6234615CurrentTrain: epoch 15, batch    45 | loss: 14.3736338CurrentTrain: epoch 15, batch    46 | loss: 9.6635518CurrentTrain: epoch 15, batch    47 | loss: 11.8653324CurrentTrain: epoch 15, batch    48 | loss: 9.5549104CurrentTrain: epoch 15, batch    49 | loss: 14.9578857CurrentTrain: epoch 15, batch    50 | loss: 9.2260253CurrentTrain: epoch 15, batch    51 | loss: 11.3892962CurrentTrain: epoch 15, batch    52 | loss: 8.1496658CurrentTrain: epoch 15, batch    53 | loss: 7.2117460CurrentTrain: epoch 15, batch    54 | loss: 13.0121858CurrentTrain: epoch 15, batch    55 | loss: 13.1555515CurrentTrain: epoch 15, batch    56 | loss: 8.0015840CurrentTrain: epoch 15, batch    57 | loss: 10.3055036CurrentTrain: epoch 15, batch    58 | loss: 14.1175181CurrentTrain: epoch 15, batch    59 | loss: 12.8221860CurrentTrain: epoch 15, batch    60 | loss: 15.0108769CurrentTrain: epoch 15, batch    61 | loss: 7.8747675CurrentTrain: epoch  7, batch    62 | loss: 7.4496698CurrentTrain: epoch 15, batch     0 | loss: 14.4939170CurrentTrain: epoch 15, batch     1 | loss: 12.8261417CurrentTrain: epoch 15, batch     2 | loss: 11.1922366CurrentTrain: epoch 15, batch     3 | loss: 8.7373309CurrentTrain: epoch 15, batch     4 | loss: 8.6232613CurrentTrain: epoch 15, batch     5 | loss: 6.6650429CurrentTrain: epoch 15, batch     6 | loss: 10.9730858CurrentTrain: epoch 15, batch     7 | loss: 9.2357536CurrentTrain: epoch 15, batch     8 | loss: 6.6620790CurrentTrain: epoch 15, batch     9 | loss: 12.3509138CurrentTrain: epoch 15, batch    10 | loss: 7.3389203CurrentTrain: epoch 15, batch    11 | loss: 13.0826682CurrentTrain: epoch 15, batch    12 | loss: 9.8363391CurrentTrain: epoch 15, batch    13 | loss: 14.2520442CurrentTrain: epoch 15, batch    14 | loss: 10.9973654CurrentTrain: epoch 15, batch    15 | loss: 8.1495524CurrentTrain: epoch 15, batch    16 | loss: 12.1467250CurrentTrain: epoch 15, batch    17 | loss: 9.9923224CurrentTrain: epoch 15, batch    18 | loss: 18.9516425CurrentTrain: epoch 15, batch    19 | loss: 13.6963708CurrentTrain: epoch 15, batch    20 | loss: 23.6996305CurrentTrain: epoch 15, batch    21 | loss: 9.9277137CurrentTrain: epoch 15, batch    22 | loss: 25.3113329CurrentTrain: epoch 15, batch    23 | loss: 9.8392064CurrentTrain: epoch 15, batch    24 | loss: 8.4759554CurrentTrain: epoch 15, batch    25 | loss: 11.4847268CurrentTrain: epoch 15, batch    26 | loss: 8.5809686CurrentTrain: epoch 15, batch    27 | loss: 13.9057949CurrentTrain: epoch 15, batch    28 | loss: 23.3301852CurrentTrain: epoch 15, batch    29 | loss: 10.8417763CurrentTrain: epoch 15, batch    30 | loss: 15.3646728CurrentTrain: epoch 15, batch    31 | loss: 10.0018448CurrentTrain: epoch 15, batch    32 | loss: 8.7085322CurrentTrain: epoch 15, batch    33 | loss: 15.6009146CurrentTrain: epoch 15, batch    34 | loss: 15.0963136CurrentTrain: epoch 15, batch    35 | loss: 8.6421874CurrentTrain: epoch 15, batch    36 | loss: 13.1375018CurrentTrain: epoch 15, batch    37 | loss: 11.7672717CurrentTrain: epoch 15, batch    38 | loss: 12.5034384CurrentTrain: epoch 15, batch    39 | loss: 14.3294713CurrentTrain: epoch 15, batch    40 | loss: 7.9471385CurrentTrain: epoch 15, batch    41 | loss: 12.0582960CurrentTrain: epoch 15, batch    42 | loss: 11.3939245CurrentTrain: epoch 15, batch    43 | loss: 8.2879997CurrentTrain: epoch 15, batch    44 | loss: 7.6622795CurrentTrain: epoch 15, batch    45 | loss: 11.6882968CurrentTrain: epoch 15, batch    46 | loss: 11.4256909CurrentTrain: epoch 15, batch    47 | loss: 8.6546750CurrentTrain: epoch 15, batch    48 | loss: 14.9071756CurrentTrain: epoch 15, batch    49 | loss: 9.1887483CurrentTrain: epoch 15, batch    50 | loss: 10.0125065CurrentTrain: epoch 15, batch    51 | loss: 11.4353739CurrentTrain: epoch 15, batch    52 | loss: 8.7857066CurrentTrain: epoch 15, batch    53 | loss: 9.4886945CurrentTrain: epoch 15, batch    54 | loss: 9.9298947CurrentTrain: epoch 15, batch    55 | loss: 9.7407065CurrentTrain: epoch 15, batch    56 | loss: 25.6671117CurrentTrain: epoch 15, batch    57 | loss: 8.8782218CurrentTrain: epoch 15, batch    58 | loss: 15.2509014CurrentTrain: epoch 15, batch    59 | loss: 12.9084444CurrentTrain: epoch 15, batch    60 | loss: 9.8974155CurrentTrain: epoch 15, batch    61 | loss: 12.3794325CurrentTrain: epoch  7, batch    62 | loss: 14.4537759CurrentTrain: epoch 15, batch     0 | loss: 12.5104795CurrentTrain: epoch 15, batch     1 | loss: 11.7199329CurrentTrain: epoch 15, batch     2 | loss: 11.2668589CurrentTrain: epoch 15, batch     3 | loss: 16.3261791CurrentTrain: epoch 15, batch     4 | loss: 6.5227725CurrentTrain: epoch 15, batch     5 | loss: 8.4686624CurrentTrain: epoch 15, batch     6 | loss: 8.7970979CurrentTrain: epoch 15, batch     7 | loss: 21.2805466CurrentTrain: epoch 15, batch     8 | loss: 6.1437878CurrentTrain: epoch 15, batch     9 | loss: 14.5401168CurrentTrain: epoch 15, batch    10 | loss: 22.5879565CurrentTrain: epoch 15, batch    11 | loss: 8.3116275CurrentTrain: epoch 15, batch    12 | loss: 9.4464797CurrentTrain: epoch 15, batch    13 | loss: 10.0263369CurrentTrain: epoch 15, batch    14 | loss: 9.3483361CurrentTrain: epoch 15, batch    15 | loss: 7.0685612CurrentTrain: epoch 15, batch    16 | loss: 15.3230105CurrentTrain: epoch 15, batch    17 | loss: 30.8441425CurrentTrain: epoch 15, batch    18 | loss: 14.4685553CurrentTrain: epoch 15, batch    19 | loss: 10.0287814CurrentTrain: epoch 15, batch    20 | loss: 12.6904499CurrentTrain: epoch 15, batch    21 | loss: 9.0066697CurrentTrain: epoch 15, batch    22 | loss: 12.2976431CurrentTrain: epoch 15, batch    23 | loss: 9.4817794CurrentTrain: epoch 15, batch    24 | loss: 10.6503620CurrentTrain: epoch 15, batch    25 | loss: 14.6206879CurrentTrain: epoch 15, batch    26 | loss: 13.1293121CurrentTrain: epoch 15, batch    27 | loss: 11.7488044CurrentTrain: epoch 15, batch    28 | loss: 10.2692476CurrentTrain: epoch 15, batch    29 | loss: 11.7126059CurrentTrain: epoch 15, batch    30 | loss: 16.8575383CurrentTrain: epoch 15, batch    31 | loss: 10.0095546CurrentTrain: epoch 15, batch    32 | loss: 8.4868447CurrentTrain: epoch 15, batch    33 | loss: 11.6583128CurrentTrain: epoch 15, batch    34 | loss: 9.4069411CurrentTrain: epoch 15, batch    35 | loss: 6.5700096CurrentTrain: epoch 15, batch    36 | loss: 11.0886936CurrentTrain: epoch 15, batch    37 | loss: 11.0362779CurrentTrain: epoch 15, batch    38 | loss: 10.0292905CurrentTrain: epoch 15, batch    39 | loss: 8.5501782CurrentTrain: epoch 15, batch    40 | loss: 8.2489539CurrentTrain: epoch 15, batch    41 | loss: 9.4793625CurrentTrain: epoch 15, batch    42 | loss: 10.3775909CurrentTrain: epoch 15, batch    43 | loss: 8.5141637CurrentTrain: epoch 15, batch    44 | loss: 8.7660113CurrentTrain: epoch 15, batch    45 | loss: 8.7959496CurrentTrain: epoch 15, batch    46 | loss: 9.2138631CurrentTrain: epoch 15, batch    47 | loss: 12.3681229CurrentTrain: epoch 15, batch    48 | loss: 21.1379462CurrentTrain: epoch 15, batch    49 | loss: 9.6820648CurrentTrain: epoch 15, batch    50 | loss: 23.0477753CurrentTrain: epoch 15, batch    51 | loss: 8.3987930CurrentTrain: epoch 15, batch    52 | loss: 6.8002095CurrentTrain: epoch 15, batch    53 | loss: 11.7707379CurrentTrain: epoch 15, batch    54 | loss: 13.4807532CurrentTrain: epoch 15, batch    55 | loss: 14.2551033CurrentTrain: epoch 15, batch    56 | loss: 8.8772264CurrentTrain: epoch 15, batch    57 | loss: 7.4786359CurrentTrain: epoch 15, batch    58 | loss: 10.4498217CurrentTrain: epoch 15, batch    59 | loss: 17.6500392CurrentTrain: epoch 15, batch    60 | loss: 9.0433147CurrentTrain: epoch 15, batch    61 | loss: 14.1498618CurrentTrain: epoch  7, batch    62 | loss: 6.3783786CurrentTrain: epoch 15, batch     0 | loss: 9.9363245CurrentTrain: epoch 15, batch     1 | loss: 8.8691539CurrentTrain: epoch 15, batch     2 | loss: 9.7660302CurrentTrain: epoch 15, batch     3 | loss: 6.7331588CurrentTrain: epoch 15, batch     4 | loss: 12.6416490CurrentTrain: epoch 15, batch     5 | loss: 11.4561817CurrentTrain: epoch 15, batch     6 | loss: 17.0386393CurrentTrain: epoch 15, batch     7 | loss: 13.0319051CurrentTrain: epoch 15, batch     8 | loss: 9.9528234CurrentTrain: epoch 15, batch     9 | loss: 11.2601612CurrentTrain: epoch 15, batch    10 | loss: 5.8725030CurrentTrain: epoch 15, batch    11 | loss: 14.1147262CurrentTrain: epoch 15, batch    12 | loss: 9.6385496CurrentTrain: epoch 15, batch    13 | loss: 19.1511746CurrentTrain: epoch 15, batch    14 | loss: 11.7692998CurrentTrain: epoch 15, batch    15 | loss: 19.7463588CurrentTrain: epoch 15, batch    16 | loss: 20.1767135CurrentTrain: epoch 15, batch    17 | loss: 14.8605990CurrentTrain: epoch 15, batch    18 | loss: 14.1621697CurrentTrain: epoch 15, batch    19 | loss: 7.3936305CurrentTrain: epoch 15, batch    20 | loss: 9.6087433CurrentTrain: epoch 15, batch    21 | loss: 12.8100196CurrentTrain: epoch 15, batch    22 | loss: 7.9969598CurrentTrain: epoch 15, batch    23 | loss: 11.5235199CurrentTrain: epoch 15, batch    24 | loss: 7.5377822CurrentTrain: epoch 15, batch    25 | loss: 10.6984820CurrentTrain: epoch 15, batch    26 | loss: 6.5798110CurrentTrain: epoch 15, batch    27 | loss: 9.2457423CurrentTrain: epoch 15, batch    28 | loss: 16.4695612CurrentTrain: epoch 15, batch    29 | loss: 11.1730379CurrentTrain: epoch 15, batch    30 | loss: 8.3547832CurrentTrain: epoch 15, batch    31 | loss: 12.9803934CurrentTrain: epoch 15, batch    32 | loss: 33.0807716CurrentTrain: epoch 15, batch    33 | loss: 18.0032805CurrentTrain: epoch 15, batch    34 | loss: 15.1607607CurrentTrain: epoch 15, batch    35 | loss: 15.8016894CurrentTrain: epoch 15, batch    36 | loss: 10.4565454CurrentTrain: epoch 15, batch    37 | loss: 8.4737058CurrentTrain: epoch 15, batch    38 | loss: 9.2813577CurrentTrain: epoch 15, batch    39 | loss: 10.5372891CurrentTrain: epoch 15, batch    40 | loss: 12.3118574CurrentTrain: epoch 15, batch    41 | loss: 15.3478002CurrentTrain: epoch 15, batch    42 | loss: 9.9390580CurrentTrain: epoch 15, batch    43 | loss: 8.5660907CurrentTrain: epoch 15, batch    44 | loss: 8.9007083CurrentTrain: epoch 15, batch    45 | loss: 9.8950728CurrentTrain: epoch 15, batch    46 | loss: 13.0904980CurrentTrain: epoch 15, batch    47 | loss: 6.6728748CurrentTrain: epoch 15, batch    48 | loss: 8.2707751CurrentTrain: epoch 15, batch    49 | loss: 8.5012046CurrentTrain: epoch 15, batch    50 | loss: 9.5840770CurrentTrain: epoch 15, batch    51 | loss: 12.4192196CurrentTrain: epoch 15, batch    52 | loss: 8.6652756CurrentTrain: epoch 15, batch    53 | loss: 13.5072588CurrentTrain: epoch 15, batch    54 | loss: 9.5780813CurrentTrain: epoch 15, batch    55 | loss: 23.3962830CurrentTrain: epoch 15, batch    56 | loss: 12.6507440CurrentTrain: epoch 15, batch    57 | loss: 7.8811574CurrentTrain: epoch 15, batch    58 | loss: 13.0454407CurrentTrain: epoch 15, batch    59 | loss: 6.8291911CurrentTrain: epoch 15, batch    60 | loss: 8.4335206CurrentTrain: epoch 15, batch    61 | loss: 9.5674100CurrentTrain: epoch  7, batch    62 | loss: 8.7437113
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: while the milanese painting scene was dominated by neoclassic painters andrea appiani and luigi sabatelli , giovannni migliara stayed with the historical themes and medieval subtlety of romanticism .
Head Entity: andrea appiani
Tail Entity: neoclassic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: during the early 20th century, the surrealist movement gained prominence, with artists like salvador dalí and max ernst pushing the boundaries of creativity and imagination.  
Head Entity: salvador dalí  
Tail Entity: surrealist  

Relation: movement  
Context: the beat generation was a literary movement that emerged in the 1950s, characterized by its rejection of conventional society and exploration of spirituality, with figures like jack kerouac and allen ginsberg at the forefront.  
Head Entity: jack kerouac  
Tail Entity: beat generation  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: lyons hill is a restored village , and former parish with church , now part of the community of ardclough in north county kildare .
Head Entity: ardclough
Tail Entity: county kildare
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: The city of San Francisco is known for its iconic Golden Gate Bridge and is part of the larger San Francisco County in California.  
Head Entity: San Francisco  
Tail Entity: San Francisco County  

Relation: located in the administrative territorial entity  
Context: The village of Haverfordwest serves as the county town of Pembrokeshire, situated in the picturesque region of Wales.  
Head Entity: Haverfordwest  
Tail Entity: Pembrokeshire  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the nation as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: hank henshaw became a member of the sinestro corps during the sinestro corps war .
Head Entity: hank henshaw
Tail Entity: sinestro corps
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: after years of dedication, she was finally recognized as a member of the national ballet company.  
Head Entity: she  
Tail Entity: national ballet company  

Relation: member of  
Context: the renowned scientist was proud to be a member of the international academy of sciences.  
Head Entity: the renowned scientist  
Tail Entity: international academy of sciences  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: on july 2 , 1935 , the rip van winkle bridge over the hudson river between catskill and greenport was opened to traffic .
Head Entity: rip van winkle bridge
Tail Entity: hudson river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: The Golden Gate Bridge spans the Golden Gate Strait, connecting San Francisco to Marin County, and is a vital link for traffic crossing over the water.  
Head Entity: Golden Gate Bridge  
Tail Entity: Golden Gate Strait  

Relation: crosses  
Context: The Channel Tunnel, also known as the Chunnel, runs beneath the English Channel, providing a crucial transportation route between the UK and mainland Europe.  
Head Entity: Channel Tunnel  
Tail Entity: English Channel  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: it is on the panj river just downstream from the mouth of the bartang river at the point where the panj briefly turns west before resuming its northerly course .
Head Entity: panj river
Tail Entity: bartang river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The Missouri River, which is a major tributary of the Mississippi River, flows through several states before joining the larger river system.  
Head Entity: Missouri River  
Tail Entity: Mississippi River  

Relation: tributary  
Context: The Rio Grande serves as a natural border between the United States and Mexico, and it has several tributaries, including the Pecos River.  
Head Entity: Rio Grande  
Tail Entity: Pecos River  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: on 3 july 1937 , the count married rocafort - atuzarrra in a lavish wedding in havana , cuba attended by the president of cuba , federico laredo brú .
Head Entity: federico laredo brú
Tail Entity: president of cuba
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: After serving as the mayor of New York City for eight years, Bill de Blasio announced his candidacy for governor in 2022.  
Head Entity: Bill de Blasio  
Tail Entity: mayor of New York City  

Relation: position held  
Context: In 2015, Angela Merkel was re-elected as Chancellor of Germany, continuing her leadership role in the European Union.  
Head Entity: Angela Merkel  
Tail Entity: Chancellor of Germany  
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: a headcrab is a fictional alien parasitoid found in the " half - life " video game series created by valve software .
Head Entity: half - life
Tail Entity: valve software
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: The popular game "The Witcher 3: Wild Hunt" was developed by CD Projekt Red, a well-known Polish video game developer.  
Head Entity: The Witcher 3: Wild Hunt  
Tail Entity: CD Projekt Red  

Relation: developer  
Context: The innovative mobile game "Angry Birds" was created by Rovio Entertainment, a Finnish company that specializes in game development.  
Head Entity: Angry Birds  
Tail Entity: Rovio Entertainment  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: twin method was an alternative metal band based in liverpool , england .
Head Entity: twin method
Tail Entity: liverpool
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the famous tech startup was established in silicon valley, california.  
Head Entity: the famous tech startup  
Tail Entity: silicon valley  

Relation: location of formation  
Context: the renowned art collective originated in new york city, new york.  
Head Entity: the renowned art collective  
Tail Entity: new york city  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: out here is the fifth album by the american rock band love , released in december 1969 on blue thumb records in the united states , and harvest in the united kingdom .
Head Entity: love
Tail Entity: american
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous Italian dish pizza has gained popularity worldwide, but its roots can be traced back to Naples, Italy, where it was first created.  
Head Entity: pizza  
Tail Entity: Italian  

Relation: country of origin  
Context: The iconic brand Rolex is known for its luxury watches, which are meticulously crafted in Switzerland, a country renowned for its watchmaking expertise.  
Head Entity: Rolex  
Tail Entity: Switzerland  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.79%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 94.08%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 94.32%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.57%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.68%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.61%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.96%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.40%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.61%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.67%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.98%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 96.08%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.16%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.33%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.28%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 96.35%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.43%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.45%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 96.27%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 96.23%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.30%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 96.02%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.98%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.91%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.97%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.24%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.79%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 94.08%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 94.32%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.57%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.68%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.61%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.96%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.40%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.61%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.67%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.98%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 96.08%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.16%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.33%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.28%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 96.35%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.43%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.45%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 96.27%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 96.23%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.30%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 96.02%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.98%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.91%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.97%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.24%   
cur_acc:  ['0.9524']
his_acc:  ['0.9524']
CurrentTrain: epoch 15, batch     0 | loss: 15.6831873CurrentTrain: epoch 15, batch     1 | loss: 19.8474537CurrentTrain: epoch 15, batch     2 | loss: 18.4308809CurrentTrain: epoch  1, batch     3 | loss: 8.3963892CurrentTrain: epoch 15, batch     0 | loss: 14.6339499CurrentTrain: epoch 15, batch     1 | loss: 12.9603207CurrentTrain: epoch 15, batch     2 | loss: 13.5871032CurrentTrain: epoch  1, batch     3 | loss: 18.2198793CurrentTrain: epoch 15, batch     0 | loss: 10.9863675CurrentTrain: epoch 15, batch     1 | loss: 17.8296402CurrentTrain: epoch 15, batch     2 | loss: 13.9888390CurrentTrain: epoch  1, batch     3 | loss: 7.3609292CurrentTrain: epoch 15, batch     0 | loss: 14.3463826CurrentTrain: epoch 15, batch     1 | loss: 14.2972250CurrentTrain: epoch 15, batch     2 | loss: 13.1671325CurrentTrain: epoch  1, batch     3 | loss: 9.1000839CurrentTrain: epoch 15, batch     0 | loss: 13.7720488CurrentTrain: epoch 15, batch     1 | loss: 9.0787685CurrentTrain: epoch 15, batch     2 | loss: 14.6002808CurrentTrain: epoch  1, batch     3 | loss: 9.2625954CurrentTrain: epoch 15, batch     0 | loss: 9.3055529CurrentTrain: epoch 15, batch     1 | loss: 11.6227895CurrentTrain: epoch 15, batch     2 | loss: 14.2421356CurrentTrain: epoch  1, batch     3 | loss: 7.1454417CurrentTrain: epoch 15, batch     0 | loss: 11.0470156CurrentTrain: epoch 15, batch     1 | loss: 8.2296637CurrentTrain: epoch 15, batch     2 | loss: 11.5373229CurrentTrain: epoch  1, batch     3 | loss: 7.7055892CurrentTrain: epoch 15, batch     0 | loss: 10.3465514CurrentTrain: epoch 15, batch     1 | loss: 7.0388378CurrentTrain: epoch 15, batch     2 | loss: 14.6765746CurrentTrain: epoch  1, batch     3 | loss: 9.3702590CurrentTrain: epoch 15, batch     0 | loss: 9.0051149CurrentTrain: epoch 15, batch     1 | loss: 13.3442939CurrentTrain: epoch 15, batch     2 | loss: 8.6707650CurrentTrain: epoch  1, batch     3 | loss: 8.6043822CurrentTrain: epoch 15, batch     0 | loss: 26.6192442CurrentTrain: epoch 15, batch     1 | loss: 14.3381627CurrentTrain: epoch 15, batch     2 | loss: 10.9190224CurrentTrain: epoch  1, batch     3 | loss: 6.3152069
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: in 2004 the catalan government gave him the george cross .
Head Entity: george cross
Tail Entity: catalan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The new environmental regulations introduced by the European Union will affect all member states.  
Head Entity: environmental regulations  
Tail Entity: European Union  

Relation: applies to jurisdiction  
Context: The Supreme Court's ruling has significant implications for the state of California's water rights.  
Head Entity: Supreme Court  
Tail Entity: California  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: in 1998 , opeth singer and songwriter mikael åkerfeldt used part of a sentence from " drip drip " for the title of the album " my arms , your hearse " .
Head Entity: my arms , your hearse
Tail Entity: opeth
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: The famous band Coldplay released their new album "Music of the Spheres" in 2021, showcasing their unique sound and style.  
Head Entity: Music of the Spheres  
Tail Entity: Coldplay  

Relation: performer  
Context: In the documentary about the legendary musician, the film highlighted how David Bowie influenced countless artists with his innovative approach to music and performance.  
Head Entity: David Bowie  
Tail Entity: artists
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the glasflügel h-201 standard " libelle " ( german : " dragonfly " ) is an early composite standard class single - seat sailplane produced by glasflügel from 1967 .
Head Entity: dragonfly
Tail Entity: glasflügel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The iPhone 13, a smartphone developed by Apple, was released in September 2021 and features advanced camera technology and improved battery life.  
Head Entity: iPhone 13  
Tail Entity: Apple  

Relation: manufacturer  
Context: The Tesla Model S is an all-electric luxury sedan that has been produced by Tesla, Inc. since 2012, showcasing innovative technology and performance.  
Head Entity: Tesla Model S  
Tail Entity: Tesla, Inc.  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: in contrast to gwladys and roger 's allegiance to the house of lancaster , their three sons were staunch yorkists during the wars of the roses .
Head Entity: wars of the roses
Tail Entity: house of lancaster
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: During the annual tech conference, several leading companies showcased their latest innovations, with Google and Microsoft being the most prominent participants.  
Head Entity: tech conference  
Tail Entity: Google  

Relation: participant  
Context: The local community organized a charity run, where various schools and organizations, including the Red Cross, participated to raise funds for the homeless.  
Head Entity: charity run  
Tail Entity: Red Cross  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the application is compatible with various operating systems, including macOS, Windows, and Ubuntu.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: Many developers prefer to use Linux as their operating system due to its flexibility and open-source nature.  
Head Entity: developers  
Tail Entity: Linux  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in july 2007 , griffin lost by a technical knockout to light heavyweight champion danny green of the world boxing association at challenge stadium in perth , australia .
Head Entity: danny green
Tail Entity: light heavyweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: during the 2020 olympics, the men's 100 meters final showcased athletes from various competition classes, with usain bolt representing the sprinting class.  
Head Entity: usain bolt  
Tail Entity: sprinting  

Relation: competition class  
Context: the formula one race in monaco is known for its high-speed competition class, where lewis hamilton competed fiercely against other top drivers.  
Head Entity: lewis hamilton  
Tail Entity: formula one
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: the qup gained 3 seats from its previous election result , and won a 10 % swing to it in primary votes , but failed to defeat the incumbent labor government led by ted theodore .
Head Entity: previous election
Tail Entity: ted theodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: After a closely contested campaign, the voters chose Sarah Johnson as their representative, marking a significant shift in the political landscape of the district.  
Head Entity: closely contested campaign  
Tail Entity: Sarah Johnson  

Relation: successful candidate  
Context: In the recent mayoral election, the community rallied behind Mark Thompson, who promised to bring new initiatives to the city, ultimately leading to his victory.  
Head Entity: recent mayoral election  
Tail Entity: Mark Thompson  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: jenő szervánszky is the father of the pianist , valéria szervánszky and the brother of the composer , endre szervánszky and violinist , péter szervánszky .
Head Entity: péter szervánszky
Tail Entity: endre szervánszky
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: emma and oliver are both children of the same parents, making them siblings. they often play together and share a close bond.  
Head Entity: emma  
Tail Entity: oliver  

Relation: sibling  
Context: during the family reunion, it was clear that both lucas and mia inherited their parents' artistic talents, as they are siblings who excel in painting and music.  
Head Entity: lucas  
Tail Entity: mia  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: two of her sons , tunis and thomas tingey rose to prominence in the union navy during the civil war .
Head Entity: tunis
Tail Entity: union navy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: after serving in the army, he transitioned to a leadership role in the air force, where he implemented several key strategies.  
Head Entity: air force  
Tail Entity: army  

Relation: military branch  
Context: the general was honored for his service in the marine corps, where he led numerous successful missions.  
Head Entity: marine corps  
Tail Entity: general  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: in 1406 adolf married marie of burgundy , daughter of john the fearless and margaret of bavaria .
Head Entity: john the fearless
Tail Entity: marie of burgundy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: in 1980, michael and debbie welcomed their first child, a daughter named sarah, into the world.  
Head Entity: michael  
Tail Entity: sarah  

Relation: child  
Context: during the summer of 1995, elizabeth gave birth to her second child, a son named james, who quickly became the pride of the family.  
Head Entity: elizabeth  
Tail Entity: james  
MemoryTrain:  epoch 15, batch     0 | loss: 5.7255234MemoryTrain:  epoch 15, batch     1 | loss: 8.8111435MemoryTrain:  epoch 15, batch     2 | loss: 7.0550268MemoryTrain:  epoch 11, batch     3 | loss: 5.2961647MemoryTrain:  epoch 15, batch     0 | loss: 7.5958334MemoryTrain:  epoch 15, batch     1 | loss: 7.1567884MemoryTrain:  epoch 15, batch     2 | loss: 5.1799736MemoryTrain:  epoch 11, batch     3 | loss: 7.8052015MemoryTrain:  epoch 15, batch     0 | loss: 5.6068260MemoryTrain:  epoch 15, batch     1 | loss: 7.7143806MemoryTrain:  epoch 15, batch     2 | loss: 4.8178732MemoryTrain:  epoch 11, batch     3 | loss: 10.3592822MemoryTrain:  epoch 15, batch     0 | loss: 5.7732595MemoryTrain:  epoch 15, batch     1 | loss: 8.2975521MemoryTrain:  epoch 15, batch     2 | loss: 4.6189216MemoryTrain:  epoch 11, batch     3 | loss: 5.5395114MemoryTrain:  epoch 15, batch     0 | loss: 3.1549660MemoryTrain:  epoch 15, batch     1 | loss: 7.1427828MemoryTrain:  epoch 15, batch     2 | loss: 3.0754120MemoryTrain:  epoch 11, batch     3 | loss: 2.3765010MemoryTrain:  epoch 15, batch     0 | loss: 9.1280688MemoryTrain:  epoch 15, batch     1 | loss: 4.0787509MemoryTrain:  epoch 15, batch     2 | loss: 5.7178412MemoryTrain:  epoch 11, batch     3 | loss: 4.1031813MemoryTrain:  epoch 15, batch     0 | loss: 7.1276318MemoryTrain:  epoch 15, batch     1 | loss: 4.4932207MemoryTrain:  epoch 15, batch     2 | loss: 5.1806600MemoryTrain:  epoch 11, batch     3 | loss: 2.9814919MemoryTrain:  epoch 15, batch     0 | loss: 2.6798074MemoryTrain:  epoch 15, batch     1 | loss: 5.5113921MemoryTrain:  epoch 15, batch     2 | loss: 2.8011526MemoryTrain:  epoch 11, batch     3 | loss: 11.7499107MemoryTrain:  epoch 15, batch     0 | loss: 7.4128300MemoryTrain:  epoch 15, batch     1 | loss: 4.6933132MemoryTrain:  epoch 15, batch     2 | loss: 2.6757882MemoryTrain:  epoch 11, batch     3 | loss: 1.7951980MemoryTrain:  epoch 15, batch     0 | loss: 2.6864165MemoryTrain:  epoch 15, batch     1 | loss: 5.6536983MemoryTrain:  epoch 15, batch     2 | loss: 12.1998023MemoryTrain:  epoch 11, batch     3 | loss: 4.3787635
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 54.17%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 51.56%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 55.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 58.33%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 61.61%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 68.06%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 72.16%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 75.45%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 75.83%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 76.95%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 77.57%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 78.82%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 79.93%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 80.31%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 80.65%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 81.77%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 82.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.69%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.48%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.48%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.36%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 86.76%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 87.14%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 87.84%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.16%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 88.30%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 88.28%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 88.11%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 88.39%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.52%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 88.35%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 87.36%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 86.55%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 85.77%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 85.55%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 84.82%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 84.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 84.80%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 84.98%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 85.14%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 85.19%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 85.34%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 85.38%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 85.20%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 84.91%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 85.06%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 84.27%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 84.22%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 83.97%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 83.13%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 92.28%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 92.81%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 92.33%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 91.58%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 91.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.83%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 92.13%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 92.41%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.46%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.94%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.16%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.37%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.57%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.57%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.92%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 94.08%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.23%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.51%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 94.62%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 94.60%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 94.58%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.70%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 94.28%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 94.01%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.13%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 94.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.00%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 93.75%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.87%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 93.52%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 93.42%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 93.42%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 93.32%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 93.43%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 93.44%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 93.55%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 93.25%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 92.58%   [EVAL] batch:   64 | acc: 50.00%,  total acc: 91.92%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 91.38%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 90.95%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 90.72%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 90.40%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 90.45%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 90.58%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 90.28%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 90.33%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 90.37%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 90.33%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 90.26%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 90.14%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 90.19%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 90.23%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 90.35%   [EVAL] batch:   81 | acc: 93.75%,  total acc: 90.40%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 90.29%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 90.40%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 90.29%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 90.26%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 90.30%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 90.34%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 90.45%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 90.56%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 90.66%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 90.76%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 90.86%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 90.96%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 91.05%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 91.24%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 91.33%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 91.50%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 91.54%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 91.44%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 91.47%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 91.55%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 91.57%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 91.18%   [EVAL] batch:  107 | acc: 43.75%,  total acc: 90.74%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 90.42%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 90.11%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 89.86%   [EVAL] batch:  111 | acc: 68.75%,  total acc: 89.68%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 89.66%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 89.64%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 89.73%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 89.71%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 89.74%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 89.78%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 89.71%   [EVAL] batch:  119 | acc: 62.50%,  total acc: 89.48%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 89.46%   [EVAL] batch:  121 | acc: 68.75%,  total acc: 89.29%   [EVAL] batch:  122 | acc: 50.00%,  total acc: 88.97%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 88.86%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 88.70%   
cur_acc:  ['0.9524', '0.8313']
his_acc:  ['0.9524', '0.8870']
CurrentTrain: epoch 15, batch     0 | loss: 13.0651214CurrentTrain: epoch 15, batch     1 | loss: 12.6422555CurrentTrain: epoch 15, batch     2 | loss: 16.9857381CurrentTrain: epoch  1, batch     3 | loss: 10.9074990CurrentTrain: epoch 15, batch     0 | loss: 16.5247666CurrentTrain: epoch 15, batch     1 | loss: 11.7593129CurrentTrain: epoch 15, batch     2 | loss: 11.7798517CurrentTrain: epoch  1, batch     3 | loss: 7.1383144CurrentTrain: epoch 15, batch     0 | loss: 16.2115695CurrentTrain: epoch 15, batch     1 | loss: 9.6622763CurrentTrain: epoch 15, batch     2 | loss: 9.2928506CurrentTrain: epoch  1, batch     3 | loss: 8.3657738CurrentTrain: epoch 15, batch     0 | loss: 7.7147148CurrentTrain: epoch 15, batch     1 | loss: 6.9859515CurrentTrain: epoch 15, batch     2 | loss: 6.8929952CurrentTrain: epoch  1, batch     3 | loss: 9.7853407CurrentTrain: epoch 15, batch     0 | loss: 10.1350551CurrentTrain: epoch 15, batch     1 | loss: 12.0451499CurrentTrain: epoch 15, batch     2 | loss: 10.2547975CurrentTrain: epoch  1, batch     3 | loss: 7.0005555CurrentTrain: epoch 15, batch     0 | loss: 12.4225568CurrentTrain: epoch 15, batch     1 | loss: 10.4067124CurrentTrain: epoch 15, batch     2 | loss: 7.3785024CurrentTrain: epoch  1, batch     3 | loss: 8.8985620CurrentTrain: epoch 15, batch     0 | loss: 14.0546924CurrentTrain: epoch 15, batch     1 | loss: 16.5553866CurrentTrain: epoch 15, batch     2 | loss: 7.3536367CurrentTrain: epoch  1, batch     3 | loss: 8.6927564CurrentTrain: epoch 15, batch     0 | loss: 9.5651928CurrentTrain: epoch 15, batch     1 | loss: 9.6363459CurrentTrain: epoch 15, batch     2 | loss: 13.1938587CurrentTrain: epoch  1, batch     3 | loss: 6.7847107CurrentTrain: epoch 15, batch     0 | loss: 7.2065430CurrentTrain: epoch 15, batch     1 | loss: 4.6574663CurrentTrain: epoch 15, batch     2 | loss: 9.3296812CurrentTrain: epoch  1, batch     3 | loss: 6.8759634CurrentTrain: epoch 15, batch     0 | loss: 5.1834181CurrentTrain: epoch 15, batch     1 | loss: 13.2791272CurrentTrain: epoch 15, batch     2 | loss: 6.7496312CurrentTrain: epoch  1, batch     3 | loss: 6.6210877
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: rohan is the son of narayana murthy , founder of infosys , and his wife sudha murthy , an engineer and author of note .
Head Entity: narayana murthy
Tail Entity: sudha murthy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: elon musk, the CEO of spacex and tesla, was previously married to talulah riley, a talented actress and writer.  
Head Entity: elon musk  
Tail Entity: talulah riley  

Relation: spouse  
Context: barack obama, the 44th president of the united states, has been married to michelle obama since 1992, and they have two daughters together.  
Head Entity: barack obama  
Tail Entity: michelle obama  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: " i 'm a cuckoo " was belle & sebastian 's second single from " dear catastrophe waitress " , released on rough trade records in 2004 .
Head Entity: belle & sebastian
Tail Entity: rough trade records
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: The album "Random Access Memories" by Daft Punk was released under the Columbia Records label in 2013.  
Head Entity: Daft Punk  
Tail Entity: Columbia Records  

Relation: record label  
Context: Taylor Swift's latest album "Evermore" was produced by her own label, Republic Records, and released in 2020.  
Head Entity: Taylor Swift  
Tail Entity: Republic Records  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: " the many adventures of winnie the pooh " is a 1977 american animated buddy musical comedy film produced by walt disney productions and distributed by buena vista distribution .
Head Entity: the many adventures of winnie the pooh
Tail Entity: buena vista distribution
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: "Inception" is a 2010 science fiction film directed by Christopher Nolan and distributed by Warner Bros. Pictures.  
Head Entity: Inception  
Tail Entity: Warner Bros. Pictures  

Relation: distributor  
Context: "The Dark Knight" is a 2008 superhero film directed by Christopher Nolan and distributed by Warner Bros. Pictures.  
Head Entity: The Dark Knight  
Tail Entity: Warner Bros. Pictures  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: in 1796 carondelet commissioned pintado to conduct a survey of lands granted to ambrosio longue - épée in iberville county on the west bank of the mississippi river opposite baton rouge .
Head Entity: baton rouge
Tail Entity: mississippi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: the city of new orleans is situated on the banks of the mississippi river, making it a vibrant hub for trade and culture.  
Head Entity: new orleans  
Tail Entity: mississippi river  

Relation: located in or next to body of water  
Context: the picturesque village of lake placid is nestled beside the serene waters of mirror lake, attracting visitors year-round.  
Head Entity: lake placid  
Tail Entity: mirror lake  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: cbs and viacom split again in 2005 , but both are still owned by national amusements .
Head Entity: national amusements
Tail Entity: viacom
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Relation: subsidiary  
Context: In 2018, Facebook acquired Instagram, which now operates as a subsidiary under the social media giant.  
Head Entity: Facebook  
Tail Entity: Instagram  

Relation: subsidiary  
Context: Toyota has several subsidiaries, including Lexus, which focuses on luxury vehicles.  
Head Entity: Toyota  
Tail Entity: Lexus  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: he challenged seven other swimmers in heat four including ukraine 's rostyslav svanidze , a top 16 finalist in atlanta four years earlier .
Head Entity: rostyslav svanidze
Tail Entity: ukraine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: after moving to the united states, she became a prominent figure in the tech industry, representing her home country of india at various international conferences.  
Head Entity: she  
Tail Entity: india  

Relation: country of citizenship  
Context: during the tournament, the athlete proudly wore the colors of his homeland, showcasing his allegiance to spain while competing against other nations.  
Head Entity: the athlete  
Tail Entity: spain  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: nina cites " duffy , muse , and other pop and alternative artists " as her major influences .
Head Entity: duffy
Tail Entity: pop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the film "inception" is often categorized as a science fiction thriller, blending elements of action and psychological drama.  
Head Entity: inception  
Tail Entity: science fiction  

Relation: genre  
Context: the band "coldplay" is known for their alternative rock sound, which incorporates elements of pop and electronic music.  
Head Entity: coldplay  
Tail Entity: alternative rock  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: san lázaro is 660 km far from asunción and is located at the exact site of the confluence of the apa river with paraguay river , bordering brazil - chaco .
Head Entity: apa river
Tail Entity: paraguay river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: the mississippi river flows into the gulf of mexico, creating a rich delta ecosystem that supports diverse wildlife and plant species.  
Head Entity: mississippi river  
Tail Entity: gulf of mexico  

Relation: mouth of the watercourse  
Context: the thames river empties into the north sea, providing a vital shipping route for trade and commerce in the region.  
Head Entity: thames river  
Tail Entity: north sea  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: bradley halliday ( born 10 july 1995 ) is an english professional footballer who plays as a defender or a midfielder for league two club cambridge united .
Head Entity: bradley halliday
Tail Entity: defender
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: alex morgan ( born 2 july 1989 ) is an american soccer player who plays as a forward for the national team and club orlando pride in the national women's soccer league.  
Head Entity: alex morgan  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: lebron james ( born 30 december 1984 ) is an american professional basketball player who plays as a small forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: small forward  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: the stade louis ii in monaco was the venue for the uefa super cup every year since 1998 .
Head Entity: 1998
Tail Entity: uefa super cup
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2020 NBA season was significantly impacted by the COVID-19 pandemic, leading to a shortened schedule and a unique playoff format.  
Head Entity: 2020  
Tail Entity: NBA season  

Relation: sports season of league or competition  
Context: The 2021 Formula 1 season introduced new regulations and saw Lewis Hamilton competing for his eighth world championship title.  
Head Entity: 2021  
Tail Entity: Formula 1 season  
MemoryTrain:  epoch 15, batch     0 | loss: 3.9650134MemoryTrain:  epoch 15, batch     1 | loss: 4.5601794MemoryTrain:  epoch 15, batch     2 | loss: 4.9613972MemoryTrain:  epoch 15, batch     3 | loss: 7.1852209MemoryTrain:  epoch 15, batch     4 | loss: 4.8357743MemoryTrain:  epoch  9, batch     5 | loss: 6.2152869MemoryTrain:  epoch 15, batch     0 | loss: 3.8018756MemoryTrain:  epoch 15, batch     1 | loss: 4.0653018MemoryTrain:  epoch 15, batch     2 | loss: 4.6088064MemoryTrain:  epoch 15, batch     3 | loss: 5.3310701MemoryTrain:  epoch 15, batch     4 | loss: 4.2407523MemoryTrain:  epoch  9, batch     5 | loss: 5.9073407MemoryTrain:  epoch 15, batch     0 | loss: 4.3844462MemoryTrain:  epoch 15, batch     1 | loss: 3.6558960MemoryTrain:  epoch 15, batch     2 | loss: 3.8954717MemoryTrain:  epoch 15, batch     3 | loss: 4.8139143MemoryTrain:  epoch 15, batch     4 | loss: 4.6296443MemoryTrain:  epoch  9, batch     5 | loss: 1.8698824MemoryTrain:  epoch 15, batch     0 | loss: 4.7194607MemoryTrain:  epoch 15, batch     1 | loss: 4.0037540MemoryTrain:  epoch 15, batch     2 | loss: 5.7927332MemoryTrain:  epoch 15, batch     3 | loss: 3.4072501MemoryTrain:  epoch 15, batch     4 | loss: 6.4804566MemoryTrain:  epoch  9, batch     5 | loss: 4.6121671MemoryTrain:  epoch 15, batch     0 | loss: 6.5070707MemoryTrain:  epoch 15, batch     1 | loss: 2.7389300MemoryTrain:  epoch 15, batch     2 | loss: 4.9979700MemoryTrain:  epoch 15, batch     3 | loss: 3.3864419MemoryTrain:  epoch 15, batch     4 | loss: 3.4939546MemoryTrain:  epoch  9, batch     5 | loss: 9.4277465MemoryTrain:  epoch 15, batch     0 | loss: 2.4653043MemoryTrain:  epoch 15, batch     1 | loss: 3.0279193MemoryTrain:  epoch 15, batch     2 | loss: 5.6618477MemoryTrain:  epoch 15, batch     3 | loss: 2.9860075MemoryTrain:  epoch 15, batch     4 | loss: 2.2564042MemoryTrain:  epoch  9, batch     5 | loss: 8.9248051MemoryTrain:  epoch 15, batch     0 | loss: 3.4224394MemoryTrain:  epoch 15, batch     1 | loss: 2.3602130MemoryTrain:  epoch 15, batch     2 | loss: 3.4487477MemoryTrain:  epoch 15, batch     3 | loss: 2.7320914MemoryTrain:  epoch 15, batch     4 | loss: 1.9841007MemoryTrain:  epoch  9, batch     5 | loss: 4.2648999MemoryTrain:  epoch 15, batch     0 | loss: 1.9848430MemoryTrain:  epoch 15, batch     1 | loss: 2.5011049MemoryTrain:  epoch 15, batch     2 | loss: 2.6112124MemoryTrain:  epoch 15, batch     3 | loss: 2.0601234MemoryTrain:  epoch 15, batch     4 | loss: 2.0656805MemoryTrain:  epoch  9, batch     5 | loss: 4.3219825MemoryTrain:  epoch 15, batch     0 | loss: 3.4060288MemoryTrain:  epoch 15, batch     1 | loss: 4.2512974MemoryTrain:  epoch 15, batch     2 | loss: 4.3785925MemoryTrain:  epoch 15, batch     3 | loss: 5.7139914MemoryTrain:  epoch 15, batch     4 | loss: 2.5592934MemoryTrain:  epoch  9, batch     5 | loss: 1.5000740MemoryTrain:  epoch 15, batch     0 | loss: 4.2208711MemoryTrain:  epoch 15, batch     1 | loss: 2.7779239MemoryTrain:  epoch 15, batch     2 | loss: 2.3467793MemoryTrain:  epoch 15, batch     3 | loss: 1.8428210MemoryTrain:  epoch 15, batch     4 | loss: 4.1520832MemoryTrain:  epoch  9, batch     5 | loss: 6.4607142
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 45.31%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 46.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 51.04%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 55.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 60.94%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 63.89%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 67.61%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 71.15%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 72.32%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 72.08%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 73.44%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 73.90%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 73.96%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 75.62%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 75.60%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 76.14%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 76.09%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 77.00%   [EVAL] batch:   25 | acc: 81.25%,  total acc: 77.16%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 76.85%   [EVAL] batch:   27 | acc: 75.00%,  total acc: 76.79%   [EVAL] batch:   28 | acc: 75.00%,  total acc: 76.72%   [EVAL] batch:   29 | acc: 56.25%,  total acc: 76.04%   [EVAL] batch:   30 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 74.41%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 74.05%   [EVAL] batch:   33 | acc: 37.50%,  total acc: 72.98%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 72.50%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 71.53%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 71.11%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 70.56%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 70.67%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 70.94%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 71.04%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 70.98%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 71.37%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 71.16%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 70.14%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 69.29%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 68.22%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 67.58%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 66.58%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 66.00%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 67.31%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 67.92%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 68.52%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 69.09%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 69.64%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 70.18%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 70.69%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 71.19%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 71.56%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 72.03%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 72.48%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 72.12%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 88.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 89.34%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 89.47%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 90.48%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 90.06%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 89.40%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 88.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.18%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 89.96%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 90.09%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.42%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 90.73%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 90.82%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.10%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 90.62%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 90.18%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 90.10%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 90.20%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 90.30%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.54%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.01%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 91.13%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 91.05%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 90.97%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 90.90%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 90.43%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 90.23%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.43%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 90.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 90.32%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 90.26%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 90.33%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 90.51%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 90.23%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.29%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 90.35%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 90.19%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 90.36%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 90.42%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 90.57%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 90.52%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 90.28%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 89.75%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 89.42%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 89.02%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 88.71%   [EVAL] batch:   67 | acc: 81.25%,  total acc: 88.60%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 88.13%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 88.12%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 88.20%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 87.93%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 87.84%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 87.92%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 87.92%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 88.08%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 88.07%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 87.98%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 88.05%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 87.97%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 88.12%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 87.80%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 87.65%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 87.35%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 87.06%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 86.77%   [EVAL] batch:   86 | acc: 81.25%,  total acc: 86.71%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 86.51%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 86.66%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 86.88%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 87.02%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 87.16%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 87.43%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 87.57%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 87.69%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 87.82%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 87.94%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 88.06%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 88.18%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 88.24%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 88.29%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 88.34%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 88.45%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 88.50%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 88.14%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 87.67%   [EVAL] batch:  108 | acc: 43.75%,  total acc: 87.27%   [EVAL] batch:  109 | acc: 50.00%,  total acc: 86.93%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 86.66%   [EVAL] batch:  111 | acc: 56.25%,  total acc: 86.38%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 86.28%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 86.29%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 86.41%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 86.42%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 86.49%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 86.55%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 86.45%   [EVAL] batch:  119 | acc: 31.25%,  total acc: 85.99%   [EVAL] batch:  120 | acc: 37.50%,  total acc: 85.59%   [EVAL] batch:  121 | acc: 25.00%,  total acc: 85.09%   [EVAL] batch:  122 | acc: 31.25%,  total acc: 84.65%   [EVAL] batch:  123 | acc: 31.25%,  total acc: 84.22%   [EVAL] batch:  124 | acc: 25.00%,  total acc: 83.75%   [EVAL] batch:  125 | acc: 43.75%,  total acc: 83.43%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 83.07%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 82.81%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 82.56%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 82.31%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 82.25%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 82.24%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 82.38%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 82.42%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 82.41%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 82.44%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 82.57%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 82.56%   [EVAL] batch:  138 | acc: 87.50%,  total acc: 82.60%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 82.50%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 82.58%   [EVAL] batch:  141 | acc: 81.25%,  total acc: 82.57%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 82.52%   [EVAL] batch:  143 | acc: 93.75%,  total acc: 82.60%   [EVAL] batch:  144 | acc: 87.50%,  total acc: 82.63%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 82.58%   [EVAL] batch:  146 | acc: 87.50%,  total acc: 82.61%   [EVAL] batch:  147 | acc: 75.00%,  total acc: 82.56%   [EVAL] batch:  148 | acc: 87.50%,  total acc: 82.59%   [EVAL] batch:  149 | acc: 87.50%,  total acc: 82.62%   [EVAL] batch:  150 | acc: 81.25%,  total acc: 82.62%   [EVAL] batch:  151 | acc: 68.75%,  total acc: 82.52%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 82.48%   [EVAL] batch:  153 | acc: 75.00%,  total acc: 82.43%   [EVAL] batch:  154 | acc: 56.25%,  total acc: 82.26%   [EVAL] batch:  155 | acc: 43.75%,  total acc: 82.01%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 81.85%   [EVAL] batch:  157 | acc: 62.50%,  total acc: 81.72%   [EVAL] batch:  158 | acc: 37.50%,  total acc: 81.45%   [EVAL] batch:  159 | acc: 56.25%,  total acc: 81.29%   [EVAL] batch:  160 | acc: 37.50%,  total acc: 81.02%   [EVAL] batch:  161 | acc: 56.25%,  total acc: 80.86%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 80.67%   [EVAL] batch:  163 | acc: 75.00%,  total acc: 80.64%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 80.64%   [EVAL] batch:  165 | acc: 75.00%,  total acc: 80.61%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 80.54%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 80.58%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 80.47%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 80.15%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 79.86%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 79.51%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 79.26%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 78.92%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 78.68%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 78.80%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 78.92%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 79.04%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 79.16%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 79.27%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 79.39%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 79.50%   [EVAL] batch:  182 | acc: 100.00%,  total acc: 79.61%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 79.72%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 79.80%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 79.91%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 80.01%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 79.85%   
cur_acc:  ['0.9524', '0.8313', '0.7212']
his_acc:  ['0.9524', '0.8870', '0.7985']
CurrentTrain: epoch 15, batch     0 | loss: 11.4920952CurrentTrain: epoch 15, batch     1 | loss: 17.2427236CurrentTrain: epoch 15, batch     2 | loss: 18.7758877CurrentTrain: epoch  1, batch     3 | loss: 9.1744395CurrentTrain: epoch 15, batch     0 | loss: 15.0450781CurrentTrain: epoch 15, batch     1 | loss: 16.1718761CurrentTrain: epoch 15, batch     2 | loss: 15.0896439CurrentTrain: epoch  1, batch     3 | loss: 8.8559904CurrentTrain: epoch 15, batch     0 | loss: 10.2776568CurrentTrain: epoch 15, batch     1 | loss: 20.2595927CurrentTrain: epoch 15, batch     2 | loss: 17.1001814CurrentTrain: epoch  1, batch     3 | loss: 8.5276477CurrentTrain: epoch 15, batch     0 | loss: 13.0605430CurrentTrain: epoch 15, batch     1 | loss: 11.5768310CurrentTrain: epoch 15, batch     2 | loss: 10.8767505CurrentTrain: epoch  1, batch     3 | loss: 8.8653437CurrentTrain: epoch 15, batch     0 | loss: 7.0322078CurrentTrain: epoch 15, batch     1 | loss: 18.0753378CurrentTrain: epoch 15, batch     2 | loss: 11.6402294CurrentTrain: epoch  1, batch     3 | loss: 6.7836537CurrentTrain: epoch 15, batch     0 | loss: 9.1509337CurrentTrain: epoch 15, batch     1 | loss: 8.0355446CurrentTrain: epoch 15, batch     2 | loss: 18.6823538CurrentTrain: epoch  1, batch     3 | loss: 8.9638763CurrentTrain: epoch 15, batch     0 | loss: 10.2283666CurrentTrain: epoch 15, batch     1 | loss: 8.4528641CurrentTrain: epoch 15, batch     2 | loss: 16.5708317CurrentTrain: epoch  1, batch     3 | loss: 8.5405130CurrentTrain: epoch 15, batch     0 | loss: 9.2794982CurrentTrain: epoch 15, batch     1 | loss: 9.4750514CurrentTrain: epoch 15, batch     2 | loss: 7.5388470CurrentTrain: epoch  1, batch     3 | loss: 7.9861462CurrentTrain: epoch 15, batch     0 | loss: 8.5202525CurrentTrain: epoch 15, batch     1 | loss: 11.4222717CurrentTrain: epoch 15, batch     2 | loss: 10.6311095CurrentTrain: epoch  1, batch     3 | loss: 5.4965681CurrentTrain: epoch 15, batch     0 | loss: 6.7889768CurrentTrain: epoch 15, batch     1 | loss: 19.6249658CurrentTrain: epoch 15, batch     2 | loss: 7.9209651CurrentTrain: epoch  1, batch     3 | loss: 15.6825678
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: ( 1988 ) , " night watch " ( 2004 ) , " day watch " ( 2006 ) and " legend № 17 " ( 2013 ) .
Head Entity: day watch
Tail Entity: night watch
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: ( 1995 ) , " The Phantom Menace " ( 1999 ) , " Attack of the Clones " ( 2002 ) and " Revenge of the Sith " ( 2005 ) .  
Head Entity: Attack of the Clones  
Tail Entity: The Phantom Menace  

Relation: follows  
Context: ( 2010 ) , " The Hunger Games " ( 2012 ) , " Catching Fire " ( 2013 ) and " Mockingjay " ( 2014 ) .  
Head Entity: Catching Fire  
Tail Entity: The Hunger Games  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: dick packer was a u.s. soccer center forward who was a member of the u.s. team at the 1956 summer olympics .
Head Entity: dick packer
Tail Entity: soccer
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: serena williams is a professional tennis player who has won numerous grand slam titles throughout her career.  
Head Entity: serena williams  
Tail Entity: tennis  

Relation: sport  
Context: lebron james is known for his exceptional skills in basketball and has played for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: basketball  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael, the youngest son, had always looked up to his father, robert, for guidance and support.  
Head Entity: michael  
Tail Entity: robert  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: since 2009 , culshaw has starred in the bbc one comedy sketch show " the impressions show " alongside debra stephenson .
Head Entity: the impressions show
Tail Entity: bbc one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: The popular series "Breaking Bad" first aired on AMC, captivating audiences with its intense storytelling and character development.  
Head Entity: Breaking Bad  
Tail Entity: AMC  

Relation: original network  
Context: "Friends" became a cultural phenomenon after its debut on NBC, attracting millions of viewers each week during its run.  
Head Entity: Friends  
Tail Entity: NBC  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: he was also nominated twice for the filmfare best telugu actor award , for the films bhale bhale magadivoy and " gentleman " .
Head Entity: gentleman
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: The movie "Parasite" received critical acclaim and was originally produced in Korean, showcasing the unique storytelling style of South Korean cinema.  
Head Entity: Parasite  
Tail Entity: Korean  

Relation: original language of film or TV show  
Context: The animated series "Avatar: The Last Airbender" was created in English, captivating audiences with its rich world-building and character development.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: in the summer of 2013 he returned to montenegro and joined with fk rudar pljevlja , playing with the club for two seasons in the montenegrin first league .
Head Entity: fk rudar pljevlja
Tail Entity: montenegrin first league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: After a successful stint in the English Premier League, the player signed with FC Barcelona, competing in La Liga for the next three seasons.  
Head Entity: FC Barcelona  
Tail Entity: La Liga  

Relation: league  
Context: The athlete made headlines when he transferred to the New York Red Bulls, where he played in Major League Soccer for several years.  
Head Entity: New York Red Bulls  
Tail Entity: Major League Soccer  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: she is supposedly the third and youngest child of seti i and tuya , and the younger sister of ramesses ii and tia .
Head Entity: tia
Tail Entity: tuya
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: Cleopatra was the daughter of Ptolemy XII and his wife, who was also her mother, Cleopatra VI.  
Head Entity: Cleopatra  
Tail Entity: Cleopatra VI  

Relation: mother  
Context: In Norse mythology, Frigg is known as the mother of Baldr, the god of light and purity.  
Head Entity: Baldr  
Tail Entity: Frigg  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres, showcasing his talent on the cello.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: a u.s. government - funded $ 36 million bridge over the panj river connects sher khan bandar in afghanistan with nizhniy pyanzh in tajikistan , which transport more than 150 trucks or 1,000 cars daily .
Head Entity: sher khan bandar
Tail Entity: afghanistan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the ancient city of petra, located in southern jordan, is famous for its rock-cut architecture and water conduit system, attracting thousands of tourists each year.  
Head Entity: petra  
Tail Entity: jordan  

Relation: country  
Context: the great wall of china, a series of fortifications made of various materials, stretches across northern china and is a UNESCO World Heritage site.  
Head Entity: great wall of china  
Tail Entity: china  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: klaus kinski , who would play dracula himself nine years later in " nosferatu the vampyre " , is also featured in the film as renfield .
Head Entity: nosferatu the vampyre
Tail Entity: renfield
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: in the animated series "avatar: the last airbender," zuko is a complex character who struggles with his identity and family legacy.  
Head Entity: avatar: the last airbender  
Tail Entity: zuko  

Relation: characters  
Context: the novel "pride and prejudice" features elizabeth bennet, who navigates societal expectations and her feelings for mr. darcy.  
Head Entity: pride and prejudice  
Tail Entity: elizabeth bennet  
MemoryTrain:  epoch 15, batch     0 | loss: 5.8466261MemoryTrain:  epoch 15, batch     1 | loss: 4.0494943MemoryTrain:  epoch 15, batch     2 | loss: 3.6302855MemoryTrain:  epoch 15, batch     3 | loss: 3.5962766MemoryTrain:  epoch 15, batch     4 | loss: 5.5667189MemoryTrain:  epoch 15, batch     5 | loss: 3.3664186MemoryTrain:  epoch 15, batch     6 | loss: 5.3543132MemoryTrain:  epoch  7, batch     7 | loss: 2.8345909MemoryTrain:  epoch 15, batch     0 | loss: 3.3624247MemoryTrain:  epoch 15, batch     1 | loss: 5.0506310MemoryTrain:  epoch 15, batch     2 | loss: 2.9083805MemoryTrain:  epoch 15, batch     3 | loss: 4.0635961MemoryTrain:  epoch 15, batch     4 | loss: 5.0861094MemoryTrain:  epoch 15, batch     5 | loss: 4.3734538MemoryTrain:  epoch 15, batch     6 | loss: 3.8077196MemoryTrain:  epoch  7, batch     7 | loss: 1.9934609MemoryTrain:  epoch 15, batch     0 | loss: 4.4791868MemoryTrain:  epoch 15, batch     1 | loss: 5.5491241MemoryTrain:  epoch 15, batch     2 | loss: 9.9690538MemoryTrain:  epoch 15, batch     3 | loss: 4.3555169MemoryTrain:  epoch 15, batch     4 | loss: 3.1070491MemoryTrain:  epoch 15, batch     5 | loss: 2.8157199MemoryTrain:  epoch 15, batch     6 | loss: 3.9973194MemoryTrain:  epoch  7, batch     7 | loss: 2.9847171MemoryTrain:  epoch 15, batch     0 | loss: 2.4947783MemoryTrain:  epoch 15, batch     1 | loss: 2.1732495MemoryTrain:  epoch 15, batch     2 | loss: 3.2125669MemoryTrain:  epoch 15, batch     3 | loss: 2.7131500MemoryTrain:  epoch 15, batch     4 | loss: 2.4807188MemoryTrain:  epoch 15, batch     5 | loss: 3.1842770MemoryTrain:  epoch 15, batch     6 | loss: 2.7719368MemoryTrain:  epoch  7, batch     7 | loss: 1.5761236MemoryTrain:  epoch 15, batch     0 | loss: 3.5018460MemoryTrain:  epoch 15, batch     1 | loss: 3.4129830MemoryTrain:  epoch 15, batch     2 | loss: 4.1321541MemoryTrain:  epoch 15, batch     3 | loss: 2.8348643MemoryTrain:  epoch 15, batch     4 | loss: 2.5442755MemoryTrain:  epoch 15, batch     5 | loss: 2.0474883MemoryTrain:  epoch 15, batch     6 | loss: 1.6275309MemoryTrain:  epoch  7, batch     7 | loss: 3.9460149MemoryTrain:  epoch 15, batch     0 | loss: 2.1743994MemoryTrain:  epoch 15, batch     1 | loss: 2.7202415MemoryTrain:  epoch 15, batch     2 | loss: 3.9336054MemoryTrain:  epoch 15, batch     3 | loss: 2.1427581MemoryTrain:  epoch 15, batch     4 | loss: 1.8716852MemoryTrain:  epoch 15, batch     5 | loss: 4.9259383MemoryTrain:  epoch 15, batch     6 | loss: 2.5298408MemoryTrain:  epoch  7, batch     7 | loss: 4.1600414MemoryTrain:  epoch 15, batch     0 | loss: 1.9744908MemoryTrain:  epoch 15, batch     1 | loss: 1.7786192MemoryTrain:  epoch 15, batch     2 | loss: 1.7143818MemoryTrain:  epoch 15, batch     3 | loss: 3.3636784MemoryTrain:  epoch 15, batch     4 | loss: 1.4701567MemoryTrain:  epoch 15, batch     5 | loss: 6.8572353MemoryTrain:  epoch 15, batch     6 | loss: 1.8743110MemoryTrain:  epoch  7, batch     7 | loss: 1.6460233MemoryTrain:  epoch 15, batch     0 | loss: 4.4148024MemoryTrain:  epoch 15, batch     1 | loss: 4.0945467MemoryTrain:  epoch 15, batch     2 | loss: 1.6877662MemoryTrain:  epoch 15, batch     3 | loss: 1.6544324MemoryTrain:  epoch 15, batch     4 | loss: 1.4354222MemoryTrain:  epoch 15, batch     5 | loss: 1.6301356MemoryTrain:  epoch 15, batch     6 | loss: 1.9710106MemoryTrain:  epoch  7, batch     7 | loss: 1.7754731MemoryTrain:  epoch 15, batch     0 | loss: 4.5624848MemoryTrain:  epoch 15, batch     1 | loss: 3.6358337MemoryTrain:  epoch 15, batch     2 | loss: 3.1550274MemoryTrain:  epoch 15, batch     3 | loss: 4.6344973MemoryTrain:  epoch 15, batch     4 | loss: 1.6901360MemoryTrain:  epoch 15, batch     5 | loss: 1.3805888MemoryTrain:  epoch 15, batch     6 | loss: 1.8399422MemoryTrain:  epoch  7, batch     7 | loss: 1.4722264MemoryTrain:  epoch 15, batch     0 | loss: 3.9103575MemoryTrain:  epoch 15, batch     1 | loss: 2.3157048MemoryTrain:  epoch 15, batch     2 | loss: 2.4792336MemoryTrain:  epoch 15, batch     3 | loss: 1.8977603MemoryTrain:  epoch 15, batch     4 | loss: 2.7014318MemoryTrain:  epoch 15, batch     5 | loss: 1.6295981MemoryTrain:  epoch 15, batch     6 | loss: 2.1728734MemoryTrain:  epoch  7, batch     7 | loss: 1.3463856
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 25.00%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 26.56%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 25.00%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 27.08%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 30.36%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 35.94%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 39.58%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 43.75%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 46.59%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 48.44%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 49.52%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 50.83%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 48.83%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 50.37%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 51.39%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 51.64%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 54.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 57.95%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 59.78%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 61.20%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 62.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 63.70%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 65.05%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 66.07%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 67.24%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 68.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 71.21%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 72.50%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 73.09%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 73.65%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 74.01%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 73.56%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 73.44%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 72.71%   [EVAL] batch:   41 | acc: 37.50%,  total acc: 71.88%   [EVAL] batch:   42 | acc: 25.00%,  total acc: 70.78%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 70.74%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 71.39%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 71.74%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 72.34%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 72.79%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 73.62%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 73.41%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 73.32%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 73.23%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 73.26%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 73.30%   [EVAL] batch:   55 | acc: 37.50%,  total acc: 72.66%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 72.37%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 72.63%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 72.67%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 72.81%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 72.95%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 73.39%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 72.92%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 82.95%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 83.85%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 84.62%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 85.27%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 86.76%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 87.83%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 87.81%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 87.80%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 86.68%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 86.72%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 86.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.02%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 87.72%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 87.93%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.71%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.87%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 88.60%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 88.21%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 88.19%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 88.34%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 88.32%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.62%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.91%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.18%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 89.39%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 89.06%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 89.03%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 89.13%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 88.70%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 88.54%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.78%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 88.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 88.73%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 88.70%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 88.80%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 89.00%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 88.75%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 88.84%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 88.82%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 88.25%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 87.92%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 87.92%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 88.01%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 87.90%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 87.70%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 86.72%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 86.35%   [EVAL] batch:   65 | acc: 43.75%,  total acc: 85.70%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 84.98%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 84.47%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 83.97%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 83.84%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 83.80%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 83.51%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 83.39%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 83.45%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 83.50%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 83.63%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 83.69%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 83.65%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 83.78%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 83.95%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 83.54%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 83.13%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 82.74%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 82.35%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 82.05%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 81.75%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 81.61%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 81.81%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 82.01%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 82.34%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 82.53%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 82.71%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 82.89%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 83.07%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 83.25%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 83.42%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 83.91%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 84.01%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 84.16%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 84.31%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 84.46%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 84.55%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 84.23%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 83.51%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 82.91%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 82.27%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 81.81%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 81.42%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 81.31%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 81.36%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 81.52%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 81.68%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 81.84%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 81.88%   [EVAL] batch:  119 | acc: 12.50%,  total acc: 81.30%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 80.73%   [EVAL] batch:  121 | acc: 12.50%,  total acc: 80.17%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 79.57%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 78.98%   [EVAL] batch:  124 | acc: 18.75%,  total acc: 78.50%   [EVAL] batch:  125 | acc: 37.50%,  total acc: 78.17%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 77.95%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 77.59%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 77.28%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 77.12%   [EVAL] batch:  130 | acc: 56.25%,  total acc: 76.96%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 77.04%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 77.21%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 77.29%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 77.41%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 77.57%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 77.74%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 77.81%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 77.65%   [EVAL] batch:  139 | acc: 62.50%,  total acc: 77.54%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 77.62%   [EVAL] batch:  141 | acc: 81.25%,  total acc: 77.64%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 77.58%   [EVAL] batch:  143 | acc: 81.25%,  total acc: 77.60%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 77.72%   [EVAL] batch:  145 | acc: 68.75%,  total acc: 77.65%   [EVAL] batch:  146 | acc: 68.75%,  total acc: 77.59%   [EVAL] batch:  147 | acc: 75.00%,  total acc: 77.58%   [EVAL] batch:  148 | acc: 87.50%,  total acc: 77.64%   [EVAL] batch:  149 | acc: 75.00%,  total acc: 77.62%   [EVAL] batch:  150 | acc: 87.50%,  total acc: 77.69%   [EVAL] batch:  151 | acc: 68.75%,  total acc: 77.63%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 77.61%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 77.68%   [EVAL] batch:  154 | acc: 50.00%,  total acc: 77.50%   [EVAL] batch:  155 | acc: 50.00%,  total acc: 77.32%   [EVAL] batch:  156 | acc: 50.00%,  total acc: 77.15%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 77.02%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 76.89%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 76.80%   [EVAL] batch:  160 | acc: 56.25%,  total acc: 76.67%   [EVAL] batch:  161 | acc: 56.25%,  total acc: 76.54%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 76.46%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 76.41%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 76.36%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 76.32%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 76.31%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 76.30%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 76.26%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 75.96%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 75.69%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 75.36%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 75.22%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 74.89%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 74.75%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 74.89%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 75.04%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 75.18%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 75.31%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 75.45%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 75.59%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 75.58%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 75.48%   [EVAL] batch:  183 | acc: 62.50%,  total acc: 75.41%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 75.37%   [EVAL] batch:  185 | acc: 56.25%,  total acc: 75.27%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 75.30%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 75.23%   [EVAL] batch:  188 | acc: 25.00%,  total acc: 74.97%   [EVAL] batch:  189 | acc: 25.00%,  total acc: 74.70%   [EVAL] batch:  190 | acc: 18.75%,  total acc: 74.41%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 74.15%   [EVAL] batch:  192 | acc: 43.75%,  total acc: 74.00%   [EVAL] batch:  193 | acc: 25.00%,  total acc: 73.74%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 73.75%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 73.69%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 73.73%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 73.74%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 73.74%   [EVAL] batch:  199 | acc: 68.75%,  total acc: 73.72%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 73.66%   [EVAL] batch:  201 | acc: 37.50%,  total acc: 73.48%   [EVAL] batch:  202 | acc: 43.75%,  total acc: 73.34%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 73.28%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 73.29%   [EVAL] batch:  205 | acc: 37.50%,  total acc: 73.12%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 73.22%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 73.35%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 73.47%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 73.57%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 73.67%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 73.79%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 73.86%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 73.98%   [EVAL] batch:  214 | acc: 93.75%,  total acc: 74.07%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 74.19%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 74.31%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 74.43%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 74.54%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 74.66%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 74.77%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 74.83%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 74.92%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 75.11%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 75.08%   [EVAL] batch:  226 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:  227 | acc: 56.25%,  total acc: 74.92%   [EVAL] batch:  228 | acc: 37.50%,  total acc: 74.75%   [EVAL] batch:  229 | acc: 25.00%,  total acc: 74.54%   [EVAL] batch:  230 | acc: 50.00%,  total acc: 74.43%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 74.52%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 74.60%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 74.68%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 74.79%   [EVAL] batch:  235 | acc: 87.50%,  total acc: 74.84%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 74.92%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 74.92%   [EVAL] batch:  238 | acc: 75.00%,  total acc: 74.92%   [EVAL] batch:  239 | acc: 68.75%,  total acc: 74.90%   [EVAL] batch:  240 | acc: 62.50%,  total acc: 74.84%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 74.87%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 74.82%   [EVAL] batch:  243 | acc: 37.50%,  total acc: 74.67%   [EVAL] batch:  244 | acc: 81.25%,  total acc: 74.69%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 74.72%   [EVAL] batch:  246 | acc: 68.75%,  total acc: 74.70%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 74.72%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 74.80%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 74.88%   
cur_acc:  ['0.9524', '0.8313', '0.7212', '0.7292']
his_acc:  ['0.9524', '0.8870', '0.7985', '0.7488']
CurrentTrain: epoch 15, batch     0 | loss: 11.5556838CurrentTrain: epoch 15, batch     1 | loss: 10.5828543CurrentTrain: epoch 15, batch     2 | loss: 12.0938237CurrentTrain: epoch  1, batch     3 | loss: 9.8483256CurrentTrain: epoch 15, batch     0 | loss: 9.1116590CurrentTrain: epoch 15, batch     1 | loss: 8.7157343CurrentTrain: epoch 15, batch     2 | loss: 9.2325683CurrentTrain: epoch  1, batch     3 | loss: 9.3136722CurrentTrain: epoch 15, batch     0 | loss: 14.7064666CurrentTrain: epoch 15, batch     1 | loss: 7.3790289CurrentTrain: epoch 15, batch     2 | loss: 15.9968651CurrentTrain: epoch  1, batch     3 | loss: 9.5573663CurrentTrain: epoch 15, batch     0 | loss: 9.1473653CurrentTrain: epoch 15, batch     1 | loss: 11.7977804CurrentTrain: epoch 15, batch     2 | loss: 9.9990733CurrentTrain: epoch  1, batch     3 | loss: 7.1825520CurrentTrain: epoch 15, batch     0 | loss: 6.8911773CurrentTrain: epoch 15, batch     1 | loss: 15.5691327CurrentTrain: epoch 15, batch     2 | loss: 12.8263902CurrentTrain: epoch  1, batch     3 | loss: 5.7109368CurrentTrain: epoch 15, batch     0 | loss: 7.5961677CurrentTrain: epoch 15, batch     1 | loss: 6.2613234CurrentTrain: epoch 15, batch     2 | loss: 9.5238538CurrentTrain: epoch  1, batch     3 | loss: 6.2340689CurrentTrain: epoch 15, batch     0 | loss: 7.0608751CurrentTrain: epoch 15, batch     1 | loss: 12.2786151CurrentTrain: epoch 15, batch     2 | loss: 7.8267340CurrentTrain: epoch  1, batch     3 | loss: 16.1175983CurrentTrain: epoch 15, batch     0 | loss: 7.4998157CurrentTrain: epoch 15, batch     1 | loss: 8.8180891CurrentTrain: epoch 15, batch     2 | loss: 9.0028494CurrentTrain: epoch  1, batch     3 | loss: 6.1187847CurrentTrain: epoch 15, batch     0 | loss: 5.1293996CurrentTrain: epoch 15, batch     1 | loss: 6.4630583CurrentTrain: epoch 15, batch     2 | loss: 7.0178947CurrentTrain: epoch  1, batch     3 | loss: 5.3243409CurrentTrain: epoch 15, batch     0 | loss: 8.3738695CurrentTrain: epoch 15, batch     1 | loss: 6.0103701CurrentTrain: epoch 15, batch     2 | loss: 13.4051250CurrentTrain: epoch  1, batch     3 | loss: 5.6976411
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: karl johan aarønes ( 8 may 1900 – 12 august 1969 ) was a norwegian politician for the labour party .
Head Entity: karl johan aarønes
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: barack obama served as a member of the democratic party during his presidency from 2009 to 2017.  
Head Entity: barack obama  
Tail Entity: democratic party  

Relation: member of political party  
Context: angela merkel was a prominent member of the christian democratic union throughout her political career in germany.  
Head Entity: angela merkel  
Tail Entity: christian democratic union  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: it is a partly fictional biography , adapted by shane connaughton and jim sheridan from the book " my left foot " by christy brown .
Head Entity: my left foot
Tail Entity: christy brown
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film is based on the novel "the great gatsby" by f. scott fitzgerald, capturing the essence of the roaring twenties.  
Head Entity: the great gatsby  
Tail Entity: f. scott fitzgerald  

Relation: after a work by  
Context: this painting draws inspiration from the famous artwork "starry night" created by vincent van gogh, showcasing swirling skies and vibrant colors.  
Head Entity: starry night  
Tail Entity: vincent van gogh  
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: the influential victorian art critic john ruskin expounded theories about social justice in " unto this last " ( 1860 ) .
Head Entity: john ruskin
Tail Entity: unto this last
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: the renowned physicist albert einstein published his groundbreaking theory of relativity in "on the electrodynamics of moving bodies" (1905).  
Head Entity: albert einstein  
Tail Entity: on the electrodynamics of moving bodies  

Relation: notable work  
Context: the celebrated author j.k. rowling captivated readers with her first book in the harry potter series, "harry potter and the philosopher's stone" (1997).  
Head Entity: j.k. rowling  
Tail Entity: harry potter and the philosopher's stone  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: in 1986 fram traveled to poland to play against katowice in a very even duel fram eventually lost . sparta prague came to reykjavík 1987 to play against fram reykjavik at laugardalsvöllur stadium .
Head Entity: fram reykjavik
Tail Entity: reykjavík
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: in 2001, the tech company apple inc. moved its headquarters to cupertino, california, where it has since developed numerous innovative products.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: headquarters location  
Context: the multinational corporation unilever established its headquarters in rotterdam, netherlands, in the early 20th century, becoming a leader in consumer goods.  
Head Entity: unilever  
Tail Entity: rotterdam  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most recognized as the wolf, which belongs to the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of living organisms, the class Mammalia encompasses all mammals, which are further divided into various orders, including Primates.  
Head Entity: Mammalia  
Tail Entity: class  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: elspeth frances garman is professor of molecular biophysics at the university of oxford and a former president of the british crystallographic association .
Head Entity: elspeth frances garman
Tail Entity: molecular biophysics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: dr. amanda jones is a leading researcher in artificial intelligence and currently works at the tech innovation lab in silicon valley.  
Head Entity: dr. amanda jones  
Tail Entity: artificial intelligence  

Relation: field of work  
Context: the world health organization has appointed dr. rajesh kumar as the new director of global health initiatives, focusing on infectious diseases.  
Head Entity: dr. rajesh kumar  
Tail Entity: infectious diseases  
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: wjet erie , pennsylvania was his first official radio job outside of duties performed in the u.s. air force .
Head Entity: wjet
Tail Entity: erie , pennsylvania
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: wxyz television is the primary news station serving the city of springfield, known for its local coverage.  
Head Entity: wxyz  
Tail Entity: springfield  

Relation: licensed to broadcast to  
Context: kqed is a public television station that provides educational programming to the residents of san francisco.  
Head Entity: kqed  
Tail Entity: san francisco  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: hd 32518 b is an extrasolar planet which orbits the k - type giant star hd 32518 , located approximately 383 light years away in the constellation camelopardalis .
Head Entity: hd 32518
Tail Entity: camelopardalis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: the star sirius, known as the brightest star in the night sky, is located in the constellation canis major and is approximately 8.6 light years away from Earth.  
Head Entity: sirius  
Tail Entity: canis major  

Relation: constellation  
Context: the andromeda galaxy, which is the nearest spiral galaxy to the Milky Way, can be found in the constellation andromeda, making it a prominent feature in the night sky.  
Head Entity: andromeda galaxy  
Tail Entity: andromeda  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: aéroport is the place of gustaf iii airport , in " quartier " saint - jean of saint barthélemy in the caribbean .
Head Entity: gustaf iii airport
Tail Entity: saint barthélemy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The central train station in the city is the main hub for all regional trains, connecting travelers to various destinations including the beautiful coastal town of Brighton.  
Head Entity: central train station  
Tail Entity: Brighton  

Relation: place served by transport hub  
Context: The international airport serves as a gateway for tourists visiting the historic city of Florence, offering flights to numerous global destinations.  
Head Entity: international airport  
Tail Entity: Florence  
MemoryTrain:  epoch 15, batch     0 | loss: 1.8110195MemoryTrain:  epoch 15, batch     1 | loss: 4.1935715MemoryTrain:  epoch 15, batch     2 | loss: 7.5323706MemoryTrain:  epoch 15, batch     3 | loss: 2.3757739MemoryTrain:  epoch 15, batch     4 | loss: 4.3844316MemoryTrain:  epoch 15, batch     5 | loss: 7.1941924MemoryTrain:  epoch 15, batch     6 | loss: 6.4594031MemoryTrain:  epoch 15, batch     7 | loss: 2.8445668MemoryTrain:  epoch 15, batch     8 | loss: 4.7981298MemoryTrain:  epoch  5, batch     9 | loss: 12.9261785MemoryTrain:  epoch 15, batch     0 | loss: 3.4469238MemoryTrain:  epoch 15, batch     1 | loss: 2.9189981MemoryTrain:  epoch 15, batch     2 | loss: 2.7511851MemoryTrain:  epoch 15, batch     3 | loss: 2.3866795MemoryTrain:  epoch 15, batch     4 | loss: 2.2555287MemoryTrain:  epoch 15, batch     5 | loss: 4.2620217MemoryTrain:  epoch 15, batch     6 | loss: 2.8284258MemoryTrain:  epoch 15, batch     7 | loss: 2.3462147MemoryTrain:  epoch 15, batch     8 | loss: 2.1354164MemoryTrain:  epoch  5, batch     9 | loss: 8.3819989MemoryTrain:  epoch 15, batch     0 | loss: 2.0553396MemoryTrain:  epoch 15, batch     1 | loss: 1.8321411MemoryTrain:  epoch 15, batch     2 | loss: 3.9444987MemoryTrain:  epoch 15, batch     3 | loss: 1.8529300MemoryTrain:  epoch 15, batch     4 | loss: 2.3306325MemoryTrain:  epoch 15, batch     5 | loss: 2.2587844MemoryTrain:  epoch 15, batch     6 | loss: 4.1735171MemoryTrain:  epoch 15, batch     7 | loss: 3.7453922MemoryTrain:  epoch 15, batch     8 | loss: 2.9336010MemoryTrain:  epoch  5, batch     9 | loss: 8.1240682MemoryTrain:  epoch 15, batch     0 | loss: 1.9440557MemoryTrain:  epoch 15, batch     1 | loss: 1.7752095MemoryTrain:  epoch 15, batch     2 | loss: 1.9747810MemoryTrain:  epoch 15, batch     3 | loss: 1.9372208MemoryTrain:  epoch 15, batch     4 | loss: 2.7902652MemoryTrain:  epoch 15, batch     5 | loss: 1.4552579MemoryTrain:  epoch 15, batch     6 | loss: 1.6899208MemoryTrain:  epoch 15, batch     7 | loss: 1.8041839MemoryTrain:  epoch 15, batch     8 | loss: 2.5910887MemoryTrain:  epoch  5, batch     9 | loss: 8.6161564MemoryTrain:  epoch 15, batch     0 | loss: 3.9624598MemoryTrain:  epoch 15, batch     1 | loss: 4.3748673MemoryTrain:  epoch 15, batch     2 | loss: 2.4116536MemoryTrain:  epoch 15, batch     3 | loss: 1.5030503MemoryTrain:  epoch 15, batch     4 | loss: 3.3669211MemoryTrain:  epoch 15, batch     5 | loss: 2.6180062MemoryTrain:  epoch 15, batch     6 | loss: 4.0643076MemoryTrain:  epoch 15, batch     7 | loss: 1.6623407MemoryTrain:  epoch 15, batch     8 | loss: 1.4618125MemoryTrain:  epoch  5, batch     9 | loss: 8.3194943MemoryTrain:  epoch 15, batch     0 | loss: 8.9945824MemoryTrain:  epoch 15, batch     1 | loss: 1.7007554MemoryTrain:  epoch 15, batch     2 | loss: 1.6156915MemoryTrain:  epoch 15, batch     3 | loss: 3.9612453MemoryTrain:  epoch 15, batch     4 | loss: 1.5455410MemoryTrain:  epoch 15, batch     5 | loss: 2.6777825MemoryTrain:  epoch 15, batch     6 | loss: 1.8920700MemoryTrain:  epoch 15, batch     7 | loss: 1.6286328MemoryTrain:  epoch 15, batch     8 | loss: 1.8372151MemoryTrain:  epoch  5, batch     9 | loss: 14.8439765MemoryTrain:  epoch 15, batch     0 | loss: 3.9077961MemoryTrain:  epoch 15, batch     1 | loss: 2.5932233MemoryTrain:  epoch 15, batch     2 | loss: 2.2111133MemoryTrain:  epoch 15, batch     3 | loss: 1.8232078MemoryTrain:  epoch 15, batch     4 | loss: 2.3861591MemoryTrain:  epoch 15, batch     5 | loss: 1.5941858MemoryTrain:  epoch 15, batch     6 | loss: 2.0462853MemoryTrain:  epoch 15, batch     7 | loss: 3.9912306MemoryTrain:  epoch 15, batch     8 | loss: 3.7678080MemoryTrain:  epoch  5, batch     9 | loss: 7.3223311MemoryTrain:  epoch 15, batch     0 | loss: 1.4775139MemoryTrain:  epoch 15, batch     1 | loss: 1.9086516MemoryTrain:  epoch 15, batch     2 | loss: 1.5940583MemoryTrain:  epoch 15, batch     3 | loss: 3.8090441MemoryTrain:  epoch 15, batch     4 | loss: 1.8022280MemoryTrain:  epoch 15, batch     5 | loss: 1.5550161MemoryTrain:  epoch 15, batch     6 | loss: 1.7397636MemoryTrain:  epoch 15, batch     7 | loss: 1.7043164MemoryTrain:  epoch 15, batch     8 | loss: 2.3045691MemoryTrain:  epoch  5, batch     9 | loss: 8.0246030MemoryTrain:  epoch 15, batch     0 | loss: 5.8945886MemoryTrain:  epoch 15, batch     1 | loss: 4.1259987MemoryTrain:  epoch 15, batch     2 | loss: 1.9535207MemoryTrain:  epoch 15, batch     3 | loss: 1.6074736MemoryTrain:  epoch 15, batch     4 | loss: 1.6804617MemoryTrain:  epoch 15, batch     5 | loss: 1.9545522MemoryTrain:  epoch 15, batch     6 | loss: 1.7734126MemoryTrain:  epoch 15, batch     7 | loss: 2.5057696MemoryTrain:  epoch 15, batch     8 | loss: 1.5516308MemoryTrain:  epoch  5, batch     9 | loss: 8.0785966MemoryTrain:  epoch 15, batch     0 | loss: 1.7119233MemoryTrain:  epoch 15, batch     1 | loss: 4.2528441MemoryTrain:  epoch 15, batch     2 | loss: 2.6574005MemoryTrain:  epoch 15, batch     3 | loss: 1.8364449MemoryTrain:  epoch 15, batch     4 | loss: 1.4607928MemoryTrain:  epoch 15, batch     5 | loss: 1.5364852MemoryTrain:  epoch 15, batch     6 | loss: 1.7729039MemoryTrain:  epoch 15, batch     7 | loss: 1.5441878MemoryTrain:  epoch 15, batch     8 | loss: 1.5563899MemoryTrain:  epoch  5, batch     9 | loss: 8.1628323
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 93.75%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 91.41%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 86.46%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 87.08%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 85.29%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 84.72%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 83.88%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 82.19%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 82.74%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 82.10%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 81.79%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 81.51%   [EVAL] batch:   24 | acc: 43.75%,  total acc: 80.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.77%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.48%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 82.76%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.87%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 83.98%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 84.09%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 83.64%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 83.11%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 83.06%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.49%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.91%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.30%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 84.67%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 85.03%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 85.23%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 85.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 85.87%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 86.17%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 86.73%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 87.00%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 87.01%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 87.14%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 87.26%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 87.38%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 87.39%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 87.61%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 87.72%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 87.82%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 88.02%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 88.22%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 88.21%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 87.60%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 77.78%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 75.62%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 73.30%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 70.83%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 71.63%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 72.77%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 73.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 75.39%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 76.84%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 78.95%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 78.57%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 76.90%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 77.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.94%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 80.17%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.83%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.45%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 81.84%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 81.99%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 81.79%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 81.94%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 82.26%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 82.57%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.01%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.44%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 83.84%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 84.08%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 84.30%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 84.44%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 84.24%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 83.91%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 83.72%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 83.82%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 83.89%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 84.08%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 83.91%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 83.52%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 83.71%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 83.55%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 83.19%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 83.05%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 83.12%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 83.20%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 83.06%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 83.23%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 82.32%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 82.12%   [EVAL] batch:   65 | acc: 50.00%,  total acc: 81.63%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 80.88%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 80.42%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 79.98%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 79.84%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 79.60%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 79.71%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 79.73%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 79.83%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 79.93%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 80.03%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 80.05%   [EVAL] batch:   78 | acc: 100.00%,  total acc: 80.30%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 80.47%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 80.71%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 80.56%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 80.42%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 80.21%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 80.00%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 79.87%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 79.74%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 79.85%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 80.08%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 80.23%   [EVAL] batch:   92 | acc: 81.25%,  total acc: 80.24%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 80.45%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 80.66%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 80.86%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 81.06%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 81.44%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 81.62%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 81.81%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 81.92%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 82.04%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 82.09%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 82.26%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 82.37%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 82.01%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 81.31%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 80.68%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 80.00%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 79.50%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 79.07%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 78.93%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 79.00%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 79.18%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 79.36%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 79.54%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 79.66%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 79.57%   [EVAL] batch:  119 | acc: 12.50%,  total acc: 79.01%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 78.46%   [EVAL] batch:  121 | acc: 12.50%,  total acc: 77.92%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 77.34%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 76.76%   [EVAL] batch:  124 | acc: 12.50%,  total acc: 76.25%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 75.89%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 75.69%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 75.34%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 75.00%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 74.90%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 74.71%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 74.81%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 75.09%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 75.14%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 75.23%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 75.41%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 75.45%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 75.31%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 75.18%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 75.27%   [EVAL] batch:  141 | acc: 81.25%,  total acc: 75.31%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 75.31%   [EVAL] batch:  143 | acc: 75.00%,  total acc: 75.30%   [EVAL] batch:  144 | acc: 50.00%,  total acc: 75.13%   [EVAL] batch:  145 | acc: 62.50%,  total acc: 75.04%   [EVAL] batch:  146 | acc: 62.50%,  total acc: 74.96%   [EVAL] batch:  147 | acc: 62.50%,  total acc: 74.87%   [EVAL] batch:  148 | acc: 68.75%,  total acc: 74.83%   [EVAL] batch:  149 | acc: 62.50%,  total acc: 74.75%   [EVAL] batch:  150 | acc: 87.50%,  total acc: 74.83%   [EVAL] batch:  151 | acc: 87.50%,  total acc: 74.92%   [EVAL] batch:  152 | acc: 81.25%,  total acc: 74.96%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 75.08%   [EVAL] batch:  154 | acc: 68.75%,  total acc: 75.04%   [EVAL] batch:  155 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 74.88%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 74.76%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 74.65%   [EVAL] batch:  159 | acc: 56.25%,  total acc: 74.53%   [EVAL] batch:  160 | acc: 56.25%,  total acc: 74.42%   [EVAL] batch:  161 | acc: 50.00%,  total acc: 74.27%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 74.19%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 74.12%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 74.09%   [EVAL] batch:  165 | acc: 75.00%,  total acc: 74.10%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 74.10%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 74.11%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 74.08%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 73.79%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 73.54%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 73.22%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 72.98%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 72.74%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 72.61%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 72.76%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 73.07%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 73.22%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 73.37%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 73.52%   [EVAL] batch:  181 | acc: 81.25%,  total acc: 73.56%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 73.46%   [EVAL] batch:  183 | acc: 75.00%,  total acc: 73.47%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 73.48%   [EVAL] batch:  185 | acc: 62.50%,  total acc: 73.42%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 73.43%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 73.37%   [EVAL] batch:  188 | acc: 25.00%,  total acc: 73.12%   [EVAL] batch:  189 | acc: 31.25%,  total acc: 72.89%   [EVAL] batch:  190 | acc: 25.00%,  total acc: 72.64%   [EVAL] batch:  191 | acc: 37.50%,  total acc: 72.46%   [EVAL] batch:  192 | acc: 31.25%,  total acc: 72.25%   [EVAL] batch:  193 | acc: 25.00%,  total acc: 72.00%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 72.02%   [EVAL] batch:  195 | acc: 56.25%,  total acc: 71.94%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 71.99%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 72.03%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 72.05%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 72.06%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 72.01%   [EVAL] batch:  201 | acc: 43.75%,  total acc: 71.88%   [EVAL] batch:  202 | acc: 43.75%,  total acc: 71.74%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 71.66%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 71.65%   [EVAL] batch:  205 | acc: 37.50%,  total acc: 71.48%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 71.59%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 71.72%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 71.86%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 71.96%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 72.10%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 72.23%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 72.30%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 72.43%   [EVAL] batch:  214 | acc: 93.75%,  total acc: 72.53%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 72.78%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 72.91%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 73.03%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 73.15%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 73.27%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 73.34%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 73.43%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 73.52%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 73.64%   [EVAL] batch:  225 | acc: 56.25%,  total acc: 73.56%   [EVAL] batch:  226 | acc: 37.50%,  total acc: 73.40%   [EVAL] batch:  227 | acc: 50.00%,  total acc: 73.30%   [EVAL] batch:  228 | acc: 25.00%,  total acc: 73.09%   [EVAL] batch:  229 | acc: 12.50%,  total acc: 72.83%   [EVAL] batch:  230 | acc: 25.00%,  total acc: 72.62%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 72.71%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 72.80%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 73.03%   [EVAL] batch:  235 | acc: 87.50%,  total acc: 73.09%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 73.18%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 73.21%   [EVAL] batch:  238 | acc: 68.75%,  total acc: 73.20%   [EVAL] batch:  239 | acc: 62.50%,  total acc: 73.15%   [EVAL] batch:  240 | acc: 62.50%,  total acc: 73.11%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 73.14%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 73.10%   [EVAL] batch:  243 | acc: 31.25%,  total acc: 72.93%   [EVAL] batch:  244 | acc: 75.00%,  total acc: 72.93%   [EVAL] batch:  245 | acc: 75.00%,  total acc: 72.94%   [EVAL] batch:  246 | acc: 68.75%,  total acc: 72.93%   [EVAL] batch:  247 | acc: 87.50%,  total acc: 72.98%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 73.07%   [EVAL] batch:  249 | acc: 75.00%,  total acc: 73.08%   [EVAL] batch:  250 | acc: 100.00%,  total acc: 73.18%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 73.26%   [EVAL] batch:  252 | acc: 100.00%,  total acc: 73.37%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 73.43%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 73.50%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 73.61%   [EVAL] batch:  256 | acc: 81.25%,  total acc: 73.64%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 73.64%   [EVAL] batch:  258 | acc: 93.75%,  total acc: 73.72%   [EVAL] batch:  259 | acc: 87.50%,  total acc: 73.77%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 73.68%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 73.69%   [EVAL] batch:  262 | acc: 100.00%,  total acc: 73.79%   [EVAL] batch:  263 | acc: 87.50%,  total acc: 73.84%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 73.87%   [EVAL] batch:  265 | acc: 68.75%,  total acc: 73.85%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 73.85%   [EVAL] batch:  267 | acc: 75.00%,  total acc: 73.86%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 73.84%   [EVAL] batch:  269 | acc: 50.00%,  total acc: 73.75%   [EVAL] batch:  270 | acc: 93.75%,  total acc: 73.82%   [EVAL] batch:  271 | acc: 68.75%,  total acc: 73.81%   [EVAL] batch:  272 | acc: 75.00%,  total acc: 73.81%   [EVAL] batch:  273 | acc: 75.00%,  total acc: 73.81%   [EVAL] batch:  274 | acc: 43.75%,  total acc: 73.70%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 73.80%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 73.89%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 73.99%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 74.08%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 74.17%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 74.27%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 74.31%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 74.36%   [EVAL] batch:  283 | acc: 68.75%,  total acc: 74.34%   [EVAL] batch:  284 | acc: 87.50%,  total acc: 74.39%   [EVAL] batch:  285 | acc: 68.75%,  total acc: 74.37%   [EVAL] batch:  286 | acc: 75.00%,  total acc: 74.37%   [EVAL] batch:  287 | acc: 81.25%,  total acc: 74.39%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 74.48%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 74.57%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 74.66%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 74.74%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 74.83%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 74.89%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 74.98%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 75.06%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 75.15%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 75.23%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 75.31%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 75.40%   [EVAL] batch:  300 | acc: 87.50%,  total acc: 75.44%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 75.50%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 75.56%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 75.62%   [EVAL] batch:  304 | acc: 87.50%,  total acc: 75.66%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 75.71%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 75.77%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 75.83%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 75.89%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 75.97%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 76.05%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 76.08%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 76.00%   
cur_acc:  ['0.9524', '0.8313', '0.7212', '0.7292', '0.8760']
his_acc:  ['0.9524', '0.8870', '0.7985', '0.7488', '0.7600']
CurrentTrain: epoch 15, batch     0 | loss: 16.5497574CurrentTrain: epoch 15, batch     1 | loss: 14.8860591CurrentTrain: epoch 15, batch     2 | loss: 26.9320442CurrentTrain: epoch  1, batch     3 | loss: 21.3021695CurrentTrain: epoch 15, batch     0 | loss: 14.1844322CurrentTrain: epoch 15, batch     1 | loss: 12.5911289CurrentTrain: epoch 15, batch     2 | loss: 12.6855698CurrentTrain: epoch  1, batch     3 | loss: 9.1247155CurrentTrain: epoch 15, batch     0 | loss: 13.7033036CurrentTrain: epoch 15, batch     1 | loss: 11.7603483CurrentTrain: epoch 15, batch     2 | loss: 11.9374616CurrentTrain: epoch  1, batch     3 | loss: 7.4715073CurrentTrain: epoch 15, batch     0 | loss: 11.9971725CurrentTrain: epoch 15, batch     1 | loss: 13.0054563CurrentTrain: epoch 15, batch     2 | loss: 11.1346067CurrentTrain: epoch  1, batch     3 | loss: 8.2647487CurrentTrain: epoch 15, batch     0 | loss: 11.9778223CurrentTrain: epoch 15, batch     1 | loss: 8.4713732CurrentTrain: epoch 15, batch     2 | loss: 11.2027925CurrentTrain: epoch  1, batch     3 | loss: 9.9962612CurrentTrain: epoch 15, batch     0 | loss: 8.1485114CurrentTrain: epoch 15, batch     1 | loss: 10.5862658CurrentTrain: epoch 15, batch     2 | loss: 10.9823366CurrentTrain: epoch  1, batch     3 | loss: 8.0275282CurrentTrain: epoch 15, batch     0 | loss: 15.5286465CurrentTrain: epoch 15, batch     1 | loss: 7.6677159CurrentTrain: epoch 15, batch     2 | loss: 12.4899453CurrentTrain: epoch  1, batch     3 | loss: 7.3294005CurrentTrain: epoch 15, batch     0 | loss: 9.5930959CurrentTrain: epoch 15, batch     1 | loss: 6.8919569CurrentTrain: epoch 15, batch     2 | loss: 8.5697097CurrentTrain: epoch  1, batch     3 | loss: 17.9651453CurrentTrain: epoch 15, batch     0 | loss: 6.9137344CurrentTrain: epoch 15, batch     1 | loss: 6.4640310CurrentTrain: epoch 15, batch     2 | loss: 7.4048218CurrentTrain: epoch  1, batch     3 | loss: 13.4106826CurrentTrain: epoch 15, batch     0 | loss: 12.7518555CurrentTrain: epoch 15, batch     1 | loss: 6.8989401CurrentTrain: epoch 15, batch     2 | loss: 11.5734518CurrentTrain: epoch  1, batch     3 | loss: 7.6918385
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: it is situated in the historic lower lusatia region , about south of beeskow , and north of cottbus .
Head Entity: cottbus
Tail Entity: lower lusatia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The ancient ruins are found on the slopes of the majestic Andes mountains, providing a breathtaking view of the valley below.  
Head Entity: Andes mountains  
Tail Entity: ancient ruins  

Relation: located on terrain feature  
Context: The village is nestled in the heart of the lush Black Forest, surrounded by towering trees and serene lakes.  
Head Entity: Black Forest  
Tail Entity: village  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, also known as cop26, took place in glasgow, scotland, where numerous world leaders gathered to discuss climate action.  
Head Entity: united nations climate change conference  
Tail Entity: cop26  

Relation: participant of  
Context: the annual tech conference in san francisco attracted thousands of developers and entrepreneurs, showcasing the latest innovations in technology.  
Head Entity: annual tech conference  
Tail Entity: san francisco  
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: " runaway reptar " was written by ali marie matheson and jon cooksey and directed by john holmquist and jim duffy .
Head Entity: runaway reptar
Tail Entity: john holmquist
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: "Inception" was directed by Christopher Nolan and features a complex narrative structure that keeps viewers on the edge of their seats.  
Head Entity: Inception  
Tail Entity: Christopher Nolan  

Relation: director  
Context: The acclaimed stage play "Hamlet" was directed by Kenneth Branagh, who brought a fresh perspective to the classic Shakespearean tale.  
Head Entity: Hamlet  
Tail Entity: Kenneth Branagh  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: tata prima is a range of heavy trucks produced by tata daewoo , a wholly owned subsidiary of tata motors of india .
Head Entity: tata daewoo
Tail Entity: tata motors
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: The famous luxury brand Gucci is owned by the French conglomerate Kering, which also owns several other high-end fashion labels.  
Head Entity: Gucci  
Tail Entity: Kering  

Relation: owned by  
Context: The tech giant Microsoft is owned by its shareholders, with a significant portion of the stock held by institutional investors.  
Head Entity: Microsoft  
Tail Entity: Shareholders  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: thessaly was home to extensive neolithic and chalcolithic cultures around 6000 bc–2500 bc ( see cardium pottery , dimini and sesklo ) .
Head Entity: cardium pottery
Tail Entity: neolithic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The engine is a crucial part of the car's overall functionality, providing the necessary power to drive.  
Head Entity: engine  
Tail Entity: car  

Relation: part of  
Context: The heart is an essential part of the circulatory system, responsible for pumping blood throughout the body.  
Head Entity: heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: it served as the home of k league side gyeongnam fc and national league side changwon city fc prior to the construction of the changwon football center in 2009 .
Head Entity: changwon football center
Tail Entity: gyeongnam fc
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was once the residence of the famous artist, and it now serves as a gallery showcasing his works.  
Head Entity: historic building  
Tail Entity: famous artist  

Relation: occupant  
Context: After the renovation, the old factory became the headquarters for a tech startup that focuses on innovative software solutions.  
Head Entity: old factory  
Tail Entity: tech startup  
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: he also commissioned renowned architect richard neutra to design the iconic desert kaufmann house ( 1946 ) in palm springs , california .
Head Entity: kaufmann house
Tail Entity: richard neutra
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: the innovative design of the new city library was created by the famous architect zaha hadid, known for her futuristic structures.  
Head Entity: city library  
Tail Entity: zaha hadid  

Relation: architect  
Context: after years of planning, the historic renovation of the old courthouse was finally completed, thanks to the talented architect frank gehry.  
Head Entity: old courthouse  
Tail Entity: frank gehry  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: in 2001 brian haw set up the parliament square peace campaign outside the houses of parliament in london .
Head Entity: brian haw
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: After moving to New York City in 2010, Sarah found her dream apartment in Brooklyn.  
Head Entity: Sarah  
Tail Entity: Brooklyn  

Relation: residence  
Context: The famous author lived in a quaint cottage in the countryside for many years before relocating to the city.  
Head Entity: The famous author  
Tail Entity: the city  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Ludwig van Beethoven created some of the most celebrated symphonies in classical music history.  
Head Entity: symphonies  
Tail Entity: Ludwig van Beethoven  

Relation: composer  
Context: Wolfgang Amadeus Mozart was a prolific and influential composer of the classical era, known for his operas and symphonies.  
Head Entity: operas  
Tail Entity: Wolfgang Amadeus Mozart  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: one polish archer qualified for the women 's individual recurve by obtaining one of the eight olympic places available from the 2015 world archery championships in copenhagen , denmark .
Head Entity: 2015 world archery championships
Tail Entity: copenhagen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: The annual music festival will take place in the vibrant city of Austin, Texas, attracting thousands of fans from around the world.  
Head Entity: annual music festival  
Tail Entity: Austin, Texas  

Relation: location  
Context: The historic battle was fought near the banks of the Potomac River, a significant site in American history.  
Head Entity: historic battle  
Tail Entity: Potomac River  
MemoryTrain:  epoch 15, batch     0 | loss: 6.2462813MemoryTrain:  epoch 15, batch     1 | loss: 2.7984621MemoryTrain:  epoch 15, batch     2 | loss: 2.2587770MemoryTrain:  epoch 15, batch     3 | loss: 3.6438390MemoryTrain:  epoch 15, batch     4 | loss: 3.6522660MemoryTrain:  epoch 15, batch     5 | loss: 1.9211651MemoryTrain:  epoch 15, batch     6 | loss: 4.7563362MemoryTrain:  epoch 15, batch     7 | loss: 5.4411556MemoryTrain:  epoch 15, batch     8 | loss: 1.9243495MemoryTrain:  epoch 15, batch     9 | loss: 3.5952480MemoryTrain:  epoch 15, batch    10 | loss: 3.6345566MemoryTrain:  epoch  3, batch    11 | loss: 16.1293905MemoryTrain:  epoch 15, batch     0 | loss: 2.8906180MemoryTrain:  epoch 15, batch     1 | loss: 2.6971509MemoryTrain:  epoch 15, batch     2 | loss: 3.1937212MemoryTrain:  epoch 15, batch     3 | loss: 2.3898911MemoryTrain:  epoch 15, batch     4 | loss: 3.8900044MemoryTrain:  epoch 15, batch     5 | loss: 2.5886412MemoryTrain:  epoch 15, batch     6 | loss: 3.2920376MemoryTrain:  epoch 15, batch     7 | loss: 3.0001079MemoryTrain:  epoch 15, batch     8 | loss: 3.2408300MemoryTrain:  epoch 15, batch     9 | loss: 2.4747858MemoryTrain:  epoch 15, batch    10 | loss: 2.3432035MemoryTrain:  epoch  3, batch    11 | loss: 12.5683466MemoryTrain:  epoch 15, batch     0 | loss: 3.9366151MemoryTrain:  epoch 15, batch     1 | loss: 2.9168670MemoryTrain:  epoch 15, batch     2 | loss: 2.1866460MemoryTrain:  epoch 15, batch     3 | loss: 2.0610324MemoryTrain:  epoch 15, batch     4 | loss: 3.3569285MemoryTrain:  epoch 15, batch     5 | loss: 4.4412293MemoryTrain:  epoch 15, batch     6 | loss: 5.7280171MemoryTrain:  epoch 15, batch     7 | loss: 1.7163929MemoryTrain:  epoch 15, batch     8 | loss: 2.2103306MemoryTrain:  epoch 15, batch     9 | loss: 4.3669206MemoryTrain:  epoch 15, batch    10 | loss: 2.6710152MemoryTrain:  epoch  3, batch    11 | loss: 10.6276570MemoryTrain:  epoch 15, batch     0 | loss: 1.7324363MemoryTrain:  epoch 15, batch     1 | loss: 2.2421361MemoryTrain:  epoch 15, batch     2 | loss: 1.9115285MemoryTrain:  epoch 15, batch     3 | loss: 8.4977243MemoryTrain:  epoch 15, batch     4 | loss: 2.7741299MemoryTrain:  epoch 15, batch     5 | loss: 2.5212893MemoryTrain:  epoch 15, batch     6 | loss: 1.8122757MemoryTrain:  epoch 15, batch     7 | loss: 2.2294007MemoryTrain:  epoch 15, batch     8 | loss: 2.9183615MemoryTrain:  epoch 15, batch     9 | loss: 1.8114799MemoryTrain:  epoch 15, batch    10 | loss: 5.1099724MemoryTrain:  epoch  3, batch    11 | loss: 10.4974272MemoryTrain:  epoch 15, batch     0 | loss: 3.8361332MemoryTrain:  epoch 15, batch     1 | loss: 2.3482686MemoryTrain:  epoch 15, batch     2 | loss: 6.2439146MemoryTrain:  epoch 15, batch     3 | loss: 4.0400565MemoryTrain:  epoch 15, batch     4 | loss: 2.0912551MemoryTrain:  epoch 15, batch     5 | loss: 2.2235264MemoryTrain:  epoch 15, batch     6 | loss: 3.8929833MemoryTrain:  epoch 15, batch     7 | loss: 1.5976085MemoryTrain:  epoch 15, batch     8 | loss: 1.7674895MemoryTrain:  epoch 15, batch     9 | loss: 2.0328004MemoryTrain:  epoch 15, batch    10 | loss: 2.0733728MemoryTrain:  epoch  3, batch    11 | loss: 10.3451997MemoryTrain:  epoch 15, batch     0 | loss: 1.8821889MemoryTrain:  epoch 15, batch     1 | loss: 1.9742649MemoryTrain:  epoch 15, batch     2 | loss: 1.5516708MemoryTrain:  epoch 15, batch     3 | loss: 2.5189750MemoryTrain:  epoch 15, batch     4 | loss: 4.4674198MemoryTrain:  epoch 15, batch     5 | loss: 2.1031443MemoryTrain:  epoch 15, batch     6 | loss: 3.7500903MemoryTrain:  epoch 15, batch     7 | loss: 2.1583789MemoryTrain:  epoch 15, batch     8 | loss: 4.4277707MemoryTrain:  epoch 15, batch     9 | loss: 4.1445332MemoryTrain:  epoch 15, batch    10 | loss: 4.0913986MemoryTrain:  epoch  3, batch    11 | loss: 10.7312896MemoryTrain:  epoch 15, batch     0 | loss: 1.6113650MemoryTrain:  epoch 15, batch     1 | loss: 2.6573779MemoryTrain:  epoch 15, batch     2 | loss: 1.5188811MemoryTrain:  epoch 15, batch     3 | loss: 3.9241593MemoryTrain:  epoch 15, batch     4 | loss: 2.5714959MemoryTrain:  epoch 15, batch     5 | loss: 2.9937566MemoryTrain:  epoch 15, batch     6 | loss: 1.7959091MemoryTrain:  epoch 15, batch     7 | loss: 1.9161930MemoryTrain:  epoch 15, batch     8 | loss: 2.1740871MemoryTrain:  epoch 15, batch     9 | loss: 4.4977692MemoryTrain:  epoch 15, batch    10 | loss: 2.8739530MemoryTrain:  epoch  3, batch    11 | loss: 9.9836751MemoryTrain:  epoch 15, batch     0 | loss: 1.5993614MemoryTrain:  epoch 15, batch     1 | loss: 1.6108496MemoryTrain:  epoch 15, batch     2 | loss: 10.5195782MemoryTrain:  epoch 15, batch     3 | loss: 2.1617443MemoryTrain:  epoch 15, batch     4 | loss: 1.9546732MemoryTrain:  epoch 15, batch     5 | loss: 1.7757432MemoryTrain:  epoch 15, batch     6 | loss: 2.3162015MemoryTrain:  epoch 15, batch     7 | loss: 1.6052353MemoryTrain:  epoch 15, batch     8 | loss: 2.7371502MemoryTrain:  epoch 15, batch     9 | loss: 2.3695632MemoryTrain:  epoch 15, batch    10 | loss: 1.5113609MemoryTrain:  epoch  3, batch    11 | loss: 10.0333889MemoryTrain:  epoch 15, batch     0 | loss: 2.3827212MemoryTrain:  epoch 15, batch     1 | loss: 3.8800602MemoryTrain:  epoch 15, batch     2 | loss: 3.3113481MemoryTrain:  epoch 15, batch     3 | loss: 3.9552510MemoryTrain:  epoch 15, batch     4 | loss: 5.3427138MemoryTrain:  epoch 15, batch     5 | loss: 1.9126294MemoryTrain:  epoch 15, batch     6 | loss: 1.7004752MemoryTrain:  epoch 15, batch     7 | loss: 2.4803208MemoryTrain:  epoch 15, batch     8 | loss: 2.0290453MemoryTrain:  epoch 15, batch     9 | loss: 1.8028112MemoryTrain:  epoch 15, batch    10 | loss: 2.3421429MemoryTrain:  epoch  3, batch    11 | loss: 11.1068474MemoryTrain:  epoch 15, batch     0 | loss: 4.1051138MemoryTrain:  epoch 15, batch     1 | loss: 1.5949568MemoryTrain:  epoch 15, batch     2 | loss: 2.0808620MemoryTrain:  epoch 15, batch     3 | loss: 1.9439298MemoryTrain:  epoch 15, batch     4 | loss: 1.5160552MemoryTrain:  epoch 15, batch     5 | loss: 1.9900347MemoryTrain:  epoch 15, batch     6 | loss: 1.9663025MemoryTrain:  epoch 15, batch     7 | loss: 1.3864972MemoryTrain:  epoch 15, batch     8 | loss: 1.7352920MemoryTrain:  epoch 15, batch     9 | loss: 1.6038827MemoryTrain:  epoch 15, batch    10 | loss: 3.6767210MemoryTrain:  epoch  3, batch    11 | loss: 9.8096357
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 35.94%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 40.00%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 40.62%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 45.54%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 50.78%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 55.56%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 58.13%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 60.80%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 63.02%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 64.42%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 65.18%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 66.67%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 68.36%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 69.85%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 71.18%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 70.39%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 68.12%   [EVAL] batch:   20 | acc: 6.25%,  total acc: 65.18%   [EVAL] batch:   21 | acc: 18.75%,  total acc: 63.07%   [EVAL] batch:   22 | acc: 37.50%,  total acc: 61.96%   [EVAL] batch:   23 | acc: 18.75%,  total acc: 60.16%   [EVAL] batch:   24 | acc: 18.75%,  total acc: 58.50%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 56.49%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 54.86%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 53.12%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 51.51%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 50.00%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 48.39%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 49.02%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 50.19%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 50.92%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 52.32%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 53.30%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 53.89%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 54.93%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 55.93%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 56.72%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 57.47%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 58.04%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 59.01%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 59.94%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 60.56%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 61.14%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 61.57%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 61.98%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 62.37%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 62.75%   [EVAL] batch:   50 | acc: 31.25%,  total acc: 62.13%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:   52 | acc: 37.50%,  total acc: 62.03%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 62.27%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 62.16%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 62.28%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 61.84%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 61.42%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 61.12%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 61.46%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 61.17%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 61.29%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 60.62%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 70.31%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 68.06%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 66.25%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 63.07%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 61.46%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 62.02%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 63.84%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 65.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 69.49%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 71.18%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 72.37%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 72.19%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 72.62%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 72.16%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 71.20%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 71.61%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 72.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.32%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 74.31%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.22%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 75.86%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.42%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 77.93%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 78.41%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 77.57%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 77.32%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 77.36%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 77.63%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 78.21%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 79.27%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 79.61%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 79.94%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 79.97%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 80.14%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 80.16%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 79.79%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 79.56%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 79.72%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 79.50%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 79.53%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 79.95%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 79.63%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 79.32%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 79.46%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 79.28%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 78.88%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 78.50%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 78.54%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 78.59%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 78.53%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 78.47%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 77.64%   [EVAL] batch:   64 | acc: 56.25%,  total acc: 77.31%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 76.70%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 76.12%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 75.74%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 75.27%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 75.36%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 75.26%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 75.17%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 75.34%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 75.42%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 75.58%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 75.66%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 75.65%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 75.72%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 75.95%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 75.94%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 76.23%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 75.91%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 75.45%   [EVAL] batch:   83 | acc: 43.75%,  total acc: 75.07%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 74.49%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 74.35%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 74.07%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 73.79%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 74.02%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 74.24%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 74.18%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 74.39%   [EVAL] batch:   92 | acc: 81.25%,  total acc: 74.46%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 74.73%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 75.26%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 75.52%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 75.77%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 76.01%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 76.25%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 76.49%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 76.65%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 76.82%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 76.98%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 77.20%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 77.36%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 76.99%   [EVAL] batch:  107 | acc: 0.00%,  total acc: 76.27%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 75.69%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 75.06%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 74.55%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 74.11%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 74.00%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 74.12%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 74.35%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 74.46%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 74.63%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 74.79%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 74.68%   [EVAL] batch:  119 | acc: 12.50%,  total acc: 74.17%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 73.66%   [EVAL] batch:  121 | acc: 12.50%,  total acc: 73.16%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 72.61%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 72.08%   [EVAL] batch:  124 | acc: 12.50%,  total acc: 71.60%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 71.28%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 71.16%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 70.95%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 70.74%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 70.72%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 70.71%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 70.83%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 71.05%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 71.18%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 71.25%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 71.46%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 71.67%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 71.69%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 71.58%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 71.56%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 71.68%   [EVAL] batch:  141 | acc: 81.25%,  total acc: 71.74%   [EVAL] batch:  142 | acc: 62.50%,  total acc: 71.68%   [EVAL] batch:  143 | acc: 81.25%,  total acc: 71.74%   [EVAL] batch:  144 | acc: 25.00%,  total acc: 71.42%   [EVAL] batch:  145 | acc: 25.00%,  total acc: 71.10%   [EVAL] batch:  146 | acc: 25.00%,  total acc: 70.79%   [EVAL] batch:  147 | acc: 12.50%,  total acc: 70.40%   [EVAL] batch:  148 | acc: 31.25%,  total acc: 70.13%   [EVAL] batch:  149 | acc: 25.00%,  total acc: 69.83%   [EVAL] batch:  150 | acc: 75.00%,  total acc: 69.87%   [EVAL] batch:  151 | acc: 87.50%,  total acc: 69.98%   [EVAL] batch:  152 | acc: 62.50%,  total acc: 69.93%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 70.05%   [EVAL] batch:  154 | acc: 75.00%,  total acc: 70.08%   [EVAL] batch:  155 | acc: 43.75%,  total acc: 69.91%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 69.82%   [EVAL] batch:  157 | acc: 62.50%,  total acc: 69.78%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 69.69%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 69.65%   [EVAL] batch:  160 | acc: 75.00%,  total acc: 69.68%   [EVAL] batch:  161 | acc: 56.25%,  total acc: 69.60%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 69.56%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 69.47%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 69.43%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 69.39%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 69.42%   [EVAL] batch:  167 | acc: 68.75%,  total acc: 69.42%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 69.42%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 69.23%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 69.04%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 68.82%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 68.71%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 68.50%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 68.39%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 68.57%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 68.93%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 69.10%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 69.27%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:  181 | acc: 81.25%,  total acc: 69.51%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 69.40%   [EVAL] batch:  183 | acc: 68.75%,  total acc: 69.40%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 69.46%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 69.46%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 69.49%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 69.41%   [EVAL] batch:  188 | acc: 18.75%,  total acc: 69.15%   [EVAL] batch:  189 | acc: 31.25%,  total acc: 68.95%   [EVAL] batch:  190 | acc: 18.75%,  total acc: 68.68%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 68.42%   [EVAL] batch:  192 | acc: 37.50%,  total acc: 68.26%   [EVAL] batch:  193 | acc: 25.00%,  total acc: 68.04%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 68.11%   [EVAL] batch:  195 | acc: 81.25%,  total acc: 68.18%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 68.21%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 68.28%   [EVAL] batch:  198 | acc: 81.25%,  total acc: 68.34%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 68.41%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 68.41%   [EVAL] batch:  201 | acc: 37.50%,  total acc: 68.25%   [EVAL] batch:  202 | acc: 37.50%,  total acc: 68.10%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 68.05%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 68.05%   [EVAL] batch:  205 | acc: 31.25%,  total acc: 67.87%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 67.96%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 68.12%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 68.27%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 68.39%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 68.51%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 68.66%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 68.90%   [EVAL] batch:  214 | acc: 93.75%,  total acc: 69.01%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 69.16%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 69.30%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 69.58%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 69.72%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 69.85%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 69.96%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 70.07%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 70.17%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 70.30%   [EVAL] batch:  226 | acc: 37.50%,  total acc: 70.15%   [EVAL] batch:  227 | acc: 50.00%,  total acc: 70.07%   [EVAL] batch:  228 | acc: 25.00%,  total acc: 69.87%   [EVAL] batch:  229 | acc: 12.50%,  total acc: 69.62%   [EVAL] batch:  230 | acc: 37.50%,  total acc: 69.48%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 69.59%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 69.69%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 69.92%   [EVAL] batch:  235 | acc: 87.50%,  total acc: 69.99%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 70.12%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 70.14%   [EVAL] batch:  238 | acc: 68.75%,  total acc: 70.14%   [EVAL] batch:  239 | acc: 75.00%,  total acc: 70.16%   [EVAL] batch:  240 | acc: 68.75%,  total acc: 70.15%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 70.20%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 70.16%   [EVAL] batch:  243 | acc: 37.50%,  total acc: 70.03%   [EVAL] batch:  244 | acc: 81.25%,  total acc: 70.08%   [EVAL] batch:  245 | acc: 75.00%,  total acc: 70.10%   [EVAL] batch:  246 | acc: 75.00%,  total acc: 70.12%   [EVAL] batch:  247 | acc: 87.50%,  total acc: 70.19%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 70.28%   [EVAL] batch:  249 | acc: 75.00%,  total acc: 70.30%   [EVAL] batch:  250 | acc: 100.00%,  total acc: 70.42%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 70.51%   [EVAL] batch:  252 | acc: 100.00%,  total acc: 70.63%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 70.69%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 70.78%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 70.90%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 70.82%   [EVAL] batch:  257 | acc: 31.25%,  total acc: 70.66%   [EVAL] batch:  258 | acc: 50.00%,  total acc: 70.58%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 70.55%   [EVAL] batch:  260 | acc: 25.00%,  total acc: 70.38%   [EVAL] batch:  261 | acc: 56.25%,  total acc: 70.32%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 70.29%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 70.31%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 70.31%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 70.25%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 70.20%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 70.17%   [EVAL] batch:  268 | acc: 56.25%,  total acc: 70.12%   [EVAL] batch:  269 | acc: 50.00%,  total acc: 70.05%   [EVAL] batch:  270 | acc: 62.50%,  total acc: 70.02%   [EVAL] batch:  271 | acc: 56.25%,  total acc: 69.97%   [EVAL] batch:  272 | acc: 62.50%,  total acc: 69.94%   [EVAL] batch:  273 | acc: 68.75%,  total acc: 69.94%   [EVAL] batch:  274 | acc: 37.50%,  total acc: 69.82%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 69.93%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 70.04%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 70.25%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 70.36%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 70.46%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 70.52%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 70.58%   [EVAL] batch:  283 | acc: 68.75%,  total acc: 70.58%   [EVAL] batch:  284 | acc: 87.50%,  total acc: 70.64%   [EVAL] batch:  285 | acc: 68.75%,  total acc: 70.63%   [EVAL] batch:  286 | acc: 75.00%,  total acc: 70.64%   [EVAL] batch:  287 | acc: 81.25%,  total acc: 70.68%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 70.78%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 70.88%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 70.98%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 71.08%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 71.18%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 71.26%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 71.36%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 71.45%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 71.55%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 71.64%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 71.83%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 71.91%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 71.98%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 72.05%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 72.12%   [EVAL] batch:  304 | acc: 87.50%,  total acc: 72.17%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 72.24%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 72.31%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 72.38%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 72.45%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 72.54%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 72.63%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 72.68%   [EVAL] batch:  312 | acc: 68.75%,  total acc: 72.66%   [EVAL] batch:  313 | acc: 43.75%,  total acc: 72.57%   [EVAL] batch:  314 | acc: 25.00%,  total acc: 72.42%   [EVAL] batch:  315 | acc: 37.50%,  total acc: 72.31%   [EVAL] batch:  316 | acc: 43.75%,  total acc: 72.22%   [EVAL] batch:  317 | acc: 37.50%,  total acc: 72.11%   [EVAL] batch:  318 | acc: 68.75%,  total acc: 72.10%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 72.15%   [EVAL] batch:  320 | acc: 87.50%,  total acc: 72.20%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 72.22%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 72.31%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 72.32%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 72.37%   [EVAL] batch:  325 | acc: 87.50%,  total acc: 72.41%   [EVAL] batch:  326 | acc: 75.00%,  total acc: 72.42%   [EVAL] batch:  327 | acc: 93.75%,  total acc: 72.48%   [EVAL] batch:  328 | acc: 93.75%,  total acc: 72.55%   [EVAL] batch:  329 | acc: 93.75%,  total acc: 72.61%   [EVAL] batch:  330 | acc: 93.75%,  total acc: 72.68%   [EVAL] batch:  331 | acc: 18.75%,  total acc: 72.52%   [EVAL] batch:  332 | acc: 18.75%,  total acc: 72.35%   [EVAL] batch:  333 | acc: 6.25%,  total acc: 72.16%   [EVAL] batch:  334 | acc: 37.50%,  total acc: 72.05%   [EVAL] batch:  335 | acc: 18.75%,  total acc: 71.89%   [EVAL] batch:  336 | acc: 18.75%,  total acc: 71.74%   [EVAL] batch:  337 | acc: 18.75%,  total acc: 71.58%   [EVAL] batch:  338 | acc: 0.00%,  total acc: 71.37%   [EVAL] batch:  339 | acc: 18.75%,  total acc: 71.21%   [EVAL] batch:  340 | acc: 0.00%,  total acc: 71.00%   [EVAL] batch:  341 | acc: 12.50%,  total acc: 70.83%   [EVAL] batch:  342 | acc: 0.00%,  total acc: 70.63%   [EVAL] batch:  343 | acc: 25.00%,  total acc: 70.49%   [EVAL] batch:  344 | acc: 81.25%,  total acc: 70.53%   [EVAL] batch:  345 | acc: 87.50%,  total acc: 70.57%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 70.62%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 70.69%   [EVAL] batch:  348 | acc: 87.50%,  total acc: 70.74%   [EVAL] batch:  349 | acc: 75.00%,  total acc: 70.75%   [EVAL] batch:  350 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:  351 | acc: 87.50%,  total acc: 70.88%   [EVAL] batch:  352 | acc: 81.25%,  total acc: 70.91%   [EVAL] batch:  353 | acc: 87.50%,  total acc: 70.96%   [EVAL] batch:  354 | acc: 93.75%,  total acc: 71.02%   [EVAL] batch:  355 | acc: 100.00%,  total acc: 71.10%   [EVAL] batch:  356 | acc: 93.75%,  total acc: 71.17%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 71.23%   [EVAL] batch:  358 | acc: 68.75%,  total acc: 71.22%   [EVAL] batch:  359 | acc: 93.75%,  total acc: 71.28%   [EVAL] batch:  360 | acc: 81.25%,  total acc: 71.31%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 71.36%   [EVAL] batch:  362 | acc: 50.00%,  total acc: 71.30%   [EVAL] batch:  363 | acc: 62.50%,  total acc: 71.27%   [EVAL] batch:  364 | acc: 43.75%,  total acc: 71.20%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 71.16%   [EVAL] batch:  366 | acc: 75.00%,  total acc: 71.17%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 71.13%   [EVAL] batch:  368 | acc: 56.25%,  total acc: 71.09%   [EVAL] batch:  369 | acc: 43.75%,  total acc: 71.01%   [EVAL] batch:  370 | acc: 37.50%,  total acc: 70.92%   [EVAL] batch:  371 | acc: 56.25%,  total acc: 70.88%   [EVAL] batch:  372 | acc: 62.50%,  total acc: 70.86%   [EVAL] batch:  373 | acc: 62.50%,  total acc: 70.84%   [EVAL] batch:  374 | acc: 50.00%,  total acc: 70.78%   
cur_acc:  ['0.9524', '0.8313', '0.7212', '0.7292', '0.8760', '0.6062']
his_acc:  ['0.9524', '0.8870', '0.7985', '0.7488', '0.7600', '0.7078']
CurrentTrain: epoch 15, batch     0 | loss: 17.5201454CurrentTrain: epoch 15, batch     1 | loss: 12.9796369CurrentTrain: epoch 15, batch     2 | loss: 13.4892943CurrentTrain: epoch  1, batch     3 | loss: 29.4574265CurrentTrain: epoch 15, batch     0 | loss: 16.6025691CurrentTrain: epoch 15, batch     1 | loss: 12.9821517CurrentTrain: epoch 15, batch     2 | loss: 18.7184357CurrentTrain: epoch  1, batch     3 | loss: 9.5943608CurrentTrain: epoch 15, batch     0 | loss: 16.1654215CurrentTrain: epoch 15, batch     1 | loss: 10.5268571CurrentTrain: epoch 15, batch     2 | loss: 13.9817952CurrentTrain: epoch  1, batch     3 | loss: 20.6323070CurrentTrain: epoch 15, batch     0 | loss: 11.1100128CurrentTrain: epoch 15, batch     1 | loss: 10.3753237CurrentTrain: epoch 15, batch     2 | loss: 13.9476579CurrentTrain: epoch  1, batch     3 | loss: 8.8475024CurrentTrain: epoch 15, batch     0 | loss: 8.6626349CurrentTrain: epoch 15, batch     1 | loss: 10.4677286CurrentTrain: epoch 15, batch     2 | loss: 12.6921693CurrentTrain: epoch  1, batch     3 | loss: 8.5407394CurrentTrain: epoch 15, batch     0 | loss: 12.8667953CurrentTrain: epoch 15, batch     1 | loss: 13.5584222CurrentTrain: epoch 15, batch     2 | loss: 16.0720262CurrentTrain: epoch  1, batch     3 | loss: 6.2415535CurrentTrain: epoch 15, batch     0 | loss: 9.8559092CurrentTrain: epoch 15, batch     1 | loss: 13.5298174CurrentTrain: epoch 15, batch     2 | loss: 8.1697933CurrentTrain: epoch  1, batch     3 | loss: 7.0802883CurrentTrain: epoch 15, batch     0 | loss: 11.4433395CurrentTrain: epoch 15, batch     1 | loss: 11.7585904CurrentTrain: epoch 15, batch     2 | loss: 11.5920685CurrentTrain: epoch  1, batch     3 | loss: 7.9891584CurrentTrain: epoch 15, batch     0 | loss: 5.6593976CurrentTrain: epoch 15, batch     1 | loss: 7.8990157CurrentTrain: epoch 15, batch     2 | loss: 7.6343269CurrentTrain: epoch  1, batch     3 | loss: 8.2605543CurrentTrain: epoch 15, batch     0 | loss: 10.0578665CurrentTrain: epoch 15, batch     1 | loss: 14.0589998CurrentTrain: epoch 15, batch     2 | loss: 10.9879924CurrentTrain: epoch  1, batch     3 | loss: 6.4569040
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: bryson 's best known work is his 1985 book " evil angels " which chronicles the story of lindy chamberlain 's trial for murder , following the death of her baby daughter , azaria .
Head Entity: evil angels
Tail Entity: death
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: in her groundbreaking 2010 book "the immortal life of henrietta lacks," rebecca skloot explores the life of henrietta lacks and the impact of her cells on medical research.  
Head Entity: the immortal life of henrietta lacks  
Tail Entity: henrietta lacks  

Relation: main subject  
Context: "sapiens: a brief history of humankind" by yuval noah harari examines the history and impact of homo sapiens from the emergence of our species to the present day.  
Head Entity: sapiens: a brief history of humankind  
Tail Entity: homo sapiens  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: group b of uefa euro 2016 contained england , russia , wales and slovakia .
Head Entity: uefa euro 2016
Tail Entity: slovakia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: the 2020 summer olympics featured countries like the united states, china, and japan competing in various sports.  
Head Entity: 2020 summer olympics  
Tail Entity: japan  

Relation: participating team  
Context: the fifa world cup 2018 saw teams such as france, croatia, and belgium battling for the championship title.  
Head Entity: fifa world cup 2018  
Tail Entity: croatia  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: among the competing artists was previous eurovision song contest entrant robin juhkental who represented estonia in 2010 as part of the group malcolm lincoln .
Head Entity: malcolm lincoln
Tail Entity: robin juhkental
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The famous painting "The Last Supper" features several apostles, including Peter, who is depicted prominently on the left side of the artwork.  
Head Entity: The Last Supper  
Tail Entity: Peter  

Relation: has part  
Context: The smartphone model Galaxy S21 comes with a high-resolution camera that includes a wide-angle lens, allowing users to capture stunning landscapes.  
Head Entity: Galaxy S21  
Tail Entity: wide-angle lens  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: he was nominated for the academy award for best picture , along with steven spielberg , ian bryce , mark gordon for the film " saving private ryan " .
Head Entity: ian bryce
Tail Entity: academy award for best picture
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film "The Shape of Water" received multiple nominations, including a nod for best director for Guillermo del Toro at the Academy Awards.  
Head Entity: Guillermo del Toro  
Tail Entity: Academy Awards  

Relation: nominated for  
Context: The popular band was nominated for the Grammy Award for Best New Artist after their debut album topped the charts.  
Head Entity: the popular band  
Tail Entity: Grammy Award for Best New Artist  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: in 2007 , the county municipality created the transit authority called " skyss " that would market public transport while is would be operated by private companies based on public service obligation contracts .
Head Entity: skyss
Tail Entity: transit authority
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone known as the iPhone was first released by Apple Inc. in 2007, revolutionizing the mobile phone industry with its touch interface and app ecosystem.  
Head Entity: iPhone  
Tail Entity: smartphone  

Relation: instance of  
Context: The Great Wall of China is a series of fortifications made of various materials, built to protect Chinese states from invasions and raids, and is considered one of the most iconic structures in the world.  
Head Entity: Great Wall of China  
Tail Entity: fortification
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: abus valley ( ) is an ice - free valley southeast of turnstile ridge at the north end of britannia range .
Head Entity: turnstile ridge
Tail Entity: britannia range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the majestic peaks of the sierra nevada ( ) are known for their stunning beauty and are part of the larger cascade range .  
Head Entity: sierra nevada  
Tail Entity: cascade range  

Relation: mountain range  
Context: the famous rock formations of the appalachian mountains ( ) stretch across several states and are a significant part of the blue ridge range .  
Head Entity: appalachian mountains  
Tail Entity: blue ridge range  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: anders matthesen has also released several cds with his radio material , in addition to the animated movie " terkel in trouble " , based on one of these .
Head Entity: terkel in trouble
Tail Entity: anders matthesen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: the acclaimed film "inception" features a complex narrative that was intricately crafted by its talented screenwriter, who is known for his unique storytelling style.  
Head Entity: inception  
Tail Entity: christopher nolan  

Relation: screenwriter  
Context: the beloved animated feature "finding nemo" was brought to life through the creative vision of its screenwriter, who infused humor and heart into the script.  
Head Entity: finding nemo  
Tail Entity: andrew stanton  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the publisher found success in 1993 with tarō gomi 's " everyone poops " , a japanese language import that sold over one million copies and is now their best - known title .
Head Entity: everyone poops
Tail Entity: japanese language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The animated series "Avatar: The Last Airbender" features a rich narrative and is primarily presented in English, captivating audiences worldwide.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  

Relation: language of work or name  
Context: The famous novel "Cien años de soledad" by Gabriel García Márquez is celebrated for its intricate storytelling and is originally written in Spanish, reflecting the author's cultural roots.  
Head Entity: Cien años de soledad  
Tail Entity: Spanish  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: the aircraft was a boeing 707 - 321c that first entered service with pan american world airways ( pan am ) in 1963 .
Head Entity: boeing 707 - 321c
Tail Entity: pan american world airways
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: the new electric bus fleet is managed by the city transportation authority, which ensures efficient public transit services.  
Head Entity: electric bus fleet  
Tail Entity: city transportation authority  

Relation: operator  
Context: the state-of-the-art research facility is operated by the national science foundation, providing resources for groundbreaking studies.  
Head Entity: research facility  
Tail Entity: national science foundation  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: the most important lama of this series was the third changkya , rolpai dorje , who was preceptor to the qianlong emperor of china , and chief representative of tibetan buddhism at the qing court .
Head Entity: rolpai dorje
Tail Entity: tibetan buddhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the cathedral of notre-dame is a famous example of french gothic architecture and serves as the seat of the archbishop of paris, representing the roman catholic faith in the heart of the city.  
Head Entity: cathedral of notre-dame  
Tail Entity: roman catholic faith  

Relation: religion  
Context: the dalai lama is a prominent figure in the world, known for his teachings on compassion and non-violence, and is the spiritual leader of tibetan buddhism.  
Head Entity: dalai lama  
Tail Entity: tibetan buddhism  
MemoryTrain:  epoch 15, batch     0 | loss: 3.2693371MemoryTrain:  epoch 15, batch     1 | loss: 3.8782078MemoryTrain:  epoch 15, batch     2 | loss: 3.3847324MemoryTrain:  epoch 15, batch     3 | loss: 2.8264921MemoryTrain:  epoch 15, batch     4 | loss: 2.9258913MemoryTrain:  epoch 15, batch     5 | loss: 5.1752986MemoryTrain:  epoch 15, batch     6 | loss: 3.3686421MemoryTrain:  epoch 15, batch     7 | loss: 2.6162767MemoryTrain:  epoch 15, batch     8 | loss: 2.9592768MemoryTrain:  epoch 15, batch     9 | loss: 5.4587738MemoryTrain:  epoch 15, batch    10 | loss: 3.8492545MemoryTrain:  epoch 15, batch    11 | loss: 2.8447324MemoryTrain:  epoch 15, batch    12 | loss: 2.2167642MemoryTrain:  epoch  1, batch    13 | loss: 7.1547443MemoryTrain:  epoch 15, batch     0 | loss: 3.7074686MemoryTrain:  epoch 15, batch     1 | loss: 4.4433464MemoryTrain:  epoch 15, batch     2 | loss: 2.7861311MemoryTrain:  epoch 15, batch     3 | loss: 2.5489605MemoryTrain:  epoch 15, batch     4 | loss: 3.4018863MemoryTrain:  epoch 15, batch     5 | loss: 2.2068628MemoryTrain:  epoch 15, batch     6 | loss: 2.3478246MemoryTrain:  epoch 15, batch     7 | loss: 2.6012032MemoryTrain:  epoch 15, batch     8 | loss: 2.2837225MemoryTrain:  epoch 15, batch     9 | loss: 2.8948572MemoryTrain:  epoch 15, batch    10 | loss: 2.2134234MemoryTrain:  epoch 15, batch    11 | loss: 3.0538635MemoryTrain:  epoch 15, batch    12 | loss: 2.5952737MemoryTrain:  epoch  1, batch    13 | loss: 6.0227651MemoryTrain:  epoch 15, batch     0 | loss: 2.6908229MemoryTrain:  epoch 15, batch     1 | loss: 2.2471608MemoryTrain:  epoch 15, batch     2 | loss: 1.6587168MemoryTrain:  epoch 15, batch     3 | loss: 2.4119622MemoryTrain:  epoch 15, batch     4 | loss: 2.0742515MemoryTrain:  epoch 15, batch     5 | loss: 4.7605347MemoryTrain:  epoch 15, batch     6 | loss: 2.4010968MemoryTrain:  epoch 15, batch     7 | loss: 1.7096567MemoryTrain:  epoch 15, batch     8 | loss: 1.6435570MemoryTrain:  epoch 15, batch     9 | loss: 4.1900579MemoryTrain:  epoch 15, batch    10 | loss: 2.8999605MemoryTrain:  epoch 15, batch    11 | loss: 1.8368609MemoryTrain:  epoch 15, batch    12 | loss: 2.3486990MemoryTrain:  epoch  1, batch    13 | loss: 7.5430200MemoryTrain:  epoch 15, batch     0 | loss: 2.3918853MemoryTrain:  epoch 15, batch     1 | loss: 4.5105790MemoryTrain:  epoch 15, batch     2 | loss: 3.7584042MemoryTrain:  epoch 15, batch     3 | loss: 1.5963727MemoryTrain:  epoch 15, batch     4 | loss: 2.1707795MemoryTrain:  epoch 15, batch     5 | loss: 2.2799060MemoryTrain:  epoch 15, batch     6 | loss: 3.9016372MemoryTrain:  epoch 15, batch     7 | loss: 1.7405120MemoryTrain:  epoch 15, batch     8 | loss: 2.1726878MemoryTrain:  epoch 15, batch     9 | loss: 1.8164779MemoryTrain:  epoch 15, batch    10 | loss: 1.8464051MemoryTrain:  epoch 15, batch    11 | loss: 2.0718647MemoryTrain:  epoch 15, batch    12 | loss: 2.0184049MemoryTrain:  epoch  1, batch    13 | loss: 6.3041874MemoryTrain:  epoch 15, batch     0 | loss: 1.8351264MemoryTrain:  epoch 15, batch     1 | loss: 2.6829952MemoryTrain:  epoch 15, batch     2 | loss: 2.1584027MemoryTrain:  epoch 15, batch     3 | loss: 1.7373563MemoryTrain:  epoch 15, batch     4 | loss: 1.7216676MemoryTrain:  epoch 15, batch     5 | loss: 2.2919444MemoryTrain:  epoch 15, batch     6 | loss: 2.4488130MemoryTrain:  epoch 15, batch     7 | loss: 2.4687050MemoryTrain:  epoch 15, batch     8 | loss: 1.6898872MemoryTrain:  epoch 15, batch     9 | loss: 1.6879336MemoryTrain:  epoch 15, batch    10 | loss: 2.0602886MemoryTrain:  epoch 15, batch    11 | loss: 1.7126982MemoryTrain:  epoch 15, batch    12 | loss: 2.8340392MemoryTrain:  epoch  1, batch    13 | loss: 5.1234607MemoryTrain:  epoch 15, batch     0 | loss: 4.2823440MemoryTrain:  epoch 15, batch     1 | loss: 2.1960484MemoryTrain:  epoch 15, batch     2 | loss: 1.9697585MemoryTrain:  epoch 15, batch     3 | loss: 1.9634199MemoryTrain:  epoch 15, batch     4 | loss: 2.1429303MemoryTrain:  epoch 15, batch     5 | loss: 4.0006860MemoryTrain:  epoch 15, batch     6 | loss: 1.9731237MemoryTrain:  epoch 15, batch     7 | loss: 1.3402842MemoryTrain:  epoch 15, batch     8 | loss: 1.4339355MemoryTrain:  epoch 15, batch     9 | loss: 1.9851065MemoryTrain:  epoch 15, batch    10 | loss: 2.2052241MemoryTrain:  epoch 15, batch    11 | loss: 1.3743400MemoryTrain:  epoch 15, batch    12 | loss: 3.0436993MemoryTrain:  epoch  1, batch    13 | loss: 5.3114559MemoryTrain:  epoch 15, batch     0 | loss: 1.8323756MemoryTrain:  epoch 15, batch     1 | loss: 1.6481768MemoryTrain:  epoch 15, batch     2 | loss: 1.8110366MemoryTrain:  epoch 15, batch     3 | loss: 1.7803512MemoryTrain:  epoch 15, batch     4 | loss: 2.4540284MemoryTrain:  epoch 15, batch     5 | loss: 1.7463063MemoryTrain:  epoch 15, batch     6 | loss: 2.5406845MemoryTrain:  epoch 15, batch     7 | loss: 1.6671750MemoryTrain:  epoch 15, batch     8 | loss: 1.7553694MemoryTrain:  epoch 15, batch     9 | loss: 1.7306455MemoryTrain:  epoch 15, batch    10 | loss: 1.9378085MemoryTrain:  epoch 15, batch    11 | loss: 2.0148413MemoryTrain:  epoch 15, batch    12 | loss: 4.4896443MemoryTrain:  epoch  1, batch    13 | loss: 5.5565251MemoryTrain:  epoch 15, batch     0 | loss: 2.7807093MemoryTrain:  epoch 15, batch     1 | loss: 2.0859002MemoryTrain:  epoch 15, batch     2 | loss: 1.9857766MemoryTrain:  epoch 15, batch     3 | loss: 1.7523667MemoryTrain:  epoch 15, batch     4 | loss: 1.6950298MemoryTrain:  epoch 15, batch     5 | loss: 2.4091399MemoryTrain:  epoch 15, batch     6 | loss: 3.9091416MemoryTrain:  epoch 15, batch     7 | loss: 1.9609500MemoryTrain:  epoch 15, batch     8 | loss: 1.4656497MemoryTrain:  epoch 15, batch     9 | loss: 1.6740993MemoryTrain:  epoch 15, batch    10 | loss: 2.1799435MemoryTrain:  epoch 15, batch    11 | loss: 2.0107639MemoryTrain:  epoch 15, batch    12 | loss: 1.8647343MemoryTrain:  epoch  1, batch    13 | loss: 6.0751914MemoryTrain:  epoch 15, batch     0 | loss: 1.7468390MemoryTrain:  epoch 15, batch     1 | loss: 1.7724900MemoryTrain:  epoch 15, batch     2 | loss: 1.6999556MemoryTrain:  epoch 15, batch     3 | loss: 4.1863016MemoryTrain:  epoch 15, batch     4 | loss: 1.6042813MemoryTrain:  epoch 15, batch     5 | loss: 3.4820883MemoryTrain:  epoch 15, batch     6 | loss: 3.4865060MemoryTrain:  epoch 15, batch     7 | loss: 1.4032026MemoryTrain:  epoch 15, batch     8 | loss: 2.3420106MemoryTrain:  epoch 15, batch     9 | loss: 1.8733581MemoryTrain:  epoch 15, batch    10 | loss: 1.6296461MemoryTrain:  epoch 15, batch    11 | loss: 2.0425620MemoryTrain:  epoch 15, batch    12 | loss: 1.3364736MemoryTrain:  epoch  1, batch    13 | loss: 7.1689190MemoryTrain:  epoch 15, batch     0 | loss: 1.9445377MemoryTrain:  epoch 15, batch     1 | loss: 1.3505869MemoryTrain:  epoch 15, batch     2 | loss: 3.7578178MemoryTrain:  epoch 15, batch     3 | loss: 1.8227368MemoryTrain:  epoch 15, batch     4 | loss: 1.5515733MemoryTrain:  epoch 15, batch     5 | loss: 4.0184939MemoryTrain:  epoch 15, batch     6 | loss: 1.8519066MemoryTrain:  epoch 15, batch     7 | loss: 2.1551166MemoryTrain:  epoch 15, batch     8 | loss: 1.3507660MemoryTrain:  epoch 15, batch     9 | loss: 7.0022792MemoryTrain:  epoch 15, batch    10 | loss: 1.7206597MemoryTrain:  epoch 15, batch    11 | loss: 1.7307642MemoryTrain:  epoch 15, batch    12 | loss: 3.0233584MemoryTrain:  epoch  1, batch    13 | loss: 5.8584001
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 66.67%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 73.44%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 73.75%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 75.57%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 75.48%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 70.98%   [EVAL] batch:   14 | acc: 25.00%,  total acc: 67.92%   [EVAL] batch:   15 | acc: 25.00%,  total acc: 65.23%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 64.34%   [EVAL] batch:   17 | acc: 37.50%,  total acc: 62.85%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 61.51%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 63.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 66.48%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 69.27%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 70.25%   [EVAL] batch:   25 | acc: 12.50%,  total acc: 68.03%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 65.97%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 63.62%   [EVAL] batch:   28 | acc: 25.00%,  total acc: 62.28%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 60.62%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 58.67%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 58.98%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 60.04%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 61.03%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 61.96%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 62.67%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 63.51%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 64.14%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 64.26%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 64.84%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 64.79%   [EVAL] batch:   41 | acc: 37.50%,  total acc: 64.14%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 64.24%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 64.35%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 64.58%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 64.95%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 65.43%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 66.20%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 66.62%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 66.54%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 66.83%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 66.75%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 67.01%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 67.50%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 67.52%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 67.65%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 68.00%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 68.43%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 68.54%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 68.85%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 68.95%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 68.65%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 71.59%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 69.27%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 70.19%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 70.98%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 72.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 73.83%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 75.37%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 76.74%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 77.63%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 77.19%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 76.42%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 75.27%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 75.52%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 75.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.68%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 77.55%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.66%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 79.38%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 79.84%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 80.08%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 80.49%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 79.78%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 79.64%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 79.34%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 79.56%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 79.77%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 80.29%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 80.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 81.55%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 81.83%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 81.96%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 82.08%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 82.20%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 82.05%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 81.90%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 82.27%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 82.00%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 81.99%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 82.09%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 82.31%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 81.94%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 81.59%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 81.70%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 81.47%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 80.93%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 80.61%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 80.62%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 80.64%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 80.54%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 80.65%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 79.98%   [EVAL] batch:   64 | acc: 50.00%,  total acc: 79.52%   [EVAL] batch:   65 | acc: 43.75%,  total acc: 78.98%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 78.26%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 77.85%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 77.54%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 77.50%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 77.38%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 77.08%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 77.14%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 77.20%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 77.17%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 77.06%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 77.11%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 76.84%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 76.66%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 76.62%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 76.07%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 75.45%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 74.85%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 74.26%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 73.69%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 73.20%   [EVAL] batch:   87 | acc: 43.75%,  total acc: 72.87%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 73.10%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 73.33%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 73.15%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 73.37%   [EVAL] batch:   92 | acc: 81.25%,  total acc: 73.45%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 73.67%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 73.95%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 74.48%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 74.74%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 75.25%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 75.50%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 75.67%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 75.85%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 76.02%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 76.25%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 76.42%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 76.05%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 75.41%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 74.83%   [EVAL] batch:  109 | acc: 0.00%,  total acc: 74.15%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 73.65%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 73.21%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 73.12%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 73.25%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 73.48%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 73.60%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 73.77%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 73.94%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 73.84%   [EVAL] batch:  119 | acc: 12.50%,  total acc: 73.33%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 72.83%   [EVAL] batch:  121 | acc: 12.50%,  total acc: 72.34%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 71.80%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 71.27%   [EVAL] batch:  124 | acc: 12.50%,  total acc: 70.80%   [EVAL] batch:  125 | acc: 37.50%,  total acc: 70.54%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 70.32%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 70.07%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 69.82%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 69.71%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 69.66%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 69.79%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 70.02%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 70.15%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 70.32%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 70.50%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 70.71%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 70.74%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 70.64%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 70.54%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 70.66%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 70.69%   [EVAL] batch:  142 | acc: 62.50%,  total acc: 70.63%   [EVAL] batch:  143 | acc: 81.25%,  total acc: 70.70%   [EVAL] batch:  144 | acc: 37.50%,  total acc: 70.47%   [EVAL] batch:  145 | acc: 50.00%,  total acc: 70.33%   [EVAL] batch:  146 | acc: 43.75%,  total acc: 70.15%   [EVAL] batch:  147 | acc: 37.50%,  total acc: 69.93%   [EVAL] batch:  148 | acc: 62.50%,  total acc: 69.88%   [EVAL] batch:  149 | acc: 43.75%,  total acc: 69.71%   [EVAL] batch:  150 | acc: 75.00%,  total acc: 69.74%   [EVAL] batch:  151 | acc: 87.50%,  total acc: 69.86%   [EVAL] batch:  152 | acc: 81.25%,  total acc: 69.93%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 70.05%   [EVAL] batch:  154 | acc: 68.75%,  total acc: 70.04%   [EVAL] batch:  155 | acc: 50.00%,  total acc: 69.91%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 69.90%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 69.78%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 69.69%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 69.65%   [EVAL] batch:  160 | acc: 75.00%,  total acc: 69.68%   [EVAL] batch:  161 | acc: 56.25%,  total acc: 69.60%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 69.56%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 69.55%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 69.51%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 69.43%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 69.46%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 69.49%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 69.49%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 69.26%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 69.04%   [EVAL] batch:  171 | acc: 25.00%,  total acc: 68.79%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 68.68%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 68.46%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 68.32%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 68.50%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 68.68%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 68.86%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 69.03%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 69.20%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 69.37%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 69.40%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 69.30%   [EVAL] batch:  183 | acc: 68.75%,  total acc: 69.29%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 69.36%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 69.35%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 69.45%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 69.41%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 69.21%   [EVAL] batch:  189 | acc: 43.75%,  total acc: 69.08%   [EVAL] batch:  190 | acc: 25.00%,  total acc: 68.85%   [EVAL] batch:  191 | acc: 31.25%,  total acc: 68.65%   [EVAL] batch:  192 | acc: 37.50%,  total acc: 68.49%   [EVAL] batch:  193 | acc: 25.00%,  total acc: 68.27%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 68.27%   [EVAL] batch:  195 | acc: 56.25%,  total acc: 68.21%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 68.24%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 68.31%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 68.34%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 68.38%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 68.38%   [EVAL] batch:  201 | acc: 43.75%,  total acc: 68.25%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 68.20%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 68.14%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 68.11%   [EVAL] batch:  205 | acc: 31.25%,  total acc: 67.93%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 68.03%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 68.18%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 68.33%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 68.45%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 68.57%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 68.72%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 68.81%   [EVAL] batch:  213 | acc: 75.00%,  total acc: 68.84%   [EVAL] batch:  214 | acc: 81.25%,  total acc: 68.90%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 69.01%   [EVAL] batch:  216 | acc: 81.25%,  total acc: 69.07%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 69.21%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 69.29%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 69.43%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 69.65%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 69.76%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 69.87%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:  225 | acc: 62.50%,  total acc: 69.97%   [EVAL] batch:  226 | acc: 25.00%,  total acc: 69.77%   [EVAL] batch:  227 | acc: 56.25%,  total acc: 69.71%   [EVAL] batch:  228 | acc: 25.00%,  total acc: 69.51%   [EVAL] batch:  229 | acc: 6.25%,  total acc: 69.24%   [EVAL] batch:  230 | acc: 37.50%,  total acc: 69.10%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 69.21%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 69.31%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:  235 | acc: 93.75%,  total acc: 69.68%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 69.78%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 69.83%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 69.87%   [EVAL] batch:  239 | acc: 62.50%,  total acc: 69.84%   [EVAL] batch:  240 | acc: 68.75%,  total acc: 69.84%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 69.86%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 69.83%   [EVAL] batch:  243 | acc: 31.25%,  total acc: 69.67%   [EVAL] batch:  244 | acc: 81.25%,  total acc: 69.72%   [EVAL] batch:  245 | acc: 50.00%,  total acc: 69.64%   [EVAL] batch:  246 | acc: 56.25%,  total acc: 69.59%   [EVAL] batch:  247 | acc: 75.00%,  total acc: 69.61%   [EVAL] batch:  248 | acc: 81.25%,  total acc: 69.65%   [EVAL] batch:  249 | acc: 62.50%,  total acc: 69.62%   [EVAL] batch:  250 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 69.84%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 69.94%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 70.00%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 70.10%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 70.21%   [EVAL] batch:  256 | acc: 37.50%,  total acc: 70.09%   [EVAL] batch:  257 | acc: 18.75%,  total acc: 69.89%   [EVAL] batch:  258 | acc: 43.75%,  total acc: 69.79%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 69.74%   [EVAL] batch:  260 | acc: 18.75%,  total acc: 69.54%   [EVAL] batch:  261 | acc: 37.50%,  total acc: 69.42%   [EVAL] batch:  262 | acc: 50.00%,  total acc: 69.34%   [EVAL] batch:  263 | acc: 62.50%,  total acc: 69.32%   [EVAL] batch:  264 | acc: 56.25%,  total acc: 69.27%   [EVAL] batch:  265 | acc: 31.25%,  total acc: 69.13%   [EVAL] batch:  266 | acc: 50.00%,  total acc: 69.05%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 69.03%   [EVAL] batch:  268 | acc: 56.25%,  total acc: 68.98%   [EVAL] batch:  269 | acc: 68.75%,  total acc: 68.98%   [EVAL] batch:  270 | acc: 68.75%,  total acc: 68.98%   [EVAL] batch:  271 | acc: 68.75%,  total acc: 68.98%   [EVAL] batch:  272 | acc: 68.75%,  total acc: 68.98%   [EVAL] batch:  273 | acc: 75.00%,  total acc: 69.00%   [EVAL] batch:  274 | acc: 37.50%,  total acc: 68.89%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 69.00%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 69.11%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 69.22%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 69.33%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 69.55%   [EVAL] batch:  281 | acc: 81.25%,  total acc: 69.59%   [EVAL] batch:  282 | acc: 75.00%,  total acc: 69.61%   [EVAL] batch:  283 | acc: 56.25%,  total acc: 69.56%   [EVAL] batch:  284 | acc: 81.25%,  total acc: 69.61%   [EVAL] batch:  285 | acc: 62.50%,  total acc: 69.58%   [EVAL] batch:  286 | acc: 75.00%,  total acc: 69.60%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 69.62%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 69.72%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 69.83%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 69.93%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 70.03%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 70.22%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 70.32%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 70.42%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 70.52%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 70.62%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 70.71%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 70.81%   [EVAL] batch:  300 | acc: 81.25%,  total acc: 70.85%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 70.92%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 71.00%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 71.07%   [EVAL] batch:  304 | acc: 87.50%,  total acc: 71.13%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 71.20%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 71.27%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 71.35%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 71.42%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 71.51%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 71.60%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 71.65%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 71.59%   [EVAL] batch:  313 | acc: 12.50%,  total acc: 71.40%   [EVAL] batch:  314 | acc: 6.25%,  total acc: 71.19%   [EVAL] batch:  315 | acc: 6.25%,  total acc: 70.98%   [EVAL] batch:  316 | acc: 18.75%,  total acc: 70.82%   [EVAL] batch:  317 | acc: 6.25%,  total acc: 70.62%   [EVAL] batch:  318 | acc: 50.00%,  total acc: 70.55%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 70.57%   [EVAL] batch:  320 | acc: 87.50%,  total acc: 70.62%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 70.65%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 70.74%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 70.76%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 70.79%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 70.78%   [EVAL] batch:  326 | acc: 50.00%,  total acc: 70.72%   [EVAL] batch:  327 | acc: 62.50%,  total acc: 70.69%   [EVAL] batch:  328 | acc: 68.75%,  total acc: 70.69%   [EVAL] batch:  329 | acc: 43.75%,  total acc: 70.61%   [EVAL] batch:  330 | acc: 50.00%,  total acc: 70.54%   [EVAL] batch:  331 | acc: 12.50%,  total acc: 70.37%   [EVAL] batch:  332 | acc: 12.50%,  total acc: 70.20%   [EVAL] batch:  333 | acc: 18.75%,  total acc: 70.04%   [EVAL] batch:  334 | acc: 37.50%,  total acc: 69.94%   [EVAL] batch:  335 | acc: 25.00%,  total acc: 69.81%   [EVAL] batch:  336 | acc: 31.25%,  total acc: 69.70%   [EVAL] batch:  337 | acc: 18.75%,  total acc: 69.55%   [EVAL] batch:  338 | acc: 0.00%,  total acc: 69.34%   [EVAL] batch:  339 | acc: 6.25%,  total acc: 69.15%   [EVAL] batch:  340 | acc: 0.00%,  total acc: 68.95%   [EVAL] batch:  341 | acc: 12.50%,  total acc: 68.79%   [EVAL] batch:  342 | acc: 0.00%,  total acc: 68.59%   [EVAL] batch:  343 | acc: 12.50%,  total acc: 68.42%   [EVAL] batch:  344 | acc: 81.25%,  total acc: 68.46%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 68.53%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 68.59%   [EVAL] batch:  347 | acc: 87.50%,  total acc: 68.64%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 68.66%   [EVAL] batch:  349 | acc: 68.75%,  total acc: 68.66%   [EVAL] batch:  350 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  351 | acc: 87.50%,  total acc: 68.80%   [EVAL] batch:  352 | acc: 87.50%,  total acc: 68.86%   [EVAL] batch:  353 | acc: 87.50%,  total acc: 68.91%   [EVAL] batch:  354 | acc: 87.50%,  total acc: 68.96%   [EVAL] batch:  355 | acc: 100.00%,  total acc: 69.05%   [EVAL] batch:  356 | acc: 93.75%,  total acc: 69.12%   [EVAL] batch:  357 | acc: 87.50%,  total acc: 69.17%   [EVAL] batch:  358 | acc: 81.25%,  total acc: 69.20%   [EVAL] batch:  359 | acc: 87.50%,  total acc: 69.25%   [EVAL] batch:  360 | acc: 87.50%,  total acc: 69.30%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 69.35%   [EVAL] batch:  362 | acc: 56.25%,  total acc: 69.32%   [EVAL] batch:  363 | acc: 50.00%,  total acc: 69.27%   [EVAL] batch:  364 | acc: 50.00%,  total acc: 69.21%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 69.18%   [EVAL] batch:  366 | acc: 68.75%,  total acc: 69.18%   [EVAL] batch:  367 | acc: 50.00%,  total acc: 69.12%   [EVAL] batch:  368 | acc: 56.25%,  total acc: 69.09%   [EVAL] batch:  369 | acc: 50.00%,  total acc: 69.04%   [EVAL] batch:  370 | acc: 37.50%,  total acc: 68.95%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 68.88%   [EVAL] batch:  372 | acc: 62.50%,  total acc: 68.87%   [EVAL] batch:  373 | acc: 68.75%,  total acc: 68.87%   [EVAL] batch:  374 | acc: 50.00%,  total acc: 68.82%   [EVAL] batch:  375 | acc: 68.75%,  total acc: 68.82%   [EVAL] batch:  376 | acc: 50.00%,  total acc: 68.77%   [EVAL] batch:  377 | acc: 56.25%,  total acc: 68.73%   [EVAL] batch:  378 | acc: 56.25%,  total acc: 68.70%   [EVAL] batch:  379 | acc: 81.25%,  total acc: 68.73%   [EVAL] batch:  380 | acc: 87.50%,  total acc: 68.78%   [EVAL] batch:  381 | acc: 93.75%,  total acc: 68.85%   [EVAL] batch:  382 | acc: 93.75%,  total acc: 68.91%   [EVAL] batch:  383 | acc: 62.50%,  total acc: 68.90%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 68.94%   [EVAL] batch:  385 | acc: 93.75%,  total acc: 69.01%   [EVAL] batch:  386 | acc: 87.50%,  total acc: 69.06%   [EVAL] batch:  387 | acc: 62.50%,  total acc: 69.04%   [EVAL] batch:  388 | acc: 12.50%,  total acc: 68.89%   [EVAL] batch:  389 | acc: 25.00%,  total acc: 68.78%   [EVAL] batch:  390 | acc: 25.00%,  total acc: 68.67%   [EVAL] batch:  391 | acc: 50.00%,  total acc: 68.62%   [EVAL] batch:  392 | acc: 37.50%,  total acc: 68.54%   [EVAL] batch:  393 | acc: 37.50%,  total acc: 68.46%   [EVAL] batch:  394 | acc: 100.00%,  total acc: 68.54%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 68.62%   [EVAL] batch:  396 | acc: 93.75%,  total acc: 68.69%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 68.77%   [EVAL] batch:  398 | acc: 100.00%,  total acc: 68.84%   [EVAL] batch:  399 | acc: 93.75%,  total acc: 68.91%   [EVAL] batch:  400 | acc: 12.50%,  total acc: 68.77%   [EVAL] batch:  401 | acc: 12.50%,  total acc: 68.63%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 68.46%   [EVAL] batch:  403 | acc: 25.00%,  total acc: 68.35%   [EVAL] batch:  404 | acc: 12.50%,  total acc: 68.21%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 68.04%   [EVAL] batch:  406 | acc: 68.75%,  total acc: 68.04%   [EVAL] batch:  407 | acc: 93.75%,  total acc: 68.11%   [EVAL] batch:  408 | acc: 93.75%,  total acc: 68.17%   [EVAL] batch:  409 | acc: 93.75%,  total acc: 68.23%   [EVAL] batch:  410 | acc: 87.50%,  total acc: 68.28%   [EVAL] batch:  411 | acc: 93.75%,  total acc: 68.34%   [EVAL] batch:  412 | acc: 87.50%,  total acc: 68.39%   [EVAL] batch:  413 | acc: 68.75%,  total acc: 68.39%   [EVAL] batch:  414 | acc: 87.50%,  total acc: 68.43%   [EVAL] batch:  415 | acc: 62.50%,  total acc: 68.42%   [EVAL] batch:  416 | acc: 37.50%,  total acc: 68.35%   [EVAL] batch:  417 | acc: 68.75%,  total acc: 68.35%   [EVAL] batch:  418 | acc: 68.75%,  total acc: 68.35%   [EVAL] batch:  419 | acc: 75.00%,  total acc: 68.36%   [EVAL] batch:  420 | acc: 81.25%,  total acc: 68.39%   [EVAL] batch:  421 | acc: 87.50%,  total acc: 68.44%   [EVAL] batch:  422 | acc: 75.00%,  total acc: 68.45%   [EVAL] batch:  423 | acc: 93.75%,  total acc: 68.51%   [EVAL] batch:  424 | acc: 87.50%,  total acc: 68.56%   [EVAL] batch:  425 | acc: 62.50%,  total acc: 68.54%   [EVAL] batch:  426 | acc: 81.25%,  total acc: 68.57%   [EVAL] batch:  427 | acc: 62.50%,  total acc: 68.56%   [EVAL] batch:  428 | acc: 81.25%,  total acc: 68.59%   [EVAL] batch:  429 | acc: 93.75%,  total acc: 68.65%   [EVAL] batch:  430 | acc: 68.75%,  total acc: 68.65%   [EVAL] batch:  431 | acc: 75.00%,  total acc: 68.66%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 68.71%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 68.76%   [EVAL] batch:  434 | acc: 75.00%,  total acc: 68.78%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 68.82%   [EVAL] batch:  436 | acc: 75.00%,  total acc: 68.84%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 68.79%   
cur_acc:  ['0.9524', '0.8313', '0.7212', '0.7292', '0.8760', '0.6062', '0.6865']
his_acc:  ['0.9524', '0.8870', '0.7985', '0.7488', '0.7600', '0.7078', '0.6879']
CurrentTrain: epoch 15, batch     0 | loss: 12.4234946CurrentTrain: epoch 15, batch     1 | loss: 9.9860706CurrentTrain: epoch 15, batch     2 | loss: 19.1371626CurrentTrain: epoch  1, batch     3 | loss: 12.1145443CurrentTrain: epoch 15, batch     0 | loss: 11.4590684CurrentTrain: epoch 15, batch     1 | loss: 11.7839114CurrentTrain: epoch 15, batch     2 | loss: 12.1181645CurrentTrain: epoch  1, batch     3 | loss: 7.3486762CurrentTrain: epoch 15, batch     0 | loss: 8.6656636CurrentTrain: epoch 15, batch     1 | loss: 13.0131253CurrentTrain: epoch 15, batch     2 | loss: 12.3793798CurrentTrain: epoch  1, batch     3 | loss: 6.7636554CurrentTrain: epoch 15, batch     0 | loss: 8.2187034CurrentTrain: epoch 15, batch     1 | loss: 7.8446468CurrentTrain: epoch 15, batch     2 | loss: 10.2300714CurrentTrain: epoch  1, batch     3 | loss: 7.8804883CurrentTrain: epoch 15, batch     0 | loss: 6.1412404CurrentTrain: epoch 15, batch     1 | loss: 8.5825478CurrentTrain: epoch 15, batch     2 | loss: 11.7771593CurrentTrain: epoch  1, batch     3 | loss: 14.0994525CurrentTrain: epoch 15, batch     0 | loss: 12.2296234CurrentTrain: epoch 15, batch     1 | loss: 7.6572038CurrentTrain: epoch 15, batch     2 | loss: 11.5203707CurrentTrain: epoch  1, batch     3 | loss: 6.2693821CurrentTrain: epoch 15, batch     0 | loss: 9.3504520CurrentTrain: epoch 15, batch     1 | loss: 16.1046312CurrentTrain: epoch 15, batch     2 | loss: 9.3629950CurrentTrain: epoch  1, batch     3 | loss: 5.9656439CurrentTrain: epoch 15, batch     0 | loss: 11.7574637CurrentTrain: epoch 15, batch     1 | loss: 6.4796106CurrentTrain: epoch 15, batch     2 | loss: 7.3541546CurrentTrain: epoch  1, batch     3 | loss: 5.0957818CurrentTrain: epoch 15, batch     0 | loss: 13.0073748CurrentTrain: epoch 15, batch     1 | loss: 11.2969079CurrentTrain: epoch 15, batch     2 | loss: 7.7231404CurrentTrain: epoch  1, batch     3 | loss: 12.3477672CurrentTrain: epoch 15, batch     0 | loss: 7.6589436CurrentTrain: epoch 15, batch     1 | loss: 7.3868602CurrentTrain: epoch 15, batch     2 | loss: 15.0992960CurrentTrain: epoch  1, batch     3 | loss: 5.1838337
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the sutagao are the chibcha - speaking indios sutagaos indigenous people from the region of fusagasugá , bogotá savanna , cundinamarca , colombia .
Head Entity: colombia
Tail Entity: cundinamarca
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The state of California is known for its diverse geography and is divided into several counties, including Los Angeles County, which is the most populous county in the United States.  
Head Entity: California  
Tail Entity: Los Angeles County  

Relation: contains administrative territorial entity  
Context: The country of Japan is made up of several prefectures, with Tokyo being the most prominent and the capital of the nation.  
Head Entity: Japan  
Tail Entity: Tokyo  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: enzo is an italian given name derivative of heinz , a form of henry .
Head Entity: henry
Tail Entity: heinz
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as each other due to their close genetic relationship.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: In some circles, the terms "artificial intelligence" and "machine learning" are said to be the same as each other, although experts argue there are key differences.  
Head Entity: artificial intelligence  
Tail Entity: machine learning  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: in 2010 , ty burrell received a nomination for the television critics association award for individual achievement in comedy , winning in 2011 .
Head Entity: television critics association award for individual achievement in comedy
Tail Entity: ty burrell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: In 2022, the Boston Red Sox triumphed in the World Series, marking their fourth championship in the last two decades.  
Head Entity: World Series  
Tail Entity: Boston Red Sox  

Relation: winner  
Context: At the annual film festival, the documentary "Voices of Change" was awarded the Best Documentary prize, highlighting its impactful storytelling.  
Head Entity: Best Documentary prize  
Tail Entity: "Voices of Change"  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: the calais region was defended by the 15th army under " generaloberst " ( colonel general ) hans von salmuth , and normandy by the 7th army commanded by " generaloberst " friedrich dollmann .
Head Entity: hans von salmuth
Tail Entity: generaloberst
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: during the battle of stalingrad, the soviet forces were led by general vasily chuikov, who held the rank of major general, while the german troops were commanded by general friedrich paulus.  
Head Entity: vasily chuikov  
Tail Entity: major general  

Relation: military rank  
Context: admiral horatio nelson was a key figure in the british navy during the napoleonic wars, famously holding the rank of vice admiral at the time of the battle of trafalgar.  
Head Entity: horatio nelson  
Tail Entity: vice admiral  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: she went on to the film school at san francisco state university , when she was hired as an artist for an atari game called " electrocop " .
Head Entity: electrocop
Tail Entity: atari
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: The renowned author released her latest novel through a well-known publishing house that has been in the industry for decades.  
Head Entity: latest novel  
Tail Entity: publishing house  

Relation: publisher  
Context: After years of hard work, the independent game developer finally secured a deal with a major publisher to distribute their new game worldwide.  
Head Entity: new game  
Tail Entity: major publisher  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: following this song were the # 21 " she wants to be wanted again " , the # 2 " loved too much " and the # 17 " i have to surrender " .
Head Entity: she wants to be wanted again
Tail Entity: loved too much
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The top three movies of the year were "Inception," followed by "The Social Network" and "Toy Story 3."  
Head Entity: Inception  
Tail Entity: The Social Network  

Relation: followed by  
Context: The conference schedule included a keynote speech, followed by a panel discussion and then a networking session.  
Head Entity: keynote speech  
Tail Entity: panel discussion  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: he was also associated with robert wilkinson in producing " londina illustrata " , an illustrated account of ancient buildings in london and westminster in two volumes ( 1819–25 ) .
Head Entity: robert wilkinson
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: The headquarters of the company is situated in the bustling city of San Francisco, where it has been operating since its inception in 2010.  
Head Entity: the company  
Tail Entity: San Francisco  

Relation: work location  
Context: During her tenure at the university, she conducted groundbreaking research in neuroscience, primarily based in the labs located in Cambridge.  
Head Entity: she  
Tail Entity: Cambridge  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in enzyme catalysis.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a talented software engineer, has been working on innovative artificial intelligence projects for over a decade.  
Head Entity: john smith  
Tail Entity: software engineer  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a World Heritage Site, attracting tourists from around the globe.  
Head Entity: Petra  
Tail Entity: World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: beverly bower ( september 30 , 1925 – march 24 , 2002 ) was an american operatic soprano who had an active international opera career from the mid-1950s through the early 1970s .
Head Entity: beverly bower
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti ( october 12 , 1935 – september 6 , 2007 ) was an italian operatic tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey ( born march 27 , 1969 ) is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: soprano  
MemoryTrain:  epoch 15, batch     0 | loss: 3.1265902MemoryTrain:  epoch 15, batch     1 | loss: 3.4038412MemoryTrain:  epoch 15, batch     2 | loss: 3.2287095MemoryTrain:  epoch 15, batch     3 | loss: 5.5667029MemoryTrain:  epoch 15, batch     4 | loss: 4.8845276MemoryTrain:  epoch 15, batch     5 | loss: 2.7918448MemoryTrain:  epoch 15, batch     6 | loss: 2.5421583MemoryTrain:  epoch 15, batch     7 | loss: 3.8526469MemoryTrain:  epoch 15, batch     8 | loss: 3.2029549MemoryTrain:  epoch 15, batch     9 | loss: 3.5795694MemoryTrain:  epoch 15, batch    10 | loss: 2.9073700MemoryTrain:  epoch 15, batch    11 | loss: 3.2894292MemoryTrain:  epoch 15, batch    12 | loss: 3.1396509MemoryTrain:  epoch 15, batch    13 | loss: 3.2694268MemoryTrain:  epoch 15, batch    14 | loss: 2.8811147MemoryTrain:  epoch 15, batch     0 | loss: 1.8906691MemoryTrain:  epoch 15, batch     1 | loss: 3.8639577MemoryTrain:  epoch 15, batch     2 | loss: 2.8878961MemoryTrain:  epoch 15, batch     3 | loss: 3.1255015MemoryTrain:  epoch 15, batch     4 | loss: 2.4868233MemoryTrain:  epoch 15, batch     5 | loss: 3.0379062MemoryTrain:  epoch 15, batch     6 | loss: 4.7027460MemoryTrain:  epoch 15, batch     7 | loss: 3.5570240MemoryTrain:  epoch 15, batch     8 | loss: 2.6180610MemoryTrain:  epoch 15, batch     9 | loss: 2.7619699MemoryTrain:  epoch 15, batch    10 | loss: 2.4923112MemoryTrain:  epoch 15, batch    11 | loss: 3.7531523MemoryTrain:  epoch 15, batch    12 | loss: 2.1543041MemoryTrain:  epoch 15, batch    13 | loss: 2.4408643MemoryTrain:  epoch 15, batch    14 | loss: 1.6102660MemoryTrain:  epoch 15, batch     0 | loss: 1.8943154MemoryTrain:  epoch 15, batch     1 | loss: 1.7709670MemoryTrain:  epoch 15, batch     2 | loss: 2.0952231MemoryTrain:  epoch 15, batch     3 | loss: 2.0627178MemoryTrain:  epoch 15, batch     4 | loss: 1.9102460MemoryTrain:  epoch 15, batch     5 | loss: 2.2405068MemoryTrain:  epoch 15, batch     6 | loss: 3.3645449MemoryTrain:  epoch 15, batch     7 | loss: 1.8817805MemoryTrain:  epoch 15, batch     8 | loss: 2.1304453MemoryTrain:  epoch 15, batch     9 | loss: 1.9701513MemoryTrain:  epoch 15, batch    10 | loss: 2.2365432MemoryTrain:  epoch 15, batch    11 | loss: 1.9168506MemoryTrain:  epoch 15, batch    12 | loss: 2.3953422MemoryTrain:  epoch 15, batch    13 | loss: 2.0477492MemoryTrain:  epoch 15, batch    14 | loss: 2.0784394MemoryTrain:  epoch 15, batch     0 | loss: 3.5885155MemoryTrain:  epoch 15, batch     1 | loss: 1.7466722MemoryTrain:  epoch 15, batch     2 | loss: 2.4547127MemoryTrain:  epoch 15, batch     3 | loss: 1.7705196MemoryTrain:  epoch 15, batch     4 | loss: 1.5529815MemoryTrain:  epoch 15, batch     5 | loss: 2.1718628MemoryTrain:  epoch 15, batch     6 | loss: 1.9049796MemoryTrain:  epoch 15, batch     7 | loss: 1.8780357MemoryTrain:  epoch 15, batch     8 | loss: 4.2539420MemoryTrain:  epoch 15, batch     9 | loss: 1.4829960MemoryTrain:  epoch 15, batch    10 | loss: 1.8526992MemoryTrain:  epoch 15, batch    11 | loss: 1.8931292MemoryTrain:  epoch 15, batch    12 | loss: 1.8130856MemoryTrain:  epoch 15, batch    13 | loss: 2.3633127MemoryTrain:  epoch 15, batch    14 | loss: 2.3421099MemoryTrain:  epoch 15, batch     0 | loss: 1.7004569MemoryTrain:  epoch 15, batch     1 | loss: 1.8230116MemoryTrain:  epoch 15, batch     2 | loss: 2.2574717MemoryTrain:  epoch 15, batch     3 | loss: 1.7692618MemoryTrain:  epoch 15, batch     4 | loss: 2.9171358MemoryTrain:  epoch 15, batch     5 | loss: 4.3711925MemoryTrain:  epoch 15, batch     6 | loss: 1.9641118MemoryTrain:  epoch 15, batch     7 | loss: 1.6468097MemoryTrain:  epoch 15, batch     8 | loss: 1.4403448MemoryTrain:  epoch 15, batch     9 | loss: 1.4279845MemoryTrain:  epoch 15, batch    10 | loss: 1.3720350MemoryTrain:  epoch 15, batch    11 | loss: 2.0737076MemoryTrain:  epoch 15, batch    12 | loss: 4.0109750MemoryTrain:  epoch 15, batch    13 | loss: 2.0103110MemoryTrain:  epoch 15, batch    14 | loss: 3.1272110MemoryTrain:  epoch 15, batch     0 | loss: 1.2480674MemoryTrain:  epoch 15, batch     1 | loss: 1.5415928MemoryTrain:  epoch 15, batch     2 | loss: 1.6773557MemoryTrain:  epoch 15, batch     3 | loss: 1.2911901MemoryTrain:  epoch 15, batch     4 | loss: 2.5790520MemoryTrain:  epoch 15, batch     5 | loss: 1.4653522MemoryTrain:  epoch 15, batch     6 | loss: 1.4830509MemoryTrain:  epoch 15, batch     7 | loss: 1.8138559MemoryTrain:  epoch 15, batch     8 | loss: 1.5848802MemoryTrain:  epoch 15, batch     9 | loss: 3.8249071MemoryTrain:  epoch 15, batch    10 | loss: 1.7000155MemoryTrain:  epoch 15, batch    11 | loss: 4.3058107MemoryTrain:  epoch 15, batch    12 | loss: 1.6720449MemoryTrain:  epoch 15, batch    13 | loss: 1.5569325MemoryTrain:  epoch 15, batch    14 | loss: 2.4669237MemoryTrain:  epoch 15, batch     0 | loss: 1.7196984MemoryTrain:  epoch 15, batch     1 | loss: 1.8181710MemoryTrain:  epoch 15, batch     2 | loss: 1.6962954MemoryTrain:  epoch 15, batch     3 | loss: 1.6928724MemoryTrain:  epoch 15, batch     4 | loss: 1.7608820MemoryTrain:  epoch 15, batch     5 | loss: 1.9404436MemoryTrain:  epoch 15, batch     6 | loss: 1.6990232MemoryTrain:  epoch 15, batch     7 | loss: 1.6423546MemoryTrain:  epoch 15, batch     8 | loss: 1.3547550MemoryTrain:  epoch 15, batch     9 | loss: 1.3521045MemoryTrain:  epoch 15, batch    10 | loss: 2.4777716MemoryTrain:  epoch 15, batch    11 | loss: 1.4662133MemoryTrain:  epoch 15, batch    12 | loss: 4.0465264MemoryTrain:  epoch 15, batch    13 | loss: 1.5883452MemoryTrain:  epoch 15, batch    14 | loss: 1.5107051MemoryTrain:  epoch 15, batch     0 | loss: 1.2613002MemoryTrain:  epoch 15, batch     1 | loss: 1.4508442MemoryTrain:  epoch 15, batch     2 | loss: 3.9504974MemoryTrain:  epoch 15, batch     3 | loss: 1.9951942MemoryTrain:  epoch 15, batch     4 | loss: 1.3142644MemoryTrain:  epoch 15, batch     5 | loss: 1.4189212MemoryTrain:  epoch 15, batch     6 | loss: 1.6361221MemoryTrain:  epoch 15, batch     7 | loss: 1.9123102MemoryTrain:  epoch 15, batch     8 | loss: 1.5750192MemoryTrain:  epoch 15, batch     9 | loss: 1.5258229MemoryTrain:  epoch 15, batch    10 | loss: 9.3532443MemoryTrain:  epoch 15, batch    11 | loss: 1.2853760MemoryTrain:  epoch 15, batch    12 | loss: 1.6118890MemoryTrain:  epoch 15, batch    13 | loss: 1.7638632MemoryTrain:  epoch 15, batch    14 | loss: 3.7352399MemoryTrain:  epoch 15, batch     0 | loss: 1.6586250MemoryTrain:  epoch 15, batch     1 | loss: 1.6342636MemoryTrain:  epoch 15, batch     2 | loss: 1.7228462MemoryTrain:  epoch 15, batch     3 | loss: 4.1390190MemoryTrain:  epoch 15, batch     4 | loss: 1.7655779MemoryTrain:  epoch 15, batch     5 | loss: 1.5975899MemoryTrain:  epoch 15, batch     6 | loss: 1.4063847MemoryTrain:  epoch 15, batch     7 | loss: 1.3035213MemoryTrain:  epoch 15, batch     8 | loss: 1.7313421MemoryTrain:  epoch 15, batch     9 | loss: 1.8200843MemoryTrain:  epoch 15, batch    10 | loss: 3.9673090MemoryTrain:  epoch 15, batch    11 | loss: 1.6580732MemoryTrain:  epoch 15, batch    12 | loss: 1.4168355MemoryTrain:  epoch 15, batch    13 | loss: 1.4817849MemoryTrain:  epoch 15, batch    14 | loss: 1.6703492MemoryTrain:  epoch 15, batch     0 | loss: 1.6940783MemoryTrain:  epoch 15, batch     1 | loss: 1.4554686MemoryTrain:  epoch 15, batch     2 | loss: 4.0493939MemoryTrain:  epoch 15, batch     3 | loss: 1.6022235MemoryTrain:  epoch 15, batch     4 | loss: 1.3411932MemoryTrain:  epoch 15, batch     5 | loss: 5.1269908MemoryTrain:  epoch 15, batch     6 | loss: 1.6794084MemoryTrain:  epoch 15, batch     7 | loss: 1.8992579MemoryTrain:  epoch 15, batch     8 | loss: 1.6184044MemoryTrain:  epoch 15, batch     9 | loss: 3.7673558MemoryTrain:  epoch 15, batch    10 | loss: 1.4277538MemoryTrain:  epoch 15, batch    11 | loss: 2.2850750MemoryTrain:  epoch 15, batch    12 | loss: 4.0482397MemoryTrain:  epoch 15, batch    13 | loss: 1.6855162MemoryTrain:  epoch 15, batch    14 | loss: 3.4911553
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 53.12%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 55.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 58.33%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 57.81%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 59.03%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 60.00%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 59.09%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 59.90%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 63.84%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 65.00%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 66.02%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 67.28%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 68.40%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 69.41%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 70.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 71.73%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 73.01%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 73.91%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 74.48%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 75.50%   [EVAL] batch:   25 | acc: 68.75%,  total acc: 75.24%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 75.23%   [EVAL] batch:   27 | acc: 75.00%,  total acc: 75.22%   [EVAL] batch:   28 | acc: 81.25%,  total acc: 75.43%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 75.62%   [EVAL] batch:   30 | acc: 62.50%,  total acc: 75.20%   [EVAL] batch:   31 | acc: 37.50%,  total acc: 74.02%   [EVAL] batch:   32 | acc: 31.25%,  total acc: 72.73%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 70.96%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 69.29%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 67.88%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 66.39%   [EVAL] batch:   37 | acc: 37.50%,  total acc: 65.62%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 65.38%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 65.31%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 65.09%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 65.03%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 65.12%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 65.06%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 65.42%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 65.49%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 65.43%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 65.23%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 65.43%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 65.88%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 66.54%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 67.81%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 68.40%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 68.98%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 69.96%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 70.26%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 70.66%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 71.52%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 71.98%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 71.63%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 67.19%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 65.97%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 65.00%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 62.50%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 60.42%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 60.58%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 64.17%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 66.02%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 68.01%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 69.79%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 71.05%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 70.62%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 70.45%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 69.84%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 70.31%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 70.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 71.39%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 72.22%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 72.77%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 73.49%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 74.38%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 75.39%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 76.14%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 76.10%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 76.07%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 75.87%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 76.18%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 76.32%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 76.92%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 77.50%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 78.05%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 78.42%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 78.78%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 78.84%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 78.53%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 77.93%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 77.47%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 77.30%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 76.62%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 76.72%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 76.92%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 76.89%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 76.62%   [EVAL] batch:   54 | acc: 43.75%,  total acc: 76.02%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 75.89%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 75.77%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 75.32%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 75.11%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 75.21%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 75.31%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 75.30%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 75.20%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 74.51%   [EVAL] batch:   64 | acc: 50.00%,  total acc: 74.13%   [EVAL] batch:   65 | acc: 31.25%,  total acc: 73.48%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 72.76%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 72.43%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 72.01%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 72.05%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 72.01%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 71.79%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 72.00%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 72.04%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 72.17%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 72.04%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 72.16%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 72.04%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 71.99%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 71.99%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 71.42%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 70.63%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 69.79%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 69.12%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 68.53%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 67.96%   [EVAL] batch:   87 | acc: 31.25%,  total acc: 67.54%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 67.77%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 68.06%   [EVAL] batch:   90 | acc: 37.50%,  total acc: 67.72%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 68.00%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 68.08%   [EVAL] batch:   93 | acc: 81.25%,  total acc: 68.22%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 68.55%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 68.88%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 69.20%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 69.52%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 69.82%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 70.12%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 70.42%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 70.65%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 70.81%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 71.03%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 71.31%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 71.52%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 71.20%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 70.60%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 70.07%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 69.49%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 69.09%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 68.69%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 68.64%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 68.80%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 69.08%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 69.23%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 69.39%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 69.60%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 69.54%   [EVAL] batch:  119 | acc: 12.50%,  total acc: 69.06%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 68.60%   [EVAL] batch:  121 | acc: 18.75%,  total acc: 68.19%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 67.68%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 67.19%   [EVAL] batch:  124 | acc: 12.50%,  total acc: 66.75%   [EVAL] batch:  125 | acc: 37.50%,  total acc: 66.52%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 66.24%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 65.92%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 65.70%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 65.58%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 65.46%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 65.88%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 66.09%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 66.20%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 66.41%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 66.65%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 66.71%   [EVAL] batch:  138 | acc: 50.00%,  total acc: 66.59%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 66.52%   [EVAL] batch:  140 | acc: 62.50%,  total acc: 66.49%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 66.55%   [EVAL] batch:  142 | acc: 62.50%,  total acc: 66.52%   [EVAL] batch:  143 | acc: 75.00%,  total acc: 66.58%   [EVAL] batch:  144 | acc: 37.50%,  total acc: 66.38%   [EVAL] batch:  145 | acc: 50.00%,  total acc: 66.27%   [EVAL] batch:  146 | acc: 50.00%,  total acc: 66.16%   [EVAL] batch:  147 | acc: 37.50%,  total acc: 65.96%   [EVAL] batch:  148 | acc: 50.00%,  total acc: 65.86%   [EVAL] batch:  149 | acc: 43.75%,  total acc: 65.71%   [EVAL] batch:  150 | acc: 68.75%,  total acc: 65.73%   [EVAL] batch:  151 | acc: 81.25%,  total acc: 65.83%   [EVAL] batch:  152 | acc: 62.50%,  total acc: 65.81%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 65.95%   [EVAL] batch:  154 | acc: 50.00%,  total acc: 65.85%   [EVAL] batch:  155 | acc: 50.00%,  total acc: 65.75%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 65.72%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 65.62%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 65.53%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 65.51%   [EVAL] batch:  160 | acc: 68.75%,  total acc: 65.53%   [EVAL] batch:  161 | acc: 50.00%,  total acc: 65.43%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 65.41%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 65.43%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 65.42%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 65.44%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 65.46%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 65.51%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 65.53%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 65.29%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 65.10%   [EVAL] batch:  171 | acc: 25.00%,  total acc: 64.86%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 64.67%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 64.48%   [EVAL] batch:  174 | acc: 31.25%,  total acc: 64.29%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 64.49%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 64.69%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 64.89%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 65.08%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 65.28%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 65.47%   [EVAL] batch:  181 | acc: 81.25%,  total acc: 65.56%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 65.51%   [EVAL] batch:  183 | acc: 75.00%,  total acc: 65.56%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 65.71%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 65.73%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 65.81%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 65.72%   [EVAL] batch:  188 | acc: 18.75%,  total acc: 65.48%   [EVAL] batch:  189 | acc: 25.00%,  total acc: 65.26%   [EVAL] batch:  190 | acc: 12.50%,  total acc: 64.99%   [EVAL] batch:  191 | acc: 12.50%,  total acc: 64.71%   [EVAL] batch:  192 | acc: 25.00%,  total acc: 64.51%   [EVAL] batch:  193 | acc: 25.00%,  total acc: 64.30%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 64.33%   [EVAL] batch:  195 | acc: 81.25%,  total acc: 64.41%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 64.47%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 64.55%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 64.60%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 64.66%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 64.65%   [EVAL] batch:  201 | acc: 43.75%,  total acc: 64.54%   [EVAL] batch:  202 | acc: 50.00%,  total acc: 64.47%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 64.43%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 64.42%   [EVAL] batch:  205 | acc: 37.50%,  total acc: 64.29%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 64.40%   [EVAL] batch:  207 | acc: 93.75%,  total acc: 64.54%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 64.71%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 64.85%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 64.99%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 65.15%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 65.26%   [EVAL] batch:  213 | acc: 75.00%,  total acc: 65.30%   [EVAL] batch:  214 | acc: 75.00%,  total acc: 65.35%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 65.45%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 65.50%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 65.65%   [EVAL] batch:  218 | acc: 81.25%,  total acc: 65.72%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 65.88%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 66.04%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 66.13%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 66.26%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 66.38%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 66.53%   [EVAL] batch:  225 | acc: 50.00%,  total acc: 66.45%   [EVAL] batch:  226 | acc: 18.75%,  total acc: 66.24%   [EVAL] batch:  227 | acc: 50.00%,  total acc: 66.17%   [EVAL] batch:  228 | acc: 25.00%,  total acc: 65.99%   [EVAL] batch:  229 | acc: 6.25%,  total acc: 65.73%   [EVAL] batch:  230 | acc: 12.50%,  total acc: 65.50%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:  232 | acc: 75.00%,  total acc: 65.67%   [EVAL] batch:  233 | acc: 81.25%,  total acc: 65.73%   [EVAL] batch:  234 | acc: 93.75%,  total acc: 65.85%   [EVAL] batch:  235 | acc: 68.75%,  total acc: 65.86%   [EVAL] batch:  236 | acc: 68.75%,  total acc: 65.88%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 65.89%   [EVAL] batch:  238 | acc: 75.00%,  total acc: 65.93%   [EVAL] batch:  239 | acc: 56.25%,  total acc: 65.89%   [EVAL] batch:  240 | acc: 62.50%,  total acc: 65.87%   [EVAL] batch:  241 | acc: 68.75%,  total acc: 65.88%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 65.87%   [EVAL] batch:  243 | acc: 31.25%,  total acc: 65.73%   [EVAL] batch:  244 | acc: 81.25%,  total acc: 65.79%   [EVAL] batch:  245 | acc: 50.00%,  total acc: 65.73%   [EVAL] batch:  246 | acc: 50.00%,  total acc: 65.66%   [EVAL] batch:  247 | acc: 68.75%,  total acc: 65.68%   [EVAL] batch:  248 | acc: 81.25%,  total acc: 65.74%   [EVAL] batch:  249 | acc: 62.50%,  total acc: 65.72%   [EVAL] batch:  250 | acc: 100.00%,  total acc: 65.86%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 65.97%   [EVAL] batch:  252 | acc: 100.00%,  total acc: 66.11%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 66.19%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 66.30%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 66.43%   [EVAL] batch:  256 | acc: 43.75%,  total acc: 66.34%   [EVAL] batch:  257 | acc: 18.75%,  total acc: 66.16%   [EVAL] batch:  258 | acc: 37.50%,  total acc: 66.05%   [EVAL] batch:  259 | acc: 50.00%,  total acc: 65.99%   [EVAL] batch:  260 | acc: 18.75%,  total acc: 65.80%   [EVAL] batch:  261 | acc: 43.75%,  total acc: 65.72%   [EVAL] batch:  262 | acc: 50.00%,  total acc: 65.66%   [EVAL] batch:  263 | acc: 68.75%,  total acc: 65.67%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 65.66%   [EVAL] batch:  265 | acc: 37.50%,  total acc: 65.55%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 65.52%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 65.51%   [EVAL] batch:  268 | acc: 37.50%,  total acc: 65.40%   [EVAL] batch:  269 | acc: 25.00%,  total acc: 65.25%   [EVAL] batch:  270 | acc: 12.50%,  total acc: 65.06%   [EVAL] batch:  271 | acc: 18.75%,  total acc: 64.89%   [EVAL] batch:  272 | acc: 50.00%,  total acc: 64.84%   [EVAL] batch:  273 | acc: 25.00%,  total acc: 64.69%   [EVAL] batch:  274 | acc: 12.50%,  total acc: 64.50%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 64.63%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 64.76%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 64.88%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 65.01%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 65.13%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 65.26%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 65.25%   [EVAL] batch:  282 | acc: 68.75%,  total acc: 65.26%   [EVAL] batch:  283 | acc: 56.25%,  total acc: 65.23%   [EVAL] batch:  284 | acc: 62.50%,  total acc: 65.22%   [EVAL] batch:  285 | acc: 62.50%,  total acc: 65.21%   [EVAL] batch:  286 | acc: 56.25%,  total acc: 65.18%   [EVAL] batch:  287 | acc: 62.50%,  total acc: 65.17%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 65.29%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 65.41%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 65.53%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 65.65%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 65.76%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 65.86%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 65.97%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 66.09%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 66.20%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 66.32%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 66.43%   [EVAL] batch:  299 | acc: 93.75%,  total acc: 66.52%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 66.61%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 66.70%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 66.79%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 66.90%   [EVAL] batch:  304 | acc: 87.50%,  total acc: 66.97%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 67.08%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 67.16%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 67.25%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 67.33%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 67.44%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 67.54%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 67.61%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 67.55%   [EVAL] batch:  313 | acc: 6.25%,  total acc: 67.36%   [EVAL] batch:  314 | acc: 0.00%,  total acc: 67.14%   [EVAL] batch:  315 | acc: 6.25%,  total acc: 66.95%   [EVAL] batch:  316 | acc: 18.75%,  total acc: 66.80%   [EVAL] batch:  317 | acc: 6.25%,  total acc: 66.61%   [EVAL] batch:  318 | acc: 50.00%,  total acc: 66.56%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 66.60%   [EVAL] batch:  320 | acc: 87.50%,  total acc: 66.67%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 66.71%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 66.82%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 66.84%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 66.90%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 66.91%   [EVAL] batch:  326 | acc: 50.00%,  total acc: 66.86%   [EVAL] batch:  327 | acc: 62.50%,  total acc: 66.84%   [EVAL] batch:  328 | acc: 68.75%,  total acc: 66.85%   [EVAL] batch:  329 | acc: 43.75%,  total acc: 66.78%   [EVAL] batch:  330 | acc: 56.25%,  total acc: 66.75%   [EVAL] batch:  331 | acc: 6.25%,  total acc: 66.57%   [EVAL] batch:  332 | acc: 6.25%,  total acc: 66.39%   [EVAL] batch:  333 | acc: 12.50%,  total acc: 66.22%   [EVAL] batch:  334 | acc: 31.25%,  total acc: 66.12%   [EVAL] batch:  335 | acc: 12.50%,  total acc: 65.96%   [EVAL] batch:  336 | acc: 25.00%,  total acc: 65.84%   [EVAL] batch:  337 | acc: 6.25%,  total acc: 65.66%   [EVAL] batch:  338 | acc: 0.00%,  total acc: 65.47%   [EVAL] batch:  339 | acc: 6.25%,  total acc: 65.29%   [EVAL] batch:  340 | acc: 0.00%,  total acc: 65.10%   [EVAL] batch:  341 | acc: 12.50%,  total acc: 64.95%   [EVAL] batch:  342 | acc: 0.00%,  total acc: 64.76%   [EVAL] batch:  343 | acc: 25.00%,  total acc: 64.64%   [EVAL] batch:  344 | acc: 68.75%,  total acc: 64.66%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 64.74%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 64.81%   [EVAL] batch:  347 | acc: 87.50%,  total acc: 64.87%   [EVAL] batch:  348 | acc: 68.75%,  total acc: 64.88%   [EVAL] batch:  349 | acc: 62.50%,  total acc: 64.88%   [EVAL] batch:  350 | acc: 100.00%,  total acc: 64.98%   [EVAL] batch:  351 | acc: 87.50%,  total acc: 65.04%   [EVAL] batch:  352 | acc: 81.25%,  total acc: 65.08%   [EVAL] batch:  353 | acc: 87.50%,  total acc: 65.15%   [EVAL] batch:  354 | acc: 87.50%,  total acc: 65.21%   [EVAL] batch:  355 | acc: 100.00%,  total acc: 65.31%   [EVAL] batch:  356 | acc: 56.25%,  total acc: 65.28%   [EVAL] batch:  357 | acc: 31.25%,  total acc: 65.19%   [EVAL] batch:  358 | acc: 37.50%,  total acc: 65.11%   [EVAL] batch:  359 | acc: 37.50%,  total acc: 65.03%   [EVAL] batch:  360 | acc: 50.00%,  total acc: 64.99%   [EVAL] batch:  361 | acc: 31.25%,  total acc: 64.90%   [EVAL] batch:  362 | acc: 37.50%,  total acc: 64.82%   [EVAL] batch:  363 | acc: 62.50%,  total acc: 64.82%   [EVAL] batch:  364 | acc: 50.00%,  total acc: 64.78%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 64.75%   [EVAL] batch:  366 | acc: 68.75%,  total acc: 64.76%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 64.74%   [EVAL] batch:  368 | acc: 50.00%,  total acc: 64.70%   [EVAL] batch:  369 | acc: 50.00%,  total acc: 64.66%   [EVAL] batch:  370 | acc: 43.75%,  total acc: 64.61%   [EVAL] batch:  371 | acc: 56.25%,  total acc: 64.58%   [EVAL] batch:  372 | acc: 62.50%,  total acc: 64.58%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 64.62%   [EVAL] batch:  374 | acc: 56.25%,  total acc: 64.60%   [EVAL] batch:  375 | acc: 56.25%,  total acc: 64.58%   [EVAL] batch:  376 | acc: 50.00%,  total acc: 64.54%   [EVAL] batch:  377 | acc: 56.25%,  total acc: 64.52%   [EVAL] batch:  378 | acc: 50.00%,  total acc: 64.48%   [EVAL] batch:  379 | acc: 75.00%,  total acc: 64.51%   [EVAL] batch:  380 | acc: 75.00%,  total acc: 64.53%   [EVAL] batch:  381 | acc: 68.75%,  total acc: 64.55%   [EVAL] batch:  382 | acc: 56.25%,  total acc: 64.52%   [EVAL] batch:  383 | acc: 50.00%,  total acc: 64.49%   [EVAL] batch:  384 | acc: 56.25%,  total acc: 64.46%   [EVAL] batch:  385 | acc: 68.75%,  total acc: 64.48%   [EVAL] batch:  386 | acc: 62.50%,  total acc: 64.47%   [EVAL] batch:  387 | acc: 37.50%,  total acc: 64.40%   [EVAL] batch:  388 | acc: 12.50%,  total acc: 64.27%   [EVAL] batch:  389 | acc: 25.00%,  total acc: 64.17%   [EVAL] batch:  390 | acc: 12.50%,  total acc: 64.03%   [EVAL] batch:  391 | acc: 37.50%,  total acc: 63.97%   [EVAL] batch:  392 | acc: 43.75%,  total acc: 63.92%   [EVAL] batch:  393 | acc: 37.50%,  total acc: 63.85%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 63.92%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 64.02%   [EVAL] batch:  396 | acc: 93.75%,  total acc: 64.09%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 64.18%   [EVAL] batch:  398 | acc: 100.00%,  total acc: 64.27%   [EVAL] batch:  399 | acc: 93.75%,  total acc: 64.34%   [EVAL] batch:  400 | acc: 0.00%,  total acc: 64.18%   [EVAL] batch:  401 | acc: 6.25%,  total acc: 64.04%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 63.88%   [EVAL] batch:  403 | acc: 25.00%,  total acc: 63.78%   [EVAL] batch:  404 | acc: 12.50%,  total acc: 63.66%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 63.50%   [EVAL] batch:  406 | acc: 68.75%,  total acc: 63.51%   [EVAL] batch:  407 | acc: 93.75%,  total acc: 63.59%   [EVAL] batch:  408 | acc: 93.75%,  total acc: 63.66%   [EVAL] batch:  409 | acc: 93.75%,  total acc: 63.73%   [EVAL] batch:  410 | acc: 87.50%,  total acc: 63.79%   [EVAL] batch:  411 | acc: 93.75%,  total acc: 63.87%   [EVAL] batch:  412 | acc: 87.50%,  total acc: 63.92%   [EVAL] batch:  413 | acc: 68.75%,  total acc: 63.93%   [EVAL] batch:  414 | acc: 75.00%,  total acc: 63.96%   [EVAL] batch:  415 | acc: 50.00%,  total acc: 63.93%   [EVAL] batch:  416 | acc: 43.75%,  total acc: 63.88%   [EVAL] batch:  417 | acc: 75.00%,  total acc: 63.91%   [EVAL] batch:  418 | acc: 75.00%,  total acc: 63.93%   [EVAL] batch:  419 | acc: 81.25%,  total acc: 63.97%   [EVAL] batch:  420 | acc: 93.75%,  total acc: 64.04%   [EVAL] batch:  421 | acc: 93.75%,  total acc: 64.11%   [EVAL] batch:  422 | acc: 75.00%,  total acc: 64.14%   [EVAL] batch:  423 | acc: 93.75%,  total acc: 64.21%   [EVAL] batch:  424 | acc: 93.75%,  total acc: 64.28%   [EVAL] batch:  425 | acc: 62.50%,  total acc: 64.28%   [EVAL] batch:  426 | acc: 68.75%,  total acc: 64.29%   [EVAL] batch:  427 | acc: 68.75%,  total acc: 64.30%   [EVAL] batch:  428 | acc: 75.00%,  total acc: 64.32%   [EVAL] batch:  429 | acc: 87.50%,  total acc: 64.38%   [EVAL] batch:  430 | acc: 62.50%,  total acc: 64.37%   [EVAL] batch:  431 | acc: 68.75%,  total acc: 64.38%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 64.43%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 64.50%   [EVAL] batch:  434 | acc: 68.75%,  total acc: 64.51%   [EVAL] batch:  435 | acc: 81.25%,  total acc: 64.55%   [EVAL] batch:  436 | acc: 68.75%,  total acc: 64.56%   [EVAL] batch:  437 | acc: 68.75%,  total acc: 64.57%   [EVAL] batch:  438 | acc: 62.50%,  total acc: 64.56%   [EVAL] batch:  439 | acc: 31.25%,  total acc: 64.49%   [EVAL] batch:  440 | acc: 68.75%,  total acc: 64.50%   [EVAL] batch:  441 | acc: 68.75%,  total acc: 64.51%   [EVAL] batch:  442 | acc: 68.75%,  total acc: 64.52%   [EVAL] batch:  443 | acc: 50.00%,  total acc: 64.48%   [EVAL] batch:  444 | acc: 56.25%,  total acc: 64.47%   [EVAL] batch:  445 | acc: 68.75%,  total acc: 64.48%   [EVAL] batch:  446 | acc: 68.75%,  total acc: 64.49%   [EVAL] batch:  447 | acc: 75.00%,  total acc: 64.51%   [EVAL] batch:  448 | acc: 31.25%,  total acc: 64.43%   [EVAL] batch:  449 | acc: 100.00%,  total acc: 64.51%   [EVAL] batch:  450 | acc: 87.50%,  total acc: 64.56%   [EVAL] batch:  451 | acc: 68.75%,  total acc: 64.57%   [EVAL] batch:  452 | acc: 93.75%,  total acc: 64.64%   [EVAL] batch:  453 | acc: 81.25%,  total acc: 64.68%   [EVAL] batch:  454 | acc: 81.25%,  total acc: 64.71%   [EVAL] batch:  455 | acc: 93.75%,  total acc: 64.78%   [EVAL] batch:  456 | acc: 87.50%,  total acc: 64.82%   [EVAL] batch:  457 | acc: 93.75%,  total acc: 64.89%   [EVAL] batch:  458 | acc: 100.00%,  total acc: 64.96%   [EVAL] batch:  459 | acc: 100.00%,  total acc: 65.04%   [EVAL] batch:  460 | acc: 81.25%,  total acc: 65.08%   [EVAL] batch:  461 | acc: 100.00%,  total acc: 65.15%   [EVAL] batch:  462 | acc: 81.25%,  total acc: 65.19%   [EVAL] batch:  463 | acc: 62.50%,  total acc: 65.18%   [EVAL] batch:  464 | acc: 81.25%,  total acc: 65.22%   [EVAL] batch:  465 | acc: 87.50%,  total acc: 65.26%   [EVAL] batch:  466 | acc: 81.25%,  total acc: 65.30%   [EVAL] batch:  467 | acc: 75.00%,  total acc: 65.32%   [EVAL] batch:  468 | acc: 50.00%,  total acc: 65.29%   [EVAL] batch:  469 | acc: 25.00%,  total acc: 65.20%   [EVAL] batch:  470 | acc: 25.00%,  total acc: 65.11%   [EVAL] batch:  471 | acc: 12.50%,  total acc: 65.00%   [EVAL] batch:  472 | acc: 12.50%,  total acc: 64.89%   [EVAL] batch:  473 | acc: 25.00%,  total acc: 64.81%   [EVAL] batch:  474 | acc: 18.75%,  total acc: 64.71%   [EVAL] batch:  475 | acc: 43.75%,  total acc: 64.67%   [EVAL] batch:  476 | acc: 62.50%,  total acc: 64.66%   [EVAL] batch:  477 | acc: 56.25%,  total acc: 64.64%   [EVAL] batch:  478 | acc: 68.75%,  total acc: 64.65%   [EVAL] batch:  479 | acc: 62.50%,  total acc: 64.65%   [EVAL] batch:  480 | acc: 56.25%,  total acc: 64.63%   [EVAL] batch:  481 | acc: 81.25%,  total acc: 64.67%   [EVAL] batch:  482 | acc: 68.75%,  total acc: 64.67%   [EVAL] batch:  483 | acc: 68.75%,  total acc: 64.68%   [EVAL] batch:  484 | acc: 62.50%,  total acc: 64.68%   [EVAL] batch:  485 | acc: 56.25%,  total acc: 64.66%   [EVAL] batch:  486 | acc: 87.50%,  total acc: 64.71%   [EVAL] batch:  487 | acc: 93.75%,  total acc: 64.77%   [EVAL] batch:  488 | acc: 100.00%,  total acc: 64.84%   [EVAL] batch:  489 | acc: 100.00%,  total acc: 64.91%   [EVAL] batch:  490 | acc: 100.00%,  total acc: 64.98%   [EVAL] batch:  491 | acc: 100.00%,  total acc: 65.05%   [EVAL] batch:  492 | acc: 100.00%,  total acc: 65.12%   [EVAL] batch:  493 | acc: 100.00%,  total acc: 65.19%   [EVAL] batch:  494 | acc: 93.75%,  total acc: 65.25%   [EVAL] batch:  495 | acc: 87.50%,  total acc: 65.30%   [EVAL] batch:  496 | acc: 93.75%,  total acc: 65.35%   [EVAL] batch:  497 | acc: 93.75%,  total acc: 65.41%   [EVAL] batch:  498 | acc: 100.00%,  total acc: 65.48%   [EVAL] batch:  499 | acc: 100.00%,  total acc: 65.55%   
cur_acc:  ['0.9524', '0.8313', '0.7212', '0.7292', '0.8760', '0.6062', '0.6865', '0.7163']
his_acc:  ['0.9524', '0.8870', '0.7985', '0.7488', '0.7600', '0.7078', '0.6879', '0.6555']
--------Round  3
seed:  400
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 0 1 2 5 3 4 6]
prepared data!
CurrentTrain: epoch 15, batch     0 | loss: 20.0149927CurrentTrain: epoch 15, batch     1 | loss: 20.8952404CurrentTrain: epoch 15, batch     2 | loss: 21.0672532CurrentTrain: epoch 15, batch     3 | loss: 26.3600908CurrentTrain: epoch 15, batch     4 | loss: 24.7300763CurrentTrain: epoch 15, batch     5 | loss: 20.3547633CurrentTrain: epoch 15, batch     6 | loss: 23.3799013CurrentTrain: epoch 15, batch     7 | loss: 25.7824977CurrentTrain: epoch 15, batch     8 | loss: 22.5702032CurrentTrain: epoch 15, batch     9 | loss: 27.4555327CurrentTrain: epoch 15, batch    10 | loss: 15.5666495CurrentTrain: epoch 15, batch    11 | loss: 22.6583352CurrentTrain: epoch 15, batch    12 | loss: 25.4575388CurrentTrain: epoch 15, batch    13 | loss: 21.5787838CurrentTrain: epoch 15, batch    14 | loss: 25.0348657CurrentTrain: epoch 15, batch    15 | loss: 18.0737220CurrentTrain: epoch 15, batch    16 | loss: 20.8095905CurrentTrain: epoch 15, batch    17 | loss: 21.9671132CurrentTrain: epoch 15, batch    18 | loss: 23.4225867CurrentTrain: epoch 15, batch    19 | loss: 19.5913344CurrentTrain: epoch 15, batch    20 | loss: 19.3283252CurrentTrain: epoch 15, batch    21 | loss: 26.7085904CurrentTrain: epoch 15, batch    22 | loss: 16.0577888CurrentTrain: epoch 15, batch    23 | loss: 12.1244093CurrentTrain: epoch 15, batch    24 | loss: 23.3262132CurrentTrain: epoch 15, batch    25 | loss: 14.9345305CurrentTrain: epoch 15, batch    26 | loss: 20.5364833CurrentTrain: epoch 15, batch    27 | loss: 17.6717368CurrentTrain: epoch 15, batch    28 | loss: 20.6391130CurrentTrain: epoch 15, batch    29 | loss: 16.2209381CurrentTrain: epoch 15, batch    30 | loss: 14.3458300CurrentTrain: epoch 15, batch    31 | loss: 15.1927561CurrentTrain: epoch 15, batch    32 | loss: 18.0607385CurrentTrain: epoch 15, batch    33 | loss: 15.8925843CurrentTrain: epoch 15, batch    34 | loss: 13.0654746CurrentTrain: epoch 15, batch    35 | loss: 13.8436951CurrentTrain: epoch 15, batch    36 | loss: 15.7840237CurrentTrain: epoch 15, batch    37 | loss: 14.6257315CurrentTrain: epoch 15, batch    38 | loss: 19.1579931CurrentTrain: epoch 15, batch    39 | loss: 18.2944282CurrentTrain: epoch 15, batch    40 | loss: 20.2086277CurrentTrain: epoch 15, batch    41 | loss: 21.5740539CurrentTrain: epoch 15, batch    42 | loss: 21.3947417CurrentTrain: epoch 15, batch    43 | loss: 15.4255730CurrentTrain: epoch 15, batch    44 | loss: 14.7285786CurrentTrain: epoch 15, batch    45 | loss: 18.7021135CurrentTrain: epoch 15, batch    46 | loss: 16.5286068CurrentTrain: epoch 15, batch    47 | loss: 21.4809606CurrentTrain: epoch 15, batch    48 | loss: 21.6747398CurrentTrain: epoch 15, batch    49 | loss: 9.3781947CurrentTrain: epoch 15, batch    50 | loss: 18.2826689CurrentTrain: epoch 15, batch    51 | loss: 20.5037471CurrentTrain: epoch 15, batch    52 | loss: 12.8095641CurrentTrain: epoch 15, batch    53 | loss: 15.5486831CurrentTrain: epoch 15, batch    54 | loss: 26.1502033CurrentTrain: epoch 15, batch    55 | loss: 10.6704310CurrentTrain: epoch 15, batch    56 | loss: 12.1077520CurrentTrain: epoch 15, batch    57 | loss: 14.3545109CurrentTrain: epoch 15, batch    58 | loss: 12.8622325CurrentTrain: epoch 15, batch    59 | loss: 13.8688343CurrentTrain: epoch 15, batch    60 | loss: 13.6461651CurrentTrain: epoch 15, batch    61 | loss: 14.5605168CurrentTrain: epoch  7, batch    62 | loss: 10.8043554CurrentTrain: epoch 15, batch     0 | loss: 17.0286942CurrentTrain: epoch 15, batch     1 | loss: 21.3739235CurrentTrain: epoch 15, batch     2 | loss: 12.7035930CurrentTrain: epoch 15, batch     3 | loss: 13.4693381CurrentTrain: epoch 15, batch     4 | loss: 12.4032417CurrentTrain: epoch 15, batch     5 | loss: 12.4468828CurrentTrain: epoch 15, batch     6 | loss: 27.7531374CurrentTrain: epoch 15, batch     7 | loss: 15.4069302CurrentTrain: epoch 15, batch     8 | loss: 13.8646232CurrentTrain: epoch 15, batch     9 | loss: 16.0819948CurrentTrain: epoch 15, batch    10 | loss: 19.3909090CurrentTrain: epoch 15, batch    11 | loss: 17.0003418CurrentTrain: epoch 15, batch    12 | loss: 15.7300255CurrentTrain: epoch 15, batch    13 | loss: 13.3304819CurrentTrain: epoch 15, batch    14 | loss: 14.5659544CurrentTrain: epoch 15, batch    15 | loss: 20.6499567CurrentTrain: epoch 15, batch    16 | loss: 16.0589208CurrentTrain: epoch 15, batch    17 | loss: 14.7895468CurrentTrain: epoch 15, batch    18 | loss: 21.4617089CurrentTrain: epoch 15, batch    19 | loss: 14.9002844CurrentTrain: epoch 15, batch    20 | loss: 20.8475382CurrentTrain: epoch 15, batch    21 | loss: 10.3130274CurrentTrain: epoch 15, batch    22 | loss: 16.9241871CurrentTrain: epoch 15, batch    23 | loss: 10.5874300CurrentTrain: epoch 15, batch    24 | loss: 11.6538679CurrentTrain: epoch 15, batch    25 | loss: 15.2028432CurrentTrain: epoch 15, batch    26 | loss: 13.6569477CurrentTrain: epoch 15, batch    27 | loss: 9.9050035CurrentTrain: epoch 15, batch    28 | loss: 10.5938412CurrentTrain: epoch 15, batch    29 | loss: 15.3062167CurrentTrain: epoch 15, batch    30 | loss: 11.9671405CurrentTrain: epoch 15, batch    31 | loss: 14.7047615CurrentTrain: epoch 15, batch    32 | loss: 13.5338833CurrentTrain: epoch 15, batch    33 | loss: 12.1996971CurrentTrain: epoch 15, batch    34 | loss: 13.2964138CurrentTrain: epoch 15, batch    35 | loss: 11.5441370CurrentTrain: epoch 15, batch    36 | loss: 12.4905862CurrentTrain: epoch 15, batch    37 | loss: 12.2200384CurrentTrain: epoch 15, batch    38 | loss: 12.1302882CurrentTrain: epoch 15, batch    39 | loss: 14.2779092CurrentTrain: epoch 15, batch    40 | loss: 10.1021536CurrentTrain: epoch 15, batch    41 | loss: 15.6814563CurrentTrain: epoch 15, batch    42 | loss: 20.9059334CurrentTrain: epoch 15, batch    43 | loss: 8.7087862CurrentTrain: epoch 15, batch    44 | loss: 13.2802172CurrentTrain: epoch 15, batch    45 | loss: 17.4016784CurrentTrain: epoch 15, batch    46 | loss: 14.5567875CurrentTrain: epoch 15, batch    47 | loss: 11.3842308CurrentTrain: epoch 15, batch    48 | loss: 11.2919885CurrentTrain: epoch 15, batch    49 | loss: 12.7979103CurrentTrain: epoch 15, batch    50 | loss: 19.1452190CurrentTrain: epoch 15, batch    51 | loss: 10.3361172CurrentTrain: epoch 15, batch    52 | loss: 14.2087001CurrentTrain: epoch 15, batch    53 | loss: 29.6081073CurrentTrain: epoch 15, batch    54 | loss: 9.7439255CurrentTrain: epoch 15, batch    55 | loss: 20.0117976CurrentTrain: epoch 15, batch    56 | loss: 12.2553015CurrentTrain: epoch 15, batch    57 | loss: 13.6792415CurrentTrain: epoch 15, batch    58 | loss: 22.3462490CurrentTrain: epoch 15, batch    59 | loss: 14.5915132CurrentTrain: epoch 15, batch    60 | loss: 13.2209383CurrentTrain: epoch 15, batch    61 | loss: 14.5536149CurrentTrain: epoch  7, batch    62 | loss: 12.1674723CurrentTrain: epoch 15, batch     0 | loss: 15.1227265CurrentTrain: epoch 15, batch     1 | loss: 12.4286554CurrentTrain: epoch 15, batch     2 | loss: 24.6248081CurrentTrain: epoch 15, batch     3 | loss: 11.4074510CurrentTrain: epoch 15, batch     4 | loss: 25.2749544CurrentTrain: epoch 15, batch     5 | loss: 15.3942345CurrentTrain: epoch 15, batch     6 | loss: 15.1227021CurrentTrain: epoch 15, batch     7 | loss: 16.0566212CurrentTrain: epoch 15, batch     8 | loss: 11.2273305CurrentTrain: epoch 15, batch     9 | loss: 12.0192363CurrentTrain: epoch 15, batch    10 | loss: 12.6334964CurrentTrain: epoch 15, batch    11 | loss: 9.4407361CurrentTrain: epoch 15, batch    12 | loss: 9.5351714CurrentTrain: epoch 15, batch    13 | loss: 18.4977839CurrentTrain: epoch 15, batch    14 | loss: 19.0509029CurrentTrain: epoch 15, batch    15 | loss: 13.6473886CurrentTrain: epoch 15, batch    16 | loss: 13.6068231CurrentTrain: epoch 15, batch    17 | loss: 23.4622560CurrentTrain: epoch 15, batch    18 | loss: 9.5245906CurrentTrain: epoch 15, batch    19 | loss: 22.1178482CurrentTrain: epoch 15, batch    20 | loss: 10.9143988CurrentTrain: epoch 15, batch    21 | loss: 10.5752055CurrentTrain: epoch 15, batch    22 | loss: 18.6696506CurrentTrain: epoch 15, batch    23 | loss: 13.8691873CurrentTrain: epoch 15, batch    24 | loss: 22.0046243CurrentTrain: epoch 15, batch    25 | loss: 14.2767427CurrentTrain: epoch 15, batch    26 | loss: 18.0883202CurrentTrain: epoch 15, batch    27 | loss: 22.9433056CurrentTrain: epoch 15, batch    28 | loss: 11.0865406CurrentTrain: epoch 15, batch    29 | loss: 11.2495217CurrentTrain: epoch 15, batch    30 | loss: 13.9284060CurrentTrain: epoch 15, batch    31 | loss: 12.7948899CurrentTrain: epoch 15, batch    32 | loss: 9.8241843CurrentTrain: epoch 15, batch    33 | loss: 9.7833556CurrentTrain: epoch 15, batch    34 | loss: 11.9821816CurrentTrain: epoch 15, batch    35 | loss: 8.3663985CurrentTrain: epoch 15, batch    36 | loss: 8.6645300CurrentTrain: epoch 15, batch    37 | loss: 14.5920643CurrentTrain: epoch 15, batch    38 | loss: 10.4561421CurrentTrain: epoch 15, batch    39 | loss: 8.3311586CurrentTrain: epoch 15, batch    40 | loss: 16.2437347CurrentTrain: epoch 15, batch    41 | loss: 11.6159334CurrentTrain: epoch 15, batch    42 | loss: 19.5872295CurrentTrain: epoch 15, batch    43 | loss: 14.3758337CurrentTrain: epoch 15, batch    44 | loss: 14.8899548CurrentTrain: epoch 15, batch    45 | loss: 15.5650400CurrentTrain: epoch 15, batch    46 | loss: 10.7356575CurrentTrain: epoch 15, batch    47 | loss: 14.2443039CurrentTrain: epoch 15, batch    48 | loss: 16.9707269CurrentTrain: epoch 15, batch    49 | loss: 14.9190338CurrentTrain: epoch 15, batch    50 | loss: 10.5330257CurrentTrain: epoch 15, batch    51 | loss: 16.4922565CurrentTrain: epoch 15, batch    52 | loss: 9.1014941CurrentTrain: epoch 15, batch    53 | loss: 8.3117045CurrentTrain: epoch 15, batch    54 | loss: 19.3224674CurrentTrain: epoch 15, batch    55 | loss: 13.5943086CurrentTrain: epoch 15, batch    56 | loss: 11.0356579CurrentTrain: epoch 15, batch    57 | loss: 10.0425866CurrentTrain: epoch 15, batch    58 | loss: 12.2791859CurrentTrain: epoch 15, batch    59 | loss: 15.1501881CurrentTrain: epoch 15, batch    60 | loss: 12.0689439CurrentTrain: epoch 15, batch    61 | loss: 17.9839820CurrentTrain: epoch  7, batch    62 | loss: 27.3836149CurrentTrain: epoch 15, batch     0 | loss: 11.8667733CurrentTrain: epoch 15, batch     1 | loss: 13.5023169CurrentTrain: epoch 15, batch     2 | loss: 10.9706862CurrentTrain: epoch 15, batch     3 | loss: 14.6919830CurrentTrain: epoch 15, batch     4 | loss: 12.9896658CurrentTrain: epoch 15, batch     5 | loss: 18.2840274CurrentTrain: epoch 15, batch     6 | loss: 15.2675081CurrentTrain: epoch 15, batch     7 | loss: 16.5931877CurrentTrain: epoch 15, batch     8 | loss: 19.2011938CurrentTrain: epoch 15, batch     9 | loss: 11.6931493CurrentTrain: epoch 15, batch    10 | loss: 15.0047975CurrentTrain: epoch 15, batch    11 | loss: 25.9476160CurrentTrain: epoch 15, batch    12 | loss: 18.8886807CurrentTrain: epoch 15, batch    13 | loss: 7.7802438CurrentTrain: epoch 15, batch    14 | loss: 14.9674873CurrentTrain: epoch 15, batch    15 | loss: 15.6602713CurrentTrain: epoch 15, batch    16 | loss: 20.2323630CurrentTrain: epoch 15, batch    17 | loss: 18.1498602CurrentTrain: epoch 15, batch    18 | loss: 14.4218995CurrentTrain: epoch 15, batch    19 | loss: 9.0293248CurrentTrain: epoch 15, batch    20 | loss: 8.3516892CurrentTrain: epoch 15, batch    21 | loss: 12.5610930CurrentTrain: epoch 15, batch    22 | loss: 14.5249324CurrentTrain: epoch 15, batch    23 | loss: 18.7031801CurrentTrain: epoch 15, batch    24 | loss: 12.3465078CurrentTrain: epoch 15, batch    25 | loss: 12.6795383CurrentTrain: epoch 15, batch    26 | loss: 11.5591879CurrentTrain: epoch 15, batch    27 | loss: 14.3347980CurrentTrain: epoch 15, batch    28 | loss: 14.0435290CurrentTrain: epoch 15, batch    29 | loss: 17.6675273CurrentTrain: epoch 15, batch    30 | loss: 9.4647573CurrentTrain: epoch 15, batch    31 | loss: 8.4298935CurrentTrain: epoch 15, batch    32 | loss: 12.2175903CurrentTrain: epoch 15, batch    33 | loss: 12.2374379CurrentTrain: epoch 15, batch    34 | loss: 17.3649138CurrentTrain: epoch 15, batch    35 | loss: 14.9583535CurrentTrain: epoch 15, batch    36 | loss: 12.0651573CurrentTrain: epoch 15, batch    37 | loss: 23.0195732CurrentTrain: epoch 15, batch    38 | loss: 25.4668210CurrentTrain: epoch 15, batch    39 | loss: 10.3215953CurrentTrain: epoch 15, batch    40 | loss: 13.7626958CurrentTrain: epoch 15, batch    41 | loss: 11.0968938CurrentTrain: epoch 15, batch    42 | loss: 18.6355103CurrentTrain: epoch 15, batch    43 | loss: 14.6653807CurrentTrain: epoch 15, batch    44 | loss: 10.6798134CurrentTrain: epoch 15, batch    45 | loss: 13.2751926CurrentTrain: epoch 15, batch    46 | loss: 11.2852413CurrentTrain: epoch 15, batch    47 | loss: 14.1645979CurrentTrain: epoch 15, batch    48 | loss: 19.2829170CurrentTrain: epoch 15, batch    49 | loss: 15.8468423CurrentTrain: epoch 15, batch    50 | loss: 17.8303540CurrentTrain: epoch 15, batch    51 | loss: 23.9354964CurrentTrain: epoch 15, batch    52 | loss: 9.4988548CurrentTrain: epoch 15, batch    53 | loss: 10.7403555CurrentTrain: epoch 15, batch    54 | loss: 9.7730402CurrentTrain: epoch 15, batch    55 | loss: 17.2231219CurrentTrain: epoch 15, batch    56 | loss: 10.7750846CurrentTrain: epoch 15, batch    57 | loss: 9.6558447CurrentTrain: epoch 15, batch    58 | loss: 15.5866768CurrentTrain: epoch 15, batch    59 | loss: 11.9584630CurrentTrain: epoch 15, batch    60 | loss: 8.9248962CurrentTrain: epoch 15, batch    61 | loss: 13.3986745CurrentTrain: epoch  7, batch    62 | loss: 12.6253127CurrentTrain: epoch 15, batch     0 | loss: 10.4056245CurrentTrain: epoch 15, batch     1 | loss: 12.4175903CurrentTrain: epoch 15, batch     2 | loss: 7.0971739CurrentTrain: epoch 15, batch     3 | loss: 17.1400846CurrentTrain: epoch 15, batch     4 | loss: 10.9355051CurrentTrain: epoch 15, batch     5 | loss: 10.5559576CurrentTrain: epoch 15, batch     6 | loss: 10.1157781CurrentTrain: epoch 15, batch     7 | loss: 12.2645675CurrentTrain: epoch 15, batch     8 | loss: 12.0013499CurrentTrain: epoch 15, batch     9 | loss: 15.6625851CurrentTrain: epoch 15, batch    10 | loss: 16.9426063CurrentTrain: epoch 15, batch    11 | loss: 11.3001826CurrentTrain: epoch 15, batch    12 | loss: 20.5313791CurrentTrain: epoch 15, batch    13 | loss: 9.9577463CurrentTrain: epoch 15, batch    14 | loss: 9.7753446CurrentTrain: epoch 15, batch    15 | loss: 12.9227724CurrentTrain: epoch 15, batch    16 | loss: 13.7632058CurrentTrain: epoch 15, batch    17 | loss: 21.3811416CurrentTrain: epoch 15, batch    18 | loss: 19.2521018CurrentTrain: epoch 15, batch    19 | loss: 11.6276410CurrentTrain: epoch 15, batch    20 | loss: 13.3339976CurrentTrain: epoch 15, batch    21 | loss: 8.7105521CurrentTrain: epoch 15, batch    22 | loss: 11.8515842CurrentTrain: epoch 15, batch    23 | loss: 9.6711662CurrentTrain: epoch 15, batch    24 | loss: 15.8216924CurrentTrain: epoch 15, batch    25 | loss: 13.6530506CurrentTrain: epoch 15, batch    26 | loss: 21.0413385CurrentTrain: epoch 15, batch    27 | loss: 18.2701139CurrentTrain: epoch 15, batch    28 | loss: 9.5408966CurrentTrain: epoch 15, batch    29 | loss: 11.7764296CurrentTrain: epoch 15, batch    30 | loss: 10.5397350CurrentTrain: epoch 15, batch    31 | loss: 12.3363809CurrentTrain: epoch 15, batch    32 | loss: 7.9294670CurrentTrain: epoch 15, batch    33 | loss: 12.9386379CurrentTrain: epoch 15, batch    34 | loss: 10.3538804CurrentTrain: epoch 15, batch    35 | loss: 8.1176242CurrentTrain: epoch 15, batch    36 | loss: 11.5422849CurrentTrain: epoch 15, batch    37 | loss: 11.7770750CurrentTrain: epoch 15, batch    38 | loss: 16.4622949CurrentTrain: epoch 15, batch    39 | loss: 10.4109390CurrentTrain: epoch 15, batch    40 | loss: 11.6046136CurrentTrain: epoch 15, batch    41 | loss: 23.9574103CurrentTrain: epoch 15, batch    42 | loss: 24.7742937CurrentTrain: epoch 15, batch    43 | loss: 10.3199639CurrentTrain: epoch 15, batch    44 | loss: 10.8151250CurrentTrain: epoch 15, batch    45 | loss: 11.4635217CurrentTrain: epoch 15, batch    46 | loss: 9.9334915CurrentTrain: epoch 15, batch    47 | loss: 7.7120186CurrentTrain: epoch 15, batch    48 | loss: 15.0900974CurrentTrain: epoch 15, batch    49 | loss: 11.9764321CurrentTrain: epoch 15, batch    50 | loss: 13.1687635CurrentTrain: epoch 15, batch    51 | loss: 9.5335790CurrentTrain: epoch 15, batch    52 | loss: 9.2885316CurrentTrain: epoch 15, batch    53 | loss: 8.1637781CurrentTrain: epoch 15, batch    54 | loss: 10.9076744CurrentTrain: epoch 15, batch    55 | loss: 11.3966458CurrentTrain: epoch 15, batch    56 | loss: 12.4109797CurrentTrain: epoch 15, batch    57 | loss: 9.5007022CurrentTrain: epoch 15, batch    58 | loss: 11.2115656CurrentTrain: epoch 15, batch    59 | loss: 11.6931681CurrentTrain: epoch 15, batch    60 | loss: 8.8041285CurrentTrain: epoch 15, batch    61 | loss: 8.5039942CurrentTrain: epoch  7, batch    62 | loss: 6.6592786CurrentTrain: epoch 15, batch     0 | loss: 9.8136222CurrentTrain: epoch 15, batch     1 | loss: 8.9704680CurrentTrain: epoch 15, batch     2 | loss: 16.8499276CurrentTrain: epoch 15, batch     3 | loss: 31.1307996CurrentTrain: epoch 15, batch     4 | loss: 15.5880790CurrentTrain: epoch 15, batch     5 | loss: 12.0731156CurrentTrain: epoch 15, batch     6 | loss: 10.7223098CurrentTrain: epoch 15, batch     7 | loss: 10.6436889CurrentTrain: epoch 15, batch     8 | loss: 15.9223170CurrentTrain: epoch 15, batch     9 | loss: 11.0384091CurrentTrain: epoch 15, batch    10 | loss: 9.2564565CurrentTrain: epoch 15, batch    11 | loss: 23.9023601CurrentTrain: epoch 15, batch    12 | loss: 8.3757865CurrentTrain: epoch 15, batch    13 | loss: 12.1133195CurrentTrain: epoch 15, batch    14 | loss: 12.8818677CurrentTrain: epoch 15, batch    15 | loss: 15.1336549CurrentTrain: epoch 15, batch    16 | loss: 10.0235363CurrentTrain: epoch 15, batch    17 | loss: 11.6930836CurrentTrain: epoch 15, batch    18 | loss: 9.6652226CurrentTrain: epoch 15, batch    19 | loss: 11.6041733CurrentTrain: epoch 15, batch    20 | loss: 15.5677122CurrentTrain: epoch 15, batch    21 | loss: 11.7412125CurrentTrain: epoch 15, batch    22 | loss: 23.4372436CurrentTrain: epoch 15, batch    23 | loss: 13.9491554CurrentTrain: epoch 15, batch    24 | loss: 15.7679957CurrentTrain: epoch 15, batch    25 | loss: 14.6802455CurrentTrain: epoch 15, batch    26 | loss: 14.1263908CurrentTrain: epoch 15, batch    27 | loss: 10.7787304CurrentTrain: epoch 15, batch    28 | loss: 15.6998997CurrentTrain: epoch 15, batch    29 | loss: 12.4625427CurrentTrain: epoch 15, batch    30 | loss: 10.8046683CurrentTrain: epoch 15, batch    31 | loss: 9.0671151CurrentTrain: epoch 15, batch    32 | loss: 16.0051271CurrentTrain: epoch 15, batch    33 | loss: 12.4914630CurrentTrain: epoch 15, batch    34 | loss: 9.5033575CurrentTrain: epoch 15, batch    35 | loss: 9.2902827CurrentTrain: epoch 15, batch    36 | loss: 7.7200914CurrentTrain: epoch 15, batch    37 | loss: 11.5222278CurrentTrain: epoch 15, batch    38 | loss: 16.3916490CurrentTrain: epoch 15, batch    39 | loss: 15.0146607CurrentTrain: epoch 15, batch    40 | loss: 16.9557483CurrentTrain: epoch 15, batch    41 | loss: 9.8401815CurrentTrain: epoch 15, batch    42 | loss: 7.7144805CurrentTrain: epoch 15, batch    43 | loss: 13.4605939CurrentTrain: epoch 15, batch    44 | loss: 10.2683252CurrentTrain: epoch 15, batch    45 | loss: 21.3367612CurrentTrain: epoch 15, batch    46 | loss: 9.2788344CurrentTrain: epoch 15, batch    47 | loss: 8.6562484CurrentTrain: epoch 15, batch    48 | loss: 13.8384872CurrentTrain: epoch 15, batch    49 | loss: 13.7789983CurrentTrain: epoch 15, batch    50 | loss: 8.8381272CurrentTrain: epoch 15, batch    51 | loss: 14.4844984CurrentTrain: epoch 15, batch    52 | loss: 9.3369123CurrentTrain: epoch 15, batch    53 | loss: 7.5720949CurrentTrain: epoch 15, batch    54 | loss: 6.0285121CurrentTrain: epoch 15, batch    55 | loss: 13.3492761CurrentTrain: epoch 15, batch    56 | loss: 17.4580722CurrentTrain: epoch 15, batch    57 | loss: 16.0574534CurrentTrain: epoch 15, batch    58 | loss: 10.1347700CurrentTrain: epoch 15, batch    59 | loss: 10.2847738CurrentTrain: epoch 15, batch    60 | loss: 13.5998993CurrentTrain: epoch 15, batch    61 | loss: 11.2504383CurrentTrain: epoch  7, batch    62 | loss: 13.1268328CurrentTrain: epoch 15, batch     0 | loss: 47.6055261CurrentTrain: epoch 15, batch     1 | loss: 14.0664586CurrentTrain: epoch 15, batch     2 | loss: 25.0544823CurrentTrain: epoch 15, batch     3 | loss: 13.2299400CurrentTrain: epoch 15, batch     4 | loss: 8.7556178CurrentTrain: epoch 15, batch     5 | loss: 8.9526301CurrentTrain: epoch 15, batch     6 | loss: 14.0201212CurrentTrain: epoch 15, batch     7 | loss: 11.5664367CurrentTrain: epoch 15, batch     8 | loss: 10.2786558CurrentTrain: epoch 15, batch     9 | loss: 10.6158063CurrentTrain: epoch 15, batch    10 | loss: 10.0088437CurrentTrain: epoch 15, batch    11 | loss: 8.1939089CurrentTrain: epoch 15, batch    12 | loss: 14.1205579CurrentTrain: epoch 15, batch    13 | loss: 8.9708857CurrentTrain: epoch 15, batch    14 | loss: 12.7552388CurrentTrain: epoch 15, batch    15 | loss: 11.7984030CurrentTrain: epoch 15, batch    16 | loss: 8.0010801CurrentTrain: epoch 15, batch    17 | loss: 10.8241168CurrentTrain: epoch 15, batch    18 | loss: 15.2736196CurrentTrain: epoch 15, batch    19 | loss: 15.2146449CurrentTrain: epoch 15, batch    20 | loss: 9.6593879CurrentTrain: epoch 15, batch    21 | loss: 8.4290808CurrentTrain: epoch 15, batch    22 | loss: 7.5038776CurrentTrain: epoch 15, batch    23 | loss: 8.4950999CurrentTrain: epoch 15, batch    24 | loss: 9.1653209CurrentTrain: epoch 15, batch    25 | loss: 11.4137866CurrentTrain: epoch 15, batch    26 | loss: 17.7704517CurrentTrain: epoch 15, batch    27 | loss: 15.0369285CurrentTrain: epoch 15, batch    28 | loss: 11.5642837CurrentTrain: epoch 15, batch    29 | loss: 10.5599831CurrentTrain: epoch 15, batch    30 | loss: 11.6773610CurrentTrain: epoch 15, batch    31 | loss: 8.6710841CurrentTrain: epoch 15, batch    32 | loss: 35.0288115CurrentTrain: epoch 15, batch    33 | loss: 14.6427552CurrentTrain: epoch 15, batch    34 | loss: 15.1966630CurrentTrain: epoch 15, batch    35 | loss: 12.9091610CurrentTrain: epoch 15, batch    36 | loss: 10.7900600CurrentTrain: epoch 15, batch    37 | loss: 9.9703408CurrentTrain: epoch 15, batch    38 | loss: 9.5248414CurrentTrain: epoch 15, batch    39 | loss: 9.1763763CurrentTrain: epoch 15, batch    40 | loss: 7.9359823CurrentTrain: epoch 15, batch    41 | loss: 10.7310710CurrentTrain: epoch 15, batch    42 | loss: 17.8592510CurrentTrain: epoch 15, batch    43 | loss: 16.7775949CurrentTrain: epoch 15, batch    44 | loss: 22.6178110CurrentTrain: epoch 15, batch    45 | loss: 12.8260928CurrentTrain: epoch 15, batch    46 | loss: 18.4682794CurrentTrain: epoch 15, batch    47 | loss: 5.9905300CurrentTrain: epoch 15, batch    48 | loss: 8.6648467CurrentTrain: epoch 15, batch    49 | loss: 8.6158744CurrentTrain: epoch 15, batch    50 | loss: 7.2731569CurrentTrain: epoch 15, batch    51 | loss: 9.7450801CurrentTrain: epoch 15, batch    52 | loss: 10.5403815CurrentTrain: epoch 15, batch    53 | loss: 9.0071701CurrentTrain: epoch 15, batch    54 | loss: 7.1745235CurrentTrain: epoch 15, batch    55 | loss: 10.7988571CurrentTrain: epoch 15, batch    56 | loss: 14.8533869CurrentTrain: epoch 15, batch    57 | loss: 8.1115872CurrentTrain: epoch 15, batch    58 | loss: 9.3602063CurrentTrain: epoch 15, batch    59 | loss: 10.7359821CurrentTrain: epoch 15, batch    60 | loss: 10.2679161CurrentTrain: epoch 15, batch    61 | loss: 10.8257500CurrentTrain: epoch  7, batch    62 | loss: 9.8350513CurrentTrain: epoch 15, batch     0 | loss: 7.5929480CurrentTrain: epoch 15, batch     1 | loss: 16.1297725CurrentTrain: epoch 15, batch     2 | loss: 9.5163824CurrentTrain: epoch 15, batch     3 | loss: 10.0706434CurrentTrain: epoch 15, batch     4 | loss: 12.3980434CurrentTrain: epoch 15, batch     5 | loss: 8.7943498CurrentTrain: epoch 15, batch     6 | loss: 9.1556406CurrentTrain: epoch 15, batch     7 | loss: 8.7820521CurrentTrain: epoch 15, batch     8 | loss: 17.2068280CurrentTrain: epoch 15, batch     9 | loss: 6.4951086CurrentTrain: epoch 15, batch    10 | loss: 9.2395410CurrentTrain: epoch 15, batch    11 | loss: 8.2955584CurrentTrain: epoch 15, batch    12 | loss: 10.5479169CurrentTrain: epoch 15, batch    13 | loss: 12.7228233CurrentTrain: epoch 15, batch    14 | loss: 12.1713404CurrentTrain: epoch 15, batch    15 | loss: 11.3037371CurrentTrain: epoch 15, batch    16 | loss: 13.0350780CurrentTrain: epoch 15, batch    17 | loss: 9.2590643CurrentTrain: epoch 15, batch    18 | loss: 35.4483002CurrentTrain: epoch 15, batch    19 | loss: 6.1541389CurrentTrain: epoch 15, batch    20 | loss: 20.7055274CurrentTrain: epoch 15, batch    21 | loss: 8.6772168CurrentTrain: epoch 15, batch    22 | loss: 10.7319798CurrentTrain: epoch 15, batch    23 | loss: 11.9309747CurrentTrain: epoch 15, batch    24 | loss: 9.2399986CurrentTrain: epoch 15, batch    25 | loss: 16.8439480CurrentTrain: epoch 15, batch    26 | loss: 10.0692899CurrentTrain: epoch 15, batch    27 | loss: 16.1554220CurrentTrain: epoch 15, batch    28 | loss: 15.2442170CurrentTrain: epoch 15, batch    29 | loss: 8.5861194CurrentTrain: epoch 15, batch    30 | loss: 11.8966879CurrentTrain: epoch 15, batch    31 | loss: 16.1038838CurrentTrain: epoch 15, batch    32 | loss: 15.7809720CurrentTrain: epoch 15, batch    33 | loss: 12.4965229CurrentTrain: epoch 15, batch    34 | loss: 12.7951021CurrentTrain: epoch 15, batch    35 | loss: 6.0199504CurrentTrain: epoch 15, batch    36 | loss: 8.0228520CurrentTrain: epoch 15, batch    37 | loss: 13.6270091CurrentTrain: epoch 15, batch    38 | loss: 7.7304164CurrentTrain: epoch 15, batch    39 | loss: 19.3779625CurrentTrain: epoch 15, batch    40 | loss: 5.9755028CurrentTrain: epoch 15, batch    41 | loss: 10.5737590CurrentTrain: epoch 15, batch    42 | loss: 16.5173685CurrentTrain: epoch 15, batch    43 | loss: 9.0173334CurrentTrain: epoch 15, batch    44 | loss: 11.9822525CurrentTrain: epoch 15, batch    45 | loss: 10.7001032CurrentTrain: epoch 15, batch    46 | loss: 9.6220332CurrentTrain: epoch 15, batch    47 | loss: 12.6152532CurrentTrain: epoch 15, batch    48 | loss: 9.8332398CurrentTrain: epoch 15, batch    49 | loss: 9.6048387CurrentTrain: epoch 15, batch    50 | loss: 8.6887545CurrentTrain: epoch 15, batch    51 | loss: 6.2187792CurrentTrain: epoch 15, batch    52 | loss: 8.9566519CurrentTrain: epoch 15, batch    53 | loss: 7.5610479CurrentTrain: epoch 15, batch    54 | loss: 7.7164717CurrentTrain: epoch 15, batch    55 | loss: 16.4297363CurrentTrain: epoch 15, batch    56 | loss: 7.0712789CurrentTrain: epoch 15, batch    57 | loss: 9.0604057CurrentTrain: epoch 15, batch    58 | loss: 11.3254344CurrentTrain: epoch 15, batch    59 | loss: 10.9893466CurrentTrain: epoch 15, batch    60 | loss: 15.5069810CurrentTrain: epoch 15, batch    61 | loss: 9.7921103CurrentTrain: epoch  7, batch    62 | loss: 7.7000672CurrentTrain: epoch 15, batch     0 | loss: 21.6258484CurrentTrain: epoch 15, batch     1 | loss: 12.5796137CurrentTrain: epoch 15, batch     2 | loss: 11.6820282CurrentTrain: epoch 15, batch     3 | loss: 20.3965901CurrentTrain: epoch 15, batch     4 | loss: 8.8065012CurrentTrain: epoch 15, batch     5 | loss: 23.2472665CurrentTrain: epoch 15, batch     6 | loss: 13.9525412CurrentTrain: epoch 15, batch     7 | loss: 11.6119119CurrentTrain: epoch 15, batch     8 | loss: 8.4526563CurrentTrain: epoch 15, batch     9 | loss: 10.1122181CurrentTrain: epoch 15, batch    10 | loss: 7.4253960CurrentTrain: epoch 15, batch    11 | loss: 10.8233233CurrentTrain: epoch 15, batch    12 | loss: 10.4749373CurrentTrain: epoch 15, batch    13 | loss: 13.8575720CurrentTrain: epoch 15, batch    14 | loss: 22.6814072CurrentTrain: epoch 15, batch    15 | loss: 8.4848506CurrentTrain: epoch 15, batch    16 | loss: 10.3234922CurrentTrain: epoch 15, batch    17 | loss: 9.2467852CurrentTrain: epoch 15, batch    18 | loss: 20.0836330CurrentTrain: epoch 15, batch    19 | loss: 8.6138566CurrentTrain: epoch 15, batch    20 | loss: 8.3432084CurrentTrain: epoch 15, batch    21 | loss: 6.0443232CurrentTrain: epoch 15, batch    22 | loss: 12.0164122CurrentTrain: epoch 15, batch    23 | loss: 12.1120419CurrentTrain: epoch 15, batch    24 | loss: 11.7229215CurrentTrain: epoch 15, batch    25 | loss: 11.5090589CurrentTrain: epoch 15, batch    26 | loss: 12.6816995CurrentTrain: epoch 15, batch    27 | loss: 5.7647132CurrentTrain: epoch 15, batch    28 | loss: 7.6581805CurrentTrain: epoch 15, batch    29 | loss: 13.5161490CurrentTrain: epoch 15, batch    30 | loss: 6.3547159CurrentTrain: epoch 15, batch    31 | loss: 9.4134685CurrentTrain: epoch 15, batch    32 | loss: 10.2134677CurrentTrain: epoch 15, batch    33 | loss: 9.0647096CurrentTrain: epoch 15, batch    34 | loss: 14.0772900CurrentTrain: epoch 15, batch    35 | loss: 22.9251544CurrentTrain: epoch 15, batch    36 | loss: 22.2031147CurrentTrain: epoch 15, batch    37 | loss: 10.3885223CurrentTrain: epoch 15, batch    38 | loss: 7.4526194CurrentTrain: epoch 15, batch    39 | loss: 10.2955316CurrentTrain: epoch 15, batch    40 | loss: 7.5322260CurrentTrain: epoch 15, batch    41 | loss: 13.6880326CurrentTrain: epoch 15, batch    42 | loss: 9.6457089CurrentTrain: epoch 15, batch    43 | loss: 11.0682921CurrentTrain: epoch 15, batch    44 | loss: 10.2871850CurrentTrain: epoch 15, batch    45 | loss: 7.5988976CurrentTrain: epoch 15, batch    46 | loss: 9.8073523CurrentTrain: epoch 15, batch    47 | loss: 7.5595828CurrentTrain: epoch 15, batch    48 | loss: 7.3403677CurrentTrain: epoch 15, batch    49 | loss: 10.7941043CurrentTrain: epoch 15, batch    50 | loss: 7.3418295CurrentTrain: epoch 15, batch    51 | loss: 10.9378911CurrentTrain: epoch 15, batch    52 | loss: 13.4415160CurrentTrain: epoch 15, batch    53 | loss: 17.7162225CurrentTrain: epoch 15, batch    54 | loss: 14.5163597CurrentTrain: epoch 15, batch    55 | loss: 13.2024480CurrentTrain: epoch 15, batch    56 | loss: 8.4057237CurrentTrain: epoch 15, batch    57 | loss: 13.2512825CurrentTrain: epoch 15, batch    58 | loss: 8.0328214CurrentTrain: epoch 15, batch    59 | loss: 9.6667530CurrentTrain: epoch 15, batch    60 | loss: 10.3715225CurrentTrain: epoch 15, batch    61 | loss: 15.2499341CurrentTrain: epoch  7, batch    62 | loss: 7.6094417CurrentTrain: epoch 15, batch     0 | loss: 6.2398412CurrentTrain: epoch 15, batch     1 | loss: 8.2384645CurrentTrain: epoch 15, batch     2 | loss: 12.9842409CurrentTrain: epoch 15, batch     3 | loss: 8.7017215CurrentTrain: epoch 15, batch     4 | loss: 9.6908631CurrentTrain: epoch 15, batch     5 | loss: 10.3387940CurrentTrain: epoch 15, batch     6 | loss: 10.3882073CurrentTrain: epoch 15, batch     7 | loss: 8.4778065CurrentTrain: epoch 15, batch     8 | loss: 15.0348395CurrentTrain: epoch 15, batch     9 | loss: 10.4722368CurrentTrain: epoch 15, batch    10 | loss: 14.7044928CurrentTrain: epoch 15, batch    11 | loss: 8.6029851CurrentTrain: epoch 15, batch    12 | loss: 10.7954616CurrentTrain: epoch 15, batch    13 | loss: 12.2614543CurrentTrain: epoch 15, batch    14 | loss: 8.7025328CurrentTrain: epoch 15, batch    15 | loss: 8.3100115CurrentTrain: epoch 15, batch    16 | loss: 13.4375404CurrentTrain: epoch 15, batch    17 | loss: 9.4496790CurrentTrain: epoch 15, batch    18 | loss: 14.4605842CurrentTrain: epoch 15, batch    19 | loss: 12.5818358CurrentTrain: epoch 15, batch    20 | loss: 11.9244493CurrentTrain: epoch 15, batch    21 | loss: 9.5354034CurrentTrain: epoch 15, batch    22 | loss: 7.8441078CurrentTrain: epoch 15, batch    23 | loss: 13.6966992CurrentTrain: epoch 15, batch    24 | loss: 8.3988078CurrentTrain: epoch 15, batch    25 | loss: 9.2432725CurrentTrain: epoch 15, batch    26 | loss: 9.9207702CurrentTrain: epoch 15, batch    27 | loss: 8.2350424CurrentTrain: epoch 15, batch    28 | loss: 9.4130467CurrentTrain: epoch 15, batch    29 | loss: 10.5219463CurrentTrain: epoch 15, batch    30 | loss: 12.1550688CurrentTrain: epoch 15, batch    31 | loss: 12.1343323CurrentTrain: epoch 15, batch    32 | loss: 15.6580393CurrentTrain: epoch 15, batch    33 | loss: 10.4069708CurrentTrain: epoch 15, batch    34 | loss: 14.9102835CurrentTrain: epoch 15, batch    35 | loss: 10.3213624CurrentTrain: epoch 15, batch    36 | loss: 10.8308318CurrentTrain: epoch 15, batch    37 | loss: 10.4649913CurrentTrain: epoch 15, batch    38 | loss: 7.8263450CurrentTrain: epoch 15, batch    39 | loss: 16.6621189CurrentTrain: epoch 15, batch    40 | loss: 9.3179529CurrentTrain: epoch 15, batch    41 | loss: 8.6412223CurrentTrain: epoch 15, batch    42 | loss: 16.4267417CurrentTrain: epoch 15, batch    43 | loss: 9.7885001CurrentTrain: epoch 15, batch    44 | loss: 8.0146042CurrentTrain: epoch 15, batch    45 | loss: 7.8336454CurrentTrain: epoch 15, batch    46 | loss: 10.8568806CurrentTrain: epoch 15, batch    47 | loss: 12.6674539CurrentTrain: epoch 15, batch    48 | loss: 10.1815824CurrentTrain: epoch 15, batch    49 | loss: 12.1026209CurrentTrain: epoch 15, batch    50 | loss: 11.3706928CurrentTrain: epoch 15, batch    51 | loss: 13.5517263CurrentTrain: epoch 15, batch    52 | loss: 21.9599904CurrentTrain: epoch 15, batch    53 | loss: 15.4622945CurrentTrain: epoch 15, batch    54 | loss: 12.7251948CurrentTrain: epoch 15, batch    55 | loss: 9.3760903CurrentTrain: epoch 15, batch    56 | loss: 9.9619214CurrentTrain: epoch 15, batch    57 | loss: 10.4189348CurrentTrain: epoch 15, batch    58 | loss: 7.6630209CurrentTrain: epoch 15, batch    59 | loss: 9.5859293CurrentTrain: epoch 15, batch    60 | loss: 10.3575359CurrentTrain: epoch 15, batch    61 | loss: 8.4842196CurrentTrain: epoch  7, batch    62 | loss: 7.2279278
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: the large workshops of peter paul rubens and jacob jordaens , and the influence of anthony van dyck , made antwerp the center of the flemish baroque .
Head Entity: jacob jordaens
Tail Entity: flemish baroque
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: The emergence of surrealism in the early 20th century was significantly influenced by the works of Salvador Dalí and André Breton.  
Head Entity: Salvador Dalí  
Tail Entity: surrealism  

Relation: movement  
Context: The impact of the Harlem Renaissance on American literature and culture was profound, with figures like Langston Hughes leading the charge.  
Head Entity: Langston Hughes  
Tail Entity: Harlem Renaissance  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: lyons hill is a restored village , and former parish with church , now part of the community of ardclough in north county kildare .
Head Entity: ardclough
Tail Entity: county kildare
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: The city of San Francisco is known for its iconic Golden Gate Bridge and is part of the larger San Francisco County in California.  
Head Entity: San Francisco  
Tail Entity: San Francisco County  

Relation: located in the administrative territorial entity  
Context: The village of Haverfordwest serves as the county town of Pembrokeshire, situated in the picturesque region of Wales.  
Head Entity: Haverfordwest  
Tail Entity: Pembrokeshire  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the nation as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: he also went on a road trip from san antonio to oakland for the magnum project “ postcards from america ” with jim goldberg , alec soth , mikhael subotzky and susan meiselas .
Head Entity: jim goldberg
Tail Entity: magnum
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: After years of dedication and hard work, she was finally recognized as a key member of the prestigious art collective known as the "Creative Minds."  
Head Entity: she  
Tail Entity: Creative Minds  

Relation: member of  
Context: The renowned guitarist was proud to be a member of the legendary rock band that had shaped the music industry for decades.  
Head Entity: guitarist  
Tail Entity: rock band  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: the bosideng bridge ( ) is a highway bridge over the yangtze river in hejiang county , sichuan , china .
Head Entity: bosideng bridge
Tail Entity: yangtze river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: the golden gate bridge ( ) spans the golden gate strait, the entrance to san francisco bay from the pacific ocean.  
Head Entity: golden gate bridge  
Tail Entity: golden gate strait  

Relation: crosses  
Context: the channel tunnel ( ) runs beneath the english channel, connecting the united kingdom with mainland europe.  
Head Entity: channel tunnel  
Tail Entity: english channel  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: it is on the panj river just downstream from the mouth of the bartang river at the point where the panj briefly turns west before resuming its northerly course .
Head Entity: panj river
Tail Entity: bartang river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The Missouri River, which is a major tributary of the Mississippi River, flows through several states before joining the larger watercourse.  
Head Entity: Missouri River  
Tail Entity: Mississippi River  

Relation: tributary  
Context: The Rio Grande serves as a natural border between the United States and Mexico, and it has several tributaries, including the Pecos River.  
Head Entity: Rio Grande  
Tail Entity: Pecos River  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: in september 2001 , mimica became minister of european integration under prime minister ivica račan , position he held until december 2003 .
Head Entity: ivica račan
Tail Entity: prime minister
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: in 2015, john smith was appointed as the chief executive officer of tech innovations, a role he maintained until 2020.  
Head Entity: john smith  
Tail Entity: chief executive officer  

Relation: position held  
Context: after serving as the mayor of springfield for two terms, jane doe announced her retirement in 2019, a position she had held since 2011.  
Head Entity: jane doe  
Tail Entity: mayor  
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: a headcrab is a fictional alien parasitoid found in the " half - life " video game series created by valve software .
Head Entity: half - life
Tail Entity: valve software
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: The popular game "The Witcher 3: Wild Hunt" was developed by CD Projekt Red, a well-known Polish video game developer.  
Head Entity: The Witcher 3: Wild Hunt  
Tail Entity: CD Projekt Red  

Relation: developer  
Context: The innovative mobile game "Angry Birds" was created by Rovio Entertainment, a Finnish company that specializes in game development.  
Head Entity: Angry Birds  
Tail Entity: Rovio Entertainment  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: holy molar are a san diego based band formed in 2001 .
Head Entity: holy molar
Tail Entity: san diego
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the tech startup was established in silicon valley in 2015.  
Head Entity: the tech startup  
Tail Entity: silicon valley  

Relation: location of formation  
Context: the famous rock band was created in new york city during the late 1980s.  
Head Entity: the famous rock band  
Tail Entity: new york city  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: she has appeared in some american television shows , beginning with her regular role as snow white 's stepmother , evil queen lillian " lily " white in the series " the charmings " .
Head Entity: the charmings
Tail Entity: american
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous dish sushi is traditionally made with vinegared rice and often includes seafood, vegetables, and occasionally tropical fruits, originating from Japan.  
Head Entity: sushi  
Tail Entity: Japan  

Relation: country of origin  
Context: The iconic painting "Mona Lisa," created by Leonardo da Vinci, is housed in the Louvre Museum in Paris, France, and is a masterpiece of the Italian Renaissance.  
Head Entity: Mona Lisa  
Tail Entity: Italy  
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.23%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 90.81%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.97%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.45%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.26%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.39%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.79%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 92.82%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.89%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.93%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.93%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.10%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.26%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 94.41%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.55%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.82%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.94%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.06%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.17%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.28%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.38%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.35%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.41%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.47%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.31%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.28%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.37%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 95.23%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.20%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.15%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.23%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.21%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.29%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.26%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.54%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.23%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 90.81%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.97%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.45%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.26%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.39%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.79%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 92.82%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.89%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.93%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.93%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.10%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.26%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 94.41%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.55%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.82%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.94%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.06%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.17%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.28%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.38%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.35%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.41%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.47%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.31%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.28%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.37%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 95.23%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.20%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.15%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.23%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.21%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.29%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.26%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.54%   
cur_acc:  ['0.9454']
his_acc:  ['0.9454']
CurrentTrain: epoch 15, batch     0 | loss: 15.5058369CurrentTrain: epoch 15, batch     1 | loss: 22.1229190CurrentTrain: epoch 15, batch     2 | loss: 17.4160401CurrentTrain: epoch  1, batch     3 | loss: 12.5063018CurrentTrain: epoch 15, batch     0 | loss: 15.6745593CurrentTrain: epoch 15, batch     1 | loss: 14.4292058CurrentTrain: epoch 15, batch     2 | loss: 16.3432313CurrentTrain: epoch  1, batch     3 | loss: 12.3680924CurrentTrain: epoch 15, batch     0 | loss: 12.6355272CurrentTrain: epoch 15, batch     1 | loss: 11.5894233CurrentTrain: epoch 15, batch     2 | loss: 15.2747157CurrentTrain: epoch  1, batch     3 | loss: 7.2112160CurrentTrain: epoch 15, batch     0 | loss: 13.5135736CurrentTrain: epoch 15, batch     1 | loss: 19.9111021CurrentTrain: epoch 15, batch     2 | loss: 12.5567691CurrentTrain: epoch  1, batch     3 | loss: 11.0353865CurrentTrain: epoch 15, batch     0 | loss: 10.7070095CurrentTrain: epoch 15, batch     1 | loss: 12.9596147CurrentTrain: epoch 15, batch     2 | loss: 7.8958398CurrentTrain: epoch  1, batch     3 | loss: 11.8230311CurrentTrain: epoch 15, batch     0 | loss: 16.7015529CurrentTrain: epoch 15, batch     1 | loss: 13.4671582CurrentTrain: epoch 15, batch     2 | loss: 19.1072313CurrentTrain: epoch  1, batch     3 | loss: 6.5259707CurrentTrain: epoch 15, batch     0 | loss: 12.7297577CurrentTrain: epoch 15, batch     1 | loss: 15.7381388CurrentTrain: epoch 15, batch     2 | loss: 12.8097976CurrentTrain: epoch  1, batch     3 | loss: 15.8593650CurrentTrain: epoch 15, batch     0 | loss: 10.3453000CurrentTrain: epoch 15, batch     1 | loss: 9.8264651CurrentTrain: epoch 15, batch     2 | loss: 8.5567217CurrentTrain: epoch  1, batch     3 | loss: 6.5309727CurrentTrain: epoch 15, batch     0 | loss: 7.1793901CurrentTrain: epoch 15, batch     1 | loss: 14.7705701CurrentTrain: epoch 15, batch     2 | loss: 8.3656069CurrentTrain: epoch  1, batch     3 | loss: 8.7189925CurrentTrain: epoch 15, batch     0 | loss: 11.0825922CurrentTrain: epoch 15, batch     1 | loss: 10.0986445CurrentTrain: epoch 15, batch     2 | loss: 15.0160569CurrentTrain: epoch  1, batch     3 | loss: 7.4310037
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: auerbach is prominently featured in the documentary film , " the first basket " , about jewish basketball history .
Head Entity: the first basket
Tail Entity: basketball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: the book "sapiens" explores the history of humankind from the stone age to the modern era.  
Head Entity: sapiens  
Tail Entity: humankind  

Relation: main subject  
Context: the exhibition "van gogh: the immersive experience" showcases the life and works of the famous painter.  
Head Entity: van gogh: the immersive experience  
Tail Entity: painter  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: he represented romania at euro 2000 , his first major tournament where they beat england 3–2 , and drew 1–1 against germany .
Head Entity: euro 2000
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: The Brazilian national team showcased their skills at the 2014 FIFA World Cup, where they faced off against Germany in the semi-finals.  
Head Entity: 2014 FIFA World Cup  
Tail Entity: Germany  

Relation: participating team  
Context: During the 2021 Tokyo Olympics, the United States women's soccer team competed fiercely, ultimately playing against Canada in the semi-finals.  
Head Entity: 2021 Tokyo Olympics  
Tail Entity: Canada  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: among the competing artists was previous eurovision song contest entrant robin juhkental who represented estonia in 2010 as part of the group malcolm lincoln .
Head Entity: malcolm lincoln
Tail Entity: robin juhkental
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The famous painting "The Last Supper" features several apostles, including Peter, who is depicted prominently on the left side of the artwork.  
Head Entity: The Last Supper  
Tail Entity: Peter  

Relation: has part  
Context: The smartphone model Galaxy S21 comes with a variety of features, including a high-resolution camera that is part of its appeal to photography enthusiasts.  
Head Entity: Galaxy S21  
Tail Entity: high-resolution camera  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: for her performance in the musical - drama " crazy heart " ( 2009 ) , she was nominated for the academy award for best supporting actress .
Head Entity: crazy heart
Tail Entity: academy award for best supporting actress
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film "The Shape of Water" received critical acclaim and was nominated for several prestigious awards, including the Academy Award for Best Picture.  
Head Entity: The Shape of Water  
Tail Entity: Academy Award for Best Picture  

Relation: nominated for  
Context: In 2021, the popular series "The Crown" was nominated for multiple Emmy Awards, showcasing its excellence in television production.  
Head Entity: The Crown  
Tail Entity: Emmy Awards  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: in 2007 , the county municipality created the transit authority called " skyss " that would market public transport while is would be operated by private companies based on public service obligation contracts .
Head Entity: skyss
Tail Entity: transit authority
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone known as the iPhone has revolutionized the way we communicate and interact with technology, becoming a benchmark for mobile devices worldwide.  
Head Entity: iPhone  
Tail Entity: smartphone  

Relation: instance of  
Context: The Great Wall of China is not only a remarkable feat of engineering but also a symbol of the historical strength and resilience of the Chinese civilization.  
Head Entity: Great Wall of China  
Tail Entity: monument  
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: brand peak is a sharp snow - covered peak located east - southeast of the eternity range and northwest of mount duemler , in palmer land .
Head Entity: mount duemler
Tail Entity: eternity range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the sierra nevada is a major mountain range in the western united states, known for its stunning landscapes and diverse ecosystems, including the famous lake tahoe, which is located near the eastern edge of the range.  
Head Entity: lake tahoe  
Tail Entity: sierra nevada  

Relation: mountain range  
Context: the appalachian mountains stretch from the southern united states to eastern canada, with the blue ridge mountains being a prominent subrange that attracts hikers and nature enthusiasts.  
Head Entity: blue ridge mountains  
Tail Entity: appalachian mountains  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: shaadiram gharjode ( sadashiv amrapurkar ) is once humiliated by hoshiyar chand ( kader khan ) when shaadiram goes to inform him about a family wanting to marry their son to hoshiyar 's daughter .
Head Entity: hoshiyar
Tail Entity: kader khan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: the film "the great escape" features a compelling narrative crafted by the talented screenwriter james clavell, who masterfully brings the story of allied prisoners of war to life.  
Head Entity: james clavell  
Tail Entity: the great escape  

Relation: screenwriter  
Context: in the critically acclaimed movie "pulp fiction," the unique storytelling style is attributed to the brilliant screenwriter quentin tarantino, whose work has left a lasting impact on modern cinema.  
Head Entity: quentin tarantino  
Tail Entity: pulp fiction  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the publisher found success in 1993 with tarō gomi 's " everyone poops " , a japanese language import that sold over one million copies and is now their best - known title .
Head Entity: everyone poops
Tail Entity: japanese language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The animated series "Avatar: The Last Airbender" features a rich narrative that draws heavily from various Asian cultures, with the dialogue primarily in English but incorporating elements of Mandarin and other languages.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  

Relation: language of work or name  
Context: The famous novel "One Hundred Years of Solitude" by Gabriel García Márquez is originally written in Spanish and has been translated into numerous languages, captivating readers worldwide.  
Head Entity: One Hundred Years of Solitude  
Tail Entity: Spanish  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: the aircraft was a boeing 707 - 321c that first entered service with pan american world airways ( pan am ) in 1963 .
Head Entity: boeing 707 - 321c
Tail Entity: pan american world airways
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: the new electric bus fleet is managed by the city transportation authority, which ensures efficient public transit services.  
Head Entity: electric bus fleet  
Tail Entity: city transportation authority  

Relation: operator  
Context: the state-of-the-art research facility is operated by the national science foundation, focusing on advanced materials and nanotechnology.  
Head Entity: research facility  
Tail Entity: national science foundation  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: the most important lama of this series was the third changkya , rolpai dorje , who was preceptor to the qianlong emperor of china , and chief representative of tibetan buddhism at the qing court .
Head Entity: rolpai dorje
Tail Entity: tibetan buddhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the cathedral of notre-dame is a famous example of french gothic architecture and serves as a major center for the roman catholic faith in paris.  
Head Entity: notre-dame  
Tail Entity: roman catholic faith  

Relation: religion  
Context: the dalai lama is a prominent figure in the gelug school of tibetan buddhism and is revered by millions around the world.  
Head Entity: dalai lama  
Tail Entity: tibetan buddhism  
MemoryTrain:  epoch 15, batch     0 | loss: 9.8100528MemoryTrain:  epoch 15, batch     1 | loss: 4.3921174MemoryTrain:  epoch 15, batch     2 | loss: 7.5476634MemoryTrain:  epoch 11, batch     3 | loss: 7.5278662MemoryTrain:  epoch 15, batch     0 | loss: 6.7211476MemoryTrain:  epoch 15, batch     1 | loss: 9.5990270MemoryTrain:  epoch 15, batch     2 | loss: 10.3354554MemoryTrain:  epoch 11, batch     3 | loss: 7.2585886MemoryTrain:  epoch 15, batch     0 | loss: 3.5996220MemoryTrain:  epoch 15, batch     1 | loss: 7.1137104MemoryTrain:  epoch 15, batch     2 | loss: 3.3856260MemoryTrain:  epoch 11, batch     3 | loss: 4.9434714MemoryTrain:  epoch 15, batch     0 | loss: 8.4273871MemoryTrain:  epoch 15, batch     1 | loss: 4.4454789MemoryTrain:  epoch 15, batch     2 | loss: 7.2146556MemoryTrain:  epoch 11, batch     3 | loss: 7.9523675MemoryTrain:  epoch 15, batch     0 | loss: 4.0150828MemoryTrain:  epoch 15, batch     1 | loss: 3.6380834MemoryTrain:  epoch 15, batch     2 | loss: 3.3127834MemoryTrain:  epoch 11, batch     3 | loss: 5.9121067MemoryTrain:  epoch 15, batch     0 | loss: 5.4721741MemoryTrain:  epoch 15, batch     1 | loss: 3.7184439MemoryTrain:  epoch 15, batch     2 | loss: 6.5919565MemoryTrain:  epoch 11, batch     3 | loss: 2.7899040MemoryTrain:  epoch 15, batch     0 | loss: 5.5032895MemoryTrain:  epoch 15, batch     1 | loss: 2.7923547MemoryTrain:  epoch 15, batch     2 | loss: 5.1361383MemoryTrain:  epoch 11, batch     3 | loss: 5.6289682MemoryTrain:  epoch 15, batch     0 | loss: 4.1902799MemoryTrain:  epoch 15, batch     1 | loss: 3.8848111MemoryTrain:  epoch 15, batch     2 | loss: 6.9811876MemoryTrain:  epoch 11, batch     3 | loss: 3.0774595MemoryTrain:  epoch 15, batch     0 | loss: 3.6469778MemoryTrain:  epoch 15, batch     1 | loss: 5.1561007MemoryTrain:  epoch 15, batch     2 | loss: 3.4767214MemoryTrain:  epoch 11, batch     3 | loss: 5.0802251MemoryTrain:  epoch 15, batch     0 | loss: 7.1127126MemoryTrain:  epoch 15, batch     1 | loss: 4.3509662MemoryTrain:  epoch 15, batch     2 | loss: 5.1361252MemoryTrain:  epoch 11, batch     3 | loss: 2.2892796
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.36%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 86.54%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 78.75%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 75.00%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 73.90%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 72.22%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 70.39%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 73.21%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 74.15%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 75.27%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 76.30%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 77.00%   [EVAL] batch:   25 | acc: 68.75%,  total acc: 76.68%   [EVAL] batch:   26 | acc: 37.50%,  total acc: 75.23%   [EVAL] batch:   27 | acc: 43.75%,  total acc: 74.11%   [EVAL] batch:   28 | acc: 56.25%,  total acc: 73.49%   [EVAL] batch:   29 | acc: 43.75%,  total acc: 72.50%   [EVAL] batch:   30 | acc: 31.25%,  total acc: 71.17%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 71.68%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 72.35%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 73.16%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 73.93%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 74.65%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 75.34%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 75.99%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 76.60%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 77.03%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 77.59%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 78.63%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 79.12%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 78.89%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 78.94%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 78.72%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 78.26%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 78.19%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 78.38%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 78.31%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 78.37%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 78.30%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 78.36%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 78.64%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 78.79%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 78.95%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 79.09%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 79.34%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 79.27%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 79.41%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 79.33%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 78.67%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 82.39%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 82.29%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 82.69%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 83.48%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 84.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 86.03%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 87.17%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 87.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 87.80%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 87.78%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 88.04%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 88.02%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 88.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 88.46%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 89.22%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 89.92%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 90.23%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 90.53%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 90.81%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 90.89%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 91.39%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 91.61%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.83%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.03%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.23%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.41%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 92.59%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 92.92%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.07%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 93.09%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 93.10%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.24%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 93.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.26%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 93.39%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.52%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 93.52%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 93.20%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 92.78%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 92.69%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 92.60%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 92.42%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 92.44%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 92.46%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 92.19%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 91.73%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 91.57%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 91.42%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 91.36%   [EVAL] batch:   68 | acc: 87.50%,  total acc: 91.30%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 91.43%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 91.46%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 91.49%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 91.64%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 91.75%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 90.95%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 90.26%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 89.34%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 88.69%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 88.28%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 87.50%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 87.42%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 87.58%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 87.79%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 87.94%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 88.00%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 88.07%   [EVAL] batch:   88 | acc: 37.50%,  total acc: 87.50%   [EVAL] batch:   89 | acc: 43.75%,  total acc: 87.01%   [EVAL] batch:   90 | acc: 50.00%,  total acc: 86.61%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 86.35%   [EVAL] batch:   92 | acc: 25.00%,  total acc: 85.69%   [EVAL] batch:   93 | acc: 56.25%,  total acc: 85.37%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 85.53%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 85.61%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 85.76%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 85.91%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 86.05%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 86.19%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 86.32%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 86.53%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 86.66%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 86.79%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 86.91%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 86.92%   [EVAL] batch:  107 | acc: 68.75%,  total acc: 86.75%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 86.64%   [EVAL] batch:  109 | acc: 68.75%,  total acc: 86.48%   [EVAL] batch:  110 | acc: 75.00%,  total acc: 86.37%   [EVAL] batch:  111 | acc: 68.75%,  total acc: 86.22%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 86.23%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 86.07%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 86.09%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 85.99%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 86.00%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 86.02%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 85.98%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 86.04%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 86.07%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 85.98%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 85.90%   
cur_acc:  ['0.9454', '0.7867']
his_acc:  ['0.9454', '0.8590']
CurrentTrain: epoch 15, batch     0 | loss: 13.6673335CurrentTrain: epoch 15, batch     1 | loss: 12.9360689CurrentTrain: epoch 15, batch     2 | loss: 11.6745536CurrentTrain: epoch  1, batch     3 | loss: 12.4869818CurrentTrain: epoch 15, batch     0 | loss: 9.8854442CurrentTrain: epoch 15, batch     1 | loss: 13.2313212CurrentTrain: epoch 15, batch     2 | loss: 12.1518107CurrentTrain: epoch  1, batch     3 | loss: 8.0680583CurrentTrain: epoch 15, batch     0 | loss: 16.3858757CurrentTrain: epoch 15, batch     1 | loss: 9.4907554CurrentTrain: epoch 15, batch     2 | loss: 7.0404795CurrentTrain: epoch  1, batch     3 | loss: 6.8838107CurrentTrain: epoch 15, batch     0 | loss: 10.5878628CurrentTrain: epoch 15, batch     1 | loss: 14.0593455CurrentTrain: epoch 15, batch     2 | loss: 7.7106667CurrentTrain: epoch  1, batch     3 | loss: 7.1093384CurrentTrain: epoch 15, batch     0 | loss: 9.9869699CurrentTrain: epoch 15, batch     1 | loss: 15.3264812CurrentTrain: epoch 15, batch     2 | loss: 12.3946040CurrentTrain: epoch  1, batch     3 | loss: 9.6227245CurrentTrain: epoch 15, batch     0 | loss: 12.7122053CurrentTrain: epoch 15, batch     1 | loss: 8.5964081CurrentTrain: epoch 15, batch     2 | loss: 9.2184212CurrentTrain: epoch  1, batch     3 | loss: 9.6210273CurrentTrain: epoch 15, batch     0 | loss: 8.7463598CurrentTrain: epoch 15, batch     1 | loss: 7.7797644CurrentTrain: epoch 15, batch     2 | loss: 8.0981543CurrentTrain: epoch  1, batch     3 | loss: 6.0871072CurrentTrain: epoch 15, batch     0 | loss: 6.9016391CurrentTrain: epoch 15, batch     1 | loss: 10.5203046CurrentTrain: epoch 15, batch     2 | loss: 13.2588758CurrentTrain: epoch  1, batch     3 | loss: 5.8526835CurrentTrain: epoch 15, batch     0 | loss: 11.6246355CurrentTrain: epoch 15, batch     1 | loss: 11.1390222CurrentTrain: epoch 15, batch     2 | loss: 6.7661591CurrentTrain: epoch  1, batch     3 | loss: 6.6681736CurrentTrain: epoch 15, batch     0 | loss: 15.6408069CurrentTrain: epoch 15, batch     1 | loss: 8.4994068CurrentTrain: epoch 15, batch     2 | loss: 6.9863893CurrentTrain: epoch  1, batch     3 | loss: 10.3377995
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: ( 1988 ) , " night watch " ( 2004 ) , " day watch " ( 2006 ) and " legend № 17 " ( 2013 ) .
Head Entity: day watch
Tail Entity: night watch
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: ( 1995 ) , " The Phantom Menace " ( 1999 ) , " Attack of the Clones " ( 2002 ) and " Revenge of the Sith " ( 2005 ) .  
Head Entity: Attack of the Clones  
Tail Entity: The Phantom Menace  

Relation: follows  
Context: ( 2010 ) , " The Hunger Games " ( 2012 ) , " Catching Fire " ( 2013 ) and " Mockingjay " ( 2014 ) .  
Head Entity: Catching Fire  
Tail Entity: The Hunger Games  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: dick packer was a u.s. soccer center forward who was a member of the u.s. team at the 1956 summer olympics .
Head Entity: dick packer
Tail Entity: soccer
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: serena williams is a professional tennis player who has won numerous grand slam titles throughout her career.  
Head Entity: serena williams  
Tail Entity: tennis  

Relation: sport  
Context: lebron james is known for his exceptional skills in basketball and has played for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: basketball  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael was the father of two daughters, emily and sarah, who both excelled in their studies.  
Head Entity: michael  
Tail Entity: emily  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: he also made a special appearance in kbs ' romantic comedy drama " fight for my way " .
Head Entity: fight for my way
Tail Entity: kbs
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: the popular series "breaking bad" was first aired on amc, gaining a massive following.  
Head Entity: breaking bad  
Tail Entity: amc  

Relation: original network  
Context: the hit show "stranger things" premiered on netflix and quickly became a cultural phenomenon.  
Head Entity: stranger things  
Tail Entity: netflix  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: he was also nominated twice for the filmfare best telugu actor award , for the films bhale bhale magadivoy and " gentleman " .
Head Entity: gentleman
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: The movie "Parasite" received critical acclaim and was originally produced in Korean, showcasing the talents of its director Bong Joon-ho.  
Head Entity: Parasite  
Tail Entity: Korean  

Relation: original language of film or TV show  
Context: The animated series "Avatar: The Last Airbender" was created in English and has gained a massive following since its release.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: in the summer of 2013 he returned to montenegro and joined with fk rudar pljevlja , playing with the club for two seasons in the montenegrin first league .
Head Entity: fk rudar pljevlja
Tail Entity: montenegrin first league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: After a successful stint in the English Premier League, the player signed with FC Barcelona, competing in La Liga for the next three seasons.  
Head Entity: FC Barcelona  
Tail Entity: La Liga  

Relation: league  
Context: The young athlete made headlines when he joined the New York Yankees, showcasing his talent in Major League Baseball.  
Head Entity: New York Yankees  
Tail Entity: Major League Baseball  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: she is supposedly the third and youngest child of seti i and tuya , and the younger sister of ramesses ii and tia .
Head Entity: tia
Tail Entity: tuya
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: Cleopatra was the daughter of Ptolemy XII and his wife, who was also her mother, Cleopatra VI.  
Head Entity: Cleopatra  
Tail Entity: Cleopatra VI  

Relation: mother  
Context: In Norse mythology, Frigg is known as the mother of Baldr, the god of light and purity.  
Head Entity: Baldr  
Tail Entity: Frigg  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres. he is particularly famous for his interpretations of classical cello concertos.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: a u.s. government - funded $ 36 million bridge over the panj river connects sher khan bandar in afghanistan with nizhniy pyanzh in tajikistan , which transport more than 150 trucks or 1,000 cars daily .
Head Entity: sher khan bandar
Tail Entity: afghanistan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the ancient city of petra, located in southern jordan, is famous for its rock-cut architecture and water conduit system, attracting thousands of tourists each year.  
Head Entity: petra  
Tail Entity: jordan  

Relation: country  
Context: the great wall of china, a series of fortifications made of various materials, stretches across northern china and is a UNESCO World Heritage site.  
Head Entity: great wall of china  
Tail Entity: china  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: klaus kinski , who would play dracula himself nine years later in " nosferatu the vampyre " , is also featured in the film as renfield .
Head Entity: nosferatu the vampyre
Tail Entity: renfield
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: in the animated series "avatar: the last airbender," zuko is a complex character who struggles with his identity and family legacy.  
Head Entity: avatar: the last airbender  
Tail Entity: zuko  

Relation: characters  
Context: the novel "pride and prejudice" features elizabeth bennet, who navigates societal expectations and her feelings for mr. darcy.  
Head Entity: pride and prejudice  
Tail Entity: elizabeth bennet  
MemoryTrain:  epoch 15, batch     0 | loss: 5.2381037MemoryTrain:  epoch 15, batch     1 | loss: 6.4085545MemoryTrain:  epoch 15, batch     2 | loss: 8.2747238MemoryTrain:  epoch 15, batch     3 | loss: 6.9884576MemoryTrain:  epoch 15, batch     4 | loss: 4.9613001MemoryTrain:  epoch  9, batch     5 | loss: 4.7427803MemoryTrain:  epoch 15, batch     0 | loss: 5.4391305MemoryTrain:  epoch 15, batch     1 | loss: 4.3366544MemoryTrain:  epoch 15, batch     2 | loss: 4.1908925MemoryTrain:  epoch 15, batch     3 | loss: 4.8172551MemoryTrain:  epoch 15, batch     4 | loss: 12.4733225MemoryTrain:  epoch  9, batch     5 | loss: 5.3741807MemoryTrain:  epoch 15, batch     0 | loss: 3.2841383MemoryTrain:  epoch 15, batch     1 | loss: 3.6497585MemoryTrain:  epoch 15, batch     2 | loss: 2.8255987MemoryTrain:  epoch 15, batch     3 | loss: 4.3592947MemoryTrain:  epoch 15, batch     4 | loss: 3.2838724MemoryTrain:  epoch  9, batch     5 | loss: 5.8369949MemoryTrain:  epoch 15, batch     0 | loss: 2.7026322MemoryTrain:  epoch 15, batch     1 | loss: 3.1032401MemoryTrain:  epoch 15, batch     2 | loss: 6.3254747MemoryTrain:  epoch 15, batch     3 | loss: 3.1636889MemoryTrain:  epoch 15, batch     4 | loss: 3.0596145MemoryTrain:  epoch  9, batch     5 | loss: 2.2436041MemoryTrain:  epoch 15, batch     0 | loss: 2.9075652MemoryTrain:  epoch 15, batch     1 | loss: 3.1185897MemoryTrain:  epoch 15, batch     2 | loss: 6.2734714MemoryTrain:  epoch 15, batch     3 | loss: 3.7880418MemoryTrain:  epoch 15, batch     4 | loss: 2.7157919MemoryTrain:  epoch  9, batch     5 | loss: 4.4484760MemoryTrain:  epoch 15, batch     0 | loss: 3.3641026MemoryTrain:  epoch 15, batch     1 | loss: 2.3645170MemoryTrain:  epoch 15, batch     2 | loss: 6.1621248MemoryTrain:  epoch 15, batch     3 | loss: 2.9721645MemoryTrain:  epoch 15, batch     4 | loss: 2.1347861MemoryTrain:  epoch  9, batch     5 | loss: 3.7768521MemoryTrain:  epoch 15, batch     0 | loss: 2.1644606MemoryTrain:  epoch 15, batch     1 | loss: 3.4133481MemoryTrain:  epoch 15, batch     2 | loss: 5.1636261MemoryTrain:  epoch 15, batch     3 | loss: 2.5283593MemoryTrain:  epoch 15, batch     4 | loss: 4.5337271MemoryTrain:  epoch  9, batch     5 | loss: 4.2767149MemoryTrain:  epoch 15, batch     0 | loss: 2.7060515MemoryTrain:  epoch 15, batch     1 | loss: 2.6621291MemoryTrain:  epoch 15, batch     2 | loss: 2.1983085MemoryTrain:  epoch 15, batch     3 | loss: 5.6874879MemoryTrain:  epoch 15, batch     4 | loss: 4.2809993MemoryTrain:  epoch  9, batch     5 | loss: 5.1149170MemoryTrain:  epoch 15, batch     0 | loss: 5.1032688MemoryTrain:  epoch 15, batch     1 | loss: 3.9813024MemoryTrain:  epoch 15, batch     2 | loss: 7.0187263MemoryTrain:  epoch 15, batch     3 | loss: 6.3032445MemoryTrain:  epoch 15, batch     4 | loss: 2.6346071MemoryTrain:  epoch  9, batch     5 | loss: 1.7894617MemoryTrain:  epoch 15, batch     0 | loss: 4.8911020MemoryTrain:  epoch 15, batch     1 | loss: 2.7529377MemoryTrain:  epoch 15, batch     2 | loss: 1.6828633MemoryTrain:  epoch 15, batch     3 | loss: 2.8154278MemoryTrain:  epoch 15, batch     4 | loss: 2.2211078MemoryTrain:  epoch  9, batch     5 | loss: 4.2798825
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 47.92%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 53.12%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 45.00%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 45.83%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 51.79%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 57.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 68.18%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 72.12%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 72.32%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 73.33%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 72.66%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 72.79%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 73.36%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 74.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 75.89%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 76.70%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 77.72%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 78.65%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 79.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 79.81%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 80.32%   [EVAL] batch:   27 | acc: 68.75%,  total acc: 79.91%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 80.17%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 80.21%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 80.44%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 80.86%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.44%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 81.80%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 82.47%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 82.94%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 82.73%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 82.05%   [EVAL] batch:   39 | acc: 43.75%,  total acc: 81.09%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 80.64%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 79.76%   [EVAL] batch:   42 | acc: 31.25%,  total acc: 78.63%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 78.27%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 79.08%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 79.52%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 79.82%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 80.10%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 80.25%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 79.78%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 79.33%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 78.89%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 78.47%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 78.41%   [EVAL] batch:   55 | acc: 31.25%,  total acc: 77.57%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 77.41%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 77.26%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 77.22%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 77.29%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 77.56%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 77.92%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 77.38%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 82.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 84.93%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 86.18%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 86.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 87.20%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 87.22%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 87.23%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.98%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 88.43%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 88.62%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 88.79%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 89.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 89.52%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 90.15%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 90.44%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 90.54%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 90.80%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 91.05%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 91.28%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.51%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.72%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.92%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 92.15%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 92.33%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 92.53%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 92.42%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 92.45%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.60%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 92.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.65%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 92.67%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 92.69%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.82%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 92.73%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.75%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 92.54%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 91.92%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 91.63%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 91.46%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 91.29%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 91.13%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 91.07%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 90.53%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 90.10%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 89.87%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 89.55%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 89.25%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 89.13%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 89.20%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 89.00%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 88.89%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 88.96%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 89.02%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 89.00%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 88.32%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 87.66%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 86.78%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 86.23%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 85.86%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 85.11%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 85.06%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 85.24%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 85.51%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 85.68%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 85.78%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 85.80%   [EVAL] batch:   88 | acc: 31.25%,  total acc: 85.18%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 84.58%   [EVAL] batch:   90 | acc: 37.50%,  total acc: 84.07%   [EVAL] batch:   91 | acc: 43.75%,  total acc: 83.63%   [EVAL] batch:   92 | acc: 18.75%,  total acc: 82.93%   [EVAL] batch:   93 | acc: 56.25%,  total acc: 82.65%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 82.83%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 82.94%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 83.12%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 83.29%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 83.46%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 83.62%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 83.66%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 83.39%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 83.19%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 82.99%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 83.04%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 82.90%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 82.71%   [EVAL] batch:  107 | acc: 56.25%,  total acc: 82.47%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 82.28%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 81.93%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 81.64%   [EVAL] batch:  111 | acc: 56.25%,  total acc: 81.42%   [EVAL] batch:  112 | acc: 81.25%,  total acc: 81.42%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 81.30%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 81.36%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 81.36%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 81.41%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 81.51%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 81.51%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 81.61%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 81.71%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 81.71%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 81.66%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 81.65%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 81.65%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 81.45%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 80.86%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 80.77%   [EVAL] batch:  129 | acc: 12.50%,  total acc: 80.24%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 80.01%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 80.07%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 80.22%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 80.46%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 80.56%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 80.70%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 80.75%   [EVAL] batch:  138 | acc: 75.00%,  total acc: 80.71%   [EVAL] batch:  139 | acc: 87.50%,  total acc: 80.76%   [EVAL] batch:  140 | acc: 62.50%,  total acc: 80.63%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 80.59%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 80.55%   [EVAL] batch:  143 | acc: 81.25%,  total acc: 80.56%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 80.69%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 80.82%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 80.91%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 81.04%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 81.17%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 81.33%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 81.41%   [EVAL] batch:  152 | acc: 68.75%,  total acc: 81.33%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 81.37%   [EVAL] batch:  154 | acc: 81.25%,  total acc: 81.37%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 81.41%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 81.49%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 81.61%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 81.68%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 81.76%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 81.83%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 81.94%   [EVAL] batch:  162 | acc: 75.00%,  total acc: 81.90%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 81.75%   [EVAL] batch:  164 | acc: 43.75%,  total acc: 81.52%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 81.40%   [EVAL] batch:  166 | acc: 43.75%,  total acc: 81.18%   [EVAL] batch:  167 | acc: 31.25%,  total acc: 80.88%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 80.77%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 80.88%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 80.96%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 81.07%   [EVAL] batch:  172 | acc: 93.75%,  total acc: 81.14%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 81.21%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 81.11%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 80.97%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 80.83%   [EVAL] batch:  178 | acc: 56.25%,  total acc: 80.69%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 80.66%   [EVAL] batch:  180 | acc: 31.25%,  total acc: 80.39%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 80.32%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 80.26%   [EVAL] batch:  183 | acc: 75.00%,  total acc: 80.23%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 80.24%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 80.31%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 80.41%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 80.22%   
cur_acc:  ['0.9454', '0.7867', '0.7738']
his_acc:  ['0.9454', '0.8590', '0.8022']
CurrentTrain: epoch 15, batch     0 | loss: 17.3637091CurrentTrain: epoch 15, batch     1 | loss: 14.6610211CurrentTrain: epoch 15, batch     2 | loss: 15.9020508CurrentTrain: epoch  1, batch     3 | loss: 9.1293043CurrentTrain: epoch 15, batch     0 | loss: 17.3242117CurrentTrain: epoch 15, batch     1 | loss: 18.0280404CurrentTrain: epoch 15, batch     2 | loss: 18.0609180CurrentTrain: epoch  1, batch     3 | loss: 11.6469752CurrentTrain: epoch 15, batch     0 | loss: 8.4177214CurrentTrain: epoch 15, batch     1 | loss: 10.4181151CurrentTrain: epoch 15, batch     2 | loss: 9.2330880CurrentTrain: epoch  1, batch     3 | loss: 11.0957839CurrentTrain: epoch 15, batch     0 | loss: 10.1065987CurrentTrain: epoch 15, batch     1 | loss: 9.9407758CurrentTrain: epoch 15, batch     2 | loss: 8.9045603CurrentTrain: epoch  1, batch     3 | loss: 9.0903369CurrentTrain: epoch 15, batch     0 | loss: 15.0474730CurrentTrain: epoch 15, batch     1 | loss: 13.4774939CurrentTrain: epoch 15, batch     2 | loss: 13.9290625CurrentTrain: epoch  1, batch     3 | loss: 13.3475478CurrentTrain: epoch 15, batch     0 | loss: 9.7561689CurrentTrain: epoch 15, batch     1 | loss: 15.8815447CurrentTrain: epoch 15, batch     2 | loss: 11.9951087CurrentTrain: epoch  1, batch     3 | loss: 7.5139330CurrentTrain: epoch 15, batch     0 | loss: 13.2929123CurrentTrain: epoch 15, batch     1 | loss: 6.8033990CurrentTrain: epoch 15, batch     2 | loss: 7.3253380CurrentTrain: epoch  1, batch     3 | loss: 7.5014760CurrentTrain: epoch 15, batch     0 | loss: 10.8661935CurrentTrain: epoch 15, batch     1 | loss: 10.8879643CurrentTrain: epoch 15, batch     2 | loss: 8.2245178CurrentTrain: epoch  1, batch     3 | loss: 6.1518403CurrentTrain: epoch 15, batch     0 | loss: 9.8093033CurrentTrain: epoch 15, batch     1 | loss: 11.7585795CurrentTrain: epoch 15, batch     2 | loss: 9.8014598CurrentTrain: epoch  1, batch     3 | loss: 12.8815534CurrentTrain: epoch 15, batch     0 | loss: 15.1003924CurrentTrain: epoch 15, batch     1 | loss: 8.3071071CurrentTrain: epoch 15, batch     2 | loss: 8.3144236CurrentTrain: epoch  1, batch     3 | loss: 11.9816158
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: there was concern in the gun lobby over restrictions on second amendment rights in the united states .. ( march 7 , 2000 ) cnn .
Head Entity: second amendment
Tail Entity: united states
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The new environmental regulations introduced by the European Union aim to reduce carbon emissions across member states.  
Head Entity: environmental regulations  
Tail Entity: European Union  

Relation: applies to jurisdiction  
Context: The recent changes in tax laws have raised questions about their applicability in various states across the country.  
Head Entity: tax laws  
Tail Entity: states
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: " trouble on my mind " is a song by american hip hop recording artist pusha t , released as the lead single from his debut ep " " .
Head Entity: trouble on my mind
Tail Entity: pusha t
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: "Shape of You" is a hit song by British singer-songwriter Ed Sheeran, featured on his album "÷ (Divide)".  
Head Entity: Shape of You  
Tail Entity: Ed Sheeran  

Relation: performer  
Context: "Bohemian Rhapsody" is a classic rock song by the British band Queen, known for its unique structure and operatic elements.  
Head Entity: Bohemian Rhapsody  
Tail Entity: Queen  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the glasflügel h-201 standard " libelle " ( german : " dragonfly " ) is an early composite standard class single - seat sailplane produced by glasflügel from 1967 .
Head Entity: dragonfly
Tail Entity: glasflügel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The iPhone 13, a smartphone developed by Apple, was released in September 2021 and features advanced camera technology and improved battery life.  
Head Entity: iPhone 13  
Tail Entity: Apple  

Relation: manufacturer  
Context: The Tesla Model S is an all-electric luxury sedan that was first introduced by Tesla, Inc. in 2012, showcasing innovative technology and performance.  
Head Entity: Tesla Model S  
Tail Entity: Tesla, Inc.  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: at the 2016 summer olympics , heath and schofield won the silver k-2 200 event , finishing behind spain 's saúl craviotto and cristian toro .
Head Entity: 2016 summer olympics
Tail Entity: spain
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: during the 2020 presidential election, the democratic party nominated joe biden as their candidate, while the republican party supported donald trump.  
Head Entity: 2020 presidential election  
Tail Entity: democratic party  

Relation: participant  
Context: in the annual tech conference, google showcased its latest innovations, while apple presented its new products to the audience.  
Head Entity: annual tech conference  
Tail Entity: google  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the application is compatible with various operating systems, including macOS, Windows, and Ubuntu.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: This game runs smoothly on multiple operating systems, such as Linux, Windows 10, and older versions of Windows.  
Head Entity: game  
Tail Entity: Linux  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in july 2007 , griffin lost by a technical knockout to light heavyweight champion danny green of the world boxing association at challenge stadium in perth , australia .
Head Entity: danny green
Tail Entity: light heavyweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: during the 2020 olympics, the men's 100 meters final showcased athletes from various competition classes, with usain bolt representing the sprinting class.  
Head Entity: usain bolt  
Tail Entity: sprinting  

Relation: competition class  
Context: the formula one race in monaco is known for its high-speed competition class, where lewis hamilton competed fiercely against other top drivers.  
Head Entity: lewis hamilton  
Tail Entity: formula one
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: the qup gained 3 seats from its previous election result , and won a 10 % swing to it in primary votes , but failed to defeat the incumbent labor government led by ted theodore .
Head Entity: previous election
Tail Entity: ted theodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: After a hard-fought campaign, the voters of the district chose Sarah Johnson as their representative, marking a significant shift in the political landscape.  
Head Entity: campaign  
Tail Entity: Sarah Johnson  

Relation: successful candidate  
Context: In the recent mayoral election, the community rallied behind James Lee, who promised to bring new initiatives to the city, ultimately leading to his victory.  
Head Entity: mayoral election  
Tail Entity: James Lee  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: although the latter 's eldest son , lǫgmaðr , appears to have succeeded to the kingship , he was soon forced to contend with factions supporting his younger brothers : haraldr , and óláfr .
Head Entity: lǫgmaðr
Tail Entity: óláfr
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: In the royal family of the ancient kingdom, the two brothers, Eirik and Sigurd, were known for their fierce loyalty to each other, having grown up under the same roof and shared the same parents.  
Head Entity: Eirik  
Tail Entity: Sigurd  

Relation: sibling  
Context: During the family reunion, it became clear that both Anna and her brother, Mark, had inherited their parents' artistic talents, showcasing their skills in painting and music.  
Head Entity: Anna  
Tail Entity: Mark  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: two of her sons , tunis and thomas tingey rose to prominence in the union navy during the civil war .
Head Entity: tunis
Tail Entity: union navy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: after serving in the army, he transitioned to a leadership role in the air force, where he implemented several key strategies.  
Head Entity: air force  
Tail Entity: army  

Relation: military branch  
Context: the general was honored for his service in the marine corps, where he led numerous successful missions.  
Head Entity: marine corps  
Tail Entity: general  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: ziad rahbani is the son of the lebanese famous composer assi rahbani and nouhad haddad , the famous lebanese female singer known as fairuz .
Head Entity: assi rahbani
Tail Entity: ziad rahbani
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: in the animated series, the character of simon is the son of the adventurous couple, marie and tom.  
Head Entity: marie  
Tail Entity: simon  

Relation: child  
Context: the renowned physicist albert einstein had a daughter named lieserl, who was born in 1902.  
Head Entity: albert einstein  
Tail Entity: lieserl  
MemoryTrain:  epoch 15, batch     0 | loss: 6.1642524MemoryTrain:  epoch 15, batch     1 | loss: 5.7951040MemoryTrain:  epoch 15, batch     2 | loss: 6.3894978MemoryTrain:  epoch 15, batch     3 | loss: 5.2707183MemoryTrain:  epoch 15, batch     4 | loss: 3.5679945MemoryTrain:  epoch 15, batch     5 | loss: 5.1856818MemoryTrain:  epoch 15, batch     6 | loss: 4.2035284MemoryTrain:  epoch  7, batch     7 | loss: 2.8075232MemoryTrain:  epoch 15, batch     0 | loss: 3.5930207MemoryTrain:  epoch 15, batch     1 | loss: 3.8553911MemoryTrain:  epoch 15, batch     2 | loss: 5.2703699MemoryTrain:  epoch 15, batch     3 | loss: 3.4677364MemoryTrain:  epoch 15, batch     4 | loss: 3.3732879MemoryTrain:  epoch 15, batch     5 | loss: 3.1611898MemoryTrain:  epoch 15, batch     6 | loss: 2.2940769MemoryTrain:  epoch  7, batch     7 | loss: 6.0178235MemoryTrain:  epoch 15, batch     0 | loss: 3.2617906MemoryTrain:  epoch 15, batch     1 | loss: 3.8062604MemoryTrain:  epoch 15, batch     2 | loss: 2.5947399MemoryTrain:  epoch 15, batch     3 | loss: 5.0618592MemoryTrain:  epoch 15, batch     4 | loss: 6.0991414MemoryTrain:  epoch 15, batch     5 | loss: 5.9515872MemoryTrain:  epoch 15, batch     6 | loss: 3.2640819MemoryTrain:  epoch  7, batch     7 | loss: 3.2854391MemoryTrain:  epoch 15, batch     0 | loss: 2.5485000MemoryTrain:  epoch 15, batch     1 | loss: 3.5311618MemoryTrain:  epoch 15, batch     2 | loss: 2.2608924MemoryTrain:  epoch 15, batch     3 | loss: 2.8839219MemoryTrain:  epoch 15, batch     4 | loss: 4.7498219MemoryTrain:  epoch 15, batch     5 | loss: 3.9535825MemoryTrain:  epoch 15, batch     6 | loss: 5.3868358MemoryTrain:  epoch  7, batch     7 | loss: 4.4232881MemoryTrain:  epoch 15, batch     0 | loss: 2.3468935MemoryTrain:  epoch 15, batch     1 | loss: 4.2792041MemoryTrain:  epoch 15, batch     2 | loss: 2.2179884MemoryTrain:  epoch 15, batch     3 | loss: 2.2660374MemoryTrain:  epoch 15, batch     4 | loss: 3.2030561MemoryTrain:  epoch 15, batch     5 | loss: 4.6111096MemoryTrain:  epoch 15, batch     6 | loss: 2.0191201MemoryTrain:  epoch  7, batch     7 | loss: 1.5189386MemoryTrain:  epoch 15, batch     0 | loss: 1.5705422MemoryTrain:  epoch 15, batch     1 | loss: 2.6737166MemoryTrain:  epoch 15, batch     2 | loss: 4.2857655MemoryTrain:  epoch 15, batch     3 | loss: 2.1677179MemoryTrain:  epoch 15, batch     4 | loss: 2.1208307MemoryTrain:  epoch 15, batch     5 | loss: 1.7922184MemoryTrain:  epoch 15, batch     6 | loss: 2.8013173MemoryTrain:  epoch  7, batch     7 | loss: 4.5076165MemoryTrain:  epoch 15, batch     0 | loss: 2.2730631MemoryTrain:  epoch 15, batch     1 | loss: 1.8787253MemoryTrain:  epoch 15, batch     2 | loss: 1.9543150MemoryTrain:  epoch 15, batch     3 | loss: 2.9192847MemoryTrain:  epoch 15, batch     4 | loss: 1.8882827MemoryTrain:  epoch 15, batch     5 | loss: 6.6838185MemoryTrain:  epoch 15, batch     6 | loss: 2.0732669MemoryTrain:  epoch  7, batch     7 | loss: 1.8017668MemoryTrain:  epoch 15, batch     0 | loss: 11.4248684MemoryTrain:  epoch 15, batch     1 | loss: 1.8933589MemoryTrain:  epoch 15, batch     2 | loss: 4.5911022MemoryTrain:  epoch 15, batch     3 | loss: 2.0211910MemoryTrain:  epoch 15, batch     4 | loss: 2.3043188MemoryTrain:  epoch 15, batch     5 | loss: 1.9298790MemoryTrain:  epoch 15, batch     6 | loss: 4.3898892MemoryTrain:  epoch  7, batch     7 | loss: 1.5583301MemoryTrain:  epoch 15, batch     0 | loss: 2.1135147MemoryTrain:  epoch 15, batch     1 | loss: 1.9301796MemoryTrain:  epoch 15, batch     2 | loss: 5.2475534MemoryTrain:  epoch 15, batch     3 | loss: 3.9268783MemoryTrain:  epoch 15, batch     4 | loss: 4.5524985MemoryTrain:  epoch 15, batch     5 | loss: 1.6595713MemoryTrain:  epoch 15, batch     6 | loss: 3.9041781MemoryTrain:  epoch  7, batch     7 | loss: 1.6655717MemoryTrain:  epoch 15, batch     0 | loss: 2.0465929MemoryTrain:  epoch 15, batch     1 | loss: 1.8271113MemoryTrain:  epoch 15, batch     2 | loss: 1.8328755MemoryTrain:  epoch 15, batch     3 | loss: 1.5909913MemoryTrain:  epoch 15, batch     4 | loss: 4.3232256MemoryTrain:  epoch 15, batch     5 | loss: 2.1277167MemoryTrain:  epoch 15, batch     6 | loss: 2.4387435MemoryTrain:  epoch  7, batch     7 | loss: 3.8406863
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 9.38%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 8.33%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 9.38%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 10.00%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 11.46%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 18.75%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 26.56%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 29.86%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 35.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 39.77%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 42.19%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 45.67%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 48.66%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 50.83%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 52.73%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 54.04%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 57.89%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 57.81%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 57.14%   [EVAL] batch:   21 | acc: 50.00%,  total acc: 56.82%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 56.52%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 55.47%   [EVAL] batch:   24 | acc: 43.75%,  total acc: 55.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 56.73%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 58.33%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 59.82%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 61.21%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 63.71%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 64.84%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 65.91%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 66.91%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 67.86%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 69.59%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 70.39%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 70.99%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 70.94%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 70.88%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 71.28%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 71.80%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 72.02%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 71.39%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 70.38%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 69.55%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 69.27%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 68.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 68.87%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 69.35%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 69.69%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 70.02%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 70.45%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 70.87%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 70.50%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 69.94%   [EVAL] batch:   58 | acc: 25.00%,  total acc: 69.17%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:   60 | acc: 25.00%,  total acc: 68.03%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 67.24%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 66.47%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 81.82%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 82.21%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 85.66%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 86.84%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 86.88%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 87.20%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 86.93%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 86.41%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.78%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 87.27%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 87.93%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.71%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 89.39%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 89.71%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 89.82%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 90.37%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.87%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.09%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.31%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.52%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 91.57%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 91.48%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 91.53%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 91.17%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 90.56%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 90.23%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 90.31%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 89.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.83%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.90%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 89.98%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 90.16%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 90.00%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.07%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 89.80%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 89.01%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 88.56%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 88.44%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 88.22%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 88.10%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 88.00%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 87.40%   [EVAL] batch:   64 | acc: 50.00%,  total acc: 86.83%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 86.55%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 86.01%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 85.75%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 85.51%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 85.65%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 85.59%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 85.62%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 85.81%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 86.00%   [EVAL] batch:   75 | acc: 25.00%,  total acc: 85.20%   [EVAL] batch:   76 | acc: 18.75%,  total acc: 84.33%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 83.57%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 82.99%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 82.58%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 81.79%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 81.78%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 82.00%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 82.22%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 82.35%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 82.56%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 82.69%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 82.67%   [EVAL] batch:   88 | acc: 25.00%,  total acc: 82.02%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 81.46%   [EVAL] batch:   90 | acc: 31.25%,  total acc: 80.91%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 80.57%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 79.77%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 79.32%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 79.54%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 79.69%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 79.83%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 80.04%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 80.24%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 80.44%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 80.57%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 80.51%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 80.46%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 80.59%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 80.77%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 80.84%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 80.67%   [EVAL] batch:  107 | acc: 56.25%,  total acc: 80.44%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 80.28%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 79.89%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 79.62%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 79.35%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 79.15%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 78.89%   [EVAL] batch:  114 | acc: 75.00%,  total acc: 78.86%   [EVAL] batch:  115 | acc: 68.75%,  total acc: 78.77%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 78.85%   [EVAL] batch:  117 | acc: 68.75%,  total acc: 78.76%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 78.73%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 78.85%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 78.98%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 79.00%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 79.01%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 79.03%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 79.05%   [EVAL] batch:  125 | acc: 37.50%,  total acc: 78.72%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 78.40%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 77.98%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 77.66%   [EVAL] batch:  129 | acc: 25.00%,  total acc: 77.26%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 76.86%   [EVAL] batch:  131 | acc: 56.25%,  total acc: 76.70%   [EVAL] batch:  132 | acc: 68.75%,  total acc: 76.64%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 76.63%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 76.71%   [EVAL] batch:  135 | acc: 75.00%,  total acc: 76.70%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 76.78%   [EVAL] batch:  137 | acc: 56.25%,  total acc: 76.63%   [EVAL] batch:  138 | acc: 50.00%,  total acc: 76.44%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 76.29%   [EVAL] batch:  140 | acc: 68.75%,  total acc: 76.24%   [EVAL] batch:  141 | acc: 62.50%,  total acc: 76.14%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 76.14%   [EVAL] batch:  143 | acc: 50.00%,  total acc: 75.95%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 76.12%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 76.28%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 76.40%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:  148 | acc: 93.75%,  total acc: 76.68%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 76.79%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 76.90%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 77.01%   [EVAL] batch:  152 | acc: 81.25%,  total acc: 77.04%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 77.11%   [EVAL] batch:  154 | acc: 81.25%,  total acc: 77.14%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 77.20%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 77.31%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 77.45%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 77.56%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 77.66%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 77.76%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 77.85%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 77.72%   [EVAL] batch:  163 | acc: 25.00%,  total acc: 77.40%   [EVAL] batch:  164 | acc: 12.50%,  total acc: 77.01%   [EVAL] batch:  165 | acc: 25.00%,  total acc: 76.69%   [EVAL] batch:  166 | acc: 12.50%,  total acc: 76.31%   [EVAL] batch:  167 | acc: 6.25%,  total acc: 75.89%   [EVAL] batch:  168 | acc: 31.25%,  total acc: 75.63%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 75.77%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 75.88%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 76.02%   [EVAL] batch:  172 | acc: 93.75%,  total acc: 76.12%   [EVAL] batch:  173 | acc: 87.50%,  total acc: 76.19%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 76.25%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 76.14%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 76.06%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 75.95%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 75.80%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 75.80%   [EVAL] batch:  180 | acc: 43.75%,  total acc: 75.62%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 75.58%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 75.61%   [EVAL] batch:  183 | acc: 75.00%,  total acc: 75.61%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 75.64%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 75.74%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 75.84%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 75.66%   [EVAL] batch:  188 | acc: 6.25%,  total acc: 75.30%   [EVAL] batch:  189 | acc: 6.25%,  total acc: 74.93%   [EVAL] batch:  190 | acc: 12.50%,  total acc: 74.61%   [EVAL] batch:  191 | acc: 6.25%,  total acc: 74.25%   [EVAL] batch:  192 | acc: 31.25%,  total acc: 74.03%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 73.74%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 73.81%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 73.82%   [EVAL] batch:  196 | acc: 56.25%,  total acc: 73.73%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 73.83%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 73.84%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 73.88%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 73.91%   [EVAL] batch:  201 | acc: 93.75%,  total acc: 74.01%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 74.05%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 73.96%   [EVAL] batch:  204 | acc: 93.75%,  total acc: 74.05%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 74.15%   [EVAL] batch:  206 | acc: 75.00%,  total acc: 74.15%   [EVAL] batch:  207 | acc: 43.75%,  total acc: 74.01%   [EVAL] batch:  208 | acc: 43.75%,  total acc: 73.86%   [EVAL] batch:  209 | acc: 62.50%,  total acc: 73.81%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 73.64%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 73.44%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 73.44%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 73.57%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 73.69%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 73.81%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 73.93%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 74.05%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 74.17%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 74.29%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 74.41%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 74.52%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 74.64%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 74.75%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 74.86%   [EVAL] batch:  225 | acc: 93.75%,  total acc: 74.94%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 74.95%   [EVAL] batch:  228 | acc: 81.25%,  total acc: 74.97%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 75.05%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 75.14%   [EVAL] batch:  231 | acc: 50.00%,  total acc: 75.03%   [EVAL] batch:  232 | acc: 31.25%,  total acc: 74.84%   [EVAL] batch:  233 | acc: 43.75%,  total acc: 74.71%   [EVAL] batch:  234 | acc: 25.00%,  total acc: 74.49%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 74.44%   [EVAL] batch:  236 | acc: 37.50%,  total acc: 74.29%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 74.32%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 74.37%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 74.48%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 74.51%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 74.59%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 74.67%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 74.72%   [EVAL] batch:  244 | acc: 25.00%,  total acc: 74.52%   [EVAL] batch:  245 | acc: 37.50%,  total acc: 74.36%   [EVAL] batch:  246 | acc: 25.00%,  total acc: 74.16%   [EVAL] batch:  247 | acc: 37.50%,  total acc: 74.02%   [EVAL] batch:  248 | acc: 18.75%,  total acc: 73.80%   [EVAL] batch:  249 | acc: 31.25%,  total acc: 73.62%   
cur_acc:  ['0.9454', '0.7867', '0.7738', '0.6647']
his_acc:  ['0.9454', '0.8590', '0.8022', '0.7362']
CurrentTrain: epoch 15, batch     0 | loss: 11.6471290CurrentTrain: epoch 15, batch     1 | loss: 9.6875856CurrentTrain: epoch 15, batch     2 | loss: 15.6234292CurrentTrain: epoch  1, batch     3 | loss: 14.2099381CurrentTrain: epoch 15, batch     0 | loss: 7.5626775CurrentTrain: epoch 15, batch     1 | loss: 11.1529771CurrentTrain: epoch 15, batch     2 | loss: 13.9918652CurrentTrain: epoch  1, batch     3 | loss: 6.9489661CurrentTrain: epoch 15, batch     0 | loss: 10.2333282CurrentTrain: epoch 15, batch     1 | loss: 11.2037965CurrentTrain: epoch 15, batch     2 | loss: 7.5896717CurrentTrain: epoch  1, batch     3 | loss: 9.2502343CurrentTrain: epoch 15, batch     0 | loss: 7.4180805CurrentTrain: epoch 15, batch     1 | loss: 9.9646740CurrentTrain: epoch 15, batch     2 | loss: 7.2183072CurrentTrain: epoch  1, batch     3 | loss: 7.4911235CurrentTrain: epoch 15, batch     0 | loss: 10.9439759CurrentTrain: epoch 15, batch     1 | loss: 6.2679439CurrentTrain: epoch 15, batch     2 | loss: 6.4461638CurrentTrain: epoch  1, batch     3 | loss: 6.8398497CurrentTrain: epoch 15, batch     0 | loss: 7.3267858CurrentTrain: epoch 15, batch     1 | loss: 7.1988083CurrentTrain: epoch 15, batch     2 | loss: 7.2676607CurrentTrain: epoch  1, batch     3 | loss: 7.0559854CurrentTrain: epoch 15, batch     0 | loss: 5.1135351CurrentTrain: epoch 15, batch     1 | loss: 8.3309321CurrentTrain: epoch 15, batch     2 | loss: 11.4106962CurrentTrain: epoch  1, batch     3 | loss: 7.6965209CurrentTrain: epoch 15, batch     0 | loss: 6.9397011CurrentTrain: epoch 15, batch     1 | loss: 12.8687646CurrentTrain: epoch 15, batch     2 | loss: 7.1360847CurrentTrain: epoch  1, batch     3 | loss: 5.4709840CurrentTrain: epoch 15, batch     0 | loss: 10.6119440CurrentTrain: epoch 15, batch     1 | loss: 13.7731138CurrentTrain: epoch 15, batch     2 | loss: 7.0066263CurrentTrain: epoch  1, batch     3 | loss: 6.9332384CurrentTrain: epoch 15, batch     0 | loss: 8.0632061CurrentTrain: epoch 15, batch     1 | loss: 6.6606378CurrentTrain: epoch 15, batch     2 | loss: 6.1577463CurrentTrain: epoch  1, batch     3 | loss: 5.7168711
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: dresser is an analog to anton drexler , the founder of the nazi party which was then hijacked by adolf hitler .
Head Entity: anton drexler
Tail Entity: nazi party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: After serving as a senator, john doe became a prominent member of the democratic party, advocating for various social reforms.  
Head Entity: john doe  
Tail Entity: democratic party  

Relation: member of political party  
Context: During his tenure as a governor, jane smith was a key figure in the republican party, influencing many policies and campaigns.  
Head Entity: jane smith  
Tail Entity: republican party  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: the original story of real life escape of betty mahmoody is depicted in the movie " not without my daughter " which itself was based on betty mahmoody 's book of the same name .
Head Entity: not without my daughter
Tail Entity: betty mahmoody
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film "the great gatsby" draws heavily from f. scott fitzgerald's classic novel, capturing the essence of the roaring twenties and the complexities of love and ambition.  
Head Entity: the great gatsby  
Tail Entity: f. scott fitzgerald  

Relation: after a work by  
Context: the animated feature "the lion king" was inspired by shakespeare's play "hamlet," incorporating themes of betrayal, revenge, and the struggle for power.  
Head Entity: the lion king  
Tail Entity: shakespeare
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: several paintings are also mentioned , including rousseau 's " the dream " and van gogh 's " bedroom in arles " .
Head Entity: rousseau
Tail Entity: the dream
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: Among his many contributions to literature, one of his most acclaimed novels is "The Great Gatsby," which explores themes of wealth and social change in 1920s America.  
Head Entity: F. Scott Fitzgerald  
Tail Entity: The Great Gatsby  

Relation: notable work  
Context: The artist is best known for his groundbreaking sculpture "The Thinker," which has become a symbol of philosophy and contemplation.  
Head Entity: Auguste Rodin  
Tail Entity: The Thinker  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: at the request of the stockholm institute for bible translation , mirza khazar translated the bible – the new testament and old testament – into the azerbaijani language .
Head Entity: institute for bible translation
Tail Entity: stockholm
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: the global technology company, headquartered in san francisco, has been a leader in software development for over two decades.  
Head Entity: global technology company  
Tail Entity: san francisco  

Relation: headquarters location  
Context: after relocating its main office to new york city, the fashion brand saw a significant increase in sales and brand recognition.  
Head Entity: fashion brand  
Tail Entity: new york city  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most well-known, classified under the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of living organisms, the order Primates encompasses various families, including Hominidae, which contains humans and their closest relatives.  
Head Entity: Hominidae  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: he later collaborated with vito volterra and explored laplace transforms and other parts of functional analysis .
Head Entity: vito volterra
Tail Entity: functional analysis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: Marie Curie conducted pioneering research on radioactivity, which laid the groundwork for advancements in nuclear physics.  
Head Entity: Marie Curie  
Tail Entity: nuclear physics  

Relation: field of work  
Context: The company specializes in artificial intelligence and machine learning, providing innovative solutions for various industries.  
Head Entity: The company  
Tail Entity: artificial intelligence
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: this early involvement in radio led him to buy many small , under performing radio stations beginning with wcos in columbia , south carolina , and turn them around with new formats and improved management .
Head Entity: wcos
Tail Entity: columbia , south carolina
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: The new television station, KXYZ, has been granted the necessary permits to operate and is now officially licensed to broadcast to the entire region of Northern California.  
Head Entity: KXYZ  
Tail Entity: Northern California  

Relation: licensed to broadcast to  
Context: After acquiring the rights, the network announced that it is now licensed to broadcast to viewers in the greater Miami area, expanding its reach significantly.  
Head Entity: the network  
Tail Entity: greater Miami area  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: tau² eridani ( τ² eridani , abbreviated tau² eri , τ² eri ) , also named angetenar , is a star in the constellation of eridanus .
Head Entity: angetenar
Tail Entity: eridanus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: betelgeuse ( α orionis ) is a red supergiant star in the constellation of orion, known for its brightness and distinctive reddish hue.  
Head Entity: betelgeuse  
Tail Entity: orion  

Relation: constellation  
Context: the star deneb is located in the constellation of cygnus and is one of the brightest stars in the night sky.  
Head Entity: deneb  
Tail Entity: cygnus  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: services to the island run from san juan 's luis muñoz marín international airport , ceiba airport or isla grande airport ( 20- to 30-minute flight ) .
Head Entity: isla grande airport
Tail Entity: san juan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The city of Barcelona is well-connected to various destinations through the El Prat Airport, which offers flights to numerous European cities.  
Head Entity: El Prat Airport  
Tail Entity: Barcelona  

Relation: place served by transport hub  
Context: The central train station in Zurich provides access to multiple Swiss cities and international destinations, making it a key transport hub in the region.  
Head Entity: Zurich central train station  
Tail Entity: Zurich  
MemoryTrain:  epoch 15, batch     0 | loss: 2.6140834MemoryTrain:  epoch 15, batch     1 | loss: 2.6889421MemoryTrain:  epoch 15, batch     2 | loss: 2.8106802MemoryTrain:  epoch 15, batch     3 | loss: 3.8982703MemoryTrain:  epoch 15, batch     4 | loss: 6.0113882MemoryTrain:  epoch 15, batch     5 | loss: 7.6713495MemoryTrain:  epoch 15, batch     6 | loss: 3.3486215MemoryTrain:  epoch 15, batch     7 | loss: 2.3941946MemoryTrain:  epoch 15, batch     8 | loss: 2.8072541MemoryTrain:  epoch  5, batch     9 | loss: 13.6727042MemoryTrain:  epoch 15, batch     0 | loss: 2.6765859MemoryTrain:  epoch 15, batch     1 | loss: 2.2416293MemoryTrain:  epoch 15, batch     2 | loss: 2.0087143MemoryTrain:  epoch 15, batch     3 | loss: 2.9269264MemoryTrain:  epoch 15, batch     4 | loss: 2.6289514MemoryTrain:  epoch 15, batch     5 | loss: 2.7720763MemoryTrain:  epoch 15, batch     6 | loss: 5.9399422MemoryTrain:  epoch 15, batch     7 | loss: 1.8182770MemoryTrain:  epoch 15, batch     8 | loss: 2.3480582MemoryTrain:  epoch  5, batch     9 | loss: 8.6681768MemoryTrain:  epoch 15, batch     0 | loss: 2.1993637MemoryTrain:  epoch 15, batch     1 | loss: 2.7634099MemoryTrain:  epoch 15, batch     2 | loss: 4.2263099MemoryTrain:  epoch 15, batch     3 | loss: 4.3589648MemoryTrain:  epoch 15, batch     4 | loss: 2.3560667MemoryTrain:  epoch 15, batch     5 | loss: 2.5715631MemoryTrain:  epoch 15, batch     6 | loss: 2.2579501MemoryTrain:  epoch 15, batch     7 | loss: 1.6144392MemoryTrain:  epoch 15, batch     8 | loss: 2.4072141MemoryTrain:  epoch  5, batch     9 | loss: 8.3199620MemoryTrain:  epoch 15, batch     0 | loss: 1.7271781MemoryTrain:  epoch 15, batch     1 | loss: 2.5845261MemoryTrain:  epoch 15, batch     2 | loss: 1.5696460MemoryTrain:  epoch 15, batch     3 | loss: 3.0172347MemoryTrain:  epoch 15, batch     4 | loss: 2.4831579MemoryTrain:  epoch 15, batch     5 | loss: 1.9144778MemoryTrain:  epoch 15, batch     6 | loss: 3.6804144MemoryTrain:  epoch 15, batch     7 | loss: 2.0850221MemoryTrain:  epoch 15, batch     8 | loss: 2.0533048MemoryTrain:  epoch  5, batch     9 | loss: 8.8902705MemoryTrain:  epoch 15, batch     0 | loss: 5.0449573MemoryTrain:  epoch 15, batch     1 | loss: 2.0878360MemoryTrain:  epoch 15, batch     2 | loss: 2.0727862MemoryTrain:  epoch 15, batch     3 | loss: 4.4293949MemoryTrain:  epoch 15, batch     4 | loss: 1.9304063MemoryTrain:  epoch 15, batch     5 | loss: 1.9134519MemoryTrain:  epoch 15, batch     6 | loss: 1.9108148MemoryTrain:  epoch 15, batch     7 | loss: 1.9983843MemoryTrain:  epoch 15, batch     8 | loss: 1.6699131MemoryTrain:  epoch  5, batch     9 | loss: 22.4764488MemoryTrain:  epoch 15, batch     0 | loss: 1.5709116MemoryTrain:  epoch 15, batch     1 | loss: 1.8179247MemoryTrain:  epoch 15, batch     2 | loss: 4.0858102MemoryTrain:  epoch 15, batch     3 | loss: 2.5274222MemoryTrain:  epoch 15, batch     4 | loss: 4.5355746MemoryTrain:  epoch 15, batch     5 | loss: 1.5459057MemoryTrain:  epoch 15, batch     6 | loss: 1.9951487MemoryTrain:  epoch 15, batch     7 | loss: 1.6626209MemoryTrain:  epoch 15, batch     8 | loss: 1.8853501MemoryTrain:  epoch  5, batch     9 | loss: 8.0315967MemoryTrain:  epoch 15, batch     0 | loss: 2.2759754MemoryTrain:  epoch 15, batch     1 | loss: 1.9951469MemoryTrain:  epoch 15, batch     2 | loss: 1.5152801MemoryTrain:  epoch 15, batch     3 | loss: 1.6256439MemoryTrain:  epoch 15, batch     4 | loss: 1.8426713MemoryTrain:  epoch 15, batch     5 | loss: 1.8458809MemoryTrain:  epoch 15, batch     6 | loss: 2.1305082MemoryTrain:  epoch 15, batch     7 | loss: 1.6182896MemoryTrain:  epoch 15, batch     8 | loss: 3.8880656MemoryTrain:  epoch  5, batch     9 | loss: 7.7764795MemoryTrain:  epoch 15, batch     0 | loss: 2.7079730MemoryTrain:  epoch 15, batch     1 | loss: 1.9961748MemoryTrain:  epoch 15, batch     2 | loss: 2.3911627MemoryTrain:  epoch 15, batch     3 | loss: 1.4927379MemoryTrain:  epoch 15, batch     4 | loss: 2.4568408MemoryTrain:  epoch 15, batch     5 | loss: 1.4587379MemoryTrain:  epoch 15, batch     6 | loss: 1.7535097MemoryTrain:  epoch 15, batch     7 | loss: 1.7774624MemoryTrain:  epoch 15, batch     8 | loss: 2.6872039MemoryTrain:  epoch  5, batch     9 | loss: 7.6779010MemoryTrain:  epoch 15, batch     0 | loss: 1.3956785MemoryTrain:  epoch 15, batch     1 | loss: 3.8047502MemoryTrain:  epoch 15, batch     2 | loss: 1.7000908MemoryTrain:  epoch 15, batch     3 | loss: 1.7926366MemoryTrain:  epoch 15, batch     4 | loss: 1.8488460MemoryTrain:  epoch 15, batch     5 | loss: 3.5267873MemoryTrain:  epoch 15, batch     6 | loss: 1.8632841MemoryTrain:  epoch 15, batch     7 | loss: 3.7539917MemoryTrain:  epoch 15, batch     8 | loss: 4.4514019MemoryTrain:  epoch  5, batch     9 | loss: 8.1868776MemoryTrain:  epoch 15, batch     0 | loss: 1.6657516MemoryTrain:  epoch 15, batch     1 | loss: 3.6565180MemoryTrain:  epoch 15, batch     2 | loss: 3.4715580MemoryTrain:  epoch 15, batch     3 | loss: 1.6216482MemoryTrain:  epoch 15, batch     4 | loss: 1.8113316MemoryTrain:  epoch 15, batch     5 | loss: 1.5294672MemoryTrain:  epoch 15, batch     6 | loss: 1.6958383MemoryTrain:  epoch 15, batch     7 | loss: 1.5810760MemoryTrain:  epoch 15, batch     8 | loss: 2.2899636MemoryTrain:  epoch  5, batch     9 | loss: 8.1743891
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 95.62%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 95.45%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 94.79%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 94.23%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 92.86%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 89.06%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 87.13%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 86.11%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 84.87%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 83.75%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 83.24%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 82.07%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:   24 | acc: 43.75%,  total acc: 80.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.49%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 82.18%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 83.41%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.48%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 84.57%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 84.66%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 84.55%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 84.46%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.78%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.52%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 85.86%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 86.19%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 86.36%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 86.67%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 86.96%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 87.23%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 87.76%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 88.00%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 87.87%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 88.09%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 88.19%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 88.07%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 88.17%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 88.27%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 88.36%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 88.35%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 88.73%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 88.61%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 88.00%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 75.62%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 74.43%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 73.44%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 73.56%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 75.83%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 76.95%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 78.31%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 79.61%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 79.38%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 79.46%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 79.26%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 79.08%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 79.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.05%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 80.79%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.47%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 81.90%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.06%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 84.56%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 85.24%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 85.64%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 86.02%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.38%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.04%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 87.20%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.35%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 87.64%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 87.36%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 86.84%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 86.59%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 86.61%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 86.00%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 85.91%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 86.08%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 85.76%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 85.23%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 85.27%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 84.98%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 84.38%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 84.00%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 83.85%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 83.71%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 83.67%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 83.33%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 82.81%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 82.12%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 81.91%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 81.25%   [EVAL] batch:   67 | acc: 62.50%,  total acc: 80.97%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 80.80%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 80.98%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 81.07%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 81.08%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 81.16%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 81.42%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 81.67%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 81.09%   [EVAL] batch:   76 | acc: 18.75%,  total acc: 80.28%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 79.57%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 79.03%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 78.67%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 78.01%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 78.05%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 78.31%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 78.57%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 79.00%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 79.05%   [EVAL] batch:   88 | acc: 6.25%,  total acc: 78.23%   [EVAL] batch:   89 | acc: 18.75%,  total acc: 77.57%   [EVAL] batch:   90 | acc: 31.25%,  total acc: 77.06%   [EVAL] batch:   91 | acc: 43.75%,  total acc: 76.70%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 75.94%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 75.47%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 75.66%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 75.85%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 76.03%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 76.21%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 76.39%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 76.62%   [EVAL] batch:  100 | acc: 6.25%,  total acc: 75.93%   [EVAL] batch:  101 | acc: 0.00%,  total acc: 75.18%   [EVAL] batch:  102 | acc: 6.25%,  total acc: 74.51%   [EVAL] batch:  103 | acc: 6.25%,  total acc: 73.86%   [EVAL] batch:  104 | acc: 6.25%,  total acc: 73.21%   [EVAL] batch:  105 | acc: 18.75%,  total acc: 72.70%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 72.49%   [EVAL] batch:  107 | acc: 56.25%,  total acc: 72.34%   [EVAL] batch:  108 | acc: 68.75%,  total acc: 72.31%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 72.05%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 71.90%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 71.82%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 71.74%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 71.55%   [EVAL] batch:  114 | acc: 68.75%,  total acc: 71.52%   [EVAL] batch:  115 | acc: 68.75%,  total acc: 71.50%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 71.53%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 71.45%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 71.43%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 71.61%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 71.80%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 71.93%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 72.00%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 72.08%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 72.30%   [EVAL] batch:  125 | acc: 43.75%,  total acc: 72.07%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 71.85%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 71.53%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 71.32%   [EVAL] batch:  129 | acc: 25.00%,  total acc: 70.96%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 70.61%   [EVAL] batch:  131 | acc: 50.00%,  total acc: 70.45%   [EVAL] batch:  132 | acc: 75.00%,  total acc: 70.49%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 70.52%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 70.65%   [EVAL] batch:  135 | acc: 75.00%,  total acc: 70.68%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 70.80%   [EVAL] batch:  137 | acc: 56.25%,  total acc: 70.70%   [EVAL] batch:  138 | acc: 43.75%,  total acc: 70.50%   [EVAL] batch:  139 | acc: 50.00%,  total acc: 70.36%   [EVAL] batch:  140 | acc: 62.50%,  total acc: 70.30%   [EVAL] batch:  141 | acc: 62.50%,  total acc: 70.25%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 70.28%   [EVAL] batch:  143 | acc: 50.00%,  total acc: 70.14%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 70.34%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 70.55%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 70.71%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 70.90%   [EVAL] batch:  148 | acc: 93.75%,  total acc: 71.06%   [EVAL] batch:  149 | acc: 87.50%,  total acc: 71.17%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 71.32%   [EVAL] batch:  151 | acc: 87.50%,  total acc: 71.42%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 71.45%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 71.55%   [EVAL] batch:  154 | acc: 81.25%,  total acc: 71.61%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 71.71%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 71.86%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 72.03%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 72.21%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 72.34%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 72.48%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 72.61%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 72.55%   [EVAL] batch:  163 | acc: 31.25%,  total acc: 72.29%   [EVAL] batch:  164 | acc: 25.00%,  total acc: 72.01%   [EVAL] batch:  165 | acc: 31.25%,  total acc: 71.76%   [EVAL] batch:  166 | acc: 25.00%,  total acc: 71.48%   [EVAL] batch:  167 | acc: 12.50%,  total acc: 71.13%   [EVAL] batch:  168 | acc: 37.50%,  total acc: 70.93%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 71.10%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 71.24%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 71.40%   [EVAL] batch:  172 | acc: 100.00%,  total acc: 71.57%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 71.70%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 71.79%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 71.70%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 71.61%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 71.52%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 71.47%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 71.46%   [EVAL] batch:  180 | acc: 31.25%,  total acc: 71.24%   [EVAL] batch:  181 | acc: 56.25%,  total acc: 71.15%   [EVAL] batch:  182 | acc: 62.50%,  total acc: 71.11%   [EVAL] batch:  183 | acc: 56.25%,  total acc: 71.03%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 71.01%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 71.03%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 71.12%   [EVAL] batch:  187 | acc: 25.00%,  total acc: 70.88%   [EVAL] batch:  188 | acc: 12.50%,  total acc: 70.57%   [EVAL] batch:  189 | acc: 12.50%,  total acc: 70.26%   [EVAL] batch:  190 | acc: 12.50%,  total acc: 69.96%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 69.69%   [EVAL] batch:  192 | acc: 31.25%,  total acc: 69.49%   [EVAL] batch:  193 | acc: 31.25%,  total acc: 69.30%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 69.33%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 69.32%   [EVAL] batch:  196 | acc: 50.00%,  total acc: 69.23%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 69.32%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 69.35%   [EVAL] batch:  199 | acc: 68.75%,  total acc: 69.34%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 69.40%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 69.46%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 69.49%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 69.42%   [EVAL] batch:  204 | acc: 87.50%,  total acc: 69.51%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 69.63%   [EVAL] batch:  206 | acc: 68.75%,  total acc: 69.63%   [EVAL] batch:  207 | acc: 31.25%,  total acc: 69.44%   [EVAL] batch:  208 | acc: 43.75%,  total acc: 69.32%   [EVAL] batch:  209 | acc: 43.75%,  total acc: 69.20%   [EVAL] batch:  210 | acc: 43.75%,  total acc: 69.08%   [EVAL] batch:  211 | acc: 37.50%,  total acc: 68.93%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 68.93%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 69.04%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 69.19%   [EVAL] batch:  215 | acc: 81.25%,  total acc: 69.24%   [EVAL] batch:  216 | acc: 93.75%,  total acc: 69.35%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 69.47%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 69.61%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 69.74%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 69.88%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 70.02%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 70.15%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 70.28%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 70.42%   [EVAL] batch:  225 | acc: 93.75%,  total acc: 70.52%   [EVAL] batch:  226 | acc: 93.75%,  total acc: 70.62%   [EVAL] batch:  227 | acc: 81.25%,  total acc: 70.67%   [EVAL] batch:  228 | acc: 81.25%,  total acc: 70.72%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 70.84%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 70.94%   [EVAL] batch:  231 | acc: 50.00%,  total acc: 70.85%   [EVAL] batch:  232 | acc: 37.50%,  total acc: 70.71%   [EVAL] batch:  233 | acc: 37.50%,  total acc: 70.57%   [EVAL] batch:  234 | acc: 25.00%,  total acc: 70.37%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:  236 | acc: 25.00%,  total acc: 70.12%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 70.17%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 70.24%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 70.36%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 70.44%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 70.53%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 70.63%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 70.70%   [EVAL] batch:  244 | acc: 25.00%,  total acc: 70.51%   [EVAL] batch:  245 | acc: 37.50%,  total acc: 70.38%   [EVAL] batch:  246 | acc: 25.00%,  total acc: 70.19%   [EVAL] batch:  247 | acc: 43.75%,  total acc: 70.09%   [EVAL] batch:  248 | acc: 18.75%,  total acc: 69.88%   [EVAL] batch:  249 | acc: 31.25%,  total acc: 69.73%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 69.82%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 69.92%   [EVAL] batch:  252 | acc: 100.00%,  total acc: 70.03%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 70.10%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 70.20%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:  256 | acc: 93.75%,  total acc: 70.40%   [EVAL] batch:  257 | acc: 100.00%,  total acc: 70.52%   [EVAL] batch:  258 | acc: 100.00%,  total acc: 70.63%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 70.72%   [EVAL] batch:  260 | acc: 93.75%,  total acc: 70.81%   [EVAL] batch:  261 | acc: 87.50%,  total acc: 70.87%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 70.94%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 70.95%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 70.94%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 70.89%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 70.83%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:  268 | acc: 62.50%,  total acc: 70.79%   [EVAL] batch:  269 | acc: 62.50%,  total acc: 70.76%   [EVAL] batch:  270 | acc: 75.00%,  total acc: 70.78%   [EVAL] batch:  271 | acc: 81.25%,  total acc: 70.82%   [EVAL] batch:  272 | acc: 56.25%,  total acc: 70.76%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 70.83%   [EVAL] batch:  274 | acc: 43.75%,  total acc: 70.73%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 70.94%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 71.04%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 71.25%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 71.35%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 71.41%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 71.47%   [EVAL] batch:  283 | acc: 75.00%,  total acc: 71.48%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 71.58%   [EVAL] batch:  285 | acc: 75.00%,  total acc: 71.59%   [EVAL] batch:  286 | acc: 81.25%,  total acc: 71.62%   [EVAL] batch:  287 | acc: 81.25%,  total acc: 71.66%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 71.76%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 71.85%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 71.95%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 72.05%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 72.14%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 72.22%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 72.31%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 72.40%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 72.59%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 72.68%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 72.77%   [EVAL] batch:  300 | acc: 81.25%,  total acc: 72.80%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 72.87%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 72.94%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 73.01%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 73.03%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 73.10%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 73.17%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 73.23%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 73.28%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 73.37%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 73.45%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 73.48%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 73.40%   
cur_acc:  ['0.9454', '0.7867', '0.7738', '0.6647', '0.8800']
his_acc:  ['0.9454', '0.8590', '0.8022', '0.7362', '0.7340']
CurrentTrain: epoch 15, batch     0 | loss: 10.6982329CurrentTrain: epoch 15, batch     1 | loss: 13.3812225CurrentTrain: epoch 15, batch     2 | loss: 22.1132748CurrentTrain: epoch  1, batch     3 | loss: 9.0545179CurrentTrain: epoch 15, batch     0 | loss: 20.0616641CurrentTrain: epoch 15, batch     1 | loss: 16.6406320CurrentTrain: epoch 15, batch     2 | loss: 13.8199651CurrentTrain: epoch  1, batch     3 | loss: 9.5015349CurrentTrain: epoch 15, batch     0 | loss: 9.4949970CurrentTrain: epoch 15, batch     1 | loss: 9.4816396CurrentTrain: epoch 15, batch     2 | loss: 7.4306081CurrentTrain: epoch  1, batch     3 | loss: 11.0763436CurrentTrain: epoch 15, batch     0 | loss: 16.8959765CurrentTrain: epoch 15, batch     1 | loss: 7.7176888CurrentTrain: epoch 15, batch     2 | loss: 24.1843912CurrentTrain: epoch  1, batch     3 | loss: 6.5915610CurrentTrain: epoch 15, batch     0 | loss: 8.7765789CurrentTrain: epoch 15, batch     1 | loss: 8.7064600CurrentTrain: epoch 15, batch     2 | loss: 29.7391358CurrentTrain: epoch  1, batch     3 | loss: 6.4847790CurrentTrain: epoch 15, batch     0 | loss: 8.5915262CurrentTrain: epoch 15, batch     1 | loss: 9.0769614CurrentTrain: epoch 15, batch     2 | loss: 12.0067147CurrentTrain: epoch  1, batch     3 | loss: 6.9770140CurrentTrain: epoch 15, batch     0 | loss: 13.7707628CurrentTrain: epoch 15, batch     1 | loss: 13.6281391CurrentTrain: epoch 15, batch     2 | loss: 7.0582543CurrentTrain: epoch  1, batch     3 | loss: 7.3300588CurrentTrain: epoch 15, batch     0 | loss: 10.0135183CurrentTrain: epoch 15, batch     1 | loss: 13.5551324CurrentTrain: epoch 15, batch     2 | loss: 10.0738638CurrentTrain: epoch  1, batch     3 | loss: 6.6297510CurrentTrain: epoch 15, batch     0 | loss: 12.7706305CurrentTrain: epoch 15, batch     1 | loss: 5.9516235CurrentTrain: epoch 15, batch     2 | loss: 6.9774647CurrentTrain: epoch  1, batch     3 | loss: 7.2058191CurrentTrain: epoch 15, batch     0 | loss: 7.3768206CurrentTrain: epoch 15, batch     1 | loss: 14.7516353CurrentTrain: epoch 15, batch     2 | loss: 14.2183567CurrentTrain: epoch  1, batch     3 | loss: 5.8821699
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: " clinton crazies " is a term in american politics of the 1990s and later that refers to intense criticism of united states president bill clinton and his wife hillary clinton .
Head Entity: bill clinton
Tail Entity: hillary clinton
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: "In a lavish ceremony, actor ben affleck married singer jennifer lopez, marking their reunion after nearly two decades."  
Head Entity: ben affleck  
Tail Entity: jennifer lopez  

Relation: spouse  
Context: "After years of dating, tennis star serena williams and reddit co-founder alexis ohanian tied the knot in a beautiful ceremony."  
Head Entity: serena williams  
Tail Entity: alexis ohanian  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: " i 'm a cuckoo " was belle & sebastian 's second single from " dear catastrophe waitress " , released on rough trade records in 2004 .
Head Entity: belle & sebastian
Tail Entity: rough trade records
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: The album "Random Access Memories" by Daft Punk was released under the Columbia Records label in 2013.  
Head Entity: Daft Punk  
Tail Entity: Columbia Records  

Relation: record label  
Context: Taylor Swift's latest album "Evermore" was produced by her own label, Republic Records, and released in 2020.  
Head Entity: Taylor Swift  
Tail Entity: Republic Records  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: distributed by paramount pictures , " the italian job " was theatrically released in the united states on may 30 , 2003 , and grossed over $ 176 million worldwide .
Head Entity: the italian job
Tail Entity: paramount pictures
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: the film was distributed by universal pictures and received critical acclaim upon its release in 2015.  
Head Entity: the film  
Tail Entity: universal pictures  

Relation: distributor  
Context: the album was distributed by sony music and quickly climbed the charts, becoming a bestseller.  
Head Entity: the album  
Tail Entity: sony music  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: in 1796 carondelet commissioned pintado to conduct a survey of lands granted to ambrosio longue - épée in iberville county on the west bank of the mississippi river opposite baton rouge .
Head Entity: baton rouge
Tail Entity: mississippi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: the city of new orleans is situated on the banks of the mississippi river, making it a vibrant hub for trade and culture.  
Head Entity: new orleans  
Tail Entity: mississippi river  

Relation: located in or next to body of water  
Context: the picturesque village of lake placid is nestled beside the serene waters of lake placid, attracting tourists year-round.  
Head Entity: lake placid  
Tail Entity: lake placid  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: cbs and viacom split again in 2005 , but both are still owned by national amusements .
Head Entity: national amusements
Tail Entity: viacom
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Sample 1:  
Relation: subsidiary  
Context: In 2018, Facebook acquired WhatsApp, making it a subsidiary of the social media giant.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Sample 2:  
Relation: subsidiary  
Context: Toyota's luxury vehicle division, Lexus, operates as a subsidiary under the main company.  
Head Entity: Toyota  
Tail Entity: Lexus  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: he challenged seven other swimmers in heat four including ukraine 's rostyslav svanidze , a top 16 finalist in atlanta four years earlier .
Head Entity: rostyslav svanidze
Tail Entity: ukraine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: After years of hard work and dedication, Maria Gonzalez proudly received her citizenship certificate, officially recognizing her as a citizen of Spain.  
Head Entity: Maria Gonzalez  
Tail Entity: Spain  

Relation: country of citizenship  
Context: During the international conference, it was noted that many athletes, including Usain Bolt, represent Jamaica as their country of citizenship.  
Head Entity: Usain Bolt  
Tail Entity: Jamaica  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: nina cites " duffy , muse , and other pop and alternative artists " as her major influences .
Head Entity: duffy
Tail Entity: pop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the film "inception" is often categorized as a science fiction thriller that explores complex themes of dreams and reality.  
Head Entity: inception  
Tail Entity: science fiction  

Relation: genre  
Context: beethoven is renowned for his contributions to classical music, particularly in the realm of symphonies and sonatas.  
Head Entity: beethoven  
Tail Entity: classical music  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: san lázaro is 660 km far from asunción and is located at the exact site of the confluence of the apa river with paraguay river , bordering brazil - chaco .
Head Entity: apa river
Tail Entity: paraguay river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: the mississippi river flows into the gulf of mexico, creating a rich delta ecosystem that supports diverse wildlife and plant species.  
Head Entity: mississippi river  
Tail Entity: gulf of mexico  

Relation: mouth of the watercourse  
Context: the rhine river, known for its picturesque landscapes, empties into the north sea, providing a vital shipping route for trade in europe.  
Head Entity: rhine river  
Tail Entity: north sea  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: bradley halliday ( born 10 july 1995 ) is an english professional footballer who plays as a defender or a midfielder for league two club cambridge united .
Head Entity: bradley halliday
Tail Entity: defender
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: alex morgan ( born 2 july 1989 ) is an american soccer player who plays as a forward for the national team and club orlando pride in the national women's soccer league.  
Head Entity: alex morgan  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: lebron james ( born 30 december 1984 ) is an american professional basketball player who plays as a small forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: small forward  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: the stade louis ii in monaco was the venue for the uefa super cup every year since 1998 .
Head Entity: 1998
Tail Entity: uefa super cup
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2020 NBA season was significantly impacted by the COVID-19 pandemic, leading to a unique playoff format.  
Head Entity: 2020  
Tail Entity: NBA  

Relation: sports season of league or competition  
Context: The 2019 Rugby World Cup took place in Japan, showcasing teams from around the globe.  
Head Entity: 2019  
Tail Entity: Rugby World Cup  
MemoryTrain:  epoch 15, batch     0 | loss: 2.6124564MemoryTrain:  epoch 15, batch     1 | loss: 3.2409200MemoryTrain:  epoch 15, batch     2 | loss: 2.5631634MemoryTrain:  epoch 15, batch     3 | loss: 2.2857842MemoryTrain:  epoch 15, batch     4 | loss: 4.9187127MemoryTrain:  epoch 15, batch     5 | loss: 3.6028436MemoryTrain:  epoch 15, batch     6 | loss: 2.6467671MemoryTrain:  epoch 15, batch     7 | loss: 3.1928395MemoryTrain:  epoch 15, batch     8 | loss: 3.0133761MemoryTrain:  epoch 15, batch     9 | loss: 2.4743288MemoryTrain:  epoch 15, batch    10 | loss: 2.4940545MemoryTrain:  epoch  3, batch    11 | loss: 10.7670782MemoryTrain:  epoch 15, batch     0 | loss: 2.8957119MemoryTrain:  epoch 15, batch     1 | loss: 2.8284352MemoryTrain:  epoch 15, batch     2 | loss: 2.5523034MemoryTrain:  epoch 15, batch     3 | loss: 2.4106278MemoryTrain:  epoch 15, batch     4 | loss: 4.0602237MemoryTrain:  epoch 15, batch     5 | loss: 4.6364640MemoryTrain:  epoch 15, batch     6 | loss: 1.9755934MemoryTrain:  epoch 15, batch     7 | loss: 1.8313612MemoryTrain:  epoch 15, batch     8 | loss: 2.2233902MemoryTrain:  epoch 15, batch     9 | loss: 4.5923808MemoryTrain:  epoch 15, batch    10 | loss: 1.9651765MemoryTrain:  epoch  3, batch    11 | loss: 10.8805854MemoryTrain:  epoch 15, batch     0 | loss: 4.1408738MemoryTrain:  epoch 15, batch     1 | loss: 2.1273970MemoryTrain:  epoch 15, batch     2 | loss: 1.9887921MemoryTrain:  epoch 15, batch     3 | loss: 2.5235670MemoryTrain:  epoch 15, batch     4 | loss: 1.6683074MemoryTrain:  epoch 15, batch     5 | loss: 2.2234576MemoryTrain:  epoch 15, batch     6 | loss: 1.5422964MemoryTrain:  epoch 15, batch     7 | loss: 1.7980868MemoryTrain:  epoch 15, batch     8 | loss: 3.9147296MemoryTrain:  epoch 15, batch     9 | loss: 2.5926436MemoryTrain:  epoch 15, batch    10 | loss: 1.6503478MemoryTrain:  epoch  3, batch    11 | loss: 10.0638147MemoryTrain:  epoch 15, batch     0 | loss: 4.6033307MemoryTrain:  epoch 15, batch     1 | loss: 1.7489937MemoryTrain:  epoch 15, batch     2 | loss: 1.8488838MemoryTrain:  epoch 15, batch     3 | loss: 4.0203445MemoryTrain:  epoch 15, batch     4 | loss: 1.5486210MemoryTrain:  epoch 15, batch     5 | loss: 4.1253584MemoryTrain:  epoch 15, batch     6 | loss: 1.8485122MemoryTrain:  epoch 15, batch     7 | loss: 2.1366628MemoryTrain:  epoch 15, batch     8 | loss: 2.0685705MemoryTrain:  epoch 15, batch     9 | loss: 2.6214217MemoryTrain:  epoch 15, batch    10 | loss: 1.3977860MemoryTrain:  epoch  3, batch    11 | loss: 10.0692517MemoryTrain:  epoch 15, batch     0 | loss: 3.8360954MemoryTrain:  epoch 15, batch     1 | loss: 1.8144418MemoryTrain:  epoch 15, batch     2 | loss: 1.7623179MemoryTrain:  epoch 15, batch     3 | loss: 1.7144363MemoryTrain:  epoch 15, batch     4 | loss: 1.7119560MemoryTrain:  epoch 15, batch     5 | loss: 2.1621929MemoryTrain:  epoch 15, batch     6 | loss: 3.9245745MemoryTrain:  epoch 15, batch     7 | loss: 1.7169082MemoryTrain:  epoch 15, batch     8 | loss: 2.6954240MemoryTrain:  epoch 15, batch     9 | loss: 2.4372121MemoryTrain:  epoch 15, batch    10 | loss: 3.7139063MemoryTrain:  epoch  3, batch    11 | loss: 10.1087372MemoryTrain:  epoch 15, batch     0 | loss: 2.0224301MemoryTrain:  epoch 15, batch     1 | loss: 1.5953824MemoryTrain:  epoch 15, batch     2 | loss: 1.5955243MemoryTrain:  epoch 15, batch     3 | loss: 2.5403906MemoryTrain:  epoch 15, batch     4 | loss: 2.0554722MemoryTrain:  epoch 15, batch     5 | loss: 2.3858455MemoryTrain:  epoch 15, batch     6 | loss: 1.7747552MemoryTrain:  epoch 15, batch     7 | loss: 2.5112232MemoryTrain:  epoch 15, batch     8 | loss: 1.3780110MemoryTrain:  epoch 15, batch     9 | loss: 2.5218560MemoryTrain:  epoch 15, batch    10 | loss: 9.0136347MemoryTrain:  epoch  3, batch    11 | loss: 10.5081154MemoryTrain:  epoch 15, batch     0 | loss: 1.5358639MemoryTrain:  epoch 15, batch     1 | loss: 1.5545237MemoryTrain:  epoch 15, batch     2 | loss: 1.4725013MemoryTrain:  epoch 15, batch     3 | loss: 1.5055585MemoryTrain:  epoch 15, batch     4 | loss: 1.4129124MemoryTrain:  epoch 15, batch     5 | loss: 1.7630161MemoryTrain:  epoch 15, batch     6 | loss: 2.5945766MemoryTrain:  epoch 15, batch     7 | loss: 1.4767796MemoryTrain:  epoch 15, batch     8 | loss: 1.3857871MemoryTrain:  epoch 15, batch     9 | loss: 2.1823891MemoryTrain:  epoch 15, batch    10 | loss: 1.6140940MemoryTrain:  epoch  3, batch    11 | loss: 10.6805540MemoryTrain:  epoch 15, batch     0 | loss: 1.5699071MemoryTrain:  epoch 15, batch     1 | loss: 2.6250274MemoryTrain:  epoch 15, batch     2 | loss: 1.5398958MemoryTrain:  epoch 15, batch     3 | loss: 1.4234480MemoryTrain:  epoch 15, batch     4 | loss: 1.6319084MemoryTrain:  epoch 15, batch     5 | loss: 2.2379968MemoryTrain:  epoch 15, batch     6 | loss: 1.7649012MemoryTrain:  epoch 15, batch     7 | loss: 1.7066078MemoryTrain:  epoch 15, batch     8 | loss: 1.6135440MemoryTrain:  epoch 15, batch     9 | loss: 1.3441566MemoryTrain:  epoch 15, batch    10 | loss: 1.5737080MemoryTrain:  epoch  3, batch    11 | loss: 9.8356680MemoryTrain:  epoch 15, batch     0 | loss: 1.5801831MemoryTrain:  epoch 15, batch     1 | loss: 1.5401053MemoryTrain:  epoch 15, batch     2 | loss: 1.4436814MemoryTrain:  epoch 15, batch     3 | loss: 1.8337187MemoryTrain:  epoch 15, batch     4 | loss: 3.6708431MemoryTrain:  epoch 15, batch     5 | loss: 1.3836517MemoryTrain:  epoch 15, batch     6 | loss: 1.4932573MemoryTrain:  epoch 15, batch     7 | loss: 1.5430518MemoryTrain:  epoch 15, batch     8 | loss: 1.7594026MemoryTrain:  epoch 15, batch     9 | loss: 1.7345074MemoryTrain:  epoch 15, batch    10 | loss: 1.6717359MemoryTrain:  epoch  3, batch    11 | loss: 10.5469265MemoryTrain:  epoch 15, batch     0 | loss: 1.6082593MemoryTrain:  epoch 15, batch     1 | loss: 4.1024287MemoryTrain:  epoch 15, batch     2 | loss: 1.4019024MemoryTrain:  epoch 15, batch     3 | loss: 1.4197501MemoryTrain:  epoch 15, batch     4 | loss: 1.7368114MemoryTrain:  epoch 15, batch     5 | loss: 1.3623197MemoryTrain:  epoch 15, batch     6 | loss: 1.5653714MemoryTrain:  epoch 15, batch     7 | loss: 1.4309903MemoryTrain:  epoch 15, batch     8 | loss: 2.1706074MemoryTrain:  epoch 15, batch     9 | loss: 3.6991313MemoryTrain:  epoch 15, batch    10 | loss: 2.2598051MemoryTrain:  epoch  3, batch    11 | loss: 9.9634869
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 48.75%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 55.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 60.94%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 64.58%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 66.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 69.89%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 72.40%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 73.56%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 72.32%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 71.67%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 72.66%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 72.79%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 72.57%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 72.37%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 70.00%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 68.15%   [EVAL] batch:   21 | acc: 18.75%,  total acc: 65.91%   [EVAL] batch:   22 | acc: 0.00%,  total acc: 63.04%   [EVAL] batch:   23 | acc: 25.00%,  total acc: 61.46%   [EVAL] batch:   24 | acc: 18.75%,  total acc: 59.75%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 60.82%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 61.57%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 63.58%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 63.75%   [EVAL] batch:   30 | acc: 56.25%,  total acc: 63.51%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 63.48%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 63.45%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 63.42%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 63.39%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 63.72%   [EVAL] batch:   36 | acc: 50.00%,  total acc: 63.34%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 63.49%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 63.46%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 64.22%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 64.79%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 65.18%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 65.55%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 65.48%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 64.58%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 63.72%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 62.90%   [EVAL] batch:   47 | acc: 31.25%,  total acc: 62.24%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 61.61%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 61.38%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 62.13%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 62.86%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 63.56%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 64.24%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 64.89%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 65.51%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 65.90%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 66.06%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 66.31%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 66.77%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 67.01%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 67.14%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 66.77%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 68.18%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 66.15%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 65.87%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 67.86%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 69.17%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 70.70%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 72.43%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 73.61%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 74.67%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 74.38%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 74.70%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 74.72%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 74.46%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 74.48%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 74.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.72%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 76.62%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.46%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 78.23%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.64%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 80.08%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 80.49%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 79.96%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 79.82%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 80.03%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 80.24%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 80.59%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.09%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 81.56%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 82.01%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 82.56%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 82.92%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 82.88%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 82.58%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 82.29%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 82.53%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 82.00%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 81.99%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 82.09%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 82.31%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 82.18%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 81.70%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 81.81%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 81.36%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 80.71%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 80.19%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 80.10%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 79.92%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 79.84%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 79.66%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 79.20%   [EVAL] batch:   64 | acc: 43.75%,  total acc: 78.65%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 78.50%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 77.99%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 77.67%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 77.45%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 77.59%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 77.73%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 77.78%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 77.91%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 78.21%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 78.42%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 77.80%   [EVAL] batch:   76 | acc: 18.75%,  total acc: 77.03%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 76.36%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 75.79%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 75.55%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 74.92%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 75.30%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 75.60%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 75.81%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 76.09%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 76.29%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 76.35%   [EVAL] batch:   88 | acc: 12.50%,  total acc: 75.63%   [EVAL] batch:   89 | acc: 25.00%,  total acc: 75.07%   [EVAL] batch:   90 | acc: 25.00%,  total acc: 74.52%   [EVAL] batch:   91 | acc: 43.75%,  total acc: 74.18%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 73.45%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 73.07%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 73.29%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 73.50%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 73.71%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 73.85%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 74.05%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 74.31%   [EVAL] batch:  100 | acc: 0.00%,  total acc: 73.58%   [EVAL] batch:  101 | acc: 0.00%,  total acc: 72.86%   [EVAL] batch:  102 | acc: 0.00%,  total acc: 72.15%   [EVAL] batch:  103 | acc: 6.25%,  total acc: 71.51%   [EVAL] batch:  104 | acc: 0.00%,  total acc: 70.83%   [EVAL] batch:  105 | acc: 6.25%,  total acc: 70.22%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 69.92%   [EVAL] batch:  107 | acc: 50.00%,  total acc: 69.73%   [EVAL] batch:  108 | acc: 68.75%,  total acc: 69.72%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 69.38%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 69.26%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 69.08%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 69.14%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 68.97%   [EVAL] batch:  114 | acc: 75.00%,  total acc: 69.02%   [EVAL] batch:  115 | acc: 62.50%,  total acc: 68.97%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 69.02%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 68.96%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 69.01%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 69.27%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 69.47%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 69.67%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 69.77%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 69.86%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 70.10%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 69.79%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 69.59%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 69.24%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 69.04%   [EVAL] batch:  129 | acc: 18.75%,  total acc: 68.65%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 68.32%   [EVAL] batch:  131 | acc: 56.25%,  total acc: 68.23%   [EVAL] batch:  132 | acc: 75.00%,  total acc: 68.28%   [EVAL] batch:  133 | acc: 68.75%,  total acc: 68.28%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 68.43%   [EVAL] batch:  135 | acc: 75.00%,  total acc: 68.47%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 68.57%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 68.52%   [EVAL] batch:  138 | acc: 50.00%,  total acc: 68.39%   [EVAL] batch:  139 | acc: 43.75%,  total acc: 68.21%   [EVAL] batch:  140 | acc: 62.50%,  total acc: 68.17%   [EVAL] batch:  141 | acc: 56.25%,  total acc: 68.09%   [EVAL] batch:  142 | acc: 62.50%,  total acc: 68.05%   [EVAL] batch:  143 | acc: 56.25%,  total acc: 67.97%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 68.19%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 68.41%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 68.58%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 68.79%   [EVAL] batch:  148 | acc: 93.75%,  total acc: 68.96%   [EVAL] batch:  149 | acc: 81.25%,  total acc: 69.04%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 69.21%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 69.37%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 69.40%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 69.52%   [EVAL] batch:  154 | acc: 81.25%,  total acc: 69.60%   [EVAL] batch:  155 | acc: 93.75%,  total acc: 69.75%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 69.90%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 70.09%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 70.28%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 70.43%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 70.57%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 70.72%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 70.67%   [EVAL] batch:  163 | acc: 25.00%,  total acc: 70.39%   [EVAL] batch:  164 | acc: 12.50%,  total acc: 70.04%   [EVAL] batch:  165 | acc: 25.00%,  total acc: 69.77%   [EVAL] batch:  166 | acc: 12.50%,  total acc: 69.42%   [EVAL] batch:  167 | acc: 6.25%,  total acc: 69.05%   [EVAL] batch:  168 | acc: 31.25%,  total acc: 68.82%   [EVAL] batch:  169 | acc: 93.75%,  total acc: 68.97%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 69.08%   [EVAL] batch:  171 | acc: 87.50%,  total acc: 69.19%   [EVAL] batch:  172 | acc: 93.75%,  total acc: 69.33%   [EVAL] batch:  173 | acc: 87.50%,  total acc: 69.43%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 69.50%   [EVAL] batch:  175 | acc: 37.50%,  total acc: 69.32%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 69.17%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 69.10%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 68.99%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 68.92%   [EVAL] batch:  180 | acc: 25.00%,  total acc: 68.68%   [EVAL] batch:  181 | acc: 50.00%,  total acc: 68.58%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 68.58%   [EVAL] batch:  183 | acc: 56.25%,  total acc: 68.51%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 68.51%   [EVAL] batch:  185 | acc: 56.25%,  total acc: 68.45%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 68.52%   [EVAL] batch:  187 | acc: 25.00%,  total acc: 68.28%   [EVAL] batch:  188 | acc: 18.75%,  total acc: 68.02%   [EVAL] batch:  189 | acc: 18.75%,  total acc: 67.76%   [EVAL] batch:  190 | acc: 18.75%,  total acc: 67.51%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 67.25%   [EVAL] batch:  192 | acc: 31.25%,  total acc: 67.07%   [EVAL] batch:  193 | acc: 31.25%,  total acc: 66.88%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 66.92%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 66.93%   [EVAL] batch:  196 | acc: 56.25%,  total acc: 66.88%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 66.98%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 67.02%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 67.09%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 67.16%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 67.20%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 67.18%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 67.16%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 67.23%   [EVAL] batch:  205 | acc: 81.25%,  total acc: 67.29%   [EVAL] batch:  206 | acc: 81.25%,  total acc: 67.36%   [EVAL] batch:  207 | acc: 56.25%,  total acc: 67.31%   [EVAL] batch:  208 | acc: 43.75%,  total acc: 67.19%   [EVAL] batch:  209 | acc: 50.00%,  total acc: 67.11%   [EVAL] batch:  210 | acc: 43.75%,  total acc: 67.00%   [EVAL] batch:  211 | acc: 50.00%,  total acc: 66.92%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 66.93%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 67.06%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 67.21%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 67.33%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 67.48%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 67.60%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 67.75%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 67.90%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 68.04%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 68.19%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 68.33%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 68.47%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 68.61%   [EVAL] batch:  225 | acc: 93.75%,  total acc: 68.72%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 68.81%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 68.80%   [EVAL] batch:  228 | acc: 81.25%,  total acc: 68.86%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 68.97%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 69.07%   [EVAL] batch:  231 | acc: 50.00%,  total acc: 68.99%   [EVAL] batch:  232 | acc: 31.25%,  total acc: 68.83%   [EVAL] batch:  233 | acc: 25.00%,  total acc: 68.64%   [EVAL] batch:  234 | acc: 18.75%,  total acc: 68.43%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 68.38%   [EVAL] batch:  236 | acc: 25.00%,  total acc: 68.20%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 68.22%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 68.31%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 68.44%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 68.52%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 68.62%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 68.72%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 68.78%   [EVAL] batch:  244 | acc: 25.00%,  total acc: 68.60%   [EVAL] batch:  245 | acc: 31.25%,  total acc: 68.45%   [EVAL] batch:  246 | acc: 18.75%,  total acc: 68.24%   [EVAL] batch:  247 | acc: 43.75%,  total acc: 68.15%   [EVAL] batch:  248 | acc: 12.50%,  total acc: 67.92%   [EVAL] batch:  249 | acc: 31.25%,  total acc: 67.77%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 67.88%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 67.98%   [EVAL] batch:  252 | acc: 100.00%,  total acc: 68.11%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 68.18%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 68.28%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 68.41%   [EVAL] batch:  256 | acc: 93.75%,  total acc: 68.51%   [EVAL] batch:  257 | acc: 100.00%,  total acc: 68.63%   [EVAL] batch:  258 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 68.85%   [EVAL] batch:  260 | acc: 93.75%,  total acc: 68.94%   [EVAL] batch:  261 | acc: 87.50%,  total acc: 69.01%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 69.08%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 69.13%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 69.10%   [EVAL] batch:  265 | acc: 62.50%,  total acc: 69.08%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 69.05%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 69.05%   [EVAL] batch:  268 | acc: 62.50%,  total acc: 69.03%   [EVAL] batch:  269 | acc: 75.00%,  total acc: 69.05%   [EVAL] batch:  270 | acc: 93.75%,  total acc: 69.14%   [EVAL] batch:  271 | acc: 81.25%,  total acc: 69.19%   [EVAL] batch:  272 | acc: 56.25%,  total acc: 69.14%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 69.21%   [EVAL] batch:  274 | acc: 68.75%,  total acc: 69.20%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 69.32%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 69.43%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 69.54%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 69.65%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 69.86%   [EVAL] batch:  281 | acc: 81.25%,  total acc: 69.90%   [EVAL] batch:  282 | acc: 81.25%,  total acc: 69.94%   [EVAL] batch:  283 | acc: 75.00%,  total acc: 69.96%   [EVAL] batch:  284 | acc: 93.75%,  total acc: 70.04%   [EVAL] batch:  285 | acc: 75.00%,  total acc: 70.06%   [EVAL] batch:  286 | acc: 81.25%,  total acc: 70.10%   [EVAL] batch:  287 | acc: 81.25%,  total acc: 70.14%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 70.24%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 70.34%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 70.45%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 70.55%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 70.65%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 70.73%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 70.92%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 71.02%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 71.12%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 71.22%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 71.31%   [EVAL] batch:  300 | acc: 81.25%,  total acc: 71.35%   [EVAL] batch:  301 | acc: 87.50%,  total acc: 71.40%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 71.47%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 71.55%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 71.58%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 71.65%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 71.70%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 71.77%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 71.82%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 71.92%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 72.01%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 72.04%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 71.96%   [EVAL] batch:  313 | acc: 56.25%,  total acc: 71.91%   [EVAL] batch:  314 | acc: 43.75%,  total acc: 71.83%   [EVAL] batch:  315 | acc: 50.00%,  total acc: 71.76%   [EVAL] batch:  316 | acc: 56.25%,  total acc: 71.71%   [EVAL] batch:  317 | acc: 62.50%,  total acc: 71.68%   [EVAL] batch:  318 | acc: 68.75%,  total acc: 71.67%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 71.76%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 71.85%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 71.89%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 71.96%   [EVAL] batch:  323 | acc: 100.00%,  total acc: 72.05%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 72.10%   [EVAL] batch:  325 | acc: 81.25%,  total acc: 72.12%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 72.08%   [EVAL] batch:  327 | acc: 81.25%,  total acc: 72.10%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 72.11%   [EVAL] batch:  329 | acc: 75.00%,  total acc: 72.12%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 72.11%   [EVAL] batch:  331 | acc: 37.50%,  total acc: 72.01%   [EVAL] batch:  332 | acc: 31.25%,  total acc: 71.88%   [EVAL] batch:  333 | acc: 25.00%,  total acc: 71.74%   [EVAL] batch:  334 | acc: 12.50%,  total acc: 71.57%   [EVAL] batch:  335 | acc: 18.75%,  total acc: 71.41%   [EVAL] batch:  336 | acc: 12.50%,  total acc: 71.24%   [EVAL] batch:  337 | acc: 62.50%,  total acc: 71.21%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 71.26%   [EVAL] batch:  339 | acc: 68.75%,  total acc: 71.25%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 71.33%   [EVAL] batch:  341 | acc: 81.25%,  total acc: 71.36%   [EVAL] batch:  342 | acc: 62.50%,  total acc: 71.34%   [EVAL] batch:  343 | acc: 43.75%,  total acc: 71.26%   [EVAL] batch:  344 | acc: 75.00%,  total acc: 71.27%   [EVAL] batch:  345 | acc: 56.25%,  total acc: 71.22%   [EVAL] batch:  346 | acc: 62.50%,  total acc: 71.20%   [EVAL] batch:  347 | acc: 68.75%,  total acc: 71.19%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 71.20%   [EVAL] batch:  349 | acc: 50.00%,  total acc: 71.14%   [EVAL] batch:  350 | acc: 62.50%,  total acc: 71.12%   [EVAL] batch:  351 | acc: 87.50%,  total acc: 71.16%   [EVAL] batch:  352 | acc: 81.25%,  total acc: 71.19%   [EVAL] batch:  353 | acc: 93.75%,  total acc: 71.26%   [EVAL] batch:  354 | acc: 75.00%,  total acc: 71.27%   [EVAL] batch:  355 | acc: 81.25%,  total acc: 71.30%   [EVAL] batch:  356 | acc: 43.75%,  total acc: 71.22%   [EVAL] batch:  357 | acc: 12.50%,  total acc: 71.05%   [EVAL] batch:  358 | acc: 25.00%,  total acc: 70.93%   [EVAL] batch:  359 | acc: 31.25%,  total acc: 70.82%   [EVAL] batch:  360 | acc: 31.25%,  total acc: 70.71%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 70.65%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 70.64%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 70.72%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 70.80%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 70.88%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 70.96%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 71.04%   [EVAL] batch:  368 | acc: 87.50%,  total acc: 71.09%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 71.15%   [EVAL] batch:  370 | acc: 75.00%,  total acc: 71.16%   [EVAL] batch:  371 | acc: 87.50%,  total acc: 71.20%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 71.25%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 71.27%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 71.28%   
cur_acc:  ['0.9454', '0.7867', '0.7738', '0.6647', '0.8800', '0.6677']
his_acc:  ['0.9454', '0.8590', '0.8022', '0.7362', '0.7340', '0.7128']
CurrentTrain: epoch 15, batch     0 | loss: 10.1660297CurrentTrain: epoch 15, batch     1 | loss: 11.8377532CurrentTrain: epoch 15, batch     2 | loss: 17.4946187CurrentTrain: epoch  1, batch     3 | loss: 14.8442417CurrentTrain: epoch 15, batch     0 | loss: 11.4967240CurrentTrain: epoch 15, batch     1 | loss: 11.4517289CurrentTrain: epoch 15, batch     2 | loss: 8.7737693CurrentTrain: epoch  1, batch     3 | loss: 8.1482939CurrentTrain: epoch 15, batch     0 | loss: 8.8775363CurrentTrain: epoch 15, batch     1 | loss: 8.4342248CurrentTrain: epoch 15, batch     2 | loss: 6.0625639CurrentTrain: epoch  1, batch     3 | loss: 5.4002329CurrentTrain: epoch 15, batch     0 | loss: 7.9102519CurrentTrain: epoch 15, batch     1 | loss: 8.6884267CurrentTrain: epoch 15, batch     2 | loss: 10.1815756CurrentTrain: epoch  1, batch     3 | loss: 6.7614393CurrentTrain: epoch 15, batch     0 | loss: 9.3522969CurrentTrain: epoch 15, batch     1 | loss: 8.5186333CurrentTrain: epoch 15, batch     2 | loss: 6.8416218CurrentTrain: epoch  1, batch     3 | loss: 6.6484226CurrentTrain: epoch 15, batch     0 | loss: 5.6167853CurrentTrain: epoch 15, batch     1 | loss: 7.9430430CurrentTrain: epoch 15, batch     2 | loss: 11.0028880CurrentTrain: epoch  1, batch     3 | loss: 13.9362495CurrentTrain: epoch 15, batch     0 | loss: 19.6420474CurrentTrain: epoch 15, batch     1 | loss: 11.7723660CurrentTrain: epoch 15, batch     2 | loss: 5.8727844CurrentTrain: epoch  1, batch     3 | loss: 6.3648558CurrentTrain: epoch 15, batch     0 | loss: 6.0918690CurrentTrain: epoch 15, batch     1 | loss: 8.9329532CurrentTrain: epoch 15, batch     2 | loss: 11.4450388CurrentTrain: epoch  1, batch     3 | loss: 6.8700213CurrentTrain: epoch 15, batch     0 | loss: 9.5622076CurrentTrain: epoch 15, batch     1 | loss: 6.4929135CurrentTrain: epoch 15, batch     2 | loss: 11.8424516CurrentTrain: epoch  1, batch     3 | loss: 6.2398286CurrentTrain: epoch 15, batch     0 | loss: 9.0903026CurrentTrain: epoch 15, batch     1 | loss: 9.3943981CurrentTrain: epoch 15, batch     2 | loss: 8.0630118CurrentTrain: epoch  1, batch     3 | loss: 5.6888166
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the cantons of créteil are administrative divisions of the val - de - marne department , île - de - france region , northern france .
Head Entity: île - de - france
Tail Entity: val - de - marne
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The provinces of Canada are the primary administrative divisions, with Ontario being one of the largest provinces in the country.  
Head Entity: Canada  
Tail Entity: Ontario  

Relation: contains administrative territorial entity  
Context: The state of California is divided into several counties, including Los Angeles County, which is known for its entertainment industry.  
Head Entity: California  
Tail Entity: Los Angeles County  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: enzo is an italian given name derivative of heinz , a form of henry .
Head Entity: henry
Tail Entity: heinz
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as each other due to their close genetic relationship.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: Many historians argue that the ancient city of Byzantium is said to be the same as modern-day Istanbul, although the transition involved significant changes over time.  
Head Entity: Byzantium  
Tail Entity: Istanbul  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: in 2010 , ty burrell received a nomination for the television critics association award for individual achievement in comedy , winning in 2011 .
Head Entity: television critics association award for individual achievement in comedy
Tail Entity: ty burrell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: In 2022, the Boston Red Sox triumphed in the World Series, marking their fourth championship in the last two decades.  
Head Entity: World Series  
Tail Entity: Boston Red Sox  

Relation: winner  
Context: At the annual film festival, the documentary "Voices of Change" was awarded the Best Picture, highlighting its impactful storytelling and direction.  
Head Entity: Best Picture  
Tail Entity: Voices of Change  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: the calais region was defended by the 15th army under " generaloberst " ( colonel general ) hans von salmuth , and normandy by the 7th army commanded by " generaloberst " friedrich dollmann .
Head Entity: hans von salmuth
Tail Entity: generaloberst
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: during the battle of stalingrad, the soviet forces were led by general vasily chuikov, who held the rank of major general, while the german troops were commanded by general friedrich paulus.  
Head Entity: vasily chuikov  
Tail Entity: major general  

Relation: military rank  
Context: admiral horatio nelson was a key figure in the british navy, famously known for his rank of vice admiral during the napoleonic wars.  
Head Entity: horatio nelson  
Tail Entity: vice admiral  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: he also worked on activision games like " gun " , " " and " the amazing spider - man " .
Head Entity: gun
Tail Entity: activision
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: The latest novel by J.K. Rowling was released by Bloomsbury Publishing, captivating readers worldwide.  
Head Entity: J.K. Rowling  
Tail Entity: Bloomsbury Publishing  

Relation: publisher  
Context: The popular science magazine was launched by National Geographic, providing insights into nature and exploration.  
Head Entity: science magazine  
Tail Entity: National Geographic  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " muzai moratorium " and " shōso strip " were released on vinyl in 2008 , as a part of her 10-year anniversary celebrations .
Head Entity: muzai moratorium
Tail Entity: shōso strip
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first chapter of the book introduces the main character, while the second chapter follows with a detailed backstory.  
Head Entity: first chapter  
Tail Entity: second chapter  

Relation: followed by  
Context: The opening act of the concert was a local band, followed by a well-known artist who captivated the audience.  
Head Entity: local band  
Tail Entity: well-known artist  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: there is a memoir of his life and work by gustav queck ( jena , 1852 ) .
Head Entity: gustav queck
Tail Entity: jena
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: The headquarters of the company is situated in San Francisco, where they have been operating since 2010.  
Head Entity: the company  
Tail Entity: San Francisco  

Relation: work location  
Context: During her time at the university, she conducted research in various labs located in Cambridge.  
Head Entity: she  
Tail Entity: Cambridge  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in enzyme catalysis.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a former professional athlete, now works as a sports commentator for major networks.  
Head Entity: john smith  
Tail Entity: sports commentator  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a World Heritage Site, attracting tourists from around the globe.  
Head Entity: Petra  
Tail Entity: World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: ada adini ( 1855 – february 1924 ) was an american operatic soprano who had an active international career from 1876 up into the first decade of the 20th century .
Head Entity: ada adini
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti (1935 – 2007) was an italian operatic tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey, born in 1969, is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: singer  
MemoryTrain:  epoch 15, batch     0 | loss: 4.6609086MemoryTrain:  epoch 15, batch     1 | loss: 2.5435764MemoryTrain:  epoch 15, batch     2 | loss: 5.5145624MemoryTrain:  epoch 15, batch     3 | loss: 3.2532070MemoryTrain:  epoch 15, batch     4 | loss: 3.7377419MemoryTrain:  epoch 15, batch     5 | loss: 3.9900118MemoryTrain:  epoch 15, batch     6 | loss: 3.4397584MemoryTrain:  epoch 15, batch     7 | loss: 4.1316612MemoryTrain:  epoch 15, batch     8 | loss: 2.8146364MemoryTrain:  epoch 15, batch     9 | loss: 2.4401004MemoryTrain:  epoch 15, batch    10 | loss: 2.7789033MemoryTrain:  epoch 15, batch    11 | loss: 3.0244261MemoryTrain:  epoch 15, batch    12 | loss: 3.8955558MemoryTrain:  epoch  1, batch    13 | loss: 10.3442190MemoryTrain:  epoch 15, batch     0 | loss: 2.2841674MemoryTrain:  epoch 15, batch     1 | loss: 2.4889871MemoryTrain:  epoch 15, batch     2 | loss: 3.3521888MemoryTrain:  epoch 15, batch     3 | loss: 5.8789010MemoryTrain:  epoch 15, batch     4 | loss: 2.4591374MemoryTrain:  epoch 15, batch     5 | loss: 2.2484112MemoryTrain:  epoch 15, batch     6 | loss: 2.6911423MemoryTrain:  epoch 15, batch     7 | loss: 4.3727150MemoryTrain:  epoch 15, batch     8 | loss: 2.1986863MemoryTrain:  epoch 15, batch     9 | loss: 3.1092435MemoryTrain:  epoch 15, batch    10 | loss: 3.3714037MemoryTrain:  epoch 15, batch    11 | loss: 1.7768267MemoryTrain:  epoch 15, batch    12 | loss: 3.6107450MemoryTrain:  epoch  1, batch    13 | loss: 5.5185484MemoryTrain:  epoch 15, batch     0 | loss: 2.1254871MemoryTrain:  epoch 15, batch     1 | loss: 1.8061412MemoryTrain:  epoch 15, batch     2 | loss: 3.1145200MemoryTrain:  epoch 15, batch     3 | loss: 2.0450896MemoryTrain:  epoch 15, batch     4 | loss: 1.8292609MemoryTrain:  epoch 15, batch     5 | loss: 2.2658915MemoryTrain:  epoch 15, batch     6 | loss: 1.4258110MemoryTrain:  epoch 15, batch     7 | loss: 2.6642336MemoryTrain:  epoch 15, batch     8 | loss: 2.1230342MemoryTrain:  epoch 15, batch     9 | loss: 2.9230951MemoryTrain:  epoch 15, batch    10 | loss: 5.2824926MemoryTrain:  epoch 15, batch    11 | loss: 2.2400940MemoryTrain:  epoch 15, batch    12 | loss: 2.6373216MemoryTrain:  epoch  1, batch    13 | loss: 5.9227447MemoryTrain:  epoch 15, batch     0 | loss: 2.2994273MemoryTrain:  epoch 15, batch     1 | loss: 1.9986301MemoryTrain:  epoch 15, batch     2 | loss: 2.4427381MemoryTrain:  epoch 15, batch     3 | loss: 3.7571287MemoryTrain:  epoch 15, batch     4 | loss: 2.3474660MemoryTrain:  epoch 15, batch     5 | loss: 1.7884899MemoryTrain:  epoch 15, batch     6 | loss: 1.6879664MemoryTrain:  epoch 15, batch     7 | loss: 2.7209615MemoryTrain:  epoch 15, batch     8 | loss: 1.6031596MemoryTrain:  epoch 15, batch     9 | loss: 1.8195259MemoryTrain:  epoch 15, batch    10 | loss: 3.8534791MemoryTrain:  epoch 15, batch    11 | loss: 2.3711443MemoryTrain:  epoch 15, batch    12 | loss: 1.7809836MemoryTrain:  epoch  1, batch    13 | loss: 5.7284549MemoryTrain:  epoch 15, batch     0 | loss: 4.2279588MemoryTrain:  epoch 15, batch     1 | loss: 2.4187864MemoryTrain:  epoch 15, batch     2 | loss: 1.9391515MemoryTrain:  epoch 15, batch     3 | loss: 1.6940157MemoryTrain:  epoch 15, batch     4 | loss: 1.7412519MemoryTrain:  epoch 15, batch     5 | loss: 2.0488159MemoryTrain:  epoch 15, batch     6 | loss: 4.7342685MemoryTrain:  epoch 15, batch     7 | loss: 1.6263741MemoryTrain:  epoch 15, batch     8 | loss: 1.5870264MemoryTrain:  epoch 15, batch     9 | loss: 5.1999444MemoryTrain:  epoch 15, batch    10 | loss: 1.7707792MemoryTrain:  epoch 15, batch    11 | loss: 1.4797031MemoryTrain:  epoch 15, batch    12 | loss: 1.9233291MemoryTrain:  epoch  1, batch    13 | loss: 5.6623024MemoryTrain:  epoch 15, batch     0 | loss: 2.3250341MemoryTrain:  epoch 15, batch     1 | loss: 3.4516160MemoryTrain:  epoch 15, batch     2 | loss: 1.9710306MemoryTrain:  epoch 15, batch     3 | loss: 2.8160563MemoryTrain:  epoch 15, batch     4 | loss: 1.3726621MemoryTrain:  epoch 15, batch     5 | loss: 1.5702762MemoryTrain:  epoch 15, batch     6 | loss: 1.9481314MemoryTrain:  epoch 15, batch     7 | loss: 2.1221375MemoryTrain:  epoch 15, batch     8 | loss: 1.4866433MemoryTrain:  epoch 15, batch     9 | loss: 1.6496088MemoryTrain:  epoch 15, batch    10 | loss: 1.5777793MemoryTrain:  epoch 15, batch    11 | loss: 1.9863665MemoryTrain:  epoch 15, batch    12 | loss: 1.5201924MemoryTrain:  epoch  1, batch    13 | loss: 5.1851671MemoryTrain:  epoch 15, batch     0 | loss: 1.8676382MemoryTrain:  epoch 15, batch     1 | loss: 2.0868419MemoryTrain:  epoch 15, batch     2 | loss: 1.4446512MemoryTrain:  epoch 15, batch     3 | loss: 1.7890888MemoryTrain:  epoch 15, batch     4 | loss: 1.8872672MemoryTrain:  epoch 15, batch     5 | loss: 2.4260934MemoryTrain:  epoch 15, batch     6 | loss: 1.6528632MemoryTrain:  epoch 15, batch     7 | loss: 1.5587780MemoryTrain:  epoch 15, batch     8 | loss: 1.6440948MemoryTrain:  epoch 15, batch     9 | loss: 2.2634799MemoryTrain:  epoch 15, batch    10 | loss: 2.3137685MemoryTrain:  epoch 15, batch    11 | loss: 4.3728079MemoryTrain:  epoch 15, batch    12 | loss: 1.4238322MemoryTrain:  epoch  1, batch    13 | loss: 6.6325748MemoryTrain:  epoch 15, batch     0 | loss: 1.5167600MemoryTrain:  epoch 15, batch     1 | loss: 2.0352158MemoryTrain:  epoch 15, batch     2 | loss: 1.8285961MemoryTrain:  epoch 15, batch     3 | loss: 1.5647895MemoryTrain:  epoch 15, batch     4 | loss: 1.8510006MemoryTrain:  epoch 15, batch     5 | loss: 1.8989651MemoryTrain:  epoch 15, batch     6 | loss: 1.8323186MemoryTrain:  epoch 15, batch     7 | loss: 1.6383017MemoryTrain:  epoch 15, batch     8 | loss: 1.6642567MemoryTrain:  epoch 15, batch     9 | loss: 1.5654737MemoryTrain:  epoch 15, batch    10 | loss: 1.8940820MemoryTrain:  epoch 15, batch    11 | loss: 1.9548099MemoryTrain:  epoch 15, batch    12 | loss: 2.4824550MemoryTrain:  epoch  1, batch    13 | loss: 5.2937069MemoryTrain:  epoch 15, batch     0 | loss: 1.3746742MemoryTrain:  epoch 15, batch     1 | loss: 1.7192927MemoryTrain:  epoch 15, batch     2 | loss: 1.4805896MemoryTrain:  epoch 15, batch     3 | loss: 1.6356110MemoryTrain:  epoch 15, batch     4 | loss: 1.3872312MemoryTrain:  epoch 15, batch     5 | loss: 1.5661808MemoryTrain:  epoch 15, batch     6 | loss: 2.3896747MemoryTrain:  epoch 15, batch     7 | loss: 1.5467939MemoryTrain:  epoch 15, batch     8 | loss: 2.2096773MemoryTrain:  epoch 15, batch     9 | loss: 2.4724459MemoryTrain:  epoch 15, batch    10 | loss: 2.5734335MemoryTrain:  epoch 15, batch    11 | loss: 4.6743278MemoryTrain:  epoch 15, batch    12 | loss: 4.0305340MemoryTrain:  epoch  1, batch    13 | loss: 5.2493634MemoryTrain:  epoch 15, batch     0 | loss: 2.3428816MemoryTrain:  epoch 15, batch     1 | loss: 2.6924352MemoryTrain:  epoch 15, batch     2 | loss: 3.7235221MemoryTrain:  epoch 15, batch     3 | loss: 1.5889714MemoryTrain:  epoch 15, batch     4 | loss: 1.3815283MemoryTrain:  epoch 15, batch     5 | loss: 1.6248912MemoryTrain:  epoch 15, batch     6 | loss: 1.6192843MemoryTrain:  epoch 15, batch     7 | loss: 1.3176400MemoryTrain:  epoch 15, batch     8 | loss: 1.5459715MemoryTrain:  epoch 15, batch     9 | loss: 1.7883574MemoryTrain:  epoch 15, batch    10 | loss: 1.5034267MemoryTrain:  epoch 15, batch    11 | loss: 1.9276368MemoryTrain:  epoch 15, batch    12 | loss: 3.6675026MemoryTrain:  epoch  1, batch    13 | loss: 6.4170682
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 37.50%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 38.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 44.79%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 43.75%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 46.88%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 48.61%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 49.38%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 49.43%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 51.04%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 54.81%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 56.25%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 57.08%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 58.59%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 60.29%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 62.15%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 63.49%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 64.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 66.07%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 67.61%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 69.53%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 70.50%   [EVAL] batch:   25 | acc: 50.00%,  total acc: 69.71%   [EVAL] batch:   26 | acc: 18.75%,  total acc: 67.82%   [EVAL] batch:   27 | acc: 50.00%,  total acc: 67.19%   [EVAL] batch:   28 | acc: 62.50%,  total acc: 67.03%   [EVAL] batch:   29 | acc: 37.50%,  total acc: 66.04%   [EVAL] batch:   30 | acc: 37.50%,  total acc: 65.12%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 65.04%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 64.58%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 63.97%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 63.04%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 61.81%   [EVAL] batch:   36 | acc: 25.00%,  total acc: 60.81%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 61.02%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 61.70%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 62.34%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 62.80%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 63.54%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 64.10%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 64.63%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 64.72%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 64.67%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 64.49%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 64.32%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 64.54%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 65.00%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 65.69%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 66.35%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 66.98%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 67.59%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 68.18%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 69.08%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 69.50%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 69.81%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 70.21%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 70.39%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 70.87%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 70.54%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 74.11%   [EVAL] batch:    7 | acc: 25.00%,  total acc: 67.97%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 64.58%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 61.88%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 59.09%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 57.29%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 56.73%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 58.48%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 60.42%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 64.71%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 66.32%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 67.76%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 67.81%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 68.45%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 68.47%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 69.02%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 69.27%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 70.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 72.22%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 73.21%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 73.92%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 74.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 75.60%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 76.17%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 76.70%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 76.29%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 76.25%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 76.22%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 76.52%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 76.97%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 77.56%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 78.66%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 79.02%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 79.36%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 79.40%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 79.44%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 79.21%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 78.86%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 78.39%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 78.57%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 78.12%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 78.31%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 78.49%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 78.66%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 78.59%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 78.30%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 78.18%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 77.59%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 77.12%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 76.95%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 76.92%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 76.69%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 76.17%   [EVAL] batch:   64 | acc: 43.75%,  total acc: 75.67%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 75.57%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 75.09%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 74.82%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 74.64%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 74.64%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 74.56%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 74.39%   [EVAL] batch:   72 | acc: 62.50%,  total acc: 74.23%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 74.16%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 74.00%   [EVAL] batch:   75 | acc: 25.00%,  total acc: 73.36%   [EVAL] batch:   76 | acc: 12.50%,  total acc: 72.56%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 71.96%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 71.44%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 71.17%   [EVAL] batch:   80 | acc: 12.50%,  total acc: 70.45%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 70.50%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 70.86%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 71.21%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 71.47%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 71.80%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 72.05%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 72.09%   [EVAL] batch:   88 | acc: 18.75%,  total acc: 71.49%   [EVAL] batch:   89 | acc: 12.50%,  total acc: 70.83%   [EVAL] batch:   90 | acc: 25.00%,  total acc: 70.33%   [EVAL] batch:   91 | acc: 37.50%,  total acc: 69.97%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 69.29%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 68.95%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 69.21%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 69.47%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 69.72%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 69.90%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 70.14%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 70.44%   [EVAL] batch:  100 | acc: 0.00%,  total acc: 69.74%   [EVAL] batch:  101 | acc: 0.00%,  total acc: 69.06%   [EVAL] batch:  102 | acc: 0.00%,  total acc: 68.39%   [EVAL] batch:  103 | acc: 6.25%,  total acc: 67.79%   [EVAL] batch:  104 | acc: 0.00%,  total acc: 67.14%   [EVAL] batch:  105 | acc: 12.50%,  total acc: 66.63%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 66.41%   [EVAL] batch:  107 | acc: 56.25%,  total acc: 66.32%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 66.28%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 66.08%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 66.05%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 65.90%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 65.98%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 65.84%   [EVAL] batch:  114 | acc: 62.50%,  total acc: 65.82%   [EVAL] batch:  115 | acc: 62.50%,  total acc: 65.79%   [EVAL] batch:  116 | acc: 68.75%,  total acc: 65.81%   [EVAL] batch:  117 | acc: 56.25%,  total acc: 65.73%   [EVAL] batch:  118 | acc: 50.00%,  total acc: 65.60%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 65.83%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 66.06%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 66.24%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 66.31%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 66.43%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 66.65%   [EVAL] batch:  125 | acc: 6.25%,  total acc: 66.17%   [EVAL] batch:  126 | acc: 12.50%,  total acc: 65.75%   [EVAL] batch:  127 | acc: 6.25%,  total acc: 65.28%   [EVAL] batch:  128 | acc: 0.00%,  total acc: 64.78%   [EVAL] batch:  129 | acc: 0.00%,  total acc: 64.28%   [EVAL] batch:  130 | acc: 0.00%,  total acc: 63.79%   [EVAL] batch:  131 | acc: 43.75%,  total acc: 63.64%   [EVAL] batch:  132 | acc: 75.00%,  total acc: 63.72%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 63.85%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 64.03%   [EVAL] batch:  135 | acc: 75.00%,  total acc: 64.11%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 64.28%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 64.27%   [EVAL] batch:  138 | acc: 37.50%,  total acc: 64.07%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 63.79%   [EVAL] batch:  140 | acc: 50.00%,  total acc: 63.70%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 63.51%   [EVAL] batch:  142 | acc: 56.25%,  total acc: 63.46%   [EVAL] batch:  143 | acc: 56.25%,  total acc: 63.41%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 63.66%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 63.91%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 64.12%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 64.36%   [EVAL] batch:  148 | acc: 93.75%,  total acc: 64.56%   [EVAL] batch:  149 | acc: 81.25%,  total acc: 64.67%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 64.86%   [EVAL] batch:  151 | acc: 87.50%,  total acc: 65.01%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 65.07%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 65.22%   [EVAL] batch:  154 | acc: 81.25%,  total acc: 65.32%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 65.46%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 65.64%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 65.86%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 66.04%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 66.21%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 66.38%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 66.55%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 66.53%   [EVAL] batch:  163 | acc: 25.00%,  total acc: 66.27%   [EVAL] batch:  164 | acc: 12.50%,  total acc: 65.95%   [EVAL] batch:  165 | acc: 31.25%,  total acc: 65.74%   [EVAL] batch:  166 | acc: 12.50%,  total acc: 65.42%   [EVAL] batch:  167 | acc: 18.75%,  total acc: 65.14%   [EVAL] batch:  168 | acc: 31.25%,  total acc: 64.94%   [EVAL] batch:  169 | acc: 93.75%,  total acc: 65.11%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 65.24%   [EVAL] batch:  171 | acc: 93.75%,  total acc: 65.41%   [EVAL] batch:  172 | acc: 93.75%,  total acc: 65.57%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 65.71%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 65.62%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 65.50%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 65.38%   [EVAL] batch:  178 | acc: 31.25%,  total acc: 65.19%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 65.14%   [EVAL] batch:  180 | acc: 25.00%,  total acc: 64.92%   [EVAL] batch:  181 | acc: 56.25%,  total acc: 64.87%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 64.89%   [EVAL] batch:  183 | acc: 56.25%,  total acc: 64.84%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 64.90%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 64.99%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 65.11%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 64.96%   [EVAL] batch:  188 | acc: 6.25%,  total acc: 64.65%   [EVAL] batch:  189 | acc: 6.25%,  total acc: 64.34%   [EVAL] batch:  190 | acc: 12.50%,  total acc: 64.07%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 63.83%   [EVAL] batch:  192 | acc: 31.25%,  total acc: 63.67%   [EVAL] batch:  193 | acc: 25.00%,  total acc: 63.47%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 63.53%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 63.52%   [EVAL] batch:  196 | acc: 56.25%,  total acc: 63.48%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 63.57%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 63.63%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 63.72%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 63.77%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 63.83%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 63.82%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 63.85%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 63.93%   [EVAL] batch:  205 | acc: 81.25%,  total acc: 64.02%   [EVAL] batch:  206 | acc: 43.75%,  total acc: 63.92%   [EVAL] batch:  207 | acc: 12.50%,  total acc: 63.67%   [EVAL] batch:  208 | acc: 12.50%,  total acc: 63.43%   [EVAL] batch:  209 | acc: 6.25%,  total acc: 63.15%   [EVAL] batch:  210 | acc: 18.75%,  total acc: 62.94%   [EVAL] batch:  211 | acc: 12.50%,  total acc: 62.71%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 62.65%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 62.79%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 62.97%   [EVAL] batch:  215 | acc: 75.00%,  total acc: 63.02%   [EVAL] batch:  216 | acc: 93.75%,  total acc: 63.16%   [EVAL] batch:  217 | acc: 87.50%,  total acc: 63.27%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 63.44%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 63.61%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 63.77%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 63.94%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 64.10%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 64.26%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 64.42%   [EVAL] batch:  225 | acc: 93.75%,  total acc: 64.55%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 64.65%   [EVAL] batch:  227 | acc: 81.25%,  total acc: 64.72%   [EVAL] batch:  228 | acc: 81.25%,  total acc: 64.79%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 65.07%   [EVAL] batch:  231 | acc: 50.00%,  total acc: 65.01%   [EVAL] batch:  232 | acc: 31.25%,  total acc: 64.86%   [EVAL] batch:  233 | acc: 31.25%,  total acc: 64.72%   [EVAL] batch:  234 | acc: 25.00%,  total acc: 64.55%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 64.51%   [EVAL] batch:  236 | acc: 31.25%,  total acc: 64.37%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 64.36%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 64.46%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 64.61%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 64.70%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 64.80%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 64.92%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 65.04%   [EVAL] batch:  244 | acc: 31.25%,  total acc: 64.90%   [EVAL] batch:  245 | acc: 50.00%,  total acc: 64.84%   [EVAL] batch:  246 | acc: 31.25%,  total acc: 64.70%   [EVAL] batch:  247 | acc: 37.50%,  total acc: 64.59%   [EVAL] batch:  248 | acc: 18.75%,  total acc: 64.41%   [EVAL] batch:  249 | acc: 37.50%,  total acc: 64.30%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 64.42%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 64.53%   [EVAL] batch:  252 | acc: 100.00%,  total acc: 64.67%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 64.76%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 64.88%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 65.01%   [EVAL] batch:  256 | acc: 93.75%,  total acc: 65.13%   [EVAL] batch:  257 | acc: 100.00%,  total acc: 65.26%   [EVAL] batch:  258 | acc: 100.00%,  total acc: 65.40%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 65.50%   [EVAL] batch:  260 | acc: 87.50%,  total acc: 65.59%   [EVAL] batch:  261 | acc: 87.50%,  total acc: 65.67%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 65.76%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 65.79%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 65.80%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 65.77%   [EVAL] batch:  266 | acc: 50.00%,  total acc: 65.71%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 65.72%   [EVAL] batch:  268 | acc: 56.25%,  total acc: 65.68%   [EVAL] batch:  269 | acc: 43.75%,  total acc: 65.60%   [EVAL] batch:  270 | acc: 56.25%,  total acc: 65.57%   [EVAL] batch:  271 | acc: 50.00%,  total acc: 65.51%   [EVAL] batch:  272 | acc: 31.25%,  total acc: 65.38%   [EVAL] batch:  273 | acc: 75.00%,  total acc: 65.42%   [EVAL] batch:  274 | acc: 50.00%,  total acc: 65.36%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 65.49%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 65.61%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 65.74%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 65.86%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 65.98%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 66.10%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 66.11%   [EVAL] batch:  282 | acc: 75.00%,  total acc: 66.14%   [EVAL] batch:  283 | acc: 68.75%,  total acc: 66.15%   [EVAL] batch:  284 | acc: 68.75%,  total acc: 66.16%   [EVAL] batch:  285 | acc: 62.50%,  total acc: 66.15%   [EVAL] batch:  286 | acc: 56.25%,  total acc: 66.11%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 66.12%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 66.24%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 66.36%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 66.47%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 66.59%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 66.70%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 66.79%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 66.91%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 67.02%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 67.13%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 67.24%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 67.35%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 67.46%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 67.55%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 67.63%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 67.72%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 67.80%   [EVAL] batch:  304 | acc: 87.50%,  total acc: 67.87%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 67.95%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 68.04%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 68.12%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 68.18%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 68.29%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 68.39%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 68.45%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 68.39%   [EVAL] batch:  313 | acc: 56.25%,  total acc: 68.35%   [EVAL] batch:  314 | acc: 43.75%,  total acc: 68.27%   [EVAL] batch:  315 | acc: 43.75%,  total acc: 68.20%   [EVAL] batch:  316 | acc: 56.25%,  total acc: 68.16%   [EVAL] batch:  317 | acc: 68.75%,  total acc: 68.16%   [EVAL] batch:  318 | acc: 68.75%,  total acc: 68.16%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 68.26%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 68.36%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 68.40%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 68.48%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 68.56%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 68.62%   [EVAL] batch:  325 | acc: 62.50%,  total acc: 68.60%   [EVAL] batch:  326 | acc: 43.75%,  total acc: 68.52%   [EVAL] batch:  327 | acc: 62.50%,  total acc: 68.50%   [EVAL] batch:  328 | acc: 50.00%,  total acc: 68.45%   [EVAL] batch:  329 | acc: 68.75%,  total acc: 68.45%   [EVAL] batch:  330 | acc: 56.25%,  total acc: 68.41%   [EVAL] batch:  331 | acc: 37.50%,  total acc: 68.32%   [EVAL] batch:  332 | acc: 31.25%,  total acc: 68.21%   [EVAL] batch:  333 | acc: 25.00%,  total acc: 68.08%   [EVAL] batch:  334 | acc: 25.00%,  total acc: 67.95%   [EVAL] batch:  335 | acc: 37.50%,  total acc: 67.86%   [EVAL] batch:  336 | acc: 18.75%,  total acc: 67.71%   [EVAL] batch:  337 | acc: 68.75%,  total acc: 67.71%   [EVAL] batch:  338 | acc: 68.75%,  total acc: 67.72%   [EVAL] batch:  339 | acc: 68.75%,  total acc: 67.72%   [EVAL] batch:  340 | acc: 87.50%,  total acc: 67.78%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 67.80%   [EVAL] batch:  342 | acc: 56.25%,  total acc: 67.77%   [EVAL] batch:  343 | acc: 37.50%,  total acc: 67.68%   [EVAL] batch:  344 | acc: 68.75%,  total acc: 67.68%   [EVAL] batch:  345 | acc: 56.25%,  total acc: 67.65%   [EVAL] batch:  346 | acc: 62.50%,  total acc: 67.63%   [EVAL] batch:  347 | acc: 68.75%,  total acc: 67.64%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 67.66%   [EVAL] batch:  349 | acc: 56.25%,  total acc: 67.62%   [EVAL] batch:  350 | acc: 62.50%,  total acc: 67.61%   [EVAL] batch:  351 | acc: 68.75%,  total acc: 67.61%   [EVAL] batch:  352 | acc: 75.00%,  total acc: 67.63%   [EVAL] batch:  353 | acc: 75.00%,  total acc: 67.66%   [EVAL] batch:  354 | acc: 75.00%,  total acc: 67.68%   [EVAL] batch:  355 | acc: 81.25%,  total acc: 67.71%   [EVAL] batch:  356 | acc: 56.25%,  total acc: 67.68%   [EVAL] batch:  357 | acc: 25.00%,  total acc: 67.56%   [EVAL] batch:  358 | acc: 25.00%,  total acc: 67.44%   [EVAL] batch:  359 | acc: 43.75%,  total acc: 67.38%   [EVAL] batch:  360 | acc: 37.50%,  total acc: 67.30%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 67.25%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 67.25%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 67.34%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 67.43%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 67.52%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 67.61%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 67.70%   [EVAL] batch:  368 | acc: 93.75%,  total acc: 67.77%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 67.84%   [EVAL] batch:  370 | acc: 81.25%,  total acc: 67.87%   [EVAL] batch:  371 | acc: 93.75%,  total acc: 67.94%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 68.00%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 68.05%   [EVAL] batch:  374 | acc: 68.75%,  total acc: 68.05%   [EVAL] batch:  375 | acc: 25.00%,  total acc: 67.94%   [EVAL] batch:  376 | acc: 31.25%,  total acc: 67.84%   [EVAL] batch:  377 | acc: 43.75%,  total acc: 67.77%   [EVAL] batch:  378 | acc: 50.00%,  total acc: 67.73%   [EVAL] batch:  379 | acc: 43.75%,  total acc: 67.66%   [EVAL] batch:  380 | acc: 75.00%,  total acc: 67.68%   [EVAL] batch:  381 | acc: 37.50%,  total acc: 67.60%   [EVAL] batch:  382 | acc: 68.75%,  total acc: 67.61%   [EVAL] batch:  383 | acc: 62.50%,  total acc: 67.59%   [EVAL] batch:  384 | acc: 56.25%,  total acc: 67.56%   [EVAL] batch:  385 | acc: 50.00%,  total acc: 67.52%   [EVAL] batch:  386 | acc: 68.75%,  total acc: 67.52%   [EVAL] batch:  387 | acc: 100.00%,  total acc: 67.61%   [EVAL] batch:  388 | acc: 75.00%,  total acc: 67.63%   [EVAL] batch:  389 | acc: 68.75%,  total acc: 67.63%   [EVAL] batch:  390 | acc: 81.25%,  total acc: 67.66%   [EVAL] batch:  391 | acc: 87.50%,  total acc: 67.71%   [EVAL] batch:  392 | acc: 93.75%,  total acc: 67.78%   [EVAL] batch:  393 | acc: 87.50%,  total acc: 67.83%   [EVAL] batch:  394 | acc: 81.25%,  total acc: 67.86%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 67.95%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 68.03%   [EVAL] batch:  397 | acc: 93.75%,  total acc: 68.09%   [EVAL] batch:  398 | acc: 87.50%,  total acc: 68.14%   [EVAL] batch:  399 | acc: 93.75%,  total acc: 68.20%   [EVAL] batch:  400 | acc: 50.00%,  total acc: 68.16%   [EVAL] batch:  401 | acc: 18.75%,  total acc: 68.03%   [EVAL] batch:  402 | acc: 50.00%,  total acc: 67.99%   [EVAL] batch:  403 | acc: 62.50%,  total acc: 67.98%   [EVAL] batch:  404 | acc: 37.50%,  total acc: 67.90%   [EVAL] batch:  405 | acc: 37.50%,  total acc: 67.83%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 67.81%   [EVAL] batch:  407 | acc: 50.00%,  total acc: 67.77%   [EVAL] batch:  408 | acc: 43.75%,  total acc: 67.71%   [EVAL] batch:  409 | acc: 31.25%,  total acc: 67.62%   [EVAL] batch:  410 | acc: 18.75%,  total acc: 67.50%   [EVAL] batch:  411 | acc: 25.00%,  total acc: 67.40%   [EVAL] batch:  412 | acc: 68.75%,  total acc: 67.40%   [EVAL] batch:  413 | acc: 87.50%,  total acc: 67.45%   [EVAL] batch:  414 | acc: 87.50%,  total acc: 67.50%   [EVAL] batch:  415 | acc: 81.25%,  total acc: 67.53%   [EVAL] batch:  416 | acc: 93.75%,  total acc: 67.60%   [EVAL] batch:  417 | acc: 87.50%,  total acc: 67.64%   [EVAL] batch:  418 | acc: 87.50%,  total acc: 67.69%   [EVAL] batch:  419 | acc: 68.75%,  total acc: 67.69%   [EVAL] batch:  420 | acc: 62.50%,  total acc: 67.68%   [EVAL] batch:  421 | acc: 56.25%,  total acc: 67.65%   [EVAL] batch:  422 | acc: 56.25%,  total acc: 67.63%   [EVAL] batch:  423 | acc: 75.00%,  total acc: 67.64%   [EVAL] batch:  424 | acc: 87.50%,  total acc: 67.69%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 67.77%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 67.84%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 67.92%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 67.99%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 68.07%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 68.14%   [EVAL] batch:  431 | acc: 87.50%,  total acc: 68.19%   [EVAL] batch:  432 | acc: 93.75%,  total acc: 68.24%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 68.29%   [EVAL] batch:  434 | acc: 93.75%,  total acc: 68.35%   [EVAL] batch:  435 | acc: 81.25%,  total acc: 68.38%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 68.41%   
cur_acc:  ['0.9454', '0.7867', '0.7738', '0.6647', '0.8800', '0.6677', '0.7054']
his_acc:  ['0.9454', '0.8590', '0.8022', '0.7362', '0.7340', '0.7128', '0.6841']
CurrentTrain: epoch 15, batch     0 | loss: 13.2198403CurrentTrain: epoch 15, batch     1 | loss: 14.9900020CurrentTrain: epoch 15, batch     2 | loss: 10.5459118CurrentTrain: epoch  1, batch     3 | loss: 10.8244797CurrentTrain: epoch 15, batch     0 | loss: 14.8452395CurrentTrain: epoch 15, batch     1 | loss: 17.6802762CurrentTrain: epoch 15, batch     2 | loss: 12.2843485CurrentTrain: epoch  1, batch     3 | loss: 18.1967840CurrentTrain: epoch 15, batch     0 | loss: 10.5136216CurrentTrain: epoch 15, batch     1 | loss: 7.3939760CurrentTrain: epoch 15, batch     2 | loss: 11.0602070CurrentTrain: epoch  1, batch     3 | loss: 11.0119123CurrentTrain: epoch 15, batch     0 | loss: 9.8403584CurrentTrain: epoch 15, batch     1 | loss: 10.9268193CurrentTrain: epoch 15, batch     2 | loss: 10.7539332CurrentTrain: epoch  1, batch     3 | loss: 19.1362264CurrentTrain: epoch 15, batch     0 | loss: 8.6508598CurrentTrain: epoch 15, batch     1 | loss: 9.6358169CurrentTrain: epoch 15, batch     2 | loss: 7.9941749CurrentTrain: epoch  1, batch     3 | loss: 7.9154360CurrentTrain: epoch 15, batch     0 | loss: 7.0407625CurrentTrain: epoch 15, batch     1 | loss: 7.0714821CurrentTrain: epoch 15, batch     2 | loss: 6.7442346CurrentTrain: epoch  1, batch     3 | loss: 8.2343139CurrentTrain: epoch 15, batch     0 | loss: 8.4865999CurrentTrain: epoch 15, batch     1 | loss: 15.5456434CurrentTrain: epoch 15, batch     2 | loss: 28.0474136CurrentTrain: epoch  1, batch     3 | loss: 7.0859390CurrentTrain: epoch 15, batch     0 | loss: 9.5984232CurrentTrain: epoch 15, batch     1 | loss: 9.4744855CurrentTrain: epoch 15, batch     2 | loss: 8.0107342CurrentTrain: epoch  1, batch     3 | loss: 6.5512698CurrentTrain: epoch 15, batch     0 | loss: 8.3796137CurrentTrain: epoch 15, batch     1 | loss: 11.8847569CurrentTrain: epoch 15, batch     2 | loss: 9.7338519CurrentTrain: epoch  1, batch     3 | loss: 6.1781785CurrentTrain: epoch 15, batch     0 | loss: 6.8420646CurrentTrain: epoch 15, batch     1 | loss: 8.1384004CurrentTrain: epoch 15, batch     2 | loss: 7.6067696CurrentTrain: epoch  1, batch     3 | loss: 6.6653960
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: the andesite line , a zone of intense volcanic and seismic activity , is a major regional distinction in the pacific .
Head Entity: andesite line
Tail Entity: pacific
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The Great Barrier Reef, a stunning natural wonder, stretches along the northeastern coast of Australia.  
Head Entity: Great Barrier Reef  
Tail Entity: Australia  

Relation: located on terrain feature  
Context: Mount Everest, the highest peak in the world, is part of the Himalayas and attracts climbers from around the globe.  
Head Entity: Mount Everest  
Tail Entity: Himalayas  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, also known as cop26, took place in glasgow, scotland, where numerous world leaders gathered to discuss climate action.  
Head Entity: united nations climate change conference  
Tail Entity: cop26  

Relation: participant of  
Context: elon musk, the ceo of spacex, was a key participant in the 2021 met gala, showcasing his vision for sustainable energy and space exploration.  
Head Entity: elon musk  
Tail Entity: 2021 met gala  
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: luna subsequently had a supporting role in the comedy film " gravy " , directed by james roday , which was released in october 2015 .
Head Entity: gravy
Tail Entity: james roday
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: the acclaimed drama series "the crown" was brought to life by the visionary director peter morgan, who skillfully navigated the complexities of the royal family.  
Head Entity: the crown  
Tail Entity: peter morgan  

Relation: director  
Context: in the thrilling action movie "inception," the brilliant director christopher nolan crafted a mind-bending narrative that captivated audiences worldwide.  
Head Entity: inception  
Tail Entity: christopher nolan  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: the station is owned by the hearst television subsidiary of the hearst corporation .
Head Entity: hearst television
Tail Entity: hearst corporation
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: the famous painting is owned by the metropolitan museum of art in new york.  
Head Entity: metropolitan museum of art  
Tail Entity: famous painting  

Relation: owned by  
Context: the luxury car brand is owned by a major automotive conglomerate based in germany.  
Head Entity: major automotive conglomerate  
Tail Entity: luxury car brand  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: thessaly was home to extensive neolithic and chalcolithic cultures around 6000 bc–2500 bc ( see cardium pottery , dimini and sesklo ) .
Head Entity: cardium pottery
Tail Entity: neolithic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The engine is a crucial part of the car's overall functionality, providing the necessary power to drive.  
Head Entity: engine  
Tail Entity: car  

Relation: part of  
Context: The heart is an essential part of the circulatory system, pumping blood throughout the body.  
Head Entity: heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: the brewery closed in 1996 and is now the site of the halliwell jones stadium , home of warrington wolves , as well as a tesco store .
Head Entity: halliwell jones stadium
Tail Entity: warrington wolves
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was transformed into a cultural center, which now hosts the local art gallery and serves as the headquarters for the community theater group.  
Head Entity: cultural center  
Tail Entity: community theater group  

Relation: occupant  
Context: After the renovation, the old factory became a vibrant co-working space, attracting startups and freelancers from various industries.  
Head Entity: co-working space  
Tail Entity: startups
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: reading town hall was built in several phases between 1786 and 1897 , although the principal facade was designed by alfred waterhouse in 1875 .
Head Entity: reading town hall
Tail Entity: alfred waterhouse
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: the new art museum was designed by renowned architect zaha hadid, whose innovative style has transformed modern architecture.  
Head Entity: new art museum  
Tail Entity: zaha hadid  

Relation: architect  
Context: the iconic sydney opera house was the brainchild of architect jørn utzon, who won the design competition in 1957.  
Head Entity: sydney opera house  
Tail Entity: jørn utzon  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: cyrus alexander was born in pennsylvania , and his family soon moved to illinois .
Head Entity: cyrus alexander
Tail Entity: illinois
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: after living in new york for several years, emily decided to relocate to california for a fresh start.  
Head Entity: emily  
Tail Entity: california  

Relation: residence  
Context: during his childhood, michael spent most of his time in texas before moving to florida as a teenager.  
Head Entity: michael  
Tail Entity: florida  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Ludwig van Beethoven created some of the most celebrated symphonies in classical music history.  
Head Entity: symphonies  
Tail Entity: Ludwig van Beethoven  

Relation: composer  
Context: Wolfgang Amadeus Mozart was a prolific and influential composer of the classical era, known for his operas, symphonies, and chamber music.  
Head Entity: operas  
Tail Entity: Wolfgang Amadeus Mozart  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: one polish archer qualified for the women 's individual recurve by obtaining one of the eight olympic places available from the 2015 world archery championships in copenhagen , denmark .
Head Entity: 2015 world archery championships
Tail Entity: copenhagen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: The annual music festival will take place in the beautiful city of Barcelona, Spain, attracting thousands of visitors from around the world.  
Head Entity: annual music festival  
Tail Entity: Barcelona  

Relation: location  
Context: The historic battle was fought near the town of Gettysburg, Pennsylvania, which is now a popular tourist destination.  
Head Entity: historic battle  
Tail Entity: Gettysburg  
MemoryTrain:  epoch 15, batch     0 | loss: 2.5973065MemoryTrain:  epoch 15, batch     1 | loss: 2.8941042MemoryTrain:  epoch 15, batch     2 | loss: 2.0587868MemoryTrain:  epoch 15, batch     3 | loss: 2.5154062MemoryTrain:  epoch 15, batch     4 | loss: 3.0395947MemoryTrain:  epoch 15, batch     5 | loss: 4.3009947MemoryTrain:  epoch 15, batch     6 | loss: 4.8267819MemoryTrain:  epoch 15, batch     7 | loss: 3.1663111MemoryTrain:  epoch 15, batch     8 | loss: 2.2600782MemoryTrain:  epoch 15, batch     9 | loss: 2.8068335MemoryTrain:  epoch 15, batch    10 | loss: 4.1398642MemoryTrain:  epoch 15, batch    11 | loss: 2.9563101MemoryTrain:  epoch 15, batch    12 | loss: 3.7692821MemoryTrain:  epoch 15, batch    13 | loss: 2.8000039MemoryTrain:  epoch 15, batch    14 | loss: 3.2203504MemoryTrain:  epoch 15, batch     0 | loss: 2.8685571MemoryTrain:  epoch 15, batch     1 | loss: 1.7846260MemoryTrain:  epoch 15, batch     2 | loss: 2.5338823MemoryTrain:  epoch 15, batch     3 | loss: 3.2294399MemoryTrain:  epoch 15, batch     4 | loss: 4.8920183MemoryTrain:  epoch 15, batch     5 | loss: 2.8844871MemoryTrain:  epoch 15, batch     6 | loss: 2.6298768MemoryTrain:  epoch 15, batch     7 | loss: 2.6141541MemoryTrain:  epoch 15, batch     8 | loss: 2.9263324MemoryTrain:  epoch 15, batch     9 | loss: 3.0902905MemoryTrain:  epoch 15, batch    10 | loss: 2.6070267MemoryTrain:  epoch 15, batch    11 | loss: 1.5587668MemoryTrain:  epoch 15, batch    12 | loss: 4.8774336MemoryTrain:  epoch 15, batch    13 | loss: 2.4966379MemoryTrain:  epoch 15, batch    14 | loss: 2.0154580MemoryTrain:  epoch 15, batch     0 | loss: 1.8246646MemoryTrain:  epoch 15, batch     1 | loss: 3.5838773MemoryTrain:  epoch 15, batch     2 | loss: 2.7572778MemoryTrain:  epoch 15, batch     3 | loss: 1.8217684MemoryTrain:  epoch 15, batch     4 | loss: 2.0494137MemoryTrain:  epoch 15, batch     5 | loss: 1.5519134MemoryTrain:  epoch 15, batch     6 | loss: 1.8429812MemoryTrain:  epoch 15, batch     7 | loss: 2.0336728MemoryTrain:  epoch 15, batch     8 | loss: 2.5281955MemoryTrain:  epoch 15, batch     9 | loss: 2.7921506MemoryTrain:  epoch 15, batch    10 | loss: 1.9486723MemoryTrain:  epoch 15, batch    11 | loss: 2.1519387MemoryTrain:  epoch 15, batch    12 | loss: 4.6444399MemoryTrain:  epoch 15, batch    13 | loss: 2.0241735MemoryTrain:  epoch 15, batch    14 | loss: 2.0961882MemoryTrain:  epoch 15, batch     0 | loss: 1.6562144MemoryTrain:  epoch 15, batch     1 | loss: 1.4859976MemoryTrain:  epoch 15, batch     2 | loss: 2.2372722MemoryTrain:  epoch 15, batch     3 | loss: 1.7306799MemoryTrain:  epoch 15, batch     4 | loss: 2.0260901MemoryTrain:  epoch 15, batch     5 | loss: 3.9559311MemoryTrain:  epoch 15, batch     6 | loss: 3.8251353MemoryTrain:  epoch 15, batch     7 | loss: 2.3913823MemoryTrain:  epoch 15, batch     8 | loss: 2.2857104MemoryTrain:  epoch 15, batch     9 | loss: 4.1588091MemoryTrain:  epoch 15, batch    10 | loss: 2.5777461MemoryTrain:  epoch 15, batch    11 | loss: 3.0042193MemoryTrain:  epoch 15, batch    12 | loss: 4.1723738MemoryTrain:  epoch 15, batch    13 | loss: 2.1013560MemoryTrain:  epoch 15, batch    14 | loss: 2.0867969MemoryTrain:  epoch 15, batch     0 | loss: 4.3096463MemoryTrain:  epoch 15, batch     1 | loss: 3.9360011MemoryTrain:  epoch 15, batch     2 | loss: 1.6394728MemoryTrain:  epoch 15, batch     3 | loss: 1.3509613MemoryTrain:  epoch 15, batch     4 | loss: 1.8007075MemoryTrain:  epoch 15, batch     5 | loss: 2.1164179MemoryTrain:  epoch 15, batch     6 | loss: 4.2690692MemoryTrain:  epoch 15, batch     7 | loss: 1.7350178MemoryTrain:  epoch 15, batch     8 | loss: 1.9850423MemoryTrain:  epoch 15, batch     9 | loss: 1.6128182MemoryTrain:  epoch 15, batch    10 | loss: 2.1603463MemoryTrain:  epoch 15, batch    11 | loss: 1.5584520MemoryTrain:  epoch 15, batch    12 | loss: 2.2057435MemoryTrain:  epoch 15, batch    13 | loss: 4.0293468MemoryTrain:  epoch 15, batch    14 | loss: 1.5045654MemoryTrain:  epoch 15, batch     0 | loss: 2.2133783MemoryTrain:  epoch 15, batch     1 | loss: 1.9480088MemoryTrain:  epoch 15, batch     2 | loss: 1.5009779MemoryTrain:  epoch 15, batch     3 | loss: 2.9090995MemoryTrain:  epoch 15, batch     4 | loss: 1.6165063MemoryTrain:  epoch 15, batch     5 | loss: 1.6354794MemoryTrain:  epoch 15, batch     6 | loss: 2.6443241MemoryTrain:  epoch 15, batch     7 | loss: 3.9249961MemoryTrain:  epoch 15, batch     8 | loss: 2.0716562MemoryTrain:  epoch 15, batch     9 | loss: 1.6897666MemoryTrain:  epoch 15, batch    10 | loss: 1.8248235MemoryTrain:  epoch 15, batch    11 | loss: 1.6977005MemoryTrain:  epoch 15, batch    12 | loss: 1.3254638MemoryTrain:  epoch 15, batch    13 | loss: 1.8836118MemoryTrain:  epoch 15, batch    14 | loss: 2.4790011MemoryTrain:  epoch 15, batch     0 | loss: 3.7503470MemoryTrain:  epoch 15, batch     1 | loss: 1.6493344MemoryTrain:  epoch 15, batch     2 | loss: 1.6019923MemoryTrain:  epoch 15, batch     3 | loss: 1.4753946MemoryTrain:  epoch 15, batch     4 | loss: 1.5463227MemoryTrain:  epoch 15, batch     5 | loss: 1.7451659MemoryTrain:  epoch 15, batch     6 | loss: 1.5854355MemoryTrain:  epoch 15, batch     7 | loss: 1.7778227MemoryTrain:  epoch 15, batch     8 | loss: 1.5827765MemoryTrain:  epoch 15, batch     9 | loss: 3.7910412MemoryTrain:  epoch 15, batch    10 | loss: 1.8896289MemoryTrain:  epoch 15, batch    11 | loss: 1.8062465MemoryTrain:  epoch 15, batch    12 | loss: 3.9925505MemoryTrain:  epoch 15, batch    13 | loss: 1.4208482MemoryTrain:  epoch 15, batch    14 | loss: 1.6141761MemoryTrain:  epoch 15, batch     0 | loss: 4.0454757MemoryTrain:  epoch 15, batch     1 | loss: 1.3649546MemoryTrain:  epoch 15, batch     2 | loss: 1.3479090MemoryTrain:  epoch 15, batch     3 | loss: 1.5347009MemoryTrain:  epoch 15, batch     4 | loss: 1.4576480MemoryTrain:  epoch 15, batch     5 | loss: 3.9058872MemoryTrain:  epoch 15, batch     6 | loss: 1.6738493MemoryTrain:  epoch 15, batch     7 | loss: 1.5991842MemoryTrain:  epoch 15, batch     8 | loss: 1.2991713MemoryTrain:  epoch 15, batch     9 | loss: 1.7841508MemoryTrain:  epoch 15, batch    10 | loss: 1.6084776MemoryTrain:  epoch 15, batch    11 | loss: 1.3423848MemoryTrain:  epoch 15, batch    12 | loss: 1.6581362MemoryTrain:  epoch 15, batch    13 | loss: 1.6898745MemoryTrain:  epoch 15, batch    14 | loss: 1.6912135MemoryTrain:  epoch 15, batch     0 | loss: 1.7216041MemoryTrain:  epoch 15, batch     1 | loss: 1.4768541MemoryTrain:  epoch 15, batch     2 | loss: 1.5854569MemoryTrain:  epoch 15, batch     3 | loss: 1.4969478MemoryTrain:  epoch 15, batch     4 | loss: 1.6544934MemoryTrain:  epoch 15, batch     5 | loss: 3.8947098MemoryTrain:  epoch 15, batch     6 | loss: 4.1265908MemoryTrain:  epoch 15, batch     7 | loss: 1.5381199MemoryTrain:  epoch 15, batch     8 | loss: 1.5966548MemoryTrain:  epoch 15, batch     9 | loss: 2.3294465MemoryTrain:  epoch 15, batch    10 | loss: 1.5059982MemoryTrain:  epoch 15, batch    11 | loss: 2.2556400MemoryTrain:  epoch 15, batch    12 | loss: 1.5569929MemoryTrain:  epoch 15, batch    13 | loss: 1.4950851MemoryTrain:  epoch 15, batch    14 | loss: 1.7718054MemoryTrain:  epoch 15, batch     0 | loss: 1.3081234MemoryTrain:  epoch 15, batch     1 | loss: 1.8529656MemoryTrain:  epoch 15, batch     2 | loss: 1.3036922MemoryTrain:  epoch 15, batch     3 | loss: 6.2502314MemoryTrain:  epoch 15, batch     4 | loss: 1.2886425MemoryTrain:  epoch 15, batch     5 | loss: 1.7467587MemoryTrain:  epoch 15, batch     6 | loss: 2.3526065MemoryTrain:  epoch 15, batch     7 | loss: 1.5781445MemoryTrain:  epoch 15, batch     8 | loss: 1.6719339MemoryTrain:  epoch 15, batch     9 | loss: 1.4592121MemoryTrain:  epoch 15, batch    10 | loss: 1.6591591MemoryTrain:  epoch 15, batch    11 | loss: 1.5601069MemoryTrain:  epoch 15, batch    12 | loss: 4.2266512MemoryTrain:  epoch 15, batch    13 | loss: 4.4427477MemoryTrain:  epoch 15, batch    14 | loss: 1.6589975
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 22.92%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 25.00%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 23.75%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 23.96%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 30.36%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 36.72%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 42.36%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 45.62%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 48.30%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 51.56%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 53.85%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 55.80%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 57.50%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 59.77%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 61.03%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 62.15%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 62.17%   [EVAL] batch:   19 | acc: 12.50%,  total acc: 59.69%   [EVAL] batch:   20 | acc: 6.25%,  total acc: 57.14%   [EVAL] batch:   21 | acc: 12.50%,  total acc: 55.11%   [EVAL] batch:   22 | acc: 25.00%,  total acc: 53.80%   [EVAL] batch:   23 | acc: 6.25%,  total acc: 51.82%   [EVAL] batch:   24 | acc: 18.75%,  total acc: 50.50%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 48.80%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 47.45%   [EVAL] batch:   27 | acc: 12.50%,  total acc: 46.21%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 44.83%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 43.75%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 42.54%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 43.36%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 44.89%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 45.77%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 47.32%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 48.44%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 49.32%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 50.49%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 51.60%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 52.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 53.96%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 54.61%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 55.67%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 56.11%   [EVAL] batch:   44 | acc: 12.50%,  total acc: 55.14%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 54.08%   [EVAL] batch:   46 | acc: 6.25%,  total acc: 53.06%   [EVAL] batch:   47 | acc: 12.50%,  total acc: 52.21%   [EVAL] batch:   48 | acc: 6.25%,  total acc: 51.28%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 50.38%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 50.25%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 50.72%   [EVAL] batch:   52 | acc: 37.50%,  total acc: 50.47%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 50.93%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 51.25%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 51.56%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 51.64%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 51.62%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 51.48%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 51.98%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 51.95%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 52.32%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 51.88%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 31.25%,  total acc: 67.97%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 65.28%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 62.50%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 59.66%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 57.29%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 57.21%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 59.82%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 61.67%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 63.67%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 65.81%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 67.36%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 68.42%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 68.44%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 69.05%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 69.03%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 69.01%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 69.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 70.67%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 71.76%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 72.54%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 73.28%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 74.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 75.59%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 76.14%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 75.74%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 75.71%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 75.69%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 75.84%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 76.15%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 76.76%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 77.90%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 78.27%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 78.63%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 78.69%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 78.75%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 78.53%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 78.06%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 77.73%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 78.06%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 78.19%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 78.25%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 78.18%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 78.01%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 77.61%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 77.68%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 77.41%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 76.83%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 76.59%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 76.67%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 76.64%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 76.61%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 76.29%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 75.59%   [EVAL] batch:   64 | acc: 43.75%,  total acc: 75.10%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 74.91%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 74.35%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 73.90%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 73.46%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 73.48%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 73.24%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 73.09%   [EVAL] batch:   72 | acc: 62.50%,  total acc: 72.95%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 72.72%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 72.50%   [EVAL] batch:   75 | acc: 18.75%,  total acc: 71.79%   [EVAL] batch:   76 | acc: 0.00%,  total acc: 70.86%   [EVAL] batch:   77 | acc: 6.25%,  total acc: 70.03%   [EVAL] batch:   78 | acc: 18.75%,  total acc: 69.38%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 69.06%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 68.29%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 68.37%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 69.12%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 69.41%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 69.77%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 70.04%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 70.03%   [EVAL] batch:   88 | acc: 6.25%,  total acc: 69.31%   [EVAL] batch:   89 | acc: 12.50%,  total acc: 68.68%   [EVAL] batch:   90 | acc: 18.75%,  total acc: 68.13%   [EVAL] batch:   91 | acc: 37.50%,  total acc: 67.80%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 67.14%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 66.82%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 67.11%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 67.38%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 67.65%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 67.86%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 68.06%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 68.38%   [EVAL] batch:  100 | acc: 0.00%,  total acc: 67.70%   [EVAL] batch:  101 | acc: 0.00%,  total acc: 67.03%   [EVAL] batch:  102 | acc: 0.00%,  total acc: 66.38%   [EVAL] batch:  103 | acc: 0.00%,  total acc: 65.75%   [EVAL] batch:  104 | acc: 0.00%,  total acc: 65.12%   [EVAL] batch:  105 | acc: 0.00%,  total acc: 64.50%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 64.31%   [EVAL] batch:  107 | acc: 56.25%,  total acc: 64.24%   [EVAL] batch:  108 | acc: 68.75%,  total acc: 64.28%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 64.09%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 64.02%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 63.90%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 64.10%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 64.04%   [EVAL] batch:  114 | acc: 75.00%,  total acc: 64.13%   [EVAL] batch:  115 | acc: 68.75%,  total acc: 64.17%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 64.32%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 64.30%   [EVAL] batch:  118 | acc: 50.00%,  total acc: 64.18%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 64.43%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 64.62%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 64.81%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 64.94%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 65.07%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 65.25%   [EVAL] batch:  125 | acc: 12.50%,  total acc: 64.83%   [EVAL] batch:  126 | acc: 18.75%,  total acc: 64.47%   [EVAL] batch:  127 | acc: 12.50%,  total acc: 64.06%   [EVAL] batch:  128 | acc: 0.00%,  total acc: 63.57%   [EVAL] batch:  129 | acc: 6.25%,  total acc: 63.12%   [EVAL] batch:  130 | acc: 12.50%,  total acc: 62.74%   [EVAL] batch:  131 | acc: 43.75%,  total acc: 62.59%   [EVAL] batch:  132 | acc: 75.00%,  total acc: 62.69%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 62.83%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 63.01%   [EVAL] batch:  135 | acc: 75.00%,  total acc: 63.10%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 63.23%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 63.27%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 63.22%   [EVAL] batch:  139 | acc: 43.75%,  total acc: 63.08%   [EVAL] batch:  140 | acc: 68.75%,  total acc: 63.12%   [EVAL] batch:  141 | acc: 50.00%,  total acc: 63.03%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 63.11%   [EVAL] batch:  143 | acc: 56.25%,  total acc: 63.06%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 63.28%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 63.53%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 63.73%   [EVAL] batch:  147 | acc: 93.75%,  total acc: 63.94%   [EVAL] batch:  148 | acc: 93.75%,  total acc: 64.14%   [EVAL] batch:  149 | acc: 81.25%,  total acc: 64.25%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 64.45%   [EVAL] batch:  151 | acc: 87.50%,  total acc: 64.60%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 64.67%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 64.81%   [EVAL] batch:  154 | acc: 81.25%,  total acc: 64.92%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 65.06%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 65.25%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 65.47%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 65.64%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 65.78%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 65.95%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 66.13%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 66.10%   [EVAL] batch:  163 | acc: 37.50%,  total acc: 65.93%   [EVAL] batch:  164 | acc: 25.00%,  total acc: 65.68%   [EVAL] batch:  165 | acc: 31.25%,  total acc: 65.47%   [EVAL] batch:  166 | acc: 12.50%,  total acc: 65.16%   [EVAL] batch:  167 | acc: 12.50%,  total acc: 64.84%   [EVAL] batch:  168 | acc: 31.25%,  total acc: 64.64%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 64.74%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 64.84%   [EVAL] batch:  171 | acc: 93.75%,  total acc: 65.01%   [EVAL] batch:  172 | acc: 93.75%,  total acc: 65.17%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 65.23%   [EVAL] batch:  174 | acc: 75.00%,  total acc: 65.29%   [EVAL] batch:  175 | acc: 43.75%,  total acc: 65.16%   [EVAL] batch:  176 | acc: 31.25%,  total acc: 64.97%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 64.82%   [EVAL] batch:  178 | acc: 12.50%,  total acc: 64.53%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 64.51%   [EVAL] batch:  180 | acc: 25.00%,  total acc: 64.30%   [EVAL] batch:  181 | acc: 50.00%,  total acc: 64.22%   [EVAL] batch:  182 | acc: 62.50%,  total acc: 64.21%   [EVAL] batch:  183 | acc: 50.00%,  total acc: 64.13%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 64.12%   [EVAL] batch:  185 | acc: 56.25%,  total acc: 64.08%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 64.14%   [EVAL] batch:  187 | acc: 25.00%,  total acc: 63.93%   [EVAL] batch:  188 | acc: 6.25%,  total acc: 63.62%   [EVAL] batch:  189 | acc: 6.25%,  total acc: 63.32%   [EVAL] batch:  190 | acc: 12.50%,  total acc: 63.06%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 62.83%   [EVAL] batch:  192 | acc: 31.25%,  total acc: 62.66%   [EVAL] batch:  193 | acc: 31.25%,  total acc: 62.50%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 62.60%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 62.60%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 62.60%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 62.72%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 62.78%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 62.88%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 62.94%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 63.03%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 63.02%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 63.05%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 63.14%   [EVAL] batch:  205 | acc: 87.50%,  total acc: 63.26%   [EVAL] batch:  206 | acc: 43.75%,  total acc: 63.16%   [EVAL] batch:  207 | acc: 18.75%,  total acc: 62.95%   [EVAL] batch:  208 | acc: 12.50%,  total acc: 62.71%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 62.53%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 62.38%   [EVAL] batch:  211 | acc: 25.00%,  total acc: 62.21%   [EVAL] batch:  212 | acc: 43.75%,  total acc: 62.12%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 62.27%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 62.44%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 62.56%   [EVAL] batch:  216 | acc: 93.75%,  total acc: 62.70%   [EVAL] batch:  217 | acc: 87.50%,  total acc: 62.82%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 62.99%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 63.15%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 63.32%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 63.49%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 63.65%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 63.81%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 63.97%   [EVAL] batch:  225 | acc: 93.75%,  total acc: 64.10%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 64.21%   [EVAL] batch:  227 | acc: 93.75%,  total acc: 64.34%   [EVAL] batch:  228 | acc: 81.25%,  total acc: 64.41%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 64.57%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 64.69%   [EVAL] batch:  231 | acc: 50.00%,  total acc: 64.63%   [EVAL] batch:  232 | acc: 31.25%,  total acc: 64.48%   [EVAL] batch:  233 | acc: 25.00%,  total acc: 64.32%   [EVAL] batch:  234 | acc: 25.00%,  total acc: 64.15%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 64.12%   [EVAL] batch:  236 | acc: 31.25%,  total acc: 63.98%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 64.02%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 64.12%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 64.27%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 64.37%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 64.46%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 64.56%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 64.63%   [EVAL] batch:  244 | acc: 18.75%,  total acc: 64.44%   [EVAL] batch:  245 | acc: 18.75%,  total acc: 64.25%   [EVAL] batch:  246 | acc: 12.50%,  total acc: 64.04%   [EVAL] batch:  247 | acc: 31.25%,  total acc: 63.91%   [EVAL] batch:  248 | acc: 0.00%,  total acc: 63.65%   [EVAL] batch:  249 | acc: 31.25%,  total acc: 63.52%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 63.65%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 63.76%   [EVAL] batch:  252 | acc: 100.00%,  total acc: 63.91%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 64.00%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 64.12%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 64.26%   [EVAL] batch:  256 | acc: 87.50%,  total acc: 64.35%   [EVAL] batch:  257 | acc: 93.75%,  total acc: 64.46%   [EVAL] batch:  258 | acc: 81.25%,  total acc: 64.53%   [EVAL] batch:  259 | acc: 87.50%,  total acc: 64.62%   [EVAL] batch:  260 | acc: 75.00%,  total acc: 64.66%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 64.72%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 64.78%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 64.82%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 64.81%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 64.78%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 64.75%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 64.74%   [EVAL] batch:  268 | acc: 50.00%,  total acc: 64.68%   [EVAL] batch:  269 | acc: 31.25%,  total acc: 64.56%   [EVAL] batch:  270 | acc: 50.00%,  total acc: 64.51%   [EVAL] batch:  271 | acc: 43.75%,  total acc: 64.43%   [EVAL] batch:  272 | acc: 31.25%,  total acc: 64.31%   [EVAL] batch:  273 | acc: 50.00%,  total acc: 64.26%   [EVAL] batch:  274 | acc: 37.50%,  total acc: 64.16%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 64.29%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 64.42%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 64.55%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 64.67%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 64.80%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 64.92%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 64.94%   [EVAL] batch:  282 | acc: 75.00%,  total acc: 64.97%   [EVAL] batch:  283 | acc: 68.75%,  total acc: 64.99%   [EVAL] batch:  284 | acc: 68.75%,  total acc: 65.00%   [EVAL] batch:  285 | acc: 68.75%,  total acc: 65.01%   [EVAL] batch:  286 | acc: 62.50%,  total acc: 65.00%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 65.02%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 65.14%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 65.26%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 65.38%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 65.50%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 65.61%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 65.71%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 65.83%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 65.94%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 66.06%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 66.17%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 66.28%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 66.40%   [EVAL] batch:  300 | acc: 87.50%,  total acc: 66.47%   [EVAL] batch:  301 | acc: 87.50%,  total acc: 66.54%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 66.63%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 66.71%   [EVAL] batch:  304 | acc: 87.50%,  total acc: 66.78%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 66.87%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 66.96%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 67.05%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 67.11%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 67.32%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 67.37%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 67.31%   [EVAL] batch:  313 | acc: 37.50%,  total acc: 67.22%   [EVAL] batch:  314 | acc: 50.00%,  total acc: 67.16%   [EVAL] batch:  315 | acc: 43.75%,  total acc: 67.09%   [EVAL] batch:  316 | acc: 50.00%,  total acc: 67.03%   [EVAL] batch:  317 | acc: 56.25%,  total acc: 67.00%   [EVAL] batch:  318 | acc: 68.75%,  total acc: 67.01%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 67.11%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 67.21%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 67.27%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 67.36%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 67.44%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 67.50%   [EVAL] batch:  325 | acc: 62.50%,  total acc: 67.48%   [EVAL] batch:  326 | acc: 43.75%,  total acc: 67.41%   [EVAL] batch:  327 | acc: 62.50%,  total acc: 67.40%   [EVAL] batch:  328 | acc: 50.00%,  total acc: 67.34%   [EVAL] batch:  329 | acc: 68.75%,  total acc: 67.35%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 67.35%   [EVAL] batch:  331 | acc: 37.50%,  total acc: 67.26%   [EVAL] batch:  332 | acc: 31.25%,  total acc: 67.15%   [EVAL] batch:  333 | acc: 12.50%,  total acc: 66.99%   [EVAL] batch:  334 | acc: 12.50%,  total acc: 66.83%   [EVAL] batch:  335 | acc: 18.75%,  total acc: 66.69%   [EVAL] batch:  336 | acc: 12.50%,  total acc: 66.52%   [EVAL] batch:  337 | acc: 50.00%,  total acc: 66.48%   [EVAL] batch:  338 | acc: 68.75%,  total acc: 66.48%   [EVAL] batch:  339 | acc: 68.75%,  total acc: 66.49%   [EVAL] batch:  340 | acc: 81.25%,  total acc: 66.53%   [EVAL] batch:  341 | acc: 68.75%,  total acc: 66.54%   [EVAL] batch:  342 | acc: 56.25%,  total acc: 66.51%   [EVAL] batch:  343 | acc: 31.25%,  total acc: 66.41%   [EVAL] batch:  344 | acc: 68.75%,  total acc: 66.41%   [EVAL] batch:  345 | acc: 37.50%,  total acc: 66.33%   [EVAL] batch:  346 | acc: 62.50%,  total acc: 66.32%   [EVAL] batch:  347 | acc: 56.25%,  total acc: 66.29%   [EVAL] batch:  348 | acc: 68.75%,  total acc: 66.30%   [EVAL] batch:  349 | acc: 37.50%,  total acc: 66.21%   [EVAL] batch:  350 | acc: 56.25%,  total acc: 66.19%   [EVAL] batch:  351 | acc: 75.00%,  total acc: 66.21%   [EVAL] batch:  352 | acc: 68.75%,  total acc: 66.22%   [EVAL] batch:  353 | acc: 75.00%,  total acc: 66.24%   [EVAL] batch:  354 | acc: 75.00%,  total acc: 66.27%   [EVAL] batch:  355 | acc: 81.25%,  total acc: 66.31%   [EVAL] batch:  356 | acc: 62.50%,  total acc: 66.30%   [EVAL] batch:  357 | acc: 31.25%,  total acc: 66.20%   [EVAL] batch:  358 | acc: 31.25%,  total acc: 66.10%   [EVAL] batch:  359 | acc: 56.25%,  total acc: 66.08%   [EVAL] batch:  360 | acc: 37.50%,  total acc: 66.00%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 65.95%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 65.96%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 66.05%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 66.15%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 66.24%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 66.33%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 66.42%   [EVAL] batch:  368 | acc: 93.75%,  total acc: 66.50%   [EVAL] batch:  369 | acc: 100.00%,  total acc: 66.59%   [EVAL] batch:  370 | acc: 93.75%,  total acc: 66.66%   [EVAL] batch:  371 | acc: 93.75%,  total acc: 66.73%   [EVAL] batch:  372 | acc: 93.75%,  total acc: 66.81%   [EVAL] batch:  373 | acc: 100.00%,  total acc: 66.90%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 66.93%   [EVAL] batch:  375 | acc: 56.25%,  total acc: 66.90%   [EVAL] batch:  376 | acc: 43.75%,  total acc: 66.84%   [EVAL] batch:  377 | acc: 62.50%,  total acc: 66.83%   [EVAL] batch:  378 | acc: 62.50%,  total acc: 66.82%   [EVAL] batch:  379 | acc: 50.00%,  total acc: 66.78%   [EVAL] batch:  380 | acc: 81.25%,  total acc: 66.81%   [EVAL] batch:  381 | acc: 43.75%,  total acc: 66.75%   [EVAL] batch:  382 | acc: 68.75%,  total acc: 66.76%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 66.76%   [EVAL] batch:  384 | acc: 56.25%,  total acc: 66.74%   [EVAL] batch:  385 | acc: 50.00%,  total acc: 66.69%   [EVAL] batch:  386 | acc: 75.00%,  total acc: 66.72%   [EVAL] batch:  387 | acc: 93.75%,  total acc: 66.78%   [EVAL] batch:  388 | acc: 75.00%,  total acc: 66.81%   [EVAL] batch:  389 | acc: 81.25%,  total acc: 66.84%   [EVAL] batch:  390 | acc: 81.25%,  total acc: 66.88%   [EVAL] batch:  391 | acc: 93.75%,  total acc: 66.95%   [EVAL] batch:  392 | acc: 93.75%,  total acc: 67.02%   [EVAL] batch:  393 | acc: 87.50%,  total acc: 67.07%   [EVAL] batch:  394 | acc: 87.50%,  total acc: 67.12%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 67.20%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 67.29%   [EVAL] batch:  397 | acc: 93.75%,  total acc: 67.35%   [EVAL] batch:  398 | acc: 87.50%,  total acc: 67.40%   [EVAL] batch:  399 | acc: 93.75%,  total acc: 67.47%   [EVAL] batch:  400 | acc: 62.50%,  total acc: 67.46%   [EVAL] batch:  401 | acc: 37.50%,  total acc: 67.38%   [EVAL] batch:  402 | acc: 56.25%,  total acc: 67.35%   [EVAL] batch:  403 | acc: 68.75%,  total acc: 67.36%   [EVAL] batch:  404 | acc: 50.00%,  total acc: 67.31%   [EVAL] batch:  405 | acc: 43.75%,  total acc: 67.26%   [EVAL] batch:  406 | acc: 43.75%,  total acc: 67.20%   [EVAL] batch:  407 | acc: 50.00%,  total acc: 67.16%   [EVAL] batch:  408 | acc: 12.50%,  total acc: 67.02%   [EVAL] batch:  409 | acc: 25.00%,  total acc: 66.92%   [EVAL] batch:  410 | acc: 6.25%,  total acc: 66.77%   [EVAL] batch:  411 | acc: 6.25%,  total acc: 66.63%   [EVAL] batch:  412 | acc: 56.25%,  total acc: 66.60%   [EVAL] batch:  413 | acc: 81.25%,  total acc: 66.64%   [EVAL] batch:  414 | acc: 81.25%,  total acc: 66.67%   [EVAL] batch:  415 | acc: 81.25%,  total acc: 66.71%   [EVAL] batch:  416 | acc: 93.75%,  total acc: 66.77%   [EVAL] batch:  417 | acc: 87.50%,  total acc: 66.82%   [EVAL] batch:  418 | acc: 87.50%,  total acc: 66.87%   [EVAL] batch:  419 | acc: 75.00%,  total acc: 66.89%   [EVAL] batch:  420 | acc: 62.50%,  total acc: 66.88%   [EVAL] batch:  421 | acc: 62.50%,  total acc: 66.87%   [EVAL] batch:  422 | acc: 56.25%,  total acc: 66.84%   [EVAL] batch:  423 | acc: 75.00%,  total acc: 66.86%   [EVAL] batch:  424 | acc: 87.50%,  total acc: 66.91%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 66.99%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 67.07%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 67.14%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 67.30%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 67.37%   [EVAL] batch:  431 | acc: 87.50%,  total acc: 67.42%   [EVAL] batch:  432 | acc: 93.75%,  total acc: 67.48%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 67.53%   [EVAL] batch:  434 | acc: 93.75%,  total acc: 67.59%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 67.63%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 67.71%   [EVAL] batch:  437 | acc: 68.75%,  total acc: 67.71%   [EVAL] batch:  438 | acc: 18.75%,  total acc: 67.60%   [EVAL] batch:  439 | acc: 25.00%,  total acc: 67.50%   [EVAL] batch:  440 | acc: 25.00%,  total acc: 67.40%   [EVAL] batch:  441 | acc: 18.75%,  total acc: 67.29%   [EVAL] batch:  442 | acc: 18.75%,  total acc: 67.18%   [EVAL] batch:  443 | acc: 50.00%,  total acc: 67.15%   [EVAL] batch:  444 | acc: 75.00%,  total acc: 67.16%   [EVAL] batch:  445 | acc: 81.25%,  total acc: 67.19%   [EVAL] batch:  446 | acc: 81.25%,  total acc: 67.23%   [EVAL] batch:  447 | acc: 87.50%,  total acc: 67.27%   [EVAL] batch:  448 | acc: 68.75%,  total acc: 67.27%   [EVAL] batch:  449 | acc: 87.50%,  total acc: 67.32%   [EVAL] batch:  450 | acc: 93.75%,  total acc: 67.38%   [EVAL] batch:  451 | acc: 75.00%,  total acc: 67.39%   [EVAL] batch:  452 | acc: 81.25%,  total acc: 67.43%   [EVAL] batch:  453 | acc: 87.50%,  total acc: 67.47%   [EVAL] batch:  454 | acc: 75.00%,  total acc: 67.49%   [EVAL] batch:  455 | acc: 93.75%,  total acc: 67.54%   [EVAL] batch:  456 | acc: 31.25%,  total acc: 67.46%   [EVAL] batch:  457 | acc: 6.25%,  total acc: 67.33%   [EVAL] batch:  458 | acc: 6.25%,  total acc: 67.20%   [EVAL] batch:  459 | acc: 25.00%,  total acc: 67.11%   [EVAL] batch:  460 | acc: 12.50%,  total acc: 66.99%   [EVAL] batch:  461 | acc: 0.00%,  total acc: 66.84%   [EVAL] batch:  462 | acc: 25.00%,  total acc: 66.75%   [EVAL] batch:  463 | acc: 6.25%,  total acc: 66.62%   [EVAL] batch:  464 | acc: 18.75%,  total acc: 66.52%   [EVAL] batch:  465 | acc: 0.00%,  total acc: 66.38%   [EVAL] batch:  466 | acc: 12.50%,  total acc: 66.26%   [EVAL] batch:  467 | acc: 12.50%,  total acc: 66.15%   [EVAL] batch:  468 | acc: 25.00%,  total acc: 66.06%   [EVAL] batch:  469 | acc: 87.50%,  total acc: 66.10%   [EVAL] batch:  470 | acc: 93.75%,  total acc: 66.16%   [EVAL] batch:  471 | acc: 81.25%,  total acc: 66.19%   [EVAL] batch:  472 | acc: 100.00%,  total acc: 66.27%   [EVAL] batch:  473 | acc: 81.25%,  total acc: 66.30%   [EVAL] batch:  474 | acc: 81.25%,  total acc: 66.33%   [EVAL] batch:  475 | acc: 100.00%,  total acc: 66.40%   [EVAL] batch:  476 | acc: 93.75%,  total acc: 66.46%   [EVAL] batch:  477 | acc: 100.00%,  total acc: 66.53%   [EVAL] batch:  478 | acc: 87.50%,  total acc: 66.57%   [EVAL] batch:  479 | acc: 93.75%,  total acc: 66.63%   [EVAL] batch:  480 | acc: 100.00%,  total acc: 66.70%   [EVAL] batch:  481 | acc: 25.00%,  total acc: 66.61%   [EVAL] batch:  482 | acc: 18.75%,  total acc: 66.51%   [EVAL] batch:  483 | acc: 6.25%,  total acc: 66.39%   [EVAL] batch:  484 | acc: 6.25%,  total acc: 66.26%   [EVAL] batch:  485 | acc: 12.50%,  total acc: 66.15%   [EVAL] batch:  486 | acc: 0.00%,  total acc: 66.02%   [EVAL] batch:  487 | acc: 37.50%,  total acc: 65.96%   [EVAL] batch:  488 | acc: 50.00%,  total acc: 65.93%   [EVAL] batch:  489 | acc: 50.00%,  total acc: 65.89%   [EVAL] batch:  490 | acc: 62.50%,  total acc: 65.89%   [EVAL] batch:  491 | acc: 68.75%,  total acc: 65.89%   [EVAL] batch:  492 | acc: 68.75%,  total acc: 65.90%   [EVAL] batch:  493 | acc: 68.75%,  total acc: 65.90%   [EVAL] batch:  494 | acc: 50.00%,  total acc: 65.87%   [EVAL] batch:  495 | acc: 43.75%,  total acc: 65.83%   [EVAL] batch:  496 | acc: 62.50%,  total acc: 65.82%   [EVAL] batch:  497 | acc: 62.50%,  total acc: 65.81%   [EVAL] batch:  498 | acc: 68.75%,  total acc: 65.82%   [EVAL] batch:  499 | acc: 62.50%,  total acc: 65.81%   
cur_acc:  ['0.9454', '0.7867', '0.7738', '0.6647', '0.8800', '0.6677', '0.7054', '0.5188']
his_acc:  ['0.9454', '0.8590', '0.8022', '0.7362', '0.7340', '0.7128', '0.6841', '0.6581']
--------Round  4
seed:  500
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 5 6 4 2 1 3 0]
prepared data!
CurrentTrain: epoch 15, batch     0 | loss: 25.5084050CurrentTrain: epoch 15, batch     1 | loss: 23.6475842CurrentTrain: epoch 15, batch     2 | loss: 26.8200820CurrentTrain: epoch 15, batch     3 | loss: 32.5312857CurrentTrain: epoch 15, batch     4 | loss: 20.3533466CurrentTrain: epoch 15, batch     5 | loss: 17.8019897CurrentTrain: epoch 15, batch     6 | loss: 20.3005822CurrentTrain: epoch 15, batch     7 | loss: 17.9569321CurrentTrain: epoch 15, batch     8 | loss: 21.6981868CurrentTrain: epoch 15, batch     9 | loss: 30.7756356CurrentTrain: epoch 15, batch    10 | loss: 23.2406394CurrentTrain: epoch 15, batch    11 | loss: 16.2052920CurrentTrain: epoch 15, batch    12 | loss: 14.6722738CurrentTrain: epoch 15, batch    13 | loss: 17.4893968CurrentTrain: epoch 15, batch    14 | loss: 22.8650772CurrentTrain: epoch 15, batch    15 | loss: 16.4050510CurrentTrain: epoch 15, batch    16 | loss: 26.3366059CurrentTrain: epoch 15, batch    17 | loss: 17.3554844CurrentTrain: epoch 15, batch    18 | loss: 17.6831273CurrentTrain: epoch 15, batch    19 | loss: 20.5535051CurrentTrain: epoch 15, batch    20 | loss: 16.1392240CurrentTrain: epoch 15, batch    21 | loss: 16.1201249CurrentTrain: epoch 15, batch    22 | loss: 15.8603881CurrentTrain: epoch 15, batch    23 | loss: 16.9836888CurrentTrain: epoch 15, batch    24 | loss: 19.8043545CurrentTrain: epoch 15, batch    25 | loss: 22.5936583CurrentTrain: epoch 15, batch    26 | loss: 13.9495313CurrentTrain: epoch 15, batch    27 | loss: 19.2160681CurrentTrain: epoch 15, batch    28 | loss: 15.0548657CurrentTrain: epoch 15, batch    29 | loss: 12.8170023CurrentTrain: epoch 15, batch    30 | loss: 19.7878304CurrentTrain: epoch 15, batch    31 | loss: 15.7918508CurrentTrain: epoch 15, batch    32 | loss: 26.8624942CurrentTrain: epoch 15, batch    33 | loss: 26.5942980CurrentTrain: epoch 15, batch    34 | loss: 17.1645759CurrentTrain: epoch 15, batch    35 | loss: 16.4990814CurrentTrain: epoch 15, batch    36 | loss: 14.6951160CurrentTrain: epoch 15, batch    37 | loss: 13.7162361CurrentTrain: epoch 15, batch    38 | loss: 12.5356409CurrentTrain: epoch 15, batch    39 | loss: 13.7690222CurrentTrain: epoch 15, batch    40 | loss: 21.5448065CurrentTrain: epoch 15, batch    41 | loss: 29.1116772CurrentTrain: epoch 15, batch    42 | loss: 21.4964657CurrentTrain: epoch 15, batch    43 | loss: 17.3495010CurrentTrain: epoch 15, batch    44 | loss: 13.7698835CurrentTrain: epoch 15, batch    45 | loss: 13.8615278CurrentTrain: epoch 15, batch    46 | loss: 10.4246398CurrentTrain: epoch 15, batch    47 | loss: 15.7264383CurrentTrain: epoch 15, batch    48 | loss: 13.3792350CurrentTrain: epoch 15, batch    49 | loss: 29.4968257CurrentTrain: epoch 15, batch    50 | loss: 17.0461860CurrentTrain: epoch 15, batch    51 | loss: 20.6999508CurrentTrain: epoch 15, batch    52 | loss: 11.8854179CurrentTrain: epoch 15, batch    53 | loss: 19.2233581CurrentTrain: epoch 15, batch    54 | loss: 14.5352514CurrentTrain: epoch 15, batch    55 | loss: 20.1890225CurrentTrain: epoch 15, batch    56 | loss: 14.5428474CurrentTrain: epoch 15, batch    57 | loss: 19.4503835CurrentTrain: epoch 15, batch    58 | loss: 14.3390042CurrentTrain: epoch 15, batch    59 | loss: 16.0166660CurrentTrain: epoch 15, batch    60 | loss: 15.8329117CurrentTrain: epoch 15, batch    61 | loss: 16.7947459CurrentTrain: epoch  7, batch    62 | loss: 16.3004357CurrentTrain: epoch 15, batch     0 | loss: 15.2970402CurrentTrain: epoch 15, batch     1 | loss: 11.9878442CurrentTrain: epoch 15, batch     2 | loss: 12.3727543CurrentTrain: epoch 15, batch     3 | loss: 13.9749350CurrentTrain: epoch 15, batch     4 | loss: 9.8283073CurrentTrain: epoch 15, batch     5 | loss: 12.4408701CurrentTrain: epoch 15, batch     6 | loss: 13.8914239CurrentTrain: epoch 15, batch     7 | loss: 15.2566505CurrentTrain: epoch 15, batch     8 | loss: 14.2952404CurrentTrain: epoch 15, batch     9 | loss: 13.5621265CurrentTrain: epoch 15, batch    10 | loss: 15.6533099CurrentTrain: epoch 15, batch    11 | loss: 18.5894473CurrentTrain: epoch 15, batch    12 | loss: 10.8999835CurrentTrain: epoch 15, batch    13 | loss: 15.6524106CurrentTrain: epoch 15, batch    14 | loss: 13.4585520CurrentTrain: epoch 15, batch    15 | loss: 14.3273248CurrentTrain: epoch 15, batch    16 | loss: 14.1766122CurrentTrain: epoch 15, batch    17 | loss: 16.0630957CurrentTrain: epoch 15, batch    18 | loss: 9.6835713CurrentTrain: epoch 15, batch    19 | loss: 24.2197240CurrentTrain: epoch 15, batch    20 | loss: 19.1050137CurrentTrain: epoch 15, batch    21 | loss: 14.2770495CurrentTrain: epoch 15, batch    22 | loss: 15.6829659CurrentTrain: epoch 15, batch    23 | loss: 15.4365036CurrentTrain: epoch 15, batch    24 | loss: 13.4291873CurrentTrain: epoch 15, batch    25 | loss: 20.0273613CurrentTrain: epoch 15, batch    26 | loss: 10.0230046CurrentTrain: epoch 15, batch    27 | loss: 13.2986840CurrentTrain: epoch 15, batch    28 | loss: 13.1383168CurrentTrain: epoch 15, batch    29 | loss: 12.9431244CurrentTrain: epoch 15, batch    30 | loss: 14.3500246CurrentTrain: epoch 15, batch    31 | loss: 12.1403571CurrentTrain: epoch 15, batch    32 | loss: 15.6367927CurrentTrain: epoch 15, batch    33 | loss: 26.5073455CurrentTrain: epoch 15, batch    34 | loss: 15.8109387CurrentTrain: epoch 15, batch    35 | loss: 12.0407258CurrentTrain: epoch 15, batch    36 | loss: 11.6626954CurrentTrain: epoch 15, batch    37 | loss: 10.4686646CurrentTrain: epoch 15, batch    38 | loss: 30.9319506CurrentTrain: epoch 15, batch    39 | loss: 17.3360145CurrentTrain: epoch 15, batch    40 | loss: 11.1340546CurrentTrain: epoch 15, batch    41 | loss: 13.2336535CurrentTrain: epoch 15, batch    42 | loss: 11.4530391CurrentTrain: epoch 15, batch    43 | loss: 12.1106400CurrentTrain: epoch 15, batch    44 | loss: 12.8877749CurrentTrain: epoch 15, batch    45 | loss: 10.5020726CurrentTrain: epoch 15, batch    46 | loss: 16.1398117CurrentTrain: epoch 15, batch    47 | loss: 16.9416516CurrentTrain: epoch 15, batch    48 | loss: 15.4658555CurrentTrain: epoch 15, batch    49 | loss: 13.3883695CurrentTrain: epoch 15, batch    50 | loss: 12.5781988CurrentTrain: epoch 15, batch    51 | loss: 27.9667729CurrentTrain: epoch 15, batch    52 | loss: 12.0567219CurrentTrain: epoch 15, batch    53 | loss: 14.5894665CurrentTrain: epoch 15, batch    54 | loss: 15.2701566CurrentTrain: epoch 15, batch    55 | loss: 18.7513732CurrentTrain: epoch 15, batch    56 | loss: 10.5062721CurrentTrain: epoch 15, batch    57 | loss: 12.5031050CurrentTrain: epoch 15, batch    58 | loss: 11.3712608CurrentTrain: epoch 15, batch    59 | loss: 12.9844685CurrentTrain: epoch 15, batch    60 | loss: 17.5415653CurrentTrain: epoch 15, batch    61 | loss: 15.7393381CurrentTrain: epoch  7, batch    62 | loss: 9.5843539CurrentTrain: epoch 15, batch     0 | loss: 12.7968934CurrentTrain: epoch 15, batch     1 | loss: 25.2378203CurrentTrain: epoch 15, batch     2 | loss: 14.7032346CurrentTrain: epoch 15, batch     3 | loss: 17.9577100CurrentTrain: epoch 15, batch     4 | loss: 9.5309233CurrentTrain: epoch 15, batch     5 | loss: 14.3729305CurrentTrain: epoch 15, batch     6 | loss: 16.6691752CurrentTrain: epoch 15, batch     7 | loss: 11.9764948CurrentTrain: epoch 15, batch     8 | loss: 10.9918658CurrentTrain: epoch 15, batch     9 | loss: 15.4437517CurrentTrain: epoch 15, batch    10 | loss: 12.3012340CurrentTrain: epoch 15, batch    11 | loss: 18.2873254CurrentTrain: epoch 15, batch    12 | loss: 19.9681308CurrentTrain: epoch 15, batch    13 | loss: 11.6178154CurrentTrain: epoch 15, batch    14 | loss: 11.0961039CurrentTrain: epoch 15, batch    15 | loss: 10.8493942CurrentTrain: epoch 15, batch    16 | loss: 14.7770705CurrentTrain: epoch 15, batch    17 | loss: 22.7360708CurrentTrain: epoch 15, batch    18 | loss: 15.9408498CurrentTrain: epoch 15, batch    19 | loss: 12.6019993CurrentTrain: epoch 15, batch    20 | loss: 11.2452966CurrentTrain: epoch 15, batch    21 | loss: 16.7008235CurrentTrain: epoch 15, batch    22 | loss: 11.4401607CurrentTrain: epoch 15, batch    23 | loss: 25.8303233CurrentTrain: epoch 15, batch    24 | loss: 13.6736140CurrentTrain: epoch 15, batch    25 | loss: 13.4239100CurrentTrain: epoch 15, batch    26 | loss: 17.9446709CurrentTrain: epoch 15, batch    27 | loss: 21.6967911CurrentTrain: epoch 15, batch    28 | loss: 12.4570089CurrentTrain: epoch 15, batch    29 | loss: 18.9199696CurrentTrain: epoch 15, batch    30 | loss: 18.7463802CurrentTrain: epoch 15, batch    31 | loss: 20.7178274CurrentTrain: epoch 15, batch    32 | loss: 11.1500408CurrentTrain: epoch 15, batch    33 | loss: 13.4841459CurrentTrain: epoch 15, batch    34 | loss: 14.2976813CurrentTrain: epoch 15, batch    35 | loss: 11.3268286CurrentTrain: epoch 15, batch    36 | loss: 11.7299664CurrentTrain: epoch 15, batch    37 | loss: 10.1996421CurrentTrain: epoch 15, batch    38 | loss: 13.5254315CurrentTrain: epoch 15, batch    39 | loss: 11.7838165CurrentTrain: epoch 15, batch    40 | loss: 10.4822352CurrentTrain: epoch 15, batch    41 | loss: 10.6560674CurrentTrain: epoch 15, batch    42 | loss: 22.2306837CurrentTrain: epoch 15, batch    43 | loss: 9.9784824CurrentTrain: epoch 15, batch    44 | loss: 11.8121328CurrentTrain: epoch 15, batch    45 | loss: 12.6738446CurrentTrain: epoch 15, batch    46 | loss: 15.0825582CurrentTrain: epoch 15, batch    47 | loss: 16.2186075CurrentTrain: epoch 15, batch    48 | loss: 12.9010600CurrentTrain: epoch 15, batch    49 | loss: 10.1471775CurrentTrain: epoch 15, batch    50 | loss: 7.7946781CurrentTrain: epoch 15, batch    51 | loss: 12.7543009CurrentTrain: epoch 15, batch    52 | loss: 13.3159429CurrentTrain: epoch 15, batch    53 | loss: 12.9058256CurrentTrain: epoch 15, batch    54 | loss: 12.5444954CurrentTrain: epoch 15, batch    55 | loss: 11.2121297CurrentTrain: epoch 15, batch    56 | loss: 17.8607147CurrentTrain: epoch 15, batch    57 | loss: 16.5588956CurrentTrain: epoch 15, batch    58 | loss: 12.6795436CurrentTrain: epoch 15, batch    59 | loss: 10.2314390CurrentTrain: epoch 15, batch    60 | loss: 16.5011630CurrentTrain: epoch 15, batch    61 | loss: 16.3867070CurrentTrain: epoch  7, batch    62 | loss: 11.4283673CurrentTrain: epoch 15, batch     0 | loss: 9.9368975CurrentTrain: epoch 15, batch     1 | loss: 18.0108385CurrentTrain: epoch 15, batch     2 | loss: 10.8644631CurrentTrain: epoch 15, batch     3 | loss: 9.9901076CurrentTrain: epoch 15, batch     4 | loss: 26.3486779CurrentTrain: epoch 15, batch     5 | loss: 17.0133160CurrentTrain: epoch 15, batch     6 | loss: 12.4294770CurrentTrain: epoch 15, batch     7 | loss: 23.4806314CurrentTrain: epoch 15, batch     8 | loss: 13.7746633CurrentTrain: epoch 15, batch     9 | loss: 10.6258891CurrentTrain: epoch 15, batch    10 | loss: 12.2614166CurrentTrain: epoch 15, batch    11 | loss: 15.7793468CurrentTrain: epoch 15, batch    12 | loss: 15.1932988CurrentTrain: epoch 15, batch    13 | loss: 12.5729144CurrentTrain: epoch 15, batch    14 | loss: 23.9089943CurrentTrain: epoch 15, batch    15 | loss: 9.6457324CurrentTrain: epoch 15, batch    16 | loss: 16.6883045CurrentTrain: epoch 15, batch    17 | loss: 15.9179278CurrentTrain: epoch 15, batch    18 | loss: 10.5598593CurrentTrain: epoch 15, batch    19 | loss: 17.3838637CurrentTrain: epoch 15, batch    20 | loss: 12.2529902CurrentTrain: epoch 15, batch    21 | loss: 20.2138135CurrentTrain: epoch 15, batch    22 | loss: 12.5795975CurrentTrain: epoch 15, batch    23 | loss: 8.9836972CurrentTrain: epoch 15, batch    24 | loss: 13.7200375CurrentTrain: epoch 15, batch    25 | loss: 11.8348204CurrentTrain: epoch 15, batch    26 | loss: 12.6534872CurrentTrain: epoch 15, batch    27 | loss: 9.2411459CurrentTrain: epoch 15, batch    28 | loss: 16.3313973CurrentTrain: epoch 15, batch    29 | loss: 18.4829926CurrentTrain: epoch 15, batch    30 | loss: 12.9736916CurrentTrain: epoch 15, batch    31 | loss: 9.6945373CurrentTrain: epoch 15, batch    32 | loss: 9.3590837CurrentTrain: epoch 15, batch    33 | loss: 15.9129518CurrentTrain: epoch 15, batch    34 | loss: 14.3447831CurrentTrain: epoch 15, batch    35 | loss: 13.5302999CurrentTrain: epoch 15, batch    36 | loss: 14.7412003CurrentTrain: epoch 15, batch    37 | loss: 12.5659947CurrentTrain: epoch 15, batch    38 | loss: 7.9273022CurrentTrain: epoch 15, batch    39 | loss: 7.9976819CurrentTrain: epoch 15, batch    40 | loss: 12.2319769CurrentTrain: epoch 15, batch    41 | loss: 12.2292591CurrentTrain: epoch 15, batch    42 | loss: 10.2855731CurrentTrain: epoch 15, batch    43 | loss: 11.9944591CurrentTrain: epoch 15, batch    44 | loss: 11.3919353CurrentTrain: epoch 15, batch    45 | loss: 11.5287651CurrentTrain: epoch 15, batch    46 | loss: 17.8585863CurrentTrain: epoch 15, batch    47 | loss: 14.2379639CurrentTrain: epoch 15, batch    48 | loss: 9.4343324CurrentTrain: epoch 15, batch    49 | loss: 10.4822370CurrentTrain: epoch 15, batch    50 | loss: 11.8107178CurrentTrain: epoch 15, batch    51 | loss: 8.6693115CurrentTrain: epoch 15, batch    52 | loss: 17.9211729CurrentTrain: epoch 15, batch    53 | loss: 10.8232977CurrentTrain: epoch 15, batch    54 | loss: 11.2809588CurrentTrain: epoch 15, batch    55 | loss: 10.9704126CurrentTrain: epoch 15, batch    56 | loss: 10.2688717CurrentTrain: epoch 15, batch    57 | loss: 12.4523934CurrentTrain: epoch 15, batch    58 | loss: 9.3062905CurrentTrain: epoch 15, batch    59 | loss: 11.5837746CurrentTrain: epoch 15, batch    60 | loss: 39.8754533CurrentTrain: epoch 15, batch    61 | loss: 10.6955219CurrentTrain: epoch  7, batch    62 | loss: 15.0567274CurrentTrain: epoch 15, batch     0 | loss: 10.4762706CurrentTrain: epoch 15, batch     1 | loss: 7.3026668CurrentTrain: epoch 15, batch     2 | loss: 13.7280993CurrentTrain: epoch 15, batch     3 | loss: 15.3172028CurrentTrain: epoch 15, batch     4 | loss: 11.4513675CurrentTrain: epoch 15, batch     5 | loss: 10.3030497CurrentTrain: epoch 15, batch     6 | loss: 8.8069520CurrentTrain: epoch 15, batch     7 | loss: 22.2680810CurrentTrain: epoch 15, batch     8 | loss: 13.0932028CurrentTrain: epoch 15, batch     9 | loss: 15.5817475CurrentTrain: epoch 15, batch    10 | loss: 9.7530988CurrentTrain: epoch 15, batch    11 | loss: 23.0985481CurrentTrain: epoch 15, batch    12 | loss: 16.3518002CurrentTrain: epoch 15, batch    13 | loss: 11.7079635CurrentTrain: epoch 15, batch    14 | loss: 15.0324234CurrentTrain: epoch 15, batch    15 | loss: 13.9195271CurrentTrain: epoch 15, batch    16 | loss: 11.0125401CurrentTrain: epoch 15, batch    17 | loss: 23.2865664CurrentTrain: epoch 15, batch    18 | loss: 10.4268605CurrentTrain: epoch 15, batch    19 | loss: 7.4658840CurrentTrain: epoch 15, batch    20 | loss: 13.4371607CurrentTrain: epoch 15, batch    21 | loss: 10.1138066CurrentTrain: epoch 15, batch    22 | loss: 15.6929694CurrentTrain: epoch 15, batch    23 | loss: 13.5996476CurrentTrain: epoch 15, batch    24 | loss: 12.9243164CurrentTrain: epoch 15, batch    25 | loss: 9.6956965CurrentTrain: epoch 15, batch    26 | loss: 16.4418024CurrentTrain: epoch 15, batch    27 | loss: 16.8929885CurrentTrain: epoch 15, batch    28 | loss: 9.6476604CurrentTrain: epoch 15, batch    29 | loss: 18.8082260CurrentTrain: epoch 15, batch    30 | loss: 24.4130378CurrentTrain: epoch 15, batch    31 | loss: 21.3906536CurrentTrain: epoch 15, batch    32 | loss: 9.6102838CurrentTrain: epoch 15, batch    33 | loss: 9.9224930CurrentTrain: epoch 15, batch    34 | loss: 8.3015457CurrentTrain: epoch 15, batch    35 | loss: 15.0906516CurrentTrain: epoch 15, batch    36 | loss: 10.1836305CurrentTrain: epoch 15, batch    37 | loss: 13.3460528CurrentTrain: epoch 15, batch    38 | loss: 19.4730009CurrentTrain: epoch 15, batch    39 | loss: 25.2607583CurrentTrain: epoch 15, batch    40 | loss: 9.4407305CurrentTrain: epoch 15, batch    41 | loss: 8.9130085CurrentTrain: epoch 15, batch    42 | loss: 15.4724817CurrentTrain: epoch 15, batch    43 | loss: 14.3200181CurrentTrain: epoch 15, batch    44 | loss: 10.1833204CurrentTrain: epoch 15, batch    45 | loss: 8.1310045CurrentTrain: epoch 15, batch    46 | loss: 10.7674809CurrentTrain: epoch 15, batch    47 | loss: 12.7733271CurrentTrain: epoch 15, batch    48 | loss: 16.4119972CurrentTrain: epoch 15, batch    49 | loss: 10.6346424CurrentTrain: epoch 15, batch    50 | loss: 11.7972030CurrentTrain: epoch 15, batch    51 | loss: 14.4553512CurrentTrain: epoch 15, batch    52 | loss: 15.1162025CurrentTrain: epoch 15, batch    53 | loss: 15.6978226CurrentTrain: epoch 15, batch    54 | loss: 19.1928207CurrentTrain: epoch 15, batch    55 | loss: 11.0873876CurrentTrain: epoch 15, batch    56 | loss: 12.6760798CurrentTrain: epoch 15, batch    57 | loss: 9.0223214CurrentTrain: epoch 15, batch    58 | loss: 14.7220216CurrentTrain: epoch 15, batch    59 | loss: 11.4334204CurrentTrain: epoch 15, batch    60 | loss: 9.1504062CurrentTrain: epoch 15, batch    61 | loss: 8.8470101CurrentTrain: epoch  7, batch    62 | loss: 12.6189353CurrentTrain: epoch 15, batch     0 | loss: 8.9550380CurrentTrain: epoch 15, batch     1 | loss: 6.7579882CurrentTrain: epoch 15, batch     2 | loss: 29.5301259CurrentTrain: epoch 15, batch     3 | loss: 14.2282411CurrentTrain: epoch 15, batch     4 | loss: 8.5172857CurrentTrain: epoch 15, batch     5 | loss: 11.0265908CurrentTrain: epoch 15, batch     6 | loss: 15.7867412CurrentTrain: epoch 15, batch     7 | loss: 16.8439242CurrentTrain: epoch 15, batch     8 | loss: 16.5123662CurrentTrain: epoch 15, batch     9 | loss: 6.7852311CurrentTrain: epoch 15, batch    10 | loss: 9.0168023CurrentTrain: epoch 15, batch    11 | loss: 13.7603638CurrentTrain: epoch 15, batch    12 | loss: 9.6632244CurrentTrain: epoch 15, batch    13 | loss: 8.7407533CurrentTrain: epoch 15, batch    14 | loss: 7.3866377CurrentTrain: epoch 15, batch    15 | loss: 12.9887954CurrentTrain: epoch 15, batch    16 | loss: 15.7593409CurrentTrain: epoch 15, batch    17 | loss: 9.0547346CurrentTrain: epoch 15, batch    18 | loss: 9.5037855CurrentTrain: epoch 15, batch    19 | loss: 16.8343067CurrentTrain: epoch 15, batch    20 | loss: 9.3971179CurrentTrain: epoch 15, batch    21 | loss: 9.0039116CurrentTrain: epoch 15, batch    22 | loss: 11.5812258CurrentTrain: epoch 15, batch    23 | loss: 11.1899925CurrentTrain: epoch 15, batch    24 | loss: 12.9388899CurrentTrain: epoch 15, batch    25 | loss: 13.2001578CurrentTrain: epoch 15, batch    26 | loss: 11.9784250CurrentTrain: epoch 15, batch    27 | loss: 11.0213378CurrentTrain: epoch 15, batch    28 | loss: 16.5551939CurrentTrain: epoch 15, batch    29 | loss: 15.6303143CurrentTrain: epoch 15, batch    30 | loss: 7.8112232CurrentTrain: epoch 15, batch    31 | loss: 8.7712842CurrentTrain: epoch 15, batch    32 | loss: 8.9467151CurrentTrain: epoch 15, batch    33 | loss: 15.5628848CurrentTrain: epoch 15, batch    34 | loss: 11.8458007CurrentTrain: epoch 15, batch    35 | loss: 21.0822132CurrentTrain: epoch 15, batch    36 | loss: 24.3272957CurrentTrain: epoch 15, batch    37 | loss: 9.2688088CurrentTrain: epoch 15, batch    38 | loss: 11.6254668CurrentTrain: epoch 15, batch    39 | loss: 10.9817025CurrentTrain: epoch 15, batch    40 | loss: 10.2070324CurrentTrain: epoch 15, batch    41 | loss: 8.8923082CurrentTrain: epoch 15, batch    42 | loss: 7.8328092CurrentTrain: epoch 15, batch    43 | loss: 11.7982061CurrentTrain: epoch 15, batch    44 | loss: 8.8621339CurrentTrain: epoch 15, batch    45 | loss: 15.9177932CurrentTrain: epoch 15, batch    46 | loss: 11.1325379CurrentTrain: epoch 15, batch    47 | loss: 19.2312816CurrentTrain: epoch 15, batch    48 | loss: 10.2922158CurrentTrain: epoch 15, batch    49 | loss: 21.6388700CurrentTrain: epoch 15, batch    50 | loss: 8.0901885CurrentTrain: epoch 15, batch    51 | loss: 9.5508794CurrentTrain: epoch 15, batch    52 | loss: 9.5615090CurrentTrain: epoch 15, batch    53 | loss: 9.3402210CurrentTrain: epoch 15, batch    54 | loss: 9.8324805CurrentTrain: epoch 15, batch    55 | loss: 11.4982317CurrentTrain: epoch 15, batch    56 | loss: 9.3911083CurrentTrain: epoch 15, batch    57 | loss: 11.2844173CurrentTrain: epoch 15, batch    58 | loss: 13.3211963CurrentTrain: epoch 15, batch    59 | loss: 11.0922975CurrentTrain: epoch 15, batch    60 | loss: 16.8027496CurrentTrain: epoch 15, batch    61 | loss: 8.3789535CurrentTrain: epoch  7, batch    62 | loss: 10.9588483CurrentTrain: epoch 15, batch     0 | loss: 14.5229497CurrentTrain: epoch 15, batch     1 | loss: 7.8220516CurrentTrain: epoch 15, batch     2 | loss: 10.6163939CurrentTrain: epoch 15, batch     3 | loss: 10.2520296CurrentTrain: epoch 15, batch     4 | loss: 9.7843843CurrentTrain: epoch 15, batch     5 | loss: 10.8228946CurrentTrain: epoch 15, batch     6 | loss: 27.0043109CurrentTrain: epoch 15, batch     7 | loss: 19.9379174CurrentTrain: epoch 15, batch     8 | loss: 10.5851876CurrentTrain: epoch 15, batch     9 | loss: 9.8000402CurrentTrain: epoch 15, batch    10 | loss: 8.5950569CurrentTrain: epoch 15, batch    11 | loss: 7.6432389CurrentTrain: epoch 15, batch    12 | loss: 20.8092903CurrentTrain: epoch 15, batch    13 | loss: 8.3469180CurrentTrain: epoch 15, batch    14 | loss: 27.0562661CurrentTrain: epoch 15, batch    15 | loss: 23.8236836CurrentTrain: epoch 15, batch    16 | loss: 14.6831455CurrentTrain: epoch 15, batch    17 | loss: 7.3296549CurrentTrain: epoch 15, batch    18 | loss: 9.6719382CurrentTrain: epoch 15, batch    19 | loss: 9.2073428CurrentTrain: epoch 15, batch    20 | loss: 8.2068939CurrentTrain: epoch 15, batch    21 | loss: 9.7923676CurrentTrain: epoch 15, batch    22 | loss: 8.4315124CurrentTrain: epoch 15, batch    23 | loss: 10.7050776CurrentTrain: epoch 15, batch    24 | loss: 14.3200480CurrentTrain: epoch 15, batch    25 | loss: 9.6374715CurrentTrain: epoch 15, batch    26 | loss: 13.8790221CurrentTrain: epoch 15, batch    27 | loss: 14.7216879CurrentTrain: epoch 15, batch    28 | loss: 7.9286850CurrentTrain: epoch 15, batch    29 | loss: 7.3044026CurrentTrain: epoch 15, batch    30 | loss: 12.3419128CurrentTrain: epoch 15, batch    31 | loss: 13.1178116CurrentTrain: epoch 15, batch    32 | loss: 12.5678128CurrentTrain: epoch 15, batch    33 | loss: 15.3673458CurrentTrain: epoch 15, batch    34 | loss: 12.4543402CurrentTrain: epoch 15, batch    35 | loss: 18.9118349CurrentTrain: epoch 15, batch    36 | loss: 11.7910918CurrentTrain: epoch 15, batch    37 | loss: 10.1956851CurrentTrain: epoch 15, batch    38 | loss: 24.9593717CurrentTrain: epoch 15, batch    39 | loss: 10.2619008CurrentTrain: epoch 15, batch    40 | loss: 10.6381275CurrentTrain: epoch 15, batch    41 | loss: 8.3385739CurrentTrain: epoch 15, batch    42 | loss: 12.1577283CurrentTrain: epoch 15, batch    43 | loss: 12.3241813CurrentTrain: epoch 15, batch    44 | loss: 26.1910356CurrentTrain: epoch 15, batch    45 | loss: 15.8678609CurrentTrain: epoch 15, batch    46 | loss: 16.3994396CurrentTrain: epoch 15, batch    47 | loss: 14.4633918CurrentTrain: epoch 15, batch    48 | loss: 16.1214658CurrentTrain: epoch 15, batch    49 | loss: 10.7122496CurrentTrain: epoch 15, batch    50 | loss: 8.7420197CurrentTrain: epoch 15, batch    51 | loss: 6.6577275CurrentTrain: epoch 15, batch    52 | loss: 7.8157589CurrentTrain: epoch 15, batch    53 | loss: 23.6509526CurrentTrain: epoch 15, batch    54 | loss: 7.7212073CurrentTrain: epoch 15, batch    55 | loss: 6.4846050CurrentTrain: epoch 15, batch    56 | loss: 22.7396628CurrentTrain: epoch 15, batch    57 | loss: 14.1044275CurrentTrain: epoch 15, batch    58 | loss: 11.0247977CurrentTrain: epoch 15, batch    59 | loss: 9.6349093CurrentTrain: epoch 15, batch    60 | loss: 10.4903222CurrentTrain: epoch 15, batch    61 | loss: 10.2926283CurrentTrain: epoch  7, batch    62 | loss: 15.7879621CurrentTrain: epoch 15, batch     0 | loss: 10.2005467CurrentTrain: epoch 15, batch     1 | loss: 10.9599261CurrentTrain: epoch 15, batch     2 | loss: 11.2813585CurrentTrain: epoch 15, batch     3 | loss: 10.3022467CurrentTrain: epoch 15, batch     4 | loss: 21.4825094CurrentTrain: epoch 15, batch     5 | loss: 10.3419492CurrentTrain: epoch 15, batch     6 | loss: 8.7929621CurrentTrain: epoch 15, batch     7 | loss: 9.6054274CurrentTrain: epoch 15, batch     8 | loss: 17.7229844CurrentTrain: epoch 15, batch     9 | loss: 11.6171328CurrentTrain: epoch 15, batch    10 | loss: 7.8525646CurrentTrain: epoch 15, batch    11 | loss: 13.1269948CurrentTrain: epoch 15, batch    12 | loss: 14.1111553CurrentTrain: epoch 15, batch    13 | loss: 13.3496559CurrentTrain: epoch 15, batch    14 | loss: 15.3752522CurrentTrain: epoch 15, batch    15 | loss: 15.8248491CurrentTrain: epoch 15, batch    16 | loss: 11.0826003CurrentTrain: epoch 15, batch    17 | loss: 9.7728903CurrentTrain: epoch 15, batch    18 | loss: 11.1704675CurrentTrain: epoch 15, batch    19 | loss: 12.2997963CurrentTrain: epoch 15, batch    20 | loss: 10.3481775CurrentTrain: epoch 15, batch    21 | loss: 6.8808643CurrentTrain: epoch 15, batch    22 | loss: 8.3907085CurrentTrain: epoch 15, batch    23 | loss: 20.9580013CurrentTrain: epoch 15, batch    24 | loss: 9.8571586CurrentTrain: epoch 15, batch    25 | loss: 7.3217468CurrentTrain: epoch 15, batch    26 | loss: 9.4335215CurrentTrain: epoch 15, batch    27 | loss: 11.5284343CurrentTrain: epoch 15, batch    28 | loss: 15.2374006CurrentTrain: epoch 15, batch    29 | loss: 11.9752579CurrentTrain: epoch 15, batch    30 | loss: 10.7200784CurrentTrain: epoch 15, batch    31 | loss: 9.6002453CurrentTrain: epoch 15, batch    32 | loss: 14.8231422CurrentTrain: epoch 15, batch    33 | loss: 11.5722949CurrentTrain: epoch 15, batch    34 | loss: 9.1754250CurrentTrain: epoch 15, batch    35 | loss: 10.8258662CurrentTrain: epoch 15, batch    36 | loss: 9.1875438CurrentTrain: epoch 15, batch    37 | loss: 11.0692456CurrentTrain: epoch 15, batch    38 | loss: 11.6418262CurrentTrain: epoch 15, batch    39 | loss: 10.6115115CurrentTrain: epoch 15, batch    40 | loss: 8.9731802CurrentTrain: epoch 15, batch    41 | loss: 6.7427861CurrentTrain: epoch 15, batch    42 | loss: 13.9455760CurrentTrain: epoch 15, batch    43 | loss: 9.3117712CurrentTrain: epoch 15, batch    44 | loss: 15.7241385CurrentTrain: epoch 15, batch    45 | loss: 8.4604204CurrentTrain: epoch 15, batch    46 | loss: 10.9114999CurrentTrain: epoch 15, batch    47 | loss: 7.6903031CurrentTrain: epoch 15, batch    48 | loss: 16.4739247CurrentTrain: epoch 15, batch    49 | loss: 8.6594818CurrentTrain: epoch 15, batch    50 | loss: 10.2744019CurrentTrain: epoch 15, batch    51 | loss: 10.8113935CurrentTrain: epoch 15, batch    52 | loss: 11.9691797CurrentTrain: epoch 15, batch    53 | loss: 23.4421010CurrentTrain: epoch 15, batch    54 | loss: 16.0984686CurrentTrain: epoch 15, batch    55 | loss: 17.0423779CurrentTrain: epoch 15, batch    56 | loss: 7.7649067CurrentTrain: epoch 15, batch    57 | loss: 8.5501151CurrentTrain: epoch 15, batch    58 | loss: 15.4665853CurrentTrain: epoch 15, batch    59 | loss: 13.6310006CurrentTrain: epoch 15, batch    60 | loss: 12.0899344CurrentTrain: epoch 15, batch    61 | loss: 9.1372751CurrentTrain: epoch  7, batch    62 | loss: 11.0516013CurrentTrain: epoch 15, batch     0 | loss: 11.7224812CurrentTrain: epoch 15, batch     1 | loss: 23.2581862CurrentTrain: epoch 15, batch     2 | loss: 9.6087237CurrentTrain: epoch 15, batch     3 | loss: 11.4107188CurrentTrain: epoch 15, batch     4 | loss: 13.2118933CurrentTrain: epoch 15, batch     5 | loss: 8.4272266CurrentTrain: epoch 15, batch     6 | loss: 10.8196489CurrentTrain: epoch 15, batch     7 | loss: 16.7011526CurrentTrain: epoch 15, batch     8 | loss: 8.2275703CurrentTrain: epoch 15, batch     9 | loss: 15.4420540CurrentTrain: epoch 15, batch    10 | loss: 14.2022556CurrentTrain: epoch 15, batch    11 | loss: 14.4175839CurrentTrain: epoch 15, batch    12 | loss: 24.7388442CurrentTrain: epoch 15, batch    13 | loss: 11.6748992CurrentTrain: epoch 15, batch    14 | loss: 10.9385399CurrentTrain: epoch 15, batch    15 | loss: 8.4617577CurrentTrain: epoch 15, batch    16 | loss: 11.1144032CurrentTrain: epoch 15, batch    17 | loss: 9.1281529CurrentTrain: epoch 15, batch    18 | loss: 12.1304113CurrentTrain: epoch 15, batch    19 | loss: 8.4558915CurrentTrain: epoch 15, batch    20 | loss: 6.5285943CurrentTrain: epoch 15, batch    21 | loss: 9.4287701CurrentTrain: epoch 15, batch    22 | loss: 15.6450186CurrentTrain: epoch 15, batch    23 | loss: 7.9719660CurrentTrain: epoch 15, batch    24 | loss: 14.0956647CurrentTrain: epoch 15, batch    25 | loss: 9.1065598CurrentTrain: epoch 15, batch    26 | loss: 12.6285532CurrentTrain: epoch 15, batch    27 | loss: 8.2316556CurrentTrain: epoch 15, batch    28 | loss: 8.8569455CurrentTrain: epoch 15, batch    29 | loss: 8.2623296CurrentTrain: epoch 15, batch    30 | loss: 12.9826154CurrentTrain: epoch 15, batch    31 | loss: 7.9984386CurrentTrain: epoch 15, batch    32 | loss: 15.8042167CurrentTrain: epoch 15, batch    33 | loss: 13.6752106CurrentTrain: epoch 15, batch    34 | loss: 19.6982904CurrentTrain: epoch 15, batch    35 | loss: 12.0910207CurrentTrain: epoch 15, batch    36 | loss: 13.7942128CurrentTrain: epoch 15, batch    37 | loss: 10.0584304CurrentTrain: epoch 15, batch    38 | loss: 7.0605802CurrentTrain: epoch 15, batch    39 | loss: 9.8783751CurrentTrain: epoch 15, batch    40 | loss: 9.9214297CurrentTrain: epoch 15, batch    41 | loss: 10.8712564CurrentTrain: epoch 15, batch    42 | loss: 10.1598632CurrentTrain: epoch 15, batch    43 | loss: 7.6717303CurrentTrain: epoch 15, batch    44 | loss: 7.9356850CurrentTrain: epoch 15, batch    45 | loss: 10.8529767CurrentTrain: epoch 15, batch    46 | loss: 7.7943687CurrentTrain: epoch 15, batch    47 | loss: 16.0204450CurrentTrain: epoch 15, batch    48 | loss: 7.8403172CurrentTrain: epoch 15, batch    49 | loss: 7.4225191CurrentTrain: epoch 15, batch    50 | loss: 12.1976869CurrentTrain: epoch 15, batch    51 | loss: 14.2556737CurrentTrain: epoch 15, batch    52 | loss: 9.9643416CurrentTrain: epoch 15, batch    53 | loss: 7.6343484CurrentTrain: epoch 15, batch    54 | loss: 10.9062478CurrentTrain: epoch 15, batch    55 | loss: 11.0760550CurrentTrain: epoch 15, batch    56 | loss: 7.3565253CurrentTrain: epoch 15, batch    57 | loss: 9.0739693CurrentTrain: epoch 15, batch    58 | loss: 5.9862294CurrentTrain: epoch 15, batch    59 | loss: 14.3382594CurrentTrain: epoch 15, batch    60 | loss: 15.8607647CurrentTrain: epoch 15, batch    61 | loss: 10.1760842CurrentTrain: epoch  7, batch    62 | loss: 4.3135602CurrentTrain: epoch 15, batch     0 | loss: 8.6190016CurrentTrain: epoch 15, batch     1 | loss: 9.6858117CurrentTrain: epoch 15, batch     2 | loss: 7.7690761CurrentTrain: epoch 15, batch     3 | loss: 8.3409011CurrentTrain: epoch 15, batch     4 | loss: 9.9167194CurrentTrain: epoch 15, batch     5 | loss: 13.7834303CurrentTrain: epoch 15, batch     6 | loss: 13.2452300CurrentTrain: epoch 15, batch     7 | loss: 8.1852000CurrentTrain: epoch 15, batch     8 | loss: 11.5233401CurrentTrain: epoch 15, batch     9 | loss: 10.9659507CurrentTrain: epoch 15, batch    10 | loss: 9.4975559CurrentTrain: epoch 15, batch    11 | loss: 9.4041338CurrentTrain: epoch 15, batch    12 | loss: 7.8139063CurrentTrain: epoch 15, batch    13 | loss: 16.0007167CurrentTrain: epoch 15, batch    14 | loss: 13.9817752CurrentTrain: epoch 15, batch    15 | loss: 14.2754268CurrentTrain: epoch 15, batch    16 | loss: 9.5401221CurrentTrain: epoch 15, batch    17 | loss: 7.7108134CurrentTrain: epoch 15, batch    18 | loss: 8.1794444CurrentTrain: epoch 15, batch    19 | loss: 9.9336482CurrentTrain: epoch 15, batch    20 | loss: 8.4478482CurrentTrain: epoch 15, batch    21 | loss: 8.5442943CurrentTrain: epoch 15, batch    22 | loss: 8.8584653CurrentTrain: epoch 15, batch    23 | loss: 11.2725476CurrentTrain: epoch 15, batch    24 | loss: 17.3789401CurrentTrain: epoch 15, batch    25 | loss: 11.3695116CurrentTrain: epoch 15, batch    26 | loss: 16.2460422CurrentTrain: epoch 15, batch    27 | loss: 11.9255959CurrentTrain: epoch 15, batch    28 | loss: 9.5290364CurrentTrain: epoch 15, batch    29 | loss: 9.1686832CurrentTrain: epoch 15, batch    30 | loss: 12.2079431CurrentTrain: epoch 15, batch    31 | loss: 10.5414990CurrentTrain: epoch 15, batch    32 | loss: 24.9959524CurrentTrain: epoch 15, batch    33 | loss: 24.6389573CurrentTrain: epoch 15, batch    34 | loss: 10.3556417CurrentTrain: epoch 15, batch    35 | loss: 7.9844867CurrentTrain: epoch 15, batch    36 | loss: 10.9943587CurrentTrain: epoch 15, batch    37 | loss: 11.9762403CurrentTrain: epoch 15, batch    38 | loss: 14.1709693CurrentTrain: epoch 15, batch    39 | loss: 16.2159244CurrentTrain: epoch 15, batch    40 | loss: 7.2252698CurrentTrain: epoch 15, batch    41 | loss: 34.4883562CurrentTrain: epoch 15, batch    42 | loss: 13.6280342CurrentTrain: epoch 15, batch    43 | loss: 15.0513628CurrentTrain: epoch 15, batch    44 | loss: 10.0523737CurrentTrain: epoch 15, batch    45 | loss: 15.5399907CurrentTrain: epoch 15, batch    46 | loss: 10.7748011CurrentTrain: epoch 15, batch    47 | loss: 13.4266816CurrentTrain: epoch 15, batch    48 | loss: 9.3652099CurrentTrain: epoch 15, batch    49 | loss: 15.8093261CurrentTrain: epoch 15, batch    50 | loss: 7.2096437CurrentTrain: epoch 15, batch    51 | loss: 6.4419051CurrentTrain: epoch 15, batch    52 | loss: 8.2219977CurrentTrain: epoch 15, batch    53 | loss: 9.2284835CurrentTrain: epoch 15, batch    54 | loss: 9.6414619CurrentTrain: epoch 15, batch    55 | loss: 9.0628480CurrentTrain: epoch 15, batch    56 | loss: 15.3539242CurrentTrain: epoch 15, batch    57 | loss: 10.9487654CurrentTrain: epoch 15, batch    58 | loss: 15.7618435CurrentTrain: epoch 15, batch    59 | loss: 12.3430570CurrentTrain: epoch 15, batch    60 | loss: 9.0563430CurrentTrain: epoch 15, batch    61 | loss: 7.8497822CurrentTrain: epoch  7, batch    62 | loss: 6.4549747
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: in rome he met a number of international neo - classical artists including german - born anton rafael mengs , scottish gavin hamilton , and austrian angelica kauffman .
Head Entity: angelica kauffman
Tail Entity: neo - classical
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: The Impressionist movement, which began in the late 19th century, was characterized by a focus on light and color, and it included prominent figures such as Claude Monet and Pierre-Auguste Renoir.  
Head Entity: Claude Monet  
Tail Entity: Impressionist  

Relation: movement  
Context: The Surrealist movement emerged in the early 20th century, challenging the conventional perceptions of reality and featuring artists like Salvador Dalí and René Magritte.  
Head Entity: Salvador Dalí  
Tail Entity: Surrealist  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: lyons hill is a restored village , and former parish with church , now part of the community of ardclough in north county kildare .
Head Entity: ardclough
Tail Entity: county kildare
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: The city of San Francisco is known for its iconic Golden Gate Bridge and is part of the larger San Francisco County in California.  
Head Entity: San Francisco  
Tail Entity: San Francisco County  

Relation: located in the administrative territorial entity  
Context: The village of Haverfordwest serves as the county town of Pembrokeshire, situated in the picturesque region of Wales.  
Head Entity: Haverfordwest  
Tail Entity: Pembrokeshire  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the nation as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: he is also , conjointly with mark burrell , a leading member of the art movement known as the north sea magical realists .
Head Entity: mark burrell
Tail Entity: north sea magical realists
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: She has been a dedicated member of the local book club for over five years, where she shares her love for literature with fellow enthusiasts.  
Head Entity: She  
Tail Entity: local book club  

Relation: member of  
Context: As a talented musician, he is a prominent member of the jazz ensemble that performs regularly at the downtown music festival.  
Head Entity: he  
Tail Entity: jazz ensemble  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: on july 2 , 1935 , the rip van winkle bridge over the hudson river between catskill and greenport was opened to traffic .
Head Entity: rip van winkle bridge
Tail Entity: hudson river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: The Golden Gate Bridge spans the Golden Gate Strait, connecting San Francisco to Marin County and allowing vehicles to cross over the waterway.  
Head Entity: Golden Gate Bridge  
Tail Entity: Golden Gate Strait  

Relation: crosses  
Context: The Channel Tunnel, also known as the Chunnel, runs beneath the English Channel, providing a rail link that crosses under the sea between the UK and France.  
Head Entity: Channel Tunnel  
Tail Entity: English Channel  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: it is on the panj river just downstream from the mouth of the bartang river at the point where the panj briefly turns west before resuming its northerly course .
Head Entity: panj river
Tail Entity: bartang river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The Missouri River, which is a major tributary of the Mississippi River, flows through several states before joining the larger watercourse.  
Head Entity: Missouri River  
Tail Entity: Mississippi River  

Relation: tributary  
Context: The Rio Grande serves as a natural border between the United States and Mexico, and it has several tributaries, including the Pecos River.  
Head Entity: Rio Grande  
Tail Entity: Pecos River  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: in september 2001 , mimica became minister of european integration under prime minister ivica račan , position he held until december 2003 .
Head Entity: ivica račan
Tail Entity: prime minister
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: in 2010, barack obama was elected as the 44th president of the united states, a position he held until 2017.  
Head Entity: barack obama  
Tail Entity: president of the united states  

Relation: position held  
Context: after serving as the mayor of new york city from 2002 to 2013, michael bloomberg transitioned to a role in philanthropy, a position he held until 2020.  
Head Entity: michael bloomberg  
Tail Entity: mayor of new york city  
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: " jedi outcast " was developed by raven software and powered by the i d tech 3 game engine .
Head Entity: jedi outcast
Tail Entity: raven software
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: "The Witcher 3: Wild Hunt" was developed by CD Projekt Red and has received numerous awards for its storytelling and gameplay.  
Head Entity: The Witcher 3: Wild Hunt  
Tail Entity: CD Projekt Red  

Relation: developer  
Context: "Fortnite" was developed by Epic Games and has become a cultural phenomenon since its release.  
Head Entity: Fortnite  
Tail Entity: Epic Games  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: holy molar are a san diego based band formed in 2001 .
Head Entity: holy molar
Tail Entity: san diego
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the tech startup was established in silicon valley in 2015.  
Head Entity: the tech startup  
Tail Entity: silicon valley  

Relation: location of formation  
Context: the famous rock band was created in new york city during the late 1980s.  
Head Entity: the famous rock band  
Tail Entity: new york city  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: huff 's feature film debut as an actress was in the 2005 film " police beat " , an american crime film written by charles mudede of " the stranger " .
Head Entity: police beat
Tail Entity: american
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous dish sushi is traditionally associated with Japan, where it has been enjoyed for centuries and has become a staple of Japanese cuisine.  
Head Entity: sushi  
Tail Entity: Japan  

Relation: country of origin  
Context: The renowned painting "The Starry Night" was created by Vincent van Gogh while he was living in France, and it reflects the beauty of the night sky over the French countryside.  
Head Entity: The Starry Night  
Tail Entity: France  
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.65%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.09%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 93.47%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.01%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.99%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.98%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.34%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.51%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.67%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.93%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.90%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.60%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.69%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.74%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.70%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.67%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.64%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.72%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.45%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.42%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.39%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.47%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.55%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 95.62%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.70%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.67%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.94%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.65%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.09%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 93.47%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.01%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.99%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.98%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.34%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.51%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.67%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.93%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.90%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.60%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.69%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.74%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.70%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.67%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.64%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.72%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.45%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.42%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.39%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.47%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.55%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 95.62%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.70%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.67%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.94%   
cur_acc:  ['0.9494']
his_acc:  ['0.9494']
CurrentTrain: epoch 15, batch     0 | loss: 16.0918111CurrentTrain: epoch 15, batch     1 | loss: 15.8490981CurrentTrain: epoch 15, batch     2 | loss: 13.4109551CurrentTrain: epoch  1, batch     3 | loss: 11.8153205CurrentTrain: epoch 15, batch     0 | loss: 10.1697089CurrentTrain: epoch 15, batch     1 | loss: 11.6136848CurrentTrain: epoch 15, batch     2 | loss: 12.1740702CurrentTrain: epoch  1, batch     3 | loss: 7.9815478CurrentTrain: epoch 15, batch     0 | loss: 13.9745633CurrentTrain: epoch 15, batch     1 | loss: 7.7666473CurrentTrain: epoch 15, batch     2 | loss: 15.5642719CurrentTrain: epoch  1, batch     3 | loss: 8.4049775CurrentTrain: epoch 15, batch     0 | loss: 7.0178298CurrentTrain: epoch 15, batch     1 | loss: 8.8047161CurrentTrain: epoch 15, batch     2 | loss: 7.8758082CurrentTrain: epoch  1, batch     3 | loss: 6.5838560CurrentTrain: epoch 15, batch     0 | loss: 6.6962906CurrentTrain: epoch 15, batch     1 | loss: 6.1247559CurrentTrain: epoch 15, batch     2 | loss: 10.1016577CurrentTrain: epoch  1, batch     3 | loss: 7.6166547CurrentTrain: epoch 15, batch     0 | loss: 12.9732404CurrentTrain: epoch 15, batch     1 | loss: 8.0649657CurrentTrain: epoch 15, batch     2 | loss: 6.6245448CurrentTrain: epoch  1, batch     3 | loss: 12.3758988CurrentTrain: epoch 15, batch     0 | loss: 9.6243169CurrentTrain: epoch 15, batch     1 | loss: 6.7914285CurrentTrain: epoch 15, batch     2 | loss: 15.8844593CurrentTrain: epoch  1, batch     3 | loss: 6.7026302CurrentTrain: epoch 15, batch     0 | loss: 7.0662850CurrentTrain: epoch 15, batch     1 | loss: 7.6903290CurrentTrain: epoch 15, batch     2 | loss: 10.8748437CurrentTrain: epoch  1, batch     3 | loss: 6.6961441CurrentTrain: epoch 15, batch     0 | loss: 15.7275598CurrentTrain: epoch 15, batch     1 | loss: 12.5573591CurrentTrain: epoch 15, batch     2 | loss: 11.1670628CurrentTrain: epoch  1, batch     3 | loss: 6.0970850CurrentTrain: epoch 15, batch     0 | loss: 6.8348353CurrentTrain: epoch 15, batch     1 | loss: 5.2900084CurrentTrain: epoch 15, batch     2 | loss: 6.7301824CurrentTrain: epoch  1, batch     3 | loss: 5.7015126
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: dresser is an analog to anton drexler , the founder of the nazi party which was then hijacked by adolf hitler .
Head Entity: anton drexler
Tail Entity: nazi party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: After serving as a senator, john doe joined the democratic party, where he became an influential figure in local politics.  
Head Entity: john doe  
Tail Entity: democratic party  

Relation: member of political party  
Context: During her tenure in the government, jane smith was a prominent member of the green party, advocating for environmental policies.  
Head Entity: jane smith  
Tail Entity: green party  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: the original story of real life escape of betty mahmoody is depicted in the movie " not without my daughter " which itself was based on betty mahmoody 's book of the same name .
Head Entity: not without my daughter
Tail Entity: betty mahmoody
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film "the great gatsby" draws heavily from f. scott fitzgerald's classic novel, capturing the essence of the roaring twenties and the complexities of love and ambition.  
Head Entity: the great gatsby  
Tail Entity: f. scott fitzgerald  

Relation: after a work by  
Context: the animated feature "the lion king" was inspired by shakespeare's play "hamlet," incorporating themes of betrayal, revenge, and the struggle for power.  
Head Entity: the lion king  
Tail Entity: shakespeare
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: the influential victorian art critic john ruskin expounded theories about social justice in " unto this last " ( 1860 ) .
Head Entity: john ruskin
Tail Entity: unto this last
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: the renowned physicist albert einstein published his groundbreaking theory of relativity in "on the electrodynamics of moving bodies" (1905).  
Head Entity: albert einstein  
Tail Entity: on the electrodynamics of moving bodies  

Relation: notable work  
Context: the celebrated author j.k. rowling captivated readers with her first book in the harry potter series, "harry potter and the philosopher's stone" (1997).  
Head Entity: j.k. rowling  
Tail Entity: harry potter and the philosopher's stone  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: at the request of the stockholm institute for bible translation , mirza khazar translated the bible – the new testament and old testament – into the azerbaijani language .
Head Entity: institute for bible translation
Tail Entity: stockholm
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: The global technology company, headquartered in Cupertino, California, has been a leader in innovation for over two decades.  
Head Entity: global technology company  
Tail Entity: Cupertino  

Relation: headquarters location  
Context: The renowned automotive manufacturer moved its headquarters from Detroit to Austin, Texas, to better align with its growth strategy.  
Head Entity: automotive manufacturer  
Tail Entity: Austin  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most well-known, commonly referred to as the gray wolf, which is classified under the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of flowering plants, the family Rosaceae encompasses a wide variety of species, including roses and strawberries, which are categorized under the order Rosales.  
Head Entity: Rosaceae  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: he later collaborated with vito volterra and explored laplace transforms and other parts of functional analysis .
Head Entity: vito volterra
Tail Entity: functional analysis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: Marie Curie conducted pioneering research on radioactivity, which laid the groundwork for advancements in nuclear physics.  
Head Entity: Marie Curie  
Tail Entity: nuclear physics  

Relation: field of work  
Context: The company specializes in artificial intelligence and machine learning, providing innovative solutions for various industries.  
Head Entity: The company  
Tail Entity: artificial intelligence  
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: this early involvement in radio led him to buy many small , under performing radio stations beginning with wcos in columbia , south carolina , and turn them around with new formats and improved management .
Head Entity: wcos
Tail Entity: columbia , south carolina
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: The new television station, KXYZ, has been granted permission to broadcast to the entire metropolitan area of San Francisco, ensuring that local viewers have access to a variety of programming.  
Head Entity: KXYZ  
Tail Entity: San Francisco  

Relation: licensed to broadcast to  
Context: After acquiring the rights, the radio station WABC can now officially broadcast to listeners in the greater New York City area, reaching millions of potential audience members.  
Head Entity: WABC  
Tail Entity: greater New York City area  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: ngc 192 is a type sba spiral galaxy located in the constellation cetus .
Head Entity: ngc 192
Tail Entity: cetus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: the orion nebula is located in the constellation orion and is one of the brightest nebulae visible to the naked eye.  
Head Entity: orion nebula  
Tail Entity: orion  

Relation: constellation  
Context: the star sirius is part of the constellation canis major, which is known for its prominence in the night sky.  
Head Entity: sirius  
Tail Entity: canis major  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: services to the island run from san juan 's luis muñoz marín international airport , ceiba airport or isla grande airport ( 20- to 30-minute flight ) .
Head Entity: isla grande airport
Tail Entity: san juan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The city of Barcelona is well-connected to various destinations through the El Prat Airport, which offers flights to numerous European cities.  
Head Entity: El Prat Airport  
Tail Entity: Barcelona  

Relation: place served by transport hub  
Context: The central train station in Zurich provides access to multiple Swiss cities and international destinations, making it a key transport hub in the region.  
Head Entity: Zurich central train station  
Tail Entity: Zurich  
MemoryTrain:  epoch 15, batch     0 | loss: 4.4271185MemoryTrain:  epoch 15, batch     1 | loss: 4.7554801MemoryTrain:  epoch 15, batch     2 | loss: 5.4399713MemoryTrain:  epoch 11, batch     3 | loss: 9.0235901MemoryTrain:  epoch 15, batch     0 | loss: 5.7034869MemoryTrain:  epoch 15, batch     1 | loss: 5.7758675MemoryTrain:  epoch 15, batch     2 | loss: 7.8487938MemoryTrain:  epoch 11, batch     3 | loss: 6.6490273MemoryTrain:  epoch 15, batch     0 | loss: 3.2099686MemoryTrain:  epoch 15, batch     1 | loss: 5.5898663MemoryTrain:  epoch 15, batch     2 | loss: 4.9710061MemoryTrain:  epoch 11, batch     3 | loss: 2.6079558MemoryTrain:  epoch 15, batch     0 | loss: 4.9575467MemoryTrain:  epoch 15, batch     1 | loss: 2.9212115MemoryTrain:  epoch 15, batch     2 | loss: 5.3646888MemoryTrain:  epoch 11, batch     3 | loss: 3.5755872MemoryTrain:  epoch 15, batch     0 | loss: 5.2886048MemoryTrain:  epoch 15, batch     1 | loss: 3.2342465MemoryTrain:  epoch 15, batch     2 | loss: 2.2157022MemoryTrain:  epoch 11, batch     3 | loss: 2.3939284MemoryTrain:  epoch 15, batch     0 | loss: 2.5690030MemoryTrain:  epoch 15, batch     1 | loss: 2.2590120MemoryTrain:  epoch 15, batch     2 | loss: 5.9606523MemoryTrain:  epoch 11, batch     3 | loss: 2.5347380MemoryTrain:  epoch 15, batch     0 | loss: 3.2060534MemoryTrain:  epoch 15, batch     1 | loss: 3.2713890MemoryTrain:  epoch 15, batch     2 | loss: 5.2965274MemoryTrain:  epoch 11, batch     3 | loss: 2.7171410MemoryTrain:  epoch 15, batch     0 | loss: 8.6517951MemoryTrain:  epoch 15, batch     1 | loss: 4.1173065MemoryTrain:  epoch 15, batch     2 | loss: 3.0062127MemoryTrain:  epoch 11, batch     3 | loss: 2.6257108MemoryTrain:  epoch 15, batch     0 | loss: 5.4300681MemoryTrain:  epoch 15, batch     1 | loss: 4.1969817MemoryTrain:  epoch 15, batch     2 | loss: 4.7552256MemoryTrain:  epoch 11, batch     3 | loss: 2.6003164MemoryTrain:  epoch 15, batch     0 | loss: 5.2686543MemoryTrain:  epoch 15, batch     1 | loss: 4.0962477MemoryTrain:  epoch 15, batch     2 | loss: 1.8576833MemoryTrain:  epoch 11, batch     3 | loss: 2.5240938
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 95.54%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 96.09%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 96.53%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 97.16%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 96.35%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 96.15%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 95.09%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 93.75%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 91.80%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 90.07%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 88.54%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 85.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 86.08%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 85.33%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:   24 | acc: 37.50%,  total acc: 83.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.13%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.27%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.78%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.69%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 86.52%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 86.36%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 85.85%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 85.71%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 85.24%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 84.80%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 84.87%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.26%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.62%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.98%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 86.31%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 86.63%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 86.79%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 87.08%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 87.36%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 87.63%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 87.89%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.14%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 88.38%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 88.24%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 88.34%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 88.44%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 88.66%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 88.64%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 88.84%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 88.93%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 88.90%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 88.88%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 89.24%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 89.21%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 88.59%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 78.98%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 78.65%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 79.81%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 80.80%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 81.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 85.20%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 85.71%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 85.23%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 85.33%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 85.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.06%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 86.57%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 86.83%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 87.07%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 88.97%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 89.11%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 89.41%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 89.70%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 89.97%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.22%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.70%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.92%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 91.13%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 91.34%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 91.39%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 91.30%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 91.22%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 91.28%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.45%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 91.50%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 91.42%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 91.35%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 91.39%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 91.20%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 90.68%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 90.57%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 90.52%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 90.68%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 90.83%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 90.98%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 91.03%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 91.11%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 91.38%   [EVAL] batch:   66 | acc: 87.50%,  total acc: 91.32%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 91.45%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 91.58%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 91.61%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 91.73%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 91.84%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 91.95%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 91.98%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 92.00%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 91.78%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 91.80%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 91.59%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 90.98%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 90.62%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 90.35%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 90.09%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 89.91%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 89.88%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 89.71%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 89.61%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 89.08%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 88.99%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 89.12%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 89.24%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 89.35%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 89.47%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 89.69%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 89.47%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 89.39%   [EVAL] batch:   96 | acc: 75.00%,  total acc: 89.24%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 88.97%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 88.95%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 88.69%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 88.80%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 88.91%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 89.02%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 89.12%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 89.23%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 89.27%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 89.37%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 89.47%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 89.56%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 89.66%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 89.75%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 89.88%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 89.80%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 89.92%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 89.96%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 89.99%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 90.02%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 90.05%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 90.03%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 90.06%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 90.14%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 90.17%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 90.20%   
cur_acc:  ['0.9494', '0.8859']
his_acc:  ['0.9494', '0.9020']
CurrentTrain: epoch 15, batch     0 | loss: 13.6059546CurrentTrain: epoch 15, batch     1 | loss: 17.4753020CurrentTrain: epoch 15, batch     2 | loss: 13.4607949CurrentTrain: epoch  1, batch     3 | loss: 10.4101283CurrentTrain: epoch 15, batch     0 | loss: 17.5867166CurrentTrain: epoch 15, batch     1 | loss: 14.8199421CurrentTrain: epoch 15, batch     2 | loss: 14.0690184CurrentTrain: epoch  1, batch     3 | loss: 7.5095979CurrentTrain: epoch 15, batch     0 | loss: 15.0281782CurrentTrain: epoch 15, batch     1 | loss: 13.3909838CurrentTrain: epoch 15, batch     2 | loss: 15.1751542CurrentTrain: epoch  1, batch     3 | loss: 9.4593319CurrentTrain: epoch 15, batch     0 | loss: 12.3010801CurrentTrain: epoch 15, batch     1 | loss: 11.7360730CurrentTrain: epoch 15, batch     2 | loss: 12.1481326CurrentTrain: epoch  1, batch     3 | loss: 9.0030515CurrentTrain: epoch 15, batch     0 | loss: 11.4485368CurrentTrain: epoch 15, batch     1 | loss: 15.1234948CurrentTrain: epoch 15, batch     2 | loss: 8.4555803CurrentTrain: epoch  1, batch     3 | loss: 7.9828851CurrentTrain: epoch 15, batch     0 | loss: 8.8637212CurrentTrain: epoch 15, batch     1 | loss: 8.4218674CurrentTrain: epoch 15, batch     2 | loss: 8.3426333CurrentTrain: epoch  1, batch     3 | loss: 7.3856175CurrentTrain: epoch 15, batch     0 | loss: 9.1936553CurrentTrain: epoch 15, batch     1 | loss: 11.1466535CurrentTrain: epoch 15, batch     2 | loss: 10.3141244CurrentTrain: epoch  1, batch     3 | loss: 7.7900936CurrentTrain: epoch 15, batch     0 | loss: 13.9675140CurrentTrain: epoch 15, batch     1 | loss: 8.4309073CurrentTrain: epoch 15, batch     2 | loss: 12.3988160CurrentTrain: epoch  1, batch     3 | loss: 8.8852022CurrentTrain: epoch 15, batch     0 | loss: 6.2463235CurrentTrain: epoch 15, batch     1 | loss: 6.9533598CurrentTrain: epoch 15, batch     2 | loss: 9.2830340CurrentTrain: epoch  1, batch     3 | loss: 8.7231646CurrentTrain: epoch 15, batch     0 | loss: 8.0281451CurrentTrain: epoch 15, batch     1 | loss: 8.4396527CurrentTrain: epoch 15, batch     2 | loss: 7.6963500CurrentTrain: epoch  1, batch     3 | loss: 6.3533943
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: all of mainland australia 's alpine areas , including its highest mountain , mount kosciuszko ( ahd ) , are part of this range , called the main range .
Head Entity: mount kosciuszko
Tail Entity: main range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The Great Barrier Reef, which is the world's largest coral reef system, is situated off the coast of Queensland, Australia, and is a UNESCO World Heritage site.  
Head Entity: Great Barrier Reef  
Tail Entity: Queensland  

Relation: located on terrain feature  
Context: The Rocky Mountains stretch from Canada down to New Mexico, providing a stunning backdrop for outdoor activities and wildlife.  
Head Entity: Rocky Mountains  
Tail Entity: New Mexico  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, held in paris in 2015, saw numerous world leaders and organizations come together to discuss global warming and its impacts.  
Head Entity: united nations climate change conference  
Tail Entity: paris agreement  

Relation: participant of  
Context: the 2022 fifa world cup in qatar featured teams from around the globe competing for the prestigious trophy, showcasing the best of international football.  
Head Entity: 2022 fifa world cup  
Tail Entity: qatar
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: " runaway reptar " was written by ali marie matheson and jon cooksey and directed by john holmquist and jim duffy .
Head Entity: runaway reptar
Tail Entity: john holmquist
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: "Inception" was directed by Christopher Nolan and features a complex narrative structure that keeps viewers on the edge of their seats.  
Head Entity: Inception  
Tail Entity: Christopher Nolan  

Relation: director  
Context: The acclaimed stage play "Hamlet" was directed by Kenneth Branagh, who brought a fresh perspective to the classic Shakespearean tale.  
Head Entity: Hamlet  
Tail Entity: Kenneth Branagh  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: chernoff also reported and anchored for time warner 's now - defunct cnnfn .
Head Entity: cnnfn
Tail Entity: time warner
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: The famous painting "Starry Night" is part of the collection at the Museum of Modern Art, which is owned by the MoMA.  
Head Entity: MoMA  
Tail Entity: Museum of Modern Art  

Relation: owned by  
Context: The iconic brand Nike is owned by the multinational corporation Nike, Inc., which was founded in 1964.  
Head Entity: Nike, Inc.  
Tail Entity: Nike  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: thessaly was home to extensive neolithic and chalcolithic cultures around 6000 bc–2500 bc ( see cardium pottery , dimini and sesklo ) .
Head Entity: cardium pottery
Tail Entity: neolithic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The engine is a crucial part of the car's overall functionality, providing the necessary power to drive.  
Head Entity: engine  
Tail Entity: car  

Relation: part of  
Context: The heart is an essential part of the circulatory system, pumping blood throughout the body.  
Head Entity: heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: the brewery closed in 1996 and is now the site of the halliwell jones stadium , home of warrington wolves , as well as a tesco store .
Head Entity: halliwell jones stadium
Tail Entity: warrington wolves
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was transformed into a cultural center, which now hosts the local art gallery and serves as the headquarters for the community theater group.  
Head Entity: cultural center  
Tail Entity: community theater group  

Relation: occupant  
Context: After the renovation, the old factory became a vibrant co-working space, attracting freelancers and startups from the tech industry.  
Head Entity: co-working space  
Tail Entity: tech industry
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: the art nouveau style was brought to italy by figures such as giuseppe sommaruga and ernesto basile ( who respectively designed the palazzo castiglioni and expanded the palazzo montecitorio in rome ) .
Head Entity: palazzo castiglioni
Tail Entity: giuseppe sommaruga
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: The iconic Sydney Opera House was designed by Danish architect Jørn Utzon, who won the design competition in 1957.  
Head Entity: Sydney Opera House  
Tail Entity: Jørn Utzon  

Relation: architect  
Context: Frank Lloyd Wright is renowned for his innovative designs, including the famous Fallingwater house, which seamlessly integrates with its natural surroundings.  
Head Entity: Fallingwater  
Tail Entity: Frank Lloyd Wright  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: cyrus alexander was born in pennsylvania , and his family soon moved to illinois .
Head Entity: cyrus alexander
Tail Entity: illinois
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: after living in new york for several years, emily decided to relocate to california for a fresh start.  
Head Entity: emily  
Tail Entity: california  

Relation: residence  
Context: during his childhood, michael spent most of his time in texas before moving to florida as a teenager.  
Head Entity: michael  
Tail Entity: florida  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Ludwig van Beethoven created some of the most celebrated symphonies in classical music history.  
Head Entity: symphonies  
Tail Entity: Ludwig van Beethoven  

Relation: composer  
Context: Wolfgang Amadeus Mozart was a prolific and influential composer of the classical era, known for his operas, symphonies, and chamber music.  
Head Entity: operas  
Tail Entity: Wolfgang Amadeus Mozart  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: one polish archer qualified for the women 's individual recurve by obtaining one of the eight olympic places available from the 2015 world archery championships in copenhagen , denmark .
Head Entity: 2015 world archery championships
Tail Entity: copenhagen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: The annual music festival will take place in the vibrant city of Austin, Texas, attracting thousands of fans from around the world.  
Head Entity: annual music festival  
Tail Entity: Austin, Texas  

Relation: location  
Context: The historic battle was fought near the banks of the Potomac River, a significant site in American history.  
Head Entity: historic battle  
Tail Entity: Potomac River  
MemoryTrain:  epoch 15, batch     0 | loss: 8.3520607MemoryTrain:  epoch 15, batch     1 | loss: 5.5972578MemoryTrain:  epoch 15, batch     2 | loss: 4.5091977MemoryTrain:  epoch 15, batch     3 | loss: 4.6706480MemoryTrain:  epoch 15, batch     4 | loss: 6.0325480MemoryTrain:  epoch  9, batch     5 | loss: 6.8648737MemoryTrain:  epoch 15, batch     0 | loss: 7.6058498MemoryTrain:  epoch 15, batch     1 | loss: 3.3163082MemoryTrain:  epoch 15, batch     2 | loss: 3.0537282MemoryTrain:  epoch 15, batch     3 | loss: 5.3791496MemoryTrain:  epoch 15, batch     4 | loss: 5.1982064MemoryTrain:  epoch  9, batch     5 | loss: 3.2338703MemoryTrain:  epoch 15, batch     0 | loss: 5.3679977MemoryTrain:  epoch 15, batch     1 | loss: 3.8074765MemoryTrain:  epoch 15, batch     2 | loss: 4.7567894MemoryTrain:  epoch 15, batch     3 | loss: 5.0438350MemoryTrain:  epoch 15, batch     4 | loss: 3.5318288MemoryTrain:  epoch  9, batch     5 | loss: 4.5859856MemoryTrain:  epoch 15, batch     0 | loss: 3.6235257MemoryTrain:  epoch 15, batch     1 | loss: 3.0809126MemoryTrain:  epoch 15, batch     2 | loss: 4.4174024MemoryTrain:  epoch 15, batch     3 | loss: 5.7820888MemoryTrain:  epoch 15, batch     4 | loss: 2.6746226MemoryTrain:  epoch  9, batch     5 | loss: 7.9544707MemoryTrain:  epoch 15, batch     0 | loss: 5.8033495MemoryTrain:  epoch 15, batch     1 | loss: 2.4549455MemoryTrain:  epoch 15, batch     2 | loss: 7.7014378MemoryTrain:  epoch 15, batch     3 | loss: 2.4133776MemoryTrain:  epoch 15, batch     4 | loss: 2.2274503MemoryTrain:  epoch  9, batch     5 | loss: 1.9439941MemoryTrain:  epoch 15, batch     0 | loss: 2.8847468MemoryTrain:  epoch 15, batch     1 | loss: 4.2549149MemoryTrain:  epoch 15, batch     2 | loss: 2.4044261MemoryTrain:  epoch 15, batch     3 | loss: 3.1180810MemoryTrain:  epoch 15, batch     4 | loss: 2.3395044MemoryTrain:  epoch  9, batch     5 | loss: 1.4587175MemoryTrain:  epoch 15, batch     0 | loss: 2.7668315MemoryTrain:  epoch 15, batch     1 | loss: 3.6035218MemoryTrain:  epoch 15, batch     2 | loss: 3.2174853MemoryTrain:  epoch 15, batch     3 | loss: 1.9883800MemoryTrain:  epoch 15, batch     4 | loss: 4.1788333MemoryTrain:  epoch  9, batch     5 | loss: 2.9460954MemoryTrain:  epoch 15, batch     0 | loss: 2.3537259MemoryTrain:  epoch 15, batch     1 | loss: 2.0036458MemoryTrain:  epoch 15, batch     2 | loss: 3.9686580MemoryTrain:  epoch 15, batch     3 | loss: 3.0852096MemoryTrain:  epoch 15, batch     4 | loss: 2.3301020MemoryTrain:  epoch  9, batch     5 | loss: 2.6264042MemoryTrain:  epoch 15, batch     0 | loss: 2.2976457MemoryTrain:  epoch 15, batch     1 | loss: 3.9030475MemoryTrain:  epoch 15, batch     2 | loss: 2.1395519MemoryTrain:  epoch 15, batch     3 | loss: 1.8602613MemoryTrain:  epoch 15, batch     4 | loss: 1.5599940MemoryTrain:  epoch  9, batch     5 | loss: 3.5403152MemoryTrain:  epoch 15, batch     0 | loss: 1.6765939MemoryTrain:  epoch 15, batch     1 | loss: 4.7562096MemoryTrain:  epoch 15, batch     2 | loss: 2.5331328MemoryTrain:  epoch 15, batch     3 | loss: 4.1607724MemoryTrain:  epoch 15, batch     4 | loss: 5.1306136MemoryTrain:  epoch  9, batch     5 | loss: 3.9201280
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 39.06%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 42.50%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 49.11%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 54.69%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 59.72%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 63.75%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 66.48%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 69.27%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 70.67%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 69.20%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 69.92%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 69.85%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 69.10%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 68.09%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 67.81%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 67.56%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 68.47%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 69.29%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 69.27%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 69.75%   [EVAL] batch:   25 | acc: 25.00%,  total acc: 68.03%   [EVAL] batch:   26 | acc: 37.50%,  total acc: 66.90%   [EVAL] batch:   27 | acc: 12.50%,  total acc: 64.96%   [EVAL] batch:   28 | acc: 37.50%,  total acc: 64.01%   [EVAL] batch:   29 | acc: 31.25%,  total acc: 62.92%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 61.09%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 61.33%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 62.31%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 62.68%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 63.75%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 64.41%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 65.03%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 65.79%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 66.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 67.68%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 68.15%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 68.90%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 69.46%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 70.24%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 70.21%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 70.44%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 70.66%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 70.88%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 70.34%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 70.31%   [EVAL] batch:   52 | acc: 37.50%,  total acc: 69.69%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 69.79%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 69.55%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 69.53%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 69.30%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 68.97%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 68.64%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 68.54%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 68.24%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 68.35%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 67.66%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 67.97%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 68.06%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 66.48%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 65.10%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 65.38%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 67.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 69.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 72.43%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 73.61%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 74.67%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 75.31%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 75.30%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 75.27%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 75.52%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 76.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.92%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 77.78%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 78.57%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 79.09%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 79.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.44%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 81.05%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.63%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 82.17%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 82.99%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 83.45%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 83.88%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.29%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.06%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 85.76%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 86.14%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 86.04%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 86.07%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 86.35%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 86.15%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 86.18%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 86.20%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 85.76%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 85.34%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 85.16%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 85.20%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 85.24%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 85.49%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 85.73%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 85.96%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 86.19%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 86.31%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 86.43%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 86.63%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 86.84%   [EVAL] batch:   66 | acc: 87.50%,  total acc: 86.85%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 87.04%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 87.23%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 87.14%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 87.24%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 87.24%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 87.41%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 87.42%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 87.33%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 86.92%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 86.77%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 86.46%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 85.92%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 85.55%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 85.19%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 85.06%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 85.02%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 84.97%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 84.63%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 84.59%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 84.27%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 84.23%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 84.41%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 84.58%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 84.75%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 84.92%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 85.08%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 85.24%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 85.07%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 85.09%   [EVAL] batch:   96 | acc: 75.00%,  total acc: 84.99%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 84.76%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 84.72%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 84.50%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 84.65%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 84.80%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 84.95%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 85.10%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 85.24%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 85.32%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 85.46%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 85.59%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 85.72%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 85.85%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 85.98%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 86.10%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 86.17%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 86.13%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 86.20%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 86.31%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 86.38%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 86.44%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 86.50%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 86.56%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 86.57%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 86.63%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 86.74%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 86.84%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 86.90%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 86.46%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 86.02%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 85.64%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 85.42%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 85.19%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 84.92%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 84.90%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 84.96%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 85.07%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 85.19%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 85.25%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 85.36%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 85.37%   [EVAL] batch:  138 | acc: 50.00%,  total acc: 85.12%   [EVAL] batch:  139 | acc: 62.50%,  total acc: 84.96%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 84.97%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 84.86%   [EVAL] batch:  142 | acc: 56.25%,  total acc: 84.66%   [EVAL] batch:  143 | acc: 50.00%,  total acc: 84.42%   [EVAL] batch:  144 | acc: 62.50%,  total acc: 84.27%   [EVAL] batch:  145 | acc: 62.50%,  total acc: 84.12%   [EVAL] batch:  146 | acc: 87.50%,  total acc: 84.14%   [EVAL] batch:  147 | acc: 87.50%,  total acc: 84.16%   [EVAL] batch:  148 | acc: 68.75%,  total acc: 84.06%   [EVAL] batch:  149 | acc: 81.25%,  total acc: 84.04%   [EVAL] batch:  150 | acc: 25.00%,  total acc: 83.65%   [EVAL] batch:  151 | acc: 37.50%,  total acc: 83.35%   [EVAL] batch:  152 | acc: 12.50%,  total acc: 82.88%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 82.59%   [EVAL] batch:  154 | acc: 31.25%,  total acc: 82.26%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 81.77%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 81.69%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 81.76%   [EVAL] batch:  158 | acc: 75.00%,  total acc: 81.72%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 81.84%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 81.87%   [EVAL] batch:  161 | acc: 87.50%,  total acc: 81.91%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 81.98%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 82.01%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 82.12%   [EVAL] batch:  165 | acc: 87.50%,  total acc: 82.15%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 82.19%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 82.29%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 82.36%   [EVAL] batch:  169 | acc: 93.75%,  total acc: 82.43%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 82.42%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 82.34%   [EVAL] batch:  172 | acc: 81.25%,  total acc: 82.33%   [EVAL] batch:  173 | acc: 81.25%,  total acc: 82.33%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 82.32%   [EVAL] batch:  175 | acc: 43.75%,  total acc: 82.10%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 82.03%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 81.78%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 81.74%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 81.60%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 81.53%   [EVAL] batch:  181 | acc: 56.25%,  total acc: 81.39%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 81.22%   [EVAL] batch:  183 | acc: 50.00%,  total acc: 81.05%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 80.95%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 80.78%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 80.75%   [EVAL] batch:  187 | acc: 25.00%,  total acc: 80.45%   
cur_acc:  ['0.9494', '0.8859', '0.6766']
his_acc:  ['0.9494', '0.9020', '0.8045']
CurrentTrain: epoch 15, batch     0 | loss: 23.3329581CurrentTrain: epoch 15, batch     1 | loss: 21.3514538CurrentTrain: epoch 15, batch     2 | loss: 21.0292225CurrentTrain: epoch  1, batch     3 | loss: 14.1442414CurrentTrain: epoch 15, batch     0 | loss: 18.6971937CurrentTrain: epoch 15, batch     1 | loss: 16.2018151CurrentTrain: epoch 15, batch     2 | loss: 13.4920007CurrentTrain: epoch  1, batch     3 | loss: 8.3827298CurrentTrain: epoch 15, batch     0 | loss: 16.8894696CurrentTrain: epoch 15, batch     1 | loss: 11.5593016CurrentTrain: epoch 15, batch     2 | loss: 11.6417873CurrentTrain: epoch  1, batch     3 | loss: 8.9376272CurrentTrain: epoch 15, batch     0 | loss: 12.1019788CurrentTrain: epoch 15, batch     1 | loss: 9.2941042CurrentTrain: epoch 15, batch     2 | loss: 13.9180940CurrentTrain: epoch  1, batch     3 | loss: 6.7050482CurrentTrain: epoch 15, batch     0 | loss: 8.6500980CurrentTrain: epoch 15, batch     1 | loss: 8.4447216CurrentTrain: epoch 15, batch     2 | loss: 14.7667087CurrentTrain: epoch  1, batch     3 | loss: 7.3592135CurrentTrain: epoch 15, batch     0 | loss: 12.1289452CurrentTrain: epoch 15, batch     1 | loss: 9.3250674CurrentTrain: epoch 15, batch     2 | loss: 7.9781942CurrentTrain: epoch  1, batch     3 | loss: 7.7054526CurrentTrain: epoch 15, batch     0 | loss: 13.4484372CurrentTrain: epoch 15, batch     1 | loss: 5.4226514CurrentTrain: epoch 15, batch     2 | loss: 6.6411475CurrentTrain: epoch  1, batch     3 | loss: 12.0828374CurrentTrain: epoch 15, batch     0 | loss: 8.9666074CurrentTrain: epoch 15, batch     1 | loss: 10.2563159CurrentTrain: epoch 15, batch     2 | loss: 12.9859385CurrentTrain: epoch  1, batch     3 | loss: 7.2326420CurrentTrain: epoch 15, batch     0 | loss: 7.5128232CurrentTrain: epoch 15, batch     1 | loss: 6.3148688CurrentTrain: epoch 15, batch     2 | loss: 9.0177328CurrentTrain: epoch  1, batch     3 | loss: 7.3219555CurrentTrain: epoch 15, batch     0 | loss: 6.7212028CurrentTrain: epoch 15, batch     1 | loss: 7.5348324CurrentTrain: epoch 15, batch     2 | loss: 6.7103427CurrentTrain: epoch  1, batch     3 | loss: 13.9190876
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the cantons of créteil are administrative divisions of the val - de - marne department , île - de - france region , northern france .
Head Entity: île - de - france
Tail Entity: val - de - marne
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The provinces of Canada are significant administrative divisions within the country, with Ontario being one of the largest provinces.  
Head Entity: Canada  
Tail Entity: Ontario  

Relation: contains administrative territorial entity  
Context: The states of the United States are the primary administrative divisions, with California being one of the most populous states.  
Head Entity: United States  
Tail Entity: California  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: enzo is an italian given name derivative of heinz , a form of henry .
Head Entity: henry
Tail Entity: heinz
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as each other due to their close genetic relationship.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: In some circles, the terms "artificial intelligence" and "machine learning" are said to be the same as each other, although experts argue there are key differences.  
Head Entity: artificial intelligence  
Tail Entity: machine learning  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: he considers his father to be biggest influence on his career as his brother javier castellano recipient of four eclipse award for outstanding jockey in the row ( 2013 , 2014,2015 and 2016 ) .
Head Entity: eclipse award for outstanding jockey
Tail Entity: javier castellano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: In the thrilling finale of the championship, Sarah Thompson emerged victorious, claiming the title of best player in the tournament, while her teammate, Mark Johnson, was awarded the runner-up position.  
Head Entity: best player in the tournament  
Tail Entity: Sarah Thompson  

Relation: winner  
Context: The annual science fair concluded with Emily Chen taking home the grand prize for her innovative project on renewable energy, while her classmate, David Lee, received an honorable mention for his work on robotics.  
Head Entity: grand prize  
Tail Entity: Emily Chen  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: the calais region was defended by the 15th army under " generaloberst " ( colonel general ) hans von salmuth , and normandy by the 7th army commanded by " generaloberst " friedrich dollmann .
Head Entity: hans von salmuth
Tail Entity: generaloberst
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: during the battle of stalingrad, the soviet forces were led by general vasily chuikov, who held the rank of major general, while the german troops were commanded by general friedrich paulus.  
Head Entity: vasily chuikov  
Tail Entity: major general  

Relation: military rank  
Context: admiral horatio nelson was a prominent figure in the royal navy, known for his leadership during the napoleonic wars, where he held the rank of vice admiral.  
Head Entity: horatio nelson  
Tail Entity: vice admiral  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: tokyopop also released a collection of karin short stories under the title : " chibi vampire : airmail " on august 31 , 2010 .
Head Entity: karin
Tail Entity: tokyopop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: Penguin Random House announced the release of a new novel by the acclaimed author John Grisham, set to hit the shelves on June 15, 2021.  
Head Entity: John Grisham  
Tail Entity: Penguin Random House  

Relation: publisher  
Context: The popular science magazine, Scientific American, published an article discussing the latest advancements in renewable energy technologies.  
Head Entity: Scientific American  
Tail Entity: renewable energy technologies  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " muzai moratorium " and " shōso strip " were released on vinyl in 2008 , as a part of her 10-year anniversary celebrations .
Head Entity: muzai moratorium
Tail Entity: shōso strip
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first chapter of the book introduces the main character, while the second chapter follows with a detailed backstory.  
Head Entity: first chapter  
Tail Entity: second chapter  

Relation: followed by  
Context: In the concert lineup, the opening act performed a lively set, followed by the headliner who captivated the audience with their greatest hits.  
Head Entity: opening act  
Tail Entity: headliner  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: in 1694 , he found employment at the court chapel in weimar and was promoted to vice capellmaster ( " " ) in 1695 , succeeding august kühnel , with samuel drese as capellmaster .
Head Entity: august kühnel
Tail Entity: weimar
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: After completing his studies, John accepted a position at the tech startup in Silicon Valley, where he contributed to several innovative projects.  
Head Entity: John  
Tail Entity: Silicon Valley  

Relation: work location  
Context: The renowned architect designed several buildings in the bustling city of New York, where he gained significant recognition for his work.  
Head Entity: the renowned architect  
Tail Entity: New York  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in cancer treatment.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a talented software engineer, has been working at tech innovations for over five years.  
Head Entity: john smith  
Tail Entity: software engineer  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a World Heritage Site, attracting tourists from around the globe.  
Head Entity: Petra  
Tail Entity: World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: beverly bower ( september 30 , 1925 – march 24 , 2002 ) was an american operatic soprano who had an active international opera career from the mid-1950s through the early 1970s .
Head Entity: beverly bower
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti ( october 12 , 1935 – september 6 , 2007 ) was an italian operatic tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey ( born march 27 , 1969 ) is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: soprano  
MemoryTrain:  epoch 15, batch     0 | loss: 6.0423190MemoryTrain:  epoch 15, batch     1 | loss: 4.8527187MemoryTrain:  epoch 15, batch     2 | loss: 2.9890977MemoryTrain:  epoch 15, batch     3 | loss: 4.7241528MemoryTrain:  epoch 15, batch     4 | loss: 3.7990191MemoryTrain:  epoch 15, batch     5 | loss: 3.2768788MemoryTrain:  epoch 15, batch     6 | loss: 5.9816394MemoryTrain:  epoch  7, batch     7 | loss: 6.8349726MemoryTrain:  epoch 15, batch     0 | loss: 4.0725967MemoryTrain:  epoch 15, batch     1 | loss: 4.8237305MemoryTrain:  epoch 15, batch     2 | loss: 3.7571621MemoryTrain:  epoch 15, batch     3 | loss: 6.6793141MemoryTrain:  epoch 15, batch     4 | loss: 1.9547028MemoryTrain:  epoch 15, batch     5 | loss: 2.5804373MemoryTrain:  epoch 15, batch     6 | loss: 4.6230905MemoryTrain:  epoch  7, batch     7 | loss: 3.9078662MemoryTrain:  epoch 15, batch     0 | loss: 2.3515477MemoryTrain:  epoch 15, batch     1 | loss: 4.7199881MemoryTrain:  epoch 15, batch     2 | loss: 3.3988952MemoryTrain:  epoch 15, batch     3 | loss: 2.4430766MemoryTrain:  epoch 15, batch     4 | loss: 4.3832716MemoryTrain:  epoch 15, batch     5 | loss: 2.3048460MemoryTrain:  epoch 15, batch     6 | loss: 2.3674595MemoryTrain:  epoch  7, batch     7 | loss: 2.8152364MemoryTrain:  epoch 15, batch     0 | loss: 4.5343029MemoryTrain:  epoch 15, batch     1 | loss: 3.9526833MemoryTrain:  epoch 15, batch     2 | loss: 4.2854011MemoryTrain:  epoch 15, batch     3 | loss: 1.9522778MemoryTrain:  epoch 15, batch     4 | loss: 2.1659138MemoryTrain:  epoch 15, batch     5 | loss: 2.6264514MemoryTrain:  epoch 15, batch     6 | loss: 2.1343681MemoryTrain:  epoch  7, batch     7 | loss: 2.5431415MemoryTrain:  epoch 15, batch     0 | loss: 2.6657026MemoryTrain:  epoch 15, batch     1 | loss: 2.6801997MemoryTrain:  epoch 15, batch     2 | loss: 2.5725938MemoryTrain:  epoch 15, batch     3 | loss: 2.0413142MemoryTrain:  epoch 15, batch     4 | loss: 1.9013897MemoryTrain:  epoch 15, batch     5 | loss: 3.3246669MemoryTrain:  epoch 15, batch     6 | loss: 1.6891336MemoryTrain:  epoch  7, batch     7 | loss: 1.4321233MemoryTrain:  epoch 15, batch     0 | loss: 2.1797648MemoryTrain:  epoch 15, batch     1 | loss: 4.2435784MemoryTrain:  epoch 15, batch     2 | loss: 2.1710524MemoryTrain:  epoch 15, batch     3 | loss: 4.5138943MemoryTrain:  epoch 15, batch     4 | loss: 2.0232486MemoryTrain:  epoch 15, batch     5 | loss: 2.0651097MemoryTrain:  epoch 15, batch     6 | loss: 5.1132025MemoryTrain:  epoch  7, batch     7 | loss: 4.2902746MemoryTrain:  epoch 15, batch     0 | loss: 1.6589771MemoryTrain:  epoch 15, batch     1 | loss: 4.6363237MemoryTrain:  epoch 15, batch     2 | loss: 2.9968293MemoryTrain:  epoch 15, batch     3 | loss: 1.8597509MemoryTrain:  epoch 15, batch     4 | loss: 2.7382090MemoryTrain:  epoch 15, batch     5 | loss: 2.1982325MemoryTrain:  epoch 15, batch     6 | loss: 2.1586571MemoryTrain:  epoch  7, batch     7 | loss: 3.9348115MemoryTrain:  epoch 15, batch     0 | loss: 1.8185753MemoryTrain:  epoch 15, batch     1 | loss: 1.6462298MemoryTrain:  epoch 15, batch     2 | loss: 1.9078052MemoryTrain:  epoch 15, batch     3 | loss: 4.0980364MemoryTrain:  epoch 15, batch     4 | loss: 1.8716688MemoryTrain:  epoch 15, batch     5 | loss: 1.6790758MemoryTrain:  epoch 15, batch     6 | loss: 1.7359738MemoryTrain:  epoch  7, batch     7 | loss: 1.5059032MemoryTrain:  epoch 15, batch     0 | loss: 1.4828782MemoryTrain:  epoch 15, batch     1 | loss: 1.7003067MemoryTrain:  epoch 15, batch     2 | loss: 3.8308309MemoryTrain:  epoch 15, batch     3 | loss: 1.9524779MemoryTrain:  epoch 15, batch     4 | loss: 1.8131540MemoryTrain:  epoch 15, batch     5 | loss: 1.9420035MemoryTrain:  epoch 15, batch     6 | loss: 1.3039115MemoryTrain:  epoch  7, batch     7 | loss: 1.6994893MemoryTrain:  epoch 15, batch     0 | loss: 1.5640570MemoryTrain:  epoch 15, batch     1 | loss: 2.0249629MemoryTrain:  epoch 15, batch     2 | loss: 3.4637080MemoryTrain:  epoch 15, batch     3 | loss: 3.7822549MemoryTrain:  epoch 15, batch     4 | loss: 4.5632576MemoryTrain:  epoch 15, batch     5 | loss: 1.6367982MemoryTrain:  epoch 15, batch     6 | loss: 1.6976736MemoryTrain:  epoch  7, batch     7 | loss: 2.4448211
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 26.25%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 29.17%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 31.25%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 36.72%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 40.97%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 43.12%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 43.18%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 45.31%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 49.04%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 51.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 51.56%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 52.94%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 54.51%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 55.92%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 57.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 59.52%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 61.36%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 63.04%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 64.06%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 65.25%   [EVAL] batch:   25 | acc: 68.75%,  total acc: 65.38%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 65.51%   [EVAL] batch:   27 | acc: 75.00%,  total acc: 65.85%   [EVAL] batch:   28 | acc: 75.00%,  total acc: 66.16%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 65.62%   [EVAL] batch:   30 | acc: 50.00%,  total acc: 65.12%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 65.23%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 65.15%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 63.97%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 63.39%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 62.15%   [EVAL] batch:   36 | acc: 25.00%,  total acc: 61.15%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 61.51%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 61.86%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 62.96%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 63.69%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 64.24%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 64.77%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 65.00%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 65.22%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 65.56%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 65.82%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 66.00%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 67.31%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 67.92%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 68.52%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 69.09%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 69.64%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 70.18%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 70.69%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 71.19%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 71.67%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 72.03%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 72.48%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 72.12%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 69.64%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 69.44%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 65.91%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 64.06%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 64.42%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 66.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 68.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 72.06%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 73.61%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 75.62%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 76.49%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 75.85%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 75.54%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 76.04%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 76.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.40%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.24%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.02%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 79.53%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.85%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 81.45%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.01%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 82.54%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 82.86%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 83.78%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 84.21%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.37%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 85.57%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 85.76%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 86.08%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 85.83%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 85.87%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 85.77%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 85.68%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 85.97%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 85.75%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 85.66%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 85.58%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 85.50%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 85.19%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 84.66%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 84.60%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 84.65%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 84.70%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 84.96%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 85.10%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 85.35%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 85.48%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 85.74%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 85.96%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 86.17%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 86.10%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 86.31%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 86.50%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 86.43%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 86.53%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 86.63%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 86.82%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 86.82%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 86.83%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 86.51%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 86.36%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 86.06%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 85.52%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 85.08%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 84.72%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 84.22%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 84.04%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 83.85%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 83.60%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 83.36%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 83.12%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 83.10%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 83.29%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 83.47%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 83.65%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 83.83%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 84.01%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 84.11%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 83.68%   [EVAL] batch:   95 | acc: 75.00%,  total acc: 83.59%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 83.38%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 83.10%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 83.02%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 82.50%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 82.67%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 82.84%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 83.01%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 83.17%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 83.43%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 83.74%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 83.89%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 84.03%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 84.18%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 84.32%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 84.40%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 84.43%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 84.51%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 84.64%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 84.72%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 84.80%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 84.87%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 84.95%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 84.97%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 85.04%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 85.23%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 85.30%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 84.82%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 84.40%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 84.03%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 83.72%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 83.51%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 83.25%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 83.10%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 83.18%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 83.30%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 83.38%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 83.46%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 83.53%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 83.51%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 83.32%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 83.12%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 83.16%   [EVAL] batch:  141 | acc: 62.50%,  total acc: 83.01%   [EVAL] batch:  142 | acc: 62.50%,  total acc: 82.87%   [EVAL] batch:  143 | acc: 43.75%,  total acc: 82.60%   [EVAL] batch:  144 | acc: 56.25%,  total acc: 82.41%   [EVAL] batch:  145 | acc: 43.75%,  total acc: 82.15%   [EVAL] batch:  146 | acc: 87.50%,  total acc: 82.19%   [EVAL] batch:  147 | acc: 75.00%,  total acc: 82.14%   [EVAL] batch:  148 | acc: 50.00%,  total acc: 81.92%   [EVAL] batch:  149 | acc: 75.00%,  total acc: 81.88%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 81.37%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 80.96%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 80.56%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 80.07%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 79.68%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 79.21%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 79.10%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 79.23%   [EVAL] batch:  158 | acc: 75.00%,  total acc: 79.21%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 79.34%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 79.43%   [EVAL] batch:  161 | acc: 87.50%,  total acc: 79.48%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 79.56%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 79.61%   [EVAL] batch:  164 | acc: 93.75%,  total acc: 79.70%   [EVAL] batch:  165 | acc: 87.50%,  total acc: 79.74%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 79.83%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 79.95%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 79.99%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 79.71%   [EVAL] batch:  170 | acc: 6.25%,  total acc: 79.28%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 78.89%   [EVAL] batch:  172 | acc: 6.25%,  total acc: 78.47%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 78.12%   [EVAL] batch:  174 | acc: 6.25%,  total acc: 77.71%   [EVAL] batch:  175 | acc: 43.75%,  total acc: 77.52%   [EVAL] batch:  176 | acc: 75.00%,  total acc: 77.51%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 77.28%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 77.27%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 77.19%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 77.14%   [EVAL] batch:  181 | acc: 62.50%,  total acc: 77.06%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 76.95%   [EVAL] batch:  183 | acc: 56.25%,  total acc: 76.83%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 76.79%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 76.65%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 76.67%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 76.46%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 76.22%   [EVAL] batch:  189 | acc: 12.50%,  total acc: 75.89%   [EVAL] batch:  190 | acc: 37.50%,  total acc: 75.69%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 75.42%   [EVAL] batch:  192 | acc: 37.50%,  total acc: 75.23%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 75.03%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 74.97%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 74.97%   [EVAL] batch:  197 | acc: 62.50%,  total acc: 74.91%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 74.69%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 74.81%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 74.81%   [EVAL] batch:  201 | acc: 62.50%,  total acc: 74.75%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 74.72%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 74.66%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 74.70%   [EVAL] batch:  205 | acc: 81.25%,  total acc: 74.73%   [EVAL] batch:  206 | acc: 75.00%,  total acc: 74.73%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 74.85%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 74.97%   [EVAL] batch:  209 | acc: 100.00%,  total acc: 75.09%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 75.18%   [EVAL] batch:  211 | acc: 87.50%,  total acc: 75.24%   [EVAL] batch:  212 | acc: 81.25%,  total acc: 75.26%   [EVAL] batch:  213 | acc: 68.75%,  total acc: 75.23%   [EVAL] batch:  214 | acc: 68.75%,  total acc: 75.20%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 75.26%   [EVAL] batch:  216 | acc: 50.00%,  total acc: 75.14%   [EVAL] batch:  217 | acc: 62.50%,  total acc: 75.09%   [EVAL] batch:  218 | acc: 50.00%,  total acc: 74.97%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 74.94%   [EVAL] batch:  220 | acc: 31.25%,  total acc: 74.75%   [EVAL] batch:  221 | acc: 43.75%,  total acc: 74.61%   [EVAL] batch:  222 | acc: 31.25%,  total acc: 74.41%   [EVAL] batch:  223 | acc: 25.00%,  total acc: 74.19%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 74.08%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 74.06%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 74.09%   [EVAL] batch:  227 | acc: 81.25%,  total acc: 74.12%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 74.24%   [EVAL] batch:  229 | acc: 81.25%,  total acc: 74.27%   [EVAL] batch:  230 | acc: 87.50%,  total acc: 74.32%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 74.41%   [EVAL] batch:  232 | acc: 62.50%,  total acc: 74.36%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 74.44%   [EVAL] batch:  234 | acc: 62.50%,  total acc: 74.39%   [EVAL] batch:  235 | acc: 75.00%,  total acc: 74.39%   [EVAL] batch:  236 | acc: 75.00%,  total acc: 74.39%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 74.45%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 74.56%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 74.66%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 74.77%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 74.87%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 74.97%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 75.08%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 75.18%   [EVAL] batch:  245 | acc: 100.00%,  total acc: 75.28%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 75.38%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 75.48%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 75.55%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 75.65%   
cur_acc:  ['0.9494', '0.8859', '0.6766', '0.7212']
his_acc:  ['0.9494', '0.9020', '0.8045', '0.7565']
CurrentTrain: epoch 15, batch     0 | loss: 21.4935005CurrentTrain: epoch 15, batch     1 | loss: 11.6797109CurrentTrain: epoch 15, batch     2 | loss: 10.9022304CurrentTrain: epoch  1, batch     3 | loss: 13.1601786CurrentTrain: epoch 15, batch     0 | loss: 10.2499574CurrentTrain: epoch 15, batch     1 | loss: 18.1347506CurrentTrain: epoch 15, batch     2 | loss: 9.0448247CurrentTrain: epoch  1, batch     3 | loss: 11.4841134CurrentTrain: epoch 15, batch     0 | loss: 9.8297343CurrentTrain: epoch 15, batch     1 | loss: 11.8959324CurrentTrain: epoch 15, batch     2 | loss: 14.3182870CurrentTrain: epoch  1, batch     3 | loss: 6.9978531CurrentTrain: epoch 15, batch     0 | loss: 8.9986641CurrentTrain: epoch 15, batch     1 | loss: 6.8599569CurrentTrain: epoch 15, batch     2 | loss: 8.5466059CurrentTrain: epoch  1, batch     3 | loss: 9.1663187CurrentTrain: epoch 15, batch     0 | loss: 17.8433858CurrentTrain: epoch 15, batch     1 | loss: 10.5669931CurrentTrain: epoch 15, batch     2 | loss: 8.5778377CurrentTrain: epoch  1, batch     3 | loss: 8.9228784CurrentTrain: epoch 15, batch     0 | loss: 20.5293619CurrentTrain: epoch 15, batch     1 | loss: 10.8409450CurrentTrain: epoch 15, batch     2 | loss: 9.6062301CurrentTrain: epoch  1, batch     3 | loss: 6.6123753CurrentTrain: epoch 15, batch     0 | loss: 8.8512123CurrentTrain: epoch 15, batch     1 | loss: 13.9977803CurrentTrain: epoch 15, batch     2 | loss: 9.4722540CurrentTrain: epoch  1, batch     3 | loss: 8.3702389CurrentTrain: epoch 15, batch     0 | loss: 8.8979865CurrentTrain: epoch 15, batch     1 | loss: 18.3868057CurrentTrain: epoch 15, batch     2 | loss: 11.6483502CurrentTrain: epoch  1, batch     3 | loss: 5.4077231CurrentTrain: epoch 15, batch     0 | loss: 8.5385133CurrentTrain: epoch 15, batch     1 | loss: 12.7579471CurrentTrain: epoch 15, batch     2 | loss: 7.6140965CurrentTrain: epoch  1, batch     3 | loss: 6.1292287CurrentTrain: epoch 15, batch     0 | loss: 8.6077722CurrentTrain: epoch 15, batch     1 | loss: 13.0758543CurrentTrain: epoch 15, batch     2 | loss: 8.7176876CurrentTrain: epoch  1, batch     3 | loss: 6.6787963
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: george lawson sheldon ( may 31 , 1870 – april 4 , 1960 ) was 14th governor of nebraska with service from 1907 until 1909 .
Head Entity: governor
Tail Entity: nebraska
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The Environmental Protection Agency (EPA) is responsible for enforcing regulations related to environmental protection in the United States.  
Head Entity: Environmental Protection Agency  
Tail Entity: United States  

Relation: applies to jurisdiction  
Context: The California Consumer Privacy Act (CCPA) provides residents of California with specific rights regarding their personal information.  
Head Entity: California Consumer Privacy Act  
Tail Entity: California  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: " tenth avenue freeze - out " is the second song on bruce springsteen 's breakthrough album " born to run " , released in 1975 .
Head Entity: tenth avenue freeze - out
Tail Entity: bruce springsteen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: "Shape of You" is a popular song by Ed Sheeran that topped charts worldwide after its release in 2017.  
Head Entity: Shape of You  
Tail Entity: Ed Sheeran  

Relation: performer  
Context: The Beatles are widely regarded as one of the most influential bands in the history of music, with hits like "Hey Jude" and "Let It Be."  
Head Entity: The Beatles  
Tail Entity: Hey Jude  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the next full - frame digital slrs were the canon eos-1ds of late 2002 , followed by kodak 's dcs pro 14n in 2003 .
Head Entity: dcs pro 14n
Tail Entity: kodak
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The latest smartphone model, the Galaxy S21, was released by Samsung in January 2021, showcasing advanced camera technology and performance.  
Head Entity: Galaxy S21  
Tail Entity: Samsung  

Relation: manufacturer  
Context: The iconic Model T was produced by Ford Motor Company, revolutionizing the automotive industry in the early 20th century.  
Head Entity: Model T  
Tail Entity: Ford Motor Company  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: norway was represented in the eurovision song contest 2005 by the song " in my dreams " performed by wig wam .
Head Entity: eurovision song contest 2005
Tail Entity: wig wam
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: the united states was represented in the 2020 summer olympics by a team of athletes competing in various sports.  
Head Entity: 2020 summer olympics  
Tail Entity: united states  

Relation: participant  
Context: the annual tech conference featured several startups, including a presentation by innovative solutions inc.  
Head Entity: annual tech conference  
Tail Entity: innovative solutions inc.  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the application is compatible with various operating systems, including macOS, Windows, and Ubuntu.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: Many developers prefer to use Linux as their primary operating system due to its flexibility and open-source nature.  
Head Entity: developers  
Tail Entity: Linux  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in his next bout , he won the wbc light welterweight title by defeating roger mayweather for a second time .
Head Entity: roger mayweather
Tail Entity: light welterweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: The team secured their place in the tournament by qualifying under the UEFA Champions League regulations.  
Head Entity: UEFA Champions League  
Tail Entity: tournament  

Relation: competition class  
Context: The athlete competed in the 100 meters event, which is classified under the Olympic Games standards.  
Head Entity: Olympic Games  
Tail Entity: 100 meters event  
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: the qup gained 3 seats from its previous election result , and won a 10 % swing to it in primary votes , but failed to defeat the incumbent labor government led by ted theodore .
Head Entity: previous election
Tail Entity: ted theodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: After a closely contested campaign, the voters chose Sarah Johnson as their representative, marking a significant shift in the local political landscape.  
Head Entity: closely contested campaign  
Tail Entity: Sarah Johnson  

Relation: successful candidate  
Context: In the recent mayoral election, the community rallied behind Mark Thompson, who promised to bring change and transparency to the office.  
Head Entity: recent mayoral election  
Tail Entity: Mark Thompson  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: the deaths of his brothers wenceslaus ii ( 1487 ) , casimir ii ( 1490 ) and władysław ( 1494 ) allowed jan v to reunificated the whole duchy of zator .
Head Entity: casimir ii
Tail Entity: wenceslaus ii
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: both elizabeth and her brother, charles, were known for their contributions to the arts and culture of their time.  
Head Entity: elizabeth  
Tail Entity: charles  

Relation: sibling  
Context: during the family reunion, it was heartwarming to see how much john and his sister, sarah, resembled each other in both looks and personality.  
Head Entity: john  
Tail Entity: sarah  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: dennis chalker is a retired navy seal , inventor and author who has written six books about the united states navy seals .
Head Entity: dennis chalker
Tail Entity: united states navy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: the general served in the air force for over twenty years before retiring and taking on a civilian role in defense consulting.  
Head Entity: the general  
Tail Entity: air force  

Relation: military branch  
Context: during the ceremony, the admiral was recognized for his service in the coast guard, where he led several important missions.  
Head Entity: the admiral  
Tail Entity: coast guard  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: in 1406 adolf married marie of burgundy , daughter of john the fearless and margaret of bavaria .
Head Entity: john the fearless
Tail Entity: marie of burgundy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: in 1980, michael and sarah welcomed their first child, a daughter named emily, into the world.  
Head Entity: michael  
Tail Entity: emily  

Relation: child  
Context: during the family reunion, it was revealed that elizabeth is the proud mother of three children, including her youngest, a son named alex.  
Head Entity: elizabeth  
Tail Entity: alex  
MemoryTrain:  epoch 15, batch     0 | loss: 2.7604404MemoryTrain:  epoch 15, batch     1 | loss: 5.5572222MemoryTrain:  epoch 15, batch     2 | loss: 5.8237473MemoryTrain:  epoch 15, batch     3 | loss: 2.9482687MemoryTrain:  epoch 15, batch     4 | loss: 3.6234041MemoryTrain:  epoch 15, batch     5 | loss: 3.3533490MemoryTrain:  epoch 15, batch     6 | loss: 4.8338596MemoryTrain:  epoch 15, batch     7 | loss: 3.1419374MemoryTrain:  epoch 15, batch     8 | loss: 3.6842326MemoryTrain:  epoch  5, batch     9 | loss: 9.3697544MemoryTrain:  epoch 15, batch     0 | loss: 6.7999131MemoryTrain:  epoch 15, batch     1 | loss: 2.5821416MemoryTrain:  epoch 15, batch     2 | loss: 2.8653626MemoryTrain:  epoch 15, batch     3 | loss: 3.3461354MemoryTrain:  epoch 15, batch     4 | loss: 2.1350658MemoryTrain:  epoch 15, batch     5 | loss: 3.0279550MemoryTrain:  epoch 15, batch     6 | loss: 2.6054698MemoryTrain:  epoch 15, batch     7 | loss: 2.4536028MemoryTrain:  epoch 15, batch     8 | loss: 3.4941910MemoryTrain:  epoch  5, batch     9 | loss: 9.0594845MemoryTrain:  epoch 15, batch     0 | loss: 2.3051438MemoryTrain:  epoch 15, batch     1 | loss: 1.9695450MemoryTrain:  epoch 15, batch     2 | loss: 2.6032603MemoryTrain:  epoch 15, batch     3 | loss: 5.0876958MemoryTrain:  epoch 15, batch     4 | loss: 4.5645454MemoryTrain:  epoch 15, batch     5 | loss: 4.2653809MemoryTrain:  epoch 15, batch     6 | loss: 2.3583762MemoryTrain:  epoch 15, batch     7 | loss: 2.2087610MemoryTrain:  epoch 15, batch     8 | loss: 2.1036621MemoryTrain:  epoch  5, batch     9 | loss: 8.0514201MemoryTrain:  epoch 15, batch     0 | loss: 1.7275874MemoryTrain:  epoch 15, batch     1 | loss: 4.6254516MemoryTrain:  epoch 15, batch     2 | loss: 2.3485311MemoryTrain:  epoch 15, batch     3 | loss: 3.9409419MemoryTrain:  epoch 15, batch     4 | loss: 2.3454161MemoryTrain:  epoch 15, batch     5 | loss: 2.0775819MemoryTrain:  epoch 15, batch     6 | loss: 2.3544299MemoryTrain:  epoch 15, batch     7 | loss: 3.2082609MemoryTrain:  epoch 15, batch     8 | loss: 4.3875756MemoryTrain:  epoch  5, batch     9 | loss: 8.5984798MemoryTrain:  epoch 15, batch     0 | loss: 2.7282767MemoryTrain:  epoch 15, batch     1 | loss: 2.3101720MemoryTrain:  epoch 15, batch     2 | loss: 2.1235624MemoryTrain:  epoch 15, batch     3 | loss: 4.1266762MemoryTrain:  epoch 15, batch     4 | loss: 2.0191976MemoryTrain:  epoch 15, batch     5 | loss: 2.3066835MemoryTrain:  epoch 15, batch     6 | loss: 2.1170466MemoryTrain:  epoch 15, batch     7 | loss: 4.7019865MemoryTrain:  epoch 15, batch     8 | loss: 2.2301336MemoryTrain:  epoch  5, batch     9 | loss: 8.3261244MemoryTrain:  epoch 15, batch     0 | loss: 2.3903796MemoryTrain:  epoch 15, batch     1 | loss: 1.9135171MemoryTrain:  epoch 15, batch     2 | loss: 1.9749738MemoryTrain:  epoch 15, batch     3 | loss: 4.3216455MemoryTrain:  epoch 15, batch     4 | loss: 1.8021848MemoryTrain:  epoch 15, batch     5 | loss: 4.4110465MemoryTrain:  epoch 15, batch     6 | loss: 1.7737752MemoryTrain:  epoch 15, batch     7 | loss: 2.1647697MemoryTrain:  epoch 15, batch     8 | loss: 4.2521880MemoryTrain:  epoch  5, batch     9 | loss: 7.8252092MemoryTrain:  epoch 15, batch     0 | loss: 1.7936826MemoryTrain:  epoch 15, batch     1 | loss: 2.3158748MemoryTrain:  epoch 15, batch     2 | loss: 2.2082120MemoryTrain:  epoch 15, batch     3 | loss: 2.9012171MemoryTrain:  epoch 15, batch     4 | loss: 2.7634986MemoryTrain:  epoch 15, batch     5 | loss: 2.6891560MemoryTrain:  epoch 15, batch     6 | loss: 3.7943687MemoryTrain:  epoch 15, batch     7 | loss: 1.9532645MemoryTrain:  epoch 15, batch     8 | loss: 4.0564152MemoryTrain:  epoch  5, batch     9 | loss: 8.6102492MemoryTrain:  epoch 15, batch     0 | loss: 1.5332546MemoryTrain:  epoch 15, batch     1 | loss: 2.7385273MemoryTrain:  epoch 15, batch     2 | loss: 4.0140602MemoryTrain:  epoch 15, batch     3 | loss: 1.8875420MemoryTrain:  epoch 15, batch     4 | loss: 1.7128243MemoryTrain:  epoch 15, batch     5 | loss: 1.4313442MemoryTrain:  epoch 15, batch     6 | loss: 1.6956799MemoryTrain:  epoch 15, batch     7 | loss: 2.0508613MemoryTrain:  epoch 15, batch     8 | loss: 4.3428000MemoryTrain:  epoch  5, batch     9 | loss: 8.0347986MemoryTrain:  epoch 15, batch     0 | loss: 1.4334469MemoryTrain:  epoch 15, batch     1 | loss: 1.8173857MemoryTrain:  epoch 15, batch     2 | loss: 1.4827943MemoryTrain:  epoch 15, batch     3 | loss: 1.8256174MemoryTrain:  epoch 15, batch     4 | loss: 3.8829474MemoryTrain:  epoch 15, batch     5 | loss: 1.8201245MemoryTrain:  epoch 15, batch     6 | loss: 1.8819339MemoryTrain:  epoch 15, batch     7 | loss: 1.5805002MemoryTrain:  epoch 15, batch     8 | loss: 1.8096349MemoryTrain:  epoch  5, batch     9 | loss: 8.1786107MemoryTrain:  epoch 15, batch     0 | loss: 1.5973936MemoryTrain:  epoch 15, batch     1 | loss: 1.4416272MemoryTrain:  epoch 15, batch     2 | loss: 4.2782967MemoryTrain:  epoch 15, batch     3 | loss: 1.7101250MemoryTrain:  epoch 15, batch     4 | loss: 1.4031686MemoryTrain:  epoch 15, batch     5 | loss: 2.6874396MemoryTrain:  epoch 15, batch     6 | loss: 1.4550922MemoryTrain:  epoch 15, batch     7 | loss: 4.0994028MemoryTrain:  epoch 15, batch     8 | loss: 1.4135833MemoryTrain:  epoch  5, batch     9 | loss: 7.6313864
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 69.64%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 70.31%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 71.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 72.16%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 73.56%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 74.55%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 75.37%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 76.04%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 76.64%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 75.62%   [EVAL] batch:   20 | acc: 56.25%,  total acc: 74.70%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 74.15%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 73.64%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 73.96%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 74.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.24%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 75.93%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 76.79%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 77.16%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 77.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 78.63%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 79.30%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 79.92%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 80.51%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 81.07%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 81.60%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 82.09%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 82.40%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 82.53%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 82.34%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 82.16%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 82.56%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 82.53%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 82.22%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 81.79%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 81.38%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 80.74%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 80.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 81.00%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 81.37%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 81.37%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 81.59%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 81.70%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 81.14%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 80.60%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 80.30%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 79.79%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 79.51%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 78.73%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 77.98%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 70.31%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 68.12%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 64.77%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 62.50%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 62.98%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 65.18%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 67.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 70.59%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 73.03%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 73.75%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 74.11%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 73.58%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 73.10%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 73.70%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 74.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.24%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 76.16%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.01%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 77.59%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.03%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 80.30%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 80.88%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 81.77%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 82.26%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 82.73%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.17%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 83.99%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 84.08%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 84.30%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 84.23%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 83.75%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 83.15%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 82.45%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 82.03%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 81.76%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 81.00%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 81.00%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 81.01%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 81.01%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 80.67%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 80.23%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 80.25%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 80.37%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 80.28%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 80.61%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 80.83%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 80.84%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 81.05%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 81.45%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 81.73%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 82.01%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 82.00%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 82.26%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 82.52%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 82.59%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 82.66%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 82.73%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 82.96%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 83.02%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 83.08%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 82.73%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 82.63%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 82.37%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 81.88%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 81.48%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 81.10%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 80.72%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 80.57%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 80.43%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 80.07%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 79.94%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 79.74%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 79.76%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 79.99%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 80.21%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 80.43%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 80.64%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 80.85%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 80.98%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 80.66%   [EVAL] batch:   95 | acc: 75.00%,  total acc: 80.60%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 80.41%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 80.17%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 80.11%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 79.75%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 79.95%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 80.15%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 80.34%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 80.53%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 80.71%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 80.84%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 81.02%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 81.19%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 81.36%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 81.53%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 81.70%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 81.81%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 81.86%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 81.74%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 81.85%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 81.90%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 81.84%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 81.83%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 81.88%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 81.98%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 82.02%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 82.12%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 82.27%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 82.36%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 82.45%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 81.99%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 81.64%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 81.30%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 80.96%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 80.72%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 80.49%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 80.35%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 80.40%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 80.50%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 80.56%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 80.61%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 80.70%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 80.71%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 80.53%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 80.36%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 80.41%   [EVAL] batch:  141 | acc: 62.50%,  total acc: 80.28%   [EVAL] batch:  142 | acc: 62.50%,  total acc: 80.16%   [EVAL] batch:  143 | acc: 43.75%,  total acc: 79.90%   [EVAL] batch:  144 | acc: 62.50%,  total acc: 79.78%   [EVAL] batch:  145 | acc: 31.25%,  total acc: 79.45%   [EVAL] batch:  146 | acc: 81.25%,  total acc: 79.46%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 79.39%   [EVAL] batch:  148 | acc: 50.00%,  total acc: 79.19%   [EVAL] batch:  149 | acc: 68.75%,  total acc: 79.12%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 78.64%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 78.21%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 77.82%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 77.35%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 76.98%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 76.48%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 76.43%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 76.58%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 76.61%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 76.76%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 76.86%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 76.97%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 77.03%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 77.10%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 77.23%   [EVAL] batch:  165 | acc: 87.50%,  total acc: 77.30%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 77.36%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 77.49%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 77.51%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 77.21%   [EVAL] batch:  170 | acc: 6.25%,  total acc: 76.79%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 76.45%   [EVAL] batch:  172 | acc: 6.25%,  total acc: 76.05%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 75.72%   [EVAL] batch:  174 | acc: 12.50%,  total acc: 75.36%   [EVAL] batch:  175 | acc: 31.25%,  total acc: 75.11%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:  177 | acc: 31.25%,  total acc: 74.75%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 74.62%   [EVAL] batch:  179 | acc: 50.00%,  total acc: 74.48%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 74.45%   [EVAL] batch:  181 | acc: 43.75%,  total acc: 74.28%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 74.18%   [EVAL] batch:  183 | acc: 50.00%,  total acc: 74.05%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 74.09%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 73.96%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 74.00%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 73.80%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 73.58%   [EVAL] batch:  189 | acc: 25.00%,  total acc: 73.32%   [EVAL] batch:  190 | acc: 43.75%,  total acc: 73.17%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 72.92%   [EVAL] batch:  192 | acc: 37.50%,  total acc: 72.73%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 72.55%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 72.53%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 72.51%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 72.49%   [EVAL] batch:  197 | acc: 68.75%,  total acc: 72.47%   [EVAL] batch:  198 | acc: 37.50%,  total acc: 72.30%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 72.44%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 72.36%   [EVAL] batch:  201 | acc: 50.00%,  total acc: 72.25%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 72.17%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 72.09%   [EVAL] batch:  204 | acc: 56.25%,  total acc: 72.01%   [EVAL] batch:  205 | acc: 81.25%,  total acc: 72.06%   [EVAL] batch:  206 | acc: 81.25%,  total acc: 72.10%   [EVAL] batch:  207 | acc: 93.75%,  total acc: 72.21%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 72.34%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 72.44%   [EVAL] batch:  210 | acc: 87.50%,  total acc: 72.51%   [EVAL] batch:  211 | acc: 93.75%,  total acc: 72.61%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 72.62%   [EVAL] batch:  213 | acc: 50.00%,  total acc: 72.52%   [EVAL] batch:  214 | acc: 43.75%,  total acc: 72.38%   [EVAL] batch:  215 | acc: 81.25%,  total acc: 72.42%   [EVAL] batch:  216 | acc: 43.75%,  total acc: 72.29%   [EVAL] batch:  217 | acc: 56.25%,  total acc: 72.22%   [EVAL] batch:  218 | acc: 50.00%,  total acc: 72.12%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 72.07%   [EVAL] batch:  220 | acc: 37.50%,  total acc: 71.92%   [EVAL] batch:  221 | acc: 50.00%,  total acc: 71.82%   [EVAL] batch:  222 | acc: 25.00%,  total acc: 71.61%   [EVAL] batch:  223 | acc: 31.25%,  total acc: 71.43%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 71.33%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 71.35%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 71.39%   [EVAL] batch:  227 | acc: 81.25%,  total acc: 71.44%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 71.56%   [EVAL] batch:  229 | acc: 81.25%,  total acc: 71.60%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 71.70%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 71.79%   [EVAL] batch:  232 | acc: 81.25%,  total acc: 71.83%   [EVAL] batch:  233 | acc: 87.50%,  total acc: 71.90%   [EVAL] batch:  234 | acc: 87.50%,  total acc: 71.97%   [EVAL] batch:  235 | acc: 68.75%,  total acc: 71.95%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 72.02%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 72.11%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 72.23%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 72.34%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 72.46%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 72.57%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 72.69%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 72.80%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 72.91%   [EVAL] batch:  245 | acc: 100.00%,  total acc: 73.02%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 73.13%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 73.24%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 73.34%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 73.45%   [EVAL] batch:  250 | acc: 75.00%,  total acc: 73.46%   [EVAL] batch:  251 | acc: 62.50%,  total acc: 73.41%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 73.44%   [EVAL] batch:  253 | acc: 56.25%,  total acc: 73.38%   [EVAL] batch:  254 | acc: 68.75%,  total acc: 73.36%   [EVAL] batch:  255 | acc: 68.75%,  total acc: 73.34%   [EVAL] batch:  256 | acc: 75.00%,  total acc: 73.35%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 73.35%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 73.33%   [EVAL] batch:  259 | acc: 81.25%,  total acc: 73.37%   [EVAL] batch:  260 | acc: 81.25%,  total acc: 73.40%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 73.38%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 73.46%   [EVAL] batch:  263 | acc: 87.50%,  total acc: 73.51%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 73.54%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 73.54%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 73.57%   [EVAL] batch:  267 | acc: 87.50%,  total acc: 73.62%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 73.68%   [EVAL] batch:  269 | acc: 56.25%,  total acc: 73.61%   [EVAL] batch:  270 | acc: 56.25%,  total acc: 73.55%   [EVAL] batch:  271 | acc: 62.50%,  total acc: 73.51%   [EVAL] batch:  272 | acc: 62.50%,  total acc: 73.47%   [EVAL] batch:  273 | acc: 81.25%,  total acc: 73.49%   [EVAL] batch:  274 | acc: 81.25%,  total acc: 73.52%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 73.62%   [EVAL] batch:  276 | acc: 93.75%,  total acc: 73.69%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 73.79%   [EVAL] batch:  278 | acc: 87.50%,  total acc: 73.84%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 73.93%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 74.02%   [EVAL] batch:  281 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 74.20%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 74.30%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 74.39%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 74.48%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 74.56%   [EVAL] batch:  287 | acc: 93.75%,  total acc: 74.63%   [EVAL] batch:  288 | acc: 87.50%,  total acc: 74.68%   [EVAL] batch:  289 | acc: 75.00%,  total acc: 74.68%   [EVAL] batch:  290 | acc: 75.00%,  total acc: 74.68%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 74.72%   [EVAL] batch:  292 | acc: 93.75%,  total acc: 74.79%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 74.81%   [EVAL] batch:  294 | acc: 68.75%,  total acc: 74.79%   [EVAL] batch:  295 | acc: 62.50%,  total acc: 74.75%   [EVAL] batch:  296 | acc: 62.50%,  total acc: 74.71%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 74.71%   [EVAL] batch:  298 | acc: 56.25%,  total acc: 74.64%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 74.67%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 74.73%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 74.79%   [EVAL] batch:  302 | acc: 87.50%,  total acc: 74.83%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 74.86%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 74.92%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 74.96%   [EVAL] batch:  306 | acc: 50.00%,  total acc: 74.88%   [EVAL] batch:  307 | acc: 50.00%,  total acc: 74.80%   [EVAL] batch:  308 | acc: 62.50%,  total acc: 74.76%   [EVAL] batch:  309 | acc: 50.00%,  total acc: 74.68%   [EVAL] batch:  310 | acc: 62.50%,  total acc: 74.64%   [EVAL] batch:  311 | acc: 31.25%,  total acc: 74.50%   [EVAL] batch:  312 | acc: 31.25%,  total acc: 74.36%   
cur_acc:  ['0.9494', '0.8859', '0.6766', '0.7212', '0.7798']
his_acc:  ['0.9494', '0.9020', '0.8045', '0.7565', '0.7436']
CurrentTrain: epoch 15, batch     0 | loss: 20.6736272CurrentTrain: epoch 15, batch     1 | loss: 14.8666524CurrentTrain: epoch 15, batch     2 | loss: 16.5531558CurrentTrain: epoch  1, batch     3 | loss: 13.0197236CurrentTrain: epoch 15, batch     0 | loss: 9.8277250CurrentTrain: epoch 15, batch     1 | loss: 10.8945000CurrentTrain: epoch 15, batch     2 | loss: 9.7664898CurrentTrain: epoch  1, batch     3 | loss: 7.4197080CurrentTrain: epoch 15, batch     0 | loss: 15.4493798CurrentTrain: epoch 15, batch     1 | loss: 17.3655077CurrentTrain: epoch 15, batch     2 | loss: 16.9480557CurrentTrain: epoch  1, batch     3 | loss: 7.6628612CurrentTrain: epoch 15, batch     0 | loss: 11.3872920CurrentTrain: epoch 15, batch     1 | loss: 10.8407128CurrentTrain: epoch 15, batch     2 | loss: 9.4002789CurrentTrain: epoch  1, batch     3 | loss: 8.9996430CurrentTrain: epoch 15, batch     0 | loss: 7.4269793CurrentTrain: epoch 15, batch     1 | loss: 16.4771674CurrentTrain: epoch 15, batch     2 | loss: 8.9793646CurrentTrain: epoch  1, batch     3 | loss: 7.1820894CurrentTrain: epoch 15, batch     0 | loss: 9.7299014CurrentTrain: epoch 15, batch     1 | loss: 16.8619819CurrentTrain: epoch 15, batch     2 | loss: 10.3911559CurrentTrain: epoch  1, batch     3 | loss: 6.1451342CurrentTrain: epoch 15, batch     0 | loss: 10.3608176CurrentTrain: epoch 15, batch     1 | loss: 17.2986281CurrentTrain: epoch 15, batch     2 | loss: 8.5700828CurrentTrain: epoch  1, batch     3 | loss: 9.7951061CurrentTrain: epoch 15, batch     0 | loss: 12.3688507CurrentTrain: epoch 15, batch     1 | loss: 14.2449806CurrentTrain: epoch 15, batch     2 | loss: 9.9668515CurrentTrain: epoch  1, batch     3 | loss: 6.8254281CurrentTrain: epoch 15, batch     0 | loss: 14.9711568CurrentTrain: epoch 15, batch     1 | loss: 9.2113765CurrentTrain: epoch 15, batch     2 | loss: 11.8820094CurrentTrain: epoch  1, batch     3 | loss: 16.5099431CurrentTrain: epoch 15, batch     0 | loss: 7.4888520CurrentTrain: epoch 15, batch     1 | loss: 8.4621316CurrentTrain: epoch 15, batch     2 | loss: 8.7766575CurrentTrain: epoch  1, batch     3 | loss: 5.6741989
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: ( 1988 ) , " night watch " ( 2004 ) , " day watch " ( 2006 ) and " legend № 17 " ( 2013 ) .
Head Entity: day watch
Tail Entity: night watch
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: ( 1995 ) , " The Phantom Menace " ( 1999 ) , " Attack of the Clones " ( 2002 ) and " Revenge of the Sith " ( 2005 ) .  
Head Entity: Attack of the Clones  
Tail Entity: The Phantom Menace  

Relation: follows  
Context: ( 2010 ) , " The Hunger Games " ( 2012 ) , " Catching Fire " ( 2013 ) and " Mockingjay " ( 2014 ) .  
Head Entity: Catching Fire  
Tail Entity: The Hunger Games  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: it is primarily used for baseball and serves as the home stadium for yaquis de obregón .
Head Entity: yaquis de obregón
Tail Entity: baseball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: she has been a professional athlete in tennis since she was a teenager and has won several championships.  
Head Entity: she  
Tail Entity: tennis  

Relation: sport  
Context: the city is known for its vibrant soccer culture and is home to several professional teams.  
Head Entity: the city  
Tail Entity: soccer  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael, the youngest son, had always looked up to his father, robert, for guidance and support.  
Head Entity: michael  
Tail Entity: robert  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: since 2009 , culshaw has starred in the bbc one comedy sketch show " the impressions show " alongside debra stephenson .
Head Entity: the impressions show
Tail Entity: bbc one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: The popular series "Breaking Bad" first aired on AMC, captivating audiences with its intense storytelling and character development.  
Head Entity: Breaking Bad  
Tail Entity: AMC  

Relation: original network  
Context: "Friends" became a cultural phenomenon after it premiered on NBC, showcasing the lives of six friends living in New York City.  
Head Entity: Friends  
Tail Entity: NBC  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: he was also nominated twice for the filmfare best telugu actor award , for the films bhale bhale magadivoy and " gentleman " .
Head Entity: gentleman
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: The movie "Parasite" was critically acclaimed and won several awards, showcasing the brilliance of its original Korean dialogue.  
Head Entity: Parasite  
Tail Entity: Korean  

Relation: original language of film or TV show  
Context: The animated series "Naruto" gained immense popularity worldwide, but it was originally produced in Japanese.  
Head Entity: Naruto  
Tail Entity: Japanese  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: in the summer of 2013 he returned to montenegro and joined with fk rudar pljevlja , playing with the club for two seasons in the montenegrin first league .
Head Entity: fk rudar pljevlja
Tail Entity: montenegrin first league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: After a successful stint in the English Premier League, the player signed with FC Barcelona, competing in La Liga for the next three seasons.  
Head Entity: FC Barcelona  
Tail Entity: La Liga  

Relation: league  
Context: The young athlete made headlines when he joined the New York Yankees, showcasing his talent in Major League Baseball during the summer season.  
Head Entity: New York Yankees  
Tail Entity: Major League Baseball  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: she is supposedly the third and youngest child of seti i and tuya , and the younger sister of ramesses ii and tia .
Head Entity: tia
Tail Entity: tuya
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: Cleopatra was the daughter of Ptolemy XII and his wife, who was also her mother, Cleopatra VI.  
Head Entity: Cleopatra  
Tail Entity: Cleopatra VI  

Relation: mother  
Context: In Norse mythology, Loki is said to have fathered several children, including the goddess Hel, who is the daughter of the giantess Angerboda, her mother.  
Head Entity: Hel  
Tail Entity: Angerboda  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres. he has dedicated his life to music and is known for his mastery of the cello.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: ploegsteert is a village in belgium located in the municipality of comines - warneton in the hainaut province and is the most westerly settlement of the walloon region .
Head Entity: hainaut
Tail Entity: belgium
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the city of prague is the capital of the czech republic and is known for its beautiful architecture and rich history.  
Head Entity: prague  
Tail Entity: czech republic  

Relation: country  
Context: the great barrier reef is located off the coast of australia and is the largest coral reef system in the world.  
Head Entity: great barrier reef  
Tail Entity: australia  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: fox then cast him as philip marlowe in " the brasher doubloon " ( 1947 ) , a b picture version of the novel " the high window " by raymond chandler .
Head Entity: the high window
Tail Entity: philip marlowe
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: in the animated series "avatar: the last airbender," the character aang is the last airbender and the avatar who must bring peace to the world.  
Head Entity: avatar: the last airbender  
Tail Entity: aang  

Relation: characters  
Context: the novel "pride and prejudice" features elizabeth bennet, who navigates issues of class, marriage, and morality in early 19th century england.  
Head Entity: pride and prejudice  
Tail Entity: elizabeth bennet  
MemoryTrain:  epoch 15, batch     0 | loss: 4.2073145MemoryTrain:  epoch 15, batch     1 | loss: 3.1414031MemoryTrain:  epoch 15, batch     2 | loss: 3.5244701MemoryTrain:  epoch 15, batch     3 | loss: 3.8195227MemoryTrain:  epoch 15, batch     4 | loss: 2.5415170MemoryTrain:  epoch 15, batch     5 | loss: 2.6411457MemoryTrain:  epoch 15, batch     6 | loss: 3.5872228MemoryTrain:  epoch 15, batch     7 | loss: 5.4947283MemoryTrain:  epoch 15, batch     8 | loss: 3.2786607MemoryTrain:  epoch 15, batch     9 | loss: 5.9228431MemoryTrain:  epoch 15, batch    10 | loss: 2.6557632MemoryTrain:  epoch  3, batch    11 | loss: 11.0562698MemoryTrain:  epoch 15, batch     0 | loss: 3.0602651MemoryTrain:  epoch 15, batch     1 | loss: 5.3813721MemoryTrain:  epoch 15, batch     2 | loss: 3.6241839MemoryTrain:  epoch 15, batch     3 | loss: 2.6977734MemoryTrain:  epoch 15, batch     4 | loss: 2.2893108MemoryTrain:  epoch 15, batch     5 | loss: 2.6913745MemoryTrain:  epoch 15, batch     6 | loss: 3.2374961MemoryTrain:  epoch 15, batch     7 | loss: 2.3468150MemoryTrain:  epoch 15, batch     8 | loss: 2.2347994MemoryTrain:  epoch 15, batch     9 | loss: 3.0487077MemoryTrain:  epoch 15, batch    10 | loss: 4.8553011MemoryTrain:  epoch  3, batch    11 | loss: 10.2612160MemoryTrain:  epoch 15, batch     0 | loss: 3.9416347MemoryTrain:  epoch 15, batch     1 | loss: 2.8123293MemoryTrain:  epoch 15, batch     2 | loss: 3.0439358MemoryTrain:  epoch 15, batch     3 | loss: 2.0696412MemoryTrain:  epoch 15, batch     4 | loss: 4.4650707MemoryTrain:  epoch 15, batch     5 | loss: 2.5051509MemoryTrain:  epoch 15, batch     6 | loss: 3.0088913MemoryTrain:  epoch 15, batch     7 | loss: 3.0877105MemoryTrain:  epoch 15, batch     8 | loss: 2.2337764MemoryTrain:  epoch 15, batch     9 | loss: 2.7452017MemoryTrain:  epoch 15, batch    10 | loss: 5.2275888MemoryTrain:  epoch  3, batch    11 | loss: 11.3150916MemoryTrain:  epoch 15, batch     0 | loss: 2.1019256MemoryTrain:  epoch 15, batch     1 | loss: 4.6889363MemoryTrain:  epoch 15, batch     2 | loss: 3.8946654MemoryTrain:  epoch 15, batch     3 | loss: 2.0614210MemoryTrain:  epoch 15, batch     4 | loss: 2.4669178MemoryTrain:  epoch 15, batch     5 | loss: 1.8464916MemoryTrain:  epoch 15, batch     6 | loss: 2.9008154MemoryTrain:  epoch 15, batch     7 | loss: 2.3205415MemoryTrain:  epoch 15, batch     8 | loss: 2.6335801MemoryTrain:  epoch 15, batch     9 | loss: 2.1308573MemoryTrain:  epoch 15, batch    10 | loss: 2.1243121MemoryTrain:  epoch  3, batch    11 | loss: 9.8403688MemoryTrain:  epoch 15, batch     0 | loss: 4.5789268MemoryTrain:  epoch 15, batch     1 | loss: 1.9155767MemoryTrain:  epoch 15, batch     2 | loss: 2.1196826MemoryTrain:  epoch 15, batch     3 | loss: 1.9082965MemoryTrain:  epoch 15, batch     4 | loss: 2.0333048MemoryTrain:  epoch 15, batch     5 | loss: 4.4522335MemoryTrain:  epoch 15, batch     6 | loss: 1.5775041MemoryTrain:  epoch 15, batch     7 | loss: 1.7787966MemoryTrain:  epoch 15, batch     8 | loss: 2.4790477MemoryTrain:  epoch 15, batch     9 | loss: 4.1147708MemoryTrain:  epoch 15, batch    10 | loss: 2.0466342MemoryTrain:  epoch  3, batch    11 | loss: 10.9100587MemoryTrain:  epoch 15, batch     0 | loss: 1.5790896MemoryTrain:  epoch 15, batch     1 | loss: 1.7324251MemoryTrain:  epoch 15, batch     2 | loss: 1.7912895MemoryTrain:  epoch 15, batch     3 | loss: 3.9725759MemoryTrain:  epoch 15, batch     4 | loss: 2.1578160MemoryTrain:  epoch 15, batch     5 | loss: 3.9004643MemoryTrain:  epoch 15, batch     6 | loss: 2.4565624MemoryTrain:  epoch 15, batch     7 | loss: 2.2014135MemoryTrain:  epoch 15, batch     8 | loss: 1.7799018MemoryTrain:  epoch 15, batch     9 | loss: 2.0294524MemoryTrain:  epoch 15, batch    10 | loss: 1.8486778MemoryTrain:  epoch  3, batch    11 | loss: 10.0219638MemoryTrain:  epoch 15, batch     0 | loss: 1.7227968MemoryTrain:  epoch 15, batch     1 | loss: 4.4765310MemoryTrain:  epoch 15, batch     2 | loss: 1.5398251MemoryTrain:  epoch 15, batch     3 | loss: 1.5573540MemoryTrain:  epoch 15, batch     4 | loss: 1.6873624MemoryTrain:  epoch 15, batch     5 | loss: 1.6565382MemoryTrain:  epoch 15, batch     6 | loss: 1.8330911MemoryTrain:  epoch 15, batch     7 | loss: 3.5853846MemoryTrain:  epoch 15, batch     8 | loss: 1.5835277MemoryTrain:  epoch 15, batch     9 | loss: 1.5093153MemoryTrain:  epoch 15, batch    10 | loss: 1.8945058MemoryTrain:  epoch  3, batch    11 | loss: 10.5079227MemoryTrain:  epoch 15, batch     0 | loss: 2.0174118MemoryTrain:  epoch 15, batch     1 | loss: 6.1354707MemoryTrain:  epoch 15, batch     2 | loss: 1.9248442MemoryTrain:  epoch 15, batch     3 | loss: 1.9386855MemoryTrain:  epoch 15, batch     4 | loss: 1.4595093MemoryTrain:  epoch 15, batch     5 | loss: 1.3923441MemoryTrain:  epoch 15, batch     6 | loss: 3.6478425MemoryTrain:  epoch 15, batch     7 | loss: 1.6025982MemoryTrain:  epoch 15, batch     8 | loss: 1.7305887MemoryTrain:  epoch 15, batch     9 | loss: 2.2020350MemoryTrain:  epoch 15, batch    10 | loss: 1.6589332MemoryTrain:  epoch  3, batch    11 | loss: 9.7282199MemoryTrain:  epoch 15, batch     0 | loss: 2.5929705MemoryTrain:  epoch 15, batch     1 | loss: 1.5648557MemoryTrain:  epoch 15, batch     2 | loss: 1.5799965MemoryTrain:  epoch 15, batch     3 | loss: 5.0922060MemoryTrain:  epoch 15, batch     4 | loss: 1.3719310MemoryTrain:  epoch 15, batch     5 | loss: 1.9045469MemoryTrain:  epoch 15, batch     6 | loss: 1.7532009MemoryTrain:  epoch 15, batch     7 | loss: 2.3208423MemoryTrain:  epoch 15, batch     8 | loss: 1.7202894MemoryTrain:  epoch 15, batch     9 | loss: 1.4411546MemoryTrain:  epoch 15, batch    10 | loss: 3.9789745MemoryTrain:  epoch  3, batch    11 | loss: 9.6075306MemoryTrain:  epoch 15, batch     0 | loss: 4.5426738MemoryTrain:  epoch 15, batch     1 | loss: 2.2400868MemoryTrain:  epoch 15, batch     2 | loss: 1.5911390MemoryTrain:  epoch 15, batch     3 | loss: 2.4013335MemoryTrain:  epoch 15, batch     4 | loss: 4.6413148MemoryTrain:  epoch 15, batch     5 | loss: 1.8421329MemoryTrain:  epoch 15, batch     6 | loss: 1.5185856MemoryTrain:  epoch 15, batch     7 | loss: 3.7728562MemoryTrain:  epoch 15, batch     8 | loss: 1.3406653MemoryTrain:  epoch 15, batch     9 | loss: 1.6221215MemoryTrain:  epoch 15, batch    10 | loss: 1.3956172MemoryTrain:  epoch  3, batch    11 | loss: 9.9784715
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 20.83%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 23.44%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 22.50%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 21.88%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 27.68%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 35.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 43.06%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 48.75%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 51.70%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 55.21%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 55.36%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 55.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 55.08%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 55.51%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 55.90%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 55.92%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 58.13%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 60.12%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 61.65%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 63.32%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 64.58%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 66.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 67.31%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 68.52%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 69.42%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 70.47%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 71.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 72.38%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 73.24%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 74.05%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 74.63%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 75.18%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 75.69%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 76.35%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 76.15%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 74.84%   [EVAL] batch:   39 | acc: 37.50%,  total acc: 73.91%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 73.02%   [EVAL] batch:   41 | acc: 25.00%,  total acc: 71.88%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 70.49%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 70.31%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 71.20%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 71.81%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 72.14%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 72.32%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 72.75%   [EVAL] batch:   50 | acc: 50.00%,  total acc: 72.30%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 72.24%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 71.82%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 71.99%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 72.05%   [EVAL] batch:   55 | acc: 25.00%,  total acc: 71.21%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 70.83%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 70.58%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 70.23%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 70.31%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 70.59%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 70.87%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 70.44%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 73.75%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 70.45%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 69.71%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 71.43%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 74.22%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 75.37%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 76.39%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 77.30%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 77.68%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 76.99%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 76.36%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 76.30%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 76.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.64%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.47%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.24%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 79.74%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.42%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.05%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 81.64%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.20%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 82.72%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 83.51%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 83.95%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.78%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.52%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 85.90%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 85.80%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 85.28%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 84.78%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 84.44%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 84.11%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 84.31%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 83.62%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 83.58%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 83.53%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 83.49%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 83.22%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 82.73%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 82.59%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 82.35%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 82.33%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 82.10%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 82.08%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 82.17%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 82.16%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 82.14%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 82.32%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 82.60%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 82.86%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 82.84%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 83.09%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 83.24%   [EVAL] batch:   69 | acc: 50.00%,  total acc: 82.77%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 82.48%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 82.12%   [EVAL] batch:   72 | acc: 56.25%,  total acc: 81.76%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 81.59%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 80.92%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 80.84%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 80.53%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 80.06%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 79.61%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 79.24%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 78.81%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 78.69%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 78.65%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 78.24%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 77.95%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 77.98%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 78.23%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 78.47%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 78.71%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 78.94%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 79.32%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 79.08%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 79.10%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 78.93%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 78.70%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 78.66%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 78.25%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 78.47%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 78.68%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 78.88%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 79.09%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 79.29%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 79.36%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 79.56%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 79.75%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 79.93%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 80.11%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 80.29%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 80.37%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 80.26%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 80.27%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 80.33%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 80.29%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 80.30%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 80.36%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 80.47%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 80.53%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 80.64%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 80.79%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 80.85%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 80.95%   [EVAL] batch:  125 | acc: 18.75%,  total acc: 80.46%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 80.12%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 79.79%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 79.51%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 79.38%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 79.15%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 79.02%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 79.09%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 79.20%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 79.26%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 79.37%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 79.43%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 79.44%   [EVAL] batch:  138 | acc: 68.75%,  total acc: 79.36%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 79.29%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 79.34%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 79.31%   [EVAL] batch:  142 | acc: 93.75%,  total acc: 79.41%   [EVAL] batch:  143 | acc: 62.50%,  total acc: 79.30%   [EVAL] batch:  144 | acc: 56.25%,  total acc: 79.14%   [EVAL] batch:  145 | acc: 25.00%,  total acc: 78.77%   [EVAL] batch:  146 | acc: 81.25%,  total acc: 78.78%   [EVAL] batch:  147 | acc: 62.50%,  total acc: 78.67%   [EVAL] batch:  148 | acc: 56.25%,  total acc: 78.52%   [EVAL] batch:  149 | acc: 68.75%,  total acc: 78.46%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 77.98%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 77.51%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 77.04%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 76.58%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 76.17%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 75.68%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 75.64%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 75.75%   [EVAL] batch:  158 | acc: 75.00%,  total acc: 75.75%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 75.90%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 76.01%   [EVAL] batch:  161 | acc: 87.50%,  total acc: 76.08%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 76.15%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 76.22%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 76.36%   [EVAL] batch:  165 | acc: 87.50%,  total acc: 76.43%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 76.53%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 76.67%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 76.74%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 76.43%   [EVAL] batch:  170 | acc: 0.00%,  total acc: 75.99%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 75.62%   [EVAL] batch:  172 | acc: 6.25%,  total acc: 75.22%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 74.89%   [EVAL] batch:  174 | acc: 12.50%,  total acc: 74.54%   [EVAL] batch:  175 | acc: 25.00%,  total acc: 74.25%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 74.15%   [EVAL] batch:  177 | acc: 31.25%,  total acc: 73.91%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 73.78%   [EVAL] batch:  179 | acc: 50.00%,  total acc: 73.65%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 73.58%   [EVAL] batch:  181 | acc: 43.75%,  total acc: 73.42%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 73.29%   [EVAL] batch:  183 | acc: 50.00%,  total acc: 73.17%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 73.18%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 73.05%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 73.09%   [EVAL] batch:  187 | acc: 31.25%,  total acc: 72.87%   [EVAL] batch:  188 | acc: 37.50%,  total acc: 72.69%   [EVAL] batch:  189 | acc: 50.00%,  total acc: 72.57%   [EVAL] batch:  190 | acc: 37.50%,  total acc: 72.38%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 72.14%   [EVAL] batch:  192 | acc: 43.75%,  total acc: 71.99%   [EVAL] batch:  193 | acc: 43.75%,  total acc: 71.84%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 71.83%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 71.81%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 71.80%   [EVAL] batch:  197 | acc: 62.50%,  total acc: 71.75%   [EVAL] batch:  198 | acc: 25.00%,  total acc: 71.51%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 71.66%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 71.58%   [EVAL] batch:  201 | acc: 50.00%,  total acc: 71.47%   [EVAL] batch:  202 | acc: 50.00%,  total acc: 71.37%   [EVAL] batch:  203 | acc: 50.00%,  total acc: 71.26%   [EVAL] batch:  204 | acc: 50.00%,  total acc: 71.16%   [EVAL] batch:  205 | acc: 81.25%,  total acc: 71.21%   [EVAL] batch:  206 | acc: 68.75%,  total acc: 71.20%   [EVAL] batch:  207 | acc: 93.75%,  total acc: 71.30%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 71.44%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 71.55%   [EVAL] batch:  210 | acc: 81.25%,  total acc: 71.59%   [EVAL] batch:  211 | acc: 87.50%,  total acc: 71.67%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 71.68%   [EVAL] batch:  213 | acc: 43.75%,  total acc: 71.55%   [EVAL] batch:  214 | acc: 37.50%,  total acc: 71.40%   [EVAL] batch:  215 | acc: 81.25%,  total acc: 71.44%   [EVAL] batch:  216 | acc: 43.75%,  total acc: 71.31%   [EVAL] batch:  217 | acc: 50.00%,  total acc: 71.22%   [EVAL] batch:  218 | acc: 50.00%,  total acc: 71.12%   [EVAL] batch:  219 | acc: 37.50%,  total acc: 70.97%   [EVAL] batch:  220 | acc: 25.00%,  total acc: 70.76%   [EVAL] batch:  221 | acc: 25.00%,  total acc: 70.55%   [EVAL] batch:  222 | acc: 12.50%,  total acc: 70.29%   [EVAL] batch:  223 | acc: 12.50%,  total acc: 70.03%   [EVAL] batch:  224 | acc: 25.00%,  total acc: 69.83%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 69.86%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 69.91%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 69.93%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 70.06%   [EVAL] batch:  229 | acc: 81.25%,  total acc: 70.11%   [EVAL] batch:  230 | acc: 87.50%,  total acc: 70.18%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 70.29%   [EVAL] batch:  232 | acc: 62.50%,  total acc: 70.25%   [EVAL] batch:  233 | acc: 81.25%,  total acc: 70.30%   [EVAL] batch:  234 | acc: 50.00%,  total acc: 70.21%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 70.15%   [EVAL] batch:  236 | acc: 68.75%,  total acc: 70.15%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 70.22%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 70.35%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 70.47%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 70.59%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 70.71%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 70.95%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 71.05%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 71.11%   [EVAL] batch:  246 | acc: 87.50%,  total acc: 71.18%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 71.30%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 71.41%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 71.53%   [EVAL] batch:  250 | acc: 75.00%,  total acc: 71.54%   [EVAL] batch:  251 | acc: 68.75%,  total acc: 71.53%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 71.59%   [EVAL] batch:  253 | acc: 68.75%,  total acc: 71.58%   [EVAL] batch:  254 | acc: 68.75%,  total acc: 71.57%   [EVAL] batch:  255 | acc: 81.25%,  total acc: 71.61%   [EVAL] batch:  256 | acc: 81.25%,  total acc: 71.64%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 71.66%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 71.65%   [EVAL] batch:  259 | acc: 81.25%,  total acc: 71.68%   [EVAL] batch:  260 | acc: 81.25%,  total acc: 71.72%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 71.71%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 71.79%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 71.83%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 71.86%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 71.90%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 71.93%   [EVAL] batch:  267 | acc: 87.50%,  total acc: 71.99%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 72.03%   [EVAL] batch:  269 | acc: 56.25%,  total acc: 71.97%   [EVAL] batch:  270 | acc: 50.00%,  total acc: 71.89%   [EVAL] batch:  271 | acc: 50.00%,  total acc: 71.81%   [EVAL] batch:  272 | acc: 56.25%,  total acc: 71.75%   [EVAL] batch:  273 | acc: 68.75%,  total acc: 71.74%   [EVAL] batch:  274 | acc: 75.00%,  total acc: 71.75%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 71.85%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 71.95%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 72.05%   [EVAL] batch:  278 | acc: 93.75%,  total acc: 72.13%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 72.23%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 72.33%   [EVAL] batch:  281 | acc: 100.00%,  total acc: 72.43%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 72.53%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 72.62%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 72.72%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 72.81%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 72.91%   [EVAL] batch:  287 | acc: 100.00%,  total acc: 73.00%   [EVAL] batch:  288 | acc: 87.50%,  total acc: 73.05%   [EVAL] batch:  289 | acc: 75.00%,  total acc: 73.06%   [EVAL] batch:  290 | acc: 68.75%,  total acc: 73.05%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 73.10%   [EVAL] batch:  292 | acc: 93.75%,  total acc: 73.17%   [EVAL] batch:  293 | acc: 68.75%,  total acc: 73.15%   [EVAL] batch:  294 | acc: 31.25%,  total acc: 73.01%   [EVAL] batch:  295 | acc: 18.75%,  total acc: 72.83%   [EVAL] batch:  296 | acc: 25.00%,  total acc: 72.66%   [EVAL] batch:  297 | acc: 25.00%,  total acc: 72.50%   [EVAL] batch:  298 | acc: 37.50%,  total acc: 72.39%   [EVAL] batch:  299 | acc: 31.25%,  total acc: 72.25%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 72.32%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 72.39%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 72.46%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 72.49%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 72.56%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 72.63%   [EVAL] batch:  306 | acc: 31.25%,  total acc: 72.50%   [EVAL] batch:  307 | acc: 31.25%,  total acc: 72.36%   [EVAL] batch:  308 | acc: 43.75%,  total acc: 72.27%   [EVAL] batch:  309 | acc: 18.75%,  total acc: 72.10%   [EVAL] batch:  310 | acc: 25.00%,  total acc: 71.95%   [EVAL] batch:  311 | acc: 6.25%,  total acc: 71.73%   [EVAL] batch:  312 | acc: 25.00%,  total acc: 71.59%   [EVAL] batch:  313 | acc: 31.25%,  total acc: 71.46%   [EVAL] batch:  314 | acc: 12.50%,  total acc: 71.27%   [EVAL] batch:  315 | acc: 12.50%,  total acc: 71.08%   [EVAL] batch:  316 | acc: 31.25%,  total acc: 70.96%   [EVAL] batch:  317 | acc: 25.00%,  total acc: 70.81%   [EVAL] batch:  318 | acc: 25.00%,  total acc: 70.67%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 70.72%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 70.81%   [EVAL] batch:  321 | acc: 100.00%,  total acc: 70.90%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 70.96%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 71.01%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 71.08%   [EVAL] batch:  325 | acc: 56.25%,  total acc: 71.03%   [EVAL] batch:  326 | acc: 31.25%,  total acc: 70.91%   [EVAL] batch:  327 | acc: 50.00%,  total acc: 70.85%   [EVAL] batch:  328 | acc: 68.75%,  total acc: 70.84%   [EVAL] batch:  329 | acc: 62.50%,  total acc: 70.81%   [EVAL] batch:  330 | acc: 43.75%,  total acc: 70.73%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 70.80%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 70.89%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 70.98%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 71.04%   [EVAL] batch:  335 | acc: 93.75%,  total acc: 71.11%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 71.20%   [EVAL] batch:  337 | acc: 100.00%,  total acc: 71.28%   [EVAL] batch:  338 | acc: 100.00%,  total acc: 71.37%   [EVAL] batch:  339 | acc: 93.75%,  total acc: 71.43%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 71.52%   [EVAL] batch:  341 | acc: 100.00%,  total acc: 71.60%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:  343 | acc: 100.00%,  total acc: 71.77%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 71.85%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 71.93%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 71.97%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 72.04%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 72.12%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 72.20%   [EVAL] batch:  350 | acc: 37.50%,  total acc: 72.10%   [EVAL] batch:  351 | acc: 12.50%,  total acc: 71.93%   [EVAL] batch:  352 | acc: 50.00%,  total acc: 71.87%   [EVAL] batch:  353 | acc: 31.25%,  total acc: 71.75%   [EVAL] batch:  354 | acc: 12.50%,  total acc: 71.58%   [EVAL] batch:  355 | acc: 31.25%,  total acc: 71.47%   [EVAL] batch:  356 | acc: 93.75%,  total acc: 71.53%   [EVAL] batch:  357 | acc: 87.50%,  total acc: 71.58%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 71.64%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 71.72%   [EVAL] batch:  360 | acc: 75.00%,  total acc: 71.73%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 71.77%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 71.76%   [EVAL] batch:  363 | acc: 62.50%,  total acc: 71.74%   [EVAL] batch:  364 | acc: 68.75%,  total acc: 71.73%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 71.69%   [EVAL] batch:  366 | acc: 81.25%,  total acc: 71.71%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 71.67%   [EVAL] batch:  368 | acc: 25.00%,  total acc: 71.54%   [EVAL] batch:  369 | acc: 75.00%,  total acc: 71.55%   [EVAL] batch:  370 | acc: 50.00%,  total acc: 71.50%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 71.44%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 71.48%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 71.52%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 71.55%   
cur_acc:  ['0.9494', '0.8859', '0.6766', '0.7212', '0.7798', '0.7044']
his_acc:  ['0.9494', '0.9020', '0.8045', '0.7565', '0.7436', '0.7155']
CurrentTrain: epoch 15, batch     0 | loss: 19.5806227CurrentTrain: epoch 15, batch     1 | loss: 14.9309033CurrentTrain: epoch 15, batch     2 | loss: 10.9436903CurrentTrain: epoch  1, batch     3 | loss: 9.2479245CurrentTrain: epoch 15, batch     0 | loss: 14.3628632CurrentTrain: epoch 15, batch     1 | loss: 12.6395343CurrentTrain: epoch 15, batch     2 | loss: 10.0585018CurrentTrain: epoch  1, batch     3 | loss: 7.4008662CurrentTrain: epoch 15, batch     0 | loss: 8.1972146CurrentTrain: epoch 15, batch     1 | loss: 10.2625108CurrentTrain: epoch 15, batch     2 | loss: 9.9963274CurrentTrain: epoch  1, batch     3 | loss: 9.7153381CurrentTrain: epoch 15, batch     0 | loss: 7.8444871CurrentTrain: epoch 15, batch     1 | loss: 7.8996747CurrentTrain: epoch 15, batch     2 | loss: 7.2305642CurrentTrain: epoch  1, batch     3 | loss: 13.7110372CurrentTrain: epoch 15, batch     0 | loss: 8.4827053CurrentTrain: epoch 15, batch     1 | loss: 6.9404093CurrentTrain: epoch 15, batch     2 | loss: 14.5686108CurrentTrain: epoch  1, batch     3 | loss: 5.9692561CurrentTrain: epoch 15, batch     0 | loss: 12.4428061CurrentTrain: epoch 15, batch     1 | loss: 8.6842020CurrentTrain: epoch 15, batch     2 | loss: 6.3198573CurrentTrain: epoch  1, batch     3 | loss: 7.9092590CurrentTrain: epoch 15, batch     0 | loss: 8.8615149CurrentTrain: epoch 15, batch     1 | loss: 6.0094130CurrentTrain: epoch 15, batch     2 | loss: 6.7363072CurrentTrain: epoch  1, batch     3 | loss: 6.4117459CurrentTrain: epoch 15, batch     0 | loss: 6.5508487CurrentTrain: epoch 15, batch     1 | loss: 6.8539277CurrentTrain: epoch 15, batch     2 | loss: 17.6520920CurrentTrain: epoch  1, batch     3 | loss: 6.2029083CurrentTrain: epoch 15, batch     0 | loss: 9.1053233CurrentTrain: epoch 15, batch     1 | loss: 9.9305998CurrentTrain: epoch 15, batch     2 | loss: 13.1298211CurrentTrain: epoch  1, batch     3 | loss: 5.6610760CurrentTrain: epoch 15, batch     0 | loss: 10.4654426CurrentTrain: epoch 15, batch     1 | loss: 8.1315180CurrentTrain: epoch 15, batch     2 | loss: 5.8370840CurrentTrain: epoch  1, batch     3 | loss: 5.8716403
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: " clinton crazies " is a term in american politics of the 1990s and later that refers to intense criticism of united states president bill clinton and his wife hillary clinton .
Head Entity: bill clinton
Tail Entity: hillary clinton
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: "In a lavish ceremony, actor ben affleck married singer jennifer lopez, marking their reunion after nearly two decades."  
Head Entity: ben affleck  
Tail Entity: jennifer lopez  

Relation: spouse  
Context: "After years of dating, tennis star serena williams and reddit co-founder alexis ohanian tied the knot in a beautiful ceremony."  
Head Entity: serena williams  
Tail Entity: alexis ohanian  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: they briefly reformed in 1992 , when sub pop re - released " primal rock therapy " on cd with five additional unreleased tracks .
Head Entity: primal rock therapy
Tail Entity: sub pop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: The album "Future Nostalgia" was released under the Warner Records label, showcasing Dua Lipa's unique style and sound.  
Head Entity: Future Nostalgia  
Tail Entity: Warner Records  

Relation: record label  
Context: After signing with Columbia Records, the band released their highly anticipated debut album, which quickly climbed the charts.  
Head Entity: debut album  
Tail Entity: Columbia Records  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: distributed by paramount pictures , " the italian job " was theatrically released in the united states on may 30 , 2003 , and grossed over $ 176 million worldwide .
Head Entity: the italian job
Tail Entity: paramount pictures
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: the film was distributed by universal pictures and received critical acclaim upon its release in 2015.  
Head Entity: the film  
Tail Entity: universal pictures  

Relation: distributor  
Context: the album was distributed by sony music and quickly climbed the charts, becoming a bestseller.  
Head Entity: the album  
Tail Entity: sony music  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: one account notes , alston and his son , peter also , practiced their counterfeiting operation , at stack island , in the lower mississippi river , about 170 miles upriver from natchez .
Head Entity: stack island
Tail Entity: mississippi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: the picturesque village of portsmouth is situated along the banks of the serene river thames, providing stunning views and a tranquil atmosphere for its residents.  
Head Entity: portsmouth  
Tail Entity: river thames  

Relation: located in or next to body of water  
Context: during our trip, we discovered that the charming town of venice is built on a series of islands surrounded by the beautiful adriatic sea, making it a unique destination for tourists.  
Head Entity: venice  
Tail Entity: adriatic sea  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: following the sale of amstrad plc to rupert murdoch 's bskyb , viglen is now lord sugar 's sole it establishment .
Head Entity: bskyb
Tail Entity: amstrad
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Relation: subsidiary  
Context: In 2018, Facebook acquired WhatsApp, making it a subsidiary of the social media giant.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Relation: subsidiary  
Context: After the merger, the new company announced that its subsidiary, XYZ Corp, would continue to operate independently.  
Head Entity: new company  
Tail Entity: XYZ Corp  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: he challenged seven other swimmers in heat four including ukraine 's rostyslav svanidze , a top 16 finalist in atlanta four years earlier .
Head Entity: rostyslav svanidze
Tail Entity: ukraine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: after moving to the united states, she became a naturalized citizen and proudly represents her home country of brazil in international competitions.  
Head Entity: she  
Tail Entity: brazil  

Relation: country of citizenship  
Context: during the conference, the renowned scientist from germany discussed his groundbreaking research while highlighting the importance of his citizenship in fostering innovation.  
Head Entity: the renowned scientist  
Tail Entity: germany  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: buck moved to austin in the mid-1970s and joined the blues rock group , the fabulous thunderbirds , along with keith ferguson , jimmie vaughan , and kim wilson .
Head Entity: the fabulous thunderbirds
Tail Entity: blues rock
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the album "thriller" by michael jackson is widely regarded as a landmark in pop music history, blending elements of rock, funk, and soul.  
Head Entity: michael jackson  
Tail Entity: pop  

Relation: genre  
Context: the film "inception," directed by christopher nolan, is a complex narrative that combines science fiction with psychological thriller elements.  
Head Entity: inception  
Tail Entity: science fiction  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: the widening of the danube at the mouths of karaš and nera was known as a ram lake .
Head Entity: nera
Tail Entity: danube
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: The Mississippi River flows into the Gulf of Mexico, marking the mouth of the watercourse.  
Head Entity: Mississippi River  
Tail Entity: Gulf of Mexico  

Relation: mouth of the watercourse  
Context: The Amazon River's mouth is located near the city of Belém, where it meets the Atlantic Ocean.  
Head Entity: Amazon River  
Tail Entity: Atlantic Ocean  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: bradley halliday ( born 10 july 1995 ) is an english professional footballer who plays as a defender or a midfielder for league two club cambridge united .
Head Entity: bradley halliday
Tail Entity: defender
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: alex morgan ( born 2 july 1989 ) is an american soccer player who plays as a forward for the national team and the club orlando pride in the national women's soccer league.  
Head Entity: alex morgan  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: lebron james ( born 30 december 1984 ) is an american professional basketball player who plays as a small forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: small forward  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: hammami made his international debut for tunisia in 2007 , and represented them at the africa cup of nations in 2010 and 2013 .
Head Entity: 2010
Tail Entity: africa cup of nations
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2021 UEFA European Championship was held in various cities across Europe, showcasing top national teams competing for the title.  
Head Entity: 2021  
Tail Entity: UEFA European Championship  

Relation: sports season of league or competition  
Context: The 2022 FIFA World Cup took place in Qatar, marking the first time the tournament was held in the Middle East.  
Head Entity: 2022  
Tail Entity: FIFA World Cup  
MemoryTrain:  epoch 15, batch     0 | loss: 3.3333129MemoryTrain:  epoch 15, batch     1 | loss: 2.6382482MemoryTrain:  epoch 15, batch     2 | loss: 3.5610199MemoryTrain:  epoch 15, batch     3 | loss: 2.3687910MemoryTrain:  epoch 15, batch     4 | loss: 2.9395612MemoryTrain:  epoch 15, batch     5 | loss: 2.2380812MemoryTrain:  epoch 15, batch     6 | loss: 4.5788243MemoryTrain:  epoch 15, batch     7 | loss: 2.8262105MemoryTrain:  epoch 15, batch     8 | loss: 3.0390066MemoryTrain:  epoch 15, batch     9 | loss: 6.3221025MemoryTrain:  epoch 15, batch    10 | loss: 2.9226100MemoryTrain:  epoch 15, batch    11 | loss: 3.5549416MemoryTrain:  epoch 15, batch    12 | loss: 4.1972592MemoryTrain:  epoch  1, batch    13 | loss: 6.1905171MemoryTrain:  epoch 15, batch     0 | loss: 2.5055267MemoryTrain:  epoch 15, batch     1 | loss: 4.8533994MemoryTrain:  epoch 15, batch     2 | loss: 4.7381234MemoryTrain:  epoch 15, batch     3 | loss: 2.0225322MemoryTrain:  epoch 15, batch     4 | loss: 2.7030151MemoryTrain:  epoch 15, batch     5 | loss: 2.1901193MemoryTrain:  epoch 15, batch     6 | loss: 2.2319653MemoryTrain:  epoch 15, batch     7 | loss: 1.9743743MemoryTrain:  epoch 15, batch     8 | loss: 2.4232849MemoryTrain:  epoch 15, batch     9 | loss: 2.5144781MemoryTrain:  epoch 15, batch    10 | loss: 2.2947595MemoryTrain:  epoch 15, batch    11 | loss: 4.2138039MemoryTrain:  epoch 15, batch    12 | loss: 2.6139227MemoryTrain:  epoch  1, batch    13 | loss: 5.5876430MemoryTrain:  epoch 15, batch     0 | loss: 1.9449898MemoryTrain:  epoch 15, batch     1 | loss: 2.0959753MemoryTrain:  epoch 15, batch     2 | loss: 3.3787144MemoryTrain:  epoch 15, batch     3 | loss: 2.0147524MemoryTrain:  epoch 15, batch     4 | loss: 1.9045775MemoryTrain:  epoch 15, batch     5 | loss: 2.2462727MemoryTrain:  epoch 15, batch     6 | loss: 2.3583666MemoryTrain:  epoch 15, batch     7 | loss: 2.6740439MemoryTrain:  epoch 15, batch     8 | loss: 2.7674696MemoryTrain:  epoch 15, batch     9 | loss: 2.7991726MemoryTrain:  epoch 15, batch    10 | loss: 3.8413408MemoryTrain:  epoch 15, batch    11 | loss: 3.0705873MemoryTrain:  epoch 15, batch    12 | loss: 1.6390314MemoryTrain:  epoch  1, batch    13 | loss: 9.6032698MemoryTrain:  epoch 15, batch     0 | loss: 3.6356433MemoryTrain:  epoch 15, batch     1 | loss: 1.8350516MemoryTrain:  epoch 15, batch     2 | loss: 1.8151288MemoryTrain:  epoch 15, batch     3 | loss: 1.9643928MemoryTrain:  epoch 15, batch     4 | loss: 2.0187974MemoryTrain:  epoch 15, batch     5 | loss: 4.2989832MemoryTrain:  epoch 15, batch     6 | loss: 2.0575578MemoryTrain:  epoch 15, batch     7 | loss: 4.3413949MemoryTrain:  epoch 15, batch     8 | loss: 2.1172507MemoryTrain:  epoch 15, batch     9 | loss: 1.6607842MemoryTrain:  epoch 15, batch    10 | loss: 4.2822139MemoryTrain:  epoch 15, batch    11 | loss: 2.2075970MemoryTrain:  epoch 15, batch    12 | loss: 2.0793494MemoryTrain:  epoch  1, batch    13 | loss: 5.7600938MemoryTrain:  epoch 15, batch     0 | loss: 1.6804655MemoryTrain:  epoch 15, batch     1 | loss: 1.4603727MemoryTrain:  epoch 15, batch     2 | loss: 2.0543248MemoryTrain:  epoch 15, batch     3 | loss: 2.2043362MemoryTrain:  epoch 15, batch     4 | loss: 3.8972503MemoryTrain:  epoch 15, batch     5 | loss: 2.2463485MemoryTrain:  epoch 15, batch     6 | loss: 2.5699203MemoryTrain:  epoch 15, batch     7 | loss: 3.8548540MemoryTrain:  epoch 15, batch     8 | loss: 1.8539800MemoryTrain:  epoch 15, batch     9 | loss: 2.4887909MemoryTrain:  epoch 15, batch    10 | loss: 3.9683528MemoryTrain:  epoch 15, batch    11 | loss: 1.8759733MemoryTrain:  epoch 15, batch    12 | loss: 1.7386694MemoryTrain:  epoch  1, batch    13 | loss: 5.4865472MemoryTrain:  epoch 15, batch     0 | loss: 1.5513129MemoryTrain:  epoch 15, batch     1 | loss: 2.0942191MemoryTrain:  epoch 15, batch     2 | loss: 1.7862975MemoryTrain:  epoch 15, batch     3 | loss: 1.7923099MemoryTrain:  epoch 15, batch     4 | loss: 1.9293482MemoryTrain:  epoch 15, batch     5 | loss: 2.0437891MemoryTrain:  epoch 15, batch     6 | loss: 1.4214380MemoryTrain:  epoch 15, batch     7 | loss: 4.0903869MemoryTrain:  epoch 15, batch     8 | loss: 2.0497913MemoryTrain:  epoch 15, batch     9 | loss: 1.6958764MemoryTrain:  epoch 15, batch    10 | loss: 1.8272982MemoryTrain:  epoch 15, batch    11 | loss: 1.7185730MemoryTrain:  epoch 15, batch    12 | loss: 1.4969073MemoryTrain:  epoch  1, batch    13 | loss: 5.4448000MemoryTrain:  epoch 15, batch     0 | loss: 1.7005963MemoryTrain:  epoch 15, batch     1 | loss: 4.2450212MemoryTrain:  epoch 15, batch     2 | loss: 1.5866679MemoryTrain:  epoch 15, batch     3 | loss: 1.5614056MemoryTrain:  epoch 15, batch     4 | loss: 1.6229160MemoryTrain:  epoch 15, batch     5 | loss: 1.3531030MemoryTrain:  epoch 15, batch     6 | loss: 3.6186514MemoryTrain:  epoch 15, batch     7 | loss: 1.3809139MemoryTrain:  epoch 15, batch     8 | loss: 1.7712617MemoryTrain:  epoch 15, batch     9 | loss: 1.5233363MemoryTrain:  epoch 15, batch    10 | loss: 2.0782718MemoryTrain:  epoch 15, batch    11 | loss: 1.7059137MemoryTrain:  epoch 15, batch    12 | loss: 3.2013614MemoryTrain:  epoch  1, batch    13 | loss: 5.5997947MemoryTrain:  epoch 15, batch     0 | loss: 1.9942042MemoryTrain:  epoch 15, batch     1 | loss: 1.8587413MemoryTrain:  epoch 15, batch     2 | loss: 1.9737806MemoryTrain:  epoch 15, batch     3 | loss: 1.3005960MemoryTrain:  epoch 15, batch     4 | loss: 3.3137068MemoryTrain:  epoch 15, batch     5 | loss: 1.6498314MemoryTrain:  epoch 15, batch     6 | loss: 4.0448922MemoryTrain:  epoch 15, batch     7 | loss: 2.5528606MemoryTrain:  epoch 15, batch     8 | loss: 1.5875957MemoryTrain:  epoch 15, batch     9 | loss: 1.5734644MemoryTrain:  epoch 15, batch    10 | loss: 1.6193422MemoryTrain:  epoch 15, batch    11 | loss: 1.8872469MemoryTrain:  epoch 15, batch    12 | loss: 1.5197636MemoryTrain:  epoch  1, batch    13 | loss: 5.8464788MemoryTrain:  epoch 15, batch     0 | loss: 2.7347517MemoryTrain:  epoch 15, batch     1 | loss: 1.5630756MemoryTrain:  epoch 15, batch     2 | loss: 1.6812611MemoryTrain:  epoch 15, batch     3 | loss: 1.6981303MemoryTrain:  epoch 15, batch     4 | loss: 1.8222873MemoryTrain:  epoch 15, batch     5 | loss: 2.5395724MemoryTrain:  epoch 15, batch     6 | loss: 1.4435916MemoryTrain:  epoch 15, batch     7 | loss: 1.6842165MemoryTrain:  epoch 15, batch     8 | loss: 3.5079434MemoryTrain:  epoch 15, batch     9 | loss: 1.7298428MemoryTrain:  epoch 15, batch    10 | loss: 2.2077938MemoryTrain:  epoch 15, batch    11 | loss: 1.3856225MemoryTrain:  epoch 15, batch    12 | loss: 1.5708813MemoryTrain:  epoch  1, batch    13 | loss: 4.5902980MemoryTrain:  epoch 15, batch     0 | loss: 1.6835027MemoryTrain:  epoch 15, batch     1 | loss: 3.9253746MemoryTrain:  epoch 15, batch     2 | loss: 1.5706100MemoryTrain:  epoch 15, batch     3 | loss: 2.3598786MemoryTrain:  epoch 15, batch     4 | loss: 1.5392482MemoryTrain:  epoch 15, batch     5 | loss: 1.5957534MemoryTrain:  epoch 15, batch     6 | loss: 3.8232683MemoryTrain:  epoch 15, batch     7 | loss: 1.3474368MemoryTrain:  epoch 15, batch     8 | loss: 1.8161260MemoryTrain:  epoch 15, batch     9 | loss: 1.7708889MemoryTrain:  epoch 15, batch    10 | loss: 3.9485339MemoryTrain:  epoch 15, batch    11 | loss: 1.6373527MemoryTrain:  epoch 15, batch    12 | loss: 1.6677944MemoryTrain:  epoch  1, batch    13 | loss: 6.4353901
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 29.69%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 33.75%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 34.38%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 41.96%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 49.22%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 53.47%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 56.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 59.09%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 63.94%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 62.95%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 63.33%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 65.23%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 66.18%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 66.32%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 66.78%   [EVAL] batch:   19 | acc: 37.50%,  total acc: 65.31%   [EVAL] batch:   20 | acc: 56.25%,  total acc: 64.88%   [EVAL] batch:   21 | acc: 18.75%,  total acc: 62.78%   [EVAL] batch:   22 | acc: 31.25%,  total acc: 61.41%   [EVAL] batch:   23 | acc: 56.25%,  total acc: 61.20%   [EVAL] batch:   24 | acc: 50.00%,  total acc: 60.75%   [EVAL] batch:   25 | acc: 25.00%,  total acc: 59.38%   [EVAL] batch:   26 | acc: 43.75%,  total acc: 58.80%   [EVAL] batch:   27 | acc: 56.25%,  total acc: 58.71%   [EVAL] batch:   28 | acc: 50.00%,  total acc: 58.41%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 58.13%   [EVAL] batch:   30 | acc: 50.00%,  total acc: 57.86%   [EVAL] batch:   31 | acc: 50.00%,  total acc: 57.62%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 57.20%   [EVAL] batch:   33 | acc: 37.50%,  total acc: 56.62%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 56.43%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 55.90%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 55.41%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 55.10%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 55.61%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 56.56%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 57.16%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 57.74%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 58.43%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 58.81%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 58.33%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 57.74%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 56.91%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 56.90%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 56.38%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 56.12%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 56.99%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 57.81%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 58.61%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 59.38%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 60.11%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 60.83%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 61.07%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 60.88%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 61.02%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 60.94%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 61.07%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 61.29%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 60.81%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 68.06%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 66.88%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 63.64%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 61.98%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 64.73%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 66.25%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 67.97%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 69.49%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 72.04%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 71.43%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 71.02%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 70.38%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 70.57%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 71.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.12%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 73.15%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 74.78%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 75.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 76.41%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 76.95%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 77.65%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 77.76%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 77.68%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 77.78%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 78.04%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 78.29%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 78.85%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 79.88%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 80.52%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 80.40%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 80.28%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 79.89%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 79.39%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 79.04%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 79.21%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 78.75%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 78.80%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 78.85%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 78.89%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 78.82%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 78.41%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 78.57%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 78.07%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 77.37%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 76.69%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 76.67%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 76.74%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 76.71%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 76.79%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 77.05%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 77.40%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 77.75%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 77.80%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 78.35%   [EVAL] batch:   69 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 77.90%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 77.78%   [EVAL] batch:   72 | acc: 62.50%,  total acc: 77.57%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 77.53%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 77.17%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 76.89%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 76.87%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 76.52%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 76.11%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 75.70%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 75.39%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 74.92%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 74.85%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 74.56%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 74.35%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 74.07%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 74.15%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 74.44%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 74.72%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 75.27%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 75.54%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 75.73%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 75.53%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 75.59%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 75.45%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 75.26%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 75.32%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 75.25%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 75.49%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 75.73%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 75.96%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 76.19%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 76.30%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 76.52%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 76.74%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 76.95%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 77.16%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 77.36%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 77.57%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 77.65%   [EVAL] batch:  113 | acc: 75.00%,  total acc: 77.63%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 77.77%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 77.91%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 77.88%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 77.91%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 77.89%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 78.02%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 78.10%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 78.18%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 78.35%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 78.48%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 78.60%   [EVAL] batch:  125 | acc: 12.50%,  total acc: 78.08%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 77.66%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 77.29%   [EVAL] batch:  128 | acc: 18.75%,  total acc: 76.84%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 76.63%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 76.38%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 76.28%   [EVAL] batch:  132 | acc: 75.00%,  total acc: 76.27%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 76.35%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 76.44%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 76.52%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 76.60%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 76.63%   [EVAL] batch:  138 | acc: 68.75%,  total acc: 76.57%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 76.52%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 76.64%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 76.63%   [EVAL] batch:  142 | acc: 93.75%,  total acc: 76.75%   [EVAL] batch:  143 | acc: 43.75%,  total acc: 76.52%   [EVAL] batch:  144 | acc: 25.00%,  total acc: 76.16%   [EVAL] batch:  145 | acc: 0.00%,  total acc: 75.64%   [EVAL] batch:  146 | acc: 12.50%,  total acc: 75.21%   [EVAL] batch:  147 | acc: 37.50%,  total acc: 74.96%   [EVAL] batch:  148 | acc: 12.50%,  total acc: 74.54%   [EVAL] batch:  149 | acc: 12.50%,  total acc: 74.12%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 73.68%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 73.27%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 72.83%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 72.40%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 72.06%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 71.59%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 71.58%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 71.76%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 71.82%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 71.99%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 72.13%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 72.26%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 72.35%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 72.45%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 72.61%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 72.74%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 72.83%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 72.99%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 73.08%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 72.79%   [EVAL] batch:  170 | acc: 12.50%,  total acc: 72.44%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 72.09%   [EVAL] batch:  172 | acc: 6.25%,  total acc: 71.71%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 71.44%   [EVAL] batch:  174 | acc: 18.75%,  total acc: 71.14%   [EVAL] batch:  175 | acc: 31.25%,  total acc: 70.92%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 70.83%   [EVAL] batch:  177 | acc: 25.00%,  total acc: 70.58%   [EVAL] batch:  178 | acc: 43.75%,  total acc: 70.43%   [EVAL] batch:  179 | acc: 50.00%,  total acc: 70.31%   [EVAL] batch:  180 | acc: 43.75%,  total acc: 70.17%   [EVAL] batch:  181 | acc: 43.75%,  total acc: 70.02%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 69.95%   [EVAL] batch:  183 | acc: 43.75%,  total acc: 69.80%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 69.83%   [EVAL] batch:  185 | acc: 43.75%,  total acc: 69.69%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 69.72%   [EVAL] batch:  187 | acc: 25.00%,  total acc: 69.48%   [EVAL] batch:  188 | acc: 12.50%,  total acc: 69.18%   [EVAL] batch:  189 | acc: 18.75%,  total acc: 68.91%   [EVAL] batch:  190 | acc: 12.50%,  total acc: 68.62%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 68.39%   [EVAL] batch:  192 | acc: 31.25%,  total acc: 68.20%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 67.94%   [EVAL] batch:  194 | acc: 62.50%,  total acc: 67.92%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 67.92%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 67.93%   [EVAL] batch:  197 | acc: 62.50%,  total acc: 67.90%   [EVAL] batch:  198 | acc: 25.00%,  total acc: 67.68%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 67.84%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 67.85%   [EVAL] batch:  201 | acc: 50.00%,  total acc: 67.76%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 67.76%   [EVAL] batch:  203 | acc: 50.00%,  total acc: 67.68%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 67.68%   [EVAL] batch:  205 | acc: 81.25%,  total acc: 67.75%   [EVAL] batch:  206 | acc: 81.25%,  total acc: 67.81%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 68.12%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 68.24%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 68.36%   [EVAL] batch:  211 | acc: 93.75%,  total acc: 68.48%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 68.49%   [EVAL] batch:  213 | acc: 43.75%,  total acc: 68.37%   [EVAL] batch:  214 | acc: 37.50%,  total acc: 68.23%   [EVAL] batch:  215 | acc: 75.00%,  total acc: 68.26%   [EVAL] batch:  216 | acc: 37.50%,  total acc: 68.12%   [EVAL] batch:  217 | acc: 50.00%,  total acc: 68.03%   [EVAL] batch:  218 | acc: 50.00%,  total acc: 67.95%   [EVAL] batch:  219 | acc: 43.75%,  total acc: 67.84%   [EVAL] batch:  220 | acc: 25.00%,  total acc: 67.65%   [EVAL] batch:  221 | acc: 25.00%,  total acc: 67.45%   [EVAL] batch:  222 | acc: 12.50%,  total acc: 67.21%   [EVAL] batch:  223 | acc: 6.25%,  total acc: 66.94%   [EVAL] batch:  224 | acc: 18.75%,  total acc: 66.72%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 66.76%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 66.82%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 66.86%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 67.00%   [EVAL] batch:  229 | acc: 81.25%,  total acc: 67.07%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 67.18%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 67.30%   [EVAL] batch:  232 | acc: 56.25%,  total acc: 67.25%   [EVAL] batch:  233 | acc: 81.25%,  total acc: 67.31%   [EVAL] batch:  234 | acc: 62.50%,  total acc: 67.29%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 67.24%   [EVAL] batch:  236 | acc: 75.00%,  total acc: 67.27%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 67.36%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 67.49%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 67.63%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 67.76%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 67.90%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 68.03%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 68.16%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 68.27%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 68.34%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 68.45%   [EVAL] batch:  247 | acc: 93.75%,  total acc: 68.55%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 68.67%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 68.80%   [EVAL] batch:  250 | acc: 68.75%,  total acc: 68.80%   [EVAL] batch:  251 | acc: 62.50%,  total acc: 68.77%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 68.80%   [EVAL] batch:  253 | acc: 62.50%,  total acc: 68.77%   [EVAL] batch:  254 | acc: 68.75%,  total acc: 68.77%   [EVAL] batch:  255 | acc: 81.25%,  total acc: 68.82%   [EVAL] batch:  256 | acc: 75.00%,  total acc: 68.85%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 68.90%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 68.89%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 68.92%   [EVAL] batch:  260 | acc: 81.25%,  total acc: 68.97%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 68.99%   [EVAL] batch:  262 | acc: 100.00%,  total acc: 69.11%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 69.13%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 69.15%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 69.17%   [EVAL] batch:  266 | acc: 87.50%,  total acc: 69.24%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 69.33%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 69.33%   [EVAL] batch:  269 | acc: 56.25%,  total acc: 69.28%   [EVAL] batch:  270 | acc: 50.00%,  total acc: 69.21%   [EVAL] batch:  271 | acc: 56.25%,  total acc: 69.16%   [EVAL] batch:  272 | acc: 56.25%,  total acc: 69.12%   [EVAL] batch:  273 | acc: 62.50%,  total acc: 69.09%   [EVAL] batch:  274 | acc: 75.00%,  total acc: 69.11%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 69.23%   [EVAL] batch:  276 | acc: 93.75%,  total acc: 69.31%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 69.42%   [EVAL] batch:  278 | acc: 81.25%,  total acc: 69.47%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 69.58%   [EVAL] batch:  280 | acc: 93.75%,  total acc: 69.66%   [EVAL] batch:  281 | acc: 100.00%,  total acc: 69.77%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 69.88%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 69.98%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 70.09%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 70.19%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 70.30%   [EVAL] batch:  287 | acc: 100.00%,  total acc: 70.40%   [EVAL] batch:  288 | acc: 93.75%,  total acc: 70.48%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 70.54%   [EVAL] batch:  290 | acc: 81.25%,  total acc: 70.58%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 70.65%   [EVAL] batch:  292 | acc: 93.75%,  total acc: 70.73%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 70.77%   [EVAL] batch:  294 | acc: 25.00%,  total acc: 70.61%   [EVAL] batch:  295 | acc: 12.50%,  total acc: 70.42%   [EVAL] batch:  296 | acc: 18.75%,  total acc: 70.24%   [EVAL] batch:  297 | acc: 43.75%,  total acc: 70.16%   [EVAL] batch:  298 | acc: 43.75%,  total acc: 70.07%   [EVAL] batch:  299 | acc: 37.50%,  total acc: 69.96%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 70.04%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 70.12%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 70.19%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 70.23%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 70.31%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 70.36%   [EVAL] batch:  306 | acc: 31.25%,  total acc: 70.24%   [EVAL] batch:  307 | acc: 18.75%,  total acc: 70.07%   [EVAL] batch:  308 | acc: 25.00%,  total acc: 69.92%   [EVAL] batch:  309 | acc: 6.25%,  total acc: 69.72%   [EVAL] batch:  310 | acc: 12.50%,  total acc: 69.53%   [EVAL] batch:  311 | acc: 6.25%,  total acc: 69.33%   [EVAL] batch:  312 | acc: 12.50%,  total acc: 69.15%   [EVAL] batch:  313 | acc: 31.25%,  total acc: 69.03%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 68.87%   [EVAL] batch:  315 | acc: 6.25%,  total acc: 68.67%   [EVAL] batch:  316 | acc: 6.25%,  total acc: 68.47%   [EVAL] batch:  317 | acc: 31.25%,  total acc: 68.36%   [EVAL] batch:  318 | acc: 25.00%,  total acc: 68.22%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 68.26%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 68.34%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 68.40%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 68.46%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 68.50%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 68.54%   [EVAL] batch:  325 | acc: 56.25%,  total acc: 68.50%   [EVAL] batch:  326 | acc: 18.75%,  total acc: 68.35%   [EVAL] batch:  327 | acc: 31.25%,  total acc: 68.24%   [EVAL] batch:  328 | acc: 68.75%,  total acc: 68.24%   [EVAL] batch:  329 | acc: 68.75%,  total acc: 68.24%   [EVAL] batch:  330 | acc: 43.75%,  total acc: 68.16%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 68.24%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 68.34%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 68.43%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 68.51%   [EVAL] batch:  335 | acc: 93.75%,  total acc: 68.58%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 68.68%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:  338 | acc: 100.00%,  total acc: 68.84%   [EVAL] batch:  339 | acc: 100.00%,  total acc: 68.93%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 69.02%   [EVAL] batch:  341 | acc: 100.00%,  total acc: 69.12%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 69.21%   [EVAL] batch:  343 | acc: 100.00%,  total acc: 69.30%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 69.38%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 69.47%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 69.52%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 69.59%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 69.66%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:  350 | acc: 50.00%,  total acc: 69.69%   [EVAL] batch:  351 | acc: 25.00%,  total acc: 69.57%   [EVAL] batch:  352 | acc: 68.75%,  total acc: 69.56%   [EVAL] batch:  353 | acc: 31.25%,  total acc: 69.46%   [EVAL] batch:  354 | acc: 31.25%,  total acc: 69.35%   [EVAL] batch:  355 | acc: 43.75%,  total acc: 69.28%   [EVAL] batch:  356 | acc: 93.75%,  total acc: 69.35%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 69.41%   [EVAL] batch:  358 | acc: 87.50%,  total acc: 69.46%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 69.55%   [EVAL] batch:  360 | acc: 87.50%,  total acc: 69.60%   [EVAL] batch:  361 | acc: 81.25%,  total acc: 69.63%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 69.65%   [EVAL] batch:  363 | acc: 62.50%,  total acc: 69.63%   [EVAL] batch:  364 | acc: 62.50%,  total acc: 69.61%   [EVAL] batch:  365 | acc: 62.50%,  total acc: 69.59%   [EVAL] batch:  366 | acc: 75.00%,  total acc: 69.60%   [EVAL] batch:  367 | acc: 50.00%,  total acc: 69.55%   [EVAL] batch:  368 | acc: 25.00%,  total acc: 69.43%   [EVAL] batch:  369 | acc: 68.75%,  total acc: 69.43%   [EVAL] batch:  370 | acc: 56.25%,  total acc: 69.39%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 69.32%   [EVAL] batch:  372 | acc: 75.00%,  total acc: 69.34%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 69.37%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 69.38%   [EVAL] batch:  375 | acc: 31.25%,  total acc: 69.28%   [EVAL] batch:  376 | acc: 25.00%,  total acc: 69.16%   [EVAL] batch:  377 | acc: 37.50%,  total acc: 69.08%   [EVAL] batch:  378 | acc: 25.00%,  total acc: 68.96%   [EVAL] batch:  379 | acc: 50.00%,  total acc: 68.91%   [EVAL] batch:  380 | acc: 37.50%,  total acc: 68.83%   [EVAL] batch:  381 | acc: 87.50%,  total acc: 68.88%   [EVAL] batch:  382 | acc: 100.00%,  total acc: 68.96%   [EVAL] batch:  383 | acc: 87.50%,  total acc: 69.01%   [EVAL] batch:  384 | acc: 81.25%,  total acc: 69.04%   [EVAL] batch:  385 | acc: 87.50%,  total acc: 69.09%   [EVAL] batch:  386 | acc: 100.00%,  total acc: 69.17%   [EVAL] batch:  387 | acc: 81.25%,  total acc: 69.20%   [EVAL] batch:  388 | acc: 50.00%,  total acc: 69.15%   [EVAL] batch:  389 | acc: 68.75%,  total acc: 69.15%   [EVAL] batch:  390 | acc: 93.75%,  total acc: 69.21%   [EVAL] batch:  391 | acc: 81.25%,  total acc: 69.24%   [EVAL] batch:  392 | acc: 68.75%,  total acc: 69.24%   [EVAL] batch:  393 | acc: 75.00%,  total acc: 69.26%   [EVAL] batch:  394 | acc: 37.50%,  total acc: 69.18%   [EVAL] batch:  395 | acc: 56.25%,  total acc: 69.14%   [EVAL] batch:  396 | acc: 18.75%,  total acc: 69.02%   [EVAL] batch:  397 | acc: 31.25%,  total acc: 68.92%   [EVAL] batch:  398 | acc: 56.25%,  total acc: 68.89%   [EVAL] batch:  399 | acc: 50.00%,  total acc: 68.84%   [EVAL] batch:  400 | acc: 25.00%,  total acc: 68.73%   [EVAL] batch:  401 | acc: 43.75%,  total acc: 68.67%   [EVAL] batch:  402 | acc: 56.25%,  total acc: 68.64%   [EVAL] batch:  403 | acc: 50.00%,  total acc: 68.60%   [EVAL] batch:  404 | acc: 50.00%,  total acc: 68.55%   [EVAL] batch:  405 | acc: 50.00%,  total acc: 68.50%   [EVAL] batch:  406 | acc: 50.00%,  total acc: 68.46%   [EVAL] batch:  407 | acc: 43.75%,  total acc: 68.40%   [EVAL] batch:  408 | acc: 37.50%,  total acc: 68.32%   [EVAL] batch:  409 | acc: 50.00%,  total acc: 68.28%   [EVAL] batch:  410 | acc: 37.50%,  total acc: 68.20%   [EVAL] batch:  411 | acc: 37.50%,  total acc: 68.13%   [EVAL] batch:  412 | acc: 43.75%,  total acc: 68.07%   [EVAL] batch:  413 | acc: 75.00%,  total acc: 68.09%   [EVAL] batch:  414 | acc: 93.75%,  total acc: 68.15%   [EVAL] batch:  415 | acc: 81.25%,  total acc: 68.18%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 68.21%   [EVAL] batch:  417 | acc: 87.50%,  total acc: 68.26%   [EVAL] batch:  418 | acc: 75.00%,  total acc: 68.27%   [EVAL] batch:  419 | acc: 37.50%,  total acc: 68.20%   [EVAL] batch:  420 | acc: 31.25%,  total acc: 68.11%   [EVAL] batch:  421 | acc: 18.75%,  total acc: 67.99%   [EVAL] batch:  422 | acc: 56.25%,  total acc: 67.97%   [EVAL] batch:  423 | acc: 31.25%,  total acc: 67.88%   [EVAL] batch:  424 | acc: 43.75%,  total acc: 67.82%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 67.90%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 68.05%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 68.12%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 68.20%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 68.27%   [EVAL] batch:  431 | acc: 75.00%,  total acc: 68.29%   [EVAL] batch:  432 | acc: 50.00%,  total acc: 68.24%   [EVAL] batch:  433 | acc: 68.75%,  total acc: 68.25%   [EVAL] batch:  434 | acc: 56.25%,  total acc: 68.22%   [EVAL] batch:  435 | acc: 68.75%,  total acc: 68.22%   [EVAL] batch:  436 | acc: 75.00%,  total acc: 68.24%   [EVAL] batch:  437 | acc: 31.25%,  total acc: 68.15%   
cur_acc:  ['0.9494', '0.8859', '0.6766', '0.7212', '0.7798', '0.7044', '0.6081']
his_acc:  ['0.9494', '0.9020', '0.8045', '0.7565', '0.7436', '0.7155', '0.6815']
CurrentTrain: epoch 15, batch     0 | loss: 15.9129695CurrentTrain: epoch 15, batch     1 | loss: 13.1040063CurrentTrain: epoch 15, batch     2 | loss: 14.0970495CurrentTrain: epoch  1, batch     3 | loss: 8.2970814CurrentTrain: epoch 15, batch     0 | loss: 15.9723105CurrentTrain: epoch 15, batch     1 | loss: 13.0490723CurrentTrain: epoch 15, batch     2 | loss: 18.7820842CurrentTrain: epoch  1, batch     3 | loss: 8.6142055CurrentTrain: epoch 15, batch     0 | loss: 9.4288761CurrentTrain: epoch 15, batch     1 | loss: 8.8710763CurrentTrain: epoch 15, batch     2 | loss: 8.8210378CurrentTrain: epoch  1, batch     3 | loss: 9.4966976CurrentTrain: epoch 15, batch     0 | loss: 21.1774533CurrentTrain: epoch 15, batch     1 | loss: 7.1545624CurrentTrain: epoch 15, batch     2 | loss: 14.9696202CurrentTrain: epoch  1, batch     3 | loss: 7.5961344CurrentTrain: epoch 15, batch     0 | loss: 9.8825194CurrentTrain: epoch 15, batch     1 | loss: 27.9249805CurrentTrain: epoch 15, batch     2 | loss: 12.2877745CurrentTrain: epoch  1, batch     3 | loss: 6.1164653CurrentTrain: epoch 15, batch     0 | loss: 7.8118007CurrentTrain: epoch 15, batch     1 | loss: 8.4813569CurrentTrain: epoch 15, batch     2 | loss: 11.8397749CurrentTrain: epoch  1, batch     3 | loss: 8.9106630CurrentTrain: epoch 15, batch     0 | loss: 14.0945058CurrentTrain: epoch 15, batch     1 | loss: 9.8213414CurrentTrain: epoch 15, batch     2 | loss: 11.0202784CurrentTrain: epoch  1, batch     3 | loss: 5.7239568CurrentTrain: epoch 15, batch     0 | loss: 9.7900983CurrentTrain: epoch 15, batch     1 | loss: 12.7378188CurrentTrain: epoch 15, batch     2 | loss: 8.5563424CurrentTrain: epoch  1, batch     3 | loss: 5.5124000CurrentTrain: epoch 15, batch     0 | loss: 11.9601542CurrentTrain: epoch 15, batch     1 | loss: 7.7867528CurrentTrain: epoch 15, batch     2 | loss: 5.9369496CurrentTrain: epoch  1, batch     3 | loss: 6.3847412CurrentTrain: epoch 15, batch     0 | loss: 8.7228201CurrentTrain: epoch 15, batch     1 | loss: 11.7616072CurrentTrain: epoch 15, batch     2 | loss: 9.5877812CurrentTrain: epoch  1, batch     3 | loss: 8.2180438
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: bryson 's best known work is his 1985 book " evil angels " which chronicles the story of lindy chamberlain 's trial for murder , following the death of her baby daughter , azaria .
Head Entity: evil angels
Tail Entity: death
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: in her groundbreaking 2010 book "the immortal life of henrietta lacks," rebecca skloot explores the life of henrietta lacks and the impact of her cells on medical research.  
Head Entity: the immortal life of henrietta lacks  
Tail Entity: henrietta lacks  

Relation: main subject  
Context: "sapiens: a brief history of humankind" by yuval noah harari examines the history and evolution of our species from the emergence of Homo sapiens to the present day.  
Head Entity: sapiens: a brief history of humankind  
Tail Entity: Homo sapiens  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: group b of uefa euro 2016 contained england , russia , wales and slovakia .
Head Entity: uefa euro 2016
Tail Entity: slovakia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: the 2020 summer olympics featured countries like the united states, china, and japan competing in various sports.  
Head Entity: 2020 summer olympics  
Tail Entity: japan  

Relation: participating team  
Context: the fifa world cup 2018 saw teams such as france, croatia, and belgium battling for the championship title.  
Head Entity: fifa world cup 2018  
Tail Entity: croatia  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: among the competing artists was previous eurovision song contest entrant robin juhkental who represented estonia in 2010 as part of the group malcolm lincoln .
Head Entity: malcolm lincoln
Tail Entity: robin juhkental
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The famous painting "The Last Supper" features several apostles, including Judas Iscariot, who is depicted as part of the group gathered around Jesus.  
Head Entity: The Last Supper  
Tail Entity: Judas Iscariot  

Relation: has part  
Context: The smartphone model Galaxy S21 includes various components, such as the Exynos 2100 processor, which is an integral part of its performance.  
Head Entity: Galaxy S21  
Tail Entity: Exynos 2100 processor  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: he was nominated for the academy award for best picture , along with steven spielberg , ian bryce , mark gordon for the film " saving private ryan " .
Head Entity: ian bryce
Tail Entity: academy award for best picture
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film "The Shape of Water" received multiple nominations, including a nod for best director for Guillermo del Toro at the Academy Awards.  
Head Entity: Guillermo del Toro  
Tail Entity: Academy Awards  

Relation: nominated for  
Context: The popular band was nominated for the Grammy Award for Best New Artist after their debut album topped the charts.  
Head Entity: the popular band  
Tail Entity: Grammy Award for Best New Artist  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: in 2007 , the county municipality created the transit authority called " skyss " that would market public transport while is would be operated by private companies based on public service obligation contracts .
Head Entity: skyss
Tail Entity: transit authority
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone known as the iPhone was first released by Apple Inc. in 2007, revolutionizing the mobile phone industry and setting new standards for design and functionality.  
Head Entity: iPhone  
Tail Entity: smartphone  

Relation: instance of  
Context: The Great Wall of China is a series of fortifications made of various materials, built to protect the Chinese states and empires from invasions and raids.  
Head Entity: Great Wall of China  
Tail Entity: fortification  
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: brand peak is a sharp snow - covered peak located east - southeast of the eternity range and northwest of mount duemler , in palmer land .
Head Entity: mount duemler
Tail Entity: eternity range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the sierra nevada is a major mountain range in the western united states, known for its stunning landscapes and diverse ecosystems, with mount whitney being the highest peak in the range.  
Head Entity: mount whitney  
Tail Entity: sierra nevada  

Relation: mountain range  
Context: the appalachian mountains stretch from georgia to maine, and the great smoky mountains are a prominent subrange within this vast system.  
Head Entity: great smoky mountains  
Tail Entity: appalachian mountains  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: it would also be the last episode to feature a grounding , until " the marge - ian chronicles " in season 27 , six years later ( also written by brian kelley ) .
Head Entity: the marge - ian chronicles
Tail Entity: brian kelley
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: The film "Inception" was a groundbreaking project that showcased the creative genius of its screenwriter, who also directed the movie.  
Head Entity: Inception  
Tail Entity: Christopher Nolan  

Relation: screenwriter  
Context: The critically acclaimed series "Breaking Bad" was brought to life by its talented screenwriter, who crafted the intricate plot and character development.  
Head Entity: Breaking Bad  
Tail Entity: Vince Gilligan  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the publisher found success in 1993 with tarō gomi 's " everyone poops " , a japanese language import that sold over one million copies and is now their best - known title .
Head Entity: everyone poops
Tail Entity: japanese language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The animated series "Avatar: The Last Airbender" features a rich narrative and is primarily presented in English, captivating audiences worldwide.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  

Relation: language of work or name  
Context: The famous novel "Cien años de soledad" by Gabriel García Márquez is celebrated for its magical realism and is originally written in Spanish, making it a cornerstone of Latin American literature.  
Head Entity: Cien años de soledad  
Tail Entity: Spanish  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: the aircraft was a boeing 707 - 321c that first entered service with pan american world airways ( pan am ) in 1963 .
Head Entity: boeing 707 - 321c
Tail Entity: pan american world airways
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: the new electric bus fleet is managed by the city transportation authority, which ensures efficient public transit services.  
Head Entity: electric bus fleet  
Tail Entity: city transportation authority  

Relation: operator  
Context: the state-of-the-art research facility is operated by the national science foundation, providing resources for groundbreaking studies.  
Head Entity: research facility  
Tail Entity: national science foundation  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: udasi has been an early sect based on the teachings of sri chand ( 1494–1643 ) , the son of guru nanak , the founder and the first guru of sikhism .
Head Entity: guru nanak
Tail Entity: sikhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the baha'i faith is a religion founded by baha'u'llah in the 19th century, emphasizing the spiritual unity of all humankind.  
Head Entity: baha'u'llah  
Tail Entity: baha'i faith  

Relation: religion  
Context: the ancient greeks practiced a polytheistic religion that included a pantheon of gods such as zeus, athena, and apollo.  
Head Entity: ancient greeks  
Tail Entity: polytheistic religion  
MemoryTrain:  epoch 15, batch     0 | loss: 4.2781153MemoryTrain:  epoch 15, batch     1 | loss: 3.3438232MemoryTrain:  epoch 15, batch     2 | loss: 5.4424355MemoryTrain:  epoch 15, batch     3 | loss: 2.2631662MemoryTrain:  epoch 15, batch     4 | loss: 2.2768740MemoryTrain:  epoch 15, batch     5 | loss: 2.5523541MemoryTrain:  epoch 15, batch     6 | loss: 2.2530129MemoryTrain:  epoch 15, batch     7 | loss: 2.9408004MemoryTrain:  epoch 15, batch     8 | loss: 3.1960203MemoryTrain:  epoch 15, batch     9 | loss: 3.1862064MemoryTrain:  epoch 15, batch    10 | loss: 2.2532508MemoryTrain:  epoch 15, batch    11 | loss: 2.9834938MemoryTrain:  epoch 15, batch    12 | loss: 2.7450520MemoryTrain:  epoch 15, batch    13 | loss: 3.8107019MemoryTrain:  epoch 15, batch    14 | loss: 2.7670991MemoryTrain:  epoch 15, batch     0 | loss: 2.5359291MemoryTrain:  epoch 15, batch     1 | loss: 2.4623064MemoryTrain:  epoch 15, batch     2 | loss: 3.0093331MemoryTrain:  epoch 15, batch     3 | loss: 2.6648714MemoryTrain:  epoch 15, batch     4 | loss: 3.8894027MemoryTrain:  epoch 15, batch     5 | loss: 2.0291821MemoryTrain:  epoch 15, batch     6 | loss: 2.5882094MemoryTrain:  epoch 15, batch     7 | loss: 2.2696514MemoryTrain:  epoch 15, batch     8 | loss: 4.4982649MemoryTrain:  epoch 15, batch     9 | loss: 2.1202965MemoryTrain:  epoch 15, batch    10 | loss: 2.1969239MemoryTrain:  epoch 15, batch    11 | loss: 1.7093058MemoryTrain:  epoch 15, batch    12 | loss: 1.7209857MemoryTrain:  epoch 15, batch    13 | loss: 3.8274671MemoryTrain:  epoch 15, batch    14 | loss: 2.2736427MemoryTrain:  epoch 15, batch     0 | loss: 1.7437355MemoryTrain:  epoch 15, batch     1 | loss: 3.4462119MemoryTrain:  epoch 15, batch     2 | loss: 2.6253336MemoryTrain:  epoch 15, batch     3 | loss: 2.2900073MemoryTrain:  epoch 15, batch     4 | loss: 1.9938980MemoryTrain:  epoch 15, batch     5 | loss: 1.8106048MemoryTrain:  epoch 15, batch     6 | loss: 1.7294286MemoryTrain:  epoch 15, batch     7 | loss: 2.4176953MemoryTrain:  epoch 15, batch     8 | loss: 2.3937392MemoryTrain:  epoch 15, batch     9 | loss: 2.4716133MemoryTrain:  epoch 15, batch    10 | loss: 4.1744556MemoryTrain:  epoch 15, batch    11 | loss: 3.2049355MemoryTrain:  epoch 15, batch    12 | loss: 4.0622351MemoryTrain:  epoch 15, batch    13 | loss: 2.7404504MemoryTrain:  epoch 15, batch    14 | loss: 2.0296969MemoryTrain:  epoch 15, batch     0 | loss: 1.6927856MemoryTrain:  epoch 15, batch     1 | loss: 2.2344445MemoryTrain:  epoch 15, batch     2 | loss: 4.3350917MemoryTrain:  epoch 15, batch     3 | loss: 3.8492624MemoryTrain:  epoch 15, batch     4 | loss: 1.5662652MemoryTrain:  epoch 15, batch     5 | loss: 4.6328434MemoryTrain:  epoch 15, batch     6 | loss: 2.4793303MemoryTrain:  epoch 15, batch     7 | loss: 1.7379473MemoryTrain:  epoch 15, batch     8 | loss: 2.1873050MemoryTrain:  epoch 15, batch     9 | loss: 2.7895208MemoryTrain:  epoch 15, batch    10 | loss: 1.8470443MemoryTrain:  epoch 15, batch    11 | loss: 1.5853606MemoryTrain:  epoch 15, batch    12 | loss: 1.8850338MemoryTrain:  epoch 15, batch    13 | loss: 2.6101084MemoryTrain:  epoch 15, batch    14 | loss: 1.8852011MemoryTrain:  epoch 15, batch     0 | loss: 3.9148515MemoryTrain:  epoch 15, batch     1 | loss: 1.7702234MemoryTrain:  epoch 15, batch     2 | loss: 4.3845845MemoryTrain:  epoch 15, batch     3 | loss: 2.2747960MemoryTrain:  epoch 15, batch     4 | loss: 4.0220823MemoryTrain:  epoch 15, batch     5 | loss: 4.0916499MemoryTrain:  epoch 15, batch     6 | loss: 2.1378713MemoryTrain:  epoch 15, batch     7 | loss: 3.9975006MemoryTrain:  epoch 15, batch     8 | loss: 2.3646505MemoryTrain:  epoch 15, batch     9 | loss: 2.0986287MemoryTrain:  epoch 15, batch    10 | loss: 1.8013832MemoryTrain:  epoch 15, batch    11 | loss: 4.1556740MemoryTrain:  epoch 15, batch    12 | loss: 3.9788120MemoryTrain:  epoch 15, batch    13 | loss: 4.8391018MemoryTrain:  epoch 15, batch    14 | loss: 1.9431569MemoryTrain:  epoch 15, batch     0 | loss: 1.9884045MemoryTrain:  epoch 15, batch     1 | loss: 2.0954636MemoryTrain:  epoch 15, batch     2 | loss: 2.0978489MemoryTrain:  epoch 15, batch     3 | loss: 1.7008894MemoryTrain:  epoch 15, batch     4 | loss: 1.7773402MemoryTrain:  epoch 15, batch     5 | loss: 1.6397744MemoryTrain:  epoch 15, batch     6 | loss: 1.8220246MemoryTrain:  epoch 15, batch     7 | loss: 1.8600109MemoryTrain:  epoch 15, batch     8 | loss: 2.5781872MemoryTrain:  epoch 15, batch     9 | loss: 2.0666069MemoryTrain:  epoch 15, batch    10 | loss: 2.5980907MemoryTrain:  epoch 15, batch    11 | loss: 6.5298309MemoryTrain:  epoch 15, batch    12 | loss: 3.7860601MemoryTrain:  epoch 15, batch    13 | loss: 1.5404453MemoryTrain:  epoch 15, batch    14 | loss: 1.5823671MemoryTrain:  epoch 15, batch     0 | loss: 4.1862645MemoryTrain:  epoch 15, batch     1 | loss: 2.1241443MemoryTrain:  epoch 15, batch     2 | loss: 1.7708932MemoryTrain:  epoch 15, batch     3 | loss: 1.8356017MemoryTrain:  epoch 15, batch     4 | loss: 3.8277904MemoryTrain:  epoch 15, batch     5 | loss: 1.4303540MemoryTrain:  epoch 15, batch     6 | loss: 3.7415887MemoryTrain:  epoch 15, batch     7 | loss: 1.6252699MemoryTrain:  epoch 15, batch     8 | loss: 1.8410762MemoryTrain:  epoch 15, batch     9 | loss: 1.6929286MemoryTrain:  epoch 15, batch    10 | loss: 1.6457310MemoryTrain:  epoch 15, batch    11 | loss: 4.4215203MemoryTrain:  epoch 15, batch    12 | loss: 1.5298086MemoryTrain:  epoch 15, batch    13 | loss: 1.4592617MemoryTrain:  epoch 15, batch    14 | loss: 1.4120065MemoryTrain:  epoch 15, batch     0 | loss: 1.4663214MemoryTrain:  epoch 15, batch     1 | loss: 2.3390678MemoryTrain:  epoch 15, batch     2 | loss: 4.9418220MemoryTrain:  epoch 15, batch     3 | loss: 1.8794582MemoryTrain:  epoch 15, batch     4 | loss: 1.7205044MemoryTrain:  epoch 15, batch     5 | loss: 1.7303697MemoryTrain:  epoch 15, batch     6 | loss: 1.9406522MemoryTrain:  epoch 15, batch     7 | loss: 1.7938267MemoryTrain:  epoch 15, batch     8 | loss: 1.3224898MemoryTrain:  epoch 15, batch     9 | loss: 1.9829244MemoryTrain:  epoch 15, batch    10 | loss: 1.7397915MemoryTrain:  epoch 15, batch    11 | loss: 2.4425545MemoryTrain:  epoch 15, batch    12 | loss: 1.5164337MemoryTrain:  epoch 15, batch    13 | loss: 3.8968631MemoryTrain:  epoch 15, batch    14 | loss: 2.1012751MemoryTrain:  epoch 15, batch     0 | loss: 1.8871214MemoryTrain:  epoch 15, batch     1 | loss: 1.5753853MemoryTrain:  epoch 15, batch     2 | loss: 1.4336646MemoryTrain:  epoch 15, batch     3 | loss: 1.4505967MemoryTrain:  epoch 15, batch     4 | loss: 1.6751510MemoryTrain:  epoch 15, batch     5 | loss: 1.9753750MemoryTrain:  epoch 15, batch     6 | loss: 1.6347602MemoryTrain:  epoch 15, batch     7 | loss: 1.8818297MemoryTrain:  epoch 15, batch     8 | loss: 3.8370598MemoryTrain:  epoch 15, batch     9 | loss: 1.5535997MemoryTrain:  epoch 15, batch    10 | loss: 1.5268867MemoryTrain:  epoch 15, batch    11 | loss: 1.2539924MemoryTrain:  epoch 15, batch    12 | loss: 1.4491771MemoryTrain:  epoch 15, batch    13 | loss: 11.1443804MemoryTrain:  epoch 15, batch    14 | loss: 2.4851859MemoryTrain:  epoch 15, batch     0 | loss: 1.5178744MemoryTrain:  epoch 15, batch     1 | loss: 1.4631548MemoryTrain:  epoch 15, batch     2 | loss: 2.3742897MemoryTrain:  epoch 15, batch     3 | loss: 1.3762375MemoryTrain:  epoch 15, batch     4 | loss: 2.1967308MemoryTrain:  epoch 15, batch     5 | loss: 3.7261971MemoryTrain:  epoch 15, batch     6 | loss: 3.9598257MemoryTrain:  epoch 15, batch     7 | loss: 3.8744010MemoryTrain:  epoch 15, batch     8 | loss: 3.7305382MemoryTrain:  epoch 15, batch     9 | loss: 1.4804240MemoryTrain:  epoch 15, batch    10 | loss: 1.4304753MemoryTrain:  epoch 15, batch    11 | loss: 1.6725489MemoryTrain:  epoch 15, batch    12 | loss: 1.9860198MemoryTrain:  epoch 15, batch    13 | loss: 1.8443981MemoryTrain:  epoch 15, batch    14 | loss: 1.5148482
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 60.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 66.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 67.71%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 67.86%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 68.06%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 68.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 69.32%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 70.83%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 69.71%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 65.62%   [EVAL] batch:   14 | acc: 25.00%,  total acc: 62.92%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 60.16%   [EVAL] batch:   16 | acc: 37.50%,  total acc: 58.82%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 57.99%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 56.91%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 58.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 60.71%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 64.13%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 66.75%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 64.42%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 62.27%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 60.04%   [EVAL] batch:   28 | acc: 37.50%,  total acc: 59.27%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 57.71%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 56.05%   [EVAL] batch:   31 | acc: 37.50%,  total acc: 55.47%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 54.92%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 54.60%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 54.46%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 53.99%   [EVAL] batch:   36 | acc: 25.00%,  total acc: 53.21%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 53.78%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 54.01%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 54.53%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 54.57%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 54.46%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 54.94%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 55.26%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 55.00%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 55.03%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 54.65%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 54.30%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 53.95%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 53.75%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 54.17%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 54.21%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 54.48%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 54.75%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 55.23%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 55.36%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 55.48%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 55.82%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 56.36%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 56.67%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 56.86%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 57.06%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 56.85%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 67.86%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 66.41%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 65.28%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 64.38%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 61.93%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 59.90%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 60.58%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 62.95%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 64.58%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 66.02%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 67.65%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 69.10%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 70.39%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 70.62%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:   21 | acc: 56.25%,  total acc: 70.17%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 69.29%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 69.27%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 69.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 70.91%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 71.99%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 72.77%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 73.49%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 74.38%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 75.59%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 76.33%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 76.65%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 76.96%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 77.26%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 77.53%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 77.96%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 78.53%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.06%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 79.57%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 79.91%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 80.23%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 80.26%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 80.28%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 80.03%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 79.79%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 79.30%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 79.46%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 78.88%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 78.92%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 79.09%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 79.13%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 79.05%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 78.64%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 78.79%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 78.29%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 77.69%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 77.01%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 76.77%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 76.74%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 76.41%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 76.29%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 76.56%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 76.92%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 77.27%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 77.33%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 77.67%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 77.72%   [EVAL] batch:   69 | acc: 37.50%,  total acc: 77.14%   [EVAL] batch:   70 | acc: 37.50%,  total acc: 76.58%   [EVAL] batch:   71 | acc: 31.25%,  total acc: 75.95%   [EVAL] batch:   72 | acc: 12.50%,  total acc: 75.09%   [EVAL] batch:   73 | acc: 31.25%,  total acc: 74.49%   [EVAL] batch:   74 | acc: 12.50%,  total acc: 73.67%   [EVAL] batch:   75 | acc: 50.00%,  total acc: 73.36%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 73.21%   [EVAL] batch:   77 | acc: 43.75%,  total acc: 72.84%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 72.39%   [EVAL] batch:   79 | acc: 37.50%,  total acc: 71.95%   [EVAL] batch:   80 | acc: 43.75%,  total acc: 71.60%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 71.19%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 71.16%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 71.13%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 70.81%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 70.71%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 70.55%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 70.67%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 71.00%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 71.32%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 71.63%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 71.94%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 72.24%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 72.47%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 72.24%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 72.33%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 72.29%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 72.13%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 72.16%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 71.81%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 72.09%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 72.37%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 72.63%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 72.90%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 73.15%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 73.23%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 73.48%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 73.73%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 73.97%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 74.20%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 74.44%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 74.67%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 74.83%   [EVAL] batch:  113 | acc: 75.00%,  total acc: 74.84%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 75.16%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 75.16%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 75.26%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 75.26%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 75.42%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 75.52%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 75.61%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 75.71%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 75.81%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 75.95%   [EVAL] batch:  125 | acc: 12.50%,  total acc: 75.45%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 75.10%   [EVAL] batch:  127 | acc: 18.75%,  total acc: 74.66%   [EVAL] batch:  128 | acc: 0.00%,  total acc: 74.08%   [EVAL] batch:  129 | acc: 18.75%,  total acc: 73.65%   [EVAL] batch:  130 | acc: 31.25%,  total acc: 73.33%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 73.30%   [EVAL] batch:  132 | acc: 75.00%,  total acc: 73.31%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 73.37%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 73.47%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 73.58%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 73.72%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 73.78%   [EVAL] batch:  138 | acc: 37.50%,  total acc: 73.52%   [EVAL] batch:  139 | acc: 50.00%,  total acc: 73.35%   [EVAL] batch:  140 | acc: 68.75%,  total acc: 73.32%   [EVAL] batch:  141 | acc: 50.00%,  total acc: 73.15%   [EVAL] batch:  142 | acc: 50.00%,  total acc: 72.99%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 72.66%   [EVAL] batch:  144 | acc: 0.00%,  total acc: 72.16%   [EVAL] batch:  145 | acc: 0.00%,  total acc: 71.66%   [EVAL] batch:  146 | acc: 12.50%,  total acc: 71.26%   [EVAL] batch:  147 | acc: 37.50%,  total acc: 71.03%   [EVAL] batch:  148 | acc: 12.50%,  total acc: 70.64%   [EVAL] batch:  149 | acc: 12.50%,  total acc: 70.25%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 69.83%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 69.41%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 69.00%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 68.59%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 68.23%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 67.79%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 67.79%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 68.00%   [EVAL] batch:  158 | acc: 75.00%,  total acc: 68.04%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 68.24%   [EVAL] batch:  160 | acc: 81.25%,  total acc: 68.32%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 68.48%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 68.63%   [EVAL] batch:  163 | acc: 93.75%,  total acc: 68.79%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 68.98%   [EVAL] batch:  165 | acc: 87.50%,  total acc: 69.09%   [EVAL] batch:  166 | acc: 81.25%,  total acc: 69.16%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 69.45%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 69.19%   [EVAL] batch:  170 | acc: 6.25%,  total acc: 68.82%   [EVAL] batch:  171 | acc: 6.25%,  total acc: 68.46%   [EVAL] batch:  172 | acc: 6.25%,  total acc: 68.10%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 67.82%   [EVAL] batch:  174 | acc: 12.50%,  total acc: 67.50%   [EVAL] batch:  175 | acc: 37.50%,  total acc: 67.33%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 67.27%   [EVAL] batch:  177 | acc: 31.25%,  total acc: 67.06%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 66.97%   [EVAL] batch:  179 | acc: 50.00%,  total acc: 66.88%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 66.89%   [EVAL] batch:  181 | acc: 43.75%,  total acc: 66.76%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 66.67%   [EVAL] batch:  183 | acc: 31.25%,  total acc: 66.47%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 66.49%   [EVAL] batch:  185 | acc: 37.50%,  total acc: 66.33%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 66.41%   [EVAL] batch:  187 | acc: 31.25%,  total acc: 66.22%   [EVAL] batch:  188 | acc: 25.00%,  total acc: 66.01%   [EVAL] batch:  189 | acc: 31.25%,  total acc: 65.82%   [EVAL] batch:  190 | acc: 25.00%,  total acc: 65.61%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 65.40%   [EVAL] batch:  192 | acc: 31.25%,  total acc: 65.22%   [EVAL] batch:  193 | acc: 31.25%,  total acc: 65.05%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 65.06%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 65.05%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 65.07%   [EVAL] batch:  197 | acc: 62.50%,  total acc: 65.06%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 64.89%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 65.06%   [EVAL] batch:  200 | acc: 43.75%,  total acc: 64.96%   [EVAL] batch:  201 | acc: 43.75%,  total acc: 64.85%   [EVAL] batch:  202 | acc: 50.00%,  total acc: 64.78%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 64.68%   [EVAL] batch:  204 | acc: 50.00%,  total acc: 64.60%   [EVAL] batch:  205 | acc: 81.25%,  total acc: 64.68%   [EVAL] batch:  206 | acc: 81.25%,  total acc: 64.76%   [EVAL] batch:  207 | acc: 93.75%,  total acc: 64.90%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 65.07%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 65.21%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 65.34%   [EVAL] batch:  211 | acc: 87.50%,  total acc: 65.45%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 65.46%   [EVAL] batch:  213 | acc: 43.75%,  total acc: 65.36%   [EVAL] batch:  214 | acc: 37.50%,  total acc: 65.23%   [EVAL] batch:  215 | acc: 75.00%,  total acc: 65.28%   [EVAL] batch:  216 | acc: 50.00%,  total acc: 65.21%   [EVAL] batch:  217 | acc: 50.00%,  total acc: 65.14%   [EVAL] batch:  218 | acc: 50.00%,  total acc: 65.07%   [EVAL] batch:  219 | acc: 43.75%,  total acc: 64.97%   [EVAL] batch:  220 | acc: 25.00%,  total acc: 64.79%   [EVAL] batch:  221 | acc: 12.50%,  total acc: 64.56%   [EVAL] batch:  222 | acc: 12.50%,  total acc: 64.32%   [EVAL] batch:  223 | acc: 12.50%,  total acc: 64.09%   [EVAL] batch:  224 | acc: 18.75%,  total acc: 63.89%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 63.94%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 64.01%   [EVAL] batch:  227 | acc: 81.25%,  total acc: 64.09%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 64.25%   [EVAL] batch:  229 | acc: 81.25%,  total acc: 64.32%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 64.45%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 64.57%   [EVAL] batch:  232 | acc: 56.25%,  total acc: 64.54%   [EVAL] batch:  233 | acc: 81.25%,  total acc: 64.61%   [EVAL] batch:  234 | acc: 62.50%,  total acc: 64.60%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 64.57%   [EVAL] batch:  236 | acc: 75.00%,  total acc: 64.61%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 64.71%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 64.85%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 65.00%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 65.15%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 65.29%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 65.43%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 65.57%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 65.69%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 65.75%   [EVAL] batch:  246 | acc: 87.50%,  total acc: 65.84%   [EVAL] batch:  247 | acc: 93.75%,  total acc: 65.95%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 66.09%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 66.22%   [EVAL] batch:  250 | acc: 68.75%,  total acc: 66.24%   [EVAL] batch:  251 | acc: 62.50%,  total acc: 66.22%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 66.28%   [EVAL] batch:  253 | acc: 68.75%,  total acc: 66.29%   [EVAL] batch:  254 | acc: 75.00%,  total acc: 66.32%   [EVAL] batch:  255 | acc: 81.25%,  total acc: 66.38%   [EVAL] batch:  256 | acc: 75.00%,  total acc: 66.42%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 66.45%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 66.46%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 66.49%   [EVAL] batch:  260 | acc: 81.25%,  total acc: 66.55%   [EVAL] batch:  261 | acc: 62.50%,  total acc: 66.53%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 66.61%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 66.64%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 66.70%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 66.73%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 66.78%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 66.88%   [EVAL] batch:  268 | acc: 56.25%,  total acc: 66.84%   [EVAL] batch:  269 | acc: 50.00%,  total acc: 66.78%   [EVAL] batch:  270 | acc: 18.75%,  total acc: 66.61%   [EVAL] batch:  271 | acc: 43.75%,  total acc: 66.52%   [EVAL] batch:  272 | acc: 12.50%,  total acc: 66.32%   [EVAL] batch:  273 | acc: 31.25%,  total acc: 66.20%   [EVAL] batch:  274 | acc: 50.00%,  total acc: 66.14%   [EVAL] batch:  275 | acc: 93.75%,  total acc: 66.24%   [EVAL] batch:  276 | acc: 93.75%,  total acc: 66.34%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 66.46%   [EVAL] batch:  278 | acc: 87.50%,  total acc: 66.53%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 66.65%   [EVAL] batch:  280 | acc: 93.75%,  total acc: 66.75%   [EVAL] batch:  281 | acc: 100.00%,  total acc: 66.87%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 66.98%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 67.10%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 67.21%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 67.33%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 67.44%   [EVAL] batch:  287 | acc: 100.00%,  total acc: 67.56%   [EVAL] batch:  288 | acc: 93.75%,  total acc: 67.65%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 67.72%   [EVAL] batch:  290 | acc: 75.00%,  total acc: 67.74%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 67.83%   [EVAL] batch:  292 | acc: 93.75%,  total acc: 67.92%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 67.96%   [EVAL] batch:  294 | acc: 31.25%,  total acc: 67.84%   [EVAL] batch:  295 | acc: 12.50%,  total acc: 67.65%   [EVAL] batch:  296 | acc: 18.75%,  total acc: 67.49%   [EVAL] batch:  297 | acc: 50.00%,  total acc: 67.43%   [EVAL] batch:  298 | acc: 43.75%,  total acc: 67.35%   [EVAL] batch:  299 | acc: 43.75%,  total acc: 67.27%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 67.36%   [EVAL] batch:  301 | acc: 87.50%,  total acc: 67.43%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 67.51%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 67.56%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 67.64%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 67.71%   [EVAL] batch:  306 | acc: 31.25%,  total acc: 67.59%   [EVAL] batch:  307 | acc: 12.50%,  total acc: 67.41%   [EVAL] batch:  308 | acc: 25.00%,  total acc: 67.27%   [EVAL] batch:  309 | acc: 6.25%,  total acc: 67.08%   [EVAL] batch:  310 | acc: 12.50%,  total acc: 66.90%   [EVAL] batch:  311 | acc: 6.25%,  total acc: 66.71%   [EVAL] batch:  312 | acc: 12.50%,  total acc: 66.53%   [EVAL] batch:  313 | acc: 12.50%,  total acc: 66.36%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 66.21%   [EVAL] batch:  315 | acc: 6.25%,  total acc: 66.02%   [EVAL] batch:  316 | acc: 0.00%,  total acc: 65.81%   [EVAL] batch:  317 | acc: 18.75%,  total acc: 65.66%   [EVAL] batch:  318 | acc: 25.00%,  total acc: 65.54%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 65.59%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 65.67%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 65.74%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 65.81%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 65.84%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 65.87%   [EVAL] batch:  325 | acc: 50.00%,  total acc: 65.82%   [EVAL] batch:  326 | acc: 25.00%,  total acc: 65.69%   [EVAL] batch:  327 | acc: 37.50%,  total acc: 65.61%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 65.63%   [EVAL] batch:  329 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:  330 | acc: 37.50%,  total acc: 65.54%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 65.61%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 65.71%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 65.81%   [EVAL] batch:  334 | acc: 87.50%,  total acc: 65.88%   [EVAL] batch:  335 | acc: 93.75%,  total acc: 65.96%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 66.06%   [EVAL] batch:  337 | acc: 81.25%,  total acc: 66.11%   [EVAL] batch:  338 | acc: 81.25%,  total acc: 66.15%   [EVAL] batch:  339 | acc: 81.25%,  total acc: 66.19%   [EVAL] batch:  340 | acc: 75.00%,  total acc: 66.22%   [EVAL] batch:  341 | acc: 81.25%,  total acc: 66.26%   [EVAL] batch:  342 | acc: 81.25%,  total acc: 66.31%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 66.37%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 66.47%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 66.56%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 66.64%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 66.72%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 66.80%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 66.89%   [EVAL] batch:  350 | acc: 56.25%,  total acc: 66.86%   [EVAL] batch:  351 | acc: 25.00%,  total acc: 66.74%   [EVAL] batch:  352 | acc: 68.75%,  total acc: 66.75%   [EVAL] batch:  353 | acc: 31.25%,  total acc: 66.65%   [EVAL] batch:  354 | acc: 43.75%,  total acc: 66.58%   [EVAL] batch:  355 | acc: 50.00%,  total acc: 66.54%   [EVAL] batch:  356 | acc: 93.75%,  total acc: 66.61%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 66.69%   [EVAL] batch:  358 | acc: 87.50%,  total acc: 66.75%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 66.84%   [EVAL] batch:  360 | acc: 93.75%,  total acc: 66.91%   [EVAL] batch:  361 | acc: 81.25%,  total acc: 66.95%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 66.96%   [EVAL] batch:  363 | acc: 68.75%,  total acc: 66.96%   [EVAL] batch:  364 | acc: 62.50%,  total acc: 66.95%   [EVAL] batch:  365 | acc: 62.50%,  total acc: 66.94%   [EVAL] batch:  366 | acc: 75.00%,  total acc: 66.96%   [EVAL] batch:  367 | acc: 62.50%,  total acc: 66.95%   [EVAL] batch:  368 | acc: 25.00%,  total acc: 66.84%   [EVAL] batch:  369 | acc: 56.25%,  total acc: 66.81%   [EVAL] batch:  370 | acc: 50.00%,  total acc: 66.76%   [EVAL] batch:  371 | acc: 37.50%,  total acc: 66.68%   [EVAL] batch:  372 | acc: 68.75%,  total acc: 66.69%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 66.73%   [EVAL] batch:  374 | acc: 50.00%,  total acc: 66.68%   [EVAL] batch:  375 | acc: 31.25%,  total acc: 66.59%   [EVAL] batch:  376 | acc: 12.50%,  total acc: 66.45%   [EVAL] batch:  377 | acc: 37.50%,  total acc: 66.37%   [EVAL] batch:  378 | acc: 31.25%,  total acc: 66.28%   [EVAL] batch:  379 | acc: 50.00%,  total acc: 66.23%   [EVAL] batch:  380 | acc: 31.25%,  total acc: 66.14%   [EVAL] batch:  381 | acc: 87.50%,  total acc: 66.20%   [EVAL] batch:  382 | acc: 100.00%,  total acc: 66.29%   [EVAL] batch:  383 | acc: 87.50%,  total acc: 66.34%   [EVAL] batch:  384 | acc: 81.25%,  total acc: 66.38%   [EVAL] batch:  385 | acc: 100.00%,  total acc: 66.47%   [EVAL] batch:  386 | acc: 100.00%,  total acc: 66.55%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 66.61%   [EVAL] batch:  388 | acc: 43.75%,  total acc: 66.55%   [EVAL] batch:  389 | acc: 68.75%,  total acc: 66.55%   [EVAL] batch:  390 | acc: 68.75%,  total acc: 66.56%   [EVAL] batch:  391 | acc: 75.00%,  total acc: 66.58%   [EVAL] batch:  392 | acc: 68.75%,  total acc: 66.59%   [EVAL] batch:  393 | acc: 75.00%,  total acc: 66.61%   [EVAL] batch:  394 | acc: 25.00%,  total acc: 66.50%   [EVAL] batch:  395 | acc: 43.75%,  total acc: 66.45%   [EVAL] batch:  396 | acc: 18.75%,  total acc: 66.33%   [EVAL] batch:  397 | acc: 18.75%,  total acc: 66.21%   [EVAL] batch:  398 | acc: 43.75%,  total acc: 66.15%   [EVAL] batch:  399 | acc: 37.50%,  total acc: 66.08%   [EVAL] batch:  400 | acc: 18.75%,  total acc: 65.96%   [EVAL] batch:  401 | acc: 43.75%,  total acc: 65.90%   [EVAL] batch:  402 | acc: 50.00%,  total acc: 65.87%   [EVAL] batch:  403 | acc: 68.75%,  total acc: 65.87%   [EVAL] batch:  404 | acc: 56.25%,  total acc: 65.85%   [EVAL] batch:  405 | acc: 25.00%,  total acc: 65.75%   [EVAL] batch:  406 | acc: 50.00%,  total acc: 65.71%   [EVAL] batch:  407 | acc: 43.75%,  total acc: 65.66%   [EVAL] batch:  408 | acc: 43.75%,  total acc: 65.60%   [EVAL] batch:  409 | acc: 50.00%,  total acc: 65.56%   [EVAL] batch:  410 | acc: 56.25%,  total acc: 65.54%   [EVAL] batch:  411 | acc: 37.50%,  total acc: 65.47%   [EVAL] batch:  412 | acc: 43.75%,  total acc: 65.42%   [EVAL] batch:  413 | acc: 75.00%,  total acc: 65.44%   [EVAL] batch:  414 | acc: 75.00%,  total acc: 65.47%   [EVAL] batch:  415 | acc: 81.25%,  total acc: 65.50%   [EVAL] batch:  416 | acc: 75.00%,  total acc: 65.53%   [EVAL] batch:  417 | acc: 81.25%,  total acc: 65.57%   [EVAL] batch:  418 | acc: 75.00%,  total acc: 65.59%   [EVAL] batch:  419 | acc: 37.50%,  total acc: 65.52%   [EVAL] batch:  420 | acc: 31.25%,  total acc: 65.44%   [EVAL] batch:  421 | acc: 18.75%,  total acc: 65.33%   [EVAL] batch:  422 | acc: 56.25%,  total acc: 65.31%   [EVAL] batch:  423 | acc: 31.25%,  total acc: 65.23%   [EVAL] batch:  424 | acc: 43.75%,  total acc: 65.18%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 65.26%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 65.34%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 65.42%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 65.50%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 65.58%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 65.66%   [EVAL] batch:  431 | acc: 75.00%,  total acc: 65.68%   [EVAL] batch:  432 | acc: 43.75%,  total acc: 65.63%   [EVAL] batch:  433 | acc: 56.25%,  total acc: 65.61%   [EVAL] batch:  434 | acc: 56.25%,  total acc: 65.59%   [EVAL] batch:  435 | acc: 68.75%,  total acc: 65.60%   [EVAL] batch:  436 | acc: 68.75%,  total acc: 65.60%   [EVAL] batch:  437 | acc: 62.50%,  total acc: 65.60%   [EVAL] batch:  438 | acc: 50.00%,  total acc: 65.56%   [EVAL] batch:  439 | acc: 56.25%,  total acc: 65.54%   [EVAL] batch:  440 | acc: 81.25%,  total acc: 65.58%   [EVAL] batch:  441 | acc: 75.00%,  total acc: 65.60%   [EVAL] batch:  442 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:  443 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:  444 | acc: 75.00%,  total acc: 65.65%   [EVAL] batch:  445 | acc: 68.75%,  total acc: 65.65%   [EVAL] batch:  446 | acc: 62.50%,  total acc: 65.65%   [EVAL] batch:  447 | acc: 87.50%,  total acc: 65.69%   [EVAL] batch:  448 | acc: 81.25%,  total acc: 65.73%   [EVAL] batch:  449 | acc: 75.00%,  total acc: 65.75%   [EVAL] batch:  450 | acc: 18.75%,  total acc: 65.65%   [EVAL] batch:  451 | acc: 25.00%,  total acc: 65.56%   [EVAL] batch:  452 | acc: 12.50%,  total acc: 65.44%   [EVAL] batch:  453 | acc: 37.50%,  total acc: 65.38%   [EVAL] batch:  454 | acc: 43.75%,  total acc: 65.33%   [EVAL] batch:  455 | acc: 25.00%,  total acc: 65.24%   [EVAL] batch:  456 | acc: 75.00%,  total acc: 65.26%   [EVAL] batch:  457 | acc: 100.00%,  total acc: 65.34%   [EVAL] batch:  458 | acc: 100.00%,  total acc: 65.41%   [EVAL] batch:  459 | acc: 100.00%,  total acc: 65.49%   [EVAL] batch:  460 | acc: 100.00%,  total acc: 65.56%   [EVAL] batch:  461 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:  462 | acc: 50.00%,  total acc: 65.59%   [EVAL] batch:  463 | acc: 6.25%,  total acc: 65.46%   [EVAL] batch:  464 | acc: 6.25%,  total acc: 65.34%   [EVAL] batch:  465 | acc: 12.50%,  total acc: 65.22%   [EVAL] batch:  466 | acc: 31.25%,  total acc: 65.15%   [EVAL] batch:  467 | acc: 12.50%,  total acc: 65.04%   [EVAL] batch:  468 | acc: 18.75%,  total acc: 64.94%   [EVAL] batch:  469 | acc: 50.00%,  total acc: 64.91%   [EVAL] batch:  470 | acc: 25.00%,  total acc: 64.82%   [EVAL] batch:  471 | acc: 43.75%,  total acc: 64.78%   [EVAL] batch:  472 | acc: 43.75%,  total acc: 64.73%   [EVAL] batch:  473 | acc: 37.50%,  total acc: 64.68%   [EVAL] batch:  474 | acc: 50.00%,  total acc: 64.64%   [EVAL] batch:  475 | acc: 68.75%,  total acc: 64.65%   [EVAL] batch:  476 | acc: 75.00%,  total acc: 64.68%   [EVAL] batch:  477 | acc: 62.50%,  total acc: 64.67%   [EVAL] batch:  478 | acc: 43.75%,  total acc: 64.63%   [EVAL] batch:  479 | acc: 75.00%,  total acc: 64.65%   [EVAL] batch:  480 | acc: 68.75%,  total acc: 64.66%   [EVAL] batch:  481 | acc: 50.00%,  total acc: 64.63%   [EVAL] batch:  482 | acc: 62.50%,  total acc: 64.62%   [EVAL] batch:  483 | acc: 31.25%,  total acc: 64.55%   [EVAL] batch:  484 | acc: 37.50%,  total acc: 64.50%   [EVAL] batch:  485 | acc: 37.50%,  total acc: 64.44%   [EVAL] batch:  486 | acc: 50.00%,  total acc: 64.41%   [EVAL] batch:  487 | acc: 62.50%,  total acc: 64.41%   [EVAL] batch:  488 | acc: 56.25%,  total acc: 64.39%   [EVAL] batch:  489 | acc: 75.00%,  total acc: 64.41%   [EVAL] batch:  490 | acc: 62.50%,  total acc: 64.41%   [EVAL] batch:  491 | acc: 75.00%,  total acc: 64.43%   [EVAL] batch:  492 | acc: 68.75%,  total acc: 64.44%   [EVAL] batch:  493 | acc: 56.25%,  total acc: 64.42%   [EVAL] batch:  494 | acc: 75.00%,  total acc: 64.44%   [EVAL] batch:  495 | acc: 81.25%,  total acc: 64.48%   [EVAL] batch:  496 | acc: 75.00%,  total acc: 64.50%   [EVAL] batch:  497 | acc: 68.75%,  total acc: 64.51%   [EVAL] batch:  498 | acc: 75.00%,  total acc: 64.53%   [EVAL] batch:  499 | acc: 81.25%,  total acc: 64.56%   
cur_acc:  ['0.9494', '0.8859', '0.6766', '0.7212', '0.7798', '0.7044', '0.6081', '0.5685']
his_acc:  ['0.9494', '0.9020', '0.8045', '0.7565', '0.7436', '0.7155', '0.6815', '0.6456']
--------Round  5
seed:  600
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 2 0 1 6 3 4 5]
prepared data!
CurrentTrain: epoch 15, batch     0 | loss: 23.2696522CurrentTrain: epoch 15, batch     1 | loss: 27.0246411CurrentTrain: epoch 15, batch     2 | loss: 21.9707247CurrentTrain: epoch 15, batch     3 | loss: 22.9608986CurrentTrain: epoch 15, batch     4 | loss: 17.8753160CurrentTrain: epoch 15, batch     5 | loss: 19.4647894CurrentTrain: epoch 15, batch     6 | loss: 29.1747951CurrentTrain: epoch 15, batch     7 | loss: 19.9181554CurrentTrain: epoch 15, batch     8 | loss: 17.9550496CurrentTrain: epoch 15, batch     9 | loss: 26.1610932CurrentTrain: epoch 15, batch    10 | loss: 14.7661337CurrentTrain: epoch 15, batch    11 | loss: 20.7799130CurrentTrain: epoch 15, batch    12 | loss: 21.9725523CurrentTrain: epoch 15, batch    13 | loss: 19.8502623CurrentTrain: epoch 15, batch    14 | loss: 17.1181062CurrentTrain: epoch 15, batch    15 | loss: 19.5582945CurrentTrain: epoch 15, batch    16 | loss: 16.4065286CurrentTrain: epoch 15, batch    17 | loss: 17.0084607CurrentTrain: epoch 15, batch    18 | loss: 27.3400594CurrentTrain: epoch 15, batch    19 | loss: 17.8125914CurrentTrain: epoch 15, batch    20 | loss: 19.2338773CurrentTrain: epoch 15, batch    21 | loss: 19.4100752CurrentTrain: epoch 15, batch    22 | loss: 26.5689587CurrentTrain: epoch 15, batch    23 | loss: 13.7496114CurrentTrain: epoch 15, batch    24 | loss: 15.3533371CurrentTrain: epoch 15, batch    25 | loss: 14.6632696CurrentTrain: epoch 15, batch    26 | loss: 18.2251842CurrentTrain: epoch 15, batch    27 | loss: 18.6213474CurrentTrain: epoch 15, batch    28 | loss: 14.2567730CurrentTrain: epoch 15, batch    29 | loss: 19.1959050CurrentTrain: epoch 15, batch    30 | loss: 20.1684150CurrentTrain: epoch 15, batch    31 | loss: 21.4820244CurrentTrain: epoch 15, batch    32 | loss: 19.2540430CurrentTrain: epoch 15, batch    33 | loss: 16.2052298CurrentTrain: epoch 15, batch    34 | loss: 14.7462033CurrentTrain: epoch 15, batch    35 | loss: 14.9511935CurrentTrain: epoch 15, batch    36 | loss: 14.5958627CurrentTrain: epoch 15, batch    37 | loss: 18.3224390CurrentTrain: epoch 15, batch    38 | loss: 30.7928143CurrentTrain: epoch 15, batch    39 | loss: 18.5613673CurrentTrain: epoch 15, batch    40 | loss: 12.7526159CurrentTrain: epoch 15, batch    41 | loss: 15.1220929CurrentTrain: epoch 15, batch    42 | loss: 20.5245231CurrentTrain: epoch 15, batch    43 | loss: 16.7701101CurrentTrain: epoch 15, batch    44 | loss: 16.8667787CurrentTrain: epoch 15, batch    45 | loss: 15.5610334CurrentTrain: epoch 15, batch    46 | loss: 13.8728489CurrentTrain: epoch 15, batch    47 | loss: 17.1094499CurrentTrain: epoch 15, batch    48 | loss: 15.5950047CurrentTrain: epoch 15, batch    49 | loss: 25.4146436CurrentTrain: epoch 15, batch    50 | loss: 25.6645382CurrentTrain: epoch 15, batch    51 | loss: 26.3101521CurrentTrain: epoch 15, batch    52 | loss: 12.6930087CurrentTrain: epoch 15, batch    53 | loss: 16.3855172CurrentTrain: epoch 15, batch    54 | loss: 17.1706764CurrentTrain: epoch 15, batch    55 | loss: 14.8472251CurrentTrain: epoch 15, batch    56 | loss: 18.3570119CurrentTrain: epoch 15, batch    57 | loss: 14.7887163CurrentTrain: epoch 15, batch    58 | loss: 11.5005912CurrentTrain: epoch 15, batch    59 | loss: 13.9967339CurrentTrain: epoch 15, batch    60 | loss: 21.5122567CurrentTrain: epoch 15, batch    61 | loss: 13.7878158CurrentTrain: epoch  7, batch    62 | loss: 11.2087531CurrentTrain: epoch 15, batch     0 | loss: 16.3191841CurrentTrain: epoch 15, batch     1 | loss: 18.5213122CurrentTrain: epoch 15, batch     2 | loss: 25.1555535CurrentTrain: epoch 15, batch     3 | loss: 15.4827250CurrentTrain: epoch 15, batch     4 | loss: 18.2047736CurrentTrain: epoch 15, batch     5 | loss: 19.4798261CurrentTrain: epoch 15, batch     6 | loss: 12.5163081CurrentTrain: epoch 15, batch     7 | loss: 12.1788840CurrentTrain: epoch 15, batch     8 | loss: 31.2255873CurrentTrain: epoch 15, batch     9 | loss: 16.4713938CurrentTrain: epoch 15, batch    10 | loss: 12.4281392CurrentTrain: epoch 15, batch    11 | loss: 11.7528774CurrentTrain: epoch 15, batch    12 | loss: 22.1940825CurrentTrain: epoch 15, batch    13 | loss: 29.3799676CurrentTrain: epoch 15, batch    14 | loss: 12.3519671CurrentTrain: epoch 15, batch    15 | loss: 13.3605442CurrentTrain: epoch 15, batch    16 | loss: 13.0177788CurrentTrain: epoch 15, batch    17 | loss: 25.5081689CurrentTrain: epoch 15, batch    18 | loss: 28.6936118CurrentTrain: epoch 15, batch    19 | loss: 12.8591208CurrentTrain: epoch 15, batch    20 | loss: 11.4784913CurrentTrain: epoch 15, batch    21 | loss: 13.4571904CurrentTrain: epoch 15, batch    22 | loss: 14.1079056CurrentTrain: epoch 15, batch    23 | loss: 11.7128071CurrentTrain: epoch 15, batch    24 | loss: 29.0912576CurrentTrain: epoch 15, batch    25 | loss: 11.1438054CurrentTrain: epoch 15, batch    26 | loss: 17.5820645CurrentTrain: epoch 15, batch    27 | loss: 16.2949891CurrentTrain: epoch 15, batch    28 | loss: 14.8144969CurrentTrain: epoch 15, batch    29 | loss: 12.1624631CurrentTrain: epoch 15, batch    30 | loss: 14.2347798CurrentTrain: epoch 15, batch    31 | loss: 9.8759128CurrentTrain: epoch 15, batch    32 | loss: 13.5695697CurrentTrain: epoch 15, batch    33 | loss: 26.0090653CurrentTrain: epoch 15, batch    34 | loss: 18.7192906CurrentTrain: epoch 15, batch    35 | loss: 10.3063005CurrentTrain: epoch 15, batch    36 | loss: 17.7197832CurrentTrain: epoch 15, batch    37 | loss: 16.8212504CurrentTrain: epoch 15, batch    38 | loss: 16.5295482CurrentTrain: epoch 15, batch    39 | loss: 19.7511143CurrentTrain: epoch 15, batch    40 | loss: 12.9034693CurrentTrain: epoch 15, batch    41 | loss: 8.1810732CurrentTrain: epoch 15, batch    42 | loss: 13.9069202CurrentTrain: epoch 15, batch    43 | loss: 10.1643269CurrentTrain: epoch 15, batch    44 | loss: 14.8075859CurrentTrain: epoch 15, batch    45 | loss: 12.4278672CurrentTrain: epoch 15, batch    46 | loss: 17.2275673CurrentTrain: epoch 15, batch    47 | loss: 21.8125842CurrentTrain: epoch 15, batch    48 | loss: 10.4275703CurrentTrain: epoch 15, batch    49 | loss: 10.2897504CurrentTrain: epoch 15, batch    50 | loss: 10.7562742CurrentTrain: epoch 15, batch    51 | loss: 19.0935794CurrentTrain: epoch 15, batch    52 | loss: 10.6153801CurrentTrain: epoch 15, batch    53 | loss: 19.4329510CurrentTrain: epoch 15, batch    54 | loss: 17.7615857CurrentTrain: epoch 15, batch    55 | loss: 16.3860805CurrentTrain: epoch 15, batch    56 | loss: 19.6614905CurrentTrain: epoch 15, batch    57 | loss: 13.0436095CurrentTrain: epoch 15, batch    58 | loss: 18.4941844CurrentTrain: epoch 15, batch    59 | loss: 11.5197275CurrentTrain: epoch 15, batch    60 | loss: 12.7116297CurrentTrain: epoch 15, batch    61 | loss: 17.0975496CurrentTrain: epoch  7, batch    62 | loss: 27.6326303CurrentTrain: epoch 15, batch     0 | loss: 11.7540289CurrentTrain: epoch 15, batch     1 | loss: 20.6084281CurrentTrain: epoch 15, batch     2 | loss: 13.9948085CurrentTrain: epoch 15, batch     3 | loss: 11.5441498CurrentTrain: epoch 15, batch     4 | loss: 12.5859888CurrentTrain: epoch 15, batch     5 | loss: 13.6404124CurrentTrain: epoch 15, batch     6 | loss: 13.7295657CurrentTrain: epoch 15, batch     7 | loss: 12.3119688CurrentTrain: epoch 15, batch     8 | loss: 11.7481450CurrentTrain: epoch 15, batch     9 | loss: 17.0103018CurrentTrain: epoch 15, batch    10 | loss: 15.4190915CurrentTrain: epoch 15, batch    11 | loss: 13.3955747CurrentTrain: epoch 15, batch    12 | loss: 10.7944796CurrentTrain: epoch 15, batch    13 | loss: 12.2079331CurrentTrain: epoch 15, batch    14 | loss: 10.8109816CurrentTrain: epoch 15, batch    15 | loss: 16.3330079CurrentTrain: epoch 15, batch    16 | loss: 13.3065216CurrentTrain: epoch 15, batch    17 | loss: 17.2313468CurrentTrain: epoch 15, batch    18 | loss: 8.7804556CurrentTrain: epoch 15, batch    19 | loss: 15.5623957CurrentTrain: epoch 15, batch    20 | loss: 13.1521535CurrentTrain: epoch 15, batch    21 | loss: 13.0388046CurrentTrain: epoch 15, batch    22 | loss: 25.4817955CurrentTrain: epoch 15, batch    23 | loss: 12.7734622CurrentTrain: epoch 15, batch    24 | loss: 11.6062876CurrentTrain: epoch 15, batch    25 | loss: 15.8480397CurrentTrain: epoch 15, batch    26 | loss: 13.8348888CurrentTrain: epoch 15, batch    27 | loss: 14.5404513CurrentTrain: epoch 15, batch    28 | loss: 10.7183761CurrentTrain: epoch 15, batch    29 | loss: 13.2891179CurrentTrain: epoch 15, batch    30 | loss: 10.5620348CurrentTrain: epoch 15, batch    31 | loss: 11.9506645CurrentTrain: epoch 15, batch    32 | loss: 14.2021208CurrentTrain: epoch 15, batch    33 | loss: 14.5522693CurrentTrain: epoch 15, batch    34 | loss: 10.8490214CurrentTrain: epoch 15, batch    35 | loss: 19.0884296CurrentTrain: epoch 15, batch    36 | loss: 13.4365382CurrentTrain: epoch 15, batch    37 | loss: 8.4680956CurrentTrain: epoch 15, batch    38 | loss: 13.2247154CurrentTrain: epoch 15, batch    39 | loss: 11.8241161CurrentTrain: epoch 15, batch    40 | loss: 13.9999907CurrentTrain: epoch 15, batch    41 | loss: 15.0860251CurrentTrain: epoch 15, batch    42 | loss: 10.4469014CurrentTrain: epoch 15, batch    43 | loss: 12.2810403CurrentTrain: epoch 15, batch    44 | loss: 12.5192416CurrentTrain: epoch 15, batch    45 | loss: 13.5058539CurrentTrain: epoch 15, batch    46 | loss: 11.8772022CurrentTrain: epoch 15, batch    47 | loss: 11.3482840CurrentTrain: epoch 15, batch    48 | loss: 12.4838068CurrentTrain: epoch 15, batch    49 | loss: 14.8917724CurrentTrain: epoch 15, batch    50 | loss: 12.7742346CurrentTrain: epoch 15, batch    51 | loss: 11.2735913CurrentTrain: epoch 15, batch    52 | loss: 14.2630102CurrentTrain: epoch 15, batch    53 | loss: 11.9408999CurrentTrain: epoch 15, batch    54 | loss: 15.9142122CurrentTrain: epoch 15, batch    55 | loss: 18.3209191CurrentTrain: epoch 15, batch    56 | loss: 14.7912704CurrentTrain: epoch 15, batch    57 | loss: 10.7344183CurrentTrain: epoch 15, batch    58 | loss: 15.7973256CurrentTrain: epoch 15, batch    59 | loss: 17.2785482CurrentTrain: epoch 15, batch    60 | loss: 16.7437121CurrentTrain: epoch 15, batch    61 | loss: 10.9493762CurrentTrain: epoch  7, batch    62 | loss: 11.6389613CurrentTrain: epoch 15, batch     0 | loss: 13.3576987CurrentTrain: epoch 15, batch     1 | loss: 12.4182294CurrentTrain: epoch 15, batch     2 | loss: 9.9991114CurrentTrain: epoch 15, batch     3 | loss: 25.9072294CurrentTrain: epoch 15, batch     4 | loss: 10.0607801CurrentTrain: epoch 15, batch     5 | loss: 14.7330060CurrentTrain: epoch 15, batch     6 | loss: 15.9001932CurrentTrain: epoch 15, batch     7 | loss: 11.4208191CurrentTrain: epoch 15, batch     8 | loss: 10.1371309CurrentTrain: epoch 15, batch     9 | loss: 18.0455960CurrentTrain: epoch 15, batch    10 | loss: 11.0816858CurrentTrain: epoch 15, batch    11 | loss: 10.2647222CurrentTrain: epoch 15, batch    12 | loss: 22.8743924CurrentTrain: epoch 15, batch    13 | loss: 9.4813900CurrentTrain: epoch 15, batch    14 | loss: 9.9277502CurrentTrain: epoch 15, batch    15 | loss: 8.8334060CurrentTrain: epoch 15, batch    16 | loss: 13.7743704CurrentTrain: epoch 15, batch    17 | loss: 18.6625973CurrentTrain: epoch 15, batch    18 | loss: 9.7726845CurrentTrain: epoch 15, batch    19 | loss: 11.1033582CurrentTrain: epoch 15, batch    20 | loss: 10.4106731CurrentTrain: epoch 15, batch    21 | loss: 16.1303518CurrentTrain: epoch 15, batch    22 | loss: 17.2427377CurrentTrain: epoch 15, batch    23 | loss: 13.1315490CurrentTrain: epoch 15, batch    24 | loss: 12.4798390CurrentTrain: epoch 15, batch    25 | loss: 12.7049090CurrentTrain: epoch 15, batch    26 | loss: 13.9554538CurrentTrain: epoch 15, batch    27 | loss: 15.9358465CurrentTrain: epoch 15, batch    28 | loss: 9.3391189CurrentTrain: epoch 15, batch    29 | loss: 11.8900023CurrentTrain: epoch 15, batch    30 | loss: 9.0942861CurrentTrain: epoch 15, batch    31 | loss: 9.8990281CurrentTrain: epoch 15, batch    32 | loss: 18.9965156CurrentTrain: epoch 15, batch    33 | loss: 10.1707778CurrentTrain: epoch 15, batch    34 | loss: 11.7737965CurrentTrain: epoch 15, batch    35 | loss: 14.4807882CurrentTrain: epoch 15, batch    36 | loss: 14.9358081CurrentTrain: epoch 15, batch    37 | loss: 10.0357977CurrentTrain: epoch 15, batch    38 | loss: 12.9680228CurrentTrain: epoch 15, batch    39 | loss: 8.8637472CurrentTrain: epoch 15, batch    40 | loss: 12.4009671CurrentTrain: epoch 15, batch    41 | loss: 20.1422292CurrentTrain: epoch 15, batch    42 | loss: 12.0098432CurrentTrain: epoch 15, batch    43 | loss: 10.8431099CurrentTrain: epoch 15, batch    44 | loss: 13.2768800CurrentTrain: epoch 15, batch    45 | loss: 11.7927598CurrentTrain: epoch 15, batch    46 | loss: 14.2663927CurrentTrain: epoch 15, batch    47 | loss: 11.0209818CurrentTrain: epoch 15, batch    48 | loss: 12.2403848CurrentTrain: epoch 15, batch    49 | loss: 11.0747842CurrentTrain: epoch 15, batch    50 | loss: 10.7056692CurrentTrain: epoch 15, batch    51 | loss: 14.6602711CurrentTrain: epoch 15, batch    52 | loss: 9.7579413CurrentTrain: epoch 15, batch    53 | loss: 9.8551953CurrentTrain: epoch 15, batch    54 | loss: 9.8637802CurrentTrain: epoch 15, batch    55 | loss: 11.5196982CurrentTrain: epoch 15, batch    56 | loss: 8.8858968CurrentTrain: epoch 15, batch    57 | loss: 14.3430580CurrentTrain: epoch 15, batch    58 | loss: 11.9253333CurrentTrain: epoch 15, batch    59 | loss: 10.8686502CurrentTrain: epoch 15, batch    60 | loss: 10.5476481CurrentTrain: epoch 15, batch    61 | loss: 14.4321225CurrentTrain: epoch  7, batch    62 | loss: 6.9454353CurrentTrain: epoch 15, batch     0 | loss: 8.9926509CurrentTrain: epoch 15, batch     1 | loss: 8.9938104CurrentTrain: epoch 15, batch     2 | loss: 13.6159864CurrentTrain: epoch 15, batch     3 | loss: 12.2249329CurrentTrain: epoch 15, batch     4 | loss: 11.5881560CurrentTrain: epoch 15, batch     5 | loss: 14.0744856CurrentTrain: epoch 15, batch     6 | loss: 10.0535404CurrentTrain: epoch 15, batch     7 | loss: 14.4182228CurrentTrain: epoch 15, batch     8 | loss: 13.6676081CurrentTrain: epoch 15, batch     9 | loss: 10.4177877CurrentTrain: epoch 15, batch    10 | loss: 10.2656447CurrentTrain: epoch 15, batch    11 | loss: 12.7323078CurrentTrain: epoch 15, batch    12 | loss: 13.9010884CurrentTrain: epoch 15, batch    13 | loss: 9.7582651CurrentTrain: epoch 15, batch    14 | loss: 15.3760318CurrentTrain: epoch 15, batch    15 | loss: 12.8522264CurrentTrain: epoch 15, batch    16 | loss: 16.4467906CurrentTrain: epoch 15, batch    17 | loss: 10.1207920CurrentTrain: epoch 15, batch    18 | loss: 14.6049417CurrentTrain: epoch 15, batch    19 | loss: 9.3989859CurrentTrain: epoch 15, batch    20 | loss: 15.6354419CurrentTrain: epoch 15, batch    21 | loss: 13.6150281CurrentTrain: epoch 15, batch    22 | loss: 9.2582050CurrentTrain: epoch 15, batch    23 | loss: 10.2466384CurrentTrain: epoch 15, batch    24 | loss: 10.5429905CurrentTrain: epoch 15, batch    25 | loss: 9.3636795CurrentTrain: epoch 15, batch    26 | loss: 21.2546596CurrentTrain: epoch 15, batch    27 | loss: 17.6256697CurrentTrain: epoch 15, batch    28 | loss: 13.7636146CurrentTrain: epoch 15, batch    29 | loss: 10.8429369CurrentTrain: epoch 15, batch    30 | loss: 9.1670690CurrentTrain: epoch 15, batch    31 | loss: 9.8099315CurrentTrain: epoch 15, batch    32 | loss: 13.4018717CurrentTrain: epoch 15, batch    33 | loss: 8.3897728CurrentTrain: epoch 15, batch    34 | loss: 12.1662555CurrentTrain: epoch 15, batch    35 | loss: 11.1793643CurrentTrain: epoch 15, batch    36 | loss: 10.6543504CurrentTrain: epoch 15, batch    37 | loss: 9.3810656CurrentTrain: epoch 15, batch    38 | loss: 12.5835257CurrentTrain: epoch 15, batch    39 | loss: 18.0732830CurrentTrain: epoch 15, batch    40 | loss: 14.8553332CurrentTrain: epoch 15, batch    41 | loss: 9.8403245CurrentTrain: epoch 15, batch    42 | loss: 12.5226448CurrentTrain: epoch 15, batch    43 | loss: 15.6030562CurrentTrain: epoch 15, batch    44 | loss: 13.6435550CurrentTrain: epoch 15, batch    45 | loss: 9.6004705CurrentTrain: epoch 15, batch    46 | loss: 11.5045392CurrentTrain: epoch 15, batch    47 | loss: 23.0770764CurrentTrain: epoch 15, batch    48 | loss: 9.1164704CurrentTrain: epoch 15, batch    49 | loss: 12.1387633CurrentTrain: epoch 15, batch    50 | loss: 17.4794434CurrentTrain: epoch 15, batch    51 | loss: 9.1456078CurrentTrain: epoch 15, batch    52 | loss: 8.0328677CurrentTrain: epoch 15, batch    53 | loss: 9.9803098CurrentTrain: epoch 15, batch    54 | loss: 9.6182070CurrentTrain: epoch 15, batch    55 | loss: 12.6820735CurrentTrain: epoch 15, batch    56 | loss: 9.6593371CurrentTrain: epoch 15, batch    57 | loss: 15.7411165CurrentTrain: epoch 15, batch    58 | loss: 12.8378340CurrentTrain: epoch 15, batch    59 | loss: 7.5903745CurrentTrain: epoch 15, batch    60 | loss: 13.2903267CurrentTrain: epoch 15, batch    61 | loss: 18.1755710CurrentTrain: epoch  7, batch    62 | loss: 9.4387778CurrentTrain: epoch 15, batch     0 | loss: 10.7443257CurrentTrain: epoch 15, batch     1 | loss: 26.7190554CurrentTrain: epoch 15, batch     2 | loss: 21.8293637CurrentTrain: epoch 15, batch     3 | loss: 23.8043031CurrentTrain: epoch 15, batch     4 | loss: 12.8747179CurrentTrain: epoch 15, batch     5 | loss: 9.1621619CurrentTrain: epoch 15, batch     6 | loss: 17.0603242CurrentTrain: epoch 15, batch     7 | loss: 15.9046209CurrentTrain: epoch 15, batch     8 | loss: 11.6738174CurrentTrain: epoch 15, batch     9 | loss: 15.5077317CurrentTrain: epoch 15, batch    10 | loss: 13.7104596CurrentTrain: epoch 15, batch    11 | loss: 8.8714302CurrentTrain: epoch 15, batch    12 | loss: 8.5272869CurrentTrain: epoch 15, batch    13 | loss: 10.1718510CurrentTrain: epoch 15, batch    14 | loss: 7.0571785CurrentTrain: epoch 15, batch    15 | loss: 15.6141702CurrentTrain: epoch 15, batch    16 | loss: 21.8345590CurrentTrain: epoch 15, batch    17 | loss: 7.5689183CurrentTrain: epoch 15, batch    18 | loss: 9.0402427CurrentTrain: epoch 15, batch    19 | loss: 7.9322876CurrentTrain: epoch 15, batch    20 | loss: 11.1520560CurrentTrain: epoch 15, batch    21 | loss: 9.5212664CurrentTrain: epoch 15, batch    22 | loss: 12.5609554CurrentTrain: epoch 15, batch    23 | loss: 11.6641885CurrentTrain: epoch 15, batch    24 | loss: 15.8155440CurrentTrain: epoch 15, batch    25 | loss: 10.8260821CurrentTrain: epoch 15, batch    26 | loss: 9.9290076CurrentTrain: epoch 15, batch    27 | loss: 9.1128438CurrentTrain: epoch 15, batch    28 | loss: 9.9203172CurrentTrain: epoch 15, batch    29 | loss: 37.2000044CurrentTrain: epoch 15, batch    30 | loss: 15.3917470CurrentTrain: epoch 15, batch    31 | loss: 12.1071108CurrentTrain: epoch 15, batch    32 | loss: 9.3200293CurrentTrain: epoch 15, batch    33 | loss: 15.5479106CurrentTrain: epoch 15, batch    34 | loss: 12.7259834CurrentTrain: epoch 15, batch    35 | loss: 10.7699805CurrentTrain: epoch 15, batch    36 | loss: 9.1897083CurrentTrain: epoch 15, batch    37 | loss: 7.7570699CurrentTrain: epoch 15, batch    38 | loss: 11.3561477CurrentTrain: epoch 15, batch    39 | loss: 8.8701338CurrentTrain: epoch 15, batch    40 | loss: 14.4301572CurrentTrain: epoch 15, batch    41 | loss: 11.9385788CurrentTrain: epoch 15, batch    42 | loss: 14.7717861CurrentTrain: epoch 15, batch    43 | loss: 14.6187479CurrentTrain: epoch 15, batch    44 | loss: 20.4518589CurrentTrain: epoch 15, batch    45 | loss: 9.3326757CurrentTrain: epoch 15, batch    46 | loss: 8.8763008CurrentTrain: epoch 15, batch    47 | loss: 11.1198273CurrentTrain: epoch 15, batch    48 | loss: 15.6819205CurrentTrain: epoch 15, batch    49 | loss: 9.9203067CurrentTrain: epoch 15, batch    50 | loss: 10.3360436CurrentTrain: epoch 15, batch    51 | loss: 15.2083174CurrentTrain: epoch 15, batch    52 | loss: 22.4633897CurrentTrain: epoch 15, batch    53 | loss: 9.7854831CurrentTrain: epoch 15, batch    54 | loss: 7.4227860CurrentTrain: epoch 15, batch    55 | loss: 30.8759462CurrentTrain: epoch 15, batch    56 | loss: 9.8002056CurrentTrain: epoch 15, batch    57 | loss: 12.3792381CurrentTrain: epoch 15, batch    58 | loss: 11.4385807CurrentTrain: epoch 15, batch    59 | loss: 12.7965188CurrentTrain: epoch 15, batch    60 | loss: 11.5876499CurrentTrain: epoch 15, batch    61 | loss: 8.3741622CurrentTrain: epoch  7, batch    62 | loss: 7.6419740CurrentTrain: epoch 15, batch     0 | loss: 10.1000026CurrentTrain: epoch 15, batch     1 | loss: 19.6565041CurrentTrain: epoch 15, batch     2 | loss: 10.4678212CurrentTrain: epoch 15, batch     3 | loss: 10.0137579CurrentTrain: epoch 15, batch     4 | loss: 8.7315865CurrentTrain: epoch 15, batch     5 | loss: 9.1978230CurrentTrain: epoch 15, batch     6 | loss: 8.8122103CurrentTrain: epoch 15, batch     7 | loss: 12.5175188CurrentTrain: epoch 15, batch     8 | loss: 9.2042175CurrentTrain: epoch 15, batch     9 | loss: 9.6911556CurrentTrain: epoch 15, batch    10 | loss: 20.0386788CurrentTrain: epoch 15, batch    11 | loss: 8.7567078CurrentTrain: epoch 15, batch    12 | loss: 10.0001785CurrentTrain: epoch 15, batch    13 | loss: 15.5640592CurrentTrain: epoch 15, batch    14 | loss: 11.2733791CurrentTrain: epoch 15, batch    15 | loss: 10.0233174CurrentTrain: epoch 15, batch    16 | loss: 13.7882476CurrentTrain: epoch 15, batch    17 | loss: 12.3785465CurrentTrain: epoch 15, batch    18 | loss: 15.7677543CurrentTrain: epoch 15, batch    19 | loss: 10.0187199CurrentTrain: epoch 15, batch    20 | loss: 10.8271137CurrentTrain: epoch 15, batch    21 | loss: 9.8770820CurrentTrain: epoch 15, batch    22 | loss: 6.0365377CurrentTrain: epoch 15, batch    23 | loss: 9.3281276CurrentTrain: epoch 15, batch    24 | loss: 11.7442646CurrentTrain: epoch 15, batch    25 | loss: 12.7835921CurrentTrain: epoch 15, batch    26 | loss: 6.7295357CurrentTrain: epoch 15, batch    27 | loss: 14.2611662CurrentTrain: epoch 15, batch    28 | loss: 11.0595406CurrentTrain: epoch 15, batch    29 | loss: 9.0826733CurrentTrain: epoch 15, batch    30 | loss: 8.1825731CurrentTrain: epoch 15, batch    31 | loss: 7.4852782CurrentTrain: epoch 15, batch    32 | loss: 9.2958074CurrentTrain: epoch 15, batch    33 | loss: 9.7693165CurrentTrain: epoch 15, batch    34 | loss: 13.5598602CurrentTrain: epoch 15, batch    35 | loss: 13.1550817CurrentTrain: epoch 15, batch    36 | loss: 8.5038086CurrentTrain: epoch 15, batch    37 | loss: 13.9661831CurrentTrain: epoch 15, batch    38 | loss: 9.3762319CurrentTrain: epoch 15, batch    39 | loss: 11.0090895CurrentTrain: epoch 15, batch    40 | loss: 14.8701104CurrentTrain: epoch 15, batch    41 | loss: 14.2005638CurrentTrain: epoch 15, batch    42 | loss: 16.5434645CurrentTrain: epoch 15, batch    43 | loss: 12.4247675CurrentTrain: epoch 15, batch    44 | loss: 16.0306929CurrentTrain: epoch 15, batch    45 | loss: 10.1959251CurrentTrain: epoch 15, batch    46 | loss: 8.2961693CurrentTrain: epoch 15, batch    47 | loss: 12.0670377CurrentTrain: epoch 15, batch    48 | loss: 9.0089644CurrentTrain: epoch 15, batch    49 | loss: 7.2888786CurrentTrain: epoch 15, batch    50 | loss: 9.0379035CurrentTrain: epoch 15, batch    51 | loss: 19.4784728CurrentTrain: epoch 15, batch    52 | loss: 9.5768874CurrentTrain: epoch 15, batch    53 | loss: 14.1560478CurrentTrain: epoch 15, batch    54 | loss: 8.7268904CurrentTrain: epoch 15, batch    55 | loss: 10.4802518CurrentTrain: epoch 15, batch    56 | loss: 9.0194202CurrentTrain: epoch 15, batch    57 | loss: 9.9208142CurrentTrain: epoch 15, batch    58 | loss: 14.0148716CurrentTrain: epoch 15, batch    59 | loss: 9.4952796CurrentTrain: epoch 15, batch    60 | loss: 14.7746294CurrentTrain: epoch 15, batch    61 | loss: 9.4389850CurrentTrain: epoch  7, batch    62 | loss: 11.5736849CurrentTrain: epoch 15, batch     0 | loss: 12.3752045CurrentTrain: epoch 15, batch     1 | loss: 24.3040562CurrentTrain: epoch 15, batch     2 | loss: 10.3385253CurrentTrain: epoch 15, batch     3 | loss: 10.5429483CurrentTrain: epoch 15, batch     4 | loss: 9.5484068CurrentTrain: epoch 15, batch     5 | loss: 14.6918597CurrentTrain: epoch 15, batch     6 | loss: 13.2724582CurrentTrain: epoch 15, batch     7 | loss: 9.7973270CurrentTrain: epoch 15, batch     8 | loss: 9.5075242CurrentTrain: epoch 15, batch     9 | loss: 9.4543104CurrentTrain: epoch 15, batch    10 | loss: 13.9158079CurrentTrain: epoch 15, batch    11 | loss: 19.8595110CurrentTrain: epoch 15, batch    12 | loss: 9.3645814CurrentTrain: epoch 15, batch    13 | loss: 9.8908464CurrentTrain: epoch 15, batch    14 | loss: 9.5588737CurrentTrain: epoch 15, batch    15 | loss: 11.4444097CurrentTrain: epoch 15, batch    16 | loss: 11.2678359CurrentTrain: epoch 15, batch    17 | loss: 11.9558248CurrentTrain: epoch 15, batch    18 | loss: 10.3704876CurrentTrain: epoch 15, batch    19 | loss: 10.6893028CurrentTrain: epoch 15, batch    20 | loss: 11.3121922CurrentTrain: epoch 15, batch    21 | loss: 9.0866300CurrentTrain: epoch 15, batch    22 | loss: 14.3343281CurrentTrain: epoch 15, batch    23 | loss: 10.6452399CurrentTrain: epoch 15, batch    24 | loss: 8.8456050CurrentTrain: epoch 15, batch    25 | loss: 9.9685030CurrentTrain: epoch 15, batch    26 | loss: 9.0613280CurrentTrain: epoch 15, batch    27 | loss: 7.2457609CurrentTrain: epoch 15, batch    28 | loss: 11.4174744CurrentTrain: epoch 15, batch    29 | loss: 10.5070833CurrentTrain: epoch 15, batch    30 | loss: 11.6663098CurrentTrain: epoch 15, batch    31 | loss: 8.6572262CurrentTrain: epoch 15, batch    32 | loss: 16.5934783CurrentTrain: epoch 15, batch    33 | loss: 7.9406764CurrentTrain: epoch 15, batch    34 | loss: 8.7374202CurrentTrain: epoch 15, batch    35 | loss: 8.5132624CurrentTrain: epoch 15, batch    36 | loss: 22.2674391CurrentTrain: epoch 15, batch    37 | loss: 8.9816443CurrentTrain: epoch 15, batch    38 | loss: 12.9794655CurrentTrain: epoch 15, batch    39 | loss: 10.4006195CurrentTrain: epoch 15, batch    40 | loss: 12.3093704CurrentTrain: epoch 15, batch    41 | loss: 13.7473811CurrentTrain: epoch 15, batch    42 | loss: 11.4705708CurrentTrain: epoch 15, batch    43 | loss: 12.7633217CurrentTrain: epoch 15, batch    44 | loss: 15.0571124CurrentTrain: epoch 15, batch    45 | loss: 9.7227928CurrentTrain: epoch 15, batch    46 | loss: 17.3987094CurrentTrain: epoch 15, batch    47 | loss: 13.6224347CurrentTrain: epoch 15, batch    48 | loss: 14.8749097CurrentTrain: epoch 15, batch    49 | loss: 11.9516374CurrentTrain: epoch 15, batch    50 | loss: 7.9641385CurrentTrain: epoch 15, batch    51 | loss: 24.9866058CurrentTrain: epoch 15, batch    52 | loss: 9.4011520CurrentTrain: epoch 15, batch    53 | loss: 9.0772169CurrentTrain: epoch 15, batch    54 | loss: 17.8142660CurrentTrain: epoch 15, batch    55 | loss: 14.7235624CurrentTrain: epoch 15, batch    56 | loss: 17.2825742CurrentTrain: epoch 15, batch    57 | loss: 10.9785735CurrentTrain: epoch 15, batch    58 | loss: 14.1396679CurrentTrain: epoch 15, batch    59 | loss: 8.4288209CurrentTrain: epoch 15, batch    60 | loss: 12.6404791CurrentTrain: epoch 15, batch    61 | loss: 9.1888807CurrentTrain: epoch  7, batch    62 | loss: 10.4067572CurrentTrain: epoch 15, batch     0 | loss: 10.3887051CurrentTrain: epoch 15, batch     1 | loss: 13.7872650CurrentTrain: epoch 15, batch     2 | loss: 8.1299635CurrentTrain: epoch 15, batch     3 | loss: 10.9712310CurrentTrain: epoch 15, batch     4 | loss: 8.7715862CurrentTrain: epoch 15, batch     5 | loss: 9.0671744CurrentTrain: epoch 15, batch     6 | loss: 13.7688803CurrentTrain: epoch 15, batch     7 | loss: 7.2036999CurrentTrain: epoch 15, batch     8 | loss: 9.6342479CurrentTrain: epoch 15, batch     9 | loss: 12.0502157CurrentTrain: epoch 15, batch    10 | loss: 9.6229923CurrentTrain: epoch 15, batch    11 | loss: 13.1177058CurrentTrain: epoch 15, batch    12 | loss: 10.6818510CurrentTrain: epoch 15, batch    13 | loss: 10.2379689CurrentTrain: epoch 15, batch    14 | loss: 8.2604596CurrentTrain: epoch 15, batch    15 | loss: 7.9550815CurrentTrain: epoch 15, batch    16 | loss: 12.8458294CurrentTrain: epoch 15, batch    17 | loss: 20.2773039CurrentTrain: epoch 15, batch    18 | loss: 8.2338072CurrentTrain: epoch 15, batch    19 | loss: 24.2037297CurrentTrain: epoch 15, batch    20 | loss: 9.0582976CurrentTrain: epoch 15, batch    21 | loss: 10.6019527CurrentTrain: epoch 15, batch    22 | loss: 20.8441008CurrentTrain: epoch 15, batch    23 | loss: 8.8725584CurrentTrain: epoch 15, batch    24 | loss: 7.6477774CurrentTrain: epoch 15, batch    25 | loss: 13.6184320CurrentTrain: epoch 15, batch    26 | loss: 8.9762172CurrentTrain: epoch 15, batch    27 | loss: 9.9992582CurrentTrain: epoch 15, batch    28 | loss: 8.3380965CurrentTrain: epoch 15, batch    29 | loss: 14.1804156CurrentTrain: epoch 15, batch    30 | loss: 8.2833641CurrentTrain: epoch 15, batch    31 | loss: 10.7490682CurrentTrain: epoch 15, batch    32 | loss: 13.1144816CurrentTrain: epoch 15, batch    33 | loss: 9.3844849CurrentTrain: epoch 15, batch    34 | loss: 13.9982552CurrentTrain: epoch 15, batch    35 | loss: 8.1025580CurrentTrain: epoch 15, batch    36 | loss: 12.3232149CurrentTrain: epoch 15, batch    37 | loss: 12.8549961CurrentTrain: epoch 15, batch    38 | loss: 7.1663029CurrentTrain: epoch 15, batch    39 | loss: 21.0431391CurrentTrain: epoch 15, batch    40 | loss: 9.7181153CurrentTrain: epoch 15, batch    41 | loss: 9.0258725CurrentTrain: epoch 15, batch    42 | loss: 9.8037245CurrentTrain: epoch 15, batch    43 | loss: 32.3862645CurrentTrain: epoch 15, batch    44 | loss: 10.9564552CurrentTrain: epoch 15, batch    45 | loss: 18.8271451CurrentTrain: epoch 15, batch    46 | loss: 11.1650072CurrentTrain: epoch 15, batch    47 | loss: 7.9272787CurrentTrain: epoch 15, batch    48 | loss: 18.0929291CurrentTrain: epoch 15, batch    49 | loss: 19.3615221CurrentTrain: epoch 15, batch    50 | loss: 16.2755129CurrentTrain: epoch 15, batch    51 | loss: 7.4562056CurrentTrain: epoch 15, batch    52 | loss: 8.7428818CurrentTrain: epoch 15, batch    53 | loss: 9.9978378CurrentTrain: epoch 15, batch    54 | loss: 15.2921817CurrentTrain: epoch 15, batch    55 | loss: 10.1245627CurrentTrain: epoch 15, batch    56 | loss: 10.2576090CurrentTrain: epoch 15, batch    57 | loss: 12.2341215CurrentTrain: epoch 15, batch    58 | loss: 9.1810381CurrentTrain: epoch 15, batch    59 | loss: 15.4733815CurrentTrain: epoch 15, batch    60 | loss: 12.3521861CurrentTrain: epoch 15, batch    61 | loss: 13.6952173CurrentTrain: epoch  7, batch    62 | loss: 13.7637496CurrentTrain: epoch 15, batch     0 | loss: 6.6896407CurrentTrain: epoch 15, batch     1 | loss: 12.8637753CurrentTrain: epoch 15, batch     2 | loss: 15.4794335CurrentTrain: epoch 15, batch     3 | loss: 8.2626857CurrentTrain: epoch 15, batch     4 | loss: 9.6920573CurrentTrain: epoch 15, batch     5 | loss: 9.5304076CurrentTrain: epoch 15, batch     6 | loss: 14.2320186CurrentTrain: epoch 15, batch     7 | loss: 15.1194627CurrentTrain: epoch 15, batch     8 | loss: 12.1292502CurrentTrain: epoch 15, batch     9 | loss: 6.8219772CurrentTrain: epoch 15, batch    10 | loss: 9.3939208CurrentTrain: epoch 15, batch    11 | loss: 8.4326466CurrentTrain: epoch 15, batch    12 | loss: 7.7387935CurrentTrain: epoch 15, batch    13 | loss: 8.8254099CurrentTrain: epoch 15, batch    14 | loss: 8.1959560CurrentTrain: epoch 15, batch    15 | loss: 8.8231429CurrentTrain: epoch 15, batch    16 | loss: 12.6388104CurrentTrain: epoch 15, batch    17 | loss: 7.7888708CurrentTrain: epoch 15, batch    18 | loss: 8.6922963CurrentTrain: epoch 15, batch    19 | loss: 8.7881021CurrentTrain: epoch 15, batch    20 | loss: 8.3062815CurrentTrain: epoch 15, batch    21 | loss: 20.5008494CurrentTrain: epoch 15, batch    22 | loss: 9.7614821CurrentTrain: epoch 15, batch    23 | loss: 22.7932812CurrentTrain: epoch 15, batch    24 | loss: 10.3911797CurrentTrain: epoch 15, batch    25 | loss: 9.6152488CurrentTrain: epoch 15, batch    26 | loss: 16.2248483CurrentTrain: epoch 15, batch    27 | loss: 10.5481093CurrentTrain: epoch 15, batch    28 | loss: 10.0407773CurrentTrain: epoch 15, batch    29 | loss: 10.3959516CurrentTrain: epoch 15, batch    30 | loss: 10.4666399CurrentTrain: epoch 15, batch    31 | loss: 22.0146138CurrentTrain: epoch 15, batch    32 | loss: 11.3285424CurrentTrain: epoch 15, batch    33 | loss: 8.9418303CurrentTrain: epoch 15, batch    34 | loss: 9.6236154CurrentTrain: epoch 15, batch    35 | loss: 9.2637366CurrentTrain: epoch 15, batch    36 | loss: 11.9010568CurrentTrain: epoch 15, batch    37 | loss: 12.4753268CurrentTrain: epoch 15, batch    38 | loss: 8.9149579CurrentTrain: epoch 15, batch    39 | loss: 8.8274498CurrentTrain: epoch 15, batch    40 | loss: 14.2170414CurrentTrain: epoch 15, batch    41 | loss: 13.8945794CurrentTrain: epoch 15, batch    42 | loss: 13.3957133CurrentTrain: epoch 15, batch    43 | loss: 8.0157538CurrentTrain: epoch 15, batch    44 | loss: 7.2329207CurrentTrain: epoch 15, batch    45 | loss: 14.9082388CurrentTrain: epoch 15, batch    46 | loss: 8.2700899CurrentTrain: epoch 15, batch    47 | loss: 8.1562990CurrentTrain: epoch 15, batch    48 | loss: 10.0742695CurrentTrain: epoch 15, batch    49 | loss: 8.5091197CurrentTrain: epoch 15, batch    50 | loss: 12.4463181CurrentTrain: epoch 15, batch    51 | loss: 9.7965945CurrentTrain: epoch 15, batch    52 | loss: 12.8889395CurrentTrain: epoch 15, batch    53 | loss: 11.9281900CurrentTrain: epoch 15, batch    54 | loss: 8.5200351CurrentTrain: epoch 15, batch    55 | loss: 9.0469724CurrentTrain: epoch 15, batch    56 | loss: 8.3549251CurrentTrain: epoch 15, batch    57 | loss: 7.9694205CurrentTrain: epoch 15, batch    58 | loss: 11.5491327CurrentTrain: epoch 15, batch    59 | loss: 10.7275949CurrentTrain: epoch 15, batch    60 | loss: 17.2255766CurrentTrain: epoch 15, batch    61 | loss: 8.4376182CurrentTrain: epoch  7, batch    62 | loss: 6.2511821
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: the large workshops of peter paul rubens and jacob jordaens , and the influence of anthony van dyck , made antwerp the center of the flemish baroque .
Head Entity: jacob jordaens
Tail Entity: flemish baroque
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: The emergence of surrealism in the early 20th century was significantly influenced by the works of Salvador Dalí and André Breton, who sought to challenge conventional perceptions of reality.  
Head Entity: Salvador Dalí  
Tail Entity: surrealism  

Relation: movement  
Context: The rise of the Impressionist movement in France was marked by the innovative techniques of artists like Claude Monet and Pierre-Auguste Renoir, who focused on capturing light and its changing qualities.  
Head Entity: Claude Monet  
Tail Entity: Impressionist movement  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: lyons hill is a restored village , and former parish with church , now part of the community of ardclough in north county kildare .
Head Entity: ardclough
Tail Entity: county kildare
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: The city of San Francisco is known for its iconic Golden Gate Bridge and is part of the larger San Francisco County in California.  
Head Entity: San Francisco  
Tail Entity: San Francisco County  

Relation: located in the administrative territorial entity  
Context: The village of Haverfordwest serves as the county town of Pembrokeshire, situated in the picturesque region of Wales.  
Head Entity: Haverfordwest  
Tail Entity: Pembrokeshire  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the nation as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: in october 1913 , jessie fauset wrote to joel spingarn , who was deeply involved with the naacp , explaining lee ’s situation .
Head Entity: joel spingarn
Tail Entity: naacp
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: during her time at the university, sarah joined the debate team, where she honed her skills and made lasting friendships with fellow members.  
Head Entity: sarah  
Tail Entity: debate team  

Relation: member of  
Context: after years of dedication to the sport, michael was proud to be recognized as a member of the national soccer league, representing his hometown.  
Head Entity: michael  
Tail Entity: national soccer league  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: the belleville turnpike bridge ( also known as the rutgers street bridge ) crosses the passaic river , connecting belleville to north arlington .
Head Entity: belleville turnpike bridge
Tail Entity: passaic river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: the golden gate bridge crosses the golden gate strait, linking san francisco to marin county.  
Head Entity: golden gate bridge  
Tail Entity: golden gate strait  

Relation: crosses  
Context: the london bridge crosses the river thames, providing a route between the city of london and southwark.  
Head Entity: london bridge  
Tail Entity: river thames  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: it is on the panj river just downstream from the mouth of the bartang river at the point where the panj briefly turns west before resuming its northerly course .
Head Entity: panj river
Tail Entity: bartang river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The Missouri River, which is a major tributary of the Mississippi River, flows through several states before joining the larger watercourse.  
Head Entity: Missouri River  
Tail Entity: Mississippi River  

Relation: tributary  
Context: The Rio Grande serves as a natural border between the United States and Mexico, and it has several tributaries, including the Pecos River.  
Head Entity: Rio Grande  
Tail Entity: Pecos River  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: on 3 july 1937 , the count married rocafort - atuzarrra in a lavish wedding in havana , cuba attended by the president of cuba , federico laredo brú .
Head Entity: federico laredo brú
Tail Entity: president of cuba
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: In 2020, Kamala Harris was elected as the Vice President of the United States, making history as the first woman and first person of South Asian descent to hold the office.  
Head Entity: Kamala Harris  
Tail Entity: Vice President of the United States  

Relation: position held  
Context: Before becoming the Prime Minister, Boris Johnson served as the Mayor of London from 2008 to 2016, where he implemented several key policies.  
Head Entity: Boris Johnson  
Tail Entity: Mayor of London  
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: " jedi outcast " was developed by raven software and powered by the i d tech 3 game engine .
Head Entity: jedi outcast
Tail Entity: raven software
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: "The Witcher 3: Wild Hunt" was developed by CD Projekt Red and has received numerous awards for its storytelling and gameplay.  
Head Entity: The Witcher 3: Wild Hunt  
Tail Entity: CD Projekt Red  

Relation: developer  
Context: "Minecraft" was developed by Mojang Studios and has become one of the best-selling video games of all time.  
Head Entity: Minecraft  
Tail Entity: Mojang Studios  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: third power was an american psychedelic hard rock band formed in 1969 in detroit , michigan .
Head Entity: third power
Tail Entity: detroit
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the united nations was established in 1945 in san francisco, california, to promote international cooperation.  
Head Entity: united nations  
Tail Entity: san francisco  

Relation: location of formation  
Context: the famous rock band the beatles originated in liverpool, england, in the early 1960s.  
Head Entity: the beatles  
Tail Entity: liverpool  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: she has appeared in some american television shows , beginning with her regular role as snow white 's stepmother , evil queen lillian " lily " white in the series " the charmings " .
Head Entity: the charmings
Tail Entity: american
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous dish sushi is traditionally made with vinegared rice and often includes seafood, vegetables, and occasionally tropical fruits, originating from Japan.  
Head Entity: sushi  
Tail Entity: Japan  

Relation: country of origin  
Context: The iconic painting "Mona Lisa," known for its captivating smile, was created by the renowned artist Leonardo da Vinci during the Italian Renaissance in Italy.  
Head Entity: Mona Lisa  
Tail Entity: Italy  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.32%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.57%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.68%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.61%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.96%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.40%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.61%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.67%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.98%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 96.08%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.16%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.33%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.28%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 96.35%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.43%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.45%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 96.27%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 96.23%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.30%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 96.02%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.98%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 96.01%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 96.08%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 96.15%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 96.21%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 96.17%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.44%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.32%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.57%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.68%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.61%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.96%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.40%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.61%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.67%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.98%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 96.08%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.16%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.33%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.28%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 96.35%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.43%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.45%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 96.27%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 96.23%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.30%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 96.02%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.98%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 96.01%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 96.08%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 96.15%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 96.21%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 96.17%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.44%   
cur_acc:  ['0.9544']
his_acc:  ['0.9544']
CurrentTrain: epoch 15, batch     0 | loss: 14.2267788CurrentTrain: epoch 15, batch     1 | loss: 15.2171340CurrentTrain: epoch 15, batch     2 | loss: 16.6442721CurrentTrain: epoch  1, batch     3 | loss: 13.0161842CurrentTrain: epoch 15, batch     0 | loss: 12.6064292CurrentTrain: epoch 15, batch     1 | loss: 12.6126704CurrentTrain: epoch 15, batch     2 | loss: 10.7879690CurrentTrain: epoch  1, batch     3 | loss: 17.0946907CurrentTrain: epoch 15, batch     0 | loss: 24.1038118CurrentTrain: epoch 15, batch     1 | loss: 23.3031035CurrentTrain: epoch 15, batch     2 | loss: 20.8043158CurrentTrain: epoch  1, batch     3 | loss: 10.7529913CurrentTrain: epoch 15, batch     0 | loss: 15.8952576CurrentTrain: epoch 15, batch     1 | loss: 9.4964274CurrentTrain: epoch 15, batch     2 | loss: 15.5590779CurrentTrain: epoch  1, batch     3 | loss: 6.8447499CurrentTrain: epoch 15, batch     0 | loss: 9.5628832CurrentTrain: epoch 15, batch     1 | loss: 8.0611485CurrentTrain: epoch 15, batch     2 | loss: 11.9139266CurrentTrain: epoch  1, batch     3 | loss: 13.1632263CurrentTrain: epoch 15, batch     0 | loss: 12.1057183CurrentTrain: epoch 15, batch     1 | loss: 15.1489359CurrentTrain: epoch 15, batch     2 | loss: 8.4849648CurrentTrain: epoch  1, batch     3 | loss: 8.9106867CurrentTrain: epoch 15, batch     0 | loss: 12.8173834CurrentTrain: epoch 15, batch     1 | loss: 13.9960598CurrentTrain: epoch 15, batch     2 | loss: 9.4594041CurrentTrain: epoch  1, batch     3 | loss: 9.2083763CurrentTrain: epoch 15, batch     0 | loss: 9.1829931CurrentTrain: epoch 15, batch     1 | loss: 11.2865045CurrentTrain: epoch 15, batch     2 | loss: 12.1959372CurrentTrain: epoch  1, batch     3 | loss: 5.9962992CurrentTrain: epoch 15, batch     0 | loss: 12.1895986CurrentTrain: epoch 15, batch     1 | loss: 14.9262111CurrentTrain: epoch 15, batch     2 | loss: 10.4022969CurrentTrain: epoch  1, batch     3 | loss: 6.3221954CurrentTrain: epoch 15, batch     0 | loss: 7.8431670CurrentTrain: epoch 15, batch     1 | loss: 9.9635387CurrentTrain: epoch 15, batch     2 | loss: 12.9910077CurrentTrain: epoch  1, batch     3 | loss: 6.6559118
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: in 2004 the catalan government gave him the george cross .
Head Entity: george cross
Tail Entity: catalan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The new environmental regulations introduced by the European Union aim to reduce carbon emissions across member states.  
Head Entity: European Union  
Tail Entity: member states  

Relation: applies to jurisdiction  
Context: The Supreme Court's ruling clarified that federal laws take precedence over state laws in matters of immigration.  
Head Entity: federal laws  
Tail Entity: state laws  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: " tenth avenue freeze - out " is the second song on bruce springsteen 's breakthrough album " born to run " , released in 1975 .
Head Entity: tenth avenue freeze - out
Tail Entity: bruce springsteen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: "Shape of You" is a popular song by Ed Sheeran that topped charts worldwide after its release in 2017.  
Head Entity: Shape of You  
Tail Entity: Ed Sheeran  

Relation: performer  
Context: The Beatles are widely regarded as one of the most influential bands in the history of music, with hits like "Hey Jude" and "Let It Be."  
Head Entity: The Beatles  
Tail Entity: Hey Jude  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the next full - frame digital slrs were the canon eos-1ds of late 2002 , followed by kodak 's dcs pro 14n in 2003 .
Head Entity: dcs pro 14n
Tail Entity: kodak
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The latest smartphone model, the Galaxy S21, was released by Samsung in January 2021, showcasing advanced camera technology and performance.  
Head Entity: Galaxy S21  
Tail Entity: Samsung  

Relation: manufacturer  
Context: The iconic Model T was produced by Ford Motor Company, revolutionizing the automotive industry in the early 20th century.  
Head Entity: Model T  
Tail Entity: Ford Motor Company  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: in contrast to gwladys and roger 's allegiance to the house of lancaster , their three sons were staunch yorkists during the wars of the roses .
Head Entity: wars of the roses
Tail Entity: house of lancaster
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: During the annual tech conference, several startups showcased their innovative products, while industry leaders discussed the future of technology.  
Head Entity: tech conference  
Tail Entity: startups  

Relation: participant  
Context: The local community organized a charity run to raise funds for the homeless, with many volunteers helping to make the event a success.  
Head Entity: charity run  
Tail Entity: volunteers  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The application is compatible with various operating systems, including macOS, Windows, and Linux, ensuring a wide user base.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: This game runs smoothly on multiple operating systems, such as Ubuntu, Windows 10, and Fedora, providing flexibility for gamers.  
Head Entity: game  
Tail Entity: Ubuntu  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in july 2007 , griffin lost by a technical knockout to light heavyweight champion danny green of the world boxing association at challenge stadium in perth , australia .
Head Entity: danny green
Tail Entity: light heavyweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: during the 2020 olympics, the men's 100 meters final showcased athletes from various competition classes, with usain bolt representing the sprinting class.  
Head Entity: usain bolt  
Tail Entity: sprinting  

Relation: competition class  
Context: the formula one race in monaco is known for its high-speed competition class, where lewis hamilton competed fiercely against other top drivers.  
Head Entity: lewis hamilton  
Tail Entity: formula one
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: the qup gained 3 seats from its previous election result , and won a 10 % swing to it in primary votes , but failed to defeat the incumbent labor government led by ted theodore .
Head Entity: previous election
Tail Entity: ted theodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: After a closely contested campaign, the voters chose Sarah Johnson as their representative, marking a significant shift in the political landscape of the district.  
Head Entity: closely contested campaign  
Tail Entity: Sarah Johnson  

Relation: successful candidate  
Context: In the recent mayoral election, the community rallied behind Mark Thompson, who promised to bring new initiatives to the city, ultimately leading to his victory.  
Head Entity: recent mayoral election  
Tail Entity: Mark Thompson  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: jenő szervánszky is the father of the pianist , valéria szervánszky and the brother of the composer , endre szervánszky and violinist , péter szervánszky .
Head Entity: péter szervánszky
Tail Entity: endre szervánszky
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: emma and oliver are both children of the same parents, making them siblings. they often play together and share a close bond.  
Head Entity: emma  
Tail Entity: oliver  

Relation: sibling  
Context: during the family reunion, it was clear that both lucas and mia inherited their parents' artistic talents, as they are siblings who excel in painting and music.  
Head Entity: lucas  
Tail Entity: mia  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: iain harrison is a competitive shooter and former british army captain .
Head Entity: iain harrison
Tail Entity: british army
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: sergeant johnson served in the united states marine corps during his military career.  
Head Entity: sergeant johnson  
Tail Entity: united states marine corps  

Relation: military branch  
Context: general smith was a prominent figure in the royal air force, leading several key missions.  
Head Entity: general smith  
Tail Entity: royal air force  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: ziad rahbani is the son of the lebanese famous composer assi rahbani and nouhad haddad , the famous lebanese female singer known as fairuz .
Head Entity: assi rahbani
Tail Entity: ziad rahbani
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: in the animated series, the character of simon is the son of the adventurous couple, marie and tom.  
Head Entity: marie  
Tail Entity: simon  

Relation: child  
Context: the renowned physicist albert einstein had a daughter named lieserl, who was born in 1902.  
Head Entity: albert einstein  
Tail Entity: lieserl  
MemoryTrain:  epoch 15, batch     0 | loss: 8.5184223MemoryTrain:  epoch 15, batch     1 | loss: 6.6655911MemoryTrain:  epoch 15, batch     2 | loss: 7.8679002MemoryTrain:  epoch 11, batch     3 | loss: 7.5828280MemoryTrain:  epoch 15, batch     0 | loss: 7.5442464MemoryTrain:  epoch 15, batch     1 | loss: 6.0542497MemoryTrain:  epoch 15, batch     2 | loss: 6.3288913MemoryTrain:  epoch 11, batch     3 | loss: 5.2629465MemoryTrain:  epoch 15, batch     0 | loss: 3.4615896MemoryTrain:  epoch 15, batch     1 | loss: 5.9610339MemoryTrain:  epoch 15, batch     2 | loss: 3.2577107MemoryTrain:  epoch 11, batch     3 | loss: 4.0569720MemoryTrain:  epoch 15, batch     0 | loss: 3.3613867MemoryTrain:  epoch 15, batch     1 | loss: 6.4777397MemoryTrain:  epoch 15, batch     2 | loss: 4.1254615MemoryTrain:  epoch 11, batch     3 | loss: 2.2546545MemoryTrain:  epoch 15, batch     0 | loss: 2.4355707MemoryTrain:  epoch 15, batch     1 | loss: 4.1699239MemoryTrain:  epoch 15, batch     2 | loss: 4.0059654MemoryTrain:  epoch 11, batch     3 | loss: 2.2313641MemoryTrain:  epoch 15, batch     0 | loss: 3.1865958MemoryTrain:  epoch 15, batch     1 | loss: 3.5625650MemoryTrain:  epoch 15, batch     2 | loss: 3.7146380MemoryTrain:  epoch 11, batch     3 | loss: 6.6881652MemoryTrain:  epoch 15, batch     0 | loss: 3.0757586MemoryTrain:  epoch 15, batch     1 | loss: 5.2021073MemoryTrain:  epoch 15, batch     2 | loss: 3.3899850MemoryTrain:  epoch 11, batch     3 | loss: 4.5098598MemoryTrain:  epoch 15, batch     0 | loss: 10.4847040MemoryTrain:  epoch 15, batch     1 | loss: 2.7622110MemoryTrain:  epoch 15, batch     2 | loss: 3.3092019MemoryTrain:  epoch 11, batch     3 | loss: 2.2432147MemoryTrain:  epoch 15, batch     0 | loss: 2.8563219MemoryTrain:  epoch 15, batch     1 | loss: 7.0151938MemoryTrain:  epoch 15, batch     2 | loss: 4.5176320MemoryTrain:  epoch 11, batch     3 | loss: 2.9241203MemoryTrain:  epoch 15, batch     0 | loss: 3.4719901MemoryTrain:  epoch 15, batch     1 | loss: 4.1187368MemoryTrain:  epoch 15, batch     2 | loss: 3.2131723MemoryTrain:  epoch 11, batch     3 | loss: 3.4436570
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 56.25%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 57.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 59.38%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 63.39%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 67.36%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 68.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 69.32%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 70.31%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 72.60%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 73.66%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 74.17%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 75.39%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 76.84%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 79.28%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 78.44%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 78.57%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 79.26%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 79.08%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 78.91%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 79.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.05%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 80.79%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 81.03%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 81.68%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.86%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 83.01%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 84.01%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 84.46%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 84.90%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 85.30%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 85.69%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 85.90%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 85.67%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 86.01%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 86.19%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 86.22%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 85.42%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 84.10%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 83.24%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 82.94%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 82.27%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 81.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 81.99%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 82.21%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 82.43%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 82.52%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 82.73%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 82.92%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 83.00%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 83.08%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 82.94%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 82.71%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 82.89%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 82.66%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 82.04%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 91.54%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 92.26%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 91.03%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 90.89%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 90.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.87%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 91.20%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.52%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 91.59%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.14%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.38%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.61%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 92.83%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 93.06%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.24%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 93.42%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.59%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.90%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 94.04%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 94.03%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 94.03%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 94.02%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 93.75%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 93.49%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.62%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 93.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.26%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 93.15%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 93.16%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.29%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 93.07%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.08%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 93.09%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 93.10%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 93.22%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 93.23%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 93.34%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 93.35%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 92.86%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 92.58%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 92.12%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 91.67%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 91.04%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 90.72%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 90.40%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 90.36%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 90.32%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 89.93%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 89.81%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 89.78%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 89.75%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 89.80%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 89.77%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 89.66%   [EVAL] batch:   78 | acc: 100.00%,  total acc: 89.79%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 89.92%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 90.05%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 90.02%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 89.68%   [EVAL] batch:   83 | acc: 93.75%,  total acc: 89.73%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 89.56%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 89.53%   [EVAL] batch:   86 | acc: 75.00%,  total acc: 89.37%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 89.42%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 89.54%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 89.65%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 89.63%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 89.74%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 89.85%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 89.83%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 89.93%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 90.04%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 90.14%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 90.24%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 90.34%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 90.44%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 90.47%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 90.50%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 90.41%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 90.38%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 90.48%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 90.51%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 90.25%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 89.76%   [EVAL] batch:  108 | acc: 43.75%,  total acc: 89.33%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 88.86%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 88.63%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 88.28%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 88.27%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 88.27%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 88.37%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 88.36%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 88.41%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 88.45%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 88.50%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 88.44%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 88.43%   [EVAL] batch:  121 | acc: 62.50%,  total acc: 88.22%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 88.16%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 88.10%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 88.05%   
cur_acc:  ['0.9544', '0.8204']
his_acc:  ['0.9544', '0.8805']
CurrentTrain: epoch 15, batch     0 | loss: 18.5578753CurrentTrain: epoch 15, batch     1 | loss: 21.1179310CurrentTrain: epoch 15, batch     2 | loss: 14.7956118CurrentTrain: epoch  1, batch     3 | loss: 12.1701770CurrentTrain: epoch 15, batch     0 | loss: 12.3812513CurrentTrain: epoch 15, batch     1 | loss: 13.7014364CurrentTrain: epoch 15, batch     2 | loss: 16.6306623CurrentTrain: epoch  1, batch     3 | loss: 6.8452419CurrentTrain: epoch 15, batch     0 | loss: 16.4134357CurrentTrain: epoch 15, batch     1 | loss: 10.4373740CurrentTrain: epoch 15, batch     2 | loss: 10.6719646CurrentTrain: epoch  1, batch     3 | loss: 8.9808822CurrentTrain: epoch 15, batch     0 | loss: 21.4100917CurrentTrain: epoch 15, batch     1 | loss: 21.7845564CurrentTrain: epoch 15, batch     2 | loss: 9.9726623CurrentTrain: epoch  1, batch     3 | loss: 8.1130068CurrentTrain: epoch 15, batch     0 | loss: 8.3454836CurrentTrain: epoch 15, batch     1 | loss: 12.6006378CurrentTrain: epoch 15, batch     2 | loss: 15.0605158CurrentTrain: epoch  1, batch     3 | loss: 9.1719324CurrentTrain: epoch 15, batch     0 | loss: 9.1632965CurrentTrain: epoch 15, batch     1 | loss: 7.9908951CurrentTrain: epoch 15, batch     2 | loss: 10.5611193CurrentTrain: epoch  1, batch     3 | loss: 8.5499670CurrentTrain: epoch 15, batch     0 | loss: 10.2109380CurrentTrain: epoch 15, batch     1 | loss: 11.7893067CurrentTrain: epoch 15, batch     2 | loss: 17.4462746CurrentTrain: epoch  1, batch     3 | loss: 8.1570810CurrentTrain: epoch 15, batch     0 | loss: 9.9890788CurrentTrain: epoch 15, batch     1 | loss: 11.8591379CurrentTrain: epoch 15, batch     2 | loss: 13.7383781CurrentTrain: epoch  1, batch     3 | loss: 7.9103416CurrentTrain: epoch 15, batch     0 | loss: 8.1781765CurrentTrain: epoch 15, batch     1 | loss: 10.5283761CurrentTrain: epoch 15, batch     2 | loss: 9.3808454CurrentTrain: epoch  1, batch     3 | loss: 6.3971259CurrentTrain: epoch 15, batch     0 | loss: 10.2591627CurrentTrain: epoch 15, batch     1 | loss: 6.0620639CurrentTrain: epoch 15, batch     2 | loss: 8.9321244CurrentTrain: epoch  1, batch     3 | loss: 8.7612183
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: for more information on the grand mosque seizure itself , see " the siege of mecca " by yaroslav trofimov .
Head Entity: the siege of mecca
Tail Entity: grand mosque seizure
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: in her book "the power of habit," charles duhigg explores the science behind why habits exist and how they can be changed.  
Head Entity: the power of habit  
Tail Entity: science of habits  

Relation: main subject  
Context: "sapiens: a brief history of humankind" by yuval noah harari provides a compelling overview of the history and impact of our species.  
Head Entity: sapiens: a brief history of humankind  
Tail Entity: history of humankind  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: he represented romania at euro 2000 , his first major tournament where they beat england 3–2 , and drew 1–1 against germany .
Head Entity: euro 2000
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: The Brazilian national team showcased their skills at the 2014 FIFA World Cup, where they faced off against Germany in the semi-finals.  
Head Entity: 2014 FIFA World Cup  
Tail Entity: Germany  

Relation: participating team  
Context: During the 2021 Tokyo Olympics, the United States women's soccer team competed fiercely, ultimately playing against Canada in the semi-finals.  
Head Entity: 2021 Tokyo Olympics  
Tail Entity: Canada  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: among the competing artists was previous eurovision song contest entrant robin juhkental who represented estonia in 2010 as part of the group malcolm lincoln .
Head Entity: malcolm lincoln
Tail Entity: robin juhkental
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The famous painting "The Last Supper" features several apostles, including Judas Iscariot, who is depicted as part of the group gathered around Jesus.  
Head Entity: The Last Supper  
Tail Entity: Judas Iscariot  

Relation: has part  
Context: The smartphone model Galaxy S21 includes various components, such as the Exynos 2100 processor, which is an integral part of its performance.  
Head Entity: Galaxy S21  
Tail Entity: Exynos 2100 processor  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: for her performance in the musical - drama " crazy heart " ( 2009 ) , she was nominated for the academy award for best supporting actress .
Head Entity: crazy heart
Tail Entity: academy award for best supporting actress
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film "The Shape of Water" received critical acclaim and was nominated for several prestigious awards, including the Academy Award for Best Picture.  
Head Entity: The Shape of Water  
Tail Entity: Academy Award for Best Picture  

Relation: nominated for  
Context: In 2021, the song "Fight for You" from the film "Judas and the Black Messiah" was nominated for the Grammy Award for Best Song Written for Visual Media.  
Head Entity: Fight for You  
Tail Entity: Grammy Award for Best Song Written for Visual Media  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: on march 2015 , cube entertainment launched the multi - national girl group clc including original members seunghee , yujin , seungyeon , sorn and yeeun .
Head Entity: clc
Tail Entity: girl group
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone model Galaxy S21 was released by Samsung in January 2021, showcasing advanced technology and features.  
Head Entity: Galaxy S21  
Tail Entity: smartphone model  

Relation: instance of  
Context: The famous painting Starry Night was created by Vincent van Gogh in 1889 and is considered a masterpiece of post-impressionism.  
Head Entity: Starry Night  
Tail Entity: painting  
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: brand peak is a sharp snow - covered peak located east - southeast of the eternity range and northwest of mount duemler , in palmer land .
Head Entity: mount duemler
Tail Entity: eternity range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the rocky mountains extend from Canada through the United States and into Mexico, forming a major mountain range in North America.  
Head Entity: rocky mountains  
Tail Entity: North America  

Relation: mountain range  
Context: the appalachian mountains are one of the oldest mountain ranges in the world, stretching from Canada down to Alabama.  
Head Entity: appalachian mountains  
Tail Entity: Alabama  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: it would also be the last episode to feature a grounding , until " the marge - ian chronicles " in season 27 , six years later ( also written by brian kelley ) .
Head Entity: the marge - ian chronicles
Tail Entity: brian kelley
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: The film "Inception" was a groundbreaking project that showcased the visionary talents of its creator, with the screenplay crafted by the brilliant Christopher Nolan.  
Head Entity: Inception  
Tail Entity: Christopher Nolan  

Relation: screenwriter  
Context: The beloved animated feature "Toy Story" was brought to life through the imaginative script penned by the talented Joss Whedon.  
Head Entity: Toy Story  
Tail Entity: Joss Whedon  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: includes definitions from " american heritage dictionary of the english language " , " collins english dictionary : complete and unabridged " , " american heritage science dictionary " , and " wordnet 3.0 " .
Head Entity: wordnet
Tail Entity: english language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The novel "Cien años de soledad" is a masterpiece of Latin American literature, originally written in Spanish by Gabriel García Márquez.  
Head Entity: Cien años de soledad  
Tail Entity: Spanish  

Relation: language of work or name  
Context: The popular animated series "Avatar: The Last Airbender" features dialogue primarily in English, appealing to a wide audience across various cultures.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: the aircraft was a boeing 707 - 321c that first entered service with pan american world airways ( pan am ) in 1963 .
Head Entity: boeing 707 - 321c
Tail Entity: pan american world airways
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: the new electric bus fleet is managed by the city transportation authority, which ensures efficient public transit services.  
Head Entity: electric bus fleet  
Tail Entity: city transportation authority  

Relation: operator  
Context: the state-of-the-art research facility is operated by the national science foundation, focusing on advanced materials and nanotechnology.  
Head Entity: research facility  
Tail Entity: national science foundation  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: the most important lama of this series was the third changkya , rolpai dorje , who was preceptor to the qianlong emperor of china , and chief representative of tibetan buddhism at the qing court .
Head Entity: rolpai dorje
Tail Entity: tibetan buddhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the cathedral of notre-dame is a famous example of french gothic architecture and serves as the seat of the archbishop of paris, representing the roman catholic faith in the heart of the city.  
Head Entity: cathedral of notre-dame  
Tail Entity: roman catholic faith  

Relation: religion  
Context: the dalai lama is a prominent figure in the world, known for his teachings on compassion and non-violence, and he is the spiritual leader of tibetan buddhism.  
Head Entity: dalai lama  
Tail Entity: tibetan buddhism  
MemoryTrain:  epoch 15, batch     0 | loss: 5.3329720MemoryTrain:  epoch 15, batch     1 | loss: 5.1191352MemoryTrain:  epoch 15, batch     2 | loss: 4.8844664MemoryTrain:  epoch 15, batch     3 | loss: 7.6270084MemoryTrain:  epoch 15, batch     4 | loss: 4.7971302MemoryTrain:  epoch  9, batch     5 | loss: 2.5046950MemoryTrain:  epoch 15, batch     0 | loss: 3.8300090MemoryTrain:  epoch 15, batch     1 | loss: 4.9435998MemoryTrain:  epoch 15, batch     2 | loss: 3.9902439MemoryTrain:  epoch 15, batch     3 | loss: 4.0422491MemoryTrain:  epoch 15, batch     4 | loss: 5.1008457MemoryTrain:  epoch  9, batch     5 | loss: 5.5576777MemoryTrain:  epoch 15, batch     0 | loss: 2.4799458MemoryTrain:  epoch 15, batch     1 | loss: 5.5766522MemoryTrain:  epoch 15, batch     2 | loss: 3.2467752MemoryTrain:  epoch 15, batch     3 | loss: 6.7617573MemoryTrain:  epoch 15, batch     4 | loss: 5.1504987MemoryTrain:  epoch  9, batch     5 | loss: 5.3618571MemoryTrain:  epoch 15, batch     0 | loss: 3.0322711MemoryTrain:  epoch 15, batch     1 | loss: 4.7588535MemoryTrain:  epoch 15, batch     2 | loss: 2.6506341MemoryTrain:  epoch 15, batch     3 | loss: 4.5711181MemoryTrain:  epoch 15, batch     4 | loss: 2.0104563MemoryTrain:  epoch  9, batch     5 | loss: 5.0741804MemoryTrain:  epoch 15, batch     0 | loss: 4.5688080MemoryTrain:  epoch 15, batch     1 | loss: 7.5804590MemoryTrain:  epoch 15, batch     2 | loss: 2.4745806MemoryTrain:  epoch 15, batch     3 | loss: 1.9211017MemoryTrain:  epoch 15, batch     4 | loss: 4.5003570MemoryTrain:  epoch  9, batch     5 | loss: 2.9167612MemoryTrain:  epoch 15, batch     0 | loss: 5.7271706MemoryTrain:  epoch 15, batch     1 | loss: 1.9091617MemoryTrain:  epoch 15, batch     2 | loss: 2.1981981MemoryTrain:  epoch 15, batch     3 | loss: 2.7763963MemoryTrain:  epoch 15, batch     4 | loss: 3.2661890MemoryTrain:  epoch  9, batch     5 | loss: 2.5663380MemoryTrain:  epoch 15, batch     0 | loss: 2.8416884MemoryTrain:  epoch 15, batch     1 | loss: 3.2002800MemoryTrain:  epoch 15, batch     2 | loss: 4.5070392MemoryTrain:  epoch 15, batch     3 | loss: 2.3605381MemoryTrain:  epoch 15, batch     4 | loss: 4.0477199MemoryTrain:  epoch  9, batch     5 | loss: 5.3924113MemoryTrain:  epoch 15, batch     0 | loss: 2.0008851MemoryTrain:  epoch 15, batch     1 | loss: 6.6551944MemoryTrain:  epoch 15, batch     2 | loss: 1.8652907MemoryTrain:  epoch 15, batch     3 | loss: 4.9316741MemoryTrain:  epoch 15, batch     4 | loss: 2.3240353MemoryTrain:  epoch  9, batch     5 | loss: 4.7977856MemoryTrain:  epoch 15, batch     0 | loss: 2.2295483MemoryTrain:  epoch 15, batch     1 | loss: 2.2269844MemoryTrain:  epoch 15, batch     2 | loss: 2.4690688MemoryTrain:  epoch 15, batch     3 | loss: 4.2121478MemoryTrain:  epoch 15, batch     4 | loss: 2.0354795MemoryTrain:  epoch  9, batch     5 | loss: 3.8835410MemoryTrain:  epoch 15, batch     0 | loss: 3.1384017MemoryTrain:  epoch 15, batch     1 | loss: 2.1642873MemoryTrain:  epoch 15, batch     2 | loss: 2.2793629MemoryTrain:  epoch 15, batch     3 | loss: 4.8212707MemoryTrain:  epoch 15, batch     4 | loss: 2.1212898MemoryTrain:  epoch  9, batch     5 | loss: 4.4957296
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 54.17%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 60.94%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 63.54%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 66.07%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 73.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 75.57%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 77.60%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 77.40%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 74.11%   [EVAL] batch:   14 | acc: 25.00%,  total acc: 70.83%   [EVAL] batch:   15 | acc: 31.25%,  total acc: 68.36%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 67.28%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 65.97%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 64.47%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 67.86%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 69.03%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 70.38%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 71.61%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 72.75%   [EVAL] batch:   25 | acc: 18.75%,  total acc: 70.67%   [EVAL] batch:   26 | acc: 25.00%,  total acc: 68.98%   [EVAL] batch:   27 | acc: 25.00%,  total acc: 67.41%   [EVAL] batch:   28 | acc: 56.25%,  total acc: 67.03%   [EVAL] batch:   29 | acc: 18.75%,  total acc: 65.42%   [EVAL] batch:   30 | acc: 18.75%,  total acc: 63.91%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 64.65%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 65.53%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 66.54%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 67.50%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 68.40%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 69.26%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 70.07%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 71.25%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 71.95%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 72.62%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 73.26%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 73.86%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 73.89%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 74.05%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 74.20%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 73.83%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 73.85%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 74.25%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 73.90%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 73.68%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 73.94%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 74.07%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 74.32%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 74.55%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 74.78%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 75.21%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 75.21%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 75.41%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 75.40%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 74.80%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 81.82%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 82.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 84.56%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 85.07%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 85.86%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 85.62%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 85.71%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 85.80%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 84.78%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 84.64%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 84.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.99%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.69%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 88.05%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 88.21%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.85%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 89.14%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.42%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.94%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.18%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 90.26%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 90.34%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 90.42%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 90.08%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 89.63%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 89.45%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.67%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 89.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.46%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.54%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 89.62%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 89.81%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 89.77%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 89.91%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 89.87%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 89.94%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 90.16%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 90.22%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 90.08%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 89.55%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 89.23%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 88.92%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 88.62%   [EVAL] batch:   67 | acc: 81.25%,  total acc: 88.51%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 88.22%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 88.04%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 87.85%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 87.50%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 87.42%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 87.25%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 87.34%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 87.26%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 87.03%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 87.11%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 87.19%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 86.51%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 85.77%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 85.12%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 84.12%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 83.43%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 82.61%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 82.24%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 82.44%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 82.64%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 82.76%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 82.95%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 83.13%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 83.18%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 83.36%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 83.53%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 83.70%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 83.86%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 84.03%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 84.19%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 84.28%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 84.16%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 84.13%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 84.23%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 84.32%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 84.05%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 83.62%   [EVAL] batch:  108 | acc: 43.75%,  total acc: 83.26%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 82.84%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 82.66%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 82.25%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 82.30%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 82.35%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 82.50%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 82.44%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 82.48%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 82.57%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 82.67%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 82.66%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 82.70%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 82.68%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 82.67%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 82.61%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 82.70%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 82.49%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 82.19%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 82.03%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 82.03%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 81.92%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 81.82%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 81.82%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 81.95%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 81.95%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 81.99%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 82.12%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 82.25%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 82.20%   [EVAL] batch:  138 | acc: 31.25%,  total acc: 81.83%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 81.43%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 81.07%   [EVAL] batch:  141 | acc: 50.00%,  total acc: 80.85%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 80.59%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 80.30%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 80.43%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 80.57%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 80.65%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 80.79%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 80.91%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 81.04%   [EVAL] batch:  150 | acc: 18.75%,  total acc: 80.63%   [EVAL] batch:  151 | acc: 25.00%,  total acc: 80.26%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 79.90%   [EVAL] batch:  153 | acc: 56.25%,  total acc: 79.75%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 79.35%   [EVAL] batch:  155 | acc: 18.75%,  total acc: 78.97%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 79.02%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 79.11%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 79.25%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 79.38%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 79.50%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 79.63%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 79.75%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 79.88%   [EVAL] batch:  164 | acc: 87.50%,  total acc: 79.92%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 80.05%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 80.16%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 80.28%   [EVAL] batch:  168 | acc: 100.00%,  total acc: 80.40%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 80.37%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 80.37%   [EVAL] batch:  171 | acc: 81.25%,  total acc: 80.38%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 80.24%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 80.21%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 80.29%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 80.15%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 80.05%   [EVAL] batch:  177 | acc: 87.50%,  total acc: 80.09%   [EVAL] batch:  178 | acc: 81.25%,  total acc: 80.10%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 80.14%   [EVAL] batch:  180 | acc: 87.50%,  total acc: 80.18%   [EVAL] batch:  181 | acc: 87.50%,  total acc: 80.22%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 80.26%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 80.30%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 80.27%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 80.31%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 80.28%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 80.05%   
cur_acc:  ['0.9544', '0.8204', '0.7480']
his_acc:  ['0.9544', '0.8805', '0.8005']
CurrentTrain: epoch 15, batch     0 | loss: 16.1223580CurrentTrain: epoch 15, batch     1 | loss: 11.2232472CurrentTrain: epoch 15, batch     2 | loss: 13.9949557CurrentTrain: epoch  1, batch     3 | loss: 9.9016357CurrentTrain: epoch 15, batch     0 | loss: 17.3403502CurrentTrain: epoch 15, batch     1 | loss: 11.6466434CurrentTrain: epoch 15, batch     2 | loss: 16.4751187CurrentTrain: epoch  1, batch     3 | loss: 8.7227491CurrentTrain: epoch 15, batch     0 | loss: 8.0051208CurrentTrain: epoch 15, batch     1 | loss: 10.8138942CurrentTrain: epoch 15, batch     2 | loss: 9.7313895CurrentTrain: epoch  1, batch     3 | loss: 13.6935713CurrentTrain: epoch 15, batch     0 | loss: 10.0883358CurrentTrain: epoch 15, batch     1 | loss: 6.8537253CurrentTrain: epoch 15, batch     2 | loss: 9.8155201CurrentTrain: epoch  1, batch     3 | loss: 11.2708979CurrentTrain: epoch 15, batch     0 | loss: 12.6668013CurrentTrain: epoch 15, batch     1 | loss: 12.4492989CurrentTrain: epoch 15, batch     2 | loss: 15.3962312CurrentTrain: epoch  1, batch     3 | loss: 7.0775469CurrentTrain: epoch 15, batch     0 | loss: 15.9586172CurrentTrain: epoch 15, batch     1 | loss: 10.2654018CurrentTrain: epoch 15, batch     2 | loss: 8.6069003CurrentTrain: epoch  1, batch     3 | loss: 6.0613864CurrentTrain: epoch 15, batch     0 | loss: 8.5271814CurrentTrain: epoch 15, batch     1 | loss: 11.1104835CurrentTrain: epoch 15, batch     2 | loss: 11.4068462CurrentTrain: epoch  1, batch     3 | loss: 7.8543605CurrentTrain: epoch 15, batch     0 | loss: 9.4742483CurrentTrain: epoch 15, batch     1 | loss: 8.5958484CurrentTrain: epoch 15, batch     2 | loss: 8.3078998CurrentTrain: epoch  1, batch     3 | loss: 6.2440656CurrentTrain: epoch 15, batch     0 | loss: 12.4164778CurrentTrain: epoch 15, batch     1 | loss: 8.9567433CurrentTrain: epoch 15, batch     2 | loss: 13.1707629CurrentTrain: epoch  1, batch     3 | loss: 6.3771663CurrentTrain: epoch 15, batch     0 | loss: 15.8963505CurrentTrain: epoch 15, batch     1 | loss: 7.5349321CurrentTrain: epoch 15, batch     2 | loss: 7.5856093CurrentTrain: epoch  1, batch     3 | loss: 17.1288078
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: ( 1988 ) , " night watch " ( 2004 ) , " day watch " ( 2006 ) and " legend № 17 " ( 2013 ) .
Head Entity: day watch
Tail Entity: night watch
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: ( 1995 ) , " The Phantom Menace " ( 1999 ) , " Attack of the Clones " ( 2002 ) and " Revenge of the Sith " ( 2005 ) .  
Head Entity: Attack of the Clones  
Tail Entity: The Phantom Menace  

Relation: follows  
Context: ( 2010 ) , " The Hunger Games " ( 2012 ) , " Catching Fire " ( 2013 ) and " Mockingjay " ( 2014 ) .  
Head Entity: Catching Fire  
Tail Entity: The Hunger Games  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: dick packer was a u.s. soccer center forward who was a member of the u.s. team at the 1956 summer olympics .
Head Entity: dick packer
Tail Entity: soccer
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: serena williams is a professional tennis player who has won numerous grand slam titles throughout her career.  
Head Entity: serena williams  
Tail Entity: tennis  

Relation: sport  
Context: lebron james is known for his exceptional skills in basketball and has played for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: basketball  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: gilman 's odd experiences seem to escalate as he dreams that he signs the " book of azathoth " under the commands of keziah , brown jenkin , and the infamous " black man . "
Head Entity: black man
Tail Entity: azathoth
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: In the ancient tales, it is said that Zeus, the king of the gods, had many children, including Hercules, who was known for his incredible strength and heroic deeds.  
Head Entity: Zeus  
Tail Entity: Hercules  

Relation: father  
Context: The story of the Targaryens is filled with intrigue, and Aegon the Conqueror is often remembered as the father of the dynasty that would rule Westeros for generations.  
Head Entity: Aegon the Conqueror  
Tail Entity: Targaryens  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: he also made a special appearance in kbs ' romantic comedy drama " fight for my way " .
Head Entity: fight for my way
Tail Entity: kbs
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: the popular series "breaking bad" was first aired on amc, gaining a massive following.  
Head Entity: breaking bad  
Tail Entity: amc  

Relation: original network  
Context: the hit show "stranger things" premiered on netflix and quickly became a cultural phenomenon.  
Head Entity: stranger things  
Tail Entity: netflix  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: the 1954 film , directed by kadri venkata reddy , " peddamanushulu " was honoured with the first president 's silver medal for best feature film in telugu .
Head Entity: peddamanushulu
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: the 1994 animated film "The Lion King," produced by Walt Disney, was originally created in English before being dubbed into several other languages.  
Head Entity: The Lion King  
Tail Entity: English  

Relation: original language of film or TV show  
Context: the critically acclaimed 2001 film "Amélie," directed by Jean-Pierre Jeunet, was filmed in French and has since gained a global audience.  
Head Entity: Amélie  
Tail Entity: French  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: the league champions for the fifth time in their history ( and the second season running ) were taunton town , but runners - up mangotsfield united took promotion to the southern league .
Head Entity: mangotsfield united
Tail Entity: southern league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: After an impressive season, the team secured a spot in the prestigious league, where they will compete against top clubs, including their rivals, the city wanderers.  
Head Entity: city wanderers  
Tail Entity: prestigious league  

Relation: league  
Context: The young athlete showcased his skills in the national league, where he played for the local favorites, the river city stars, before moving to a more competitive environment.  
Head Entity: river city stars  
Tail Entity: national league  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: she is supposedly the third and youngest child of seti i and tuya , and the younger sister of ramesses ii and tia .
Head Entity: tia
Tail Entity: tuya
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: Cleopatra was the daughter of Ptolemy XII and his wife, who was also her mother, Cleopatra VI.  
Head Entity: Cleopatra  
Tail Entity: Cleopatra VI  

Relation: mother  
Context: In Norse mythology, Loki is said to have fathered several children, including the goddess Hel, who is the daughter of the giantess Angerboda, her mother.  
Head Entity: Hel  
Tail Entity: Angerboda  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres. he is particularly celebrated for his interpretations of classical cello concertos.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: the type locality given is " fan - si - pan mountains , tonkin , indo - china " ( mount fansipan , sa pa district , nghệ an province , northwestern vietnam ) .
Head Entity: sa pa district
Tail Entity: vietnam
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the capital city of the country is located in the central region, known for its rich history and cultural heritage, particularly in the city of "hanoi" which is the heart of "vietnam".  
Head Entity: hanoi  
Tail Entity: vietnam  

Relation: country  
Context: the famous landmark "eiffel tower" is situated in the city of "paris", which is the capital of the beautiful country known for its art, fashion, and cuisine, "france".  
Head Entity: paris  
Tail Entity: france  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: hagrid suggests in " harry potter and the chamber of secrets " that " " they 're startin ' ter think the job 's jinxed .
Head Entity: harry potter and the chamber of secrets
Tail Entity: hagrid
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: In the animated series "Avatar: The Last Airbender," Aang, the last Airbender, teams up with Katara and Sokka to defeat the Fire Nation.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: Aang  

Relation: characters  
Context: The novel "Pride and Prejudice" features Elizabeth Bennet as she navigates societal expectations and her relationship with Mr. Darcy.  
Head Entity: Pride and Prejudice  
Tail Entity: Elizabeth Bennet  
MemoryTrain:  epoch 15, batch     0 | loss: 5.0664889MemoryTrain:  epoch 15, batch     1 | loss: 5.0986755MemoryTrain:  epoch 15, batch     2 | loss: 4.2114007MemoryTrain:  epoch 15, batch     3 | loss: 5.0544228MemoryTrain:  epoch 15, batch     4 | loss: 3.0662329MemoryTrain:  epoch 15, batch     5 | loss: 3.8040979MemoryTrain:  epoch 15, batch     6 | loss: 4.0866264MemoryTrain:  epoch  7, batch     7 | loss: 3.5402059MemoryTrain:  epoch 15, batch     0 | loss: 4.1333871MemoryTrain:  epoch 15, batch     1 | loss: 2.9565826MemoryTrain:  epoch 15, batch     2 | loss: 5.1773075MemoryTrain:  epoch 15, batch     3 | loss: 3.7530445MemoryTrain:  epoch 15, batch     4 | loss: 4.9861134MemoryTrain:  epoch 15, batch     5 | loss: 4.5940595MemoryTrain:  epoch 15, batch     6 | loss: 3.6822362MemoryTrain:  epoch  7, batch     7 | loss: 3.1913657MemoryTrain:  epoch 15, batch     0 | loss: 4.1671112MemoryTrain:  epoch 15, batch     1 | loss: 3.5584391MemoryTrain:  epoch 15, batch     2 | loss: 4.6786705MemoryTrain:  epoch 15, batch     3 | loss: 2.7150392MemoryTrain:  epoch 15, batch     4 | loss: 8.1590846MemoryTrain:  epoch 15, batch     5 | loss: 2.4353857MemoryTrain:  epoch 15, batch     6 | loss: 3.5815761MemoryTrain:  epoch  7, batch     7 | loss: 2.9791263MemoryTrain:  epoch 15, batch     0 | loss: 4.3931333MemoryTrain:  epoch 15, batch     1 | loss: 2.3171699MemoryTrain:  epoch 15, batch     2 | loss: 2.2013571MemoryTrain:  epoch 15, batch     3 | loss: 4.3060585MemoryTrain:  epoch 15, batch     4 | loss: 3.1904191MemoryTrain:  epoch 15, batch     5 | loss: 2.4936405MemoryTrain:  epoch 15, batch     6 | loss: 2.8364603MemoryTrain:  epoch  7, batch     7 | loss: 2.1026285MemoryTrain:  epoch 15, batch     0 | loss: 2.9366617MemoryTrain:  epoch 15, batch     1 | loss: 3.1961668MemoryTrain:  epoch 15, batch     2 | loss: 2.1375602MemoryTrain:  epoch 15, batch     3 | loss: 2.0894049MemoryTrain:  epoch 15, batch     4 | loss: 2.7946531MemoryTrain:  epoch 15, batch     5 | loss: 4.0616813MemoryTrain:  epoch 15, batch     6 | loss: 2.6896545MemoryTrain:  epoch  7, batch     7 | loss: 3.5138695MemoryTrain:  epoch 15, batch     0 | loss: 4.2621098MemoryTrain:  epoch 15, batch     1 | loss: 3.1232418MemoryTrain:  epoch 15, batch     2 | loss: 3.1653698MemoryTrain:  epoch 15, batch     3 | loss: 3.3190175MemoryTrain:  epoch 15, batch     4 | loss: 1.9295520MemoryTrain:  epoch 15, batch     5 | loss: 2.3141489MemoryTrain:  epoch 15, batch     6 | loss: 2.4174892MemoryTrain:  epoch  7, batch     7 | loss: 2.0712752MemoryTrain:  epoch 15, batch     0 | loss: 3.6729808MemoryTrain:  epoch 15, batch     1 | loss: 2.2577907MemoryTrain:  epoch 15, batch     2 | loss: 2.2572108MemoryTrain:  epoch 15, batch     3 | loss: 4.2277756MemoryTrain:  epoch 15, batch     4 | loss: 2.1629825MemoryTrain:  epoch 15, batch     5 | loss: 1.7668496MemoryTrain:  epoch 15, batch     6 | loss: 3.0331721MemoryTrain:  epoch  7, batch     7 | loss: 1.4899775MemoryTrain:  epoch 15, batch     0 | loss: 2.2478788MemoryTrain:  epoch 15, batch     1 | loss: 2.8988971MemoryTrain:  epoch 15, batch     2 | loss: 1.6414109MemoryTrain:  epoch 15, batch     3 | loss: 2.3613475MemoryTrain:  epoch 15, batch     4 | loss: 1.4310856MemoryTrain:  epoch 15, batch     5 | loss: 2.1888114MemoryTrain:  epoch 15, batch     6 | loss: 2.1561743MemoryTrain:  epoch  7, batch     7 | loss: 1.5884399MemoryTrain:  epoch 15, batch     0 | loss: 1.9477067MemoryTrain:  epoch 15, batch     1 | loss: 4.3091911MemoryTrain:  epoch 15, batch     2 | loss: 1.6597265MemoryTrain:  epoch 15, batch     3 | loss: 3.2475307MemoryTrain:  epoch 15, batch     4 | loss: 2.8902767MemoryTrain:  epoch 15, batch     5 | loss: 2.1889451MemoryTrain:  epoch 15, batch     6 | loss: 1.9319114MemoryTrain:  epoch  7, batch     7 | loss: 1.4753475MemoryTrain:  epoch 15, batch     0 | loss: 1.6910806MemoryTrain:  epoch 15, batch     1 | loss: 1.7591316MemoryTrain:  epoch 15, batch     2 | loss: 2.1684719MemoryTrain:  epoch 15, batch     3 | loss: 1.5785548MemoryTrain:  epoch 15, batch     4 | loss: 4.1101665MemoryTrain:  epoch 15, batch     5 | loss: 1.6243490MemoryTrain:  epoch 15, batch     6 | loss: 3.0671775MemoryTrain:  epoch  7, batch     7 | loss: 1.4263876
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 25.00%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 23.75%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 25.00%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 30.36%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 35.16%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 38.89%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 43.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 46.59%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 48.96%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 48.56%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 46.43%   [EVAL] batch:   14 | acc: 25.00%,  total acc: 45.00%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 43.36%   [EVAL] batch:   16 | acc: 31.25%,  total acc: 42.65%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 42.71%   [EVAL] batch:   18 | acc: 43.75%,  total acc: 42.76%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 45.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 48.21%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 50.28%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 52.45%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 54.43%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 57.69%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 58.80%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 59.82%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 60.99%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 61.88%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 62.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 65.15%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 65.99%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 66.79%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 67.53%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 68.24%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 68.59%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 68.28%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 68.14%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 67.71%   [EVAL] batch:   42 | acc: 37.50%,  total acc: 67.01%   [EVAL] batch:   43 | acc: 43.75%,  total acc: 66.48%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 67.80%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 68.48%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 69.77%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 70.38%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 70.10%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 69.95%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 69.69%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 69.79%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 69.77%   [EVAL] batch:   55 | acc: 25.00%,  total acc: 68.97%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 68.86%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 68.86%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 68.54%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 69.06%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 69.46%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 69.05%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 80.11%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 79.81%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 80.80%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 81.67%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 82.42%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 83.09%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 84.03%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 84.87%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 83.15%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 83.07%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 82.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.17%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.80%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 84.70%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.69%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.13%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.55%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 86.95%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 87.14%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 87.84%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.16%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.46%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.02%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 89.29%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 89.39%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 89.49%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 89.31%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 88.86%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 88.16%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 88.02%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.27%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 88.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 88.11%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 88.22%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 88.33%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 88.52%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 88.62%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 88.49%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 88.04%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 87.82%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 87.71%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 87.70%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 86.82%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 86.54%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 86.17%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 85.54%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 85.39%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 84.87%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 84.73%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 84.68%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 84.46%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 84.42%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 84.25%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 84.13%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 84.09%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 83.89%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 83.62%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 83.80%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 83.08%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 82.30%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 81.47%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 80.51%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 79.65%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 78.88%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 78.55%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 79.03%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 79.19%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 79.42%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 79.64%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 79.72%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 79.93%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 80.14%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 80.35%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 80.55%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 80.74%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 80.94%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 81.06%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 81.19%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 81.01%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 81.01%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 81.13%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 80.96%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 80.27%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 79.64%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 78.98%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 78.60%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 78.12%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 78.10%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 78.18%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 78.37%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 78.39%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 78.53%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 78.65%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 78.62%   [EVAL] batch:  119 | acc: 18.75%,  total acc: 78.12%   [EVAL] batch:  120 | acc: 31.25%,  total acc: 77.74%   [EVAL] batch:  121 | acc: 18.75%,  total acc: 77.25%   [EVAL] batch:  122 | acc: 25.00%,  total acc: 76.83%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 76.26%   [EVAL] batch:  124 | acc: 25.00%,  total acc: 75.85%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 75.50%   [EVAL] batch:  126 | acc: 18.75%,  total acc: 75.05%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 74.90%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 74.95%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 74.86%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 74.76%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 74.76%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 74.81%   [EVAL] batch:  133 | acc: 62.50%,  total acc: 74.72%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 74.77%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 74.86%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 74.95%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 74.86%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 74.42%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 74.06%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 73.76%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 73.50%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 73.30%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 73.05%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 73.23%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 73.42%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 73.55%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 73.73%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 73.91%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 74.08%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 73.63%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 73.15%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 72.71%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 72.36%   [EVAL] batch:  154 | acc: 25.00%,  total acc: 72.06%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 71.63%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 71.66%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 71.80%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 71.97%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 72.15%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 72.32%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 72.49%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 72.75%   [EVAL] batch:  164 | acc: 93.75%,  total acc: 72.88%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 73.04%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 73.20%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 73.36%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 73.45%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 73.38%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 73.39%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 73.40%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 73.23%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 73.17%   [EVAL] batch:  174 | acc: 75.00%,  total acc: 73.18%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 73.12%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 73.09%   [EVAL] batch:  177 | acc: 81.25%,  total acc: 73.14%   [EVAL] batch:  178 | acc: 87.50%,  total acc: 73.22%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 73.30%   [EVAL] batch:  180 | acc: 87.50%,  total acc: 73.38%   [EVAL] batch:  181 | acc: 87.50%,  total acc: 73.45%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 73.53%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 73.64%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 73.65%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 73.72%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 73.73%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 73.67%   [EVAL] batch:  188 | acc: 25.00%,  total acc: 73.41%   [EVAL] batch:  189 | acc: 18.75%,  total acc: 73.12%   [EVAL] batch:  190 | acc: 12.50%,  total acc: 72.81%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 72.56%   [EVAL] batch:  192 | acc: 37.50%,  total acc: 72.38%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 72.20%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 72.21%   [EVAL] batch:  195 | acc: 56.25%,  total acc: 72.13%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 72.18%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 72.22%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 72.24%   [EVAL] batch:  199 | acc: 68.75%,  total acc: 72.22%   [EVAL] batch:  200 | acc: 31.25%,  total acc: 72.01%   [EVAL] batch:  201 | acc: 6.25%,  total acc: 71.69%   [EVAL] batch:  202 | acc: 18.75%,  total acc: 71.43%   [EVAL] batch:  203 | acc: 31.25%,  total acc: 71.23%   [EVAL] batch:  204 | acc: 43.75%,  total acc: 71.10%   [EVAL] batch:  205 | acc: 25.00%,  total acc: 70.87%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 70.95%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 71.23%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 71.34%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 71.48%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 71.61%   [EVAL] batch:  212 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:  213 | acc: 81.25%,  total acc: 71.79%   [EVAL] batch:  214 | acc: 87.50%,  total acc: 71.86%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 71.99%   [EVAL] batch:  216 | acc: 81.25%,  total acc: 72.03%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 72.16%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 72.26%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 72.39%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 72.51%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 72.58%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 72.67%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 72.77%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 72.89%   [EVAL] batch:  225 | acc: 62.50%,  total acc: 72.84%   [EVAL] batch:  226 | acc: 68.75%,  total acc: 72.82%   [EVAL] batch:  227 | acc: 56.25%,  total acc: 72.75%   [EVAL] batch:  228 | acc: 43.75%,  total acc: 72.63%   [EVAL] batch:  229 | acc: 56.25%,  total acc: 72.55%   [EVAL] batch:  230 | acc: 25.00%,  total acc: 72.35%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 72.41%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 72.51%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 72.62%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 72.74%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 72.85%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 72.97%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 72.98%   [EVAL] batch:  238 | acc: 62.50%,  total acc: 72.93%   [EVAL] batch:  239 | acc: 50.00%,  total acc: 72.84%   [EVAL] batch:  240 | acc: 68.75%,  total acc: 72.82%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 72.88%   [EVAL] batch:  242 | acc: 43.75%,  total acc: 72.76%   [EVAL] batch:  243 | acc: 31.25%,  total acc: 72.59%   [EVAL] batch:  244 | acc: 81.25%,  total acc: 72.63%   [EVAL] batch:  245 | acc: 56.25%,  total acc: 72.56%   [EVAL] batch:  246 | acc: 56.25%,  total acc: 72.49%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 72.53%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 72.64%   [EVAL] batch:  249 | acc: 87.50%,  total acc: 72.70%   
cur_acc:  ['0.9544', '0.8204', '0.7480', '0.6905']
his_acc:  ['0.9544', '0.8805', '0.8005', '0.7270']
CurrentTrain: epoch 15, batch     0 | loss: 13.7225627CurrentTrain: epoch 15, batch     1 | loss: 14.2150609CurrentTrain: epoch 15, batch     2 | loss: 16.1838203CurrentTrain: epoch  1, batch     3 | loss: 12.1239336CurrentTrain: epoch 15, batch     0 | loss: 19.5164131CurrentTrain: epoch 15, batch     1 | loss: 23.0708302CurrentTrain: epoch 15, batch     2 | loss: 12.7480447CurrentTrain: epoch  1, batch     3 | loss: 11.6260673CurrentTrain: epoch 15, batch     0 | loss: 11.5834189CurrentTrain: epoch 15, batch     1 | loss: 11.6025256CurrentTrain: epoch 15, batch     2 | loss: 14.0822710CurrentTrain: epoch  1, batch     3 | loss: 8.6880714CurrentTrain: epoch 15, batch     0 | loss: 10.4006506CurrentTrain: epoch 15, batch     1 | loss: 16.9337832CurrentTrain: epoch 15, batch     2 | loss: 20.7367293CurrentTrain: epoch  1, batch     3 | loss: 9.9247784CurrentTrain: epoch 15, batch     0 | loss: 9.1138882CurrentTrain: epoch 15, batch     1 | loss: 12.5887690CurrentTrain: epoch 15, batch     2 | loss: 9.4570635CurrentTrain: epoch  1, batch     3 | loss: 10.0971362CurrentTrain: epoch 15, batch     0 | loss: 9.7862910CurrentTrain: epoch 15, batch     1 | loss: 6.4969229CurrentTrain: epoch 15, batch     2 | loss: 15.4521447CurrentTrain: epoch  1, batch     3 | loss: 5.9404419CurrentTrain: epoch 15, batch     0 | loss: 8.2776623CurrentTrain: epoch 15, batch     1 | loss: 8.7973460CurrentTrain: epoch 15, batch     2 | loss: 7.0635042CurrentTrain: epoch  1, batch     3 | loss: 7.9267822CurrentTrain: epoch 15, batch     0 | loss: 9.0292695CurrentTrain: epoch 15, batch     1 | loss: 10.4403237CurrentTrain: epoch 15, batch     2 | loss: 15.2600393CurrentTrain: epoch  1, batch     3 | loss: 7.6912919CurrentTrain: epoch 15, batch     0 | loss: 10.6572361CurrentTrain: epoch 15, batch     1 | loss: 17.2161082CurrentTrain: epoch 15, batch     2 | loss: 6.3178355CurrentTrain: epoch  1, batch     3 | loss: 7.2583059CurrentTrain: epoch 15, batch     0 | loss: 8.0179081CurrentTrain: epoch 15, batch     1 | loss: 7.6201640CurrentTrain: epoch 15, batch     2 | loss: 8.8034629CurrentTrain: epoch  1, batch     3 | loss: 7.1292244
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: it is situated in the historic lower lusatia region , about south of beeskow , and north of cottbus .
Head Entity: cottbus
Tail Entity: lower lusatia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The ancient ruins are found near the banks of the river Nile, surrounded by vast desert landscapes.  
Head Entity: Nile  
Tail Entity: desert  

Relation: located on terrain feature  
Context: The village is nestled in the foothills of the Rocky Mountains, offering stunning views of the surrounding peaks.  
Head Entity: village  
Tail Entity: Rocky Mountains  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, held in paris in 2015, saw numerous world leaders and activists come together to discuss global warming and its impacts.  
Head Entity: united nations climate change conference  
Tail Entity: paris agreement  

Relation: participant of  
Context: the 2022 fifa world cup in qatar featured teams from around the globe competing for the prestigious trophy, with fans cheering for their favorite nations.  
Head Entity: 2022 fifa world cup  
Tail Entity: qatar
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: luna subsequently had a supporting role in the comedy film " gravy " , directed by james roday , which was released in october 2015 .
Head Entity: gravy
Tail Entity: james roday
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: the acclaimed drama series "the crown" was brought to life by the visionary director peter morgan, who skillfully navigated the complexities of the royal family.  
Head Entity: the crown  
Tail Entity: peter morgan  

Relation: director  
Context: in the thrilling action movie "inception," the brilliant director christopher nolan crafted a mind-bending narrative that captivated audiences worldwide.  
Head Entity: inception  
Tail Entity: christopher nolan  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: the station is owned by the hearst television subsidiary of the hearst corporation .
Head Entity: hearst television
Tail Entity: hearst corporation
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: the famous painting is owned by the metropolitan museum of art in new york.  
Head Entity: metropolitan museum of art  
Tail Entity: famous painting  

Relation: owned by  
Context: the luxury car brand is owned by a major automotive conglomerate based in germany.  
Head Entity: major automotive conglomerate  
Tail Entity: luxury car brand  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: thessaly was home to extensive neolithic and chalcolithic cultures around 6000 bc–2500 bc ( see cardium pottery , dimini and sesklo ) .
Head Entity: cardium pottery
Tail Entity: neolithic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The engine is a crucial part of the car's overall functionality, providing the necessary power to drive.  
Head Entity: engine  
Tail Entity: car  

Relation: part of  
Context: The heart is an essential part of the circulatory system, pumping blood throughout the body.  
Head Entity: heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: the brewery closed in 1996 and is now the site of the halliwell jones stadium , home of warrington wolves , as well as a tesco store .
Head Entity: halliwell jones stadium
Tail Entity: warrington wolves
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was transformed into a cultural center, which now hosts the local art gallery and serves as the headquarters for the community theater group.  
Head Entity: cultural center  
Tail Entity: community theater group  

Relation: occupant  
Context: After the renovation, the old factory became a vibrant co-working space, attracting freelancers and startups from the tech industry.  
Head Entity: co-working space  
Tail Entity: tech industry
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: he also commissioned renowned architect richard neutra to design the iconic desert kaufmann house ( 1946 ) in palm springs , california .
Head Entity: kaufmann house
Tail Entity: richard neutra
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: the innovative design of the new city hall was created by the famous architect frank gehry, known for his unique and modern architectural style.  
Head Entity: city hall  
Tail Entity: frank gehry  

Relation: architect  
Context: after years of planning, the museum was finally built under the guidance of architect zaha hadid, whose work is celebrated for its futuristic designs.  
Head Entity: museum  
Tail Entity: zaha hadid  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: fantcha was born as francelina durão almeida in mindelo on são vicente island , cape verde .
Head Entity: fantcha
Tail Entity: são vicente island
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: after living in new york city for several years, she decided to move back to her hometown in los angeles.  
Head Entity: she  
Tail Entity: los angeles  

Relation: residence  
Context: the famous author spent most of his life in a quaint cottage located in the picturesque village of haworth.  
Head Entity: the famous author  
Tail Entity: haworth  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Wolfgang Amadeus Mozart created over 600 works, including symphonies, operas, and chamber music.  
Head Entity: symphonies  
Tail Entity: Wolfgang Amadeus Mozart  

Relation: composer  
Context: Ludwig van Beethoven, a pivotal figure in the transition between the Classical and Romantic eras, is best known for his symphonies and sonatas.  
Head Entity: sonatas  
Tail Entity: Ludwig van Beethoven  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: one polish archer qualified for the women 's individual recurve by obtaining one of the eight olympic places available from the 2015 world archery championships in copenhagen , denmark .
Head Entity: 2015 world archery championships
Tail Entity: copenhagen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: The annual music festival will take place in the vibrant city of Austin, Texas, attracting thousands of fans from around the world.  
Head Entity: annual music festival  
Tail Entity: Austin, Texas  

Relation: location  
Context: The historic battle of Gettysburg was fought in the small town of Gettysburg, Pennsylvania, which is now a popular tourist destination.  
Head Entity: battle of Gettysburg  
Tail Entity: Gettysburg, Pennsylvania  
MemoryTrain:  epoch 15, batch     0 | loss: 6.4686535MemoryTrain:  epoch 15, batch     1 | loss: 2.1537753MemoryTrain:  epoch 15, batch     2 | loss: 3.3410414MemoryTrain:  epoch 15, batch     3 | loss: 5.6403741MemoryTrain:  epoch 15, batch     4 | loss: 5.9993158MemoryTrain:  epoch 15, batch     5 | loss: 2.7529280MemoryTrain:  epoch 15, batch     6 | loss: 4.3278854MemoryTrain:  epoch 15, batch     7 | loss: 3.7594570MemoryTrain:  epoch 15, batch     8 | loss: 4.7617968MemoryTrain:  epoch  5, batch     9 | loss: 11.7602561MemoryTrain:  epoch 15, batch     0 | loss: 1.9690445MemoryTrain:  epoch 15, batch     1 | loss: 3.0763019MemoryTrain:  epoch 15, batch     2 | loss: 2.9737516MemoryTrain:  epoch 15, batch     3 | loss: 3.2136669MemoryTrain:  epoch 15, batch     4 | loss: 3.4678641MemoryTrain:  epoch 15, batch     5 | loss: 4.6016902MemoryTrain:  epoch 15, batch     6 | loss: 5.1419230MemoryTrain:  epoch 15, batch     7 | loss: 3.6594736MemoryTrain:  epoch 15, batch     8 | loss: 2.3623205MemoryTrain:  epoch  5, batch     9 | loss: 14.8545866MemoryTrain:  epoch 15, batch     0 | loss: 2.2376423MemoryTrain:  epoch 15, batch     1 | loss: 2.5832244MemoryTrain:  epoch 15, batch     2 | loss: 1.9307927MemoryTrain:  epoch 15, batch     3 | loss: 3.9765430MemoryTrain:  epoch 15, batch     4 | loss: 2.2138258MemoryTrain:  epoch 15, batch     5 | loss: 2.9296075MemoryTrain:  epoch 15, batch     6 | loss: 1.9793258MemoryTrain:  epoch 15, batch     7 | loss: 2.9726671MemoryTrain:  epoch 15, batch     8 | loss: 3.0633403MemoryTrain:  epoch  5, batch     9 | loss: 8.3602660MemoryTrain:  epoch 15, batch     0 | loss: 3.2139385MemoryTrain:  epoch 15, batch     1 | loss: 2.5774039MemoryTrain:  epoch 15, batch     2 | loss: 3.2117387MemoryTrain:  epoch 15, batch     3 | loss: 2.8353730MemoryTrain:  epoch 15, batch     4 | loss: 2.9051224MemoryTrain:  epoch 15, batch     5 | loss: 3.4559091MemoryTrain:  epoch 15, batch     6 | loss: 3.1726653MemoryTrain:  epoch 15, batch     7 | loss: 4.1530585MemoryTrain:  epoch 15, batch     8 | loss: 4.0310214MemoryTrain:  epoch  5, batch     9 | loss: 22.3371040MemoryTrain:  epoch 15, batch     0 | loss: 4.4646610MemoryTrain:  epoch 15, batch     1 | loss: 1.9758393MemoryTrain:  epoch 15, batch     2 | loss: 3.9436634MemoryTrain:  epoch 15, batch     3 | loss: 2.3653847MemoryTrain:  epoch 15, batch     4 | loss: 2.5148248MemoryTrain:  epoch 15, batch     5 | loss: 5.5247832MemoryTrain:  epoch 15, batch     6 | loss: 6.6837682MemoryTrain:  epoch 15, batch     7 | loss: 5.1247885MemoryTrain:  epoch 15, batch     8 | loss: 2.3971785MemoryTrain:  epoch  5, batch     9 | loss: 8.3328691MemoryTrain:  epoch 15, batch     0 | loss: 4.8677163MemoryTrain:  epoch 15, batch     1 | loss: 4.9217880MemoryTrain:  epoch 15, batch     2 | loss: 2.1622657MemoryTrain:  epoch 15, batch     3 | loss: 2.4207684MemoryTrain:  epoch 15, batch     4 | loss: 4.5995018MemoryTrain:  epoch 15, batch     5 | loss: 4.8195287MemoryTrain:  epoch 15, batch     6 | loss: 2.2993934MemoryTrain:  epoch 15, batch     7 | loss: 1.9016262MemoryTrain:  epoch 15, batch     8 | loss: 3.2681741MemoryTrain:  epoch  5, batch     9 | loss: 7.5893028MemoryTrain:  epoch 15, batch     0 | loss: 1.7562815MemoryTrain:  epoch 15, batch     1 | loss: 2.2313333MemoryTrain:  epoch 15, batch     2 | loss: 1.8530418MemoryTrain:  epoch 15, batch     3 | loss: 2.8840539MemoryTrain:  epoch 15, batch     4 | loss: 2.5080645MemoryTrain:  epoch 15, batch     5 | loss: 1.7682711MemoryTrain:  epoch 15, batch     6 | loss: 2.2587392MemoryTrain:  epoch 15, batch     7 | loss: 2.4231883MemoryTrain:  epoch 15, batch     8 | loss: 2.0689962MemoryTrain:  epoch  5, batch     9 | loss: 8.1827538MemoryTrain:  epoch 15, batch     0 | loss: 2.0613761MemoryTrain:  epoch 15, batch     1 | loss: 4.1032656MemoryTrain:  epoch 15, batch     2 | loss: 2.0827743MemoryTrain:  epoch 15, batch     3 | loss: 4.0207133MemoryTrain:  epoch 15, batch     4 | loss: 5.1109997MemoryTrain:  epoch 15, batch     5 | loss: 2.1152658MemoryTrain:  epoch 15, batch     6 | loss: 2.7415471MemoryTrain:  epoch 15, batch     7 | loss: 1.9289882MemoryTrain:  epoch 15, batch     8 | loss: 1.6170860MemoryTrain:  epoch  5, batch     9 | loss: 13.5959177MemoryTrain:  epoch 15, batch     0 | loss: 1.6603678MemoryTrain:  epoch 15, batch     1 | loss: 2.0983707MemoryTrain:  epoch 15, batch     2 | loss: 1.8952820MemoryTrain:  epoch 15, batch     3 | loss: 1.8204246MemoryTrain:  epoch 15, batch     4 | loss: 2.3637951MemoryTrain:  epoch 15, batch     5 | loss: 1.7462272MemoryTrain:  epoch 15, batch     6 | loss: 1.9423752MemoryTrain:  epoch 15, batch     7 | loss: 1.9790889MemoryTrain:  epoch 15, batch     8 | loss: 2.1730335MemoryTrain:  epoch  5, batch     9 | loss: 8.6979193MemoryTrain:  epoch 15, batch     0 | loss: 2.0978610MemoryTrain:  epoch 15, batch     1 | loss: 2.9735654MemoryTrain:  epoch 15, batch     2 | loss: 1.5997615MemoryTrain:  epoch 15, batch     3 | loss: 2.3298501MemoryTrain:  epoch 15, batch     4 | loss: 4.4019014MemoryTrain:  epoch 15, batch     5 | loss: 1.9079696MemoryTrain:  epoch 15, batch     6 | loss: 2.0258741MemoryTrain:  epoch 15, batch     7 | loss: 1.9580902MemoryTrain:  epoch 15, batch     8 | loss: 1.8836004MemoryTrain:  epoch  5, batch     9 | loss: 25.3502860
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 20.83%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 25.00%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 26.25%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 23.96%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 31.25%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 38.28%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 43.75%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 47.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 51.14%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 54.17%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 57.21%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 58.93%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 61.25%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 63.28%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 65.07%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 67.11%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 66.25%   [EVAL] batch:   20 | acc: 25.00%,  total acc: 64.29%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 64.77%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 64.67%   [EVAL] batch:   23 | acc: 37.50%,  total acc: 63.54%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 63.50%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 61.30%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 59.49%   [EVAL] batch:   27 | acc: 12.50%,  total acc: 57.81%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 56.03%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 54.58%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 52.82%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 53.52%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 54.73%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 55.70%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 56.96%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 57.81%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 58.78%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 59.70%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 60.58%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 61.56%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 62.20%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 62.80%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 63.66%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 64.35%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 64.44%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 64.81%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 64.76%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 64.84%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 65.31%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 65.25%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 65.07%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 65.26%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 65.09%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 65.80%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 65.96%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 65.90%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 65.73%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 65.57%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 65.47%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 65.52%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 64.88%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 72.73%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 70.83%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 71.63%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 74.58%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 75.78%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 76.84%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 77.78%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 78.95%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 78.44%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 78.57%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 77.84%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 76.90%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 77.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.88%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.70%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.46%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 79.96%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 81.84%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 82.90%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 83.21%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 83.68%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 84.12%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 84.54%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.67%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 86.01%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 86.19%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 86.22%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 86.11%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 85.60%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 84.97%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 84.64%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 84.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 84.80%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 84.86%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 85.02%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 85.19%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 85.00%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 85.16%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 84.98%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 84.59%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 84.43%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 84.48%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 84.63%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 84.48%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 84.23%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 83.59%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 83.27%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 82.95%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 82.18%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 82.08%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 81.61%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 81.43%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 80.99%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 80.82%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 80.74%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 80.67%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 80.59%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 80.60%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 80.53%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 80.38%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 80.47%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 80.56%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 79.95%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 79.14%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 78.27%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 77.35%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 76.53%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 75.93%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 75.64%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 75.91%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 76.18%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 76.30%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 76.81%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 76.99%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 77.24%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 77.47%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 77.71%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 77.93%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 78.16%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 78.38%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 78.53%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 78.68%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 78.64%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 78.67%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 78.87%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 79.01%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 78.74%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 78.18%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 77.58%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 77.10%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 76.80%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 76.34%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 76.22%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 76.32%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 76.52%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 76.56%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 76.66%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 76.80%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 76.79%   [EVAL] batch:  119 | acc: 18.75%,  total acc: 76.30%   [EVAL] batch:  120 | acc: 31.25%,  total acc: 75.93%   [EVAL] batch:  121 | acc: 18.75%,  total acc: 75.46%   [EVAL] batch:  122 | acc: 25.00%,  total acc: 75.05%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 74.50%   [EVAL] batch:  124 | acc: 31.25%,  total acc: 74.15%   [EVAL] batch:  125 | acc: 43.75%,  total acc: 73.91%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 73.52%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 73.34%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 73.40%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 73.32%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 73.23%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 73.30%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 73.36%   [EVAL] batch:  133 | acc: 56.25%,  total acc: 73.23%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 73.24%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 73.30%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 73.36%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 73.28%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 72.84%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 72.50%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 72.16%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 71.92%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 71.68%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 71.44%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 71.64%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 71.83%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 71.98%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 72.17%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 72.36%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 72.54%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 72.14%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 71.71%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 71.28%   [EVAL] batch:  153 | acc: 12.50%,  total acc: 70.90%   [EVAL] batch:  154 | acc: 25.00%,  total acc: 70.60%   [EVAL] batch:  155 | acc: 18.75%,  total acc: 70.27%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 70.30%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 70.45%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 70.60%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 70.74%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 70.85%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 70.99%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 71.13%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 71.07%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 71.02%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 70.93%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 70.96%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 70.91%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 70.82%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 70.77%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:  171 | acc: 62.50%,  total acc: 70.78%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 70.59%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 70.55%   [EVAL] batch:  174 | acc: 75.00%,  total acc: 70.57%   [EVAL] batch:  175 | acc: 75.00%,  total acc: 70.60%   [EVAL] batch:  176 | acc: 81.25%,  total acc: 70.66%   [EVAL] batch:  177 | acc: 75.00%,  total acc: 70.68%   [EVAL] batch:  178 | acc: 93.75%,  total acc: 70.81%   [EVAL] batch:  179 | acc: 93.75%,  total acc: 70.94%   [EVAL] batch:  180 | acc: 81.25%,  total acc: 70.99%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 71.02%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 71.11%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 71.23%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 71.22%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 71.27%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 71.29%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 71.21%   [EVAL] batch:  188 | acc: 18.75%,  total acc: 70.93%   [EVAL] batch:  189 | acc: 37.50%,  total acc: 70.76%   [EVAL] batch:  190 | acc: 18.75%,  total acc: 70.48%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 70.21%   [EVAL] batch:  192 | acc: 31.25%,  total acc: 70.01%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 69.85%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 69.94%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 69.96%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 70.02%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 70.08%   [EVAL] batch:  198 | acc: 87.50%,  total acc: 70.16%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 70.19%   [EVAL] batch:  200 | acc: 37.50%,  total acc: 70.02%   [EVAL] batch:  201 | acc: 6.25%,  total acc: 69.71%   [EVAL] batch:  202 | acc: 12.50%,  total acc: 69.43%   [EVAL] batch:  203 | acc: 31.25%,  total acc: 69.24%   [EVAL] batch:  204 | acc: 31.25%,  total acc: 69.05%   [EVAL] batch:  205 | acc: 18.75%,  total acc: 68.81%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 68.90%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 69.05%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 69.20%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 69.32%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 69.46%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 69.60%   [EVAL] batch:  212 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:  213 | acc: 81.25%,  total acc: 69.80%   [EVAL] batch:  214 | acc: 81.25%,  total acc: 69.85%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 69.99%   [EVAL] batch:  216 | acc: 81.25%,  total acc: 70.05%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 70.18%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 70.26%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 70.40%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 70.53%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 70.61%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 70.71%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 70.84%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 70.97%   [EVAL] batch:  225 | acc: 62.50%,  total acc: 70.93%   [EVAL] batch:  226 | acc: 68.75%,  total acc: 70.93%   [EVAL] batch:  227 | acc: 50.00%,  total acc: 70.83%   [EVAL] batch:  228 | acc: 31.25%,  total acc: 70.66%   [EVAL] batch:  229 | acc: 56.25%,  total acc: 70.60%   [EVAL] batch:  230 | acc: 25.00%,  total acc: 70.40%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 70.47%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 70.57%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 70.70%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 70.82%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 70.95%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 71.04%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 71.09%   [EVAL] batch:  238 | acc: 62.50%,  total acc: 71.05%   [EVAL] batch:  239 | acc: 62.50%,  total acc: 71.02%   [EVAL] batch:  240 | acc: 56.25%,  total acc: 70.95%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 71.02%   [EVAL] batch:  242 | acc: 37.50%,  total acc: 70.88%   [EVAL] batch:  243 | acc: 31.25%,  total acc: 70.72%   [EVAL] batch:  244 | acc: 81.25%,  total acc: 70.77%   [EVAL] batch:  245 | acc: 62.50%,  total acc: 70.73%   [EVAL] batch:  246 | acc: 50.00%,  total acc: 70.65%   [EVAL] batch:  247 | acc: 75.00%,  total acc: 70.67%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 70.76%   [EVAL] batch:  249 | acc: 81.25%,  total acc: 70.80%   [EVAL] batch:  250 | acc: 18.75%,  total acc: 70.59%   [EVAL] batch:  251 | acc: 25.00%,  total acc: 70.41%   [EVAL] batch:  252 | acc: 18.75%,  total acc: 70.21%   [EVAL] batch:  253 | acc: 37.50%,  total acc: 70.08%   [EVAL] batch:  254 | acc: 31.25%,  total acc: 69.93%   [EVAL] batch:  255 | acc: 12.50%,  total acc: 69.70%   [EVAL] batch:  256 | acc: 75.00%,  total acc: 69.72%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 69.79%   [EVAL] batch:  258 | acc: 87.50%,  total acc: 69.86%   [EVAL] batch:  259 | acc: 81.25%,  total acc: 69.90%   [EVAL] batch:  260 | acc: 87.50%,  total acc: 69.97%   [EVAL] batch:  261 | acc: 87.50%,  total acc: 70.04%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 70.13%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 70.17%   [EVAL] batch:  264 | acc: 93.75%,  total acc: 70.26%   [EVAL] batch:  265 | acc: 93.75%,  total acc: 70.35%   [EVAL] batch:  266 | acc: 93.75%,  total acc: 70.44%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 70.52%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 70.54%   [EVAL] batch:  269 | acc: 50.00%,  total acc: 70.46%   [EVAL] batch:  270 | acc: 25.00%,  total acc: 70.30%   [EVAL] batch:  271 | acc: 75.00%,  total acc: 70.31%   [EVAL] batch:  272 | acc: 62.50%,  total acc: 70.28%   [EVAL] batch:  273 | acc: 37.50%,  total acc: 70.16%   [EVAL] batch:  274 | acc: 62.50%,  total acc: 70.14%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 69.90%   [EVAL] batch:  276 | acc: 12.50%,  total acc: 69.70%   [EVAL] batch:  277 | acc: 12.50%,  total acc: 69.49%   [EVAL] batch:  278 | acc: 6.25%,  total acc: 69.27%   [EVAL] batch:  279 | acc: 12.50%,  total acc: 69.06%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 68.82%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 68.84%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 68.93%   [EVAL] batch:  283 | acc: 87.50%,  total acc: 68.99%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 69.10%   [EVAL] batch:  285 | acc: 87.50%,  total acc: 69.17%   [EVAL] batch:  286 | acc: 93.75%,  total acc: 69.25%   [EVAL] batch:  287 | acc: 93.75%,  total acc: 69.34%   [EVAL] batch:  288 | acc: 93.75%,  total acc: 69.42%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 69.59%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 69.65%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 69.83%   [EVAL] batch:  294 | acc: 68.75%,  total acc: 69.83%   [EVAL] batch:  295 | acc: 81.25%,  total acc: 69.87%   [EVAL] batch:  296 | acc: 62.50%,  total acc: 69.84%   [EVAL] batch:  297 | acc: 68.75%,  total acc: 69.84%   [EVAL] batch:  298 | acc: 87.50%,  total acc: 69.90%   [EVAL] batch:  299 | acc: 62.50%,  total acc: 69.88%   [EVAL] batch:  300 | acc: 56.25%,  total acc: 69.83%   [EVAL] batch:  301 | acc: 75.00%,  total acc: 69.85%   [EVAL] batch:  302 | acc: 56.25%,  total acc: 69.80%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 69.88%   [EVAL] batch:  304 | acc: 75.00%,  total acc: 69.90%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 69.91%   [EVAL] batch:  306 | acc: 62.50%,  total acc: 69.89%   [EVAL] batch:  307 | acc: 56.25%,  total acc: 69.85%   [EVAL] batch:  308 | acc: 56.25%,  total acc: 69.80%   [EVAL] batch:  309 | acc: 68.75%,  total acc: 69.80%   [EVAL] batch:  310 | acc: 56.25%,  total acc: 69.75%   [EVAL] batch:  311 | acc: 68.75%,  total acc: 69.75%   [EVAL] batch:  312 | acc: 25.00%,  total acc: 69.61%   
cur_acc:  ['0.9544', '0.8204', '0.7480', '0.6905', '0.6488']
his_acc:  ['0.9544', '0.8805', '0.8005', '0.7270', '0.6961']
CurrentTrain: epoch 15, batch     0 | loss: 11.2179678CurrentTrain: epoch 15, batch     1 | loss: 11.1313068CurrentTrain: epoch 15, batch     2 | loss: 17.9291975CurrentTrain: epoch  1, batch     3 | loss: 10.5923686CurrentTrain: epoch 15, batch     0 | loss: 9.2239402CurrentTrain: epoch 15, batch     1 | loss: 11.9042935CurrentTrain: epoch 15, batch     2 | loss: 9.1340722CurrentTrain: epoch  1, batch     3 | loss: 6.1245464CurrentTrain: epoch 15, batch     0 | loss: 11.9174618CurrentTrain: epoch 15, batch     1 | loss: 8.6792382CurrentTrain: epoch 15, batch     2 | loss: 10.0176364CurrentTrain: epoch  1, batch     3 | loss: 6.5602983CurrentTrain: epoch 15, batch     0 | loss: 7.8695521CurrentTrain: epoch 15, batch     1 | loss: 7.6983895CurrentTrain: epoch 15, batch     2 | loss: 7.4296394CurrentTrain: epoch  1, batch     3 | loss: 9.0842658CurrentTrain: epoch 15, batch     0 | loss: 8.0767381CurrentTrain: epoch 15, batch     1 | loss: 8.7989430CurrentTrain: epoch 15, batch     2 | loss: 5.3909881CurrentTrain: epoch  1, batch     3 | loss: 6.1972069CurrentTrain: epoch 15, batch     0 | loss: 5.0197155CurrentTrain: epoch 15, batch     1 | loss: 10.0311382CurrentTrain: epoch 15, batch     2 | loss: 10.4148923CurrentTrain: epoch  1, batch     3 | loss: 6.5214277CurrentTrain: epoch 15, batch     0 | loss: 7.9014833CurrentTrain: epoch 15, batch     1 | loss: 7.6113961CurrentTrain: epoch 15, batch     2 | loss: 8.2290176CurrentTrain: epoch  1, batch     3 | loss: 6.1971186CurrentTrain: epoch 15, batch     0 | loss: 8.7540261CurrentTrain: epoch 15, batch     1 | loss: 7.7764381CurrentTrain: epoch 15, batch     2 | loss: 11.0406096CurrentTrain: epoch  1, batch     3 | loss: 5.6412214CurrentTrain: epoch 15, batch     0 | loss: 6.9386123CurrentTrain: epoch 15, batch     1 | loss: 13.6425645CurrentTrain: epoch 15, batch     2 | loss: 4.3889655CurrentTrain: epoch  1, batch     3 | loss: 6.3345896CurrentTrain: epoch 15, batch     0 | loss: 6.5277258CurrentTrain: epoch 15, batch     1 | loss: 7.7504300CurrentTrain: epoch 15, batch     2 | loss: 8.1593668CurrentTrain: epoch  1, batch     3 | loss: 5.9872641
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: " clinton crazies " is a term in american politics of the 1990s and later that refers to intense criticism of united states president bill clinton and his wife hillary clinton .
Head Entity: bill clinton
Tail Entity: hillary clinton
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: "In a lavish ceremony, actor Ryan Reynolds married actress Blake Lively, solidifying their status as one of Hollywood's power couples."  
Head Entity: Ryan Reynolds  
Tail Entity: Blake Lively  

Relation: spouse  
Context: "The former president Barack Obama and Michelle Obama have been married since 1992, often seen as a model couple in public life."  
Head Entity: Barack Obama  
Tail Entity: Michelle Obama  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: it was first released on a single in the uk by parlophone in september 1967 , and on the group 's self - titled album " tomorrow " in february 1968 .
Head Entity: tomorrow
Tail Entity: parlophone
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: The band's debut album was launched under the iconic label Atlantic Records, which has been home to many legendary artists.  
Head Entity: debut album  
Tail Entity: Atlantic Records  

Relation: record label  
Context: After signing with Universal Music, the artist released their latest single, which quickly climbed the charts.  
Head Entity: latest single  
Tail Entity: Universal Music  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: distributed by paramount pictures , " the italian job " was theatrically released in the united states on may 30 , 2003 , and grossed over $ 176 million worldwide .
Head Entity: the italian job
Tail Entity: paramount pictures
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: the film was distributed by universal pictures and received critical acclaim upon its release in 2015.  
Head Entity: the film  
Tail Entity: universal pictures  

Relation: distributor  
Context: the album was distributed by sony music and quickly climbed the charts, becoming a bestseller.  
Head Entity: the album  
Tail Entity: sony music  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: in 1796 carondelet commissioned pintado to conduct a survey of lands granted to ambrosio longue - épée in iberville county on the west bank of the mississippi river opposite baton rouge .
Head Entity: baton rouge
Tail Entity: mississippi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: the city of new orleans is situated on the banks of the mississippi river, making it a vibrant hub for trade and culture.  
Head Entity: new orleans  
Tail Entity: mississippi river  

Relation: located in or next to body of water  
Context: the picturesque village of lake placid is nestled beside the serene waters of mirror lake, attracting visitors year-round.  
Head Entity: lake placid  
Tail Entity: mirror lake  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: following the sale of amstrad plc to rupert murdoch 's bskyb , viglen is now lord sugar 's sole it establishment .
Head Entity: bskyb
Tail Entity: amstrad
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Relation: subsidiary  
Context: In 2018, Facebook acquired WhatsApp, making it a subsidiary of the social media giant.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Relation: subsidiary  
Context: After the merger, the new company announced that its subsidiary, XYZ Corp, would continue to operate independently.  
Head Entity: new company  
Tail Entity: XYZ Corp  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: he challenged seven other swimmers in heat four including ukraine 's rostyslav svanidze , a top 16 finalist in atlanta four years earlier .
Head Entity: rostyslav svanidze
Tail Entity: ukraine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: after moving to the united states, she became a naturalized citizen and proudly represented her country of birth in international competitions.  
Head Entity: she  
Tail Entity: united states  

Relation: country of citizenship  
Context: the renowned scientist was born in germany but later acquired citizenship in canada, where he conducted most of his groundbreaking research.  
Head Entity: the renowned scientist  
Tail Entity: canada  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: buck moved to austin in the mid-1970s and joined the blues rock group , the fabulous thunderbirds , along with keith ferguson , jimmie vaughan , and kim wilson .
Head Entity: the fabulous thunderbirds
Tail Entity: blues rock
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the album "thriller" by michael jackson is widely regarded as a landmark in pop music history, blending elements of rock, funk, and soul.  
Head Entity: michael jackson  
Tail Entity: pop  

Relation: genre  
Context: the film "inception," directed by christopher nolan, is a complex narrative that combines science fiction with psychological thriller elements.  
Head Entity: inception  
Tail Entity: science fiction  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: san lázaro is 660 km far from asunción and is located at the exact site of the confluence of the apa river with paraguay river , bordering brazil - chaco .
Head Entity: apa river
Tail Entity: paraguay river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: the mississippi river flows into the gulf of mexico, creating a rich delta ecosystem that supports diverse wildlife and plant species.  
Head Entity: mississippi river  
Tail Entity: gulf of mexico  

Relation: mouth of the watercourse  
Context: the rhine river, which runs through several countries in europe, ultimately empties into the north sea, providing a vital shipping route for international trade.  
Head Entity: rhine river  
Tail Entity: north sea  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: bradley halliday ( born 10 july 1995 ) is an english professional footballer who plays as a defender or a midfielder for league two club cambridge united .
Head Entity: bradley halliday
Tail Entity: defender
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: alex morgan ( born 2 july 1989 ) is an american soccer player who plays as a forward for the national team and the club san diego wave fc in the nwsl.  
Head Entity: alex morgan  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: lebron james ( born 30 december 1984 ) is an american professional basketball player who plays as a small forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: small forward  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: hammami made his international debut for tunisia in 2007 , and represented them at the africa cup of nations in 2010 and 2013 .
Head Entity: 2010
Tail Entity: africa cup of nations
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2021 UEFA European Championship was held in various cities across Europe, showcasing top national teams competing for the title.  
Head Entity: 2021  
Tail Entity: UEFA European Championship  

Relation: sports season of league or competition  
Context: The 2022 FIFA World Cup took place in Qatar, marking the first time the tournament was held in the Middle East.  
Head Entity: 2022  
Tail Entity: FIFA World Cup  
MemoryTrain:  epoch 15, batch     0 | loss: 3.2615274MemoryTrain:  epoch 15, batch     1 | loss: 6.2071482MemoryTrain:  epoch 15, batch     2 | loss: 3.2746763MemoryTrain:  epoch 15, batch     3 | loss: 2.8161071MemoryTrain:  epoch 15, batch     4 | loss: 3.5992208MemoryTrain:  epoch 15, batch     5 | loss: 3.4193661MemoryTrain:  epoch 15, batch     6 | loss: 6.4010361MemoryTrain:  epoch 15, batch     7 | loss: 3.7768569MemoryTrain:  epoch 15, batch     8 | loss: 4.4192370MemoryTrain:  epoch 15, batch     9 | loss: 4.9728774MemoryTrain:  epoch 15, batch    10 | loss: 3.6249787MemoryTrain:  epoch  3, batch    11 | loss: 11.3123947MemoryTrain:  epoch 15, batch     0 | loss: 3.1423381MemoryTrain:  epoch 15, batch     1 | loss: 4.5436231MemoryTrain:  epoch 15, batch     2 | loss: 2.6433866MemoryTrain:  epoch 15, batch     3 | loss: 6.2311400MemoryTrain:  epoch 15, batch     4 | loss: 4.7980769MemoryTrain:  epoch 15, batch     5 | loss: 3.3641329MemoryTrain:  epoch 15, batch     6 | loss: 2.9641273MemoryTrain:  epoch 15, batch     7 | loss: 4.1392893MemoryTrain:  epoch 15, batch     8 | loss: 3.3047452MemoryTrain:  epoch 15, batch     9 | loss: 2.8626965MemoryTrain:  epoch 15, batch    10 | loss: 3.9659260MemoryTrain:  epoch  3, batch    11 | loss: 11.4279907MemoryTrain:  epoch 15, batch     0 | loss: 2.2154498MemoryTrain:  epoch 15, batch     1 | loss: 2.2131895MemoryTrain:  epoch 15, batch     2 | loss: 2.3473709MemoryTrain:  epoch 15, batch     3 | loss: 2.0734204MemoryTrain:  epoch 15, batch     4 | loss: 2.7074153MemoryTrain:  epoch 15, batch     5 | loss: 2.6116100MemoryTrain:  epoch 15, batch     6 | loss: 2.2280369MemoryTrain:  epoch 15, batch     7 | loss: 2.6174286MemoryTrain:  epoch 15, batch     8 | loss: 2.9543128MemoryTrain:  epoch 15, batch     9 | loss: 2.8688379MemoryTrain:  epoch 15, batch    10 | loss: 2.5594894MemoryTrain:  epoch  3, batch    11 | loss: 9.8619232MemoryTrain:  epoch 15, batch     0 | loss: 1.9441916MemoryTrain:  epoch 15, batch     1 | loss: 1.9509034MemoryTrain:  epoch 15, batch     2 | loss: 1.6365409MemoryTrain:  epoch 15, batch     3 | loss: 2.2586171MemoryTrain:  epoch 15, batch     4 | loss: 2.5270919MemoryTrain:  epoch 15, batch     5 | loss: 3.0620494MemoryTrain:  epoch 15, batch     6 | loss: 3.2181214MemoryTrain:  epoch 15, batch     7 | loss: 3.6902123MemoryTrain:  epoch 15, batch     8 | loss: 5.1399015MemoryTrain:  epoch 15, batch     9 | loss: 4.3379293MemoryTrain:  epoch 15, batch    10 | loss: 2.1196003MemoryTrain:  epoch  3, batch    11 | loss: 10.4916841MemoryTrain:  epoch 15, batch     0 | loss: 3.8391494MemoryTrain:  epoch 15, batch     1 | loss: 2.8111816MemoryTrain:  epoch 15, batch     2 | loss: 1.6709676MemoryTrain:  epoch 15, batch     3 | loss: 2.1415375MemoryTrain:  epoch 15, batch     4 | loss: 1.6390478MemoryTrain:  epoch 15, batch     5 | loss: 2.4700391MemoryTrain:  epoch 15, batch     6 | loss: 1.9999337MemoryTrain:  epoch 15, batch     7 | loss: 2.3894317MemoryTrain:  epoch 15, batch     8 | loss: 2.1246898MemoryTrain:  epoch 15, batch     9 | loss: 2.5262540MemoryTrain:  epoch 15, batch    10 | loss: 3.3442802MemoryTrain:  epoch  3, batch    11 | loss: 10.2966970MemoryTrain:  epoch 15, batch     0 | loss: 1.7566458MemoryTrain:  epoch 15, batch     1 | loss: 1.7319183MemoryTrain:  epoch 15, batch     2 | loss: 1.8567691MemoryTrain:  epoch 15, batch     3 | loss: 1.8319084MemoryTrain:  epoch 15, batch     4 | loss: 2.9828032MemoryTrain:  epoch 15, batch     5 | loss: 1.6981483MemoryTrain:  epoch 15, batch     6 | loss: 3.1317148MemoryTrain:  epoch 15, batch     7 | loss: 1.7353834MemoryTrain:  epoch 15, batch     8 | loss: 2.0529514MemoryTrain:  epoch 15, batch     9 | loss: 1.6628364MemoryTrain:  epoch 15, batch    10 | loss: 4.5617250MemoryTrain:  epoch  3, batch    11 | loss: 11.1084440MemoryTrain:  epoch 15, batch     0 | loss: 1.6854242MemoryTrain:  epoch 15, batch     1 | loss: 2.1113360MemoryTrain:  epoch 15, batch     2 | loss: 2.3833602MemoryTrain:  epoch 15, batch     3 | loss: 1.7820534MemoryTrain:  epoch 15, batch     4 | loss: 1.7015972MemoryTrain:  epoch 15, batch     5 | loss: 4.0727600MemoryTrain:  epoch 15, batch     6 | loss: 4.2474003MemoryTrain:  epoch 15, batch     7 | loss: 1.4480046MemoryTrain:  epoch 15, batch     8 | loss: 4.0732146MemoryTrain:  epoch 15, batch     9 | loss: 1.7253483MemoryTrain:  epoch 15, batch    10 | loss: 1.4836839MemoryTrain:  epoch  3, batch    11 | loss: 10.3003320MemoryTrain:  epoch 15, batch     0 | loss: 2.0177718MemoryTrain:  epoch 15, batch     1 | loss: 4.0268572MemoryTrain:  epoch 15, batch     2 | loss: 1.9087092MemoryTrain:  epoch 15, batch     3 | loss: 1.8524571MemoryTrain:  epoch 15, batch     4 | loss: 2.0350409MemoryTrain:  epoch 15, batch     5 | loss: 1.5843129MemoryTrain:  epoch 15, batch     6 | loss: 1.6805747MemoryTrain:  epoch 15, batch     7 | loss: 1.6269297MemoryTrain:  epoch 15, batch     8 | loss: 1.6340551MemoryTrain:  epoch 15, batch     9 | loss: 4.3234879MemoryTrain:  epoch 15, batch    10 | loss: 2.2654901MemoryTrain:  epoch  3, batch    11 | loss: 10.2172030MemoryTrain:  epoch 15, batch     0 | loss: 1.8067520MemoryTrain:  epoch 15, batch     1 | loss: 1.7890633MemoryTrain:  epoch 15, batch     2 | loss: 1.9248202MemoryTrain:  epoch 15, batch     3 | loss: 2.2982998MemoryTrain:  epoch 15, batch     4 | loss: 1.5869387MemoryTrain:  epoch 15, batch     5 | loss: 4.0428495MemoryTrain:  epoch 15, batch     6 | loss: 1.7651493MemoryTrain:  epoch 15, batch     7 | loss: 1.9612291MemoryTrain:  epoch 15, batch     8 | loss: 2.3324422MemoryTrain:  epoch 15, batch     9 | loss: 1.3948727MemoryTrain:  epoch 15, batch    10 | loss: 3.3050696MemoryTrain:  epoch  3, batch    11 | loss: 10.5684553MemoryTrain:  epoch 15, batch     0 | loss: 1.7411951MemoryTrain:  epoch 15, batch     1 | loss: 1.6317386MemoryTrain:  epoch 15, batch     2 | loss: 1.4197140MemoryTrain:  epoch 15, batch     3 | loss: 4.3851385MemoryTrain:  epoch 15, batch     4 | loss: 1.7403515MemoryTrain:  epoch 15, batch     5 | loss: 3.9424399MemoryTrain:  epoch 15, batch     6 | loss: 4.4104143MemoryTrain:  epoch 15, batch     7 | loss: 1.4911200MemoryTrain:  epoch 15, batch     8 | loss: 1.6891840MemoryTrain:  epoch 15, batch     9 | loss: 2.3667919MemoryTrain:  epoch 15, batch    10 | loss: 1.4672080MemoryTrain:  epoch  3, batch    11 | loss: 9.7929116
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 56.25%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 57.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 60.42%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 65.18%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 71.59%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 73.96%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 73.21%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 73.44%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 74.26%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 73.96%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 74.34%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 73.44%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 73.51%   [EVAL] batch:   21 | acc: 56.25%,  total acc: 72.73%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 73.10%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 72.75%   [EVAL] batch:   25 | acc: 75.00%,  total acc: 72.84%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 73.15%   [EVAL] batch:   27 | acc: 81.25%,  total acc: 73.44%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 74.14%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 74.38%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 74.40%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 74.22%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 73.48%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 73.16%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 72.86%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 72.74%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 72.30%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 72.20%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 72.28%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 72.34%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 72.10%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 72.32%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 72.53%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 72.59%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 71.67%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 70.92%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 70.08%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 69.66%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 68.75%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 68.25%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 68.87%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 69.47%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 70.05%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 70.60%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 71.14%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 71.65%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 71.60%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 71.23%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 70.76%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 70.62%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 70.39%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 70.06%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 69.54%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 66.67%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 67.79%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 69.64%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 71.25%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 72.66%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 73.90%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 75.99%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 75.94%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 76.19%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 76.14%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 75.54%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 76.04%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 76.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.16%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.01%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 79.31%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.65%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.82%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 81.62%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 81.43%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 81.58%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 82.05%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.50%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 82.93%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 83.58%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 83.66%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 83.70%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 83.38%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 83.67%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 83.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 83.70%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 83.77%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 83.96%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 84.26%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 84.09%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 84.26%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 84.10%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 83.41%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 83.16%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 83.12%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 83.09%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 82.86%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 82.64%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 82.23%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 81.92%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 81.72%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 80.97%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 80.79%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 80.34%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 80.18%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 79.84%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 79.60%   [EVAL] batch:   72 | acc: 56.25%,  total acc: 79.28%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 79.14%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 79.00%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 78.70%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 78.57%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 78.29%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 77.85%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 77.66%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 77.47%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 76.91%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 76.13%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 75.30%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 74.41%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 73.62%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 72.84%   [EVAL] batch:   87 | acc: 43.75%,  total acc: 72.51%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 72.82%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 73.12%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 73.28%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 73.57%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 73.86%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 74.07%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 74.34%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 74.61%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 74.87%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 75.13%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 75.38%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 75.62%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 75.80%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 75.98%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 75.97%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 76.02%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 76.19%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 76.36%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 76.11%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 75.64%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 75.23%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 74.83%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 74.66%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 74.27%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 74.17%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 74.29%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 74.51%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 74.52%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 74.68%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 74.84%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 74.89%   [EVAL] batch:  119 | acc: 18.75%,  total acc: 74.43%   [EVAL] batch:  120 | acc: 18.75%,  total acc: 73.97%   [EVAL] batch:  121 | acc: 12.50%,  total acc: 73.46%   [EVAL] batch:  122 | acc: 25.00%,  total acc: 73.07%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 72.48%   [EVAL] batch:  124 | acc: 18.75%,  total acc: 72.05%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 71.73%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 71.41%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 71.29%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 71.32%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 71.25%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 71.09%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 71.16%   [EVAL] batch:  132 | acc: 68.75%,  total acc: 71.15%   [EVAL] batch:  133 | acc: 62.50%,  total acc: 71.08%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 71.11%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 71.23%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 71.30%   [EVAL] batch:  137 | acc: 56.25%,  total acc: 71.20%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 70.86%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 70.54%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 70.21%   [EVAL] batch:  141 | acc: 50.00%,  total acc: 70.07%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 69.84%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 69.62%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 69.78%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 69.99%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 70.15%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 70.35%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 70.55%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 70.71%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 70.24%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 69.78%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 69.36%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 68.91%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 68.55%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 68.15%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 68.19%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 68.35%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 68.47%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 68.63%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 68.94%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 69.10%   [EVAL] batch:  163 | acc: 50.00%,  total acc: 68.98%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 68.94%   [EVAL] batch:  165 | acc: 50.00%,  total acc: 68.83%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 68.86%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 68.90%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 68.86%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 68.82%   [EVAL] batch:  170 | acc: 68.75%,  total acc: 68.82%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 68.82%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 68.64%   [EVAL] batch:  173 | acc: 56.25%,  total acc: 68.57%   [EVAL] batch:  174 | acc: 75.00%,  total acc: 68.61%   [EVAL] batch:  175 | acc: 75.00%,  total acc: 68.64%   [EVAL] batch:  176 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:  177 | acc: 81.25%,  total acc: 68.82%   [EVAL] batch:  178 | acc: 81.25%,  total acc: 68.89%   [EVAL] batch:  179 | acc: 93.75%,  total acc: 69.03%   [EVAL] batch:  180 | acc: 81.25%,  total acc: 69.10%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 69.13%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 69.23%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 69.36%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 69.36%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 69.42%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 69.45%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 69.38%   [EVAL] batch:  188 | acc: 25.00%,  total acc: 69.15%   [EVAL] batch:  189 | acc: 31.25%,  total acc: 68.95%   [EVAL] batch:  190 | acc: 18.75%,  total acc: 68.68%   [EVAL] batch:  191 | acc: 31.25%,  total acc: 68.49%   [EVAL] batch:  192 | acc: 37.50%,  total acc: 68.33%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 68.17%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 68.24%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 68.21%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 68.27%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 68.34%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 68.37%   [EVAL] batch:  199 | acc: 68.75%,  total acc: 68.38%   [EVAL] batch:  200 | acc: 25.00%,  total acc: 68.16%   [EVAL] batch:  201 | acc: 6.25%,  total acc: 67.85%   [EVAL] batch:  202 | acc: 12.50%,  total acc: 67.58%   [EVAL] batch:  203 | acc: 25.00%,  total acc: 67.37%   [EVAL] batch:  204 | acc: 25.00%,  total acc: 67.16%   [EVAL] batch:  205 | acc: 18.75%,  total acc: 66.93%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 67.03%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 67.34%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 67.47%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 67.62%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 67.78%   [EVAL] batch:  212 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:  213 | acc: 87.50%,  total acc: 68.02%   [EVAL] batch:  214 | acc: 87.50%,  total acc: 68.11%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 68.26%   [EVAL] batch:  216 | acc: 81.25%,  total acc: 68.32%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 68.46%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 68.55%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 68.69%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 68.83%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 68.92%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 69.03%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 69.14%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 69.28%   [EVAL] batch:  225 | acc: 43.75%,  total acc: 69.16%   [EVAL] batch:  226 | acc: 62.50%,  total acc: 69.14%   [EVAL] batch:  227 | acc: 56.25%,  total acc: 69.08%   [EVAL] batch:  228 | acc: 25.00%,  total acc: 68.89%   [EVAL] batch:  229 | acc: 37.50%,  total acc: 68.75%   [EVAL] batch:  230 | acc: 6.25%,  total acc: 68.48%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 68.56%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 68.67%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 68.80%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 68.94%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 69.07%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 69.17%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 69.14%   [EVAL] batch:  238 | acc: 50.00%,  total acc: 69.06%   [EVAL] batch:  239 | acc: 43.75%,  total acc: 68.96%   [EVAL] batch:  240 | acc: 56.25%,  total acc: 68.91%   [EVAL] batch:  241 | acc: 68.75%,  total acc: 68.90%   [EVAL] batch:  242 | acc: 37.50%,  total acc: 68.78%   [EVAL] batch:  243 | acc: 31.25%,  total acc: 68.62%   [EVAL] batch:  244 | acc: 81.25%,  total acc: 68.67%   [EVAL] batch:  245 | acc: 62.50%,  total acc: 68.65%   [EVAL] batch:  246 | acc: 43.75%,  total acc: 68.55%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 68.60%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 68.70%   [EVAL] batch:  249 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:  250 | acc: 0.00%,  total acc: 68.48%   [EVAL] batch:  251 | acc: 12.50%,  total acc: 68.25%   [EVAL] batch:  252 | acc: 0.00%,  total acc: 67.98%   [EVAL] batch:  253 | acc: 6.25%,  total acc: 67.74%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 67.48%   [EVAL] batch:  255 | acc: 12.50%,  total acc: 67.26%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 67.27%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 67.32%   [EVAL] batch:  258 | acc: 81.25%,  total acc: 67.37%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 67.40%   [EVAL] batch:  260 | acc: 81.25%,  total acc: 67.46%   [EVAL] batch:  261 | acc: 87.50%,  total acc: 67.53%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 67.63%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 67.66%   [EVAL] batch:  264 | acc: 87.50%,  total acc: 67.74%   [EVAL] batch:  265 | acc: 93.75%,  total acc: 67.83%   [EVAL] batch:  266 | acc: 93.75%,  total acc: 67.93%   [EVAL] batch:  267 | acc: 87.50%,  total acc: 68.00%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 68.01%   [EVAL] batch:  269 | acc: 12.50%,  total acc: 67.80%   [EVAL] batch:  270 | acc: 0.00%,  total acc: 67.55%   [EVAL] batch:  271 | acc: 18.75%,  total acc: 67.37%   [EVAL] batch:  272 | acc: 31.25%,  total acc: 67.24%   [EVAL] batch:  273 | acc: 0.00%,  total acc: 66.99%   [EVAL] batch:  274 | acc: 12.50%,  total acc: 66.80%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 66.58%   [EVAL] batch:  276 | acc: 12.50%,  total acc: 66.38%   [EVAL] batch:  277 | acc: 12.50%,  total acc: 66.19%   [EVAL] batch:  278 | acc: 6.25%,  total acc: 65.97%   [EVAL] batch:  279 | acc: 12.50%,  total acc: 65.78%   [EVAL] batch:  280 | acc: 6.25%,  total acc: 65.57%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 65.58%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 65.66%   [EVAL] batch:  283 | acc: 81.25%,  total acc: 65.71%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 65.83%   [EVAL] batch:  285 | acc: 87.50%,  total acc: 65.91%   [EVAL] batch:  286 | acc: 87.50%,  total acc: 65.98%   [EVAL] batch:  287 | acc: 100.00%,  total acc: 66.10%   [EVAL] batch:  288 | acc: 93.75%,  total acc: 66.20%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 66.31%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 66.39%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 66.44%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 66.55%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 66.65%   [EVAL] batch:  294 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:  295 | acc: 75.00%,  total acc: 66.70%   [EVAL] batch:  296 | acc: 62.50%,  total acc: 66.69%   [EVAL] batch:  297 | acc: 68.75%,  total acc: 66.69%   [EVAL] batch:  298 | acc: 87.50%,  total acc: 66.76%   [EVAL] batch:  299 | acc: 62.50%,  total acc: 66.75%   [EVAL] batch:  300 | acc: 50.00%,  total acc: 66.69%   [EVAL] batch:  301 | acc: 62.50%,  total acc: 66.68%   [EVAL] batch:  302 | acc: 56.25%,  total acc: 66.65%   [EVAL] batch:  303 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:  304 | acc: 62.50%,  total acc: 66.66%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 66.69%   [EVAL] batch:  306 | acc: 56.25%,  total acc: 66.65%   [EVAL] batch:  307 | acc: 56.25%,  total acc: 66.62%   [EVAL] batch:  308 | acc: 56.25%,  total acc: 66.59%   [EVAL] batch:  309 | acc: 68.75%,  total acc: 66.59%   [EVAL] batch:  310 | acc: 56.25%,  total acc: 66.56%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 66.61%   [EVAL] batch:  312 | acc: 43.75%,  total acc: 66.53%   [EVAL] batch:  313 | acc: 50.00%,  total acc: 66.48%   [EVAL] batch:  314 | acc: 81.25%,  total acc: 66.53%   [EVAL] batch:  315 | acc: 37.50%,  total acc: 66.44%   [EVAL] batch:  316 | acc: 56.25%,  total acc: 66.40%   [EVAL] batch:  317 | acc: 75.00%,  total acc: 66.43%   [EVAL] batch:  318 | acc: 81.25%,  total acc: 66.48%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 66.58%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 66.69%   [EVAL] batch:  322 | acc: 81.25%,  total acc: 66.74%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 66.78%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 66.85%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 66.87%   [EVAL] batch:  326 | acc: 62.50%,  total acc: 66.86%   [EVAL] batch:  327 | acc: 75.00%,  total acc: 66.88%   [EVAL] batch:  328 | acc: 81.25%,  total acc: 66.93%   [EVAL] batch:  329 | acc: 81.25%,  total acc: 66.97%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 66.98%   [EVAL] batch:  331 | acc: 75.00%,  total acc: 67.00%   [EVAL] batch:  332 | acc: 56.25%,  total acc: 66.97%   [EVAL] batch:  333 | acc: 75.00%,  total acc: 66.99%   [EVAL] batch:  334 | acc: 62.50%,  total acc: 66.98%   [EVAL] batch:  335 | acc: 87.50%,  total acc: 67.04%   [EVAL] batch:  336 | acc: 56.25%,  total acc: 67.01%   [EVAL] batch:  337 | acc: 75.00%,  total acc: 67.03%   [EVAL] batch:  338 | acc: 81.25%,  total acc: 67.07%   [EVAL] batch:  339 | acc: 68.75%,  total acc: 67.08%   [EVAL] batch:  340 | acc: 93.75%,  total acc: 67.16%   [EVAL] batch:  341 | acc: 81.25%,  total acc: 67.20%   [EVAL] batch:  342 | acc: 93.75%,  total acc: 67.27%   [EVAL] batch:  343 | acc: 62.50%,  total acc: 67.26%   [EVAL] batch:  344 | acc: 56.25%,  total acc: 67.23%   [EVAL] batch:  345 | acc: 56.25%,  total acc: 67.20%   [EVAL] batch:  346 | acc: 62.50%,  total acc: 67.18%   [EVAL] batch:  347 | acc: 62.50%,  total acc: 67.17%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 67.19%   [EVAL] batch:  349 | acc: 62.50%,  total acc: 67.18%   [EVAL] batch:  350 | acc: 68.75%,  total acc: 67.18%   [EVAL] batch:  351 | acc: 75.00%,  total acc: 67.21%   [EVAL] batch:  352 | acc: 62.50%,  total acc: 67.19%   [EVAL] batch:  353 | acc: 75.00%,  total acc: 67.21%   [EVAL] batch:  354 | acc: 81.25%,  total acc: 67.25%   [EVAL] batch:  355 | acc: 87.50%,  total acc: 67.31%   [EVAL] batch:  356 | acc: 50.00%,  total acc: 67.26%   [EVAL] batch:  357 | acc: 31.25%,  total acc: 67.16%   [EVAL] batch:  358 | acc: 31.25%,  total acc: 67.06%   [EVAL] batch:  359 | acc: 43.75%,  total acc: 67.00%   [EVAL] batch:  360 | acc: 31.25%,  total acc: 66.90%   [EVAL] batch:  361 | acc: 43.75%,  total acc: 66.83%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 66.84%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 66.93%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 67.02%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 67.11%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 67.20%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 67.29%   [EVAL] batch:  368 | acc: 87.50%,  total acc: 67.34%   [EVAL] batch:  369 | acc: 62.50%,  total acc: 67.33%   [EVAL] batch:  370 | acc: 43.75%,  total acc: 67.27%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 67.20%   [EVAL] batch:  372 | acc: 68.75%,  total acc: 67.21%   [EVAL] batch:  373 | acc: 50.00%,  total acc: 67.16%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 67.15%   
cur_acc:  ['0.9544', '0.8204', '0.7480', '0.6905', '0.6488', '0.6954']
his_acc:  ['0.9544', '0.8805', '0.8005', '0.7270', '0.6961', '0.6715']
CurrentTrain: epoch 15, batch     0 | loss: 12.6208279CurrentTrain: epoch 15, batch     1 | loss: 14.3322752CurrentTrain: epoch 15, batch     2 | loss: 25.9417873CurrentTrain: epoch  1, batch     3 | loss: 23.2981135CurrentTrain: epoch 15, batch     0 | loss: 11.0965750CurrentTrain: epoch 15, batch     1 | loss: 11.2745250CurrentTrain: epoch 15, batch     2 | loss: 8.9228614CurrentTrain: epoch  1, batch     3 | loss: 9.7863398CurrentTrain: epoch 15, batch     0 | loss: 13.6533034CurrentTrain: epoch 15, batch     1 | loss: 10.7569489CurrentTrain: epoch 15, batch     2 | loss: 9.1742795CurrentTrain: epoch  1, batch     3 | loss: 6.1395164CurrentTrain: epoch 15, batch     0 | loss: 8.7466040CurrentTrain: epoch 15, batch     1 | loss: 14.4074167CurrentTrain: epoch 15, batch     2 | loss: 8.3838929CurrentTrain: epoch  1, batch     3 | loss: 7.2885187CurrentTrain: epoch 15, batch     0 | loss: 8.1018333CurrentTrain: epoch 15, batch     1 | loss: 5.5520374CurrentTrain: epoch 15, batch     2 | loss: 12.3340796CurrentTrain: epoch  1, batch     3 | loss: 6.5272461CurrentTrain: epoch 15, batch     0 | loss: 6.4639583CurrentTrain: epoch 15, batch     1 | loss: 8.3865291CurrentTrain: epoch 15, batch     2 | loss: 7.2481158CurrentTrain: epoch  1, batch     3 | loss: 6.7033742CurrentTrain: epoch 15, batch     0 | loss: 9.3286471CurrentTrain: epoch 15, batch     1 | loss: 6.9002747CurrentTrain: epoch 15, batch     2 | loss: 16.2305714CurrentTrain: epoch  1, batch     3 | loss: 12.4548960CurrentTrain: epoch 15, batch     0 | loss: 5.6032396CurrentTrain: epoch 15, batch     1 | loss: 11.1410558CurrentTrain: epoch 15, batch     2 | loss: 8.2828041CurrentTrain: epoch  1, batch     3 | loss: 5.9595409CurrentTrain: epoch 15, batch     0 | loss: 10.1412407CurrentTrain: epoch 15, batch     1 | loss: 10.6346750CurrentTrain: epoch 15, batch     2 | loss: 9.2661197CurrentTrain: epoch  1, batch     3 | loss: 5.7945165CurrentTrain: epoch 15, batch     0 | loss: 8.3138995CurrentTrain: epoch 15, batch     1 | loss: 12.3876887CurrentTrain: epoch 15, batch     2 | loss: 11.2143325CurrentTrain: epoch  1, batch     3 | loss: 6.5294600
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the sutagao are the chibcha - speaking indios sutagaos indigenous people from the region of fusagasugá , bogotá savanna , cundinamarca , colombia .
Head Entity: colombia
Tail Entity: cundinamarca
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The state of California is known for its diverse geography and is divided into several counties, including Los Angeles County, which is the most populous county in the United States.  
Head Entity: California  
Tail Entity: Los Angeles County  

Relation: contains administrative territorial entity  
Context: The country of Japan is made up of several prefectures, with Tokyo being the most famous and densely populated among them.  
Head Entity: Japan  
Tail Entity: Tokyo  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: enzo is an italian given name derivative of heinz , a form of henry .
Head Entity: henry
Tail Entity: heinz
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as each other due to their close genetic relationship.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: Many historians argue that the ancient city of Byzantium is said to be the same as modern-day Istanbul, although the transition involved significant changes over time.  
Head Entity: Byzantium  
Tail Entity: Istanbul  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: in 2010 , ty burrell received a nomination for the television critics association award for individual achievement in comedy , winning in 2011 .
Head Entity: television critics association award for individual achievement in comedy
Tail Entity: ty burrell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: In 2022, the Boston Red Sox triumphed in the World Series, marking their fourth championship in the last two decades.  
Head Entity: World Series  
Tail Entity: Boston Red Sox  

Relation: winner  
Context: At the annual film festival, the documentary "Voices of Change" was awarded the Best Documentary prize, highlighting its impactful storytelling.  
Head Entity: Best Documentary prize  
Tail Entity: "Voices of Change"  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: the railroad car they were deported in was attached to the end of the last train out of drancy which also carried drancy commandant ss hauptsturmführer alois brunner and other german military personnel .
Head Entity: alois brunner
Tail Entity: hauptsturmführer
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: General John Smith was awarded the title of Major General for his exceptional leadership during the conflict, which significantly contributed to the victory of his battalion.  
Head Entity: John Smith  
Tail Entity: Major General  

Relation: military rank  
Context: During the ceremony, Colonel Jane Doe was recognized for her outstanding service and was promoted to the rank of Brigadier General, marking a significant milestone in her military career.  
Head Entity: Jane Doe  
Tail Entity: Brigadier General  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: she went on to the film school at san francisco state university , when she was hired as an artist for an atari game called " electrocop " .
Head Entity: electrocop
Tail Entity: atari
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: The renowned author released her latest novel through a well-known publishing house that has been in the industry for decades.  
Head Entity: latest novel  
Tail Entity: publishing house  

Relation: publisher  
Context: After years of hard work, the independent game developer finally secured a deal with a major publisher to distribute their new game worldwide.  
Head Entity: new game  
Tail Entity: major publisher  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " muzai moratorium " and " shōso strip " were released on vinyl in 2008 , as a part of her 10-year anniversary celebrations .
Head Entity: muzai moratorium
Tail Entity: shōso strip
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first chapter of the book introduces the main character, while the second chapter follows with a detailed backstory.  
Head Entity: first chapter  
Tail Entity: second chapter  

Relation: followed by  
Context: In the concert lineup, the opening act was a local band, followed by a well-known pop artist who headlined the event.  
Head Entity: local band  
Tail Entity: well-known pop artist  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: there is a memoir of his life and work by gustav queck ( jena , 1852 ) .
Head Entity: gustav queck
Tail Entity: jena
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: the headquarters of the company is situated in san francisco, where they have been operating since 2010.  
Head Entity: the company  
Tail Entity: san francisco  

Relation: work location  
Context: during her time at the university, she conducted research in various labs located in boston.  
Head Entity: she  
Tail Entity: boston  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in cancer treatment.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a talented software engineer, has been working at tech innovations for over five years.  
Head Entity: john smith  
Tail Entity: software engineer  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a World Heritage Site, attracting tourists from around the globe.  
Head Entity: ancient city of Petra  
Tail Entity: World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: beverly bower ( september 30 , 1925 – march 24 , 2002 ) was an american operatic soprano who had an active international opera career from the mid-1950s through the early 1970s .
Head Entity: beverly bower
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti ( october 12 , 1935 – september 6 , 2007 ) was an italian tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey ( born march 27 , 1969 ) is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: singer  
MemoryTrain:  epoch 15, batch     0 | loss: 2.6388597MemoryTrain:  epoch 15, batch     1 | loss: 2.3045426MemoryTrain:  epoch 15, batch     2 | loss: 3.0124655MemoryTrain:  epoch 15, batch     3 | loss: 4.6708514MemoryTrain:  epoch 15, batch     4 | loss: 3.2404385MemoryTrain:  epoch 15, batch     5 | loss: 3.7126019MemoryTrain:  epoch 15, batch     6 | loss: 2.9079084MemoryTrain:  epoch 15, batch     7 | loss: 4.3369997MemoryTrain:  epoch 15, batch     8 | loss: 3.0841913MemoryTrain:  epoch 15, batch     9 | loss: 5.4097570MemoryTrain:  epoch 15, batch    10 | loss: 2.2274976MemoryTrain:  epoch 15, batch    11 | loss: 5.4247507MemoryTrain:  epoch 15, batch    12 | loss: 4.2999818MemoryTrain:  epoch  1, batch    13 | loss: 7.8712012MemoryTrain:  epoch 15, batch     0 | loss: 1.7179581MemoryTrain:  epoch 15, batch     1 | loss: 5.6075173MemoryTrain:  epoch 15, batch     2 | loss: 5.3878103MemoryTrain:  epoch 15, batch     3 | loss: 2.6109279MemoryTrain:  epoch 15, batch     4 | loss: 5.1137701MemoryTrain:  epoch 15, batch     5 | loss: 2.8019363MemoryTrain:  epoch 15, batch     6 | loss: 5.1889199MemoryTrain:  epoch 15, batch     7 | loss: 2.1668710MemoryTrain:  epoch 15, batch     8 | loss: 3.2469112MemoryTrain:  epoch 15, batch     9 | loss: 2.5099547MemoryTrain:  epoch 15, batch    10 | loss: 2.7224382MemoryTrain:  epoch 15, batch    11 | loss: 2.4634285MemoryTrain:  epoch 15, batch    12 | loss: 2.4632799MemoryTrain:  epoch  1, batch    13 | loss: 7.7438917MemoryTrain:  epoch 15, batch     0 | loss: 4.6069447MemoryTrain:  epoch 15, batch     1 | loss: 4.9080877MemoryTrain:  epoch 15, batch     2 | loss: 1.6114430MemoryTrain:  epoch 15, batch     3 | loss: 1.4998552MemoryTrain:  epoch 15, batch     4 | loss: 1.7974390MemoryTrain:  epoch 15, batch     5 | loss: 5.1075923MemoryTrain:  epoch 15, batch     6 | loss: 1.9654175MemoryTrain:  epoch 15, batch     7 | loss: 2.5880669MemoryTrain:  epoch 15, batch     8 | loss: 4.8553428MemoryTrain:  epoch 15, batch     9 | loss: 2.7342727MemoryTrain:  epoch 15, batch    10 | loss: 1.8238379MemoryTrain:  epoch 15, batch    11 | loss: 2.6140726MemoryTrain:  epoch 15, batch    12 | loss: 2.7681215MemoryTrain:  epoch  1, batch    13 | loss: 5.4390249MemoryTrain:  epoch 15, batch     0 | loss: 2.5035734MemoryTrain:  epoch 15, batch     1 | loss: 4.0705655MemoryTrain:  epoch 15, batch     2 | loss: 2.0687475MemoryTrain:  epoch 15, batch     3 | loss: 2.0276598MemoryTrain:  epoch 15, batch     4 | loss: 2.1008666MemoryTrain:  epoch 15, batch     5 | loss: 1.9514282MemoryTrain:  epoch 15, batch     6 | loss: 1.9976765MemoryTrain:  epoch 15, batch     7 | loss: 4.9645746MemoryTrain:  epoch 15, batch     8 | loss: 2.5192797MemoryTrain:  epoch 15, batch     9 | loss: 2.5799164MemoryTrain:  epoch 15, batch    10 | loss: 1.9365118MemoryTrain:  epoch 15, batch    11 | loss: 2.0468621MemoryTrain:  epoch 15, batch    12 | loss: 1.8341780MemoryTrain:  epoch  1, batch    13 | loss: 5.3696813MemoryTrain:  epoch 15, batch     0 | loss: 4.0583020MemoryTrain:  epoch 15, batch     1 | loss: 2.2416441MemoryTrain:  epoch 15, batch     2 | loss: 2.9176513MemoryTrain:  epoch 15, batch     3 | loss: 1.7079934MemoryTrain:  epoch 15, batch     4 | loss: 1.8224063MemoryTrain:  epoch 15, batch     5 | loss: 1.9207795MemoryTrain:  epoch 15, batch     6 | loss: 1.7683692MemoryTrain:  epoch 15, batch     7 | loss: 2.3519035MemoryTrain:  epoch 15, batch     8 | loss: 1.6759865MemoryTrain:  epoch 15, batch     9 | loss: 2.5387930MemoryTrain:  epoch 15, batch    10 | loss: 4.9322277MemoryTrain:  epoch 15, batch    11 | loss: 2.4495606MemoryTrain:  epoch 15, batch    12 | loss: 4.3966820MemoryTrain:  epoch  1, batch    13 | loss: 6.2844153MemoryTrain:  epoch 15, batch     0 | loss: 2.3967346MemoryTrain:  epoch 15, batch     1 | loss: 1.4878547MemoryTrain:  epoch 15, batch     2 | loss: 2.1562603MemoryTrain:  epoch 15, batch     3 | loss: 6.4086997MemoryTrain:  epoch 15, batch     4 | loss: 1.6404691MemoryTrain:  epoch 15, batch     5 | loss: 2.3470623MemoryTrain:  epoch 15, batch     6 | loss: 2.6630168MemoryTrain:  epoch 15, batch     7 | loss: 2.6176411MemoryTrain:  epoch 15, batch     8 | loss: 1.8572608MemoryTrain:  epoch 15, batch     9 | loss: 4.3791123MemoryTrain:  epoch 15, batch    10 | loss: 1.8235900MemoryTrain:  epoch 15, batch    11 | loss: 1.6145917MemoryTrain:  epoch 15, batch    12 | loss: 4.2141235MemoryTrain:  epoch  1, batch    13 | loss: 5.6342301MemoryTrain:  epoch 15, batch     0 | loss: 1.8517124MemoryTrain:  epoch 15, batch     1 | loss: 4.2384141MemoryTrain:  epoch 15, batch     2 | loss: 4.2353068MemoryTrain:  epoch 15, batch     3 | loss: 1.8037774MemoryTrain:  epoch 15, batch     4 | loss: 1.5599955MemoryTrain:  epoch 15, batch     5 | loss: 3.6062596MemoryTrain:  epoch 15, batch     6 | loss: 1.9522667MemoryTrain:  epoch 15, batch     7 | loss: 2.0190487MemoryTrain:  epoch 15, batch     8 | loss: 1.7964406MemoryTrain:  epoch 15, batch     9 | loss: 2.3278429MemoryTrain:  epoch 15, batch    10 | loss: 1.6417235MemoryTrain:  epoch 15, batch    11 | loss: 2.1497130MemoryTrain:  epoch 15, batch    12 | loss: 1.4806595MemoryTrain:  epoch  1, batch    13 | loss: 6.3794142MemoryTrain:  epoch 15, batch     0 | loss: 2.1221071MemoryTrain:  epoch 15, batch     1 | loss: 1.9028143MemoryTrain:  epoch 15, batch     2 | loss: 1.4427592MemoryTrain:  epoch 15, batch     3 | loss: 1.5160037MemoryTrain:  epoch 15, batch     4 | loss: 3.6838581MemoryTrain:  epoch 15, batch     5 | loss: 2.3416297MemoryTrain:  epoch 15, batch     6 | loss: 1.6726378MemoryTrain:  epoch 15, batch     7 | loss: 1.8086947MemoryTrain:  epoch 15, batch     8 | loss: 1.3595997MemoryTrain:  epoch 15, batch     9 | loss: 2.5164906MemoryTrain:  epoch 15, batch    10 | loss: 2.0118640MemoryTrain:  epoch 15, batch    11 | loss: 1.4053215MemoryTrain:  epoch 15, batch    12 | loss: 1.7073617MemoryTrain:  epoch  1, batch    13 | loss: 4.8902747MemoryTrain:  epoch 15, batch     0 | loss: 3.8781172MemoryTrain:  epoch 15, batch     1 | loss: 1.5899572MemoryTrain:  epoch 15, batch     2 | loss: 2.1252367MemoryTrain:  epoch 15, batch     3 | loss: 1.5984121MemoryTrain:  epoch 15, batch     4 | loss: 1.7354271MemoryTrain:  epoch 15, batch     5 | loss: 1.6400906MemoryTrain:  epoch 15, batch     6 | loss: 1.7801865MemoryTrain:  epoch 15, batch     7 | loss: 1.5717233MemoryTrain:  epoch 15, batch     8 | loss: 3.8819378MemoryTrain:  epoch 15, batch     9 | loss: 1.6508233MemoryTrain:  epoch 15, batch    10 | loss: 1.4985016MemoryTrain:  epoch 15, batch    11 | loss: 1.5900366MemoryTrain:  epoch 15, batch    12 | loss: 1.8318265MemoryTrain:  epoch  1, batch    13 | loss: 11.9340447MemoryTrain:  epoch 15, batch     0 | loss: 3.7979689MemoryTrain:  epoch 15, batch     1 | loss: 1.7764098MemoryTrain:  epoch 15, batch     2 | loss: 1.6194707MemoryTrain:  epoch 15, batch     3 | loss: 1.3198773MemoryTrain:  epoch 15, batch     4 | loss: 1.4632998MemoryTrain:  epoch 15, batch     5 | loss: 1.3072737MemoryTrain:  epoch 15, batch     6 | loss: 1.7620186MemoryTrain:  epoch 15, batch     7 | loss: 1.6628280MemoryTrain:  epoch 15, batch     8 | loss: 1.7363739MemoryTrain:  epoch 15, batch     9 | loss: 2.5418066MemoryTrain:  epoch 15, batch    10 | loss: 1.5830807MemoryTrain:  epoch 15, batch    11 | loss: 2.5565047MemoryTrain:  epoch 15, batch    12 | loss: 1.2974037MemoryTrain:  epoch  1, batch    13 | loss: 5.9743887
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 45.83%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 45.31%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 47.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 52.08%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 51.79%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 54.69%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 58.75%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 57.95%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 58.85%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 61.54%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 62.05%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 62.92%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 63.67%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 65.07%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 67.01%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 68.42%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 69.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 71.13%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 72.44%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 73.64%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 74.48%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 75.50%   [EVAL] batch:   25 | acc: 75.00%,  total acc: 75.48%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 75.46%   [EVAL] batch:   27 | acc: 81.25%,  total acc: 75.67%   [EVAL] batch:   28 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 74.61%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 74.24%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 72.79%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 71.61%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 70.14%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 68.75%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 69.05%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 69.19%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 69.32%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 69.72%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 69.84%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 69.81%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 69.53%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 69.64%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 70.00%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 70.59%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 71.70%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 72.22%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 72.73%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 73.21%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 73.57%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 73.81%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 74.05%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 74.38%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 74.59%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 74.60%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 66.88%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 64.77%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 63.54%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 63.46%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 67.08%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 70.59%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 73.03%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 71.88%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 72.32%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 72.16%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 71.74%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 72.40%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 72.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.56%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 74.54%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.45%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 76.08%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.62%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 77.93%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 78.60%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 78.31%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 78.21%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 78.21%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 78.45%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 79.01%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 80.03%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 80.52%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 80.68%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 80.56%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 80.16%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 79.65%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 79.30%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 79.21%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 78.88%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 78.80%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 78.85%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 79.13%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 78.82%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 78.30%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 78.24%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 78.07%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 77.59%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 77.33%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 77.29%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 77.46%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 77.22%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 76.98%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 76.56%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 76.44%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 76.14%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 75.84%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 76.01%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 75.82%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 75.71%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 75.35%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 75.09%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 74.74%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 74.49%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 74.58%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 74.42%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 74.43%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 74.28%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 74.13%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 74.06%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 73.92%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 73.40%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 72.67%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 71.80%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 70.96%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 70.13%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 69.40%   [EVAL] batch:   87 | acc: 43.75%,  total acc: 69.11%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 69.45%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 69.72%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 69.92%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 70.24%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 70.50%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 70.74%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 71.05%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 71.35%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 71.65%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 71.94%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 72.22%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 72.71%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 73.00%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 73.08%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 73.33%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 73.53%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 73.31%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 72.69%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 72.19%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 71.65%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 71.34%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 70.87%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 70.91%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 71.05%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 71.25%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 71.39%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 71.53%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 71.66%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 71.74%   [EVAL] batch:  119 | acc: 25.00%,  total acc: 71.35%   [EVAL] batch:  120 | acc: 37.50%,  total acc: 71.07%   [EVAL] batch:  121 | acc: 18.75%,  total acc: 70.65%   [EVAL] batch:  122 | acc: 37.50%,  total acc: 70.38%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 69.81%   [EVAL] batch:  124 | acc: 50.00%,  total acc: 69.65%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 69.30%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 68.95%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 68.85%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 68.90%   [EVAL] batch:  129 | acc: 37.50%,  total acc: 68.65%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 68.51%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 68.47%   [EVAL] batch:  132 | acc: 56.25%,  total acc: 68.37%   [EVAL] batch:  133 | acc: 43.75%,  total acc: 68.19%   [EVAL] batch:  134 | acc: 56.25%,  total acc: 68.10%   [EVAL] batch:  135 | acc: 68.75%,  total acc: 68.11%   [EVAL] batch:  136 | acc: 62.50%,  total acc: 68.07%   [EVAL] batch:  137 | acc: 43.75%,  total acc: 67.89%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 67.54%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 67.23%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 66.93%   [EVAL] batch:  141 | acc: 50.00%,  total acc: 66.81%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 66.61%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 66.41%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 66.59%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 66.82%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 67.01%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 67.23%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 67.45%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 67.62%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 67.18%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 66.74%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 66.30%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 65.87%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 65.56%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 65.18%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 65.21%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 65.39%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 65.57%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 65.74%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 65.88%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 66.05%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 66.22%   [EVAL] batch:  163 | acc: 50.00%,  total acc: 66.12%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 66.10%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 66.08%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 66.13%   [EVAL] batch:  167 | acc: 68.75%,  total acc: 66.15%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 66.09%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 66.07%   [EVAL] batch:  170 | acc: 68.75%,  total acc: 66.08%   [EVAL] batch:  171 | acc: 50.00%,  total acc: 65.99%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 65.82%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 65.80%   [EVAL] batch:  174 | acc: 68.75%,  total acc: 65.82%   [EVAL] batch:  175 | acc: 75.00%,  total acc: 65.87%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 65.89%   [EVAL] batch:  177 | acc: 87.50%,  total acc: 66.01%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 66.06%   [EVAL] batch:  179 | acc: 93.75%,  total acc: 66.22%   [EVAL] batch:  180 | acc: 81.25%,  total acc: 66.30%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 66.35%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 66.46%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 66.61%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 66.62%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 66.70%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 66.74%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 66.59%   [EVAL] batch:  188 | acc: 6.25%,  total acc: 66.27%   [EVAL] batch:  189 | acc: 12.50%,  total acc: 65.99%   [EVAL] batch:  190 | acc: 0.00%,  total acc: 65.64%   [EVAL] batch:  191 | acc: 6.25%,  total acc: 65.33%   [EVAL] batch:  192 | acc: 12.50%,  total acc: 65.06%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 64.82%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 64.87%   [EVAL] batch:  195 | acc: 87.50%,  total acc: 64.99%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 65.07%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 65.12%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 65.17%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 65.22%   [EVAL] batch:  200 | acc: 43.75%,  total acc: 65.11%   [EVAL] batch:  201 | acc: 12.50%,  total acc: 64.85%   [EVAL] batch:  202 | acc: 12.50%,  total acc: 64.59%   [EVAL] batch:  203 | acc: 18.75%,  total acc: 64.37%   [EVAL] batch:  204 | acc: 25.00%,  total acc: 64.18%   [EVAL] batch:  205 | acc: 18.75%,  total acc: 63.96%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 64.10%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 64.27%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 64.44%   [EVAL] batch:  209 | acc: 87.50%,  total acc: 64.55%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 64.69%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 64.86%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 64.96%   [EVAL] batch:  213 | acc: 87.50%,  total acc: 65.07%   [EVAL] batch:  214 | acc: 87.50%,  total acc: 65.17%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 65.34%   [EVAL] batch:  216 | acc: 81.25%,  total acc: 65.41%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 65.57%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 65.70%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 65.85%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 66.01%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 66.10%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 66.23%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 66.35%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 66.50%   [EVAL] batch:  225 | acc: 50.00%,  total acc: 66.43%   [EVAL] batch:  226 | acc: 68.75%,  total acc: 66.44%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 66.42%   [EVAL] batch:  228 | acc: 25.00%,  total acc: 66.24%   [EVAL] batch:  229 | acc: 62.50%,  total acc: 66.22%   [EVAL] batch:  230 | acc: 25.00%,  total acc: 66.04%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 66.14%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 66.26%   [EVAL] batch:  233 | acc: 81.25%,  total acc: 66.32%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 66.46%   [EVAL] batch:  235 | acc: 87.50%,  total acc: 66.55%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 66.61%   [EVAL] batch:  237 | acc: 56.25%,  total acc: 66.57%   [EVAL] batch:  238 | acc: 56.25%,  total acc: 66.53%   [EVAL] batch:  239 | acc: 43.75%,  total acc: 66.43%   [EVAL] batch:  240 | acc: 50.00%,  total acc: 66.36%   [EVAL] batch:  241 | acc: 68.75%,  total acc: 66.37%   [EVAL] batch:  242 | acc: 37.50%,  total acc: 66.26%   [EVAL] batch:  243 | acc: 31.25%,  total acc: 66.11%   [EVAL] batch:  244 | acc: 81.25%,  total acc: 66.17%   [EVAL] batch:  245 | acc: 62.50%,  total acc: 66.16%   [EVAL] batch:  246 | acc: 50.00%,  total acc: 66.09%   [EVAL] batch:  247 | acc: 75.00%,  total acc: 66.13%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 66.24%   [EVAL] batch:  249 | acc: 87.50%,  total acc: 66.33%   [EVAL] batch:  250 | acc: 6.25%,  total acc: 66.09%   [EVAL] batch:  251 | acc: 25.00%,  total acc: 65.92%   [EVAL] batch:  252 | acc: 0.00%,  total acc: 65.66%   [EVAL] batch:  253 | acc: 12.50%,  total acc: 65.45%   [EVAL] batch:  254 | acc: 12.50%,  total acc: 65.25%   [EVAL] batch:  255 | acc: 18.75%,  total acc: 65.06%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 65.03%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 65.07%   [EVAL] batch:  258 | acc: 87.50%,  total acc: 65.15%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 65.19%   [EVAL] batch:  260 | acc: 75.00%,  total acc: 65.23%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 65.29%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 65.38%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 65.41%   [EVAL] batch:  264 | acc: 87.50%,  total acc: 65.50%   [EVAL] batch:  265 | acc: 93.75%,  total acc: 65.60%   [EVAL] batch:  266 | acc: 87.50%,  total acc: 65.68%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 65.79%   [EVAL] batch:  268 | acc: 62.50%,  total acc: 65.78%   [EVAL] batch:  269 | acc: 25.00%,  total acc: 65.62%   [EVAL] batch:  270 | acc: 6.25%,  total acc: 65.41%   [EVAL] batch:  271 | acc: 18.75%,  total acc: 65.23%   [EVAL] batch:  272 | acc: 37.50%,  total acc: 65.13%   [EVAL] batch:  273 | acc: 12.50%,  total acc: 64.94%   [EVAL] batch:  274 | acc: 31.25%,  total acc: 64.82%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 64.61%   [EVAL] batch:  276 | acc: 12.50%,  total acc: 64.42%   [EVAL] batch:  277 | acc: 0.00%,  total acc: 64.19%   [EVAL] batch:  278 | acc: 6.25%,  total acc: 63.98%   [EVAL] batch:  279 | acc: 12.50%,  total acc: 63.79%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 63.57%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 63.59%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 63.67%   [EVAL] batch:  283 | acc: 75.00%,  total acc: 63.71%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 63.84%   [EVAL] batch:  285 | acc: 81.25%,  total acc: 63.90%   [EVAL] batch:  286 | acc: 81.25%,  total acc: 63.96%   [EVAL] batch:  287 | acc: 100.00%,  total acc: 64.08%   [EVAL] batch:  288 | acc: 93.75%,  total acc: 64.19%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 64.31%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 64.39%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 64.45%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 64.57%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 64.67%   [EVAL] batch:  294 | acc: 25.00%,  total acc: 64.53%   [EVAL] batch:  295 | acc: 37.50%,  total acc: 64.44%   [EVAL] batch:  296 | acc: 25.00%,  total acc: 64.31%   [EVAL] batch:  297 | acc: 0.00%,  total acc: 64.09%   [EVAL] batch:  298 | acc: 62.50%,  total acc: 64.09%   [EVAL] batch:  299 | acc: 18.75%,  total acc: 63.94%   [EVAL] batch:  300 | acc: 56.25%,  total acc: 63.91%   [EVAL] batch:  301 | acc: 75.00%,  total acc: 63.95%   [EVAL] batch:  302 | acc: 56.25%,  total acc: 63.92%   [EVAL] batch:  303 | acc: 75.00%,  total acc: 63.96%   [EVAL] batch:  304 | acc: 68.75%,  total acc: 63.98%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 64.01%   [EVAL] batch:  306 | acc: 43.75%,  total acc: 63.95%   [EVAL] batch:  307 | acc: 43.75%,  total acc: 63.88%   [EVAL] batch:  308 | acc: 56.25%,  total acc: 63.86%   [EVAL] batch:  309 | acc: 62.50%,  total acc: 63.85%   [EVAL] batch:  310 | acc: 50.00%,  total acc: 63.81%   [EVAL] batch:  311 | acc: 68.75%,  total acc: 63.82%   [EVAL] batch:  312 | acc: 31.25%,  total acc: 63.72%   [EVAL] batch:  313 | acc: 56.25%,  total acc: 63.69%   [EVAL] batch:  314 | acc: 25.00%,  total acc: 63.57%   [EVAL] batch:  315 | acc: 37.50%,  total acc: 63.49%   [EVAL] batch:  316 | acc: 56.25%,  total acc: 63.47%   [EVAL] batch:  317 | acc: 62.50%,  total acc: 63.46%   [EVAL] batch:  318 | acc: 68.75%,  total acc: 63.48%   [EVAL] batch:  319 | acc: 93.75%,  total acc: 63.57%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 63.67%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 63.70%   [EVAL] batch:  322 | acc: 81.25%,  total acc: 63.76%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 63.81%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 63.88%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 63.90%   [EVAL] batch:  326 | acc: 50.00%,  total acc: 63.86%   [EVAL] batch:  327 | acc: 56.25%,  total acc: 63.83%   [EVAL] batch:  328 | acc: 68.75%,  total acc: 63.85%   [EVAL] batch:  329 | acc: 68.75%,  total acc: 63.86%   [EVAL] batch:  330 | acc: 62.50%,  total acc: 63.86%   [EVAL] batch:  331 | acc: 50.00%,  total acc: 63.82%   [EVAL] batch:  332 | acc: 50.00%,  total acc: 63.78%   [EVAL] batch:  333 | acc: 50.00%,  total acc: 63.74%   [EVAL] batch:  334 | acc: 31.25%,  total acc: 63.64%   [EVAL] batch:  335 | acc: 50.00%,  total acc: 63.60%   [EVAL] batch:  336 | acc: 31.25%,  total acc: 63.50%   [EVAL] batch:  337 | acc: 56.25%,  total acc: 63.48%   [EVAL] batch:  338 | acc: 75.00%,  total acc: 63.51%   [EVAL] batch:  339 | acc: 62.50%,  total acc: 63.51%   [EVAL] batch:  340 | acc: 75.00%,  total acc: 63.54%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 63.58%   [EVAL] batch:  342 | acc: 68.75%,  total acc: 63.59%   [EVAL] batch:  343 | acc: 50.00%,  total acc: 63.55%   [EVAL] batch:  344 | acc: 62.50%,  total acc: 63.55%   [EVAL] batch:  345 | acc: 56.25%,  total acc: 63.53%   [EVAL] batch:  346 | acc: 50.00%,  total acc: 63.49%   [EVAL] batch:  347 | acc: 62.50%,  total acc: 63.49%   [EVAL] batch:  348 | acc: 81.25%,  total acc: 63.54%   [EVAL] batch:  349 | acc: 62.50%,  total acc: 63.54%   [EVAL] batch:  350 | acc: 81.25%,  total acc: 63.59%   [EVAL] batch:  351 | acc: 75.00%,  total acc: 63.62%   [EVAL] batch:  352 | acc: 81.25%,  total acc: 63.67%   [EVAL] batch:  353 | acc: 75.00%,  total acc: 63.70%   [EVAL] batch:  354 | acc: 81.25%,  total acc: 63.75%   [EVAL] batch:  355 | acc: 87.50%,  total acc: 63.82%   [EVAL] batch:  356 | acc: 56.25%,  total acc: 63.80%   [EVAL] batch:  357 | acc: 25.00%,  total acc: 63.69%   [EVAL] batch:  358 | acc: 31.25%,  total acc: 63.60%   [EVAL] batch:  359 | acc: 43.75%,  total acc: 63.54%   [EVAL] batch:  360 | acc: 37.50%,  total acc: 63.47%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 63.43%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 63.45%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 63.55%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 63.65%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 63.75%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 63.85%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 63.94%   [EVAL] batch:  368 | acc: 87.50%,  total acc: 64.01%   [EVAL] batch:  369 | acc: 50.00%,  total acc: 63.97%   [EVAL] batch:  370 | acc: 37.50%,  total acc: 63.90%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 63.86%   [EVAL] batch:  372 | acc: 68.75%,  total acc: 63.87%   [EVAL] batch:  373 | acc: 50.00%,  total acc: 63.84%   [EVAL] batch:  374 | acc: 56.25%,  total acc: 63.82%   [EVAL] batch:  375 | acc: 37.50%,  total acc: 63.75%   [EVAL] batch:  376 | acc: 50.00%,  total acc: 63.71%   [EVAL] batch:  377 | acc: 50.00%,  total acc: 63.67%   [EVAL] batch:  378 | acc: 43.75%,  total acc: 63.62%   [EVAL] batch:  379 | acc: 56.25%,  total acc: 63.60%   [EVAL] batch:  380 | acc: 75.00%,  total acc: 63.63%   [EVAL] batch:  381 | acc: 50.00%,  total acc: 63.60%   [EVAL] batch:  382 | acc: 75.00%,  total acc: 63.63%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 63.64%   [EVAL] batch:  384 | acc: 81.25%,  total acc: 63.69%   [EVAL] batch:  385 | acc: 50.00%,  total acc: 63.65%   [EVAL] batch:  386 | acc: 68.75%,  total acc: 63.66%   [EVAL] batch:  387 | acc: 93.75%,  total acc: 63.74%   [EVAL] batch:  388 | acc: 68.75%,  total acc: 63.75%   [EVAL] batch:  389 | acc: 75.00%,  total acc: 63.78%   [EVAL] batch:  390 | acc: 75.00%,  total acc: 63.81%   [EVAL] batch:  391 | acc: 87.50%,  total acc: 63.87%   [EVAL] batch:  392 | acc: 100.00%,  total acc: 63.96%   [EVAL] batch:  393 | acc: 93.75%,  total acc: 64.04%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 64.11%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 64.20%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 64.29%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 64.38%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 64.46%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 64.55%   [EVAL] batch:  400 | acc: 75.00%,  total acc: 64.57%   [EVAL] batch:  401 | acc: 75.00%,  total acc: 64.60%   [EVAL] batch:  402 | acc: 81.25%,  total acc: 64.64%   [EVAL] batch:  403 | acc: 56.25%,  total acc: 64.62%   [EVAL] batch:  404 | acc: 75.00%,  total acc: 64.65%   [EVAL] batch:  405 | acc: 75.00%,  total acc: 64.67%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 64.67%   [EVAL] batch:  407 | acc: 62.50%,  total acc: 64.66%   [EVAL] batch:  408 | acc: 25.00%,  total acc: 64.56%   [EVAL] batch:  409 | acc: 31.25%,  total acc: 64.48%   [EVAL] batch:  410 | acc: 18.75%,  total acc: 64.37%   [EVAL] batch:  411 | acc: 18.75%,  total acc: 64.26%   [EVAL] batch:  412 | acc: 68.75%,  total acc: 64.27%   [EVAL] batch:  413 | acc: 68.75%,  total acc: 64.28%   [EVAL] batch:  414 | acc: 68.75%,  total acc: 64.29%   [EVAL] batch:  415 | acc: 68.75%,  total acc: 64.30%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 64.34%   [EVAL] batch:  417 | acc: 75.00%,  total acc: 64.37%   [EVAL] batch:  418 | acc: 75.00%,  total acc: 64.39%   [EVAL] batch:  419 | acc: 87.50%,  total acc: 64.45%   [EVAL] batch:  420 | acc: 75.00%,  total acc: 64.47%   [EVAL] batch:  421 | acc: 68.75%,  total acc: 64.48%   [EVAL] batch:  422 | acc: 56.25%,  total acc: 64.47%   [EVAL] batch:  423 | acc: 75.00%,  total acc: 64.49%   [EVAL] batch:  424 | acc: 87.50%,  total acc: 64.54%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 64.63%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 64.71%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 64.79%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 64.87%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 64.96%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 65.04%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 65.10%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 65.16%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 65.21%   [EVAL] batch:  434 | acc: 93.75%,  total acc: 65.27%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 65.32%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 65.40%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 65.37%   
cur_acc:  ['0.9544', '0.8204', '0.7480', '0.6905', '0.6488', '0.6954', '0.7460']
his_acc:  ['0.9544', '0.8805', '0.8005', '0.7270', '0.6961', '0.6715', '0.6537']
CurrentTrain: epoch 15, batch     0 | loss: 12.2408841CurrentTrain: epoch 15, batch     1 | loss: 16.9788729CurrentTrain: epoch 15, batch     2 | loss: 14.4768056CurrentTrain: epoch  1, batch     3 | loss: 7.8067477CurrentTrain: epoch 15, batch     0 | loss: 8.9676013CurrentTrain: epoch 15, batch     1 | loss: 8.5068052CurrentTrain: epoch 15, batch     2 | loss: 14.6024626CurrentTrain: epoch  1, batch     3 | loss: 5.6067209CurrentTrain: epoch 15, batch     0 | loss: 20.1647640CurrentTrain: epoch 15, batch     1 | loss: 10.4703558CurrentTrain: epoch 15, batch     2 | loss: 9.0807842CurrentTrain: epoch  1, batch     3 | loss: 5.4790063CurrentTrain: epoch 15, batch     0 | loss: 17.3625748CurrentTrain: epoch 15, batch     1 | loss: 17.7498463CurrentTrain: epoch 15, batch     2 | loss: 13.9743575CurrentTrain: epoch  1, batch     3 | loss: 7.2981242CurrentTrain: epoch 15, batch     0 | loss: 8.9691057CurrentTrain: epoch 15, batch     1 | loss: 13.2113840CurrentTrain: epoch 15, batch     2 | loss: 6.1321916CurrentTrain: epoch  1, batch     3 | loss: 8.6058042CurrentTrain: epoch 15, batch     0 | loss: 13.6186777CurrentTrain: epoch 15, batch     1 | loss: 6.3610343CurrentTrain: epoch 15, batch     2 | loss: 8.9582285CurrentTrain: epoch  1, batch     3 | loss: 14.8631487CurrentTrain: epoch 15, batch     0 | loss: 8.7463836CurrentTrain: epoch 15, batch     1 | loss: 6.5176875CurrentTrain: epoch 15, batch     2 | loss: 7.0091580CurrentTrain: epoch  1, batch     3 | loss: 5.5989764CurrentTrain: epoch 15, batch     0 | loss: 10.3287720CurrentTrain: epoch 15, batch     1 | loss: 6.3117972CurrentTrain: epoch 15, batch     2 | loss: 8.0340719CurrentTrain: epoch  1, batch     3 | loss: 6.0583157CurrentTrain: epoch 15, batch     0 | loss: 5.6403653CurrentTrain: epoch 15, batch     1 | loss: 4.8974292CurrentTrain: epoch 15, batch     2 | loss: 11.7632349CurrentTrain: epoch  1, batch     3 | loss: 6.5319763CurrentTrain: epoch 15, batch     0 | loss: 6.8602622CurrentTrain: epoch 15, batch     1 | loss: 6.3712624CurrentTrain: epoch 15, batch     2 | loss: 9.5262629CurrentTrain: epoch  1, batch     3 | loss: 7.2854193
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: alongside evan durbin and hugh gaitskell , he brought the thinking of john maynard keynes to the labour party , especially in relation to price determination .
Head Entity: hugh gaitskell
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: After serving as the mayor of Chicago, he became a prominent figure in the Democratic Party, advocating for social justice and economic reform.  
Head Entity: mayor of Chicago  
Tail Entity: Democratic Party  

Relation: member of political party  
Context: Throughout her career, she has been a staunch advocate for environmental policies as a member of the Green Party, pushing for sustainable practices at both local and national levels.  
Head Entity: she  
Tail Entity: Green Party  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: it is a partly fictional biography , adapted by shane connaughton and jim sheridan from the book " my left foot " by christy brown .
Head Entity: my left foot
Tail Entity: christy brown
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film is based on the novel "the great gatsby" written by f. scott fitzgerald, which explores themes of wealth and love in the 1920s.  
Head Entity: the great gatsby  
Tail Entity: f. scott fitzgerald  

Relation: after a work by  
Context: this musical is inspired by the play "hamlet" by william shakespeare, reimagining the classic tragedy in a contemporary setting.  
Head Entity: hamlet  
Tail Entity: william shakespeare  
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: the influential victorian art critic john ruskin expounded theories about social justice in " unto this last " ( 1860 ) .
Head Entity: john ruskin
Tail Entity: unto this last
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: the renowned physicist albert einstein published his groundbreaking theory of relativity in "on the electrodynamics of moving bodies" (1905).  
Head Entity: albert einstein  
Tail Entity: on the electrodynamics of moving bodies  

Relation: notable work  
Context: the celebrated author j.k. rowling captivated readers with her fantasy series, particularly in "harry potter and the philosopher's stone" (1997).  
Head Entity: j.k. rowling  
Tail Entity: harry potter and the philosopher's stone  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: at the request of the stockholm institute for bible translation , mirza khazar translated the bible – the new testament and old testament – into the azerbaijani language .
Head Entity: institute for bible translation
Tail Entity: stockholm
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: the global technology company, headquartered in san francisco, has been a leader in software development for over two decades.  
Head Entity: global technology company  
Tail Entity: san francisco  

Relation: headquarters location  
Context: after relocating its main office to new york city, the fashion brand saw a significant increase in sales and brand recognition.  
Head Entity: fashion brand  
Tail Entity: new york city  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most recognized as the wolf, which belongs to the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of plants, the family Rosaceae encompasses a variety of species, including roses and strawberries, which are categorized under the order Rosales.  
Head Entity: Rosaceae  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: elspeth frances garman is professor of molecular biophysics at the university of oxford and a former president of the british crystallographic association .
Head Entity: elspeth frances garman
Tail Entity: molecular biophysics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: dr. amanda jones is a leading researcher in artificial intelligence and currently works at the tech innovation lab in silicon valley.  
Head Entity: dr. amanda jones  
Tail Entity: artificial intelligence  

Relation: field of work  
Context: the world health organization has appointed dr. rajesh kumar as the new director of global health initiatives, focusing on infectious diseases.  
Head Entity: dr. rajesh kumar  
Tail Entity: infectious diseases  
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: this early involvement in radio led him to buy many small , under performing radio stations beginning with wcos in columbia , south carolina , and turn them around with new formats and improved management .
Head Entity: wcos
Tail Entity: columbia , south carolina
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: The new regulations allowed the station KXYZ to expand its reach, enabling it to serve the community of Springfield more effectively.  
Head Entity: KXYZ  
Tail Entity: Springfield  

Relation: licensed to broadcast to  
Context: After acquiring the rights, the network announced that WABC would now be able to broadcast its programs to the entire region of New York City.  
Head Entity: WABC  
Tail Entity: New York City  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: ngc 192 is a type sba spiral galaxy located in the constellation cetus .
Head Entity: ngc 192
Tail Entity: cetus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: the orion nebula is located in the constellation orion and is one of the brightest nebulae visible to the naked eye.  
Head Entity: orion nebula  
Tail Entity: orion  

Relation: constellation  
Context: the star sirius is part of the constellation canis major, which is known for its prominence in the night sky.  
Head Entity: sirius  
Tail Entity: canis major  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: services to the island run from san juan 's luis muñoz marín international airport , ceiba airport or isla grande airport ( 20- to 30-minute flight ) .
Head Entity: isla grande airport
Tail Entity: san juan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The city of Barcelona is well-connected to various destinations through the El Prat Airport, which offers flights to numerous European cities.  
Head Entity: El Prat Airport  
Tail Entity: Barcelona  

Relation: place served by transport hub  
Context: The central train station in Zurich provides access to multiple Swiss cities and international destinations, making it a key transport hub in the region.  
Head Entity: Zurich central train station  
Tail Entity: Zurich  
MemoryTrain:  epoch 15, batch     0 | loss: 2.1259656MemoryTrain:  epoch 15, batch     1 | loss: 2.0446262MemoryTrain:  epoch 15, batch     2 | loss: 2.1894189MemoryTrain:  epoch 15, batch     3 | loss: 2.0892969MemoryTrain:  epoch 15, batch     4 | loss: 3.4747987MemoryTrain:  epoch 15, batch     5 | loss: 2.7498525MemoryTrain:  epoch 15, batch     6 | loss: 2.1842612MemoryTrain:  epoch 15, batch     7 | loss: 2.3841543MemoryTrain:  epoch 15, batch     8 | loss: 2.1965734MemoryTrain:  epoch 15, batch     9 | loss: 3.0757250MemoryTrain:  epoch 15, batch    10 | loss: 2.9254813MemoryTrain:  epoch 15, batch    11 | loss: 4.1685599MemoryTrain:  epoch 15, batch    12 | loss: 2.1650566MemoryTrain:  epoch 15, batch    13 | loss: 2.5945836MemoryTrain:  epoch 15, batch    14 | loss: 2.6134507MemoryTrain:  epoch 15, batch     0 | loss: 2.0164114MemoryTrain:  epoch 15, batch     1 | loss: 3.1295029MemoryTrain:  epoch 15, batch     2 | loss: 1.6612296MemoryTrain:  epoch 15, batch     3 | loss: 1.7778573MemoryTrain:  epoch 15, batch     4 | loss: 1.8171296MemoryTrain:  epoch 15, batch     5 | loss: 2.3896771MemoryTrain:  epoch 15, batch     6 | loss: 1.5228913MemoryTrain:  epoch 15, batch     7 | loss: 2.4074032MemoryTrain:  epoch 15, batch     8 | loss: 4.6613645MemoryTrain:  epoch 15, batch     9 | loss: 2.3601508MemoryTrain:  epoch 15, batch    10 | loss: 1.7659560MemoryTrain:  epoch 15, batch    11 | loss: 1.8899537MemoryTrain:  epoch 15, batch    12 | loss: 1.8355741MemoryTrain:  epoch 15, batch    13 | loss: 2.0663772MemoryTrain:  epoch 15, batch    14 | loss: 1.9291892MemoryTrain:  epoch 15, batch     0 | loss: 1.7977359MemoryTrain:  epoch 15, batch     1 | loss: 1.6966225MemoryTrain:  epoch 15, batch     2 | loss: 1.8709746MemoryTrain:  epoch 15, batch     3 | loss: 1.6145753MemoryTrain:  epoch 15, batch     4 | loss: 2.3614656MemoryTrain:  epoch 15, batch     5 | loss: 1.6208675MemoryTrain:  epoch 15, batch     6 | loss: 1.6981719MemoryTrain:  epoch 15, batch     7 | loss: 1.5528823MemoryTrain:  epoch 15, batch     8 | loss: 2.4478048MemoryTrain:  epoch 15, batch     9 | loss: 1.9847604MemoryTrain:  epoch 15, batch    10 | loss: 1.6866196MemoryTrain:  epoch 15, batch    11 | loss: 1.7850828MemoryTrain:  epoch 15, batch    12 | loss: 1.4212864MemoryTrain:  epoch 15, batch    13 | loss: 4.3089920MemoryTrain:  epoch 15, batch    14 | loss: 2.0563460MemoryTrain:  epoch 15, batch     0 | loss: 1.4894374MemoryTrain:  epoch 15, batch     1 | loss: 1.6951700MemoryTrain:  epoch 15, batch     2 | loss: 1.3859322MemoryTrain:  epoch 15, batch     3 | loss: 2.7754638MemoryTrain:  epoch 15, batch     4 | loss: 2.1898163MemoryTrain:  epoch 15, batch     5 | loss: 4.0337604MemoryTrain:  epoch 15, batch     6 | loss: 2.5243609MemoryTrain:  epoch 15, batch     7 | loss: 2.7381569MemoryTrain:  epoch 15, batch     8 | loss: 1.5209307MemoryTrain:  epoch 15, batch     9 | loss: 1.5975863MemoryTrain:  epoch 15, batch    10 | loss: 2.3517417MemoryTrain:  epoch 15, batch    11 | loss: 1.5057930MemoryTrain:  epoch 15, batch    12 | loss: 1.7115111MemoryTrain:  epoch 15, batch    13 | loss: 1.4786846MemoryTrain:  epoch 15, batch    14 | loss: 2.0150406MemoryTrain:  epoch 15, batch     0 | loss: 1.4672939MemoryTrain:  epoch 15, batch     1 | loss: 4.2077940MemoryTrain:  epoch 15, batch     2 | loss: 1.6543863MemoryTrain:  epoch 15, batch     3 | loss: 2.2184952MemoryTrain:  epoch 15, batch     4 | loss: 4.1614134MemoryTrain:  epoch 15, batch     5 | loss: 2.2116169MemoryTrain:  epoch 15, batch     6 | loss: 1.5166853MemoryTrain:  epoch 15, batch     7 | loss: 1.2861749MemoryTrain:  epoch 15, batch     8 | loss: 4.8543830MemoryTrain:  epoch 15, batch     9 | loss: 1.7017072MemoryTrain:  epoch 15, batch    10 | loss: 1.7009859MemoryTrain:  epoch 15, batch    11 | loss: 2.1526008MemoryTrain:  epoch 15, batch    12 | loss: 1.8120760MemoryTrain:  epoch 15, batch    13 | loss: 1.5122555MemoryTrain:  epoch 15, batch    14 | loss: 6.7531210MemoryTrain:  epoch 15, batch     0 | loss: 1.5754358MemoryTrain:  epoch 15, batch     1 | loss: 1.4366427MemoryTrain:  epoch 15, batch     2 | loss: 1.6481618MemoryTrain:  epoch 15, batch     3 | loss: 3.8403948MemoryTrain:  epoch 15, batch     4 | loss: 3.5266540MemoryTrain:  epoch 15, batch     5 | loss: 1.3935166MemoryTrain:  epoch 15, batch     6 | loss: 2.0658978MemoryTrain:  epoch 15, batch     7 | loss: 4.3645315MemoryTrain:  epoch 15, batch     8 | loss: 1.5279413MemoryTrain:  epoch 15, batch     9 | loss: 1.5399832MemoryTrain:  epoch 15, batch    10 | loss: 1.5718429MemoryTrain:  epoch 15, batch    11 | loss: 2.6588635MemoryTrain:  epoch 15, batch    12 | loss: 3.6137091MemoryTrain:  epoch 15, batch    13 | loss: 1.9228978MemoryTrain:  epoch 15, batch    14 | loss: 1.4911516MemoryTrain:  epoch 15, batch     0 | loss: 1.3952533MemoryTrain:  epoch 15, batch     1 | loss: 1.9001699MemoryTrain:  epoch 15, batch     2 | loss: 1.5579175MemoryTrain:  epoch 15, batch     3 | loss: 1.4421902MemoryTrain:  epoch 15, batch     4 | loss: 1.4117342MemoryTrain:  epoch 15, batch     5 | loss: 1.5128580MemoryTrain:  epoch 15, batch     6 | loss: 2.5022330MemoryTrain:  epoch 15, batch     7 | loss: 1.5031583MemoryTrain:  epoch 15, batch     8 | loss: 1.3328589MemoryTrain:  epoch 15, batch     9 | loss: 1.8754232MemoryTrain:  epoch 15, batch    10 | loss: 1.4346955MemoryTrain:  epoch 15, batch    11 | loss: 1.8439987MemoryTrain:  epoch 15, batch    12 | loss: 4.0458279MemoryTrain:  epoch 15, batch    13 | loss: 1.5546463MemoryTrain:  epoch 15, batch    14 | loss: 1.4226820MemoryTrain:  epoch 15, batch     0 | loss: 1.3547950MemoryTrain:  epoch 15, batch     1 | loss: 1.4886893MemoryTrain:  epoch 15, batch     2 | loss: 1.3376781MemoryTrain:  epoch 15, batch     3 | loss: 1.3636895MemoryTrain:  epoch 15, batch     4 | loss: 2.1525179MemoryTrain:  epoch 15, batch     5 | loss: 1.4417217MemoryTrain:  epoch 15, batch     6 | loss: 1.5035284MemoryTrain:  epoch 15, batch     7 | loss: 1.6274847MemoryTrain:  epoch 15, batch     8 | loss: 1.3948286MemoryTrain:  epoch 15, batch     9 | loss: 1.6667677MemoryTrain:  epoch 15, batch    10 | loss: 1.5583287MemoryTrain:  epoch 15, batch    11 | loss: 1.2938996MemoryTrain:  epoch 15, batch    12 | loss: 1.2666097MemoryTrain:  epoch 15, batch    13 | loss: 3.7838351MemoryTrain:  epoch 15, batch    14 | loss: 1.5291209MemoryTrain:  epoch 15, batch     0 | loss: 1.2875078MemoryTrain:  epoch 15, batch     1 | loss: 3.9503765MemoryTrain:  epoch 15, batch     2 | loss: 2.3260745MemoryTrain:  epoch 15, batch     3 | loss: 1.6699175MemoryTrain:  epoch 15, batch     4 | loss: 4.6883549MemoryTrain:  epoch 15, batch     5 | loss: 1.3943541MemoryTrain:  epoch 15, batch     6 | loss: 1.6086237MemoryTrain:  epoch 15, batch     7 | loss: 1.4602434MemoryTrain:  epoch 15, batch     8 | loss: 4.0189426MemoryTrain:  epoch 15, batch     9 | loss: 2.1795392MemoryTrain:  epoch 15, batch    10 | loss: 1.6767330MemoryTrain:  epoch 15, batch    11 | loss: 1.6066591MemoryTrain:  epoch 15, batch    12 | loss: 1.8801840MemoryTrain:  epoch 15, batch    13 | loss: 1.4832360MemoryTrain:  epoch 15, batch    14 | loss: 1.5304781MemoryTrain:  epoch 15, batch     0 | loss: 1.4367491MemoryTrain:  epoch 15, batch     1 | loss: 1.5323044MemoryTrain:  epoch 15, batch     2 | loss: 1.3165771MemoryTrain:  epoch 15, batch     3 | loss: 1.2681923MemoryTrain:  epoch 15, batch     4 | loss: 1.2916936MemoryTrain:  epoch 15, batch     5 | loss: 1.4801899MemoryTrain:  epoch 15, batch     6 | loss: 1.2697691MemoryTrain:  epoch 15, batch     7 | loss: 3.5329127MemoryTrain:  epoch 15, batch     8 | loss: 1.6598203MemoryTrain:  epoch 15, batch     9 | loss: 1.4586819MemoryTrain:  epoch 15, batch    10 | loss: 1.6465411MemoryTrain:  epoch 15, batch    11 | loss: 3.6502717MemoryTrain:  epoch 15, batch    12 | loss: 3.9759440MemoryTrain:  epoch 15, batch    13 | loss: 1.5104289MemoryTrain:  epoch 15, batch    14 | loss: 2.0059333
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 84.13%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 83.04%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 79.69%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 77.94%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 77.08%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 75.99%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 74.69%   [EVAL] batch:   20 | acc: 56.25%,  total acc: 73.81%   [EVAL] batch:   21 | acc: 56.25%,  total acc: 73.01%   [EVAL] batch:   22 | acc: 43.75%,  total acc: 71.74%   [EVAL] batch:   23 | acc: 43.75%,  total acc: 70.57%   [EVAL] batch:   24 | acc: 43.75%,  total acc: 69.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 70.67%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 71.76%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 72.77%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 73.71%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 74.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 75.40%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 75.39%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 75.38%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 74.82%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 74.82%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 74.48%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 73.99%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 73.85%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 74.52%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 75.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 75.76%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 76.34%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 76.89%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 77.27%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 77.78%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 78.26%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 78.72%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 79.59%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 79.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 80.15%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 80.41%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 80.66%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 80.90%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 81.02%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 81.36%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 81.58%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 81.79%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 81.99%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 82.29%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 82.58%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 82.76%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 82.24%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 68.75%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 68.06%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 66.25%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 63.64%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 61.98%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 62.02%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 64.29%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 65.83%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 67.58%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 69.49%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 72.04%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 71.56%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 71.73%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 71.31%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 70.92%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 71.61%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 72.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.08%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 74.07%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 75.65%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.22%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 77.54%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 77.84%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 77.21%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 76.79%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 76.52%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 76.64%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 77.24%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 77.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 78.35%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 78.57%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 78.92%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 79.12%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 78.89%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 78.26%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 77.66%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 77.21%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 77.17%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 76.75%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 76.72%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 76.80%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 76.77%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 76.39%   [EVAL] batch:   54 | acc: 43.75%,  total acc: 75.80%   [EVAL] batch:   55 | acc: 56.25%,  total acc: 75.45%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 75.33%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 74.89%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 74.58%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 74.69%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 74.80%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 74.70%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 74.31%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 73.54%   [EVAL] batch:   64 | acc: 56.25%,  total acc: 73.27%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 73.01%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 72.57%   [EVAL] batch:   67 | acc: 62.50%,  total acc: 72.43%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 72.01%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 71.96%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 71.65%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 71.44%   [EVAL] batch:   72 | acc: 56.25%,  total acc: 71.23%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 70.95%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 71.00%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 70.89%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 70.94%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 70.91%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 70.81%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 70.86%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 70.99%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 70.43%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 69.73%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 68.90%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 68.09%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 67.37%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 66.74%   [EVAL] batch:   87 | acc: 43.75%,  total acc: 66.48%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 66.78%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 67.15%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 67.24%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 67.53%   [EVAL] batch:   92 | acc: 81.25%,  total acc: 67.67%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 67.89%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 68.22%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 68.55%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 68.88%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 69.20%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 69.51%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 69.81%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 70.05%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 70.28%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 70.33%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 70.43%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 70.65%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 70.87%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 70.68%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 70.14%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 69.61%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 69.15%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 68.98%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 68.58%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 68.58%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 68.97%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 69.13%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 69.28%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 69.44%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 69.49%   [EVAL] batch:  119 | acc: 25.00%,  total acc: 69.11%   [EVAL] batch:  120 | acc: 31.25%,  total acc: 68.80%   [EVAL] batch:  121 | acc: 18.75%,  total acc: 68.39%   [EVAL] batch:  122 | acc: 43.75%,  total acc: 68.19%   [EVAL] batch:  123 | acc: 12.50%,  total acc: 67.74%   [EVAL] batch:  124 | acc: 43.75%,  total acc: 67.55%   [EVAL] batch:  125 | acc: 18.75%,  total acc: 67.16%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 66.83%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 66.50%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 66.38%   [EVAL] batch:  129 | acc: 37.50%,  total acc: 66.15%   [EVAL] batch:  130 | acc: 31.25%,  total acc: 65.89%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 65.86%   [EVAL] batch:  132 | acc: 56.25%,  total acc: 65.79%   [EVAL] batch:  133 | acc: 43.75%,  total acc: 65.62%   [EVAL] batch:  134 | acc: 56.25%,  total acc: 65.56%   [EVAL] batch:  135 | acc: 68.75%,  total acc: 65.58%   [EVAL] batch:  136 | acc: 56.25%,  total acc: 65.51%   [EVAL] batch:  137 | acc: 37.50%,  total acc: 65.31%   [EVAL] batch:  138 | acc: 6.25%,  total acc: 64.88%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 64.55%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 64.27%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 64.13%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 63.94%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 63.76%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 63.97%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 64.21%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 64.41%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 64.65%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 64.89%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 65.08%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 64.65%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 64.27%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 63.89%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 63.51%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 63.19%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 62.82%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 62.82%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 63.01%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 63.21%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 63.40%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 63.55%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 63.73%   [EVAL] batch:  162 | acc: 75.00%,  total acc: 63.80%   [EVAL] batch:  163 | acc: 18.75%,  total acc: 63.53%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 63.48%   [EVAL] batch:  165 | acc: 31.25%,  total acc: 63.29%   [EVAL] batch:  166 | acc: 56.25%,  total acc: 63.25%   [EVAL] batch:  167 | acc: 50.00%,  total acc: 63.17%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 63.13%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 63.12%   [EVAL] batch:  170 | acc: 68.75%,  total acc: 63.16%   [EVAL] batch:  171 | acc: 62.50%,  total acc: 63.15%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 63.04%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 63.07%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 63.18%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 63.17%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 63.10%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 63.06%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 63.06%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 63.19%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 63.23%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 63.29%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 63.42%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 63.59%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 63.68%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 63.88%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 63.94%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 63.86%   [EVAL] batch:  188 | acc: 6.25%,  total acc: 63.56%   [EVAL] batch:  189 | acc: 12.50%,  total acc: 63.29%   [EVAL] batch:  190 | acc: 0.00%,  total acc: 62.96%   [EVAL] batch:  191 | acc: 6.25%,  total acc: 62.66%   [EVAL] batch:  192 | acc: 12.50%,  total acc: 62.40%   [EVAL] batch:  193 | acc: 25.00%,  total acc: 62.21%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 62.24%   [EVAL] batch:  195 | acc: 93.75%,  total acc: 62.40%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 62.63%   [EVAL] batch:  198 | acc: 87.50%,  total acc: 62.75%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 62.88%   [EVAL] batch:  200 | acc: 43.75%,  total acc: 62.78%   [EVAL] batch:  201 | acc: 6.25%,  total acc: 62.50%   [EVAL] batch:  202 | acc: 12.50%,  total acc: 62.25%   [EVAL] batch:  203 | acc: 25.00%,  total acc: 62.07%   [EVAL] batch:  204 | acc: 31.25%,  total acc: 61.92%   [EVAL] batch:  205 | acc: 6.25%,  total acc: 61.65%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 61.78%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 61.96%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 62.14%   [EVAL] batch:  209 | acc: 87.50%,  total acc: 62.26%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 62.41%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 62.59%   [EVAL] batch:  212 | acc: 100.00%,  total acc: 62.76%   [EVAL] batch:  213 | acc: 81.25%,  total acc: 62.85%   [EVAL] batch:  214 | acc: 81.25%,  total acc: 62.94%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 63.08%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 63.13%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 63.30%   [EVAL] batch:  218 | acc: 81.25%,  total acc: 63.38%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 63.55%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 63.72%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 63.82%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 63.96%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 64.09%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 64.25%   [EVAL] batch:  225 | acc: 43.75%,  total acc: 64.16%   [EVAL] batch:  226 | acc: 68.75%,  total acc: 64.18%   [EVAL] batch:  227 | acc: 56.25%,  total acc: 64.14%   [EVAL] batch:  228 | acc: 25.00%,  total acc: 63.97%   [EVAL] batch:  229 | acc: 50.00%,  total acc: 63.91%   [EVAL] batch:  230 | acc: 12.50%,  total acc: 63.69%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 63.79%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 63.92%   [EVAL] batch:  233 | acc: 87.50%,  total acc: 64.02%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 64.18%   [EVAL] batch:  235 | acc: 93.75%,  total acc: 64.30%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 64.37%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 64.36%   [EVAL] batch:  238 | acc: 56.25%,  total acc: 64.33%   [EVAL] batch:  239 | acc: 50.00%,  total acc: 64.27%   [EVAL] batch:  240 | acc: 50.00%,  total acc: 64.21%   [EVAL] batch:  241 | acc: 68.75%,  total acc: 64.23%   [EVAL] batch:  242 | acc: 43.75%,  total acc: 64.15%   [EVAL] batch:  243 | acc: 31.25%,  total acc: 64.01%   [EVAL] batch:  244 | acc: 75.00%,  total acc: 64.06%   [EVAL] batch:  245 | acc: 56.25%,  total acc: 64.02%   [EVAL] batch:  246 | acc: 37.50%,  total acc: 63.92%   [EVAL] batch:  247 | acc: 75.00%,  total acc: 63.96%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 64.08%   [EVAL] batch:  249 | acc: 56.25%,  total acc: 64.05%   [EVAL] batch:  250 | acc: 6.25%,  total acc: 63.82%   [EVAL] batch:  251 | acc: 18.75%,  total acc: 63.64%   [EVAL] batch:  252 | acc: 6.25%,  total acc: 63.41%   [EVAL] batch:  253 | acc: 12.50%,  total acc: 63.21%   [EVAL] batch:  254 | acc: 18.75%,  total acc: 63.04%   [EVAL] batch:  255 | acc: 6.25%,  total acc: 62.82%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 62.82%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 62.89%   [EVAL] batch:  258 | acc: 87.50%,  total acc: 62.98%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 63.03%   [EVAL] batch:  260 | acc: 75.00%,  total acc: 63.07%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 63.14%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 63.21%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 63.28%   [EVAL] batch:  264 | acc: 87.50%,  total acc: 63.37%   [EVAL] batch:  265 | acc: 87.50%,  total acc: 63.46%   [EVAL] batch:  266 | acc: 87.50%,  total acc: 63.55%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 63.67%   [EVAL] batch:  268 | acc: 56.25%,  total acc: 63.64%   [EVAL] batch:  269 | acc: 25.00%,  total acc: 63.50%   [EVAL] batch:  270 | acc: 6.25%,  total acc: 63.28%   [EVAL] batch:  271 | acc: 25.00%,  total acc: 63.14%   [EVAL] batch:  272 | acc: 37.50%,  total acc: 63.05%   [EVAL] batch:  273 | acc: 12.50%,  total acc: 62.86%   [EVAL] batch:  274 | acc: 31.25%,  total acc: 62.75%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 62.55%   [EVAL] batch:  276 | acc: 12.50%,  total acc: 62.36%   [EVAL] batch:  277 | acc: 0.00%,  total acc: 62.14%   [EVAL] batch:  278 | acc: 6.25%,  total acc: 61.94%   [EVAL] batch:  279 | acc: 12.50%,  total acc: 61.76%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 61.54%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 61.57%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 61.66%   [EVAL] batch:  283 | acc: 75.00%,  total acc: 61.71%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 61.84%   [EVAL] batch:  285 | acc: 81.25%,  total acc: 61.91%   [EVAL] batch:  286 | acc: 75.00%,  total acc: 61.96%   [EVAL] batch:  287 | acc: 93.75%,  total acc: 62.07%   [EVAL] batch:  288 | acc: 93.75%,  total acc: 62.18%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 62.31%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 62.39%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 62.48%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 62.61%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 62.71%   [EVAL] batch:  294 | acc: 25.00%,  total acc: 62.58%   [EVAL] batch:  295 | acc: 31.25%,  total acc: 62.48%   [EVAL] batch:  296 | acc: 12.50%,  total acc: 62.31%   [EVAL] batch:  297 | acc: 6.25%,  total acc: 62.12%   [EVAL] batch:  298 | acc: 56.25%,  total acc: 62.10%   [EVAL] batch:  299 | acc: 18.75%,  total acc: 61.96%   [EVAL] batch:  300 | acc: 37.50%,  total acc: 61.88%   [EVAL] batch:  301 | acc: 62.50%,  total acc: 61.88%   [EVAL] batch:  302 | acc: 37.50%,  total acc: 61.80%   [EVAL] batch:  303 | acc: 56.25%,  total acc: 61.78%   [EVAL] batch:  304 | acc: 56.25%,  total acc: 61.76%   [EVAL] batch:  305 | acc: 62.50%,  total acc: 61.76%   [EVAL] batch:  306 | acc: 50.00%,  total acc: 61.73%   [EVAL] batch:  307 | acc: 50.00%,  total acc: 61.69%   [EVAL] batch:  308 | acc: 37.50%,  total acc: 61.61%   [EVAL] batch:  309 | acc: 56.25%,  total acc: 61.59%   [EVAL] batch:  310 | acc: 43.75%,  total acc: 61.54%   [EVAL] batch:  311 | acc: 75.00%,  total acc: 61.58%   [EVAL] batch:  312 | acc: 25.00%,  total acc: 61.46%   [EVAL] batch:  313 | acc: 43.75%,  total acc: 61.41%   [EVAL] batch:  314 | acc: 56.25%,  total acc: 61.39%   [EVAL] batch:  315 | acc: 43.75%,  total acc: 61.33%   [EVAL] batch:  316 | acc: 56.25%,  total acc: 61.32%   [EVAL] batch:  317 | acc: 62.50%,  total acc: 61.32%   [EVAL] batch:  318 | acc: 68.75%,  total acc: 61.34%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 61.46%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 61.58%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 61.65%   [EVAL] batch:  322 | acc: 81.25%,  total acc: 61.71%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 61.79%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 61.87%   [EVAL] batch:  325 | acc: 62.50%,  total acc: 61.87%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 61.85%   [EVAL] batch:  327 | acc: 68.75%,  total acc: 61.87%   [EVAL] batch:  328 | acc: 68.75%,  total acc: 61.89%   [EVAL] batch:  329 | acc: 68.75%,  total acc: 61.91%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 61.93%   [EVAL] batch:  331 | acc: 56.25%,  total acc: 61.92%   [EVAL] batch:  332 | acc: 43.75%,  total acc: 61.86%   [EVAL] batch:  333 | acc: 68.75%,  total acc: 61.88%   [EVAL] batch:  334 | acc: 31.25%,  total acc: 61.79%   [EVAL] batch:  335 | acc: 43.75%,  total acc: 61.74%   [EVAL] batch:  336 | acc: 43.75%,  total acc: 61.68%   [EVAL] batch:  337 | acc: 56.25%,  total acc: 61.67%   [EVAL] batch:  338 | acc: 75.00%,  total acc: 61.71%   [EVAL] batch:  339 | acc: 62.50%,  total acc: 61.71%   [EVAL] batch:  340 | acc: 68.75%,  total acc: 61.73%   [EVAL] batch:  341 | acc: 68.75%,  total acc: 61.75%   [EVAL] batch:  342 | acc: 56.25%,  total acc: 61.73%   [EVAL] batch:  343 | acc: 50.00%,  total acc: 61.70%   [EVAL] batch:  344 | acc: 68.75%,  total acc: 61.72%   [EVAL] batch:  345 | acc: 50.00%,  total acc: 61.69%   [EVAL] batch:  346 | acc: 62.50%,  total acc: 61.69%   [EVAL] batch:  347 | acc: 68.75%,  total acc: 61.71%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 61.75%   [EVAL] batch:  349 | acc: 62.50%,  total acc: 61.75%   [EVAL] batch:  350 | acc: 81.25%,  total acc: 61.81%   [EVAL] batch:  351 | acc: 75.00%,  total acc: 61.84%   [EVAL] batch:  352 | acc: 81.25%,  total acc: 61.90%   [EVAL] batch:  353 | acc: 75.00%,  total acc: 61.94%   [EVAL] batch:  354 | acc: 68.75%,  total acc: 61.95%   [EVAL] batch:  355 | acc: 87.50%,  total acc: 62.03%   [EVAL] batch:  356 | acc: 62.50%,  total acc: 62.03%   [EVAL] batch:  357 | acc: 31.25%,  total acc: 61.94%   [EVAL] batch:  358 | acc: 31.25%,  total acc: 61.86%   [EVAL] batch:  359 | acc: 50.00%,  total acc: 61.82%   [EVAL] batch:  360 | acc: 37.50%,  total acc: 61.76%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 61.72%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 61.76%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 61.86%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 61.97%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 62.07%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 62.18%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 62.28%   [EVAL] batch:  368 | acc: 87.50%,  total acc: 62.35%   [EVAL] batch:  369 | acc: 56.25%,  total acc: 62.33%   [EVAL] batch:  370 | acc: 31.25%,  total acc: 62.25%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 62.21%   [EVAL] batch:  372 | acc: 68.75%,  total acc: 62.23%   [EVAL] batch:  373 | acc: 50.00%,  total acc: 62.20%   [EVAL] batch:  374 | acc: 50.00%,  total acc: 62.17%   [EVAL] batch:  375 | acc: 37.50%,  total acc: 62.10%   [EVAL] batch:  376 | acc: 31.25%,  total acc: 62.02%   [EVAL] batch:  377 | acc: 50.00%,  total acc: 61.99%   [EVAL] batch:  378 | acc: 43.75%,  total acc: 61.94%   [EVAL] batch:  379 | acc: 43.75%,  total acc: 61.89%   [EVAL] batch:  380 | acc: 56.25%,  total acc: 61.88%   [EVAL] batch:  381 | acc: 43.75%,  total acc: 61.83%   [EVAL] batch:  382 | acc: 62.50%,  total acc: 61.83%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 61.85%   [EVAL] batch:  384 | acc: 62.50%,  total acc: 61.85%   [EVAL] batch:  385 | acc: 50.00%,  total acc: 61.82%   [EVAL] batch:  386 | acc: 62.50%,  total acc: 61.82%   [EVAL] batch:  387 | acc: 93.75%,  total acc: 61.90%   [EVAL] batch:  388 | acc: 75.00%,  total acc: 61.94%   [EVAL] batch:  389 | acc: 81.25%,  total acc: 61.99%   [EVAL] batch:  390 | acc: 75.00%,  total acc: 62.02%   [EVAL] batch:  391 | acc: 87.50%,  total acc: 62.09%   [EVAL] batch:  392 | acc: 100.00%,  total acc: 62.18%   [EVAL] batch:  393 | acc: 93.75%,  total acc: 62.26%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 62.34%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 62.44%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 62.53%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 62.63%   [EVAL] batch:  398 | acc: 87.50%,  total acc: 62.69%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 62.78%   [EVAL] batch:  400 | acc: 75.00%,  total acc: 62.81%   [EVAL] batch:  401 | acc: 75.00%,  total acc: 62.84%   [EVAL] batch:  402 | acc: 81.25%,  total acc: 62.89%   [EVAL] batch:  403 | acc: 68.75%,  total acc: 62.90%   [EVAL] batch:  404 | acc: 87.50%,  total acc: 62.96%   [EVAL] batch:  405 | acc: 81.25%,  total acc: 63.01%   [EVAL] batch:  406 | acc: 68.75%,  total acc: 63.02%   [EVAL] batch:  407 | acc: 62.50%,  total acc: 63.02%   [EVAL] batch:  408 | acc: 25.00%,  total acc: 62.93%   [EVAL] batch:  409 | acc: 31.25%,  total acc: 62.85%   [EVAL] batch:  410 | acc: 18.75%,  total acc: 62.74%   [EVAL] batch:  411 | acc: 18.75%,  total acc: 62.64%   [EVAL] batch:  412 | acc: 75.00%,  total acc: 62.67%   [EVAL] batch:  413 | acc: 62.50%,  total acc: 62.67%   [EVAL] batch:  414 | acc: 81.25%,  total acc: 62.71%   [EVAL] batch:  415 | acc: 75.00%,  total acc: 62.74%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 62.80%   [EVAL] batch:  417 | acc: 75.00%,  total acc: 62.83%   [EVAL] batch:  418 | acc: 81.25%,  total acc: 62.87%   [EVAL] batch:  419 | acc: 87.50%,  total acc: 62.93%   [EVAL] batch:  420 | acc: 68.75%,  total acc: 62.95%   [EVAL] batch:  421 | acc: 68.75%,  total acc: 62.96%   [EVAL] batch:  422 | acc: 62.50%,  total acc: 62.96%   [EVAL] batch:  423 | acc: 75.00%,  total acc: 62.99%   [EVAL] batch:  424 | acc: 87.50%,  total acc: 63.04%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 63.13%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 63.22%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 63.30%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 63.39%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 63.47%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 63.56%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 63.63%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 63.68%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 63.74%   [EVAL] batch:  434 | acc: 93.75%,  total acc: 63.81%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 63.86%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 63.94%   [EVAL] batch:  437 | acc: 93.75%,  total acc: 64.01%   [EVAL] batch:  438 | acc: 87.50%,  total acc: 64.07%   [EVAL] batch:  439 | acc: 81.25%,  total acc: 64.11%   [EVAL] batch:  440 | acc: 100.00%,  total acc: 64.19%   [EVAL] batch:  441 | acc: 81.25%,  total acc: 64.23%   [EVAL] batch:  442 | acc: 100.00%,  total acc: 64.31%   [EVAL] batch:  443 | acc: 100.00%,  total acc: 64.39%   [EVAL] batch:  444 | acc: 68.75%,  total acc: 64.40%   [EVAL] batch:  445 | acc: 87.50%,  total acc: 64.45%   [EVAL] batch:  446 | acc: 81.25%,  total acc: 64.49%   [EVAL] batch:  447 | acc: 75.00%,  total acc: 64.51%   [EVAL] batch:  448 | acc: 81.25%,  total acc: 64.55%   [EVAL] batch:  449 | acc: 81.25%,  total acc: 64.58%   [EVAL] batch:  450 | acc: 56.25%,  total acc: 64.56%   [EVAL] batch:  451 | acc: 62.50%,  total acc: 64.56%   [EVAL] batch:  452 | acc: 68.75%,  total acc: 64.57%   [EVAL] batch:  453 | acc: 43.75%,  total acc: 64.52%   [EVAL] batch:  454 | acc: 50.00%,  total acc: 64.49%   [EVAL] batch:  455 | acc: 62.50%,  total acc: 64.49%   [EVAL] batch:  456 | acc: 50.00%,  total acc: 64.46%   [EVAL] batch:  457 | acc: 62.50%,  total acc: 64.45%   [EVAL] batch:  458 | acc: 62.50%,  total acc: 64.45%   [EVAL] batch:  459 | acc: 43.75%,  total acc: 64.40%   [EVAL] batch:  460 | acc: 43.75%,  total acc: 64.36%   [EVAL] batch:  461 | acc: 43.75%,  total acc: 64.31%   [EVAL] batch:  462 | acc: 68.75%,  total acc: 64.32%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 64.40%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 64.48%   [EVAL] batch:  465 | acc: 100.00%,  total acc: 64.55%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 64.63%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 64.70%   [EVAL] batch:  468 | acc: 93.75%,  total acc: 64.77%   [EVAL] batch:  469 | acc: 68.75%,  total acc: 64.77%   [EVAL] batch:  470 | acc: 75.00%,  total acc: 64.80%   [EVAL] batch:  471 | acc: 62.50%,  total acc: 64.79%   [EVAL] batch:  472 | acc: 56.25%,  total acc: 64.77%   [EVAL] batch:  473 | acc: 75.00%,  total acc: 64.79%   [EVAL] batch:  474 | acc: 37.50%,  total acc: 64.74%   [EVAL] batch:  475 | acc: 100.00%,  total acc: 64.81%   [EVAL] batch:  476 | acc: 100.00%,  total acc: 64.88%   [EVAL] batch:  477 | acc: 100.00%,  total acc: 64.96%   [EVAL] batch:  478 | acc: 100.00%,  total acc: 65.03%   [EVAL] batch:  479 | acc: 100.00%,  total acc: 65.10%   [EVAL] batch:  480 | acc: 93.75%,  total acc: 65.16%   [EVAL] batch:  481 | acc: 100.00%,  total acc: 65.24%   [EVAL] batch:  482 | acc: 100.00%,  total acc: 65.31%   [EVAL] batch:  483 | acc: 100.00%,  total acc: 65.38%   [EVAL] batch:  484 | acc: 100.00%,  total acc: 65.45%   [EVAL] batch:  485 | acc: 100.00%,  total acc: 65.52%   [EVAL] batch:  486 | acc: 93.75%,  total acc: 65.58%   [EVAL] batch:  487 | acc: 100.00%,  total acc: 65.65%   [EVAL] batch:  488 | acc: 87.50%,  total acc: 65.70%   [EVAL] batch:  489 | acc: 93.75%,  total acc: 65.75%   [EVAL] batch:  490 | acc: 100.00%,  total acc: 65.82%   [EVAL] batch:  491 | acc: 87.50%,  total acc: 65.87%   [EVAL] batch:  492 | acc: 93.75%,  total acc: 65.92%   [EVAL] batch:  493 | acc: 93.75%,  total acc: 65.98%   [EVAL] batch:  494 | acc: 100.00%,  total acc: 66.05%   [EVAL] batch:  495 | acc: 87.50%,  total acc: 66.09%   [EVAL] batch:  496 | acc: 100.00%,  total acc: 66.16%   [EVAL] batch:  497 | acc: 100.00%,  total acc: 66.23%   [EVAL] batch:  498 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:  499 | acc: 93.75%,  total acc: 66.35%   
cur_acc:  ['0.9544', '0.8204', '0.7480', '0.6905', '0.6488', '0.6954', '0.7460', '0.8224']
his_acc:  ['0.9544', '0.8805', '0.8005', '0.7270', '0.6961', '0.6715', '0.6537', '0.6635']
----------END
his_acc mean:  [0.9516 0.8665 0.7942 0.7518 0.7339 0.7019 0.6785 0.6568]
