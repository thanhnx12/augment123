#############params############
cuda
Task=Tacred, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=0, gen_num=0
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 1 1 0]
Losses:  11.743795394897461 10.685749053955078 0.5290232300758362
CurrentTrain: epoch  0, batch     0 | loss: 11.7437954Losses:  10.485512733459473 9.375999450683594 0.5547565817832947
CurrentTrain: epoch  0, batch     1 | loss: 10.4855127Losses:  9.89106273651123 8.778961181640625 0.5560508966445923
CurrentTrain: epoch  0, batch     2 | loss: 9.8910627Losses:  10.914077758789062 9.788379669189453 0.5628492832183838
CurrentTrain: epoch  0, batch     3 | loss: 10.9140778Losses:  10.557802200317383 9.545846939086914 0.5059775114059448
CurrentTrain: epoch  0, batch     4 | loss: 10.5578022Losses:  8.103322982788086 7.082682132720947 0.5103205442428589
CurrentTrain: epoch  0, batch     5 | loss: 8.1033230Losses:  8.167547225952148 7.148955821990967 0.5092959403991699
CurrentTrain: epoch  0, batch     6 | loss: 8.1675472Losses:  9.723185539245605 8.789750099182129 0.4667178988456726
CurrentTrain: epoch  0, batch     7 | loss: 9.7231855Losses:  10.373894691467285 9.51111125946045 0.4313916563987732
CurrentTrain: epoch  0, batch     8 | loss: 10.3738947Losses:  9.326930046081543 8.443913459777832 0.44150832295417786
CurrentTrain: epoch  0, batch     9 | loss: 9.3269300Losses:  8.574690818786621 7.664058208465576 0.45531630516052246
CurrentTrain: epoch  0, batch    10 | loss: 8.5746908Losses:  8.132680892944336 7.186964511871338 0.472858190536499
CurrentTrain: epoch  0, batch    11 | loss: 8.1326809Losses:  8.697803497314453 7.955514907836914 0.37114447355270386
CurrentTrain: epoch  0, batch    12 | loss: 8.6978035Losses:  8.419251441955566 7.782864570617676 0.31819337606430054
CurrentTrain: epoch  0, batch    13 | loss: 8.4192514Losses:  9.222953796386719 8.632782936096191 0.29508525133132935
CurrentTrain: epoch  0, batch    14 | loss: 9.2229538Losses:  7.308052062988281 6.704017639160156 0.3020171523094177
CurrentTrain: epoch  0, batch    15 | loss: 7.3080521Losses:  10.18571662902832 9.620035171508789 0.28284066915512085
CurrentTrain: epoch  0, batch    16 | loss: 10.1857166Losses:  12.231613159179688 11.782750129699707 0.224431574344635
CurrentTrain: epoch  0, batch    17 | loss: 12.2316132Losses:  12.937129974365234 12.454937934875488 0.24109578132629395
CurrentTrain: epoch  0, batch    18 | loss: 12.9371300Losses:  8.408238410949707 7.867868423461914 0.27018505334854126
CurrentTrain: epoch  0, batch    19 | loss: 8.4082384Losses:  8.083599090576172 7.577557563781738 0.2530210018157959
CurrentTrain: epoch  0, batch    20 | loss: 8.0835991Losses:  10.359468460083008 9.686065673828125 0.33670133352279663
CurrentTrain: epoch  0, batch    21 | loss: 10.3594685Losses:  8.869698524475098 8.534016609191895 0.16784101724624634
CurrentTrain: epoch  0, batch    22 | loss: 8.8696985Losses:  8.079728126525879 7.738555908203125 0.17058619856834412
CurrentTrain: epoch  0, batch    23 | loss: 8.0797281Losses:  8.05693531036377 7.69670295715332 0.1801161766052246
CurrentTrain: epoch  0, batch    24 | loss: 8.0569353Losses:  6.887990474700928 6.540471076965332 0.1737598031759262
CurrentTrain: epoch  0, batch    25 | loss: 6.8879905Losses:  9.248196601867676 8.835113525390625 0.20654159784317017
CurrentTrain: epoch  0, batch    26 | loss: 9.2481966Losses:  7.624256610870361 7.375728607177734 0.12426392734050751
CurrentTrain: epoch  0, batch    27 | loss: 7.6242566Losses:  9.484636306762695 9.204329490661621 0.140153169631958
CurrentTrain: epoch  0, batch    28 | loss: 9.4846363Losses:  6.345453262329102 6.061019420623779 0.14221695065498352
CurrentTrain: epoch  0, batch    29 | loss: 6.3454533Losses:  9.42766284942627 9.108722686767578 0.15947028994560242
CurrentTrain: epoch  0, batch    30 | loss: 9.4276628Losses:  7.976992130279541 7.666153907775879 0.1554192155599594
CurrentTrain: epoch  0, batch    31 | loss: 7.9769921Losses:  8.432636260986328 8.177362442016602 0.12763711810112
CurrentTrain: epoch  0, batch    32 | loss: 8.4326363Losses:  7.516328811645508 7.294661045074463 0.1108337864279747
CurrentTrain: epoch  0, batch    33 | loss: 7.5163288Losses:  10.562607765197754 10.375892639160156 0.0933573842048645
CurrentTrain: epoch  0, batch    34 | loss: 10.5626078Losses:  9.886804580688477 9.671756744384766 0.10752415657043457
CurrentTrain: epoch  0, batch    35 | loss: 9.8868046Losses:  7.91763973236084 7.733250617980957 0.09219447523355484
CurrentTrain: epoch  0, batch    36 | loss: 7.9176397Losses:  1.8564133644104004 1.6291364431381226 0.11363847553730011
CurrentTrain: epoch  0, batch    37 | loss: 1.8564134Losses:  5.751972198486328 5.6150970458984375 0.0684375986456871
CurrentTrain: epoch  1, batch     0 | loss: 5.7519722Losses:  6.208125591278076 6.043543815612793 0.08229096233844757
CurrentTrain: epoch  1, batch     1 | loss: 6.2081256Losses:  6.92758846282959 6.78439998626709 0.07159432023763657
CurrentTrain: epoch  1, batch     2 | loss: 6.9275885Losses:  5.518702507019043 5.361462593078613 0.07861991226673126
CurrentTrain: epoch  1, batch     3 | loss: 5.5187025Losses:  6.762611389160156 6.610842227935791 0.07588458061218262
CurrentTrain: epoch  1, batch     4 | loss: 6.7626114Losses:  9.407363891601562 9.27535343170166 0.06600522249937057
CurrentTrain: epoch  1, batch     5 | loss: 9.4073639Losses:  6.815699100494385 6.67673397064209 0.06948249787092209
CurrentTrain: epoch  1, batch     6 | loss: 6.8156991Losses:  7.291828155517578 7.153866767883301 0.06898059695959091
CurrentTrain: epoch  1, batch     7 | loss: 7.2918282Losses:  8.09736156463623 7.964556694030762 0.06640225648880005
CurrentTrain: epoch  1, batch     8 | loss: 8.0973616Losses:  6.421774864196777 6.2805280685424805 0.07062327861785889
CurrentTrain: epoch  1, batch     9 | loss: 6.4217749Losses:  7.7613205909729 7.630667209625244 0.06532666087150574
CurrentTrain: epoch  1, batch    10 | loss: 7.7613206Losses:  5.552865505218506 5.437598705291748 0.05763347074389458
CurrentTrain: epoch  1, batch    11 | loss: 5.5528655Losses:  6.179978370666504 6.044034004211426 0.06797227263450623
CurrentTrain: epoch  1, batch    12 | loss: 6.1799784Losses:  5.281879901885986 5.173001289367676 0.0544392429292202
CurrentTrain: epoch  1, batch    13 | loss: 5.2818799Losses:  7.335803985595703 7.210010528564453 0.06289666891098022
CurrentTrain: epoch  1, batch    14 | loss: 7.3358040Losses:  6.148840427398682 6.0278120040893555 0.06051409989595413
CurrentTrain: epoch  1, batch    15 | loss: 6.1488404Losses:  6.024827480316162 5.922280311584473 0.05127368122339249
CurrentTrain: epoch  1, batch    16 | loss: 6.0248275Losses:  7.913486957550049 7.802267074584961 0.05560987815260887
CurrentTrain: epoch  1, batch    17 | loss: 7.9134870Losses:  6.036013603210449 5.910606861114502 0.06270343065261841
CurrentTrain: epoch  1, batch    18 | loss: 6.0360136Losses:  8.675650596618652 8.573036193847656 0.05130714550614357
CurrentTrain: epoch  1, batch    19 | loss: 8.6756506Losses:  6.292470932006836 6.19370174407959 0.049384571611881256
CurrentTrain: epoch  1, batch    20 | loss: 6.2924709Losses:  9.356383323669434 9.255487442016602 0.05044785141944885
CurrentTrain: epoch  1, batch    21 | loss: 9.3563833Losses:  6.4966301918029785 6.403602600097656 0.04651381075382233
CurrentTrain: epoch  1, batch    22 | loss: 6.4966302Losses:  6.256545543670654 6.158578872680664 0.04898344725370407
CurrentTrain: epoch  1, batch    23 | loss: 6.2565455Losses:  7.574609756469727 7.466894626617432 0.05385761708021164
CurrentTrain: epoch  1, batch    24 | loss: 7.5746098Losses:  7.23239278793335 7.106639862060547 0.06287650763988495
CurrentTrain: epoch  1, batch    25 | loss: 7.2323928Losses:  8.343902587890625 8.241432189941406 0.051235344260931015
CurrentTrain: epoch  1, batch    26 | loss: 8.3439026Losses:  9.339163780212402 9.194171905517578 0.07249601185321808
CurrentTrain: epoch  1, batch    27 | loss: 9.3391638Losses:  10.861932754516602 10.747230529785156 0.057351309806108475
CurrentTrain: epoch  1, batch    28 | loss: 10.8619328Losses:  7.119902610778809 7.010097026824951 0.05490284413099289
CurrentTrain: epoch  1, batch    29 | loss: 7.1199026Losses:  5.30835485458374 5.225399971008301 0.04147748649120331
CurrentTrain: epoch  1, batch    30 | loss: 5.3083549Losses:  8.295461654663086 8.187884330749512 0.053788766264915466
CurrentTrain: epoch  1, batch    31 | loss: 8.2954617Losses:  5.50717306137085 5.4195051193237305 0.04383402317762375
CurrentTrain: epoch  1, batch    32 | loss: 5.5071731Losses:  6.583901882171631 6.4912824630737305 0.0463097058236599
CurrentTrain: epoch  1, batch    33 | loss: 6.5839019Losses:  5.881813049316406 5.787801742553711 0.0470055527985096
CurrentTrain: epoch  1, batch    34 | loss: 5.8818130Losses:  6.1389312744140625 6.056075096130371 0.04142813757061958
CurrentTrain: epoch  1, batch    35 | loss: 6.1389313Losses:  5.776668071746826 5.692782878875732 0.04194250702857971
CurrentTrain: epoch  1, batch    36 | loss: 5.7766681Losses:  1.2231957912445068 1.090503215789795 0.06634625792503357
CurrentTrain: epoch  1, batch    37 | loss: 1.2231958Losses:  4.468153953552246 4.379671096801758 0.04424137622117996
CurrentTrain: epoch  2, batch     0 | loss: 4.4681540Losses:  5.988766193389893 5.900935173034668 0.04391553997993469
CurrentTrain: epoch  2, batch     1 | loss: 5.9887662Losses:  5.082767009735107 5.007405757904053 0.03768060356378555
CurrentTrain: epoch  2, batch     2 | loss: 5.0827670Losses:  9.073410034179688 8.961090087890625 0.05615994334220886
CurrentTrain: epoch  2, batch     3 | loss: 9.0734100Losses:  8.385270118713379 8.298446655273438 0.04341171681880951
CurrentTrain: epoch  2, batch     4 | loss: 8.3852701Losses:  12.362658500671387 12.274240493774414 0.044208817183971405
CurrentTrain: epoch  2, batch     5 | loss: 12.3626585Losses:  4.799469947814941 4.715338230133057 0.042065784335136414
CurrentTrain: epoch  2, batch     6 | loss: 4.7994699Losses:  6.550870895385742 6.468229293823242 0.041320908814668655
CurrentTrain: epoch  2, batch     7 | loss: 6.5508709Losses:  6.291351318359375 6.209792137145996 0.040779612958431244
CurrentTrain: epoch  2, batch     8 | loss: 6.2913513Losses:  5.845112323760986 5.765851020812988 0.039630718529224396
CurrentTrain: epoch  2, batch     9 | loss: 5.8451123Losses:  5.799620628356934 5.718088150024414 0.040766164660453796
CurrentTrain: epoch  2, batch    10 | loss: 5.7996206Losses:  6.367162227630615 6.2926130294799805 0.03727458417415619
CurrentTrain: epoch  2, batch    11 | loss: 6.3671622Losses:  5.231389045715332 5.159518718719482 0.035935141146183014
CurrentTrain: epoch  2, batch    12 | loss: 5.2313890Losses:  5.5620951652526855 5.483392238616943 0.03935156762599945
CurrentTrain: epoch  2, batch    13 | loss: 5.5620952Losses:  9.384919166564941 9.285496711730957 0.04971112683415413
CurrentTrain: epoch  2, batch    14 | loss: 9.3849192Losses:  6.593133449554443 6.517126083374023 0.03800365701317787
CurrentTrain: epoch  2, batch    15 | loss: 6.5931334Losses:  5.997317790985107 5.91197395324707 0.042671896517276764
CurrentTrain: epoch  2, batch    16 | loss: 5.9973178Losses:  5.210007190704346 5.1423797607421875 0.033813755959272385
CurrentTrain: epoch  2, batch    17 | loss: 5.2100072Losses:  5.521910667419434 5.456705570220947 0.032602593302726746
CurrentTrain: epoch  2, batch    18 | loss: 5.5219107Losses:  6.338925838470459 6.272068500518799 0.033428702503442764
CurrentTrain: epoch  2, batch    19 | loss: 6.3389258Losses:  6.677666187286377 6.6048383712768555 0.03641383349895477
CurrentTrain: epoch  2, batch    20 | loss: 6.6776662Losses:  5.89871883392334 5.830996513366699 0.033861178904771805
CurrentTrain: epoch  2, batch    21 | loss: 5.8987188Losses:  6.25322151184082 6.171906471252441 0.04065760597586632
CurrentTrain: epoch  2, batch    22 | loss: 6.2532215Losses:  4.456313610076904 4.391444206237793 0.03243463858962059
CurrentTrain: epoch  2, batch    23 | loss: 4.4563136Losses:  5.136266708374023 5.067930698394775 0.03416796028614044
CurrentTrain: epoch  2, batch    24 | loss: 5.1362667Losses:  5.154987335205078 5.082180023193359 0.03640354424715042
CurrentTrain: epoch  2, batch    25 | loss: 5.1549873Losses:  6.388669967651367 6.324352264404297 0.032158784568309784
CurrentTrain: epoch  2, batch    26 | loss: 6.3886700Losses:  6.232104778289795 6.1610307693481445 0.03553704172372818
CurrentTrain: epoch  2, batch    27 | loss: 6.2321048Losses:  6.175784111022949 6.097884178161621 0.038950011134147644
CurrentTrain: epoch  2, batch    28 | loss: 6.1757841Losses:  5.4430365562438965 5.379465103149414 0.03178568184375763
CurrentTrain: epoch  2, batch    29 | loss: 5.4430366Losses:  8.406885147094727 8.326276779174805 0.04030432179570198
CurrentTrain: epoch  2, batch    30 | loss: 8.4068851Losses:  5.268837928771973 5.200007438659668 0.03441533073782921
CurrentTrain: epoch  2, batch    31 | loss: 5.2688379Losses:  8.271967887878418 8.192543029785156 0.0397125780582428
CurrentTrain: epoch  2, batch    32 | loss: 8.2719679Losses:  5.584750175476074 5.519611358642578 0.03256942331790924
CurrentTrain: epoch  2, batch    33 | loss: 5.5847502Losses:  8.287369728088379 8.219596862792969 0.03388652950525284
CurrentTrain: epoch  2, batch    34 | loss: 8.2873697Losses:  7.951846599578857 7.8909077644348145 0.030469384044408798
CurrentTrain: epoch  2, batch    35 | loss: 7.9518466Losses:  7.904441833496094 7.840028762817383 0.0322064571082592
CurrentTrain: epoch  2, batch    36 | loss: 7.9044418Losses:  2.241516590118408 2.1412768363952637 0.05011983588337898
CurrentTrain: epoch  2, batch    37 | loss: 2.2415166Losses:  6.093746185302734 6.017622947692871 0.038061726838350296
CurrentTrain: epoch  3, batch     0 | loss: 6.0937462Losses:  13.257498741149902 13.165151596069336 0.04617374390363693
CurrentTrain: epoch  3, batch     1 | loss: 13.2574987Losses:  5.734119892120361 5.669606685638428 0.03225667029619217
CurrentTrain: epoch  3, batch     2 | loss: 5.7341199Losses:  5.7347588539123535 5.663622856140137 0.03556804358959198
CurrentTrain: epoch  3, batch     3 | loss: 5.7347589Losses:  5.183631420135498 5.122643947601318 0.03049372509121895
CurrentTrain: epoch  3, batch     4 | loss: 5.1836314Losses:  5.587658405303955 5.525006294250488 0.031325966119766235
CurrentTrain: epoch  3, batch     5 | loss: 5.5876584Losses:  7.541745185852051 7.455057144165039 0.043344028294086456
CurrentTrain: epoch  3, batch     6 | loss: 7.5417452Losses:  4.4429850578308105 4.386868000030518 0.028058622032403946
CurrentTrain: epoch  3, batch     7 | loss: 4.4429851Losses:  5.114030361175537 5.048612594604492 0.03270886093378067
CurrentTrain: epoch  3, batch     8 | loss: 5.1140304Losses:  4.37194299697876 4.319793224334717 0.026074809953570366
CurrentTrain: epoch  3, batch     9 | loss: 4.3719430Losses:  5.364064693450928 5.298639297485352 0.03271261230111122
CurrentTrain: epoch  3, batch    10 | loss: 5.3640647Losses:  7.256439685821533 7.167963027954102 0.04423835501074791
CurrentTrain: epoch  3, batch    11 | loss: 7.2564397Losses:  4.610846996307373 4.550890922546387 0.02997800149023533
CurrentTrain: epoch  3, batch    12 | loss: 4.6108470Losses:  5.5161614418029785 5.444482803344727 0.035839278250932693
CurrentTrain: epoch  3, batch    13 | loss: 5.5161614Losses:  5.058261394500732 4.9960737228393555 0.031093891710042953
CurrentTrain: epoch  3, batch    14 | loss: 5.0582614Losses:  5.708718299865723 5.648634433746338 0.030041975900530815
CurrentTrain: epoch  3, batch    15 | loss: 5.7087183Losses:  6.267339706420898 6.2086181640625 0.029360728338360786
CurrentTrain: epoch  3, batch    16 | loss: 6.2673397Losses:  6.553415298461914 6.483991622924805 0.034711867570877075
CurrentTrain: epoch  3, batch    17 | loss: 6.5534153Losses:  6.107571125030518 6.05305290222168 0.02725909650325775
CurrentTrain: epoch  3, batch    18 | loss: 6.1075711Losses:  6.376374244689941 6.313875198364258 0.031249480322003365
CurrentTrain: epoch  3, batch    19 | loss: 6.3763742Losses:  11.994569778442383 11.937164306640625 0.028702588751912117
CurrentTrain: epoch  3, batch    20 | loss: 11.9945698Losses:  5.805206298828125 5.748537063598633 0.028334567323327065
CurrentTrain: epoch  3, batch    21 | loss: 5.8052063Losses:  6.513366222381592 6.451296329498291 0.031034838408231735
CurrentTrain: epoch  3, batch    22 | loss: 6.5133662Losses:  7.133083343505859 7.071141719818115 0.030970927327871323
CurrentTrain: epoch  3, batch    23 | loss: 7.1330833Losses:  5.294688701629639 5.232714653015137 0.03098699264228344
CurrentTrain: epoch  3, batch    24 | loss: 5.2946887Losses:  8.792010307312012 8.711994171142578 0.040008239448070526
CurrentTrain: epoch  3, batch    25 | loss: 8.7920103Losses:  6.065739154815674 6.005785942077637 0.029976556077599525
CurrentTrain: epoch  3, batch    26 | loss: 6.0657392Losses:  7.1647725105285645 7.112669944763184 0.0260512325912714
CurrentTrain: epoch  3, batch    27 | loss: 7.1647725Losses:  4.324614524841309 4.272724151611328 0.025945188477635384
CurrentTrain: epoch  3, batch    28 | loss: 4.3246145Losses:  7.087780475616455 6.994950771331787 0.04641486704349518
CurrentTrain: epoch  3, batch    29 | loss: 7.0877805Losses:  4.977936744689941 4.921456336975098 0.02824023738503456
CurrentTrain: epoch  3, batch    30 | loss: 4.9779367Losses:  6.024559020996094 5.963591575622559 0.030483722686767578
CurrentTrain: epoch  3, batch    31 | loss: 6.0245590Losses:  9.466325759887695 9.396839141845703 0.0347432978451252
CurrentTrain: epoch  3, batch    32 | loss: 9.4663258Losses:  7.1482367515563965 7.0520524978637695 0.04809221625328064
CurrentTrain: epoch  3, batch    33 | loss: 7.1482368Losses:  5.179693698883057 5.127726078033447 0.025983750820159912
CurrentTrain: epoch  3, batch    34 | loss: 5.1796937Losses:  5.610270023345947 5.553783416748047 0.02824336662888527
CurrentTrain: epoch  3, batch    35 | loss: 5.6102700Losses:  5.355346202850342 5.303530693054199 0.025907669216394424
CurrentTrain: epoch  3, batch    36 | loss: 5.3553462Losses:  3.2737724781036377 3.201848268508911 0.03596210479736328
CurrentTrain: epoch  3, batch    37 | loss: 3.2737725Losses:  5.531818389892578 5.470673561096191 0.030572349205613136
CurrentTrain: epoch  4, batch     0 | loss: 5.5318184Losses:  5.2650251388549805 5.2153096199035645 0.024857724085450172
CurrentTrain: epoch  4, batch     1 | loss: 5.2650251Losses:  6.608076095581055 6.540950298309326 0.03356300666928291
CurrentTrain: epoch  4, batch     2 | loss: 6.6080761Losses:  6.582138538360596 6.514938831329346 0.03359989449381828
CurrentTrain: epoch  4, batch     3 | loss: 6.5821385Losses:  7.804370403289795 7.73247766494751 0.035946350544691086
CurrentTrain: epoch  4, batch     4 | loss: 7.8043704Losses:  4.9124627113342285 4.856781959533691 0.027840454131364822
CurrentTrain: epoch  4, batch     5 | loss: 4.9124627Losses:  7.629331588745117 7.5729169845581055 0.028207208961248398
CurrentTrain: epoch  4, batch     6 | loss: 7.6293316Losses:  4.438529014587402 4.384966850280762 0.02678116410970688
CurrentTrain: epoch  4, batch     7 | loss: 4.4385290Losses:  9.583677291870117 9.503868103027344 0.03990463539958
CurrentTrain: epoch  4, batch     8 | loss: 9.5836773Losses:  4.62576961517334 4.5742597579956055 0.025754906237125397
CurrentTrain: epoch  4, batch     9 | loss: 4.6257696Losses:  7.534390926361084 7.464389801025391 0.03500048816204071
CurrentTrain: epoch  4, batch    10 | loss: 7.5343909Losses:  4.688220024108887 4.640468120574951 0.023876050487160683
CurrentTrain: epoch  4, batch    11 | loss: 4.6882200Losses:  8.400073051452637 8.329863548278809 0.035104770213365555
CurrentTrain: epoch  4, batch    12 | loss: 8.4000731Losses:  4.001348972320557 3.952648401260376 0.024350253865122795
CurrentTrain: epoch  4, batch    13 | loss: 4.0013490Losses:  8.2980375289917 8.241537094116211 0.028250377625226974
CurrentTrain: epoch  4, batch    14 | loss: 8.2980375Losses:  7.654094219207764 7.601717948913574 0.026188042014837265
CurrentTrain: epoch  4, batch    15 | loss: 7.6540942Losses:  4.944101810455322 4.89034366607666 0.026879113167524338
CurrentTrain: epoch  4, batch    16 | loss: 4.9441018Losses:  5.850405693054199 5.793922424316406 0.02824174426496029
CurrentTrain: epoch  4, batch    17 | loss: 5.8504057Losses:  4.897221088409424 4.847315788269043 0.024952664971351624
CurrentTrain: epoch  4, batch    18 | loss: 4.8972211Losses:  4.246633529663086 4.199808120727539 0.023412683978676796
CurrentTrain: epoch  4, batch    19 | loss: 4.2466335Losses:  5.795609951019287 5.728328227996826 0.033640868961811066
CurrentTrain: epoch  4, batch    20 | loss: 5.7956100Losses:  5.580726146697998 5.529313087463379 0.0257064588367939
CurrentTrain: epoch  4, batch    21 | loss: 5.5807261Losses:  4.725413799285889 4.6726789474487305 0.026367442682385445
CurrentTrain: epoch  4, batch    22 | loss: 4.7254138Losses:  5.678333759307861 5.603381156921387 0.03747629374265671
CurrentTrain: epoch  4, batch    23 | loss: 5.6783338Losses:  5.753458499908447 5.683215141296387 0.03512169048190117
CurrentTrain: epoch  4, batch    24 | loss: 5.7534585Losses:  8.491548538208008 8.419405937194824 0.03607117384672165
CurrentTrain: epoch  4, batch    25 | loss: 8.4915485Losses:  5.140514373779297 5.092966079711914 0.023774081841111183
CurrentTrain: epoch  4, batch    26 | loss: 5.1405144Losses:  6.047074794769287 5.980832099914551 0.033121347427368164
CurrentTrain: epoch  4, batch    27 | loss: 6.0470748Losses:  3.5997843742370605 3.5530197620391846 0.023382289335131645
CurrentTrain: epoch  4, batch    28 | loss: 3.5997844Losses:  5.544847011566162 5.492934226989746 0.02595634013414383
CurrentTrain: epoch  4, batch    29 | loss: 5.5448470Losses:  5.946200847625732 5.8913655281066895 0.027417661622166634
CurrentTrain: epoch  4, batch    30 | loss: 5.9462008Losses:  9.872570991516113 9.798515319824219 0.03702791780233383
CurrentTrain: epoch  4, batch    31 | loss: 9.8725710Losses:  5.1026835441589355 5.042332172393799 0.030175695195794106
CurrentTrain: epoch  4, batch    32 | loss: 5.1026835Losses:  5.965497016906738 5.894249439239502 0.03562372177839279
CurrentTrain: epoch  4, batch    33 | loss: 5.9654970Losses:  6.864027976989746 6.806529998779297 0.028748981654644012
CurrentTrain: epoch  4, batch    34 | loss: 6.8640280Losses:  5.030367851257324 4.9662652015686035 0.03205125033855438
CurrentTrain: epoch  4, batch    35 | loss: 5.0303679Losses:  5.14678955078125 5.0899434089660645 0.028423095121979713
CurrentTrain: epoch  4, batch    36 | loss: 5.1467896Losses:  0.651677668094635 0.6017680168151855 0.024954814463853836
CurrentTrain: epoch  4, batch    37 | loss: 0.6516777Losses:  6.327438831329346 6.2485246658325195 0.039457060396671295
CurrentTrain: epoch  5, batch     0 | loss: 6.3274388Losses:  5.160090923309326 5.111453056335449 0.024318957701325417
CurrentTrain: epoch  5, batch     1 | loss: 5.1600909Losses:  6.3556108474731445 6.2891645431518555 0.0332232341170311
CurrentTrain: epoch  5, batch     2 | loss: 6.3556108Losses:  7.39730978012085 7.336082458496094 0.030613567680120468
CurrentTrain: epoch  5, batch     3 | loss: 7.3973098Losses:  5.374207019805908 5.3050055503845215 0.03460080549120903
CurrentTrain: epoch  5, batch     4 | loss: 5.3742070Losses:  5.643115520477295 5.580348968505859 0.03138338029384613
CurrentTrain: epoch  5, batch     5 | loss: 5.6431155Losses:  4.433163166046143 4.376855850219727 0.02815360575914383
CurrentTrain: epoch  5, batch     6 | loss: 4.4331632Losses:  5.328192710876465 5.265985488891602 0.03110371343791485
CurrentTrain: epoch  5, batch     7 | loss: 5.3281927Losses:  6.0486273765563965 5.993204116821289 0.027711641043424606
CurrentTrain: epoch  5, batch     8 | loss: 6.0486274Losses:  5.704906463623047 5.644924163818359 0.02999124862253666
CurrentTrain: epoch  5, batch     9 | loss: 5.7049065Losses:  4.591241836547852 4.546029567718506 0.022606054320931435
CurrentTrain: epoch  5, batch    10 | loss: 4.5912418Losses:  5.160041332244873 5.10191011428833 0.02906549721956253
CurrentTrain: epoch  5, batch    11 | loss: 5.1600413Losses:  4.184936046600342 4.13929557800293 0.022820239886641502
CurrentTrain: epoch  5, batch    12 | loss: 4.1849360Losses:  4.461343765258789 4.412476539611816 0.024433668702840805
CurrentTrain: epoch  5, batch    13 | loss: 4.4613438Losses:  4.057759761810303 4.0114030838012695 0.023178299888968468
CurrentTrain: epoch  5, batch    14 | loss: 4.0577598Losses:  7.892516613006592 7.842171669006348 0.02517245151102543
CurrentTrain: epoch  5, batch    15 | loss: 7.8925166Losses:  5.737246036529541 5.675506114959717 0.03086991049349308
CurrentTrain: epoch  5, batch    16 | loss: 5.7372460Losses:  6.311331748962402 6.258039474487305 0.02664606086909771
CurrentTrain: epoch  5, batch    17 | loss: 6.3113317Losses:  6.322449684143066 6.267064094543457 0.02769274078309536
CurrentTrain: epoch  5, batch    18 | loss: 6.3224497Losses:  6.851204872131348 6.783522605895996 0.03384124115109444
CurrentTrain: epoch  5, batch    19 | loss: 6.8512049Losses:  5.056973457336426 5.000278949737549 0.028347197920084
CurrentTrain: epoch  5, batch    20 | loss: 5.0569735Losses:  5.205166816711426 5.154644966125488 0.025261027738451958
CurrentTrain: epoch  5, batch    21 | loss: 5.2051668Losses:  4.281894207000732 4.233309268951416 0.02429254725575447
CurrentTrain: epoch  5, batch    22 | loss: 4.2818942Losses:  4.421745300292969 4.377357482910156 0.02219398319721222
CurrentTrain: epoch  5, batch    23 | loss: 4.4217453Losses:  5.46016263961792 5.405592918395996 0.02728496491909027
CurrentTrain: epoch  5, batch    24 | loss: 5.4601626Losses:  4.187234401702881 4.140646934509277 0.023293664678931236
CurrentTrain: epoch  5, batch    25 | loss: 4.1872344Losses:  4.738448619842529 4.682248115539551 0.028100349009037018
CurrentTrain: epoch  5, batch    26 | loss: 4.7384486Losses:  9.707779884338379 9.63684368133545 0.03546823561191559
CurrentTrain: epoch  5, batch    27 | loss: 9.7077799Losses:  8.341347694396973 8.286918640136719 0.02721431851387024
CurrentTrain: epoch  5, batch    28 | loss: 8.3413477Losses:  3.769725799560547 3.723237991333008 0.02324393205344677
CurrentTrain: epoch  5, batch    29 | loss: 3.7697258Losses:  5.941072940826416 5.878089904785156 0.031491510570049286
CurrentTrain: epoch  5, batch    30 | loss: 5.9410729Losses:  6.609675884246826 6.546145915985107 0.03176506608724594
CurrentTrain: epoch  5, batch    31 | loss: 6.6096759Losses:  9.221055030822754 9.136664390563965 0.04219544678926468
CurrentTrain: epoch  5, batch    32 | loss: 9.2210550Losses:  6.19281530380249 6.147701740264893 0.02255687117576599
CurrentTrain: epoch  5, batch    33 | loss: 6.1928153Losses:  3.5283379554748535 3.4834656715393066 0.022436117753386497
CurrentTrain: epoch  5, batch    34 | loss: 3.5283380Losses:  7.658853054046631 7.605633735656738 0.02660972997546196
CurrentTrain: epoch  5, batch    35 | loss: 7.6588531Losses:  4.552739143371582 4.503913879394531 0.02441261149942875
CurrentTrain: epoch  5, batch    36 | loss: 4.5527391Losses:  1.3387116193771362 1.2505905628204346 0.04406053572893143
CurrentTrain: epoch  5, batch    37 | loss: 1.3387116Losses:  5.706936836242676 5.661200523376465 0.022868245840072632
CurrentTrain: epoch  6, batch     0 | loss: 5.7069368Losses:  6.33814811706543 6.278453826904297 0.029847150668501854
CurrentTrain: epoch  6, batch     1 | loss: 6.3381481Losses:  8.313064575195312 8.244758605957031 0.03415321558713913
CurrentTrain: epoch  6, batch     2 | loss: 8.3130646Losses:  4.596573352813721 4.544196605682373 0.02618839032948017
CurrentTrain: epoch  6, batch     3 | loss: 4.5965734Losses:  6.014034748077393 5.947953701019287 0.03304044529795647
CurrentTrain: epoch  6, batch     4 | loss: 6.0140347Losses:  5.192142486572266 5.136298179626465 0.027922242879867554
CurrentTrain: epoch  6, batch     5 | loss: 5.1921425Losses:  9.35195541381836 9.299583435058594 0.026185892522335052
CurrentTrain: epoch  6, batch     6 | loss: 9.3519554Losses:  4.570993423461914 4.523561000823975 0.02371622435748577
CurrentTrain: epoch  6, batch     7 | loss: 4.5709934Losses:  5.600314617156982 5.556905746459961 0.021704407408833504
CurrentTrain: epoch  6, batch     8 | loss: 5.6003146Losses:  4.785253047943115 4.730389595031738 0.027431752532720566
CurrentTrain: epoch  6, batch     9 | loss: 4.7852530Losses:  3.9547038078308105 3.9043173789978027 0.025193259119987488
CurrentTrain: epoch  6, batch    10 | loss: 3.9547038Losses:  4.962878227233887 4.908432960510254 0.02722267061471939
CurrentTrain: epoch  6, batch    11 | loss: 4.9628782Losses:  3.8977692127227783 3.847475051879883 0.025147050619125366
CurrentTrain: epoch  6, batch    12 | loss: 3.8977692Losses:  5.950985908508301 5.887786388397217 0.03159979730844498
CurrentTrain: epoch  6, batch    13 | loss: 5.9509859Losses:  4.0197625160217285 3.9720640182495117 0.023849278688430786
CurrentTrain: epoch  6, batch    14 | loss: 4.0197625Losses:  4.090399742126465 4.043410301208496 0.023494778200984
CurrentTrain: epoch  6, batch    15 | loss: 4.0903997Losses:  6.979948043823242 6.911655426025391 0.03414641320705414
CurrentTrain: epoch  6, batch    16 | loss: 6.9799480Losses:  6.597942352294922 6.535130500793457 0.031405970454216
CurrentTrain: epoch  6, batch    17 | loss: 6.5979424Losses:  5.260462284088135 5.2065839767456055 0.026939107105135918
CurrentTrain: epoch  6, batch    18 | loss: 5.2604623Losses:  7.014726638793945 6.956907272338867 0.02890956774353981
CurrentTrain: epoch  6, batch    19 | loss: 7.0147266Losses:  9.164512634277344 9.091815948486328 0.036348309367895126
CurrentTrain: epoch  6, batch    20 | loss: 9.1645126Losses:  4.160189151763916 4.1096906661987305 0.025249170139431953
CurrentTrain: epoch  6, batch    21 | loss: 4.1601892Losses:  5.9217681884765625 5.864189147949219 0.028789520263671875
CurrentTrain: epoch  6, batch    22 | loss: 5.9217682Losses:  3.6428756713867188 3.597926139831543 0.02247481606900692
CurrentTrain: epoch  6, batch    23 | loss: 3.6428757Losses:  5.912613391876221 5.84451961517334 0.03404681384563446
CurrentTrain: epoch  6, batch    24 | loss: 5.9126134Losses:  8.031634330749512 7.947371959686279 0.04213133454322815
CurrentTrain: epoch  6, batch    25 | loss: 8.0316343Losses:  7.604968070983887 7.537362098693848 0.03380286693572998
CurrentTrain: epoch  6, batch    26 | loss: 7.6049681Losses:  6.04559850692749 5.968311309814453 0.0386434905230999
CurrentTrain: epoch  6, batch    27 | loss: 6.0455985Losses:  6.428716659545898 6.359186172485352 0.034765347838401794
CurrentTrain: epoch  6, batch    28 | loss: 6.4287167Losses:  5.914832592010498 5.848730087280273 0.03305117413401604
CurrentTrain: epoch  6, batch    29 | loss: 5.9148326Losses:  6.317044734954834 6.267128944396973 0.02495795488357544
CurrentTrain: epoch  6, batch    30 | loss: 6.3170447Losses:  6.0183424949646 5.959059715270996 0.0296414066106081
CurrentTrain: epoch  6, batch    31 | loss: 6.0183425Losses:  5.033483028411865 4.989313125610352 0.022085022181272507
CurrentTrain: epoch  6, batch    32 | loss: 5.0334830Losses:  4.506842613220215 4.456079959869385 0.025381440296769142
CurrentTrain: epoch  6, batch    33 | loss: 4.5068426Losses:  4.816600322723389 4.767900466918945 0.02435002289712429
CurrentTrain: epoch  6, batch    34 | loss: 4.8166003Losses:  4.066272258758545 4.020752906799316 0.02275976538658142
CurrentTrain: epoch  6, batch    35 | loss: 4.0662723Losses:  4.276458740234375 4.23101806640625 0.02272026054561138
CurrentTrain: epoch  6, batch    36 | loss: 4.2764587Losses:  0.5698087811470032 0.5151135921478271 0.02734760381281376
CurrentTrain: epoch  6, batch    37 | loss: 0.5698088Losses:  10.718317031860352 10.639347076416016 0.03948507830500603
CurrentTrain: epoch  7, batch     0 | loss: 10.7183170Losses:  4.393193244934082 4.341453552246094 0.02586979977786541
CurrentTrain: epoch  7, batch     1 | loss: 4.3931932Losses:  6.810645580291748 6.723783493041992 0.0434311218559742
CurrentTrain: epoch  7, batch     2 | loss: 6.8106456Losses:  7.09703254699707 7.027005672454834 0.03501337766647339
CurrentTrain: epoch  7, batch     3 | loss: 7.0970325Losses:  6.3638505935668945 6.301597595214844 0.03112647868692875
CurrentTrain: epoch  7, batch     4 | loss: 6.3638506Losses:  5.16764497756958 5.114326477050781 0.026659177616238594
CurrentTrain: epoch  7, batch     5 | loss: 5.1676450Losses:  4.426792621612549 4.37753963470459 0.024626411497592926
CurrentTrain: epoch  7, batch     6 | loss: 4.4267926Losses:  4.617860794067383 4.573637962341309 0.02211149036884308
CurrentTrain: epoch  7, batch     7 | loss: 4.6178608Losses:  6.2253098487854 6.163849830627441 0.03073004260659218
CurrentTrain: epoch  7, batch     8 | loss: 6.2253098Losses:  5.751686096191406 5.700636863708496 0.02552461437880993
CurrentTrain: epoch  7, batch     9 | loss: 5.7516861Losses:  4.5318803787231445 4.4763336181640625 0.027773497626185417
CurrentTrain: epoch  7, batch    10 | loss: 4.5318804Losses:  6.007899761199951 5.948118686676025 0.02989042177796364
CurrentTrain: epoch  7, batch    11 | loss: 6.0078998Losses:  4.583629131317139 4.535240173339844 0.02419457770884037
CurrentTrain: epoch  7, batch    12 | loss: 4.5836291Losses:  5.161835670471191 5.1145219802856445 0.02365679293870926
CurrentTrain: epoch  7, batch    13 | loss: 5.1618357Losses:  7.282415390014648 7.212515830993652 0.034949835389852524
CurrentTrain: epoch  7, batch    14 | loss: 7.2824154Losses:  4.101802825927734 4.049886703491211 0.02595808170735836
CurrentTrain: epoch  7, batch    15 | loss: 4.1018028Losses:  4.6457977294921875 4.591145038604736 0.02732643485069275
CurrentTrain: epoch  7, batch    16 | loss: 4.6457977Losses:  5.802069664001465 5.740392684936523 0.030838586390018463
CurrentTrain: epoch  7, batch    17 | loss: 5.8020697Losses:  5.5021748542785645 5.425705909729004 0.038234420120716095
CurrentTrain: epoch  7, batch    18 | loss: 5.5021749Losses:  5.865229606628418 5.812776565551758 0.026226595044136047
CurrentTrain: epoch  7, batch    19 | loss: 5.8652296Losses:  5.447216987609863 5.395533561706543 0.025841746479272842
CurrentTrain: epoch  7, batch    20 | loss: 5.4472170Losses:  4.941710472106934 4.890694618225098 0.025507880374789238
CurrentTrain: epoch  7, batch    21 | loss: 4.9417105Losses:  5.148646831512451 5.093784332275391 0.027431173250079155
CurrentTrain: epoch  7, batch    22 | loss: 5.1486468Losses:  9.411192893981934 9.317108154296875 0.04704245924949646
CurrentTrain: epoch  7, batch    23 | loss: 9.4111929Losses:  5.774364948272705 5.713040351867676 0.030662182718515396
CurrentTrain: epoch  7, batch    24 | loss: 5.7743649Losses:  6.1799397468566895 6.12611198425293 0.026913926005363464
CurrentTrain: epoch  7, batch    25 | loss: 6.1799397Losses:  8.897454261779785 8.818096160888672 0.03967912495136261
CurrentTrain: epoch  7, batch    26 | loss: 8.8974543Losses:  6.016347408294678 5.963440895080566 0.026453359052538872
CurrentTrain: epoch  7, batch    27 | loss: 6.0163474Losses:  7.665802478790283 7.590063571929932 0.037869375199079514
CurrentTrain: epoch  7, batch    28 | loss: 7.6658025Losses:  4.720102787017822 4.666954040527344 0.026574324816465378
CurrentTrain: epoch  7, batch    29 | loss: 4.7201028Losses:  4.997771263122559 4.94444465637207 0.02666335552930832
CurrentTrain: epoch  7, batch    30 | loss: 4.9977713Losses:  6.153132915496826 6.0860981941223145 0.03351747244596481
CurrentTrain: epoch  7, batch    31 | loss: 6.1531329Losses:  4.5133161544799805 4.451861381530762 0.030727500095963478
CurrentTrain: epoch  7, batch    32 | loss: 4.5133162Losses:  5.654722690582275 5.598087310791016 0.028317764401435852
CurrentTrain: epoch  7, batch    33 | loss: 5.6547227Losses:  6.609546184539795 6.55265998840332 0.028443025425076485
CurrentTrain: epoch  7, batch    34 | loss: 6.6095462Losses:  3.466707229614258 3.4247686862945557 0.020969243720173836
CurrentTrain: epoch  7, batch    35 | loss: 3.4667072Losses:  6.2979230880737305 6.226746082305908 0.035588521510362625
CurrentTrain: epoch  7, batch    36 | loss: 6.2979231Losses:  1.5065549612045288 1.4174137115478516 0.044570643454790115
CurrentTrain: epoch  7, batch    37 | loss: 1.5065550Losses:  6.7882914543151855 6.727731227874756 0.030280135571956635
CurrentTrain: epoch  8, batch     0 | loss: 6.7882915Losses:  4.281737804412842 4.225697994232178 0.028019791468977928
CurrentTrain: epoch  8, batch     1 | loss: 4.2817378Losses:  9.538728713989258 9.47694206237793 0.030893374234437943
CurrentTrain: epoch  8, batch     2 | loss: 9.5387287Losses:  4.281371116638184 4.228464126586914 0.026453590020537376
CurrentTrain: epoch  8, batch     3 | loss: 4.2813711Losses:  4.80027961730957 4.746265411376953 0.027007220312952995
CurrentTrain: epoch  8, batch     4 | loss: 4.8002796Losses:  4.627191066741943 4.574484825134277 0.026353104040026665
CurrentTrain: epoch  8, batch     5 | loss: 4.6271911Losses:  7.020398139953613 6.953724384307861 0.033336929976940155
CurrentTrain: epoch  8, batch     6 | loss: 7.0203981Losses:  6.582828044891357 6.522618293762207 0.03010478988289833
CurrentTrain: epoch  8, batch     7 | loss: 6.5828280Losses:  5.474130630493164 5.4237446784973145 0.02519291080534458
CurrentTrain: epoch  8, batch     8 | loss: 5.4741306Losses:  3.954706907272339 3.9073100090026855 0.02369847148656845
CurrentTrain: epoch  8, batch     9 | loss: 3.9547069Losses:  6.113706588745117 6.038009166717529 0.037848830223083496
CurrentTrain: epoch  8, batch    10 | loss: 6.1137066Losses:  6.191633224487305 6.143276214599609 0.02417851984500885
CurrentTrain: epoch  8, batch    11 | loss: 6.1916332Losses:  4.828094482421875 4.777167797088623 0.025463256984949112
CurrentTrain: epoch  8, batch    12 | loss: 4.8280945Losses:  5.033397197723389 4.977421760559082 0.027987798675894737
CurrentTrain: epoch  8, batch    13 | loss: 5.0333972Losses:  4.785469055175781 4.7245683670043945 0.0304502472281456
CurrentTrain: epoch  8, batch    14 | loss: 4.7854691Losses:  4.823663234710693 4.7744245529174805 0.02461937442421913
CurrentTrain: epoch  8, batch    15 | loss: 4.8236632Losses:  7.4206109046936035 7.354886054992676 0.03286240994930267
CurrentTrain: epoch  8, batch    16 | loss: 7.4206109Losses:  8.791730880737305 8.726189613342285 0.03277052566409111
CurrentTrain: epoch  8, batch    17 | loss: 8.7917309Losses:  4.680633068084717 4.619963645935059 0.030334606766700745
CurrentTrain: epoch  8, batch    18 | loss: 4.6806331Losses:  5.845344066619873 5.792977333068848 0.026183396577835083
CurrentTrain: epoch  8, batch    19 | loss: 5.8453441Losses:  7.4756269454956055 7.393057346343994 0.041284821927547455
CurrentTrain: epoch  8, batch    20 | loss: 7.4756269Losses:  5.3591179847717285 5.309754371643066 0.02468189038336277
CurrentTrain: epoch  8, batch    21 | loss: 5.3591180Losses:  7.202323913574219 7.125830173492432 0.03824681416153908
CurrentTrain: epoch  8, batch    22 | loss: 7.2023239Losses:  5.515572547912598 5.455541610717773 0.030015427619218826
CurrentTrain: epoch  8, batch    23 | loss: 5.5155725Losses:  5.758697986602783 5.697088241577148 0.03080495074391365
CurrentTrain: epoch  8, batch    24 | loss: 5.7586980Losses:  5.1436638832092285 5.077509880065918 0.03307688981294632
CurrentTrain: epoch  8, batch    25 | loss: 5.1436639Losses:  6.797584533691406 6.727564811706543 0.03500998020172119
CurrentTrain: epoch  8, batch    26 | loss: 6.7975845Losses:  6.280068874359131 6.204143524169922 0.037962570786476135
CurrentTrain: epoch  8, batch    27 | loss: 6.2800689Losses:  5.900979518890381 5.847463130950928 0.02675830014050007
CurrentTrain: epoch  8, batch    28 | loss: 5.9009795Losses:  8.412967681884766 8.353231430053711 0.029868092387914658
CurrentTrain: epoch  8, batch    29 | loss: 8.4129677Losses:  4.960336685180664 4.911192417144775 0.024572210386395454
CurrentTrain: epoch  8, batch    30 | loss: 4.9603367Losses:  9.756168365478516 9.673510551452637 0.041329145431518555
CurrentTrain: epoch  8, batch    31 | loss: 9.7561684Losses:  5.898523807525635 5.816267967224121 0.04112780839204788
CurrentTrain: epoch  8, batch    32 | loss: 5.8985238Losses:  4.5531535148620605 4.50128173828125 0.02593589574098587
CurrentTrain: epoch  8, batch    33 | loss: 4.5531535Losses:  3.7858335971832275 3.740907669067383 0.02246292680501938
CurrentTrain: epoch  8, batch    34 | loss: 3.7858336Losses:  5.543063163757324 5.477414131164551 0.03282451629638672
CurrentTrain: epoch  8, batch    35 | loss: 5.5430632Losses:  3.3667311668395996 3.324338912963867 0.0211961530148983
CurrentTrain: epoch  8, batch    36 | loss: 3.3667312Losses:  1.560246467590332 1.4660999774932861 0.04707322269678116
CurrentTrain: epoch  8, batch    37 | loss: 1.5602465Losses:  5.021183013916016 4.963783264160156 0.028699809685349464
CurrentTrain: epoch  9, batch     0 | loss: 5.0211830Losses:  9.656001091003418 9.595385551452637 0.030307544395327568
CurrentTrain: epoch  9, batch     1 | loss: 9.6560011Losses:  3.8691749572753906 3.827198028564453 0.020988505333662033
CurrentTrain: epoch  9, batch     2 | loss: 3.8691750Losses:  4.986314296722412 4.932677268981934 0.026818476617336273
CurrentTrain: epoch  9, batch     3 | loss: 4.9863143Losses:  5.161020755767822 5.102852821350098 0.029083965346217155
CurrentTrain: epoch  9, batch     4 | loss: 5.1610208Losses:  5.003833770751953 4.946222305297852 0.02880573645234108
CurrentTrain: epoch  9, batch     5 | loss: 5.0038338Losses:  5.809774875640869 5.764206409454346 0.022784234955906868
CurrentTrain: epoch  9, batch     6 | loss: 5.8097749Losses:  5.294314861297607 5.234001636505127 0.030156610533595085
CurrentTrain: epoch  9, batch     7 | loss: 5.2943149Losses:  3.445451021194458 3.4005565643310547 0.022447191178798676
CurrentTrain: epoch  9, batch     8 | loss: 3.4454510Losses:  9.786490440368652 9.734457969665527 0.026016272604465485
CurrentTrain: epoch  9, batch     9 | loss: 9.7864904Losses:  6.287154197692871 6.207411289215088 0.03987141698598862
CurrentTrain: epoch  9, batch    10 | loss: 6.2871542Losses:  7.518104076385498 7.453366279602051 0.0323689728975296
CurrentTrain: epoch  9, batch    11 | loss: 7.5181041Losses:  4.60593318939209 4.557858467102051 0.024037420749664307
CurrentTrain: epoch  9, batch    12 | loss: 4.6059332Losses:  4.929038047790527 4.866631031036377 0.031203435733914375
CurrentTrain: epoch  9, batch    13 | loss: 4.9290380Losses:  6.200907230377197 6.132699489593506 0.034103844314813614
CurrentTrain: epoch  9, batch    14 | loss: 6.2009072Losses:  4.164215564727783 4.112572193145752 0.02582164853811264
CurrentTrain: epoch  9, batch    15 | loss: 4.1642156Losses:  5.907151222229004 5.845076560974121 0.03103743866086006
CurrentTrain: epoch  9, batch    16 | loss: 5.9071512Losses:  7.151712894439697 7.09397029876709 0.02887134812772274
CurrentTrain: epoch  9, batch    17 | loss: 7.1517129Losses:  6.713184833526611 6.666571140289307 0.023306824266910553
CurrentTrain: epoch  9, batch    18 | loss: 6.7131848Losses:  5.974835395812988 5.9101362228393555 0.03234958276152611
CurrentTrain: epoch  9, batch    19 | loss: 5.9748354Losses:  5.520750522613525 5.465640068054199 0.027555210515856743
CurrentTrain: epoch  9, batch    20 | loss: 5.5207505Losses:  6.076895236968994 6.013104438781738 0.03189532086253166
CurrentTrain: epoch  9, batch    21 | loss: 6.0768952Losses:  7.012120246887207 6.936595916748047 0.03776216506958008
CurrentTrain: epoch  9, batch    22 | loss: 7.0121202Losses:  8.947358131408691 8.858257293701172 0.04455040395259857
CurrentTrain: epoch  9, batch    23 | loss: 8.9473581Losses:  5.806780815124512 5.7460713386535645 0.03035484440624714
CurrentTrain: epoch  9, batch    24 | loss: 5.8067808Losses:  4.992952346801758 4.934736251831055 0.029108166694641113
CurrentTrain: epoch  9, batch    25 | loss: 4.9929523Losses:  6.564367294311523 6.498958587646484 0.032704420387744904
CurrentTrain: epoch  9, batch    26 | loss: 6.5643673Losses:  6.458470344543457 6.403164863586426 0.027652734890580177
CurrentTrain: epoch  9, batch    27 | loss: 6.4584703Losses:  4.237111568450928 4.187983512878418 0.024564066901803017
CurrentTrain: epoch  9, batch    28 | loss: 4.2371116Losses:  6.488145351409912 6.406857490539551 0.04064398631453514
CurrentTrain: epoch  9, batch    29 | loss: 6.4881454Losses:  8.85306453704834 8.759331703186035 0.04686661437153816
CurrentTrain: epoch  9, batch    30 | loss: 8.8530645Losses:  6.9049530029296875 6.836297035217285 0.0343279093503952
CurrentTrain: epoch  9, batch    31 | loss: 6.9049530Losses:  5.287246227264404 5.224836349487305 0.031204821541905403
CurrentTrain: epoch  9, batch    32 | loss: 5.2872462Losses:  5.378978729248047 5.3338470458984375 0.022565845400094986
CurrentTrain: epoch  9, batch    33 | loss: 5.3789787Losses:  7.501015663146973 7.43710994720459 0.03195282444357872
CurrentTrain: epoch  9, batch    34 | loss: 7.5010157Losses:  4.9933671951293945 4.925698280334473 0.03383450210094452
CurrentTrain: epoch  9, batch    35 | loss: 4.9933672Losses:  6.085268974304199 6.022812843322754 0.031227989122271538
CurrentTrain: epoch  9, batch    36 | loss: 6.0852690Losses:  0.7032745480537415 0.6293046474456787 0.03698495402932167
CurrentTrain: epoch  9, batch    37 | loss: 0.7032745
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 85.29%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 84.03%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 83.88%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 84.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.51%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 86.14%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.73%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.17%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.58%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.91%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 87.50%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 85.29%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 84.03%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 83.88%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 84.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.51%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 86.14%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.73%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.17%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.58%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.91%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 87.50%   
cur_acc:  ['0.8750']
his_acc:  ['0.8750']
Clustering into  4  clusters
Clusters:  [1 0 0 1 0 2 1 1 3 0 1]
Losses:  10.237062454223633 9.049020767211914 0.5940209031105042
CurrentTrain: epoch  0, batch     0 | loss: 10.2370625Losses:  3.5171396732330322 2.9703049659729004 0.2734173834323883
CurrentTrain: epoch  0, batch     1 | loss: 3.5171397Losses:  8.936381340026855 8.409361839294434 0.2635098695755005
CurrentTrain: epoch  1, batch     0 | loss: 8.9363813Losses:  2.7884349822998047 2.2167787551879883 0.2858280837535858
CurrentTrain: epoch  1, batch     1 | loss: 2.7884350Losses:  7.663907051086426 7.286868095397949 0.18851947784423828
CurrentTrain: epoch  2, batch     0 | loss: 7.6639071Losses:  3.054487705230713 2.617774248123169 0.21835677325725555
CurrentTrain: epoch  2, batch     1 | loss: 3.0544877Losses:  8.575611114501953 8.210946083068848 0.18233273923397064
CurrentTrain: epoch  3, batch     0 | loss: 8.5756111Losses:  3.009059190750122 2.839874029159546 0.08459262549877167
CurrentTrain: epoch  3, batch     1 | loss: 3.0090592Losses:  7.025720119476318 6.625227451324463 0.20024624466896057
CurrentTrain: epoch  4, batch     0 | loss: 7.0257201Losses:  2.1168718338012695 1.75417959690094 0.1813461035490036
CurrentTrain: epoch  4, batch     1 | loss: 2.1168718Losses:  7.336879253387451 7.0229291915893555 0.15697506070137024
CurrentTrain: epoch  5, batch     0 | loss: 7.3368793Losses:  2.6791114807128906 2.290595531463623 0.1942579448223114
CurrentTrain: epoch  5, batch     1 | loss: 2.6791115Losses:  6.958309173583984 6.638238430023193 0.16003535687923431
CurrentTrain: epoch  6, batch     0 | loss: 6.9583092Losses:  1.9676949977874756 1.6728028059005737 0.14744609594345093
CurrentTrain: epoch  6, batch     1 | loss: 1.9676950Losses:  7.036301136016846 6.724555969238281 0.15587252378463745
CurrentTrain: epoch  7, batch     0 | loss: 7.0363011Losses:  3.179065465927124 2.8183932304382324 0.1803361475467682
CurrentTrain: epoch  7, batch     1 | loss: 3.1790655Losses:  7.133479595184326 6.80977725982666 0.16185115277767181
CurrentTrain: epoch  8, batch     0 | loss: 7.1334796Losses:  3.4324865341186523 3.128842353820801 0.15182213485240936
CurrentTrain: epoch  8, batch     1 | loss: 3.4324865Losses:  5.864518165588379 5.577049732208252 0.14373423159122467
CurrentTrain: epoch  9, batch     0 | loss: 5.8645182Losses:  1.6380417346954346 1.351104736328125 0.14346852898597717
CurrentTrain: epoch  9, batch     1 | loss: 1.6380417
Losses:  0.55483078956604 -0.0 0.27741539478302
MemoryTrain:  epoch  0, batch     0 | loss: 0.5548308Losses:  0.5231727361679077 -0.0 0.26158636808395386
MemoryTrain:  epoch  1, batch     0 | loss: 0.5231727Losses:  0.48979586362838745 -0.0 0.24489793181419373
MemoryTrain:  epoch  2, batch     0 | loss: 0.4897959Losses:  0.47833794355392456 -0.0 0.23916897177696228
MemoryTrain:  epoch  3, batch     0 | loss: 0.4783379Losses:  0.46520182490348816 -0.0 0.23260091245174408
MemoryTrain:  epoch  4, batch     0 | loss: 0.4652018Losses:  0.45350342988967896 -0.0 0.22675171494483948
MemoryTrain:  epoch  5, batch     0 | loss: 0.4535034Losses:  0.4344256520271301 -0.0 0.21721282601356506
MemoryTrain:  epoch  6, batch     0 | loss: 0.4344257Losses:  0.41668885946273804 -0.0 0.20834442973136902
MemoryTrain:  epoch  7, batch     0 | loss: 0.4166889Losses:  0.41047561168670654 -0.0 0.20523780584335327
MemoryTrain:  epoch  8, batch     0 | loss: 0.4104756Losses:  0.41153502464294434 -0.0 0.20576751232147217
MemoryTrain:  epoch  9, batch     0 | loss: 0.4115350Losses:  0.4114413857460022 -0.0 0.2057206928730011
MemoryTrain:  epoch 10, batch     0 | loss: 0.4114414Losses:  0.40570640563964844 -0.0 0.20285320281982422
MemoryTrain:  epoch 11, batch     0 | loss: 0.4057064Losses:  0.40509551763534546 -0.0 0.20254775881767273
MemoryTrain:  epoch 12, batch     0 | loss: 0.4050955Losses:  0.4052921533584595 -0.0 0.20264607667922974
MemoryTrain:  epoch 13, batch     0 | loss: 0.4052922Losses:  0.4043768644332886 -0.0 0.2021884322166443
MemoryTrain:  epoch 14, batch     0 | loss: 0.4043769Losses:  0.4041372835636139 -0.0 0.20206864178180695
MemoryTrain:  epoch 15, batch     0 | loss: 0.4041373Losses:  0.4032602310180664 -0.0 0.2016301155090332
MemoryTrain:  epoch 16, batch     0 | loss: 0.4032602Losses:  0.40301936864852905 -0.0 0.20150968432426453
MemoryTrain:  epoch 17, batch     0 | loss: 0.4030194Losses:  0.4013414978981018 -0.0 0.2006707489490509
MemoryTrain:  epoch 18, batch     0 | loss: 0.4013415Losses:  0.3987325429916382 -0.0 0.1993662714958191
MemoryTrain:  epoch 19, batch     0 | loss: 0.3987325
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 60.94%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 65.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 67.86%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 70.31%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 73.61%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 75.57%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 76.04%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 76.92%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 74.55%   
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 8.33%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 7.81%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 6.25%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 12.50%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 17.19%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 21.53%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 22.50%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 25.00%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 27.60%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 28.85%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 30.36%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 33.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 34.77%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 36.76%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 38.19%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 39.47%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 41.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 44.64%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 47.16%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 49.46%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 51.30%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 53.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 55.05%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 56.48%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 58.04%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 59.48%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 60.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 61.49%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 62.69%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 62.13%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 63.18%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 63.32%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 63.78%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 64.22%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 64.94%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 66.13%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 66.48%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 66.85%   [EVAL] batch:   46 | acc: 6.25%,  total acc: 65.56%   
cur_acc:  ['0.8750', '0.7455']
his_acc:  ['0.8750', '0.6556']
Clustering into  7  clusters
Clusters:  [1 0 2 1 0 4 6 1 5 0 1 1 2 2 3 2]
Losses:  7.24753475189209 6.499627113342285 0.37395375967025757
CurrentTrain: epoch  0, batch     0 | loss: 7.2475348Losses:  4.302431583404541 3.67043137550354 0.31600016355514526
CurrentTrain: epoch  0, batch     1 | loss: 4.3024316Losses:  7.481690406799316 6.854959487915039 0.31336551904678345
CurrentTrain: epoch  1, batch     0 | loss: 7.4816904Losses:  4.715422630310059 4.349581241607666 0.1829206347465515
CurrentTrain: epoch  1, batch     1 | loss: 4.7154226Losses:  7.683529853820801 7.216834545135498 0.23334768414497375
CurrentTrain: epoch  2, batch     0 | loss: 7.6835299Losses:  2.158146858215332 1.7846437692642212 0.18675152957439423
CurrentTrain: epoch  2, batch     1 | loss: 2.1581469Losses:  9.054235458374023 8.684616088867188 0.18480955064296722
CurrentTrain: epoch  3, batch     0 | loss: 9.0542355Losses:  4.168858051300049 3.981309652328491 0.09377414733171463
CurrentTrain: epoch  3, batch     1 | loss: 4.1688581Losses:  6.506104946136475 6.227825164794922 0.1391398012638092
CurrentTrain: epoch  4, batch     0 | loss: 6.5061049Losses:  3.2564525604248047 2.7943575382232666 0.23104755580425262
CurrentTrain: epoch  4, batch     1 | loss: 3.2564526Losses:  7.403501510620117 7.0957560539245605 0.15387260913848877
CurrentTrain: epoch  5, batch     0 | loss: 7.4035015Losses:  3.5171656608581543 3.212850570678711 0.15215758979320526
CurrentTrain: epoch  5, batch     1 | loss: 3.5171657Losses:  6.556461811065674 6.236575126647949 0.1599433869123459
CurrentTrain: epoch  6, batch     0 | loss: 6.5564618Losses:  1.0910379886627197 0.8244233727455139 0.13330727815628052
CurrentTrain: epoch  6, batch     1 | loss: 1.0910380Losses:  6.579195499420166 6.324709415435791 0.1272430121898651
CurrentTrain: epoch  7, batch     0 | loss: 6.5791955Losses:  4.410986423492432 4.118009090423584 0.14648877084255219
CurrentTrain: epoch  7, batch     1 | loss: 4.4109864Losses:  7.248019695281982 6.957247734069824 0.1453859508037567
CurrentTrain: epoch  8, batch     0 | loss: 7.2480197Losses:  1.9142537117004395 1.83575439453125 0.03924967721104622
CurrentTrain: epoch  8, batch     1 | loss: 1.9142537Losses:  6.291500091552734 6.02418851852417 0.13365575671195984
CurrentTrain: epoch  9, batch     0 | loss: 6.2915001Losses:  2.242280960083008 1.9594658613204956 0.1414075791835785
CurrentTrain: epoch  9, batch     1 | loss: 2.2422810
Losses:  1.2316105365753174 -0.0 0.6158052682876587
MemoryTrain:  epoch  0, batch     0 | loss: 1.2316105Losses:  1.1929163932800293 -0.0 0.5964581966400146
MemoryTrain:  epoch  1, batch     0 | loss: 1.1929164Losses:  1.1516081094741821 -0.0 0.5758040547370911
MemoryTrain:  epoch  2, batch     0 | loss: 1.1516081Losses:  1.137592077255249 -0.0 0.5687960386276245
MemoryTrain:  epoch  3, batch     0 | loss: 1.1375921Losses:  1.1221163272857666 -0.0 0.5610581636428833
MemoryTrain:  epoch  4, batch     0 | loss: 1.1221163Losses:  1.1245613098144531 -0.0 0.5622806549072266
MemoryTrain:  epoch  5, batch     0 | loss: 1.1245613Losses:  1.0945185422897339 -0.0 0.5472592711448669
MemoryTrain:  epoch  6, batch     0 | loss: 1.0945185Losses:  1.076146125793457 -0.0 0.5380730628967285
MemoryTrain:  epoch  7, batch     0 | loss: 1.0761461Losses:  1.0617058277130127 -0.0 0.5308529138565063
MemoryTrain:  epoch  8, batch     0 | loss: 1.0617058Losses:  1.0553197860717773 -0.0 0.5276598930358887
MemoryTrain:  epoch  9, batch     0 | loss: 1.0553198Losses:  1.0572280883789062 -0.0 0.5286140441894531
MemoryTrain:  epoch 10, batch     0 | loss: 1.0572281Losses:  1.0643770694732666 -0.0 0.5321885347366333
MemoryTrain:  epoch 11, batch     0 | loss: 1.0643771Losses:  1.0465253591537476 -0.0 0.5232626795768738
MemoryTrain:  epoch 12, batch     0 | loss: 1.0465254Losses:  1.0342183113098145 -0.0 0.5171091556549072
MemoryTrain:  epoch 13, batch     0 | loss: 1.0342183Losses:  1.026027798652649 -0.0 0.5130138993263245
MemoryTrain:  epoch 14, batch     0 | loss: 1.0260278Losses:  1.0285595655441284 -0.0 0.5142797827720642
MemoryTrain:  epoch 15, batch     0 | loss: 1.0285596Losses:  1.031922698020935 -0.0 0.5159613490104675
MemoryTrain:  epoch 16, batch     0 | loss: 1.0319227Losses:  1.0383881330490112 -0.0 0.5191940665245056
MemoryTrain:  epoch 17, batch     0 | loss: 1.0383881Losses:  1.0124919414520264 -0.0 0.5062459707260132
MemoryTrain:  epoch 18, batch     0 | loss: 1.0124919Losses:  1.0163731575012207 -0.0 0.5081865787506104
MemoryTrain:  epoch 19, batch     0 | loss: 1.0163732
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 72.92%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 67.97%   
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 20.83%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 17.19%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 17.50%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 16.67%   [EVAL] batch:    6 | acc: 6.25%,  total acc: 15.18%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 13.28%   [EVAL] batch:    8 | acc: 0.00%,  total acc: 11.81%   [EVAL] batch:    9 | acc: 0.00%,  total acc: 10.62%   [EVAL] batch:   10 | acc: 0.00%,  total acc: 9.66%   [EVAL] batch:   11 | acc: 12.50%,  total acc: 9.90%   [EVAL] batch:   12 | acc: 6.25%,  total acc: 9.62%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 11.16%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 15.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 17.97%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 21.32%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 23.61%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 26.32%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 29.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 32.74%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 35.80%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 38.59%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 40.89%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 43.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 45.43%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 47.22%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 49.11%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 50.86%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 52.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 53.83%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 55.08%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 55.11%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 55.51%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 56.25%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 56.94%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 57.77%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 58.55%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 59.13%   [EVAL] batch:   39 | acc: 37.50%,  total acc: 58.59%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 57.93%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 57.59%   [EVAL] batch:   42 | acc: 25.00%,  total acc: 56.83%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 56.68%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 56.53%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 55.71%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 56.12%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 56.51%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 55.87%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 56.38%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 56.99%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 57.69%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 58.25%   [EVAL] batch:   53 | acc: 18.75%,  total acc: 57.52%   
cur_acc:  ['0.8750', '0.7455', '0.6797']
his_acc:  ['0.8750', '0.6556', '0.5752']
Clustering into  9  clusters
Clusters:  [0 3 2 0 8 5 6 0 1 3 0 0 2 2 4 2 7 2 5 1 0]
Losses:  8.394632339477539 7.19083309173584 0.6018998026847839
CurrentTrain: epoch  0, batch     0 | loss: 8.3946323Losses:  2.8470702171325684 2.0255208015441895 0.41077470779418945
CurrentTrain: epoch  0, batch     1 | loss: 2.8470702Losses:  8.304418563842773 7.459136486053467 0.4226412773132324
CurrentTrain: epoch  1, batch     0 | loss: 8.3044186Losses:  3.2147057056427 2.6030023097991943 0.30585169792175293
CurrentTrain: epoch  1, batch     1 | loss: 3.2147057Losses:  8.138622283935547 7.304466247558594 0.41707801818847656
CurrentTrain: epoch  2, batch     0 | loss: 8.1386223Losses:  4.16363000869751 3.5816404819488525 0.2909947633743286
CurrentTrain: epoch  2, batch     1 | loss: 4.1636300Losses:  7.907340049743652 7.081701278686523 0.4128192961215973
CurrentTrain: epoch  3, batch     0 | loss: 7.9073400Losses:  2.349658966064453 1.8108092546463013 0.2694249153137207
CurrentTrain: epoch  3, batch     1 | loss: 2.3496590Losses:  6.607794761657715 5.831441879272461 0.38817641139030457
CurrentTrain: epoch  4, batch     0 | loss: 6.6077948Losses:  2.351461410522461 1.5240521430969238 0.41370460391044617
CurrentTrain: epoch  4, batch     1 | loss: 2.3514614Losses:  6.491982460021973 5.743772506713867 0.3741048574447632
CurrentTrain: epoch  5, batch     0 | loss: 6.4919825Losses:  2.153257369995117 1.4326244592666626 0.3603164255619049
CurrentTrain: epoch  5, batch     1 | loss: 2.1532574Losses:  6.044846534729004 5.322168350219727 0.3613390624523163
CurrentTrain: epoch  6, batch     0 | loss: 6.0448465Losses:  3.40036678314209 2.6172893047332764 0.3915387988090515
CurrentTrain: epoch  6, batch     1 | loss: 3.4003668Losses:  7.604862689971924 6.874815940856934 0.3650234639644623
CurrentTrain: epoch  7, batch     0 | loss: 7.6048627Losses:  3.84952974319458 3.305974006652832 0.27177780866622925
CurrentTrain: epoch  7, batch     1 | loss: 3.8495297Losses:  5.8622002601623535 5.175868511199951 0.3431658446788788
CurrentTrain: epoch  8, batch     0 | loss: 5.8622003Losses:  2.3609559535980225 1.6671572923660278 0.3468993008136749
CurrentTrain: epoch  8, batch     1 | loss: 2.3609560Losses:  5.868376731872559 5.189676284790039 0.33935031294822693
CurrentTrain: epoch  9, batch     0 | loss: 5.8683767Losses:  2.338918685913086 1.6597063541412354 0.33960622549057007
CurrentTrain: epoch  9, batch     1 | loss: 2.3389187
Losses:  1.4701348543167114 -0.0 0.7350674271583557
MemoryTrain:  epoch  0, batch     0 | loss: 1.4701349Losses:  0.5416934490203857 -0.0 0.27084672451019287
MemoryTrain:  epoch  0, batch     1 | loss: 0.5416934Losses:  1.2494474649429321 -0.0 0.6247237324714661
MemoryTrain:  epoch  1, batch     0 | loss: 1.2494475Losses:  0.5890647172927856 -0.0 0.2945323586463928
MemoryTrain:  epoch  1, batch     1 | loss: 0.5890647Losses:  1.2586309909820557 -0.0 0.6293154954910278
MemoryTrain:  epoch  2, batch     0 | loss: 1.2586310Losses:  0.42029905319213867 -0.0 0.21014952659606934
MemoryTrain:  epoch  2, batch     1 | loss: 0.4202991Losses:  1.4100714921951294 -0.0 0.7050357460975647
MemoryTrain:  epoch  3, batch     0 | loss: 1.4100715Losses:  0.6327759623527527 -0.0 0.31638798117637634
MemoryTrain:  epoch  3, batch     1 | loss: 0.6327760Losses:  1.3801796436309814 -0.0 0.6900898218154907
MemoryTrain:  epoch  4, batch     0 | loss: 1.3801796Losses:  0.6964602470397949 -0.0 0.34823012351989746
MemoryTrain:  epoch  4, batch     1 | loss: 0.6964602Losses:  1.3401316404342651 -0.0 0.6700658202171326
MemoryTrain:  epoch  5, batch     0 | loss: 1.3401316Losses:  0.5359172821044922 -0.0 0.2679586410522461
MemoryTrain:  epoch  5, batch     1 | loss: 0.5359173Losses:  1.1966136693954468 -0.0 0.5983068346977234
MemoryTrain:  epoch  6, batch     0 | loss: 1.1966137Losses:  0.9118804931640625 -0.0 0.45594024658203125
MemoryTrain:  epoch  6, batch     1 | loss: 0.9118805Losses:  1.2380335330963135 -0.0 0.6190167665481567
MemoryTrain:  epoch  7, batch     0 | loss: 1.2380335Losses:  0.5616223216056824 -0.0 0.2808111608028412
MemoryTrain:  epoch  7, batch     1 | loss: 0.5616223Losses:  1.4010568857192993 -0.0 0.7005284428596497
MemoryTrain:  epoch  8, batch     0 | loss: 1.4010569Losses:  0.9628535509109497 -0.0 0.48142677545547485
MemoryTrain:  epoch  8, batch     1 | loss: 0.9628536Losses:  1.348859190940857 -0.0 0.6744295954704285
MemoryTrain:  epoch  9, batch     0 | loss: 1.3488592Losses:  0.3102782368659973 -0.0 0.15513911843299866
MemoryTrain:  epoch  9, batch     1 | loss: 0.3102782Losses:  1.0497702360153198 -0.0 0.5248851180076599
MemoryTrain:  epoch 10, batch     0 | loss: 1.0497702Losses:  0.6514403820037842 -0.0 0.3257201910018921
MemoryTrain:  epoch 10, batch     1 | loss: 0.6514404Losses:  1.1827243566513062 -0.0 0.5913621783256531
MemoryTrain:  epoch 11, batch     0 | loss: 1.1827244Losses:  0.6468182802200317 -0.0 0.32340914011001587
MemoryTrain:  epoch 11, batch     1 | loss: 0.6468183Losses:  1.2627737522125244 -0.0 0.6313868761062622
MemoryTrain:  epoch 12, batch     0 | loss: 1.2627738Losses:  0.9095538854598999 -0.0 0.45477694272994995
MemoryTrain:  epoch 12, batch     1 | loss: 0.9095539Losses:  1.213779330253601 -0.0 0.6068896651268005
MemoryTrain:  epoch 13, batch     0 | loss: 1.2137793Losses:  0.3225043714046478 -0.0 0.1612521857023239
MemoryTrain:  epoch 13, batch     1 | loss: 0.3225044Losses:  1.3418954610824585 -0.0 0.6709477305412292
MemoryTrain:  epoch 14, batch     0 | loss: 1.3418955Losses:  0.37445181608200073 -0.0 0.18722590804100037
MemoryTrain:  epoch 14, batch     1 | loss: 0.3744518Losses:  1.0831217765808105 -0.0 0.5415608882904053
MemoryTrain:  epoch 15, batch     0 | loss: 1.0831218Losses:  0.5487105846405029 -0.0 0.27435529232025146
MemoryTrain:  epoch 15, batch     1 | loss: 0.5487106Losses:  1.1694746017456055 -0.0 0.5847373008728027
MemoryTrain:  epoch 16, batch     0 | loss: 1.1694746Losses:  0.37194758653640747 -0.0 0.18597379326820374
MemoryTrain:  epoch 16, batch     1 | loss: 0.3719476Losses:  1.2340415716171265 -0.0 0.6170207858085632
MemoryTrain:  epoch 17, batch     0 | loss: 1.2340416Losses:  0.19688498973846436 -0.0 0.09844249486923218
MemoryTrain:  epoch 17, batch     1 | loss: 0.1968850Losses:  1.2002609968185425 -0.0 0.6001304984092712
MemoryTrain:  epoch 18, batch     0 | loss: 1.2002610Losses:  0.6455813646316528 -0.0 0.3227906823158264
MemoryTrain:  epoch 18, batch     1 | loss: 0.6455814Losses:  1.2308893203735352 -0.0 0.6154446601867676
MemoryTrain:  epoch 19, batch     0 | loss: 1.2308893Losses:  0.3677496314048767 -0.0 0.18387481570243835
MemoryTrain:  epoch 19, batch     1 | loss: 0.3677496
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 75.69%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 76.25%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 76.14%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 74.04%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 3.12%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 6.25%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    6 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 5.47%   [EVAL] batch:    8 | acc: 0.00%,  total acc: 4.86%   [EVAL] batch:    9 | acc: 0.00%,  total acc: 4.38%   [EVAL] batch:   10 | acc: 0.00%,  total acc: 3.98%   [EVAL] batch:   11 | acc: 0.00%,  total acc: 3.65%   [EVAL] batch:   12 | acc: 6.25%,  total acc: 3.85%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 4.46%   [EVAL] batch:   14 | acc: 25.00%,  total acc: 5.83%   [EVAL] batch:   15 | acc: 25.00%,  total acc: 7.03%   [EVAL] batch:   16 | acc: 12.50%,  total acc: 7.35%   [EVAL] batch:   17 | acc: 18.75%,  total acc: 7.99%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 10.20%   [EVAL] batch:   19 | acc: 43.75%,  total acc: 11.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 16.07%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 19.89%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 23.37%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 26.56%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 29.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 32.21%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 34.49%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 36.38%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 38.15%   [EVAL] batch:   29 | acc: 62.50%,  total acc: 38.96%   [EVAL] batch:   30 | acc: 62.50%,  total acc: 39.72%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 40.82%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 42.23%   [EVAL] batch:   33 | acc: 37.50%,  total acc: 42.10%   [EVAL] batch:   34 | acc: 18.75%,  total acc: 41.43%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 40.45%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 39.36%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 38.49%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 37.66%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 38.12%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 39.33%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 39.88%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 40.41%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 41.05%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 41.81%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 42.39%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 43.22%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 43.88%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 44.64%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 45.75%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 46.81%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 47.84%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 48.82%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 48.73%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 48.75%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 49.00%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 49.45%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 50.32%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 51.17%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 51.88%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 52.46%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 52.92%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 53.17%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 53.71%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 54.04%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 54.07%   
cur_acc:  ['0.8750', '0.7455', '0.6797', '0.7404']
his_acc:  ['0.8750', '0.6556', '0.5752', '0.5407']
Clustering into  12  clusters
Clusters:  [ 4  1  0  4  8  3  6  2  5  1  2  4 11  0  9  0  7 10  3  5  2  0  0  0
  3  5]
Losses:  9.015568733215332 8.630877494812012 0.1923455446958542
CurrentTrain: epoch  0, batch     0 | loss: 9.0155687Losses:  2.2396366596221924 1.8654676675796509 0.18708452582359314
CurrentTrain: epoch  0, batch     1 | loss: 2.2396367Losses:  7.624874591827393 7.234592437744141 0.1951410472393036
CurrentTrain: epoch  1, batch     0 | loss: 7.6248746Losses:  2.1657936573028564 1.8671528100967407 0.14932039380073547
CurrentTrain: epoch  1, batch     1 | loss: 2.1657937Losses:  6.7922821044921875 6.447723388671875 0.1722794771194458
CurrentTrain: epoch  2, batch     0 | loss: 6.7922821Losses:  3.3775722980499268 3.0675249099731445 0.1550237238407135
CurrentTrain: epoch  2, batch     1 | loss: 3.3775723Losses:  9.032257080078125 8.720355033874512 0.15595094859600067
CurrentTrain: epoch  3, batch     0 | loss: 9.0322571Losses:  3.289106845855713 2.996953010559082 0.14607688784599304
CurrentTrain: epoch  3, batch     1 | loss: 3.2891068Losses:  7.465585708618164 7.200233459472656 0.13267618417739868
CurrentTrain: epoch  4, batch     0 | loss: 7.4655857Losses:  4.927774429321289 4.689455032348633 0.1191597729921341
CurrentTrain: epoch  4, batch     1 | loss: 4.9277744Losses:  6.605185508728027 6.344037055969238 0.1305742859840393
CurrentTrain: epoch  5, batch     0 | loss: 6.6051855Losses:  3.502734661102295 3.211155891418457 0.14578932523727417
CurrentTrain: epoch  5, batch     1 | loss: 3.5027347Losses:  7.541269779205322 7.29060173034668 0.12533408403396606
CurrentTrain: epoch  6, batch     0 | loss: 7.5412698Losses:  3.576570987701416 3.280056953430176 0.14825700223445892
CurrentTrain: epoch  6, batch     1 | loss: 3.5765710Losses:  6.845090866088867 6.602532386779785 0.12127932161092758
CurrentTrain: epoch  7, batch     0 | loss: 6.8450909Losses:  5.606383323669434 5.323226451873779 0.14157834649085999
CurrentTrain: epoch  7, batch     1 | loss: 5.6063833Losses:  7.171700477600098 6.911429405212402 0.13013559579849243
CurrentTrain: epoch  8, batch     0 | loss: 7.1717005Losses:  3.9678258895874023 3.817237138748169 0.07529440522193909
CurrentTrain: epoch  8, batch     1 | loss: 3.9678259Losses:  7.430458068847656 7.179900646209717 0.12527874112129211
CurrentTrain: epoch  9, batch     0 | loss: 7.4304581Losses:  3.3280749320983887 3.1909115314483643 0.06858164072036743
CurrentTrain: epoch  9, batch     1 | loss: 3.3280749
Losses:  1.5197111368179321 -0.0 0.7598555684089661
MemoryTrain:  epoch  0, batch     0 | loss: 1.5197111Losses:  1.4907379150390625 -0.0 0.7453689575195312
MemoryTrain:  epoch  0, batch     1 | loss: 1.4907379Losses:  1.5759564638137817 -0.0 0.7879782319068909
MemoryTrain:  epoch  1, batch     0 | loss: 1.5759565Losses:  0.8607144355773926 -0.0 0.4303572177886963
MemoryTrain:  epoch  1, batch     1 | loss: 0.8607144Losses:  1.2983001470565796 -0.0 0.6491500735282898
MemoryTrain:  epoch  2, batch     0 | loss: 1.2983001Losses:  1.416160225868225 -0.0 0.7080801129341125
MemoryTrain:  epoch  2, batch     1 | loss: 1.4161602Losses:  1.559728980064392 -0.0 0.779864490032196
MemoryTrain:  epoch  3, batch     0 | loss: 1.5597290Losses:  1.2407406568527222 -0.0 0.6203703284263611
MemoryTrain:  epoch  3, batch     1 | loss: 1.2407407Losses:  1.3840404748916626 -0.0 0.6920202374458313
MemoryTrain:  epoch  4, batch     0 | loss: 1.3840405Losses:  1.1173795461654663 -0.0 0.5586897730827332
MemoryTrain:  epoch  4, batch     1 | loss: 1.1173795Losses:  1.5343490839004517 -0.0 0.7671745419502258
MemoryTrain:  epoch  5, batch     0 | loss: 1.5343491Losses:  1.0531948804855347 -0.0 0.5265974402427673
MemoryTrain:  epoch  5, batch     1 | loss: 1.0531949Losses:  0.8302125930786133 -0.0 0.41510629653930664
MemoryTrain:  epoch  6, batch     0 | loss: 0.8302126Losses:  1.3512630462646484 -0.0 0.6756315231323242
MemoryTrain:  epoch  6, batch     1 | loss: 1.3512630Losses:  1.2746829986572266 -0.0 0.6373414993286133
MemoryTrain:  epoch  7, batch     0 | loss: 1.2746830Losses:  1.2063629627227783 -0.0 0.6031814813613892
MemoryTrain:  epoch  7, batch     1 | loss: 1.2063630Losses:  1.4809030294418335 -0.0 0.7404515147209167
MemoryTrain:  epoch  8, batch     0 | loss: 1.4809030Losses:  1.3841912746429443 -0.0 0.6920956373214722
MemoryTrain:  epoch  8, batch     1 | loss: 1.3841913Losses:  1.4402631521224976 -0.0 0.7201315760612488
MemoryTrain:  epoch  9, batch     0 | loss: 1.4402632Losses:  1.391592264175415 -0.0 0.6957961320877075
MemoryTrain:  epoch  9, batch     1 | loss: 1.3915923Losses:  1.423606276512146 -0.0 0.711803138256073
MemoryTrain:  epoch 10, batch     0 | loss: 1.4236063Losses:  1.1274752616882324 -0.0 0.5637376308441162
MemoryTrain:  epoch 10, batch     1 | loss: 1.1274753Losses:  1.3955881595611572 -0.0 0.6977940797805786
MemoryTrain:  epoch 11, batch     0 | loss: 1.3955882Losses:  1.056138038635254 -0.0 0.528069019317627
MemoryTrain:  epoch 11, batch     1 | loss: 1.0561380Losses:  1.3952842950820923 -0.0 0.6976421475410461
MemoryTrain:  epoch 12, batch     0 | loss: 1.3952843Losses:  1.2018648386001587 -0.0 0.6009324193000793
MemoryTrain:  epoch 12, batch     1 | loss: 1.2018648Losses:  1.2457115650177002 -0.0 0.6228557825088501
MemoryTrain:  epoch 13, batch     0 | loss: 1.2457116Losses:  1.258825421333313 -0.0 0.6294127106666565
MemoryTrain:  epoch 13, batch     1 | loss: 1.2588254Losses:  1.3591022491455078 -0.0 0.6795511245727539
MemoryTrain:  epoch 14, batch     0 | loss: 1.3591022Losses:  1.1862471103668213 -0.0 0.5931235551834106
MemoryTrain:  epoch 14, batch     1 | loss: 1.1862471Losses:  1.4367272853851318 -0.0 0.7183636426925659
MemoryTrain:  epoch 15, batch     0 | loss: 1.4367273Losses:  1.3533931970596313 -0.0 0.6766965985298157
MemoryTrain:  epoch 15, batch     1 | loss: 1.3533932Losses:  1.510439157485962 -0.0 0.755219578742981
MemoryTrain:  epoch 16, batch     0 | loss: 1.5104392Losses:  0.7810301184654236 -0.0 0.3905150592327118
MemoryTrain:  epoch 16, batch     1 | loss: 0.7810301Losses:  1.6265925168991089 -0.0 0.8132962584495544
MemoryTrain:  epoch 17, batch     0 | loss: 1.6265925Losses:  0.6872320771217346 -0.0 0.3436160385608673
MemoryTrain:  epoch 17, batch     1 | loss: 0.6872321Losses:  1.3167340755462646 -0.0 0.6583670377731323
MemoryTrain:  epoch 18, batch     0 | loss: 1.3167341Losses:  1.2040332555770874 -0.0 0.6020166277885437
MemoryTrain:  epoch 18, batch     1 | loss: 1.2040333Losses:  1.405737042427063 -0.0 0.7028685212135315
MemoryTrain:  epoch 19, batch     0 | loss: 1.4057370Losses:  0.9340478777885437 -0.0 0.46702393889427185
MemoryTrain:  epoch 19, batch     1 | loss: 0.9340479
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 25.00%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 26.56%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 26.25%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 28.12%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 26.79%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 28.12%   [EVAL] batch:    8 | acc: 12.50%,  total acc: 26.39%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 28.12%   [EVAL] batch:   10 | acc: 18.75%,  total acc: 27.27%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 27.60%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 27.88%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 32.14%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 35.83%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 39.45%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 41.91%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 43.75%   [EVAL] batch:   18 | acc: 25.00%,  total acc: 42.76%   [EVAL] batch:   19 | acc: 31.25%,  total acc: 42.19%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 42.26%   [EVAL] batch:   21 | acc: 18.75%,  total acc: 41.19%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 64.29%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 56.25%   [EVAL] batch:    8 | acc: 0.00%,  total acc: 50.00%   [EVAL] batch:    9 | acc: 0.00%,  total acc: 45.00%   [EVAL] batch:   10 | acc: 0.00%,  total acc: 40.91%   [EVAL] batch:   11 | acc: 0.00%,  total acc: 37.50%   [EVAL] batch:   12 | acc: 0.00%,  total acc: 34.62%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 33.93%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 36.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 37.89%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 40.07%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 41.32%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 42.43%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 44.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 47.32%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 49.72%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 51.90%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 53.65%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 55.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 57.21%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 58.56%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 60.04%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 61.42%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 62.08%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 63.10%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 64.06%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 64.96%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 64.34%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 62.68%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 61.11%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 59.80%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 58.22%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 56.89%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 57.19%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 58.23%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 58.48%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 58.87%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 59.09%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 59.86%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 59.24%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 59.04%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 59.51%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 59.18%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 59.75%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 60.54%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 61.30%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 62.03%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 61.92%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 62.27%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 62.39%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 63.15%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 63.77%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 64.38%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 64.45%   [EVAL] batch:   61 | acc: 37.50%,  total acc: 64.01%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 63.79%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 64.06%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 63.17%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 62.31%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 61.85%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 61.40%   [EVAL] batch:   68 | acc: 12.50%,  total acc: 60.69%   [EVAL] batch:   69 | acc: 25.00%,  total acc: 60.18%   [EVAL] batch:   70 | acc: 37.50%,  total acc: 59.86%   [EVAL] batch:   71 | acc: 31.25%,  total acc: 59.46%   [EVAL] batch:   72 | acc: 18.75%,  total acc: 58.90%   [EVAL] batch:   73 | acc: 31.25%,  total acc: 58.53%   [EVAL] batch:   74 | acc: 18.75%,  total acc: 58.00%   [EVAL] batch:   75 | acc: 43.75%,  total acc: 57.81%   [EVAL] batch:   76 | acc: 18.75%,  total acc: 57.31%   [EVAL] batch:   77 | acc: 31.25%,  total acc: 56.97%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 56.88%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 57.27%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 57.64%   [EVAL] batch:   81 | acc: 93.75%,  total acc: 58.08%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 58.21%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 58.33%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 58.01%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 57.63%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 57.54%   [EVAL] batch:   87 | acc: 6.25%,  total acc: 56.96%   
cur_acc:  ['0.8750', '0.7455', '0.6797', '0.7404', '0.4119']
his_acc:  ['0.8750', '0.6556', '0.5752', '0.5407', '0.5696']
Clustering into  14  clusters
Clusters:  [ 4  3  1  4  0  2  9  4  7  3  5  4 12  1 10 13  6  8  2  7  5  1  1  1
  2  7 10 11  4  1  0]
Losses:  9.073600769042969 8.264294624328613 0.40465328097343445
CurrentTrain: epoch  0, batch     0 | loss: 9.0736008Losses:  5.387603759765625 5.006797790527344 0.1904030293226242
CurrentTrain: epoch  0, batch     1 | loss: 5.3876038Losses:  6.677371025085449 5.93438720703125 0.37149181962013245
CurrentTrain: epoch  1, batch     0 | loss: 6.6773710Losses:  2.585718870162964 1.7947747707366943 0.39547207951545715
CurrentTrain: epoch  1, batch     1 | loss: 2.5857189Losses:  6.484650611877441 5.779839515686035 0.3524056375026703
CurrentTrain: epoch  2, batch     0 | loss: 6.4846506Losses:  2.260608196258545 1.5215421915054321 0.369532972574234
CurrentTrain: epoch  2, batch     1 | loss: 2.2606082Losses:  6.256531715393066 5.553194522857666 0.35166850686073303
CurrentTrain: epoch  3, batch     0 | loss: 6.2565317Losses:  2.2510035037994385 1.5599377155303955 0.3455328941345215
CurrentTrain: epoch  3, batch     1 | loss: 2.2510035Losses:  6.045764446258545 5.384346961975098 0.33070871233940125
CurrentTrain: epoch  4, batch     0 | loss: 6.0457644Losses:  2.1006627082824707 1.4193322658538818 0.3406652808189392
CurrentTrain: epoch  4, batch     1 | loss: 2.1006627Losses:  6.33806037902832 5.676016330718994 0.33102208375930786
CurrentTrain: epoch  5, batch     0 | loss: 6.3380604Losses:  2.2038254737854004 1.700190782546997 0.25181734561920166
CurrentTrain: epoch  5, batch     1 | loss: 2.2038255Losses:  5.786615371704102 5.141413688659668 0.3226008117198944
CurrentTrain: epoch  6, batch     0 | loss: 5.7866154Losses:  2.1614086627960205 1.5060852766036987 0.3276617228984833
CurrentTrain: epoch  6, batch     1 | loss: 2.1614087Losses:  5.734364986419678 5.100461006164551 0.3169519007205963
CurrentTrain: epoch  7, batch     0 | loss: 5.7343650Losses:  2.0783987045288086 1.43401038646698 0.3221941590309143
CurrentTrain: epoch  7, batch     1 | loss: 2.0783987Losses:  6.227965831756592 5.599640846252441 0.31416258215904236
CurrentTrain: epoch  8, batch     0 | loss: 6.2279658Losses:  2.125210762023926 1.625671625137329 0.24976959824562073
CurrentTrain: epoch  8, batch     1 | loss: 2.1252108Losses:  5.631906986236572 5.017297744750977 0.3073046803474426
CurrentTrain: epoch  9, batch     0 | loss: 5.6319070Losses:  2.0569381713867188 1.4326667785644531 0.3121356666088104
CurrentTrain: epoch  9, batch     1 | loss: 2.0569382
Losses:  1.4648774862289429 -0.0 0.7324387431144714
MemoryTrain:  epoch  0, batch     0 | loss: 1.4648775Losses:  1.7427634000778198 -0.0 0.8713817000389099
MemoryTrain:  epoch  0, batch     1 | loss: 1.7427634Losses:  1.5926326513290405 -0.0 0.7963163256645203
MemoryTrain:  epoch  1, batch     0 | loss: 1.5926327Losses:  1.5806361436843872 -0.0 0.7903180718421936
MemoryTrain:  epoch  1, batch     1 | loss: 1.5806361Losses:  1.6926764249801636 -0.0 0.8463382124900818
MemoryTrain:  epoch  2, batch     0 | loss: 1.6926764Losses:  1.3445054292678833 -0.0 0.6722527146339417
MemoryTrain:  epoch  2, batch     1 | loss: 1.3445054Losses:  1.7323832511901855 -0.0 0.8661916255950928
MemoryTrain:  epoch  3, batch     0 | loss: 1.7323833Losses:  1.4032514095306396 -0.0 0.7016257047653198
MemoryTrain:  epoch  3, batch     1 | loss: 1.4032514Losses:  1.4341962337493896 -0.0 0.7170981168746948
MemoryTrain:  epoch  4, batch     0 | loss: 1.4341962Losses:  1.694178819656372 -0.0 0.847089409828186
MemoryTrain:  epoch  4, batch     1 | loss: 1.6941788Losses:  1.24356210231781 -0.0 0.621781051158905
MemoryTrain:  epoch  5, batch     0 | loss: 1.2435621Losses:  1.5362383127212524 -0.0 0.7681191563606262
MemoryTrain:  epoch  5, batch     1 | loss: 1.5362383Losses:  1.426444172859192 -0.0 0.713222086429596
MemoryTrain:  epoch  6, batch     0 | loss: 1.4264442Losses:  1.8329038619995117 -0.0 0.9164519309997559
MemoryTrain:  epoch  6, batch     1 | loss: 1.8329039Losses:  1.5568634271621704 -0.0 0.7784317135810852
MemoryTrain:  epoch  7, batch     0 | loss: 1.5568634Losses:  1.4469152688980103 -0.0 0.7234576344490051
MemoryTrain:  epoch  7, batch     1 | loss: 1.4469153Losses:  1.7024116516113281 -0.0 0.8512058258056641
MemoryTrain:  epoch  8, batch     0 | loss: 1.7024117Losses:  1.4745509624481201 -0.0 0.7372754812240601
MemoryTrain:  epoch  8, batch     1 | loss: 1.4745510Losses:  1.3902459144592285 -0.0 0.6951229572296143
MemoryTrain:  epoch  9, batch     0 | loss: 1.3902459Losses:  1.5191887617111206 -0.0 0.7595943808555603
MemoryTrain:  epoch  9, batch     1 | loss: 1.5191888Losses:  1.7040231227874756 -0.0 0.8520115613937378
MemoryTrain:  epoch 10, batch     0 | loss: 1.7040231Losses:  1.4515513181686401 -0.0 0.7257756590843201
MemoryTrain:  epoch 10, batch     1 | loss: 1.4515513Losses:  1.7725328207015991 -0.0 0.8862664103507996
MemoryTrain:  epoch 11, batch     0 | loss: 1.7725328Losses:  1.3582048416137695 -0.0 0.6791024208068848
MemoryTrain:  epoch 11, batch     1 | loss: 1.3582048Losses:  1.7116775512695312 -0.0 0.8558387756347656
MemoryTrain:  epoch 12, batch     0 | loss: 1.7116776Losses:  1.5589786767959595 -0.0 0.7794893383979797
MemoryTrain:  epoch 12, batch     1 | loss: 1.5589787Losses:  1.5684797763824463 -0.0 0.7842398881912231
MemoryTrain:  epoch 13, batch     0 | loss: 1.5684798Losses:  1.4730671644210815 -0.0 0.7365335822105408
MemoryTrain:  epoch 13, batch     1 | loss: 1.4730672Losses:  1.4735872745513916 -0.0 0.7367936372756958
MemoryTrain:  epoch 14, batch     0 | loss: 1.4735873Losses:  1.3767410516738892 -0.0 0.6883705258369446
MemoryTrain:  epoch 14, batch     1 | loss: 1.3767411Losses:  1.856327772140503 -0.0 0.9281638860702515
MemoryTrain:  epoch 15, batch     0 | loss: 1.8563278Losses:  1.3326503038406372 -0.0 0.6663251519203186
MemoryTrain:  epoch 15, batch     1 | loss: 1.3326503Losses:  1.7562692165374756 -0.0 0.8781346082687378
MemoryTrain:  epoch 16, batch     0 | loss: 1.7562692Losses:  1.429726004600525 -0.0 0.7148630023002625
MemoryTrain:  epoch 16, batch     1 | loss: 1.4297260Losses:  1.515968918800354 -0.0 0.757984459400177
MemoryTrain:  epoch 17, batch     0 | loss: 1.5159689Losses:  1.6077239513397217 -0.0 0.8038619756698608
MemoryTrain:  epoch 17, batch     1 | loss: 1.6077240Losses:  1.5926382541656494 -0.0 0.7963191270828247
MemoryTrain:  epoch 18, batch     0 | loss: 1.5926383Losses:  1.3729370832443237 -0.0 0.6864685416221619
MemoryTrain:  epoch 18, batch     1 | loss: 1.3729371Losses:  1.3764054775238037 -0.0 0.6882027387619019
MemoryTrain:  epoch 19, batch     0 | loss: 1.3764055Losses:  1.7173209190368652 -0.0 0.8586604595184326
MemoryTrain:  epoch 19, batch     1 | loss: 1.7173209
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 99.22%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 95.83%   [EVAL] batch:    9 | acc: 25.00%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 25.00%,  total acc: 82.95%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 79.17%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 75.96%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 72.77%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 57.50%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 55.21%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 50.89%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 46.09%   [EVAL] batch:    8 | acc: 18.75%,  total acc: 43.06%   [EVAL] batch:    9 | acc: 0.00%,  total acc: 38.75%   [EVAL] batch:   10 | acc: 18.75%,  total acc: 36.93%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 36.98%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 35.10%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 34.38%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 37.08%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 38.28%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 40.44%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 41.67%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 42.76%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 45.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 47.62%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 50.00%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 52.17%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 53.91%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 55.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 57.45%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 58.80%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 60.04%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 61.42%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 61.88%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 62.70%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 63.48%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 64.39%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 63.05%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 61.61%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 60.07%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 58.78%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 57.40%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 56.09%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 56.41%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 57.32%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 57.74%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 58.28%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 58.52%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 59.03%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 58.29%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 58.11%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 58.46%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 57.65%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 57.63%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 57.97%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 58.41%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 58.73%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 58.91%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 59.55%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 59.93%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 59.43%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 60.02%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 60.70%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 61.15%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 61.37%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 61.19%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 61.01%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 61.33%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 60.38%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 59.56%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 58.96%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 58.73%   [EVAL] batch:   68 | acc: 6.25%,  total acc: 57.97%   [EVAL] batch:   69 | acc: 31.25%,  total acc: 57.59%   [EVAL] batch:   70 | acc: 31.25%,  total acc: 57.22%   [EVAL] batch:   71 | acc: 31.25%,  total acc: 56.86%   [EVAL] batch:   72 | acc: 18.75%,  total acc: 56.34%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 56.17%   [EVAL] batch:   74 | acc: 25.00%,  total acc: 55.75%   [EVAL] batch:   75 | acc: 50.00%,  total acc: 55.67%   [EVAL] batch:   76 | acc: 18.75%,  total acc: 55.19%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 54.81%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 54.67%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 55.08%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 55.40%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 55.72%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 55.87%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 55.95%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 55.44%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 54.94%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 54.67%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 54.97%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 55.48%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 55.97%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 56.46%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 56.93%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 57.39%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 57.85%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 58.22%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 58.46%   [EVAL] batch:   96 | acc: 18.75%,  total acc: 58.05%   [EVAL] batch:   97 | acc: 25.00%,  total acc: 57.72%   [EVAL] batch:   98 | acc: 37.50%,  total acc: 57.51%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 57.38%   [EVAL] batch:  100 | acc: 37.50%,  total acc: 57.18%   
cur_acc:  ['0.8750', '0.7455', '0.6797', '0.7404', '0.4119', '0.7277']
his_acc:  ['0.8750', '0.6556', '0.5752', '0.5407', '0.5696', '0.5718']
Clustering into  17  clusters
Clusters:  [ 1  3  9  1  0  8 13  1  4  3  5  1 14  9  6 15  2 10  8  4  5  9  9 16
  8  4  6  7  1  9 11  1  2 12  0  4]
Losses:  9.896974563598633 8.979374885559082 0.4587996006011963
CurrentTrain: epoch  0, batch     0 | loss: 9.8969746Losses:  5.475789546966553 4.68997049331665 0.39290952682495117
CurrentTrain: epoch  0, batch     1 | loss: 5.4757895Losses:  9.25182056427002 8.315652847290039 0.46808376908302307
CurrentTrain: epoch  1, batch     0 | loss: 9.2518206Losses:  4.5019917488098145 3.477058172225952 0.5124667882919312
CurrentTrain: epoch  1, batch     1 | loss: 4.5019917Losses:  7.329927444458008 6.5081706047058105 0.41087833046913147
CurrentTrain: epoch  2, batch     0 | loss: 7.3299274Losses:  3.708649158477783 2.860293388366699 0.4241778552532196
CurrentTrain: epoch  2, batch     1 | loss: 3.7086492Losses:  7.933125019073486 7.01051139831543 0.4613068699836731
CurrentTrain: epoch  3, batch     0 | loss: 7.9331250Losses:  1.6871154308319092 0.9846409559249878 0.3512372076511383
CurrentTrain: epoch  3, batch     1 | loss: 1.6871154Losses:  7.014875411987305 6.232813358306885 0.39103102684020996
CurrentTrain: epoch  4, batch     0 | loss: 7.0148754Losses:  3.591783046722412 2.7439846992492676 0.42389923334121704
CurrentTrain: epoch  4, batch     1 | loss: 3.5917830Losses:  6.790011405944824 5.966935157775879 0.41153818368911743
CurrentTrain: epoch  5, batch     0 | loss: 6.7900114Losses:  2.3986105918884277 1.6156092882156372 0.3915006220340729
CurrentTrain: epoch  5, batch     1 | loss: 2.3986106Losses:  7.672029972076416 6.881042957305908 0.39549341797828674
CurrentTrain: epoch  6, batch     0 | loss: 7.6720300Losses:  4.51788330078125 3.76723051071167 0.3753262758255005
CurrentTrain: epoch  6, batch     1 | loss: 4.5178833Losses:  7.618590354919434 6.81976842880249 0.39941105246543884
CurrentTrain: epoch  7, batch     0 | loss: 7.6185904Losses:  3.972839832305908 3.3135457038879395 0.32964712381362915
CurrentTrain: epoch  7, batch     1 | loss: 3.9728398Losses:  7.654553413391113 6.7898454666137695 0.4323539733886719
CurrentTrain: epoch  8, batch     0 | loss: 7.6545534Losses:  2.083197593688965 1.412882685661316 0.33515748381614685
CurrentTrain: epoch  8, batch     1 | loss: 2.0831976Losses:  7.476778984069824 6.757409572601318 0.35968470573425293
CurrentTrain: epoch  9, batch     0 | loss: 7.4767790Losses:  2.9932498931884766 2.1634724140167236 0.41488873958587646
CurrentTrain: epoch  9, batch     1 | loss: 2.9932499
Losses:  1.734918475151062 -0.0 0.867459237575531
MemoryTrain:  epoch  0, batch     0 | loss: 1.7349185Losses:  1.6038955450057983 -0.0 0.8019477725028992
MemoryTrain:  epoch  0, batch     1 | loss: 1.6038955Losses:  0.600172221660614 -0.0 0.300086110830307
MemoryTrain:  epoch  0, batch     2 | loss: 0.6001722Losses:  1.424990177154541 -0.0 0.7124950885772705
MemoryTrain:  epoch  1, batch     0 | loss: 1.4249902Losses:  2.118060827255249 -0.0 1.0590304136276245
MemoryTrain:  epoch  1, batch     1 | loss: 2.1180608Losses:  0.3279420733451843 -0.0 0.16397103667259216
MemoryTrain:  epoch  1, batch     2 | loss: 0.3279421Losses:  1.607444405555725 -0.0 0.8037222027778625
MemoryTrain:  epoch  2, batch     0 | loss: 1.6074444Losses:  1.5700637102127075 -0.0 0.7850318551063538
MemoryTrain:  epoch  2, batch     1 | loss: 1.5700637Losses:  0.5498120784759521 -0.0 0.2749060392379761
MemoryTrain:  epoch  2, batch     2 | loss: 0.5498121Losses:  1.6949924230575562 -0.0 0.8474962115287781
MemoryTrain:  epoch  3, batch     0 | loss: 1.6949924Losses:  1.8330860137939453 -0.0 0.9165430068969727
MemoryTrain:  epoch  3, batch     1 | loss: 1.8330860Losses:  0.44412899017333984 -0.0 0.22206449508666992
MemoryTrain:  epoch  3, batch     2 | loss: 0.4441290Losses:  1.877078890800476 -0.0 0.938539445400238
MemoryTrain:  epoch  4, batch     0 | loss: 1.8770789Losses:  1.560940146446228 -0.0 0.780470073223114
MemoryTrain:  epoch  4, batch     1 | loss: 1.5609401Losses:  0.4046348035335541 -0.0 0.20231740176677704
MemoryTrain:  epoch  4, batch     2 | loss: 0.4046348Losses:  1.7501667737960815 -0.0 0.8750833868980408
MemoryTrain:  epoch  5, batch     0 | loss: 1.7501668Losses:  1.8196289539337158 -0.0 0.9098144769668579
MemoryTrain:  epoch  5, batch     1 | loss: 1.8196290Losses:  0.5791352987289429 -0.0 0.28956764936447144
MemoryTrain:  epoch  5, batch     2 | loss: 0.5791353Losses:  1.5310505628585815 -0.0 0.7655252814292908
MemoryTrain:  epoch  6, batch     0 | loss: 1.5310506Losses:  1.6699132919311523 -0.0 0.8349566459655762
MemoryTrain:  epoch  6, batch     1 | loss: 1.6699133Losses:  0.40782031416893005 -0.0 0.20391015708446503
MemoryTrain:  epoch  6, batch     2 | loss: 0.4078203Losses:  1.4996989965438843 -0.0 0.7498494982719421
MemoryTrain:  epoch  7, batch     0 | loss: 1.4996990Losses:  1.6661605834960938 -0.0 0.8330802917480469
MemoryTrain:  epoch  7, batch     1 | loss: 1.6661606Losses:  0.6292893886566162 -0.0 0.3146446943283081
MemoryTrain:  epoch  7, batch     2 | loss: 0.6292894Losses:  1.2118457555770874 -0.0 0.6059228777885437
MemoryTrain:  epoch  8, batch     0 | loss: 1.2118458Losses:  1.6152396202087402 -0.0 0.8076198101043701
MemoryTrain:  epoch  8, batch     1 | loss: 1.6152396Losses:  0.7083879113197327 -0.0 0.35419395565986633
MemoryTrain:  epoch  8, batch     2 | loss: 0.7083879Losses:  1.6558157205581665 -0.0 0.8279078602790833
MemoryTrain:  epoch  9, batch     0 | loss: 1.6558157Losses:  1.5171960592269897 -0.0 0.7585980296134949
MemoryTrain:  epoch  9, batch     1 | loss: 1.5171961Losses:  0.5887636542320251 -0.0 0.2943818271160126
MemoryTrain:  epoch  9, batch     2 | loss: 0.5887637Losses:  1.5037391185760498 -0.0 0.7518695592880249
MemoryTrain:  epoch 10, batch     0 | loss: 1.5037391Losses:  1.6790262460708618 -0.0 0.8395131230354309
MemoryTrain:  epoch 10, batch     1 | loss: 1.6790262Losses:  0.6140889525413513 -0.0 0.30704447627067566
MemoryTrain:  epoch 10, batch     2 | loss: 0.6140890Losses:  1.4700357913970947 -0.0 0.7350178956985474
MemoryTrain:  epoch 11, batch     0 | loss: 1.4700358Losses:  1.7662937641143799 -0.0 0.8831468820571899
MemoryTrain:  epoch 11, batch     1 | loss: 1.7662938Losses:  0.6746216416358948 -0.0 0.3373108208179474
MemoryTrain:  epoch 11, batch     2 | loss: 0.6746216Losses:  1.4843066930770874 -0.0 0.7421533465385437
MemoryTrain:  epoch 12, batch     0 | loss: 1.4843067Losses:  1.5553749799728394 -0.0 0.7776874899864197
MemoryTrain:  epoch 12, batch     1 | loss: 1.5553750Losses:  0.5138579607009888 -0.0 0.2569289803504944
MemoryTrain:  epoch 12, batch     2 | loss: 0.5138580Losses:  1.6711076498031616 -0.0 0.8355538249015808
MemoryTrain:  epoch 13, batch     0 | loss: 1.6711076Losses:  1.6754145622253418 -0.0 0.8377072811126709
MemoryTrain:  epoch 13, batch     1 | loss: 1.6754146Losses:  0.6765504479408264 -0.0 0.3382752239704132
MemoryTrain:  epoch 13, batch     2 | loss: 0.6765504Losses:  1.5775521993637085 -0.0 0.7887760996818542
MemoryTrain:  epoch 14, batch     0 | loss: 1.5775522Losses:  1.7153739929199219 -0.0 0.8576869964599609
MemoryTrain:  epoch 14, batch     1 | loss: 1.7153740Losses:  0.3512237071990967 -0.0 0.17561185359954834
MemoryTrain:  epoch 14, batch     2 | loss: 0.3512237Losses:  1.6794774532318115 -0.0 0.8397387266159058
MemoryTrain:  epoch 15, batch     0 | loss: 1.6794775Losses:  1.767425298690796 -0.0 0.883712649345398
MemoryTrain:  epoch 15, batch     1 | loss: 1.7674253Losses:  0.4637255072593689 -0.0 0.23186275362968445
MemoryTrain:  epoch 15, batch     2 | loss: 0.4637255Losses:  1.339310646057129 -0.0 0.6696553230285645
MemoryTrain:  epoch 16, batch     0 | loss: 1.3393106Losses:  1.8896464109420776 -0.0 0.9448232054710388
MemoryTrain:  epoch 16, batch     1 | loss: 1.8896464Losses:  0.3604915738105774 -0.0 0.1802457869052887
MemoryTrain:  epoch 16, batch     2 | loss: 0.3604916Losses:  1.6853612661361694 -0.0 0.8426806330680847
MemoryTrain:  epoch 17, batch     0 | loss: 1.6853613Losses:  1.4719629287719727 -0.0 0.7359814643859863
MemoryTrain:  epoch 17, batch     1 | loss: 1.4719629Losses:  0.6308997273445129 -0.0 0.31544986367225647
MemoryTrain:  epoch 17, batch     2 | loss: 0.6308997Losses:  1.700433611869812 -0.0 0.850216805934906
MemoryTrain:  epoch 18, batch     0 | loss: 1.7004336Losses:  1.5164908170700073 -0.0 0.7582454085350037
MemoryTrain:  epoch 18, batch     1 | loss: 1.5164908Losses:  0.41536760330200195 -0.0 0.20768380165100098
MemoryTrain:  epoch 18, batch     2 | loss: 0.4153676Losses:  2.0073108673095703 -0.0 1.0036554336547852
MemoryTrain:  epoch 19, batch     0 | loss: 2.0073109Losses:  1.2144381999969482 -0.0 0.6072190999984741
MemoryTrain:  epoch 19, batch     1 | loss: 1.2144382Losses:  0.6614384055137634 -0.0 0.3307192027568817
MemoryTrain:  epoch 19, batch     2 | loss: 0.6614384
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 16.67%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 14.06%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 21.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 30.21%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 39.29%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 43.75%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 47.92%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 49.38%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 48.30%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 47.92%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 49.52%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 50.89%   [EVAL] batch:   14 | acc: 18.75%,  total acc: 48.75%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 28.12%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 28.75%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 28.12%   [EVAL] batch:    6 | acc: 12.50%,  total acc: 25.89%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 23.44%   [EVAL] batch:    8 | acc: 12.50%,  total acc: 22.22%   [EVAL] batch:    9 | acc: 0.00%,  total acc: 20.00%   [EVAL] batch:   10 | acc: 6.25%,  total acc: 18.75%   [EVAL] batch:   11 | acc: 25.00%,  total acc: 19.27%   [EVAL] batch:   12 | acc: 6.25%,  total acc: 18.27%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 18.75%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 22.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 24.61%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 27.57%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 29.51%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 31.25%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 34.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 37.20%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 40.06%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 42.39%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 44.53%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 46.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 48.80%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 50.23%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 51.56%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 53.02%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 53.75%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 55.04%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 57.01%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 55.51%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 53.93%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 52.60%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 51.35%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 50.16%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 49.04%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 49.53%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 50.61%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 51.19%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 51.89%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 52.41%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 52.92%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 52.31%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 52.26%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 52.60%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 51.79%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 52.12%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 52.45%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 52.88%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 53.42%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 54.05%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 54.55%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 55.13%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 54.93%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 55.60%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 56.36%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 56.88%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 56.76%   [EVAL] batch:   61 | acc: 37.50%,  total acc: 56.45%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 56.05%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 55.38%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 54.64%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 54.20%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 53.95%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 53.53%   [EVAL] batch:   69 | acc: 31.25%,  total acc: 53.21%   [EVAL] batch:   70 | acc: 37.50%,  total acc: 52.99%   [EVAL] batch:   71 | acc: 37.50%,  total acc: 52.78%   [EVAL] batch:   72 | acc: 37.50%,  total acc: 52.57%   [EVAL] batch:   73 | acc: 31.25%,  total acc: 52.28%   [EVAL] batch:   74 | acc: 31.25%,  total acc: 52.00%   [EVAL] batch:   75 | acc: 18.75%,  total acc: 51.56%   [EVAL] batch:   76 | acc: 31.25%,  total acc: 51.30%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 50.96%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 50.79%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 51.09%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 51.31%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 51.75%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 52.03%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 52.16%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 51.76%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 51.31%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 51.08%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 51.49%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 51.97%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 52.43%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 52.95%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 53.46%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 53.97%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 54.45%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 54.87%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 55.14%   [EVAL] batch:   96 | acc: 6.25%,  total acc: 54.64%   [EVAL] batch:   97 | acc: 0.00%,  total acc: 54.08%   [EVAL] batch:   98 | acc: 43.75%,  total acc: 53.98%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 53.87%   [EVAL] batch:  100 | acc: 37.50%,  total acc: 53.71%   [EVAL] batch:  101 | acc: 31.25%,  total acc: 53.49%   [EVAL] batch:  102 | acc: 12.50%,  total acc: 53.09%   [EVAL] batch:  103 | acc: 6.25%,  total acc: 52.64%   [EVAL] batch:  104 | acc: 12.50%,  total acc: 52.26%   [EVAL] batch:  105 | acc: 43.75%,  total acc: 52.18%   [EVAL] batch:  106 | acc: 81.25%,  total acc: 52.45%   [EVAL] batch:  107 | acc: 93.75%,  total acc: 52.84%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 53.04%   [EVAL] batch:  109 | acc: 81.25%,  total acc: 53.30%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 53.32%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 53.24%   [EVAL] batch:  112 | acc: 37.50%,  total acc: 53.10%   [EVAL] batch:  113 | acc: 75.00%,  total acc: 53.29%   [EVAL] batch:  114 | acc: 62.50%,  total acc: 53.37%   [EVAL] batch:  115 | acc: 18.75%,  total acc: 53.07%   
cur_acc:  ['0.8750', '0.7455', '0.6797', '0.7404', '0.4119', '0.7277', '0.4875']
his_acc:  ['0.8750', '0.6556', '0.5752', '0.5407', '0.5696', '0.5718', '0.5307']
Clustering into  18  clusters
Clusters:  [ 2  4  5  2  0 11 13  2  1  4  7  2 14  5  6 16  3 10 11  1  7  5  5 17
 11  1  6  8  2  5 12  2  3 15  0  1  2  9  7  1  0]
Losses:  7.825291633605957 6.801121711730957 0.5120850205421448
CurrentTrain: epoch  0, batch     0 | loss: 7.8252916Losses:  2.572681188583374 1.7579244375228882 0.4073784053325653
CurrentTrain: epoch  0, batch     1 | loss: 2.5726812Losses:  7.415721893310547 6.511223793029785 0.452249139547348
CurrentTrain: epoch  1, batch     0 | loss: 7.4157219Losses:  2.7163314819335938 1.903676986694336 0.4063272476196289
CurrentTrain: epoch  1, batch     1 | loss: 2.7163315Losses:  7.421755313873291 6.6533894538879395 0.3841828405857086
CurrentTrain: epoch  2, batch     0 | loss: 7.4217553Losses:  4.077481746673584 3.2505040168762207 0.41348886489868164
CurrentTrain: epoch  2, batch     1 | loss: 4.0774817Losses:  6.252190589904785 5.528321266174316 0.36193475127220154
CurrentTrain: epoch  3, batch     0 | loss: 6.2521906Losses:  1.7978172302246094 1.1046466827392578 0.3465852737426758
CurrentTrain: epoch  3, batch     1 | loss: 1.7978172Losses:  6.743523120880127 6.036933422088623 0.3532947897911072
CurrentTrain: epoch  4, batch     0 | loss: 6.7435231Losses:  2.9992499351501465 2.295531988143921 0.35185903310775757
CurrentTrain: epoch  4, batch     1 | loss: 2.9992499Losses:  6.062490463256836 5.385217666625977 0.3386363983154297
CurrentTrain: epoch  5, batch     0 | loss: 6.0624905Losses:  2.31276273727417 1.6412386894226074 0.33576202392578125
CurrentTrain: epoch  5, batch     1 | loss: 2.3127627Losses:  6.725126266479492 6.071779727935791 0.3266732692718506
CurrentTrain: epoch  6, batch     0 | loss: 6.7251263Losses:  3.087522506713867 2.5639047622680664 0.2618088126182556
CurrentTrain: epoch  6, batch     1 | loss: 3.0875225Losses:  5.88861083984375 5.233113765716553 0.32774853706359863
CurrentTrain: epoch  7, batch     0 | loss: 5.8886108Losses:  2.2885239124298096 1.626691460609436 0.33091622591018677
CurrentTrain: epoch  7, batch     1 | loss: 2.2885239Losses:  6.30772066116333 5.659914970397949 0.3239029049873352
CurrentTrain: epoch  8, batch     0 | loss: 6.3077207Losses:  2.40590763092041 1.8916566371917725 0.25712549686431885
CurrentTrain: epoch  8, batch     1 | loss: 2.4059076Losses:  5.800577163696289 5.1669158935546875 0.3168306052684784
CurrentTrain: epoch  9, batch     0 | loss: 5.8005772Losses:  2.206132650375366 1.545345425605774 0.33039364218711853
CurrentTrain: epoch  9, batch     1 | loss: 2.2061327
Losses:  1.6204582452774048 -0.0 0.8102291226387024
MemoryTrain:  epoch  0, batch     0 | loss: 1.6204582Losses:  1.5743876695632935 -0.0 0.7871938347816467
MemoryTrain:  epoch  0, batch     1 | loss: 1.5743877Losses:  1.372454285621643 -0.0 0.6862271428108215
MemoryTrain:  epoch  0, batch     2 | loss: 1.3724543Losses:  1.6069210767745972 -0.0 0.8034605383872986
MemoryTrain:  epoch  1, batch     0 | loss: 1.6069211Losses:  1.7193660736083984 -0.0 0.8596830368041992
MemoryTrain:  epoch  1, batch     1 | loss: 1.7193661Losses:  1.2279342412948608 -0.0 0.6139671206474304
MemoryTrain:  epoch  1, batch     2 | loss: 1.2279342Losses:  1.6927175521850586 -0.0 0.8463587760925293
MemoryTrain:  epoch  2, batch     0 | loss: 1.6927176Losses:  1.9246962070465088 -0.0 0.9623481035232544
MemoryTrain:  epoch  2, batch     1 | loss: 1.9246962Losses:  1.124588131904602 -0.0 0.562294065952301
MemoryTrain:  epoch  2, batch     2 | loss: 1.1245881Losses:  1.5848788022994995 -0.0 0.7924394011497498
MemoryTrain:  epoch  3, batch     0 | loss: 1.5848788Losses:  1.9023993015289307 -0.0 0.9511996507644653
MemoryTrain:  epoch  3, batch     1 | loss: 1.9023993Losses:  1.363534927368164 -0.0 0.681767463684082
MemoryTrain:  epoch  3, batch     2 | loss: 1.3635349Losses:  1.5491923093795776 -0.0 0.7745961546897888
MemoryTrain:  epoch  4, batch     0 | loss: 1.5491923Losses:  1.7636194229125977 -0.0 0.8818097114562988
MemoryTrain:  epoch  4, batch     1 | loss: 1.7636194Losses:  1.3482043743133545 -0.0 0.6741021871566772
MemoryTrain:  epoch  4, batch     2 | loss: 1.3482044Losses:  1.482590675354004 -0.0 0.741295337677002
MemoryTrain:  epoch  5, batch     0 | loss: 1.4825907Losses:  1.7646464109420776 -0.0 0.8823232054710388
MemoryTrain:  epoch  5, batch     1 | loss: 1.7646464Losses:  1.1441640853881836 -0.0 0.5720820426940918
MemoryTrain:  epoch  5, batch     2 | loss: 1.1441641Losses:  1.3690872192382812 -0.0 0.6845436096191406
MemoryTrain:  epoch  6, batch     0 | loss: 1.3690872Losses:  1.9420533180236816 -0.0 0.9710266590118408
MemoryTrain:  epoch  6, batch     1 | loss: 1.9420533Losses:  1.1380085945129395 -0.0 0.5690042972564697
MemoryTrain:  epoch  6, batch     2 | loss: 1.1380086Losses:  1.6316667795181274 -0.0 0.8158333897590637
MemoryTrain:  epoch  7, batch     0 | loss: 1.6316668Losses:  1.338930606842041 -0.0 0.6694653034210205
MemoryTrain:  epoch  7, batch     1 | loss: 1.3389306Losses:  1.3082082271575928 -0.0 0.6541041135787964
MemoryTrain:  epoch  7, batch     2 | loss: 1.3082082Losses:  1.5053647756576538 -0.0 0.7526823878288269
MemoryTrain:  epoch  8, batch     0 | loss: 1.5053648Losses:  1.7985289096832275 -0.0 0.8992644548416138
MemoryTrain:  epoch  8, batch     1 | loss: 1.7985289Losses:  1.1660244464874268 -0.0 0.5830122232437134
MemoryTrain:  epoch  8, batch     2 | loss: 1.1660244Losses:  1.5077240467071533 -0.0 0.7538620233535767
MemoryTrain:  epoch  9, batch     0 | loss: 1.5077240Losses:  1.45528244972229 -0.0 0.727641224861145
MemoryTrain:  epoch  9, batch     1 | loss: 1.4552824Losses:  1.399752140045166 -0.0 0.699876070022583
MemoryTrain:  epoch  9, batch     2 | loss: 1.3997521Losses:  1.5273933410644531 -0.0 0.7636966705322266
MemoryTrain:  epoch 10, batch     0 | loss: 1.5273933Losses:  1.6358146667480469 -0.0 0.8179073333740234
MemoryTrain:  epoch 10, batch     1 | loss: 1.6358147Losses:  1.336970567703247 -0.0 0.6684852838516235
MemoryTrain:  epoch 10, batch     2 | loss: 1.3369706Losses:  1.6841338872909546 -0.0 0.8420669436454773
MemoryTrain:  epoch 11, batch     0 | loss: 1.6841339Losses:  1.3550115823745728 -0.0 0.6775057911872864
MemoryTrain:  epoch 11, batch     1 | loss: 1.3550116Losses:  1.1398375034332275 -0.0 0.5699187517166138
MemoryTrain:  epoch 11, batch     2 | loss: 1.1398375Losses:  1.5204306840896606 -0.0 0.7602153420448303
MemoryTrain:  epoch 12, batch     0 | loss: 1.5204307Losses:  1.7301009893417358 -0.0 0.8650504946708679
MemoryTrain:  epoch 12, batch     1 | loss: 1.7301010Losses:  1.258944034576416 -0.0 0.629472017288208
MemoryTrain:  epoch 12, batch     2 | loss: 1.2589440Losses:  1.505456805229187 -0.0 0.7527284026145935
MemoryTrain:  epoch 13, batch     0 | loss: 1.5054568Losses:  1.5367157459259033 -0.0 0.7683578729629517
MemoryTrain:  epoch 13, batch     1 | loss: 1.5367157Losses:  1.4553343057632446 -0.0 0.7276671528816223
MemoryTrain:  epoch 13, batch     2 | loss: 1.4553343Losses:  1.497783899307251 -0.0 0.7488919496536255
MemoryTrain:  epoch 14, batch     0 | loss: 1.4977839Losses:  1.7308439016342163 -0.0 0.8654219508171082
MemoryTrain:  epoch 14, batch     1 | loss: 1.7308439Losses:  0.9537029266357422 -0.0 0.4768514633178711
MemoryTrain:  epoch 14, batch     2 | loss: 0.9537029Losses:  1.6152377128601074 -0.0 0.8076188564300537
MemoryTrain:  epoch 15, batch     0 | loss: 1.6152377Losses:  1.7351945638656616 -0.0 0.8675972819328308
MemoryTrain:  epoch 15, batch     1 | loss: 1.7351946Losses:  1.1712872982025146 -0.0 0.5856436491012573
MemoryTrain:  epoch 15, batch     2 | loss: 1.1712873Losses:  1.6277246475219727 -0.0 0.8138623237609863
MemoryTrain:  epoch 16, batch     0 | loss: 1.6277246Losses:  1.48524010181427 -0.0 0.742620050907135
MemoryTrain:  epoch 16, batch     1 | loss: 1.4852401Losses:  1.1208668947219849 -0.0 0.5604334473609924
MemoryTrain:  epoch 16, batch     2 | loss: 1.1208669Losses:  1.7676198482513428 -0.0 0.8838099241256714
MemoryTrain:  epoch 17, batch     0 | loss: 1.7676198Losses:  1.4718023538589478 -0.0 0.7359011769294739
MemoryTrain:  epoch 17, batch     1 | loss: 1.4718024Losses:  0.7917880415916443 -0.0 0.39589402079582214
MemoryTrain:  epoch 17, batch     2 | loss: 0.7917880Losses:  1.4931083917617798 -0.0 0.7465541958808899
MemoryTrain:  epoch 18, batch     0 | loss: 1.4931084Losses:  1.8620566129684448 -0.0 0.9310283064842224
MemoryTrain:  epoch 18, batch     1 | loss: 1.8620566Losses:  1.0504848957061768 -0.0 0.5252424478530884
MemoryTrain:  epoch 18, batch     2 | loss: 1.0504849Losses:  1.5706707239151 -0.0 0.78533536195755
MemoryTrain:  epoch 19, batch     0 | loss: 1.5706707Losses:  1.607619047164917 -0.0 0.8038095235824585
MemoryTrain:  epoch 19, batch     1 | loss: 1.6076190Losses:  1.239696741104126 -0.0 0.619848370552063
MemoryTrain:  epoch 19, batch     2 | loss: 1.2396967
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 8.33%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 7.81%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 6.25%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 14.29%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 20.31%   [EVAL] batch:    8 | acc: 12.50%,  total acc: 19.44%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 22.50%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 25.57%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 30.73%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 36.06%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 40.62%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 44.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 48.05%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 51.10%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 49.65%   
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 29.17%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 28.12%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 25.00%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 22.92%   [EVAL] batch:    6 | acc: 12.50%,  total acc: 21.43%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 19.53%   [EVAL] batch:    8 | acc: 18.75%,  total acc: 19.44%   [EVAL] batch:    9 | acc: 6.25%,  total acc: 18.12%   [EVAL] batch:   10 | acc: 25.00%,  total acc: 18.75%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 20.83%   [EVAL] batch:   12 | acc: 6.25%,  total acc: 19.71%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 20.09%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 23.75%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 26.17%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 29.04%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 30.90%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 33.22%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 36.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 39.29%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 42.05%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 44.29%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 46.35%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 48.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 50.48%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 51.85%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 53.12%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 54.74%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 55.62%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 56.85%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 58.01%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 58.52%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 57.35%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 55.71%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 54.17%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 52.70%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 51.48%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 50.32%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 50.47%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 51.37%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 52.08%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 52.62%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 52.84%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 52.92%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 52.17%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 52.39%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 53.12%   [EVAL] batch:   48 | acc: 0.00%,  total acc: 52.04%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 51.62%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 51.84%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 52.28%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 52.48%   [EVAL] batch:   53 | acc: 31.25%,  total acc: 52.08%   [EVAL] batch:   54 | acc: 25.00%,  total acc: 51.59%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 51.56%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 51.32%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 51.83%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 52.65%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 53.12%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 52.97%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 52.52%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 52.28%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 52.54%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 51.73%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 51.04%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 50.37%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 49.82%   [EVAL] batch:   68 | acc: 12.50%,  total acc: 49.28%   [EVAL] batch:   69 | acc: 25.00%,  total acc: 48.93%   [EVAL] batch:   70 | acc: 31.25%,  total acc: 48.68%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 48.70%   [EVAL] batch:   72 | acc: 18.75%,  total acc: 48.29%   [EVAL] batch:   73 | acc: 0.00%,  total acc: 47.64%   [EVAL] batch:   74 | acc: 0.00%,  total acc: 47.00%   [EVAL] batch:   75 | acc: 0.00%,  total acc: 46.38%   [EVAL] batch:   76 | acc: 0.00%,  total acc: 45.78%   [EVAL] batch:   77 | acc: 6.25%,  total acc: 45.27%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 45.09%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 45.39%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 45.91%   [EVAL] batch:   81 | acc: 93.75%,  total acc: 46.49%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 46.91%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 47.17%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 46.84%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 46.51%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 46.34%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 46.80%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 47.33%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 47.85%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 48.42%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 48.91%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 49.46%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 50.00%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 50.53%   [EVAL] batch:   95 | acc: 75.00%,  total acc: 50.78%   [EVAL] batch:   96 | acc: 37.50%,  total acc: 50.64%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 50.64%   [EVAL] batch:   98 | acc: 25.00%,  total acc: 50.38%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 50.25%   [EVAL] batch:  100 | acc: 37.50%,  total acc: 50.12%   [EVAL] batch:  101 | acc: 50.00%,  total acc: 50.12%   [EVAL] batch:  102 | acc: 31.25%,  total acc: 49.94%   [EVAL] batch:  103 | acc: 31.25%,  total acc: 49.76%   [EVAL] batch:  104 | acc: 43.75%,  total acc: 49.70%   [EVAL] batch:  105 | acc: 31.25%,  total acc: 49.53%   [EVAL] batch:  106 | acc: 81.25%,  total acc: 49.82%   [EVAL] batch:  107 | acc: 93.75%,  total acc: 50.23%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 50.46%   [EVAL] batch:  109 | acc: 81.25%,  total acc: 50.74%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 50.79%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 50.50%   [EVAL] batch:  112 | acc: 25.00%,  total acc: 50.28%   [EVAL] batch:  113 | acc: 31.25%,  total acc: 50.11%   [EVAL] batch:  114 | acc: 12.50%,  total acc: 49.78%   [EVAL] batch:  115 | acc: 18.75%,  total acc: 49.52%   [EVAL] batch:  116 | acc: 6.25%,  total acc: 49.15%   [EVAL] batch:  117 | acc: 6.25%,  total acc: 48.78%   [EVAL] batch:  118 | acc: 6.25%,  total acc: 48.42%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 48.02%   [EVAL] batch:  120 | acc: 6.25%,  total acc: 47.68%   [EVAL] batch:  121 | acc: 31.25%,  total acc: 47.54%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 47.82%   [EVAL] batch:  123 | acc: 18.75%,  total acc: 47.58%   [EVAL] batch:  124 | acc: 37.50%,  total acc: 47.50%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 47.62%   [EVAL] batch:  126 | acc: 75.00%,  total acc: 47.83%   [EVAL] batch:  127 | acc: 87.50%,  total acc: 48.14%   [EVAL] batch:  128 | acc: 100.00%,  total acc: 48.55%   [EVAL] batch:  129 | acc: 100.00%,  total acc: 48.94%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 49.33%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 49.72%   [EVAL] batch:  132 | acc: 62.50%,  total acc: 49.81%   
cur_acc:  ['0.8750', '0.7455', '0.6797', '0.7404', '0.4119', '0.7277', '0.4875', '0.4965']
his_acc:  ['0.8750', '0.6556', '0.5752', '0.5407', '0.5696', '0.5718', '0.5307', '0.4981']
--------Round  1
seed:  200
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 1 1 0]
Losses:  11.882762908935547 10.771873474121094 0.5554449558258057
CurrentTrain: epoch  0, batch     0 | loss: 11.8827629Losses:  9.37524700164795 8.237760543823242 0.5687431693077087
CurrentTrain: epoch  0, batch     1 | loss: 9.3752470Losses:  10.22464656829834 9.13183879852295 0.5464038848876953
CurrentTrain: epoch  0, batch     2 | loss: 10.2246466Losses:  8.134100914001465 7.091741561889648 0.5211797952651978
CurrentTrain: epoch  0, batch     3 | loss: 8.1341009Losses:  12.355502128601074 11.34255599975586 0.5064729452133179
CurrentTrain: epoch  0, batch     4 | loss: 12.3555021Losses:  8.780466079711914 7.753880500793457 0.5132930278778076
CurrentTrain: epoch  0, batch     5 | loss: 8.7804661Losses:  10.47217845916748 9.49030590057373 0.49093613028526306
CurrentTrain: epoch  0, batch     6 | loss: 10.4721785Losses:  9.005977630615234 8.034503936767578 0.4857369065284729
CurrentTrain: epoch  0, batch     7 | loss: 9.0059776Losses:  10.14432430267334 9.249817848205566 0.4472532868385315
CurrentTrain: epoch  0, batch     8 | loss: 10.1443243Losses:  7.528791427612305 6.654244899749756 0.43727314472198486
CurrentTrain: epoch  0, batch     9 | loss: 7.5287914Losses:  7.823267459869385 6.904319763183594 0.4594738781452179
CurrentTrain: epoch  0, batch    10 | loss: 7.8232675Losses:  9.59472942352295 8.672073364257812 0.4613281488418579
CurrentTrain: epoch  0, batch    11 | loss: 9.5947294Losses:  10.59121322631836 9.883814811706543 0.3536991775035858
CurrentTrain: epoch  0, batch    12 | loss: 10.5912132Losses:  9.201482772827148 8.49766731262207 0.3519076406955719
CurrentTrain: epoch  0, batch    13 | loss: 9.2014828Losses:  10.672791481018066 9.951602935791016 0.36059439182281494
CurrentTrain: epoch  0, batch    14 | loss: 10.6727915Losses:  9.683029174804688 8.998652458190918 0.34218835830688477
CurrentTrain: epoch  0, batch    15 | loss: 9.6830292Losses:  19.497827529907227 18.845809936523438 0.32600921392440796
CurrentTrain: epoch  0, batch    16 | loss: 19.4978275Losses:  8.233217239379883 7.674530982971191 0.27934321761131287
CurrentTrain: epoch  0, batch    17 | loss: 8.2332172Losses:  12.577476501464844 11.842323303222656 0.36757636070251465
CurrentTrain: epoch  0, batch    18 | loss: 12.5774765Losses:  8.878488540649414 8.268350601196289 0.3050687313079834
CurrentTrain: epoch  0, batch    19 | loss: 8.8784885Losses:  9.807698249816895 9.264780044555664 0.27145910263061523
CurrentTrain: epoch  0, batch    20 | loss: 9.8076982Losses:  8.965143203735352 8.443475723266602 0.2608339786529541
CurrentTrain: epoch  0, batch    21 | loss: 8.9651432Losses:  6.896011829376221 6.427920818328857 0.23404553532600403
CurrentTrain: epoch  0, batch    22 | loss: 6.8960118Losses:  10.929299354553223 10.441075325012207 0.2441120445728302
CurrentTrain: epoch  0, batch    23 | loss: 10.9292994Losses:  8.2711181640625 7.819721221923828 0.2256985604763031
CurrentTrain: epoch  0, batch    24 | loss: 8.2711182Losses:  10.016687393188477 9.67017936706543 0.17325398325920105
CurrentTrain: epoch  0, batch    25 | loss: 10.0166874Losses:  8.237213134765625 7.888092517852783 0.174560546875
CurrentTrain: epoch  0, batch    26 | loss: 8.2372131Losses:  8.394319534301758 8.083154678344727 0.15558242797851562
CurrentTrain: epoch  0, batch    27 | loss: 8.3943195Losses:  8.448479652404785 8.14598274230957 0.15124836564064026
CurrentTrain: epoch  0, batch    28 | loss: 8.4484797Losses:  6.815513610839844 6.4701433181762695 0.17268511652946472
CurrentTrain: epoch  0, batch    29 | loss: 6.8155136Losses:  9.052257537841797 8.706562995910645 0.1728471964597702
CurrentTrain: epoch  0, batch    30 | loss: 9.0522575Losses:  6.609361171722412 6.328587532043457 0.14038684964179993
CurrentTrain: epoch  0, batch    31 | loss: 6.6093612Losses:  10.090510368347168 9.74478816986084 0.1728609949350357
CurrentTrain: epoch  0, batch    32 | loss: 10.0905104Losses:  8.786240577697754 8.568283081054688 0.10897863656282425
CurrentTrain: epoch  0, batch    33 | loss: 8.7862406Losses:  10.62243938446045 10.366430282592773 0.12800440192222595
CurrentTrain: epoch  0, batch    34 | loss: 10.6224394Losses:  8.009114265441895 7.716292381286621 0.14641103148460388
CurrentTrain: epoch  0, batch    35 | loss: 8.0091143Losses:  6.774240493774414 6.477414608001709 0.148412823677063
CurrentTrain: epoch  0, batch    36 | loss: 6.7742405Losses:  2.5124008655548096 2.2711219787597656 0.12063949555158615
CurrentTrain: epoch  0, batch    37 | loss: 2.5124009Losses:  8.408491134643555 8.209022521972656 0.09973450005054474
CurrentTrain: epoch  1, batch     0 | loss: 8.4084911Losses:  13.134297370910645 12.968328475952148 0.08298465609550476
CurrentTrain: epoch  1, batch     1 | loss: 13.1342974Losses:  7.911747455596924 7.7416582107543945 0.08504470437765121
CurrentTrain: epoch  1, batch     2 | loss: 7.9117475Losses:  9.039414405822754 8.886955261230469 0.07622939348220825
CurrentTrain: epoch  1, batch     3 | loss: 9.0394144Losses:  8.53930950164795 8.356704711914062 0.09130221605300903
CurrentTrain: epoch  1, batch     4 | loss: 8.5393095Losses:  10.012832641601562 9.80039119720459 0.10622091591358185
CurrentTrain: epoch  1, batch     5 | loss: 10.0128326Losses:  6.413958549499512 6.256299018859863 0.0788298174738884
CurrentTrain: epoch  1, batch     6 | loss: 6.4139585Losses:  8.747503280639648 8.57745361328125 0.08502483367919922
CurrentTrain: epoch  1, batch     7 | loss: 8.7475033Losses:  5.698566913604736 5.553399085998535 0.07258381694555283
CurrentTrain: epoch  1, batch     8 | loss: 5.6985669Losses:  9.004646301269531 8.85623836517334 0.07420393824577332
CurrentTrain: epoch  1, batch     9 | loss: 9.0046463Losses:  7.416596412658691 7.287539005279541 0.06452862918376923
CurrentTrain: epoch  1, batch    10 | loss: 7.4165964Losses:  6.913549900054932 6.772606372833252 0.0704718604683876
CurrentTrain: epoch  1, batch    11 | loss: 6.9135499Losses:  5.930354118347168 5.795414924621582 0.0674695074558258
CurrentTrain: epoch  1, batch    12 | loss: 5.9303541Losses:  7.9268622398376465 7.781515598297119 0.07267323136329651
CurrentTrain: epoch  1, batch    13 | loss: 7.9268622Losses:  8.950530052185059 8.823378562927246 0.06357575207948685
CurrentTrain: epoch  1, batch    14 | loss: 8.9505301Losses:  5.034440994262695 4.904308795928955 0.06506604701280594
CurrentTrain: epoch  1, batch    15 | loss: 5.0344410Losses:  8.481851577758789 8.353852272033691 0.0639994740486145
CurrentTrain: epoch  1, batch    16 | loss: 8.4818516Losses:  5.241414546966553 5.13038444519043 0.05551515519618988
CurrentTrain: epoch  1, batch    17 | loss: 5.2414145Losses:  6.279734134674072 6.160755157470703 0.05948949605226517
CurrentTrain: epoch  1, batch    18 | loss: 6.2797341Losses:  7.055273056030273 6.937016487121582 0.05912819877266884
CurrentTrain: epoch  1, batch    19 | loss: 7.0552731Losses:  8.28784465789795 8.181211471557617 0.05331668257713318
CurrentTrain: epoch  1, batch    20 | loss: 8.2878447Losses:  5.816786766052246 5.711759090423584 0.05251374840736389
CurrentTrain: epoch  1, batch    21 | loss: 5.8167868Losses:  5.803476333618164 5.693165302276611 0.05515551567077637
CurrentTrain: epoch  1, batch    22 | loss: 5.8034763Losses:  9.095885276794434 8.973601341247559 0.06114216521382332
CurrentTrain: epoch  1, batch    23 | loss: 9.0958853Losses:  6.260763168334961 6.149806976318359 0.0554780513048172
CurrentTrain: epoch  1, batch    24 | loss: 6.2607632Losses:  9.631131172180176 9.49197006225586 0.06958040595054626
CurrentTrain: epoch  1, batch    25 | loss: 9.6311312Losses:  5.378451824188232 5.288714408874512 0.04486874118447304
CurrentTrain: epoch  1, batch    26 | loss: 5.3784518Losses:  7.197004318237305 7.094148635864258 0.05142785981297493
CurrentTrain: epoch  1, batch    27 | loss: 7.1970043Losses:  7.069924354553223 6.975046157836914 0.04743919149041176
CurrentTrain: epoch  1, batch    28 | loss: 7.0699244Losses:  6.322875499725342 6.207273483276367 0.05780090391635895
CurrentTrain: epoch  1, batch    29 | loss: 6.3228755Losses:  7.373071193695068 7.254022121429443 0.059524599462747574
CurrentTrain: epoch  1, batch    30 | loss: 7.3730712Losses:  7.1794891357421875 7.051383018493652 0.06405308842658997
CurrentTrain: epoch  1, batch    31 | loss: 7.1794891Losses:  5.560554504394531 5.456807613372803 0.051873449236154556
CurrentTrain: epoch  1, batch    32 | loss: 5.5605545Losses:  7.426717281341553 7.338113307952881 0.04430187866091728
CurrentTrain: epoch  1, batch    33 | loss: 7.4267173Losses:  6.393520832061768 6.214864730834961 0.08932797610759735
CurrentTrain: epoch  1, batch    34 | loss: 6.3935208Losses:  5.21881628036499 5.130005359649658 0.044405482709407806
CurrentTrain: epoch  1, batch    35 | loss: 5.2188163Losses:  4.419025421142578 4.328894138336182 0.04506567865610123
CurrentTrain: epoch  1, batch    36 | loss: 4.4190254Losses:  1.5405161380767822 1.4577155113220215 0.04140034317970276
CurrentTrain: epoch  1, batch    37 | loss: 1.5405161Losses:  6.470110893249512 6.373692512512207 0.048209115862846375
CurrentTrain: epoch  2, batch     0 | loss: 6.4701109Losses:  8.920187950134277 8.815010070800781 0.05258879065513611
CurrentTrain: epoch  2, batch     1 | loss: 8.9201880Losses:  9.593442916870117 9.514067649841309 0.039687663316726685
CurrentTrain: epoch  2, batch     2 | loss: 9.5934429Losses:  5.300633430480957 5.206265449523926 0.04718393087387085
CurrentTrain: epoch  2, batch     3 | loss: 5.3006334Losses:  6.808282375335693 6.709672451019287 0.049304913729429245
CurrentTrain: epoch  2, batch     4 | loss: 6.8082824Losses:  9.668539047241211 9.567333221435547 0.05060306563973427
CurrentTrain: epoch  2, batch     5 | loss: 9.6685390Losses:  6.244955539703369 6.157062530517578 0.04394645616412163
CurrentTrain: epoch  2, batch     6 | loss: 6.2449555Losses:  6.1136250495910645 6.031806945800781 0.04090915620326996
CurrentTrain: epoch  2, batch     7 | loss: 6.1136250Losses:  6.2829365730285645 6.202008247375488 0.040464263409376144
CurrentTrain: epoch  2, batch     8 | loss: 6.2829366Losses:  5.075891494750977 4.991787910461426 0.04205184057354927
CurrentTrain: epoch  2, batch     9 | loss: 5.0758915Losses:  6.108824729919434 6.030101776123047 0.03936142474412918
CurrentTrain: epoch  2, batch    10 | loss: 6.1088247Losses:  6.268799781799316 6.189400672912598 0.03969946131110191
CurrentTrain: epoch  2, batch    11 | loss: 6.2687998Losses:  4.881914138793945 4.810055255889893 0.035929445177316666
CurrentTrain: epoch  2, batch    12 | loss: 4.8819141Losses:  4.668033599853516 4.6017937660217285 0.03312002494931221
CurrentTrain: epoch  2, batch    13 | loss: 4.6680336Losses:  9.650300979614258 9.560480117797852 0.04491028934717178
CurrentTrain: epoch  2, batch    14 | loss: 9.6503010Losses:  7.497003078460693 7.414276599884033 0.04136322811245918
CurrentTrain: epoch  2, batch    15 | loss: 7.4970031Losses:  7.5236358642578125 7.448661804199219 0.0374869704246521
CurrentTrain: epoch  2, batch    16 | loss: 7.5236359Losses:  7.130260944366455 7.050289154052734 0.039985958486795425
CurrentTrain: epoch  2, batch    17 | loss: 7.1302609Losses:  5.159833908081055 5.088734149932861 0.035549867898225784
CurrentTrain: epoch  2, batch    18 | loss: 5.1598339Losses:  20.41103172302246 20.32362174987793 0.0437045693397522
CurrentTrain: epoch  2, batch    19 | loss: 20.4110317Losses:  12.035001754760742 11.946008682250977 0.044496532529592514
CurrentTrain: epoch  2, batch    20 | loss: 12.0350018Losses:  5.342089653015137 5.278032302856445 0.032028678804636
CurrentTrain: epoch  2, batch    21 | loss: 5.3420897Losses:  6.478728771209717 6.39877462387085 0.03997708112001419
CurrentTrain: epoch  2, batch    22 | loss: 6.4787288Losses:  9.873895645141602 9.74969482421875 0.0621006041765213
CurrentTrain: epoch  2, batch    23 | loss: 9.8738956Losses:  8.163315773010254 8.08683967590332 0.038237862288951874
CurrentTrain: epoch  2, batch    24 | loss: 8.1633158Losses:  6.83499002456665 6.751843452453613 0.041573263704776764
CurrentTrain: epoch  2, batch    25 | loss: 6.8349900Losses:  6.993854522705078 6.911025047302246 0.041414812207221985
CurrentTrain: epoch  2, batch    26 | loss: 6.9938545Losses:  5.326838970184326 5.255969047546387 0.03543492779135704
CurrentTrain: epoch  2, batch    27 | loss: 5.3268390Losses:  4.909551620483398 4.842342376708984 0.033604562282562256
CurrentTrain: epoch  2, batch    28 | loss: 4.9095516Losses:  4.622734546661377 4.55849552154541 0.03211953490972519
CurrentTrain: epoch  2, batch    29 | loss: 4.6227345Losses:  5.306475639343262 5.241814613342285 0.03233042359352112
CurrentTrain: epoch  2, batch    30 | loss: 5.3064756Losses:  8.358241081237793 8.277091979980469 0.040574561804533005
CurrentTrain: epoch  2, batch    31 | loss: 8.3582411Losses:  5.371618270874023 5.310944080352783 0.030337151139974594
CurrentTrain: epoch  2, batch    32 | loss: 5.3716183Losses:  5.328176021575928 5.264737606048584 0.03171910345554352
CurrentTrain: epoch  2, batch    33 | loss: 5.3281760Losses:  4.911313056945801 4.850593566894531 0.030359644442796707
CurrentTrain: epoch  2, batch    34 | loss: 4.9113131Losses:  6.40460205078125 6.332589149475098 0.0360063798725605
CurrentTrain: epoch  2, batch    35 | loss: 6.4046021Losses:  10.228858947753906 10.161535263061523 0.033661969006061554
CurrentTrain: epoch  2, batch    36 | loss: 10.2288589Losses:  1.527636170387268 1.4375275373458862 0.04505433887243271
CurrentTrain: epoch  2, batch    37 | loss: 1.5276362Losses:  7.077438831329346 7.014538288116455 0.03145021200180054
CurrentTrain: epoch  3, batch     0 | loss: 7.0774388Losses:  6.982109546661377 6.92450475692749 0.02880249358713627
CurrentTrain: epoch  3, batch     1 | loss: 6.9821095Losses:  5.441570281982422 5.383968830108643 0.028800755739212036
CurrentTrain: epoch  3, batch     2 | loss: 5.4415703Losses:  9.703865051269531 9.629154205322266 0.037355490028858185
CurrentTrain: epoch  3, batch     3 | loss: 9.7038651Losses:  9.071548461914062 9.005043029785156 0.03325275704264641
CurrentTrain: epoch  3, batch     4 | loss: 9.0715485Losses:  5.1885247230529785 5.119616508483887 0.03445414453744888
CurrentTrain: epoch  3, batch     5 | loss: 5.1885247Losses:  6.446224689483643 6.388082027435303 0.029071401804685593
CurrentTrain: epoch  3, batch     6 | loss: 6.4462247Losses:  5.574441432952881 5.509261608123779 0.03258996084332466
CurrentTrain: epoch  3, batch     7 | loss: 5.5744414Losses:  5.96932315826416 5.907814979553223 0.030754029750823975
CurrentTrain: epoch  3, batch     8 | loss: 5.9693232Losses:  5.885721206665039 5.814659118652344 0.035530999302864075
CurrentTrain: epoch  3, batch     9 | loss: 5.8857212Losses:  6.1407389640808105 6.075199604034424 0.03276960179209709
CurrentTrain: epoch  3, batch    10 | loss: 6.1407390Losses:  8.449681282043457 8.357723236083984 0.04597901925444603
CurrentTrain: epoch  3, batch    11 | loss: 8.4496813Losses:  7.063675403594971 7.00222110748291 0.03072715364396572
CurrentTrain: epoch  3, batch    12 | loss: 7.0636754Losses:  5.6346964836120605 5.572525978088379 0.031085340306162834
CurrentTrain: epoch  3, batch    13 | loss: 5.6346965Losses:  7.654068946838379 7.579576015472412 0.03724638745188713
CurrentTrain: epoch  3, batch    14 | loss: 7.6540689Losses:  6.027725696563721 5.9668989181518555 0.030413474887609482
CurrentTrain: epoch  3, batch    15 | loss: 6.0277257Losses:  8.156475067138672 8.054107666015625 0.051183756440877914
CurrentTrain: epoch  3, batch    16 | loss: 8.1564751Losses:  6.720004558563232 6.652221202850342 0.0338917151093483
CurrentTrain: epoch  3, batch    17 | loss: 6.7200046Losses:  6.032164096832275 5.967752456665039 0.03220576420426369
CurrentTrain: epoch  3, batch    18 | loss: 6.0321641Losses:  11.919366836547852 11.817364692687988 0.05100110545754433
CurrentTrain: epoch  3, batch    19 | loss: 11.9193668Losses:  6.545842170715332 6.475683689117432 0.03507933393120766
CurrentTrain: epoch  3, batch    20 | loss: 6.5458422Losses:  8.018084526062012 7.955108642578125 0.03148781508207321
CurrentTrain: epoch  3, batch    21 | loss: 8.0180845Losses:  7.9640350341796875 7.902687072753906 0.030673975124955177
CurrentTrain: epoch  3, batch    22 | loss: 7.9640350Losses:  4.003509998321533 3.947291612625122 0.028109215199947357
CurrentTrain: epoch  3, batch    23 | loss: 4.0035100Losses:  5.986291885375977 5.921523094177246 0.03238438069820404
CurrentTrain: epoch  3, batch    24 | loss: 5.9862919Losses:  5.4167399406433105 5.352170944213867 0.03228460252285004
CurrentTrain: epoch  3, batch    25 | loss: 5.4167399Losses:  7.961589336395264 7.884879112243652 0.03835500404238701
CurrentTrain: epoch  3, batch    26 | loss: 7.9615893Losses:  6.065909385681152 6.005965232849121 0.02997216023504734
CurrentTrain: epoch  3, batch    27 | loss: 6.0659094Losses:  9.24544620513916 9.159061431884766 0.043192192912101746
CurrentTrain: epoch  3, batch    28 | loss: 9.2454462Losses:  9.39826488494873 9.319786071777344 0.039239589124917984
CurrentTrain: epoch  3, batch    29 | loss: 9.3982649Losses:  8.00190258026123 7.946200370788574 0.027850888669490814
CurrentTrain: epoch  3, batch    30 | loss: 8.0019026Losses:  4.978237628936768 4.9252238273620605 0.026506878435611725
CurrentTrain: epoch  3, batch    31 | loss: 4.9782376Losses:  6.575295925140381 6.510906219482422 0.032194800674915314
CurrentTrain: epoch  3, batch    32 | loss: 6.5752959Losses:  9.620317459106445 9.549173355102539 0.03557183966040611
CurrentTrain: epoch  3, batch    33 | loss: 9.6203175Losses:  7.1317219734191895 7.065621852874756 0.033050134778022766
CurrentTrain: epoch  3, batch    34 | loss: 7.1317220Losses:  4.97341775894165 4.9108476638793945 0.031285062432289124
CurrentTrain: epoch  3, batch    35 | loss: 4.9734178Losses:  4.791306972503662 4.73234224319458 0.029482385143637657
CurrentTrain: epoch  3, batch    36 | loss: 4.7913070Losses:  0.6033340692520142 0.5443804264068604 0.0294768288731575
CurrentTrain: epoch  3, batch    37 | loss: 0.6033341Losses:  5.748286724090576 5.68491268157959 0.031687114387750626
CurrentTrain: epoch  4, batch     0 | loss: 5.7482867Losses:  5.112284183502197 5.0520524978637695 0.030115777626633644
CurrentTrain: epoch  4, batch     1 | loss: 5.1122842Losses:  5.004464149475098 4.957159996032715 0.023652078583836555
CurrentTrain: epoch  4, batch     2 | loss: 5.0044641Losses:  7.716880798339844 7.654583930969238 0.031148433685302734
CurrentTrain: epoch  4, batch     3 | loss: 7.7168808Losses:  6.313420295715332 6.228376388549805 0.04252190515398979
CurrentTrain: epoch  4, batch     4 | loss: 6.3134203Losses:  6.252388000488281 6.188813209533691 0.031787410378456116
CurrentTrain: epoch  4, batch     5 | loss: 6.2523880Losses:  10.709851264953613 10.590993881225586 0.05942884087562561
CurrentTrain: epoch  4, batch     6 | loss: 10.7098513Losses:  5.1692304611206055 5.096797466278076 0.03621644899249077
CurrentTrain: epoch  4, batch     7 | loss: 5.1692305Losses:  7.538414478302002 7.479506969451904 0.02945367991924286
CurrentTrain: epoch  4, batch     8 | loss: 7.5384145Losses:  7.27817964553833 7.209504127502441 0.03433781489729881
CurrentTrain: epoch  4, batch     9 | loss: 7.2781796Losses:  6.765789985656738 6.703835487365723 0.03097722679376602
CurrentTrain: epoch  4, batch    10 | loss: 6.7657900Losses:  5.962738037109375 5.90936279296875 0.026687731966376305
CurrentTrain: epoch  4, batch    11 | loss: 5.9627380Losses:  5.697177410125732 5.638388633728027 0.029394356533885002
CurrentTrain: epoch  4, batch    12 | loss: 5.6971774Losses:  6.807491302490234 6.719970703125 0.04376031458377838
CurrentTrain: epoch  4, batch    13 | loss: 6.8074913Losses:  9.182244300842285 9.100584030151367 0.04083002358675003
CurrentTrain: epoch  4, batch    14 | loss: 9.1822443Losses:  7.556769371032715 7.487978935241699 0.03439517319202423
CurrentTrain: epoch  4, batch    15 | loss: 7.5567694Losses:  5.787431240081787 5.731801986694336 0.027814598754048347
CurrentTrain: epoch  4, batch    16 | loss: 5.7874312Losses:  4.697264194488525 4.647096633911133 0.025083694607019424
CurrentTrain: epoch  4, batch    17 | loss: 4.6972642Losses:  9.362870216369629 9.291805267333984 0.035532549023628235
CurrentTrain: epoch  4, batch    18 | loss: 9.3628702Losses:  10.168336868286133 10.09324836730957 0.03754403814673424
CurrentTrain: epoch  4, batch    19 | loss: 10.1683369Losses:  13.957954406738281 13.886655807495117 0.035649374127388
CurrentTrain: epoch  4, batch    20 | loss: 13.9579544Losses:  4.199402332305908 4.14963436126709 0.02488388679921627
CurrentTrain: epoch  4, batch    21 | loss: 4.1994023Losses:  9.178457260131836 9.104863166809082 0.03679710626602173
CurrentTrain: epoch  4, batch    22 | loss: 9.1784573Losses:  5.807992458343506 5.7593231201171875 0.02433466538786888
CurrentTrain: epoch  4, batch    23 | loss: 5.8079925Losses:  6.162402153015137 6.096457481384277 0.03297245502471924
CurrentTrain: epoch  4, batch    24 | loss: 6.1624022Losses:  6.0797858238220215 6.024259567260742 0.02776312083005905
CurrentTrain: epoch  4, batch    25 | loss: 6.0797858Losses:  5.300032615661621 5.247568130493164 0.026232169941067696
CurrentTrain: epoch  4, batch    26 | loss: 5.3000326Losses:  5.615373611450195 5.550697326660156 0.032338157296180725
CurrentTrain: epoch  4, batch    27 | loss: 5.6153736Losses:  9.241450309753418 9.18065357208252 0.030398383736610413
CurrentTrain: epoch  4, batch    28 | loss: 9.2414503Losses:  5.12736701965332 5.068209648132324 0.0295786801725626
CurrentTrain: epoch  4, batch    29 | loss: 5.1273670Losses:  6.154869079589844 6.087814807891846 0.03352704271674156
CurrentTrain: epoch  4, batch    30 | loss: 6.1548691Losses:  6.4872050285339355 6.437393665313721 0.024905573576688766
CurrentTrain: epoch  4, batch    31 | loss: 6.4872050Losses:  7.323128700256348 7.2559709548950195 0.033578913658857346
CurrentTrain: epoch  4, batch    32 | loss: 7.3231287Losses:  5.641906261444092 5.594403266906738 0.02375161461532116
CurrentTrain: epoch  4, batch    33 | loss: 5.6419063Losses:  9.1707124710083 9.091083526611328 0.039814483374357224
CurrentTrain: epoch  4, batch    34 | loss: 9.1707125Losses:  6.362918853759766 6.310427665710449 0.026245638728141785
CurrentTrain: epoch  4, batch    35 | loss: 6.3629189Losses:  4.87390661239624 4.81281852722168 0.030544022098183632
CurrentTrain: epoch  4, batch    36 | loss: 4.8739066Losses:  0.7592632174491882 0.6845723390579224 0.037345439195632935
CurrentTrain: epoch  4, batch    37 | loss: 0.7592632Losses:  6.252167224884033 6.19461727142334 0.028774984180927277
CurrentTrain: epoch  5, batch     0 | loss: 6.2521672Losses:  5.609647274017334 5.545262336730957 0.03219249099493027
CurrentTrain: epoch  5, batch     1 | loss: 5.6096473Losses:  6.592911720275879 6.527471542358398 0.03271999582648277
CurrentTrain: epoch  5, batch     2 | loss: 6.5929117Losses:  5.076574325561523 5.024007797241211 0.0262832622975111
CurrentTrain: epoch  5, batch     3 | loss: 5.0765743Losses:  4.965707302093506 4.919554233551025 0.023076551035046577
CurrentTrain: epoch  5, batch     4 | loss: 4.9657073Losses:  5.546533584594727 5.491145133972168 0.027694132179021835
CurrentTrain: epoch  5, batch     5 | loss: 5.5465336Losses:  6.643570423126221 6.576994895935059 0.033287808299064636
CurrentTrain: epoch  5, batch     6 | loss: 6.6435704Losses:  4.976111888885498 4.920825481414795 0.02764328569173813
CurrentTrain: epoch  5, batch     7 | loss: 4.9761119Losses:  5.431168556213379 5.375167369842529 0.028000663965940475
CurrentTrain: epoch  5, batch     8 | loss: 5.4311686Losses:  4.297846794128418 4.250378131866455 0.023734385147690773
CurrentTrain: epoch  5, batch     9 | loss: 4.2978468Losses:  6.861867427825928 6.807316780090332 0.027275221422314644
CurrentTrain: epoch  5, batch    10 | loss: 6.8618674Losses:  6.5309576988220215 6.472196578979492 0.029380522668361664
CurrentTrain: epoch  5, batch    11 | loss: 6.5309577Losses:  5.127584457397461 5.067709922790527 0.029937278479337692
CurrentTrain: epoch  5, batch    12 | loss: 5.1275845Losses:  4.232813358306885 4.18914794921875 0.02183271385729313
CurrentTrain: epoch  5, batch    13 | loss: 4.2328134Losses:  5.421922206878662 5.360555648803711 0.03068333864212036
CurrentTrain: epoch  5, batch    14 | loss: 5.4219222Losses:  6.010566711425781 5.956215858459473 0.027175338938832283
CurrentTrain: epoch  5, batch    15 | loss: 6.0105667Losses:  5.869624614715576 5.803071975708008 0.03327639400959015
CurrentTrain: epoch  5, batch    16 | loss: 5.8696246Losses:  4.855154991149902 4.800352096557617 0.02740153856575489
CurrentTrain: epoch  5, batch    17 | loss: 4.8551550Losses:  4.875583171844482 4.821038722991943 0.02727232128381729
CurrentTrain: epoch  5, batch    18 | loss: 4.8755832Losses:  8.644146919250488 8.582709312438965 0.030718598514795303
CurrentTrain: epoch  5, batch    19 | loss: 8.6441469Losses:  8.23970890045166 8.169939041137695 0.034885141998529434
CurrentTrain: epoch  5, batch    20 | loss: 8.2397089Losses:  6.709157466888428 6.648148536682129 0.030504535883665085
CurrentTrain: epoch  5, batch    21 | loss: 6.7091575Losses:  6.005523681640625 5.943842887878418 0.03084043599665165
CurrentTrain: epoch  5, batch    22 | loss: 6.0055237Losses:  10.135479927062988 10.067041397094727 0.034219108521938324
CurrentTrain: epoch  5, batch    23 | loss: 10.1354799Losses:  4.9365363121032715 4.880723476409912 0.02790653519332409
CurrentTrain: epoch  5, batch    24 | loss: 4.9365363Losses:  4.188411712646484 4.143646240234375 0.02238272875547409
CurrentTrain: epoch  5, batch    25 | loss: 4.1884117Losses:  5.8943610191345215 5.841043472290039 0.026658713817596436
CurrentTrain: epoch  5, batch    26 | loss: 5.8943610Losses:  4.856443405151367 4.805663108825684 0.025390097871422768
CurrentTrain: epoch  5, batch    27 | loss: 4.8564434Losses:  5.707877159118652 5.6525797843933105 0.027648676186800003
CurrentTrain: epoch  5, batch    28 | loss: 5.7078772Losses:  5.8507843017578125 5.796142578125 0.027320925146341324
CurrentTrain: epoch  5, batch    29 | loss: 5.8507843Losses:  10.61499309539795 10.524221420288086 0.045385729521512985
CurrentTrain: epoch  5, batch    30 | loss: 10.6149931Losses:  5.421975135803223 5.3727569580078125 0.024609196931123734
CurrentTrain: epoch  5, batch    31 | loss: 5.4219751Losses:  5.203902721405029 5.15441370010376 0.02474440075457096
CurrentTrain: epoch  5, batch    32 | loss: 5.2039027Losses:  5.605085372924805 5.551356315612793 0.026864606887102127
CurrentTrain: epoch  5, batch    33 | loss: 5.6050854Losses:  4.621116638183594 4.5715837478637695 0.024766437709331512
CurrentTrain: epoch  5, batch    34 | loss: 4.6211166Losses:  4.427308559417725 4.379550933837891 0.023878902196884155
CurrentTrain: epoch  5, batch    35 | loss: 4.4273086Losses:  5.180126667022705 5.1300811767578125 0.0250227153301239
CurrentTrain: epoch  5, batch    36 | loss: 5.1801267Losses:  0.7063100934028625 0.6332639455795288 0.03652308136224747
CurrentTrain: epoch  5, batch    37 | loss: 0.7063101Losses:  5.459921836853027 5.401702880859375 0.029109496623277664
CurrentTrain: epoch  6, batch     0 | loss: 5.4599218Losses:  4.548705101013184 4.504663944244385 0.022020479664206505
CurrentTrain: epoch  6, batch     1 | loss: 4.5487051Losses:  5.471564292907715 5.408247470855713 0.03165841102600098
CurrentTrain: epoch  6, batch     2 | loss: 5.4715643Losses:  5.095053195953369 5.035181045532227 0.02993612177670002
CurrentTrain: epoch  6, batch     3 | loss: 5.0950532Losses:  11.171781539916992 11.079540252685547 0.046120867133140564
CurrentTrain: epoch  6, batch     4 | loss: 11.1717815Losses:  4.788502216339111 4.735627174377441 0.02643745392560959
CurrentTrain: epoch  6, batch     5 | loss: 4.7885022Losses:  5.093998908996582 5.043638706207275 0.02518012374639511
CurrentTrain: epoch  6, batch     6 | loss: 5.0939989Losses:  8.447343826293945 8.376317024230957 0.03551339730620384
CurrentTrain: epoch  6, batch     7 | loss: 8.4473438Losses:  9.55748176574707 9.476271629333496 0.04060495272278786
CurrentTrain: epoch  6, batch     8 | loss: 9.5574818Losses:  7.249405384063721 7.194987773895264 0.02720886468887329
CurrentTrain: epoch  6, batch     9 | loss: 7.2494054Losses:  5.629030704498291 5.580166816711426 0.024431923404335976
CurrentTrain: epoch  6, batch    10 | loss: 5.6290307Losses:  7.553141117095947 7.494680881500244 0.02923002652823925
CurrentTrain: epoch  6, batch    11 | loss: 7.5531411Losses:  6.609792709350586 6.545184135437012 0.03230433911085129
CurrentTrain: epoch  6, batch    12 | loss: 6.6097927Losses:  9.617291450500488 9.539867401123047 0.03871195763349533
CurrentTrain: epoch  6, batch    13 | loss: 9.6172915Losses:  9.354118347167969 9.264156341552734 0.04498123377561569
CurrentTrain: epoch  6, batch    14 | loss: 9.3541183Losses:  9.153154373168945 9.082512855529785 0.03532090038061142
CurrentTrain: epoch  6, batch    15 | loss: 9.1531544Losses:  4.676563739776611 4.629796028137207 0.02338380366563797
CurrentTrain: epoch  6, batch    16 | loss: 4.6765637Losses:  5.2517805099487305 5.194553375244141 0.028613565489649773
CurrentTrain: epoch  6, batch    17 | loss: 5.2517805Losses:  5.957393169403076 5.903919219970703 0.02673688530921936
CurrentTrain: epoch  6, batch    18 | loss: 5.9573932Losses:  7.232855319976807 7.1597185134887695 0.03656842187047005
CurrentTrain: epoch  6, batch    19 | loss: 7.2328553Losses:  5.419206619262695 5.372187614440918 0.023509565740823746
CurrentTrain: epoch  6, batch    20 | loss: 5.4192066Losses:  4.547001838684082 4.4958391189575195 0.025581320747733116
CurrentTrain: epoch  6, batch    21 | loss: 4.5470018Losses:  5.968947410583496 5.913394927978516 0.027776338160037994
CurrentTrain: epoch  6, batch    22 | loss: 5.9689474Losses:  4.676942825317383 4.623864650726318 0.026539035141468048
CurrentTrain: epoch  6, batch    23 | loss: 4.6769428Losses:  7.118871212005615 7.05019998550415 0.034335508942604065
CurrentTrain: epoch  6, batch    24 | loss: 7.1188712Losses:  5.40113639831543 5.342463493347168 0.029336534440517426
CurrentTrain: epoch  6, batch    25 | loss: 5.4011364Losses:  4.903868675231934 4.859683036804199 0.022092891857028008
CurrentTrain: epoch  6, batch    26 | loss: 4.9038687Losses:  3.587742805480957 3.5420684814453125 0.022837135940790176
CurrentTrain: epoch  6, batch    27 | loss: 3.5877428Losses:  6.833106994628906 6.766170501708984 0.03346818685531616
CurrentTrain: epoch  6, batch    28 | loss: 6.8331070Losses:  7.22646951675415 7.1655731201171875 0.030448108911514282
CurrentTrain: epoch  6, batch    29 | loss: 7.2264695Losses:  5.562049388885498 5.500884056091309 0.030582640320062637
CurrentTrain: epoch  6, batch    30 | loss: 5.5620494Losses:  7.112571716308594 7.046958923339844 0.03280646353960037
CurrentTrain: epoch  6, batch    31 | loss: 7.1125717Losses:  7.31843376159668 7.233434677124023 0.04249962419271469
CurrentTrain: epoch  6, batch    32 | loss: 7.3184338Losses:  5.830698013305664 5.772488117218018 0.029104923829436302
CurrentTrain: epoch  6, batch    33 | loss: 5.8306980Losses:  6.642977714538574 6.58172607421875 0.030625764280557632
CurrentTrain: epoch  6, batch    34 | loss: 6.6429777Losses:  5.666477203369141 5.605856895446777 0.030310146510601044
CurrentTrain: epoch  6, batch    35 | loss: 5.6664772Losses:  5.994676113128662 5.944851398468018 0.024912375956773758
CurrentTrain: epoch  6, batch    36 | loss: 5.9946761Losses:  3.096883535385132 2.9837729930877686 0.05655527114868164
CurrentTrain: epoch  6, batch    37 | loss: 3.0968835Losses:  5.456001281738281 5.391605854034424 0.0321977436542511
CurrentTrain: epoch  7, batch     0 | loss: 5.4560013Losses:  4.69672155380249 4.6442108154296875 0.026255277916789055
CurrentTrain: epoch  7, batch     1 | loss: 4.6967216Losses:  5.686055660247803 5.627259254455566 0.02939811907708645
CurrentTrain: epoch  7, batch     2 | loss: 5.6860557Losses:  4.178082466125488 4.135939598083496 0.021071499213576317
CurrentTrain: epoch  7, batch     3 | loss: 4.1780825Losses:  9.653145790100098 9.576923370361328 0.03811107203364372
CurrentTrain: epoch  7, batch     4 | loss: 9.6531458Losses:  5.977722644805908 5.912156105041504 0.03278333321213722
CurrentTrain: epoch  7, batch     5 | loss: 5.9777226Losses:  6.586304187774658 6.530671119689941 0.027816511690616608
CurrentTrain: epoch  7, batch     6 | loss: 6.5863042Losses:  5.845595359802246 5.772124290466309 0.03673551604151726
CurrentTrain: epoch  7, batch     7 | loss: 5.8455954Losses:  4.884147644042969 4.838052749633789 0.023047544062137604
CurrentTrain: epoch  7, batch     8 | loss: 4.8841476Losses:  4.817373275756836 4.764314651489258 0.026529284194111824
CurrentTrain: epoch  7, batch     9 | loss: 4.8173733Losses:  5.447659015655518 5.390202045440674 0.02872859500348568
CurrentTrain: epoch  7, batch    10 | loss: 5.4476590Losses:  9.094820022583008 9.017413139343262 0.03870341181755066
CurrentTrain: epoch  7, batch    11 | loss: 9.0948200Losses:  4.192887783050537 4.148409843444824 0.022238871082663536
CurrentTrain: epoch  7, batch    12 | loss: 4.1928878Losses:  8.70219612121582 8.632926940917969 0.03463437408208847
CurrentTrain: epoch  7, batch    13 | loss: 8.7021961Losses:  4.082464218139648 4.037132263183594 0.022666072472929955
CurrentTrain: epoch  7, batch    14 | loss: 4.0824642Losses:  6.9917802810668945 6.914083480834961 0.03884836286306381
CurrentTrain: epoch  7, batch    15 | loss: 6.9917803Losses:  6.026780128479004 5.979058265686035 0.023861035704612732
CurrentTrain: epoch  7, batch    16 | loss: 6.0267801Losses:  4.582854270935059 4.538804531097412 0.022024910897016525
CurrentTrain: epoch  7, batch    17 | loss: 4.5828543Losses:  7.199662208557129 7.134237766265869 0.03271215036511421
CurrentTrain: epoch  7, batch    18 | loss: 7.1996622Losses:  6.243171691894531 6.18711519241333 0.02802831120789051
CurrentTrain: epoch  7, batch    19 | loss: 6.2431717Losses:  4.414740085601807 4.362320899963379 0.026209525763988495
CurrentTrain: epoch  7, batch    20 | loss: 4.4147401Losses:  4.554227828979492 4.511546611785889 0.02134062722325325
CurrentTrain: epoch  7, batch    21 | loss: 4.5542278Losses:  6.114108562469482 6.051513195037842 0.031297601759433746
CurrentTrain: epoch  7, batch    22 | loss: 6.1141086Losses:  7.43958044052124 7.388051509857178 0.025764549151062965
CurrentTrain: epoch  7, batch    23 | loss: 7.4395804Losses:  5.165052890777588 5.109229564666748 0.027911635115742683
CurrentTrain: epoch  7, batch    24 | loss: 5.1650529Losses:  4.598759174346924 4.551424980163574 0.023667212575674057
CurrentTrain: epoch  7, batch    25 | loss: 4.5987592Losses:  3.9840471744537354 3.939044952392578 0.022501157596707344
CurrentTrain: epoch  7, batch    26 | loss: 3.9840472Losses:  5.243667125701904 5.194360256195068 0.024653511121869087
CurrentTrain: epoch  7, batch    27 | loss: 5.2436671Losses:  7.0336432456970215 6.967215538024902 0.03321395069360733
CurrentTrain: epoch  7, batch    28 | loss: 7.0336432Losses:  5.927533149719238 5.864192962646484 0.031670134514570236
CurrentTrain: epoch  7, batch    29 | loss: 5.9275331Losses:  4.380299091339111 4.329636573791504 0.025331230834126472
CurrentTrain: epoch  7, batch    30 | loss: 4.3802991Losses:  4.068329811096191 4.022853851318359 0.022737974300980568
CurrentTrain: epoch  7, batch    31 | loss: 4.0683298Losses:  6.622354984283447 6.56015682220459 0.031099090352654457
CurrentTrain: epoch  7, batch    32 | loss: 6.6223550Losses:  7.706994533538818 7.617800235748291 0.044597093015909195
CurrentTrain: epoch  7, batch    33 | loss: 7.7069945Losses:  5.1286821365356445 5.069966793060303 0.02935771830379963
CurrentTrain: epoch  7, batch    34 | loss: 5.1286821Losses:  10.170467376708984 10.116656303405762 0.02690533921122551
CurrentTrain: epoch  7, batch    35 | loss: 10.1704674Losses:  6.611477851867676 6.537928581237793 0.03677458316087723
CurrentTrain: epoch  7, batch    36 | loss: 6.6114779Losses:  2.5432159900665283 2.4637794494628906 0.039718251675367355
CurrentTrain: epoch  7, batch    37 | loss: 2.5432160Losses:  6.094775199890137 6.043795585632324 0.025489868596196175
CurrentTrain: epoch  8, batch     0 | loss: 6.0947752Losses:  4.7588114738464355 4.712521553039551 0.023144928738474846
CurrentTrain: epoch  8, batch     1 | loss: 4.7588115Losses:  10.585859298706055 10.51470947265625 0.03557489067316055
CurrentTrain: epoch  8, batch     2 | loss: 10.5858593Losses:  8.594999313354492 8.532646179199219 0.031176628544926643
CurrentTrain: epoch  8, batch     3 | loss: 8.5949993Losses:  6.425335884094238 6.364104270935059 0.030615821480751038
CurrentTrain: epoch  8, batch     4 | loss: 6.4253359Losses:  4.599581241607666 4.544865608215332 0.027357926592230797
CurrentTrain: epoch  8, batch     5 | loss: 4.5995812Losses:  4.089282035827637 4.045101165771484 0.022090502083301544
CurrentTrain: epoch  8, batch     6 | loss: 4.0892820Losses:  6.205575942993164 6.148526191711426 0.02852488122880459
CurrentTrain: epoch  8, batch     7 | loss: 6.2055759Losses:  5.479283332824707 5.426661491394043 0.026311013847589493
CurrentTrain: epoch  8, batch     8 | loss: 5.4792833Losses:  3.588148355484009 3.535781145095825 0.026183629408478737
CurrentTrain: epoch  8, batch     9 | loss: 3.5881484Losses:  4.060857772827148 4.015280723571777 0.022788546979427338
CurrentTrain: epoch  8, batch    10 | loss: 4.0608578Losses:  6.241087913513184 6.177957057952881 0.031565435230731964
CurrentTrain: epoch  8, batch    11 | loss: 6.2410879Losses:  6.979382514953613 6.9212646484375 0.029058953747153282
CurrentTrain: epoch  8, batch    12 | loss: 6.9793825Losses:  3.963397264480591 3.91648530960083 0.02345600351691246
CurrentTrain: epoch  8, batch    13 | loss: 3.9633973Losses:  6.675265789031982 6.607011795043945 0.0341268889605999
CurrentTrain: epoch  8, batch    14 | loss: 6.6752658Losses:  5.886880397796631 5.838714599609375 0.024082982912659645
CurrentTrain: epoch  8, batch    15 | loss: 5.8868804Losses:  4.5566229820251465 4.508190631866455 0.024216219782829285
CurrentTrain: epoch  8, batch    16 | loss: 4.5566230Losses:  3.8139493465423584 3.762077808380127 0.025935780256986618
CurrentTrain: epoch  8, batch    17 | loss: 3.8139493Losses:  6.380223274230957 6.306950569152832 0.0366363450884819
CurrentTrain: epoch  8, batch    18 | loss: 6.3802233Losses:  6.583778381347656 6.526698112487793 0.02854011580348015
CurrentTrain: epoch  8, batch    19 | loss: 6.5837784Losses:  7.621932506561279 7.5419206619262695 0.04000600427389145
CurrentTrain: epoch  8, batch    20 | loss: 7.6219325Losses:  7.367938995361328 7.295433044433594 0.036253008991479874
CurrentTrain: epoch  8, batch    21 | loss: 7.3679390Losses:  4.823757171630859 4.766709804534912 0.028523605316877365
CurrentTrain: epoch  8, batch    22 | loss: 4.8237572Losses:  6.0675950050354 5.988796234130859 0.03939947485923767
CurrentTrain: epoch  8, batch    23 | loss: 6.0675950Losses:  8.48104476928711 8.41069221496582 0.03517647087574005
CurrentTrain: epoch  8, batch    24 | loss: 8.4810448Losses:  6.510943412780762 6.44075345993042 0.03509487211704254
CurrentTrain: epoch  8, batch    25 | loss: 6.5109434Losses:  5.4734954833984375 5.406909942626953 0.03329265117645264
CurrentTrain: epoch  8, batch    26 | loss: 5.4734955Losses:  4.658164024353027 4.606241226196289 0.02596144936978817
CurrentTrain: epoch  8, batch    27 | loss: 4.6581640Losses:  3.6083126068115234 3.563227891921997 0.022542301565408707
CurrentTrain: epoch  8, batch    28 | loss: 3.6083126Losses:  5.12040901184082 5.070350646972656 0.02502910979092121
CurrentTrain: epoch  8, batch    29 | loss: 5.1204090Losses:  7.3171844482421875 7.26597785949707 0.0256032831966877
CurrentTrain: epoch  8, batch    30 | loss: 7.3171844Losses:  5.8675150871276855 5.808041572570801 0.02973676286637783
CurrentTrain: epoch  8, batch    31 | loss: 5.8675151Losses:  5.714163303375244 5.659275054931641 0.027444105595350266
CurrentTrain: epoch  8, batch    32 | loss: 5.7141633Losses:  6.465048789978027 6.4069366455078125 0.02905617468059063
CurrentTrain: epoch  8, batch    33 | loss: 6.4650488Losses:  6.9013519287109375 6.8476738929748535 0.02683907561004162
CurrentTrain: epoch  8, batch    34 | loss: 6.9013519Losses:  4.5447468757629395 4.491062164306641 0.02684226632118225
CurrentTrain: epoch  8, batch    35 | loss: 4.5447469Losses:  4.9647698402404785 4.904384613037109 0.030192697420716286
CurrentTrain: epoch  8, batch    36 | loss: 4.9647698Losses:  0.8849064111709595 0.8219839334487915 0.03146124631166458
CurrentTrain: epoch  8, batch    37 | loss: 0.8849064Losses:  4.027927398681641 3.9805784225463867 0.023674430325627327
CurrentTrain: epoch  9, batch     0 | loss: 4.0279274Losses:  4.497639179229736 4.444124221801758 0.026757370680570602
CurrentTrain: epoch  9, batch     1 | loss: 4.4976392Losses:  8.171404838562012 8.111225128173828 0.030089981853961945
CurrentTrain: epoch  9, batch     2 | loss: 8.1714048Losses:  7.030552864074707 6.977060317993164 0.026746228337287903
CurrentTrain: epoch  9, batch     3 | loss: 7.0305529Losses:  3.8966798782348633 3.852233409881592 0.022223247215151787
CurrentTrain: epoch  9, batch     4 | loss: 3.8966799Losses:  4.538094997406006 4.486357688903809 0.02586863934993744
CurrentTrain: epoch  9, batch     5 | loss: 4.5380950Losses:  5.227046012878418 5.178001403808594 0.024522308260202408
CurrentTrain: epoch  9, batch     6 | loss: 5.2270460Losses:  6.082221508026123 6.0110368728637695 0.035592202097177505
CurrentTrain: epoch  9, batch     7 | loss: 6.0822215Losses:  7.57050085067749 7.496331691741943 0.03708454966545105
CurrentTrain: epoch  9, batch     8 | loss: 7.5705009Losses:  5.842994689941406 5.792807102203369 0.025093749165534973
CurrentTrain: epoch  9, batch     9 | loss: 5.8429947Losses:  4.923325538635254 4.874454975128174 0.024435238912701607
CurrentTrain: epoch  9, batch    10 | loss: 4.9233255Losses:  4.067924499511719 4.0215654373168945 0.023179639130830765
CurrentTrain: epoch  9, batch    11 | loss: 4.0679245Losses:  5.429654121398926 5.381436347961426 0.02410881593823433
CurrentTrain: epoch  9, batch    12 | loss: 5.4296541Losses:  5.890086650848389 5.8344855308532715 0.02780057117342949
CurrentTrain: epoch  9, batch    13 | loss: 5.8900867Losses:  6.01674747467041 5.941964149475098 0.03739166259765625
CurrentTrain: epoch  9, batch    14 | loss: 6.0167475Losses:  7.210779666900635 7.144765853881836 0.03300688415765762
CurrentTrain: epoch  9, batch    15 | loss: 7.2107797Losses:  5.961344242095947 5.888276100158691 0.036534059792757034
CurrentTrain: epoch  9, batch    16 | loss: 5.9613442Losses:  4.899317264556885 4.8459882736206055 0.026664400473237038
CurrentTrain: epoch  9, batch    17 | loss: 4.8993173Losses:  5.738081932067871 5.66923713684082 0.034422412514686584
CurrentTrain: epoch  9, batch    18 | loss: 5.7380819Losses:  6.3029398918151855 6.253236293792725 0.024851910769939423
CurrentTrain: epoch  9, batch    19 | loss: 6.3029399Losses:  7.846371173858643 7.766219139099121 0.04007609561085701
CurrentTrain: epoch  9, batch    20 | loss: 7.8463712Losses:  6.049077987670898 5.981560707092285 0.033758725970983505
CurrentTrain: epoch  9, batch    21 | loss: 6.0490780Losses:  4.793693542480469 4.738979816436768 0.027356766164302826
CurrentTrain: epoch  9, batch    22 | loss: 4.7936935Losses:  4.008693218231201 3.9598212242126465 0.02443605288863182
CurrentTrain: epoch  9, batch    23 | loss: 4.0086932Losses:  4.806594371795654 4.751641273498535 0.027476463466882706
CurrentTrain: epoch  9, batch    24 | loss: 4.8065944Losses:  4.477363586425781 4.431475639343262 0.022943859919905663
CurrentTrain: epoch  9, batch    25 | loss: 4.4773636Losses:  6.844888210296631 6.78237247467041 0.031257856637239456
CurrentTrain: epoch  9, batch    26 | loss: 6.8448882Losses:  4.942302227020264 4.888389587402344 0.0269562229514122
CurrentTrain: epoch  9, batch    27 | loss: 4.9423022Losses:  4.921975612640381 4.860140800476074 0.030917415395379066
CurrentTrain: epoch  9, batch    28 | loss: 4.9219756Losses:  6.7589287757873535 6.690909385681152 0.03400971367955208
CurrentTrain: epoch  9, batch    29 | loss: 6.7589288Losses:  6.034342288970947 5.964893341064453 0.034724533557891846
CurrentTrain: epoch  9, batch    30 | loss: 6.0343423Losses:  8.315834045410156 8.228127479553223 0.043853361159563065
CurrentTrain: epoch  9, batch    31 | loss: 8.3158340Losses:  6.464168548583984 6.414103031158447 0.025032656267285347
CurrentTrain: epoch  9, batch    32 | loss: 6.4641685Losses:  8.935933113098145 8.87876033782959 0.028586456552147865
CurrentTrain: epoch  9, batch    33 | loss: 8.9359331Losses:  3.7008354663848877 3.651930809020996 0.02445228025317192
CurrentTrain: epoch  9, batch    34 | loss: 3.7008355Losses:  7.4162821769714355 7.357678413391113 0.029301920905709267
CurrentTrain: epoch  9, batch    35 | loss: 7.4162822Losses:  6.464938640594482 6.386103630065918 0.039417579770088196
CurrentTrain: epoch  9, batch    36 | loss: 6.4649386Losses:  1.418399691581726 1.3605303764343262 0.028934640809893608
CurrentTrain: epoch  9, batch    37 | loss: 1.4183997
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 90.18%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 89.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 87.11%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 86.03%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 84.72%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 83.88%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 84.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.51%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 86.14%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.74%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.96%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.39%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.79%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 88.71%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.87%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 87.31%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 90.18%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 89.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 87.11%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 86.03%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 84.72%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 83.88%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 84.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.51%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 86.14%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.74%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.96%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.39%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.79%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 88.71%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.87%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 87.31%   
cur_acc:  ['0.8731']
his_acc:  ['0.8731']
Clustering into  4  clusters
Clusters:  [3 1 1 3 0 2 3 0 1 0 0]
Losses:  10.245532989501953 9.853839874267578 0.1958465278148651
CurrentTrain: epoch  0, batch     0 | loss: 10.2455330Losses:  4.324838161468506 4.182902812957764 0.07096756994724274
CurrentTrain: epoch  0, batch     1 | loss: 4.3248382Losses:  10.449152946472168 10.085253715515137 0.1819494217634201
CurrentTrain: epoch  1, batch     0 | loss: 10.4491529Losses:  5.2486371994018555 4.805011749267578 0.22181260585784912
CurrentTrain: epoch  1, batch     1 | loss: 5.2486372Losses:  7.564101219177246 7.212118148803711 0.17599165439605713
CurrentTrain: epoch  2, batch     0 | loss: 7.5641012Losses:  2.848184108734131 2.5313313007354736 0.15842638909816742
CurrentTrain: epoch  2, batch     1 | loss: 2.8481841Losses:  8.759695053100586 8.455215454101562 0.15223965048789978
CurrentTrain: epoch  3, batch     0 | loss: 8.7596951Losses:  2.536816120147705 2.2066166400909424 0.16509968042373657
CurrentTrain: epoch  3, batch     1 | loss: 2.5368161Losses:  8.727632522583008 8.42202377319336 0.1528041660785675
CurrentTrain: epoch  4, batch     0 | loss: 8.7276325Losses:  5.582173824310303 5.316213607788086 0.13298016786575317
CurrentTrain: epoch  4, batch     1 | loss: 5.5821738Losses:  7.636120319366455 7.351385116577148 0.14236754179000854
CurrentTrain: epoch  5, batch     0 | loss: 7.6361203Losses:  3.533066987991333 3.1920053958892822 0.170530766248703
CurrentTrain: epoch  5, batch     1 | loss: 3.5330670Losses:  8.902548789978027 8.633037567138672 0.13475573062896729
CurrentTrain: epoch  6, batch     0 | loss: 8.9025488Losses:  3.3994598388671875 3.132280111312866 0.13358989357948303
CurrentTrain: epoch  6, batch     1 | loss: 3.3994598Losses:  6.692219257354736 6.430751800537109 0.1307336688041687
CurrentTrain: epoch  7, batch     0 | loss: 6.6922193Losses:  2.1090011596679688 1.8475782871246338 0.1307114213705063
CurrentTrain: epoch  7, batch     1 | loss: 2.1090012Losses:  6.435606002807617 6.184885025024414 0.1253606081008911
CurrentTrain: epoch  8, batch     0 | loss: 6.4356060Losses:  3.053154230117798 2.791809320449829 0.13067243993282318
CurrentTrain: epoch  8, batch     1 | loss: 3.0531542Losses:  8.932223320007324 8.792734146118164 0.06974435597658157
CurrentTrain: epoch  9, batch     0 | loss: 8.9322233Losses:  5.4530744552612305 5.3383049964904785 0.05738474056124687
CurrentTrain: epoch  9, batch     1 | loss: 5.4530745
Losses:  0.45739954710006714 -0.0 0.22869977355003357
MemoryTrain:  epoch  0, batch     0 | loss: 0.4573995Losses:  0.43702468276023865 -0.0 0.21851234138011932
MemoryTrain:  epoch  1, batch     0 | loss: 0.4370247Losses:  0.42150816321372986 -0.0 0.21075408160686493
MemoryTrain:  epoch  2, batch     0 | loss: 0.4215082Losses:  0.4131016433238983 -0.0 0.20655082166194916
MemoryTrain:  epoch  3, batch     0 | loss: 0.4131016Losses:  0.4085876941680908 -0.0 0.2042938470840454
MemoryTrain:  epoch  4, batch     0 | loss: 0.4085877Losses:  0.40831539034843445 -0.0 0.20415769517421722
MemoryTrain:  epoch  5, batch     0 | loss: 0.4083154Losses:  0.4058408737182617 -0.0 0.20292043685913086
MemoryTrain:  epoch  6, batch     0 | loss: 0.4058409Losses:  0.40483716130256653 -0.0 0.20241858065128326
MemoryTrain:  epoch  7, batch     0 | loss: 0.4048372Losses:  0.40282177925109863 -0.0 0.20141088962554932
MemoryTrain:  epoch  8, batch     0 | loss: 0.4028218Losses:  0.4001277685165405 -0.0 0.20006388425827026
MemoryTrain:  epoch  9, batch     0 | loss: 0.4001278Losses:  0.40168875455856323 -0.0 0.20084437727928162
MemoryTrain:  epoch 10, batch     0 | loss: 0.4016888Losses:  0.3970196545124054 -0.0 0.1985098272562027
MemoryTrain:  epoch 11, batch     0 | loss: 0.3970197Losses:  0.39504683017730713 -0.0 0.19752341508865356
MemoryTrain:  epoch 12, batch     0 | loss: 0.3950468Losses:  0.39853933453559875 -0.0 0.19926966726779938
MemoryTrain:  epoch 13, batch     0 | loss: 0.3985393Losses:  0.3969048857688904 -0.0 0.1984524428844452
MemoryTrain:  epoch 14, batch     0 | loss: 0.3969049Losses:  0.4004577398300171 -0.0 0.20022886991500854
MemoryTrain:  epoch 15, batch     0 | loss: 0.4004577Losses:  0.39411067962646484 -0.0 0.19705533981323242
MemoryTrain:  epoch 16, batch     0 | loss: 0.3941107Losses:  0.39487224817276 -0.0 0.19743612408638
MemoryTrain:  epoch 17, batch     0 | loss: 0.3948722Losses:  0.3951196074485779 -0.0 0.19755980372428894
MemoryTrain:  epoch 18, batch     0 | loss: 0.3951196Losses:  0.3951672315597534 -0.0 0.1975836157798767
MemoryTrain:  epoch 19, batch     0 | loss: 0.3951672
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 12.50%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 10.42%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 10.94%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 13.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 26.04%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 35.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 42.97%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 47.22%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 47.50%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 50.00%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 53.12%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 55.77%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 58.48%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 57.50%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 86.16%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.29%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.63%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 84.78%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 85.16%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.30%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.57%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.05%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 87.28%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 86.88%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 86.49%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 86.52%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 85.61%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 83.09%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 80.89%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 78.99%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 77.36%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 76.97%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 77.40%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 77.97%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 78.05%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 77.98%   [EVAL] batch:   42 | acc: 43.75%,  total acc: 77.18%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 77.56%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 77.78%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 77.99%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 78.06%   
cur_acc:  ['0.8731', '0.5750']
his_acc:  ['0.8731', '0.7806']
Clustering into  7  clusters
Clusters:  [1 3 3 1 0 5 1 4 6 0 0 4 1 2 3 1]
Losses:  11.355587005615234 10.016674041748047 0.6694563031196594
CurrentTrain: epoch  0, batch     0 | loss: 11.3555870Losses:  5.118277549743652 4.055105686187744 0.5315858125686646
CurrentTrain: epoch  0, batch     1 | loss: 5.1182775Losses:  9.308034896850586 8.647616386413574 0.33020904660224915
CurrentTrain: epoch  1, batch     0 | loss: 9.3080349Losses:  4.9380202293396 4.07346248626709 0.4322788417339325
CurrentTrain: epoch  1, batch     1 | loss: 4.9380202Losses:  8.874235153198242 7.951181411743164 0.4615270793437958
CurrentTrain: epoch  2, batch     0 | loss: 8.8742352Losses:  3.066263198852539 2.44203519821167 0.31211403012275696
CurrentTrain: epoch  2, batch     1 | loss: 3.0662632Losses:  8.271834373474121 7.649251461029053 0.31129148602485657
CurrentTrain: epoch  3, batch     0 | loss: 8.2718344Losses:  3.9977643489837646 3.361470937728882 0.3181467056274414
CurrentTrain: epoch  3, batch     1 | loss: 3.9977643Losses:  7.5953521728515625 6.958771705627441 0.3182903528213501
CurrentTrain: epoch  4, batch     0 | loss: 7.5953522Losses:  2.893289089202881 2.3737576007843018 0.25976577401161194
CurrentTrain: epoch  4, batch     1 | loss: 2.8932891Losses:  7.559382915496826 6.991182327270508 0.2841002643108368
CurrentTrain: epoch  5, batch     0 | loss: 7.5593829Losses:  2.8329927921295166 2.2940762042999268 0.2694582939147949
CurrentTrain: epoch  5, batch     1 | loss: 2.8329928Losses:  6.721889019012451 6.1797871589660645 0.27105093002319336
CurrentTrain: epoch  6, batch     0 | loss: 6.7218890Losses:  2.7392868995666504 2.1922550201416016 0.2735159695148468
CurrentTrain: epoch  6, batch     1 | loss: 2.7392869Losses:  8.533950805664062 8.00910758972168 0.2624213695526123
CurrentTrain: epoch  7, batch     0 | loss: 8.5339508Losses:  6.16749906539917 5.82415246963501 0.1716732233762741
CurrentTrain: epoch  7, batch     1 | loss: 6.1674991Losses:  8.867021560668945 8.381783485412598 0.2426188886165619
CurrentTrain: epoch  8, batch     0 | loss: 8.8670216Losses:  4.572023391723633 4.044130325317383 0.2639465034008026
CurrentTrain: epoch  8, batch     1 | loss: 4.5720234Losses:  6.527337074279785 6.012931823730469 0.2572025656700134
CurrentTrain: epoch  9, batch     0 | loss: 6.5273371Losses:  2.0980350971221924 1.6014608144760132 0.2482871413230896
CurrentTrain: epoch  9, batch     1 | loss: 2.0980351
Losses:  1.2423815727233887 -0.0 0.6211907863616943
MemoryTrain:  epoch  0, batch     0 | loss: 1.2423816Losses:  1.200777530670166 -0.0 0.600388765335083
MemoryTrain:  epoch  1, batch     0 | loss: 1.2007775Losses:  1.1656612157821655 -0.0 0.5828306078910828
MemoryTrain:  epoch  2, batch     0 | loss: 1.1656612Losses:  1.1225186586380005 -0.0 0.5612593293190002
MemoryTrain:  epoch  3, batch     0 | loss: 1.1225187Losses:  1.1049848794937134 -0.0 0.5524924397468567
MemoryTrain:  epoch  4, batch     0 | loss: 1.1049849Losses:  1.0859153270721436 -0.0 0.5429576635360718
MemoryTrain:  epoch  5, batch     0 | loss: 1.0859153Losses:  1.0658749341964722 -0.0 0.5329374670982361
MemoryTrain:  epoch  6, batch     0 | loss: 1.0658749Losses:  1.0529133081436157 -0.0 0.5264566540718079
MemoryTrain:  epoch  7, batch     0 | loss: 1.0529133Losses:  1.0444307327270508 -0.0 0.5222153663635254
MemoryTrain:  epoch  8, batch     0 | loss: 1.0444307Losses:  1.034375548362732 -0.0 0.517187774181366
MemoryTrain:  epoch  9, batch     0 | loss: 1.0343755Losses:  1.033782958984375 -0.0 0.5168914794921875
MemoryTrain:  epoch 10, batch     0 | loss: 1.0337830Losses:  1.0323829650878906 -0.0 0.5161914825439453
MemoryTrain:  epoch 11, batch     0 | loss: 1.0323830Losses:  1.0211493968963623 -0.0 0.5105746984481812
MemoryTrain:  epoch 12, batch     0 | loss: 1.0211494Losses:  1.016223430633545 -0.0 0.5081117153167725
MemoryTrain:  epoch 13, batch     0 | loss: 1.0162234Losses:  1.0286729335784912 -0.0 0.5143364667892456
MemoryTrain:  epoch 14, batch     0 | loss: 1.0286729Losses:  1.0023142099380493 -0.0 0.5011571049690247
MemoryTrain:  epoch 15, batch     0 | loss: 1.0023142Losses:  1.0146592855453491 -0.0 0.5073296427726746
MemoryTrain:  epoch 16, batch     0 | loss: 1.0146593Losses:  1.0113857984542847 -0.0 0.5056928992271423
MemoryTrain:  epoch 17, batch     0 | loss: 1.0113858Losses:  0.9929467439651489 -0.0 0.49647337198257446
MemoryTrain:  epoch 18, batch     0 | loss: 0.9929467Losses:  0.9933000802993774 -0.0 0.4966500401496887
MemoryTrain:  epoch 19, batch     0 | loss: 0.9933001
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 41.67%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 42.19%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 40.00%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 40.62%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 41.96%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 46.88%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 52.78%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 55.62%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 56.82%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 59.38%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 60.10%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 57.14%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 66.25%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 66.67%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 79.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 82.21%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 80.80%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 80.42%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 79.30%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 79.04%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 78.62%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 79.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 82.07%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 82.55%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 83.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 83.65%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 83.80%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.91%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 84.79%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 84.88%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 84.96%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 84.28%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 81.80%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 79.82%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 77.95%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 76.86%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 76.64%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 76.44%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 76.88%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 76.37%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 76.93%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 75.44%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 74.86%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 74.44%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 73.51%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 72.87%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 72.14%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 71.68%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 71.12%   [EVAL] batch:   50 | acc: 31.25%,  total acc: 70.34%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 69.83%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 69.46%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 69.10%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 69.32%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 69.64%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 69.96%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 69.94%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 70.23%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 70.10%   [EVAL] batch:   60 | acc: 12.50%,  total acc: 69.16%   
cur_acc:  ['0.8731', '0.5750', '0.5714']
his_acc:  ['0.8731', '0.7806', '0.6916']
Clustering into  9  clusters
Clusters:  [4 1 1 8 0 7 4 2 5 0 3 2 4 3 1 4 4 6 4 3 0]
Losses:  9.546988487243652 8.835867881774902 0.3555601239204407
CurrentTrain: epoch  0, batch     0 | loss: 9.5469885Losses:  6.060587406158447 5.264326572418213 0.39813047647476196
CurrentTrain: epoch  0, batch     1 | loss: 6.0605874Losses:  8.690217018127441 7.9848103523254395 0.35270315408706665
CurrentTrain: epoch  1, batch     0 | loss: 8.6902170Losses:  3.9333596229553223 3.177434206008911 0.3779626488685608
CurrentTrain: epoch  1, batch     1 | loss: 3.9333596Losses:  7.531203269958496 6.810650825500488 0.3602762222290039
CurrentTrain: epoch  2, batch     0 | loss: 7.5312033Losses:  2.050419330596924 1.4350895881652832 0.3076649308204651
CurrentTrain: epoch  2, batch     1 | loss: 2.0504193Losses:  6.935811519622803 6.283476829528809 0.3261672854423523
CurrentTrain: epoch  3, batch     0 | loss: 6.9358115Losses:  2.6608662605285645 2.068057060241699 0.2964046597480774
CurrentTrain: epoch  3, batch     1 | loss: 2.6608663Losses:  6.586642742156982 6.004185676574707 0.29122859239578247
CurrentTrain: epoch  4, batch     0 | loss: 6.5866427Losses:  1.7547788619995117 1.1995868682861328 0.27759602665901184
CurrentTrain: epoch  4, batch     1 | loss: 1.7547789Losses:  7.095487594604492 6.531566143035889 0.28196072578430176
CurrentTrain: epoch  5, batch     0 | loss: 7.0954876Losses:  2.727149248123169 2.4074511528015137 0.15984907746315002
CurrentTrain: epoch  5, batch     1 | loss: 2.7271492Losses:  5.939313888549805 5.418362617492676 0.2604755759239197
CurrentTrain: epoch  6, batch     0 | loss: 5.9393139Losses:  2.1443488597869873 1.5907236337661743 0.2768126428127289
CurrentTrain: epoch  6, batch     1 | loss: 2.1443489Losses:  6.155638217926025 5.639496803283691 0.2580707371234894
CurrentTrain: epoch  7, batch     0 | loss: 6.1556382Losses:  1.5825152397155762 1.0557388067245483 0.2633882164955139
CurrentTrain: epoch  7, batch     1 | loss: 1.5825152Losses:  6.258373737335205 5.751463890075684 0.2534550130367279
CurrentTrain: epoch  8, batch     0 | loss: 6.2583737Losses:  2.431675672531128 1.9176828861236572 0.25699642300605774
CurrentTrain: epoch  8, batch     1 | loss: 2.4316757Losses:  5.919430255889893 5.429315567016602 0.24505724012851715
CurrentTrain: epoch  9, batch     0 | loss: 5.9194303Losses:  1.5130715370178223 1.0199607610702515 0.246555358171463
CurrentTrain: epoch  9, batch     1 | loss: 1.5130715
Losses:  1.3473703861236572 -0.0 0.6736851930618286
MemoryTrain:  epoch  0, batch     0 | loss: 1.3473704Losses:  0.9886512756347656 -0.0 0.4943256378173828
MemoryTrain:  epoch  0, batch     1 | loss: 0.9886513Losses:  1.1948908567428589 -0.0 0.5974454283714294
MemoryTrain:  epoch  1, batch     0 | loss: 1.1948909Losses:  0.6900190711021423 -0.0 0.34500953555107117
MemoryTrain:  epoch  1, batch     1 | loss: 0.6900191Losses:  0.8694509267807007 -0.0 0.43472546339035034
MemoryTrain:  epoch  2, batch     0 | loss: 0.8694509Losses:  1.1428709030151367 -0.0 0.5714354515075684
MemoryTrain:  epoch  2, batch     1 | loss: 1.1428709Losses:  1.1312106847763062 -0.0 0.5656053423881531
MemoryTrain:  epoch  3, batch     0 | loss: 1.1312107Losses:  0.6848744750022888 -0.0 0.3424372375011444
MemoryTrain:  epoch  3, batch     1 | loss: 0.6848745Losses:  1.3463501930236816 -0.0 0.6731750965118408
MemoryTrain:  epoch  4, batch     0 | loss: 1.3463502Losses:  0.6076836585998535 -0.0 0.30384182929992676
MemoryTrain:  epoch  4, batch     1 | loss: 0.6076837Losses:  1.4282890558242798 -0.0 0.7141445279121399
MemoryTrain:  epoch  5, batch     0 | loss: 1.4282891Losses:  0.5536282658576965 -0.0 0.27681413292884827
MemoryTrain:  epoch  5, batch     1 | loss: 0.5536283Losses:  1.276779294013977 -0.0 0.6383896470069885
MemoryTrain:  epoch  6, batch     0 | loss: 1.2767793Losses:  0.7079748511314392 -0.0 0.3539874255657196
MemoryTrain:  epoch  6, batch     1 | loss: 0.7079749Losses:  1.4884089231491089 -0.0 0.7442044615745544
MemoryTrain:  epoch  7, batch     0 | loss: 1.4884089Losses:  0.4467281699180603 -0.0 0.22336408495903015
MemoryTrain:  epoch  7, batch     1 | loss: 0.4467282Losses:  1.265358805656433 -0.0 0.6326794028282166
MemoryTrain:  epoch  8, batch     0 | loss: 1.2653588Losses:  0.5474672317504883 -0.0 0.27373361587524414
MemoryTrain:  epoch  8, batch     1 | loss: 0.5474672Losses:  1.239622950553894 -0.0 0.619811475276947
MemoryTrain:  epoch  9, batch     0 | loss: 1.2396230Losses:  0.7516690492630005 -0.0 0.37583452463150024
MemoryTrain:  epoch  9, batch     1 | loss: 0.7516690Losses:  1.256152629852295 -0.0 0.6280763149261475
MemoryTrain:  epoch 10, batch     0 | loss: 1.2561526Losses:  0.5607563257217407 -0.0 0.28037816286087036
MemoryTrain:  epoch 10, batch     1 | loss: 0.5607563Losses:  1.2123134136199951 -0.0 0.6061567068099976
MemoryTrain:  epoch 11, batch     0 | loss: 1.2123134Losses:  0.3497912287712097 -0.0 0.17489561438560486
MemoryTrain:  epoch 11, batch     1 | loss: 0.3497912Losses:  1.246193766593933 -0.0 0.6230968832969666
MemoryTrain:  epoch 12, batch     0 | loss: 1.2461938Losses:  0.7701073884963989 -0.0 0.38505369424819946
MemoryTrain:  epoch 12, batch     1 | loss: 0.7701074Losses:  1.3559633493423462 -0.0 0.6779816746711731
MemoryTrain:  epoch 13, batch     0 | loss: 1.3559633Losses:  0.602165937423706 -0.0 0.301082968711853
MemoryTrain:  epoch 13, batch     1 | loss: 0.6021659Losses:  1.4111177921295166 -0.0 0.7055588960647583
MemoryTrain:  epoch 14, batch     0 | loss: 1.4111178Losses:  0.6568828821182251 -0.0 0.32844144105911255
MemoryTrain:  epoch 14, batch     1 | loss: 0.6568829Losses:  1.390100121498108 -0.0 0.695050060749054
MemoryTrain:  epoch 15, batch     0 | loss: 1.3901001Losses:  0.35554441809654236 -0.0 0.17777220904827118
MemoryTrain:  epoch 15, batch     1 | loss: 0.3555444Losses:  1.0505672693252563 -0.0 0.5252836346626282
MemoryTrain:  epoch 16, batch     0 | loss: 1.0505673Losses:  0.5134055614471436 -0.0 0.2567027807235718
MemoryTrain:  epoch 16, batch     1 | loss: 0.5134056Losses:  1.097413420677185 -0.0 0.5487067103385925
MemoryTrain:  epoch 17, batch     0 | loss: 1.0974134Losses:  0.5633487701416016 -0.0 0.2816743850708008
MemoryTrain:  epoch 17, batch     1 | loss: 0.5633488Losses:  1.3874536752700806 -0.0 0.6937268376350403
MemoryTrain:  epoch 18, batch     0 | loss: 1.3874537Losses:  0.7052041292190552 -0.0 0.3526020646095276
MemoryTrain:  epoch 18, batch     1 | loss: 0.7052041Losses:  1.2253954410552979 -0.0 0.6126977205276489
MemoryTrain:  epoch 19, batch     0 | loss: 1.2253954Losses:  0.8157967329025269 -0.0 0.4078983664512634
MemoryTrain:  epoch 19, batch     1 | loss: 0.8157967
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 12.50%,  total acc: 70.83%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 66.88%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 64.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 69.71%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 73.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 76.47%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 73.61%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 60.94%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 61.25%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 60.42%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 65.18%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 77.27%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 78.65%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 79.81%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 78.57%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 77.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 76.56%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 76.47%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 75.69%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 75.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 76.79%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 77.56%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 78.26%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 80.29%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 80.56%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 81.90%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 81.88%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 82.06%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 82.23%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 81.82%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 80.88%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 78.75%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 77.26%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 76.35%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 75.99%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 75.80%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 76.25%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 75.61%   [EVAL] batch:   41 | acc: 0.00%,  total acc: 73.81%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 72.38%   [EVAL] batch:   43 | acc: 31.25%,  total acc: 71.45%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 70.83%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 69.84%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 69.02%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 68.36%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 67.73%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 66.75%   [EVAL] batch:   50 | acc: 12.50%,  total acc: 65.69%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 64.78%   [EVAL] batch:   52 | acc: 18.75%,  total acc: 63.92%   [EVAL] batch:   53 | acc: 25.00%,  total acc: 63.19%   [EVAL] batch:   54 | acc: 25.00%,  total acc: 62.50%   [EVAL] batch:   55 | acc: 37.50%,  total acc: 62.05%   [EVAL] batch:   56 | acc: 25.00%,  total acc: 61.40%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 60.67%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 60.38%   [EVAL] batch:   59 | acc: 12.50%,  total acc: 59.58%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 59.43%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 59.58%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 59.92%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 60.35%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 60.58%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 60.70%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 60.73%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 61.31%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 61.14%   [EVAL] batch:   69 | acc: 18.75%,  total acc: 60.54%   [EVAL] batch:   70 | acc: 31.25%,  total acc: 60.12%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 60.24%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 60.79%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 61.32%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 61.83%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 62.25%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 62.74%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 62.90%   
cur_acc:  ['0.8731', '0.5750', '0.5714', '0.7361']
his_acc:  ['0.8731', '0.7806', '0.6916', '0.6290']
Clustering into  12  clusters
Clusters:  [ 4  3  0 10 11  6  4  2  7  9  1  2  4  1  3  4  4  5  4  1  8  0  0  0
  6  1]
Losses:  9.296043395996094 8.625134468078613 0.3354543149471283
CurrentTrain: epoch  0, batch     0 | loss: 9.2960434Losses:  2.8197836875915527 2.2060322761535645 0.30687564611434937
CurrentTrain: epoch  0, batch     1 | loss: 2.8197837Losses:  9.247443199157715 8.749015808105469 0.24921350181102753
CurrentTrain: epoch  1, batch     0 | loss: 9.2474432Losses:  2.509136199951172 2.054715156555176 0.22721049189567566
CurrentTrain: epoch  1, batch     1 | loss: 2.5091362Losses:  10.516236305236816 10.099075317382812 0.20858041942119598
CurrentTrain: epoch  2, batch     0 | loss: 10.5162363Losses:  2.7814738750457764 2.423205614089966 0.17913416028022766
CurrentTrain: epoch  2, batch     1 | loss: 2.7814739Losses:  7.475478649139404 7.123529434204102 0.1759745478630066
CurrentTrain: epoch  3, batch     0 | loss: 7.4754786Losses:  3.0791497230529785 2.7140729427337646 0.18253843486309052
CurrentTrain: epoch  3, batch     1 | loss: 3.0791497Losses:  10.017465591430664 9.661731719970703 0.17786704003810883
CurrentTrain: epoch  4, batch     0 | loss: 10.0174656Losses:  3.3963494300842285 3.081159830093384 0.15759478509426117
CurrentTrain: epoch  4, batch     1 | loss: 3.3963494Losses:  8.416888236999512 8.103569984436035 0.15665912628173828
CurrentTrain: epoch  5, batch     0 | loss: 8.4168882Losses:  2.694878101348877 2.3867061138153076 0.15408602356910706
CurrentTrain: epoch  5, batch     1 | loss: 2.6948781Losses:  6.8845014572143555 6.58750581741333 0.1484978199005127
CurrentTrain: epoch  6, batch     0 | loss: 6.8845015Losses:  4.764550685882568 4.559613227844238 0.10246877372264862
CurrentTrain: epoch  6, batch     1 | loss: 4.7645507Losses:  7.120107173919678 6.844018936157227 0.13804416358470917
CurrentTrain: epoch  7, batch     0 | loss: 7.1201072Losses:  2.276042938232422 1.9896626472473145 0.14319008588790894
CurrentTrain: epoch  7, batch     1 | loss: 2.2760429Losses:  7.179801940917969 6.914060592651367 0.132870614528656
CurrentTrain: epoch  8, batch     0 | loss: 7.1798019Losses:  2.1483120918273926 1.8845901489257812 0.13186095654964447
CurrentTrain: epoch  8, batch     1 | loss: 2.1483121Losses:  6.150210380554199 5.892082214355469 0.12906405329704285
CurrentTrain: epoch  9, batch     0 | loss: 6.1502104Losses:  3.273552417755127 3.012786865234375 0.13038279116153717
CurrentTrain: epoch  9, batch     1 | loss: 3.2735524
Losses:  1.4203336238861084 -0.0 0.7101668119430542
MemoryTrain:  epoch  0, batch     0 | loss: 1.4203336Losses:  1.3545221090316772 -0.0 0.6772610545158386
MemoryTrain:  epoch  0, batch     1 | loss: 1.3545221Losses:  1.461875081062317 -0.0 0.7309375405311584
MemoryTrain:  epoch  1, batch     0 | loss: 1.4618751Losses:  0.8998222351074219 -0.0 0.44991111755371094
MemoryTrain:  epoch  1, batch     1 | loss: 0.8998222Losses:  1.0452183485031128 -0.0 0.5226091742515564
MemoryTrain:  epoch  2, batch     0 | loss: 1.0452183Losses:  1.1401952505111694 -0.0 0.5700976252555847
MemoryTrain:  epoch  2, batch     1 | loss: 1.1401953Losses:  1.4193403720855713 -0.0 0.7096701860427856
MemoryTrain:  epoch  3, batch     0 | loss: 1.4193404Losses:  1.0850369930267334 -0.0 0.5425184965133667
MemoryTrain:  epoch  3, batch     1 | loss: 1.0850370Losses:  1.4553667306900024 -0.0 0.7276833653450012
MemoryTrain:  epoch  4, batch     0 | loss: 1.4553667Losses:  1.0487369298934937 -0.0 0.5243684649467468
MemoryTrain:  epoch  4, batch     1 | loss: 1.0487369Losses:  1.6110248565673828 -0.0 0.8055124282836914
MemoryTrain:  epoch  5, batch     0 | loss: 1.6110249Losses:  0.740424394607544 -0.0 0.370212197303772
MemoryTrain:  epoch  5, batch     1 | loss: 0.7404244Losses:  1.236639380455017 -0.0 0.6183196902275085
MemoryTrain:  epoch  6, batch     0 | loss: 1.2366394Losses:  1.069883108139038 -0.0 0.534941554069519
MemoryTrain:  epoch  6, batch     1 | loss: 1.0698831Losses:  1.223856806755066 -0.0 0.611928403377533
MemoryTrain:  epoch  7, batch     0 | loss: 1.2238568Losses:  1.3497495651245117 -0.0 0.6748747825622559
MemoryTrain:  epoch  7, batch     1 | loss: 1.3497496Losses:  1.3237837553024292 -0.0 0.6618918776512146
MemoryTrain:  epoch  8, batch     0 | loss: 1.3237838Losses:  1.34527587890625 -0.0 0.672637939453125
MemoryTrain:  epoch  8, batch     1 | loss: 1.3452759Losses:  1.1419936418533325 -0.0 0.5709968209266663
MemoryTrain:  epoch  9, batch     0 | loss: 1.1419936Losses:  1.1910591125488281 -0.0 0.5955295562744141
MemoryTrain:  epoch  9, batch     1 | loss: 1.1910591Losses:  1.4599554538726807 -0.0 0.7299777269363403
MemoryTrain:  epoch 10, batch     0 | loss: 1.4599555Losses:  1.0431383848190308 -0.0 0.5215691924095154
MemoryTrain:  epoch 10, batch     1 | loss: 1.0431384Losses:  1.4004597663879395 -0.0 0.7002298831939697
MemoryTrain:  epoch 11, batch     0 | loss: 1.4004598Losses:  1.1967849731445312 -0.0 0.5983924865722656
MemoryTrain:  epoch 11, batch     1 | loss: 1.1967850Losses:  1.1715929508209229 -0.0 0.5857964754104614
MemoryTrain:  epoch 12, batch     0 | loss: 1.1715930Losses:  1.18326735496521 -0.0 0.591633677482605
MemoryTrain:  epoch 12, batch     1 | loss: 1.1832674Losses:  1.1804612874984741 -0.0 0.5902306437492371
MemoryTrain:  epoch 13, batch     0 | loss: 1.1804613Losses:  1.0455355644226074 -0.0 0.5227677822113037
MemoryTrain:  epoch 13, batch     1 | loss: 1.0455356Losses:  1.1694719791412354 -0.0 0.5847359895706177
MemoryTrain:  epoch 14, batch     0 | loss: 1.1694720Losses:  1.5328023433685303 -0.0 0.7664011716842651
MemoryTrain:  epoch 14, batch     1 | loss: 1.5328023Losses:  1.5518535375595093 -0.0 0.7759267687797546
MemoryTrain:  epoch 15, batch     0 | loss: 1.5518535Losses:  0.9449426531791687 -0.0 0.47247132658958435
MemoryTrain:  epoch 15, batch     1 | loss: 0.9449427Losses:  1.1545710563659668 -0.0 0.5772855281829834
MemoryTrain:  epoch 16, batch     0 | loss: 1.1545711Losses:  1.3454335927963257 -0.0 0.6727167963981628
MemoryTrain:  epoch 16, batch     1 | loss: 1.3454336Losses:  1.299999713897705 -0.0 0.6499998569488525
MemoryTrain:  epoch 17, batch     0 | loss: 1.2999997Losses:  1.1849384307861328 -0.0 0.5924692153930664
MemoryTrain:  epoch 17, batch     1 | loss: 1.1849384Losses:  1.625482201576233 -0.0 0.8127411007881165
MemoryTrain:  epoch 18, batch     0 | loss: 1.6254822Losses:  0.5491044521331787 -0.0 0.27455222606658936
MemoryTrain:  epoch 18, batch     1 | loss: 0.5491045Losses:  1.4368572235107422 -0.0 0.7184286117553711
MemoryTrain:  epoch 19, batch     0 | loss: 1.4368572Losses:  1.0437541007995605 -0.0 0.5218770503997803
MemoryTrain:  epoch 19, batch     1 | loss: 1.0437541
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 39.58%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 32.81%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 28.75%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 28.12%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 26.79%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 29.69%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 31.25%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 33.75%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 35.23%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 35.42%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 35.10%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 39.73%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 43.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 47.27%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 49.63%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 51.39%   [EVAL] batch:   18 | acc: 12.50%,  total acc: 49.34%   [EVAL] batch:   19 | acc: 18.75%,  total acc: 47.81%   [EVAL] batch:   20 | acc: 12.50%,  total acc: 46.13%   [EVAL] batch:   21 | acc: 0.00%,  total acc: 44.03%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 54.69%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 53.75%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 53.12%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 57.14%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 69.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 72.16%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 73.96%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 72.12%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 68.75%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 67.97%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 68.38%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 68.06%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 67.76%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 68.44%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 69.64%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 70.74%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 71.74%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 74.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 74.76%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 75.67%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 76.29%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 76.25%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 76.21%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 76.14%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 74.63%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 72.86%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 71.18%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 70.44%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 70.23%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 70.19%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 70.78%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 70.27%   [EVAL] batch:   41 | acc: 0.00%,  total acc: 68.60%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 67.30%   [EVAL] batch:   43 | acc: 37.50%,  total acc: 66.62%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 66.11%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 65.08%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 64.76%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 64.45%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 63.90%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 63.00%   [EVAL] batch:   50 | acc: 12.50%,  total acc: 62.01%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 61.18%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 60.26%   [EVAL] batch:   53 | acc: 37.50%,  total acc: 59.84%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 60.34%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 60.71%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 60.75%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 60.88%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 61.02%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 60.94%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 60.55%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 60.58%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 60.71%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 60.84%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 61.06%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 61.08%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 61.10%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 61.67%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 61.50%   [EVAL] batch:   69 | acc: 31.25%,  total acc: 61.07%   [EVAL] batch:   70 | acc: 37.50%,  total acc: 60.74%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 60.76%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 61.30%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 61.82%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 62.33%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 62.83%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 63.31%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 63.62%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 63.37%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 63.12%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 62.58%   [EVAL] batch:   81 | acc: 12.50%,  total acc: 61.97%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 61.52%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 60.94%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 60.51%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 60.32%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 60.20%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 60.30%   [EVAL] batch:   88 | acc: 37.50%,  total acc: 60.04%   [EVAL] batch:   89 | acc: 37.50%,  total acc: 59.79%   [EVAL] batch:   90 | acc: 50.00%,  total acc: 59.68%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 60.12%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 60.55%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 60.97%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 61.18%   [EVAL] batch:   95 | acc: 62.50%,  total acc: 61.20%   [EVAL] batch:   96 | acc: 12.50%,  total acc: 60.70%   [EVAL] batch:   97 | acc: 18.75%,  total acc: 60.27%   [EVAL] batch:   98 | acc: 12.50%,  total acc: 59.79%   [EVAL] batch:   99 | acc: 0.00%,  total acc: 59.19%   
cur_acc:  ['0.8731', '0.5750', '0.5714', '0.7361', '0.4403']
his_acc:  ['0.8731', '0.7806', '0.6916', '0.6290', '0.5919']
Clustering into  14  clusters
Clusters:  [ 1  8  0  1 13  2  1  3  7 11  4  3  1  4  8  1  1 12  1  4  9  0  0  5
  2  4  1 10  0  6  0]
Losses:  8.604903221130371 7.797821044921875 0.4035412073135376
CurrentTrain: epoch  0, batch     0 | loss: 8.6049032Losses:  2.7130613327026367 1.9999313354492188 0.35656502842903137
CurrentTrain: epoch  0, batch     1 | loss: 2.7130613Losses:  9.109615325927734 8.401538848876953 0.35403841733932495
CurrentTrain: epoch  1, batch     0 | loss: 9.1096153Losses:  2.697988986968994 2.349879741668701 0.1740545928478241
CurrentTrain: epoch  1, batch     1 | loss: 2.6979890Losses:  7.521724700927734 6.918464660644531 0.30163007974624634
CurrentTrain: epoch  2, batch     0 | loss: 7.5217247Losses:  2.588782787322998 2.04699444770813 0.2708941400051117
CurrentTrain: epoch  2, batch     1 | loss: 2.5887828Losses:  6.644759178161621 6.07999849319458 0.28238046169281006
CurrentTrain: epoch  3, batch     0 | loss: 6.6447592Losses:  1.7889877557754517 1.170475721359253 0.30925601720809937
CurrentTrain: epoch  3, batch     1 | loss: 1.7889878Losses:  9.05892562866211 8.544576644897461 0.25717467069625854
CurrentTrain: epoch  4, batch     0 | loss: 9.0589256Losses:  3.3394861221313477 2.7874653339385986 0.2760103642940521
CurrentTrain: epoch  4, batch     1 | loss: 3.3394861Losses:  6.624754905700684 6.132134437561035 0.24631021916866302
CurrentTrain: epoch  5, batch     0 | loss: 6.6247549Losses:  2.625227689743042 2.103391170501709 0.2609182894229889
CurrentTrain: epoch  5, batch     1 | loss: 2.6252277Losses:  6.780713081359863 6.316396236419678 0.23215831816196442
CurrentTrain: epoch  6, batch     0 | loss: 6.7807131Losses:  2.8853063583374023 2.314293384552002 0.2855064272880554
CurrentTrain: epoch  6, batch     1 | loss: 2.8853064Losses:  8.93959903717041 8.452508926391602 0.24354495108127594
CurrentTrain: epoch  7, batch     0 | loss: 8.9395990Losses:  2.720198631286621 2.2630043029785156 0.22859719395637512
CurrentTrain: epoch  7, batch     1 | loss: 2.7201986Losses:  7.304436683654785 6.762845993041992 0.2707952558994293
CurrentTrain: epoch  8, batch     0 | loss: 7.3044367Losses:  2.004265785217285 1.521227240562439 0.24151931703090668
CurrentTrain: epoch  8, batch     1 | loss: 2.0042658Losses:  6.831207275390625 6.25852632522583 0.2863404154777527
CurrentTrain: epoch  9, batch     0 | loss: 6.8312073Losses:  2.2934415340423584 1.8352247476577759 0.22910836338996887
CurrentTrain: epoch  9, batch     1 | loss: 2.2934415
Losses:  1.8571290969848633 -0.0 0.9285645484924316
MemoryTrain:  epoch  0, batch     0 | loss: 1.8571291Losses:  1.4723163843154907 -0.0 0.7361581921577454
MemoryTrain:  epoch  0, batch     1 | loss: 1.4723164Losses:  1.2054991722106934 -0.0 0.6027495861053467
MemoryTrain:  epoch  1, batch     0 | loss: 1.2054992Losses:  1.5555070638656616 -0.0 0.7777535319328308
MemoryTrain:  epoch  1, batch     1 | loss: 1.5555071Losses:  1.6403141021728516 -0.0 0.8201570510864258
MemoryTrain:  epoch  2, batch     0 | loss: 1.6403141Losses:  1.320059895515442 -0.0 0.660029947757721
MemoryTrain:  epoch  2, batch     1 | loss: 1.3200599Losses:  1.5735946893692017 -0.0 0.7867973446846008
MemoryTrain:  epoch  3, batch     0 | loss: 1.5735947Losses:  1.3914772272109985 -0.0 0.6957386136054993
MemoryTrain:  epoch  3, batch     1 | loss: 1.3914772Losses:  1.5191395282745361 -0.0 0.7595697641372681
MemoryTrain:  epoch  4, batch     0 | loss: 1.5191395Losses:  1.2509812116622925 -0.0 0.6254906058311462
MemoryTrain:  epoch  4, batch     1 | loss: 1.2509812Losses:  0.9968984127044678 -0.0 0.4984492063522339
MemoryTrain:  epoch  5, batch     0 | loss: 0.9968984Losses:  1.5485376119613647 -0.0 0.7742688059806824
MemoryTrain:  epoch  5, batch     1 | loss: 1.5485376Losses:  1.344056248664856 -0.0 0.672028124332428
MemoryTrain:  epoch  6, batch     0 | loss: 1.3440562Losses:  1.6897330284118652 -0.0 0.8448665142059326
MemoryTrain:  epoch  6, batch     1 | loss: 1.6897330Losses:  1.55321204662323 -0.0 0.776606023311615
MemoryTrain:  epoch  7, batch     0 | loss: 1.5532120Losses:  1.6191059350967407 -0.0 0.8095529675483704
MemoryTrain:  epoch  7, batch     1 | loss: 1.6191059Losses:  1.470457673072815 -0.0 0.7352288365364075
MemoryTrain:  epoch  8, batch     0 | loss: 1.4704577Losses:  1.5540199279785156 -0.0 0.7770099639892578
MemoryTrain:  epoch  8, batch     1 | loss: 1.5540199Losses:  1.84140944480896 -0.0 0.92070472240448
MemoryTrain:  epoch  9, batch     0 | loss: 1.8414094Losses:  1.0715477466583252 -0.0 0.5357738733291626
MemoryTrain:  epoch  9, batch     1 | loss: 1.0715477Losses:  1.1748991012573242 -0.0 0.5874495506286621
MemoryTrain:  epoch 10, batch     0 | loss: 1.1748991Losses:  1.76686429977417 -0.0 0.883432149887085
MemoryTrain:  epoch 10, batch     1 | loss: 1.7668643Losses:  1.6834087371826172 -0.0 0.8417043685913086
MemoryTrain:  epoch 11, batch     0 | loss: 1.6834087Losses:  1.0785318613052368 -0.0 0.5392659306526184
MemoryTrain:  epoch 11, batch     1 | loss: 1.0785319Losses:  1.4769142866134644 -0.0 0.7384571433067322
MemoryTrain:  epoch 12, batch     0 | loss: 1.4769143Losses:  1.469495415687561 -0.0 0.7347477078437805
MemoryTrain:  epoch 12, batch     1 | loss: 1.4694954Losses:  1.871711254119873 -0.0 0.9358556270599365
MemoryTrain:  epoch 13, batch     0 | loss: 1.8717113Losses:  0.9873465299606323 -0.0 0.49367326498031616
MemoryTrain:  epoch 13, batch     1 | loss: 0.9873465Losses:  1.5231071710586548 -0.0 0.7615535855293274
MemoryTrain:  epoch 14, batch     0 | loss: 1.5231072Losses:  1.3328742980957031 -0.0 0.6664371490478516
MemoryTrain:  epoch 14, batch     1 | loss: 1.3328743Losses:  1.5782819986343384 -0.0 0.7891409993171692
MemoryTrain:  epoch 15, batch     0 | loss: 1.5782820Losses:  1.1875746250152588 -0.0 0.5937873125076294
MemoryTrain:  epoch 15, batch     1 | loss: 1.1875746Losses:  1.3553589582443237 -0.0 0.6776794791221619
MemoryTrain:  epoch 16, batch     0 | loss: 1.3553590Losses:  1.4397486448287964 -0.0 0.7198743224143982
MemoryTrain:  epoch 16, batch     1 | loss: 1.4397486Losses:  1.3450802564620972 -0.0 0.6725401282310486
MemoryTrain:  epoch 17, batch     0 | loss: 1.3450803Losses:  1.5127657651901245 -0.0 0.7563828825950623
MemoryTrain:  epoch 17, batch     1 | loss: 1.5127658Losses:  1.3211196660995483 -0.0 0.6605598330497742
MemoryTrain:  epoch 18, batch     0 | loss: 1.3211197Losses:  1.4748144149780273 -0.0 0.7374072074890137
MemoryTrain:  epoch 18, batch     1 | loss: 1.4748144Losses:  1.2802873849868774 -0.0 0.6401436924934387
MemoryTrain:  epoch 19, batch     0 | loss: 1.2802874Losses:  1.727125883102417 -0.0 0.8635629415512085
MemoryTrain:  epoch 19, batch     1 | loss: 1.7271259
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 73.44%   
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 31.25%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 29.46%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 26.56%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 27.08%   [EVAL] batch:    9 | acc: 12.50%,  total acc: 25.62%   [EVAL] batch:   10 | acc: 12.50%,  total acc: 24.43%   [EVAL] batch:   11 | acc: 18.75%,  total acc: 23.96%   [EVAL] batch:   12 | acc: 0.00%,  total acc: 22.12%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 22.77%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 25.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 27.73%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 30.51%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 32.29%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 33.88%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 36.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 39.29%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 42.05%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 44.57%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 46.88%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 49.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 50.96%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 52.31%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 53.79%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 55.17%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 55.83%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 56.65%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 57.42%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 57.20%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 56.07%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 54.64%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 53.82%   [EVAL] batch:   36 | acc: 25.00%,  total acc: 53.04%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 53.12%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 53.53%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 54.53%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 54.57%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 54.91%   [EVAL] batch:   42 | acc: 6.25%,  total acc: 53.78%   [EVAL] batch:   43 | acc: 25.00%,  total acc: 53.12%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 52.78%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 52.31%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 52.39%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 52.47%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 52.04%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 51.38%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 50.49%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 49.88%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 49.06%   [EVAL] batch:   53 | acc: 6.25%,  total acc: 48.26%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 47.39%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 46.54%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 45.72%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 44.94%   [EVAL] batch:   58 | acc: 6.25%,  total acc: 44.28%   [EVAL] batch:   59 | acc: 0.00%,  total acc: 43.54%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 43.44%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 43.65%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 44.25%   [EVAL] batch:   63 | acc: 62.50%,  total acc: 44.53%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 44.90%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 45.36%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 45.71%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 46.51%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 46.47%   [EVAL] batch:   69 | acc: 25.00%,  total acc: 46.16%   [EVAL] batch:   70 | acc: 31.25%,  total acc: 45.95%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 46.18%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 46.92%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 47.64%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 48.33%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 49.01%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 49.68%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 50.00%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 49.76%   [EVAL] batch:   79 | acc: 12.50%,  total acc: 49.30%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 48.77%   [EVAL] batch:   81 | acc: 12.50%,  total acc: 48.32%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 47.97%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 47.54%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 47.13%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 46.95%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 46.62%   [EVAL] batch:   87 | acc: 37.50%,  total acc: 46.52%   [EVAL] batch:   88 | acc: 25.00%,  total acc: 46.28%   [EVAL] batch:   89 | acc: 25.00%,  total acc: 46.04%   [EVAL] batch:   90 | acc: 37.50%,  total acc: 45.95%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 46.47%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 46.98%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 47.54%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 47.96%   [EVAL] batch:   95 | acc: 68.75%,  total acc: 48.18%   [EVAL] batch:   96 | acc: 18.75%,  total acc: 47.87%   [EVAL] batch:   97 | acc: 6.25%,  total acc: 47.45%   [EVAL] batch:   98 | acc: 18.75%,  total acc: 47.16%   [EVAL] batch:   99 | acc: 56.25%,  total acc: 47.25%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 47.65%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 47.79%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 48.00%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 48.50%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 48.93%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 49.41%   [EVAL] batch:  106 | acc: 25.00%,  total acc: 49.18%   
cur_acc:  ['0.8731', '0.5750', '0.5714', '0.7361', '0.4403', '0.7344']
his_acc:  ['0.8731', '0.7806', '0.6916', '0.6290', '0.5919', '0.4918']
Clustering into  17  clusters
Clusters:  [ 1  3  0  1 16  5  1  2 15 13  4 12  1  4  3 11  1  9 11  4  8  0  0  7
  5  4  1 14  0  6  0  2 10  5  4 11]
Losses:  7.435212135314941 6.692926406860352 0.37114280462265015
CurrentTrain: epoch  0, batch     0 | loss: 7.4352121Losses:  2.209014654159546 1.4959994554519653 0.35650762915611267
CurrentTrain: epoch  0, batch     1 | loss: 2.2090147Losses:  7.391597747802734 6.710348606109619 0.34062460064888
CurrentTrain: epoch  1, batch     0 | loss: 7.3915977Losses:  3.7418384552001953 3.1485371589660645 0.29665064811706543
CurrentTrain: epoch  1, batch     1 | loss: 3.7418385Losses:  5.907943248748779 5.245671272277832 0.3311360776424408
CurrentTrain: epoch  2, batch     0 | loss: 5.9079432Losses:  3.0521676540374756 2.3796043395996094 0.3362816274166107
CurrentTrain: epoch  2, batch     1 | loss: 3.0521677Losses:  6.996002197265625 6.345694541931152 0.32515379786491394
CurrentTrain: epoch  3, batch     0 | loss: 6.9960022Losses:  2.0782485008239746 1.5979444980621338 0.2401520162820816
CurrentTrain: epoch  3, batch     1 | loss: 2.0782485Losses:  6.78084659576416 6.135072231292725 0.32288721203804016
CurrentTrain: epoch  4, batch     0 | loss: 6.7808466Losses:  3.4393203258514404 2.9114153385162354 0.26395246386528015
CurrentTrain: epoch  4, batch     1 | loss: 3.4393203Losses:  5.705436706542969 5.066960334777832 0.3192380964756012
CurrentTrain: epoch  5, batch     0 | loss: 5.7054367Losses:  2.602429151535034 1.9542325735092163 0.3240983188152313
CurrentTrain: epoch  5, batch     1 | loss: 2.6024292Losses:  6.720192909240723 6.095050811767578 0.31257110834121704
CurrentTrain: epoch  6, batch     0 | loss: 6.7201929Losses:  2.0733940601348877 1.6155601739883423 0.22891691327095032
CurrentTrain: epoch  6, batch     1 | loss: 2.0733941Losses:  5.651782512664795 5.028775691986084 0.31150346994400024
CurrentTrain: epoch  7, batch     0 | loss: 5.6517825Losses:  2.4578685760498047 1.8216207027435303 0.318123996257782
CurrentTrain: epoch  7, batch     1 | loss: 2.4578686Losses:  5.634187698364258 5.016655921936035 0.30876579880714417
CurrentTrain: epoch  8, batch     0 | loss: 5.6341877Losses:  2.3642542362213135 1.7406350374221802 0.31180962920188904
CurrentTrain: epoch  8, batch     1 | loss: 2.3642542Losses:  6.151059627532959 5.539350509643555 0.3058546483516693
CurrentTrain: epoch  9, batch     0 | loss: 6.1510596Losses:  2.0041260719299316 1.383926272392273 0.31009989976882935
CurrentTrain: epoch  9, batch     1 | loss: 2.0041261
Losses:  1.8005329370498657 -0.0 0.9002664685249329
MemoryTrain:  epoch  0, batch     0 | loss: 1.8005329Losses:  1.5737680196762085 -0.0 0.7868840098381042
MemoryTrain:  epoch  0, batch     1 | loss: 1.5737680Losses:  0.5790283679962158 -0.0 0.2895141839981079
MemoryTrain:  epoch  0, batch     2 | loss: 0.5790284Losses:  1.3148404359817505 -0.0 0.6574202179908752
MemoryTrain:  epoch  1, batch     0 | loss: 1.3148404Losses:  2.0033390522003174 -0.0 1.0016695261001587
MemoryTrain:  epoch  1, batch     1 | loss: 2.0033391Losses:  0.9896324872970581 -0.0 0.49481624364852905
MemoryTrain:  epoch  1, batch     2 | loss: 0.9896325Losses:  1.7861933708190918 -0.0 0.8930966854095459
MemoryTrain:  epoch  2, batch     0 | loss: 1.7861934Losses:  1.7038904428482056 -0.0 0.8519452214241028
MemoryTrain:  epoch  2, batch     1 | loss: 1.7038904Losses:  0.6770730018615723 -0.0 0.33853650093078613
MemoryTrain:  epoch  2, batch     2 | loss: 0.6770730Losses:  1.5072475671768188 -0.0 0.7536237835884094
MemoryTrain:  epoch  3, batch     0 | loss: 1.5072476Losses:  1.7991877794265747 -0.0 0.8995938897132874
MemoryTrain:  epoch  3, batch     1 | loss: 1.7991878Losses:  0.6989827156066895 -0.0 0.3494913578033447
MemoryTrain:  epoch  3, batch     2 | loss: 0.6989827Losses:  1.5829296112060547 -0.0 0.7914648056030273
MemoryTrain:  epoch  4, batch     0 | loss: 1.5829296Losses:  1.8288283348083496 -0.0 0.9144141674041748
MemoryTrain:  epoch  4, batch     1 | loss: 1.8288283Losses:  0.590343713760376 -0.0 0.295171856880188
MemoryTrain:  epoch  4, batch     2 | loss: 0.5903437Losses:  1.78336501121521 -0.0 0.891682505607605
MemoryTrain:  epoch  5, batch     0 | loss: 1.7833650Losses:  1.5657548904418945 -0.0 0.7828774452209473
MemoryTrain:  epoch  5, batch     1 | loss: 1.5657549Losses:  0.4111030101776123 -0.0 0.20555150508880615
MemoryTrain:  epoch  5, batch     2 | loss: 0.4111030Losses:  1.712465524673462 -0.0 0.856232762336731
MemoryTrain:  epoch  6, batch     0 | loss: 1.7124655Losses:  1.5297126770019531 -0.0 0.7648563385009766
MemoryTrain:  epoch  6, batch     1 | loss: 1.5297127Losses:  0.5607552528381348 -0.0 0.2803776264190674
MemoryTrain:  epoch  6, batch     2 | loss: 0.5607553Losses:  1.6556997299194336 -0.0 0.8278498649597168
MemoryTrain:  epoch  7, batch     0 | loss: 1.6556997Losses:  1.53022038936615 -0.0 0.765110194683075
MemoryTrain:  epoch  7, batch     1 | loss: 1.5302204Losses:  0.4189978241920471 -0.0 0.20949891209602356
MemoryTrain:  epoch  7, batch     2 | loss: 0.4189978Losses:  1.380139708518982 -0.0 0.690069854259491
MemoryTrain:  epoch  8, batch     0 | loss: 1.3801397Losses:  1.9514517784118652 -0.0 0.9757258892059326
MemoryTrain:  epoch  8, batch     1 | loss: 1.9514518Losses:  0.29056814312934875 -0.0 0.14528407156467438
MemoryTrain:  epoch  8, batch     2 | loss: 0.2905681Losses:  1.86601722240448 -0.0 0.93300861120224
MemoryTrain:  epoch  9, batch     0 | loss: 1.8660172Losses:  1.548265814781189 -0.0 0.7741329073905945
MemoryTrain:  epoch  9, batch     1 | loss: 1.5482658Losses:  0.7243730425834656 -0.0 0.3621865212917328
MemoryTrain:  epoch  9, batch     2 | loss: 0.7243730Losses:  1.443049430847168 -0.0 0.721524715423584
MemoryTrain:  epoch 10, batch     0 | loss: 1.4430494Losses:  1.4217602014541626 -0.0 0.7108801007270813
MemoryTrain:  epoch 10, batch     1 | loss: 1.4217602Losses:  0.46240705251693726 -0.0 0.23120352625846863
MemoryTrain:  epoch 10, batch     2 | loss: 0.4624071Losses:  1.8882538080215454 -0.0 0.9441269040107727
MemoryTrain:  epoch 11, batch     0 | loss: 1.8882538Losses:  1.4829418659210205 -0.0 0.7414709329605103
MemoryTrain:  epoch 11, batch     1 | loss: 1.4829419Losses:  0.6756176948547363 -0.0 0.33780884742736816
MemoryTrain:  epoch 11, batch     2 | loss: 0.6756177Losses:  1.7605724334716797 -0.0 0.8802862167358398
MemoryTrain:  epoch 12, batch     0 | loss: 1.7605724Losses:  1.2015005350112915 -0.0 0.6007502675056458
MemoryTrain:  epoch 12, batch     1 | loss: 1.2015005Losses:  0.7111036777496338 -0.0 0.3555518388748169
MemoryTrain:  epoch 12, batch     2 | loss: 0.7111037Losses:  1.3338550329208374 -0.0 0.6669275164604187
MemoryTrain:  epoch 13, batch     0 | loss: 1.3338550Losses:  1.6258108615875244 -0.0 0.8129054307937622
MemoryTrain:  epoch 13, batch     1 | loss: 1.6258109Losses:  0.986635684967041 -0.0 0.4933178424835205
MemoryTrain:  epoch 13, batch     2 | loss: 0.9866357Losses:  1.8430609703063965 -0.0 0.9215304851531982
MemoryTrain:  epoch 14, batch     0 | loss: 1.8430610Losses:  1.7210547924041748 -0.0 0.8605273962020874
MemoryTrain:  epoch 14, batch     1 | loss: 1.7210548Losses:  0.581998884677887 -0.0 0.2909994423389435
MemoryTrain:  epoch 14, batch     2 | loss: 0.5819989Losses:  1.3684760332107544 -0.0 0.6842380166053772
MemoryTrain:  epoch 15, batch     0 | loss: 1.3684760Losses:  1.70133376121521 -0.0 0.850666880607605
MemoryTrain:  epoch 15, batch     1 | loss: 1.7013338Losses:  0.32258808612823486 -0.0 0.16129404306411743
MemoryTrain:  epoch 15, batch     2 | loss: 0.3225881Losses:  1.2034529447555542 -0.0 0.6017264723777771
MemoryTrain:  epoch 16, batch     0 | loss: 1.2034529Losses:  1.838675856590271 -0.0 0.9193379282951355
MemoryTrain:  epoch 16, batch     1 | loss: 1.8386759Losses:  0.5650488138198853 -0.0 0.2825244069099426
MemoryTrain:  epoch 16, batch     2 | loss: 0.5650488Losses:  1.4462000131607056 -0.0 0.7231000065803528
MemoryTrain:  epoch 17, batch     0 | loss: 1.4462000Losses:  1.6441148519515991 -0.0 0.8220574259757996
MemoryTrain:  epoch 17, batch     1 | loss: 1.6441149Losses:  0.5468244552612305 -0.0 0.27341222763061523
MemoryTrain:  epoch 17, batch     2 | loss: 0.5468245Losses:  1.8319921493530273 -0.0 0.9159960746765137
MemoryTrain:  epoch 18, batch     0 | loss: 1.8319921Losses:  1.2064247131347656 -0.0 0.6032123565673828
MemoryTrain:  epoch 18, batch     1 | loss: 1.2064247Losses:  0.22288382053375244 -0.0 0.11144191026687622
MemoryTrain:  epoch 18, batch     2 | loss: 0.2228838Losses:  1.5488661527633667 -0.0 0.7744330763816833
MemoryTrain:  epoch 19, batch     0 | loss: 1.5488662Losses:  1.455318808555603 -0.0 0.7276594042778015
MemoryTrain:  epoch 19, batch     1 | loss: 1.4553188Losses:  0.40325498580932617 -0.0 0.20162749290466309
MemoryTrain:  epoch 19, batch     2 | loss: 0.4032550
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 12.50%,  total acc: 65.97%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 64.38%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 63.64%   [EVAL] batch:   11 | acc: 25.00%,  total acc: 60.42%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 56.73%   
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 9.38%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 6.25%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 7.81%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 6.25%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 7.29%   [EVAL] batch:    6 | acc: 6.25%,  total acc: 7.14%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 6.25%   [EVAL] batch:    8 | acc: 12.50%,  total acc: 6.94%   [EVAL] batch:    9 | acc: 6.25%,  total acc: 6.88%   [EVAL] batch:   10 | acc: 0.00%,  total acc: 6.25%   [EVAL] batch:   11 | acc: 25.00%,  total acc: 7.81%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 8.17%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 9.82%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 13.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 16.41%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 19.49%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 21.88%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 23.68%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 26.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 30.06%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 33.24%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 36.14%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 38.80%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 41.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 43.51%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 45.37%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 46.88%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 48.28%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 49.17%   [EVAL] batch:   30 | acc: 62.50%,  total acc: 49.60%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 50.59%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 50.38%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 49.08%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 47.86%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 47.05%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 46.11%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 46.22%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 46.47%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 47.50%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 47.87%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 48.36%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 47.24%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 46.16%   [EVAL] batch:   44 | acc: 6.25%,  total acc: 45.28%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 44.70%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 44.15%   [EVAL] batch:   47 | acc: 25.00%,  total acc: 43.75%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 43.49%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 42.88%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 42.03%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 41.59%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 40.92%   [EVAL] batch:   53 | acc: 31.25%,  total acc: 40.74%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 41.59%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 42.41%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 42.87%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 43.21%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 43.54%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 43.75%   [EVAL] batch:   60 | acc: 25.00%,  total acc: 43.44%   [EVAL] batch:   61 | acc: 37.50%,  total acc: 43.35%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 42.96%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 42.68%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 42.21%   [EVAL] batch:   65 | acc: 31.25%,  total acc: 42.05%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 41.88%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 42.74%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 42.84%   [EVAL] batch:   69 | acc: 6.25%,  total acc: 42.32%   [EVAL] batch:   70 | acc: 6.25%,  total acc: 41.81%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 42.10%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 42.55%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 43.33%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 43.75%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 44.24%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 44.81%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 45.03%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 44.86%   [EVAL] batch:   79 | acc: 6.25%,  total acc: 44.38%   [EVAL] batch:   80 | acc: 12.50%,  total acc: 43.98%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 43.75%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 43.52%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 43.23%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 42.87%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 42.59%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 42.17%   [EVAL] batch:   87 | acc: 25.00%,  total acc: 41.97%   [EVAL] batch:   88 | acc: 12.50%,  total acc: 41.64%   [EVAL] batch:   89 | acc: 6.25%,  total acc: 41.25%   [EVAL] batch:   90 | acc: 25.00%,  total acc: 41.07%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 41.24%   [EVAL] batch:   92 | acc: 31.25%,  total acc: 41.13%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 41.16%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 41.32%   [EVAL] batch:   95 | acc: 18.75%,  total acc: 41.08%   [EVAL] batch:   96 | acc: 6.25%,  total acc: 40.72%   [EVAL] batch:   97 | acc: 12.50%,  total acc: 40.43%   [EVAL] batch:   98 | acc: 12.50%,  total acc: 40.15%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 40.12%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 40.59%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 40.93%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 41.44%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 42.01%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 42.50%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 42.98%   [EVAL] batch:  106 | acc: 81.25%,  total acc: 43.34%   [EVAL] batch:  107 | acc: 62.50%,  total acc: 43.52%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 43.81%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 44.20%   [EVAL] batch:  110 | acc: 75.00%,  total acc: 44.48%   [EVAL] batch:  111 | acc: 81.25%,  total acc: 44.81%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 45.02%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 45.18%   [EVAL] batch:  114 | acc: 18.75%,  total acc: 44.95%   [EVAL] batch:  115 | acc: 37.50%,  total acc: 44.88%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 45.03%   [EVAL] batch:  117 | acc: 25.00%,  total acc: 44.86%   [EVAL] batch:  118 | acc: 25.00%,  total acc: 44.70%   
cur_acc:  ['0.8731', '0.5750', '0.5714', '0.7361', '0.4403', '0.7344', '0.5673']
his_acc:  ['0.8731', '0.7806', '0.6916', '0.6290', '0.5919', '0.4918', '0.4470']
Clustering into  19  clusters
Clusters:  [ 0  4  5  0  2 11  0  3 15 18  1 13  0  1  4  7  0  9  7  1  2  5  5 17
 11  1  0 14  5  6 16  3 10 11  1  7  6  8  0  5 12]
Losses:  7.573476791381836 6.737597465515137 0.41793957352638245
CurrentTrain: epoch  0, batch     0 | loss: 7.5734768Losses:  3.8767056465148926 3.140362024307251 0.368171751499176
CurrentTrain: epoch  0, batch     1 | loss: 3.8767056Losses:  6.472536563873291 5.632793426513672 0.41987162828445435
CurrentTrain: epoch  1, batch     0 | loss: 6.4725366Losses:  2.1053411960601807 1.3714858293533325 0.36692771315574646
CurrentTrain: epoch  1, batch     1 | loss: 2.1053412Losses:  8.186659812927246 7.413025379180908 0.386817067861557
CurrentTrain: epoch  2, batch     0 | loss: 8.1866598Losses:  2.8596880435943604 2.3506977558135986 0.25449511408805847
CurrentTrain: epoch  2, batch     1 | loss: 2.8596880Losses:  7.616404056549072 6.866091251373291 0.37515631318092346
CurrentTrain: epoch  3, batch     0 | loss: 7.6164041Losses:  3.568016290664673 2.8298585414886475 0.3690788447856903
CurrentTrain: epoch  3, batch     1 | loss: 3.5680163Losses:  8.006031036376953 7.2769012451171875 0.3645647466182709
CurrentTrain: epoch  4, batch     0 | loss: 8.0060310Losses:  4.3209123611450195 3.7176403999328613 0.3016359210014343
CurrentTrain: epoch  4, batch     1 | loss: 4.3209124Losses:  7.31415319442749 6.627668380737305 0.34324249625205994
CurrentTrain: epoch  5, batch     0 | loss: 7.3141532Losses:  2.793952226638794 2.313563585281372 0.24019436538219452
CurrentTrain: epoch  5, batch     1 | loss: 2.7939522Losses:  6.489131450653076 5.827890396118164 0.3306204378604889
CurrentTrain: epoch  6, batch     0 | loss: 6.4891315Losses:  2.7096126079559326 2.0362679958343506 0.33667227625846863
CurrentTrain: epoch  6, batch     1 | loss: 2.7096126Losses:  6.903491497039795 6.259064674377441 0.32221344113349915
CurrentTrain: epoch  7, batch     0 | loss: 6.9034915Losses:  2.7807507514953613 2.2899410724639893 0.24540480971336365
CurrentTrain: epoch  7, batch     1 | loss: 2.7807508Losses:  6.6141557693481445 5.9703240394592285 0.32191574573516846
CurrentTrain: epoch  8, batch     0 | loss: 6.6141558Losses:  2.982825756072998 2.447625160217285 0.2676003575325012
CurrentTrain: epoch  8, batch     1 | loss: 2.9828258Losses:  7.998966217041016 7.362782001495361 0.3180921971797943
CurrentTrain: epoch  9, batch     0 | loss: 7.9989662Losses:  4.136348724365234 3.8170082569122314 0.15967033803462982
CurrentTrain: epoch  9, batch     1 | loss: 4.1363487
Losses:  1.88046395778656 -0.0 0.94023197889328
MemoryTrain:  epoch  0, batch     0 | loss: 1.8804640Losses:  1.684765100479126 -0.0 0.842382550239563
MemoryTrain:  epoch  0, batch     1 | loss: 1.6847651Losses:  1.010648488998413 -0.0 0.5053242444992065
MemoryTrain:  epoch  0, batch     2 | loss: 1.0106485Losses:  1.5595018863677979 -0.0 0.7797509431838989
MemoryTrain:  epoch  1, batch     0 | loss: 1.5595019Losses:  1.889091968536377 -0.0 0.9445459842681885
MemoryTrain:  epoch  1, batch     1 | loss: 1.8890920Losses:  1.2438796758651733 -0.0 0.6219398379325867
MemoryTrain:  epoch  1, batch     2 | loss: 1.2438797Losses:  1.7072042226791382 -0.0 0.8536021113395691
MemoryTrain:  epoch  2, batch     0 | loss: 1.7072042Losses:  1.490200161933899 -0.0 0.7451000809669495
MemoryTrain:  epoch  2, batch     1 | loss: 1.4902002Losses:  1.1930840015411377 -0.0 0.5965420007705688
MemoryTrain:  epoch  2, batch     2 | loss: 1.1930840Losses:  1.357359766960144 -0.0 0.678679883480072
MemoryTrain:  epoch  3, batch     0 | loss: 1.3573598Losses:  1.8168678283691406 -0.0 0.9084339141845703
MemoryTrain:  epoch  3, batch     1 | loss: 1.8168678Losses:  1.0762609243392944 -0.0 0.5381304621696472
MemoryTrain:  epoch  3, batch     2 | loss: 1.0762609Losses:  1.690447449684143 -0.0 0.8452237248420715
MemoryTrain:  epoch  4, batch     0 | loss: 1.6904474Losses:  1.8334182500839233 -0.0 0.9167091250419617
MemoryTrain:  epoch  4, batch     1 | loss: 1.8334183Losses:  0.8930772542953491 -0.0 0.44653862714767456
MemoryTrain:  epoch  4, batch     2 | loss: 0.8930773Losses:  1.6302417516708374 -0.0 0.8151208758354187
MemoryTrain:  epoch  5, batch     0 | loss: 1.6302418Losses:  1.7952970266342163 -0.0 0.8976485133171082
MemoryTrain:  epoch  5, batch     1 | loss: 1.7952970Losses:  0.8155311942100525 -0.0 0.40776559710502625
MemoryTrain:  epoch  5, batch     2 | loss: 0.8155312Losses:  1.86008620262146 -0.0 0.93004310131073
MemoryTrain:  epoch  6, batch     0 | loss: 1.8600862Losses:  1.3483291864395142 -0.0 0.6741645932197571
MemoryTrain:  epoch  6, batch     1 | loss: 1.3483292Losses:  1.218879222869873 -0.0 0.6094396114349365
MemoryTrain:  epoch  6, batch     2 | loss: 1.2188792Losses:  1.6332921981811523 -0.0 0.8166460990905762
MemoryTrain:  epoch  7, batch     0 | loss: 1.6332922Losses:  1.8538509607315063 -0.0 0.9269254803657532
MemoryTrain:  epoch  7, batch     1 | loss: 1.8538510Losses:  0.8555737733840942 -0.0 0.4277868866920471
MemoryTrain:  epoch  7, batch     2 | loss: 0.8555738Losses:  1.9351117610931396 -0.0 0.9675558805465698
MemoryTrain:  epoch  8, batch     0 | loss: 1.9351118Losses:  1.3333901166915894 -0.0 0.6666950583457947
MemoryTrain:  epoch  8, batch     1 | loss: 1.3333901Losses:  1.2439000606536865 -0.0 0.6219500303268433
MemoryTrain:  epoch  8, batch     2 | loss: 1.2439001Losses:  1.7473845481872559 -0.0 0.8736922740936279
MemoryTrain:  epoch  9, batch     0 | loss: 1.7473845Losses:  1.4787238836288452 -0.0 0.7393619418144226
MemoryTrain:  epoch  9, batch     1 | loss: 1.4787239Losses:  1.0497233867645264 -0.0 0.5248616933822632
MemoryTrain:  epoch  9, batch     2 | loss: 1.0497234Losses:  1.6490087509155273 -0.0 0.8245043754577637
MemoryTrain:  epoch 10, batch     0 | loss: 1.6490088Losses:  1.736619472503662 -0.0 0.868309736251831
MemoryTrain:  epoch 10, batch     1 | loss: 1.7366195Losses:  1.2247028350830078 -0.0 0.6123514175415039
MemoryTrain:  epoch 10, batch     2 | loss: 1.2247028Losses:  1.6328250169754028 -0.0 0.8164125084877014
MemoryTrain:  epoch 11, batch     0 | loss: 1.6328250Losses:  1.7867648601531982 -0.0 0.8933824300765991
MemoryTrain:  epoch 11, batch     1 | loss: 1.7867649Losses:  1.0715699195861816 -0.0 0.5357849597930908
MemoryTrain:  epoch 11, batch     2 | loss: 1.0715699Losses:  1.3575935363769531 -0.0 0.6787967681884766
MemoryTrain:  epoch 12, batch     0 | loss: 1.3575935Losses:  1.535811424255371 -0.0 0.7679057121276855
MemoryTrain:  epoch 12, batch     1 | loss: 1.5358114Losses:  1.135672926902771 -0.0 0.5678364634513855
MemoryTrain:  epoch 12, batch     2 | loss: 1.1356729Losses:  1.6125612258911133 -0.0 0.8062806129455566
MemoryTrain:  epoch 13, batch     0 | loss: 1.6125612Losses:  1.6199144124984741 -0.0 0.8099572062492371
MemoryTrain:  epoch 13, batch     1 | loss: 1.6199144Losses:  1.0420278310775757 -0.0 0.5210139155387878
MemoryTrain:  epoch 13, batch     2 | loss: 1.0420278Losses:  1.8537204265594482 -0.0 0.9268602132797241
MemoryTrain:  epoch 14, batch     0 | loss: 1.8537204Losses:  1.5586532354354858 -0.0 0.7793266177177429
MemoryTrain:  epoch 14, batch     1 | loss: 1.5586532Losses:  1.240530014038086 -0.0 0.620265007019043
MemoryTrain:  epoch 14, batch     2 | loss: 1.2405300Losses:  1.3231844902038574 -0.0 0.6615922451019287
MemoryTrain:  epoch 15, batch     0 | loss: 1.3231845Losses:  1.9441152811050415 -0.0 0.9720576405525208
MemoryTrain:  epoch 15, batch     1 | loss: 1.9441153Losses:  1.2742047309875488 -0.0 0.6371023654937744
MemoryTrain:  epoch 15, batch     2 | loss: 1.2742047Losses:  1.8358458280563354 -0.0 0.9179229140281677
MemoryTrain:  epoch 16, batch     0 | loss: 1.8358458Losses:  1.6809179782867432 -0.0 0.8404589891433716
MemoryTrain:  epoch 16, batch     1 | loss: 1.6809180Losses:  0.9493114948272705 -0.0 0.47465574741363525
MemoryTrain:  epoch 16, batch     2 | loss: 0.9493115Losses:  1.846163034439087 -0.0 0.9230815172195435
MemoryTrain:  epoch 17, batch     0 | loss: 1.8461630Losses:  1.4649914503097534 -0.0 0.7324957251548767
MemoryTrain:  epoch 17, batch     1 | loss: 1.4649915Losses:  1.1407839059829712 -0.0 0.5703919529914856
MemoryTrain:  epoch 17, batch     2 | loss: 1.1407839Losses:  1.4837265014648438 -0.0 0.7418632507324219
MemoryTrain:  epoch 18, batch     0 | loss: 1.4837265Losses:  1.7791087627410889 -0.0 0.8895543813705444
MemoryTrain:  epoch 18, batch     1 | loss: 1.7791088Losses:  1.3273098468780518 -0.0 0.6636549234390259
MemoryTrain:  epoch 18, batch     2 | loss: 1.3273098Losses:  1.5786997079849243 -0.0 0.7893498539924622
MemoryTrain:  epoch 19, batch     0 | loss: 1.5786997Losses:  1.845878005027771 -0.0 0.9229390025138855
MemoryTrain:  epoch 19, batch     1 | loss: 1.8458780Losses:  1.0264220237731934 -0.0 0.5132110118865967
MemoryTrain:  epoch 19, batch     2 | loss: 1.0264220
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 97.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 97.66%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 93.06%   [EVAL] batch:    9 | acc: 12.50%,  total acc: 85.00%   [EVAL] batch:   10 | acc: 12.50%,  total acc: 78.41%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 75.00%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 73.56%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 72.32%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 9.38%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 6.25%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 7.50%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 6.25%   [EVAL] batch:    6 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    8 | acc: 25.00%,  total acc: 8.33%   [EVAL] batch:    9 | acc: 25.00%,  total acc: 10.00%   [EVAL] batch:   10 | acc: 12.50%,  total acc: 10.23%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 11.98%   [EVAL] batch:   12 | acc: 0.00%,  total acc: 11.06%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 12.50%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 16.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 18.75%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 22.06%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 24.31%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 25.99%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 28.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 32.14%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 35.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 38.04%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 40.62%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 43.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 45.19%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 46.76%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 48.21%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 49.57%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 50.42%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 51.21%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 51.56%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 52.08%   [EVAL] batch:   33 | acc: 37.50%,  total acc: 51.65%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 51.43%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 51.74%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 51.86%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 52.30%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 52.40%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 53.28%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 53.51%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 54.32%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 53.05%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 51.85%   [EVAL] batch:   44 | acc: 6.25%,  total acc: 50.83%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 50.14%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 49.47%   [EVAL] batch:   47 | acc: 31.25%,  total acc: 49.09%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 48.72%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 48.00%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 47.06%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 46.51%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 45.75%   [EVAL] batch:   53 | acc: 37.50%,  total acc: 45.60%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 46.25%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 46.76%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 47.26%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 47.63%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 47.88%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 47.92%   [EVAL] batch:   60 | acc: 0.00%,  total acc: 47.13%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 46.37%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 45.73%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 45.02%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 44.33%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 43.75%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 43.28%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 44.12%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 44.20%   [EVAL] batch:   69 | acc: 6.25%,  total acc: 43.66%   [EVAL] batch:   70 | acc: 6.25%,  total acc: 43.13%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 43.40%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 44.18%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 44.93%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 45.67%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 46.38%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 47.08%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 47.44%   [EVAL] batch:   78 | acc: 6.25%,  total acc: 46.91%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 46.33%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 45.83%   [EVAL] batch:   81 | acc: 6.25%,  total acc: 45.35%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 44.95%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 44.64%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 44.19%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 43.90%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 43.46%   [EVAL] batch:   87 | acc: 25.00%,  total acc: 43.25%   [EVAL] batch:   88 | acc: 12.50%,  total acc: 42.91%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 42.43%   [EVAL] batch:   90 | acc: 18.75%,  total acc: 42.17%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 42.39%   [EVAL] batch:   92 | acc: 37.50%,  total acc: 42.34%   [EVAL] batch:   93 | acc: 56.25%,  total acc: 42.49%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 42.63%   [EVAL] batch:   95 | acc: 25.00%,  total acc: 42.45%   [EVAL] batch:   96 | acc: 12.50%,  total acc: 42.14%   [EVAL] batch:   97 | acc: 12.50%,  total acc: 41.84%   [EVAL] batch:   98 | acc: 12.50%,  total acc: 41.54%   [EVAL] batch:   99 | acc: 56.25%,  total acc: 41.69%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 42.14%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 42.46%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 42.90%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 43.39%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 43.87%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 44.34%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 44.51%   [EVAL] batch:  107 | acc: 56.25%,  total acc: 44.62%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 44.90%   [EVAL] batch:  109 | acc: 68.75%,  total acc: 45.11%   [EVAL] batch:  110 | acc: 75.00%,  total acc: 45.38%   [EVAL] batch:  111 | acc: 81.25%,  total acc: 45.70%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 45.85%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 45.94%   [EVAL] batch:  114 | acc: 18.75%,  total acc: 45.71%   [EVAL] batch:  115 | acc: 43.75%,  total acc: 45.69%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 45.78%   [EVAL] batch:  117 | acc: 37.50%,  total acc: 45.71%   [EVAL] batch:  118 | acc: 37.50%,  total acc: 45.64%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 46.09%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 46.44%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 46.82%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 47.26%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 47.68%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 48.10%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 48.51%   [EVAL] batch:  126 | acc: 100.00%,  total acc: 48.92%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 48.88%   [EVAL] batch:  128 | acc: 12.50%,  total acc: 48.59%   [EVAL] batch:  129 | acc: 18.75%,  total acc: 48.37%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 48.33%   [EVAL] batch:  131 | acc: 56.25%,  total acc: 48.39%   [EVAL] batch:  132 | acc: 43.75%,  total acc: 48.36%   
cur_acc:  ['0.8731', '0.5750', '0.5714', '0.7361', '0.4403', '0.7344', '0.5673', '0.7232']
his_acc:  ['0.8731', '0.7806', '0.6916', '0.6290', '0.5919', '0.4918', '0.4470', '0.4836']
--------Round  2
seed:  300
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 1 1 0]
Losses:  9.339351654052734 8.223658561706543 0.5578464269638062
CurrentTrain: epoch  0, batch     0 | loss: 9.3393517Losses:  9.703474044799805 8.604186058044434 0.5496437549591064
CurrentTrain: epoch  0, batch     1 | loss: 9.7034740Losses:  9.81266975402832 8.755260467529297 0.5287046432495117
CurrentTrain: epoch  0, batch     2 | loss: 9.8126698Losses:  10.719366073608398 9.686933517456055 0.516216516494751
CurrentTrain: epoch  0, batch     3 | loss: 10.7193661Losses:  12.379536628723145 11.421548843383789 0.4789940118789673
CurrentTrain: epoch  0, batch     4 | loss: 12.3795366Losses:  8.241266250610352 7.274058818817139 0.4836035966873169
CurrentTrain: epoch  0, batch     5 | loss: 8.2412663Losses:  11.977274894714355 11.118645668029785 0.4293145537376404
CurrentTrain: epoch  0, batch     6 | loss: 11.9772749Losses:  10.187482833862305 9.367996215820312 0.4097435176372528
CurrentTrain: epoch  0, batch     7 | loss: 10.1874828Losses:  13.522329330444336 12.690407752990723 0.4159606993198395
CurrentTrain: epoch  0, batch     8 | loss: 13.5223293Losses:  9.726284980773926 8.793542861938477 0.4663708508014679
CurrentTrain: epoch  0, batch     9 | loss: 9.7262850Losses:  8.331358909606934 7.467679977416992 0.43183937668800354
CurrentTrain: epoch  0, batch    10 | loss: 8.3313589Losses:  10.18608283996582 9.430761337280273 0.37766069173812866
CurrentTrain: epoch  0, batch    11 | loss: 10.1860828Losses:  15.224733352661133 14.435659408569336 0.39453721046447754
CurrentTrain: epoch  0, batch    12 | loss: 15.2247334Losses:  11.406302452087402 10.700142860412598 0.3530796468257904
CurrentTrain: epoch  0, batch    13 | loss: 11.4063025Losses:  8.571017265319824 7.896508693695068 0.33725428581237793
CurrentTrain: epoch  0, batch    14 | loss: 8.5710173Losses:  13.39207649230957 12.787650108337402 0.30221328139305115
CurrentTrain: epoch  0, batch    15 | loss: 13.3920765Losses:  8.100408554077148 7.516343116760254 0.29203253984451294
CurrentTrain: epoch  0, batch    16 | loss: 8.1004086Losses:  8.597967147827148 7.978654861450195 0.30965620279312134
CurrentTrain: epoch  0, batch    17 | loss: 8.5979671Losses:  7.192473888397217 6.59756326675415 0.2974552810192108
CurrentTrain: epoch  0, batch    18 | loss: 7.1924739Losses:  9.928796768188477 9.346573829650879 0.29111164808273315
CurrentTrain: epoch  0, batch    19 | loss: 9.9287968Losses:  16.88137435913086 16.39826774597168 0.24155361950397491
CurrentTrain: epoch  0, batch    20 | loss: 16.8813744Losses:  7.8355231285095215 7.271234512329102 0.2821442484855652
CurrentTrain: epoch  0, batch    21 | loss: 7.8355231Losses:  7.881104469299316 7.405797481536865 0.23765349388122559
CurrentTrain: epoch  0, batch    22 | loss: 7.8811045Losses:  9.249631881713867 8.836446762084961 0.2065924108028412
CurrentTrain: epoch  0, batch    23 | loss: 9.2496319Losses:  8.891901969909668 8.4859619140625 0.2029702067375183
CurrentTrain: epoch  0, batch    24 | loss: 8.8919020Losses:  7.114649295806885 6.823326110839844 0.14566151797771454
CurrentTrain: epoch  0, batch    25 | loss: 7.1146493Losses:  7.787638187408447 7.362987518310547 0.212325319647789
CurrentTrain: epoch  0, batch    26 | loss: 7.7876382Losses:  8.394613265991211 7.997515678405762 0.19854885339736938
CurrentTrain: epoch  0, batch    27 | loss: 8.3946133Losses:  6.985553741455078 6.681388854980469 0.15208250284194946
CurrentTrain: epoch  0, batch    28 | loss: 6.9855537Losses:  6.702554702758789 6.416840553283691 0.14285708963871002
CurrentTrain: epoch  0, batch    29 | loss: 6.7025547Losses:  7.102806568145752 6.814785003662109 0.1440107822418213
CurrentTrain: epoch  0, batch    30 | loss: 7.1028066Losses:  6.534938335418701 6.272696018218994 0.13112115859985352
CurrentTrain: epoch  0, batch    31 | loss: 6.5349383Losses:  6.753940582275391 6.466474533081055 0.1437329202890396
CurrentTrain: epoch  0, batch    32 | loss: 6.7539406Losses:  15.102961540222168 14.802967071533203 0.1499970257282257
CurrentTrain: epoch  0, batch    33 | loss: 15.1029615Losses:  7.957522869110107 7.714654445648193 0.12143423408269882
CurrentTrain: epoch  0, batch    34 | loss: 7.9575229Losses:  7.0348405838012695 6.810749053955078 0.11204572767019272
CurrentTrain: epoch  0, batch    35 | loss: 7.0348406Losses:  6.714792728424072 6.484096527099609 0.11534810066223145
CurrentTrain: epoch  0, batch    36 | loss: 6.7147927Losses:  1.1810357570648193 0.9818770885467529 0.0995793342590332
CurrentTrain: epoch  0, batch    37 | loss: 1.1810358Losses:  7.413982391357422 7.20553731918335 0.10422255098819733
CurrentTrain: epoch  1, batch     0 | loss: 7.4139824Losses:  6.09255838394165 5.910881042480469 0.09083859622478485
CurrentTrain: epoch  1, batch     1 | loss: 6.0925584Losses:  6.004892826080322 5.845126152038574 0.0798833966255188
CurrentTrain: epoch  1, batch     2 | loss: 6.0048928Losses:  5.562779426574707 5.407639026641846 0.0775701254606247
CurrentTrain: epoch  1, batch     3 | loss: 5.5627794Losses:  7.58290433883667 7.412471771240234 0.08521637320518494
CurrentTrain: epoch  1, batch     4 | loss: 7.5829043Losses:  8.069725036621094 7.917125701904297 0.0762997418642044
CurrentTrain: epoch  1, batch     5 | loss: 8.0697250Losses:  6.976337909698486 6.843184947967529 0.06657642871141434
CurrentTrain: epoch  1, batch     6 | loss: 6.9763379Losses:  5.981523513793945 5.841391563415527 0.07006604969501495
CurrentTrain: epoch  1, batch     7 | loss: 5.9815235Losses:  7.1510491371154785 6.982945919036865 0.08405151963233948
CurrentTrain: epoch  1, batch     8 | loss: 7.1510491Losses:  8.73056411743164 8.610614776611328 0.059974901378154755
CurrentTrain: epoch  1, batch     9 | loss: 8.7305641Losses:  8.731760025024414 8.571551322937012 0.08010414242744446
CurrentTrain: epoch  1, batch    10 | loss: 8.7317600Losses:  9.519617080688477 9.384211540222168 0.06770297884941101
CurrentTrain: epoch  1, batch    11 | loss: 9.5196171Losses:  6.27028226852417 6.150590896606445 0.059845782816410065
CurrentTrain: epoch  1, batch    12 | loss: 6.2702823Losses:  6.765274524688721 6.6389617919921875 0.06315639615058899
CurrentTrain: epoch  1, batch    13 | loss: 6.7652745Losses:  7.601995468139648 7.459753036499023 0.07112118601799011
CurrentTrain: epoch  1, batch    14 | loss: 7.6019955Losses:  7.356517314910889 7.230988025665283 0.06276462227106094
CurrentTrain: epoch  1, batch    15 | loss: 7.3565173Losses:  8.859376907348633 8.747007369995117 0.056184906512498856
CurrentTrain: epoch  1, batch    16 | loss: 8.8593769Losses:  14.462015151977539 14.317895889282227 0.07205957174301147
CurrentTrain: epoch  1, batch    17 | loss: 14.4620152Losses:  8.777081489562988 8.661898612976074 0.05759149417281151
CurrentTrain: epoch  1, batch    18 | loss: 8.7770815Losses:  7.121664047241211 7.016661643981934 0.05250124633312225
CurrentTrain: epoch  1, batch    19 | loss: 7.1216640Losses:  5.105511665344238 4.999783992767334 0.05286393314599991
CurrentTrain: epoch  1, batch    20 | loss: 5.1055117Losses:  6.437780857086182 6.334239959716797 0.05177038908004761
CurrentTrain: epoch  1, batch    21 | loss: 6.4377809Losses:  6.582192420959473 6.469219207763672 0.056486550718545914
CurrentTrain: epoch  1, batch    22 | loss: 6.5821924Losses:  6.5363240242004395 6.427765369415283 0.05427931621670723
CurrentTrain: epoch  1, batch    23 | loss: 6.5363240Losses:  7.819561004638672 7.708825588226318 0.05536775290966034
CurrentTrain: epoch  1, batch    24 | loss: 7.8195610Losses:  5.780700206756592 5.680960655212402 0.049869686365127563
CurrentTrain: epoch  1, batch    25 | loss: 5.7807002Losses:  8.01513385772705 7.8815813064575195 0.06677637994289398
CurrentTrain: epoch  1, batch    26 | loss: 8.0151339Losses:  6.671619892120361 6.573823928833008 0.04889797046780586
CurrentTrain: epoch  1, batch    27 | loss: 6.6716199Losses:  6.030102252960205 5.933020114898682 0.04854099825024605
CurrentTrain: epoch  1, batch    28 | loss: 6.0301023Losses:  4.768843650817871 4.674955368041992 0.04694418981671333
CurrentTrain: epoch  1, batch    29 | loss: 4.7688437Losses:  7.093191146850586 6.98047399520874 0.056358613073825836
CurrentTrain: epoch  1, batch    30 | loss: 7.0931911Losses:  6.3365631103515625 6.2479705810546875 0.04429629072546959
CurrentTrain: epoch  1, batch    31 | loss: 6.3365631Losses:  4.987709999084473 4.8939104080200195 0.046899717301130295
CurrentTrain: epoch  1, batch    32 | loss: 4.9877100Losses:  5.557549953460693 5.472527503967285 0.0425112210214138
CurrentTrain: epoch  1, batch    33 | loss: 5.5575500Losses:  7.388996601104736 7.297162055969238 0.04591725394129753
CurrentTrain: epoch  1, batch    34 | loss: 7.3889966Losses:  7.252865314483643 7.159306526184082 0.04677930846810341
CurrentTrain: epoch  1, batch    35 | loss: 7.2528653Losses:  6.195981025695801 6.103032112121582 0.04647443816065788
CurrentTrain: epoch  1, batch    36 | loss: 6.1959810Losses:  0.7298130393028259 0.6381528377532959 0.04583008959889412
CurrentTrain: epoch  1, batch    37 | loss: 0.7298130Losses:  8.289167404174805 8.196086883544922 0.04654043912887573
CurrentTrain: epoch  2, batch     0 | loss: 8.2891674Losses:  7.116855144500732 7.029521465301514 0.043666914105415344
CurrentTrain: epoch  2, batch     1 | loss: 7.1168551Losses:  5.321305274963379 5.242946624755859 0.03917929530143738
CurrentTrain: epoch  2, batch     2 | loss: 5.3213053Losses:  7.683625221252441 7.594658851623535 0.04448319226503372
CurrentTrain: epoch  2, batch     3 | loss: 7.6836252Losses:  5.7686872482299805 5.687324047088623 0.04068158566951752
CurrentTrain: epoch  2, batch     4 | loss: 5.7686872Losses:  4.870151519775391 4.784967422485352 0.042592164129018784
CurrentTrain: epoch  2, batch     5 | loss: 4.8701515Losses:  7.603346347808838 7.520559310913086 0.041393425315618515
CurrentTrain: epoch  2, batch     6 | loss: 7.6033463Losses:  5.3523173332214355 5.270107746124268 0.041104815900325775
CurrentTrain: epoch  2, batch     7 | loss: 5.3523173Losses:  7.880786895751953 7.8018798828125 0.039453450590372086
CurrentTrain: epoch  2, batch     8 | loss: 7.8807869Losses:  7.406281471252441 7.328451156616211 0.03891521319746971
CurrentTrain: epoch  2, batch     9 | loss: 7.4062815Losses:  6.158384799957275 6.0751848220825195 0.04159996658563614
CurrentTrain: epoch  2, batch    10 | loss: 6.1583848Losses:  5.00138521194458 4.933689594268799 0.03384769707918167
CurrentTrain: epoch  2, batch    11 | loss: 5.0013852Losses:  6.630621433258057 6.549161911010742 0.0407298281788826
CurrentTrain: epoch  2, batch    12 | loss: 6.6306214Losses:  10.211188316345215 10.122465133666992 0.04436163604259491
CurrentTrain: epoch  2, batch    13 | loss: 10.2111883Losses:  4.382526874542236 4.315446853637695 0.033540066331624985
CurrentTrain: epoch  2, batch    14 | loss: 4.3825269Losses:  5.704397678375244 5.629486560821533 0.03745551034808159
CurrentTrain: epoch  2, batch    15 | loss: 5.7043977Losses:  5.438507556915283 5.370767593383789 0.03387005254626274
CurrentTrain: epoch  2, batch    16 | loss: 5.4385076Losses:  5.70192813873291 5.624805450439453 0.03856131434440613
CurrentTrain: epoch  2, batch    17 | loss: 5.7019281Losses:  6.396421909332275 6.306180477142334 0.04512060433626175
CurrentTrain: epoch  2, batch    18 | loss: 6.3964219Losses:  7.510862827301025 7.436246871948242 0.03730805963277817
CurrentTrain: epoch  2, batch    19 | loss: 7.5108628Losses:  7.190062046051025 7.1016669273376465 0.04419763758778572
CurrentTrain: epoch  2, batch    20 | loss: 7.1900620Losses:  5.605216979980469 5.5357255935668945 0.034745603799819946
CurrentTrain: epoch  2, batch    21 | loss: 5.6052170Losses:  6.696275234222412 6.627281188964844 0.03449693322181702
CurrentTrain: epoch  2, batch    22 | loss: 6.6962752Losses:  5.927691459655762 5.855649471282959 0.036021098494529724
CurrentTrain: epoch  2, batch    23 | loss: 5.9276915Losses:  5.606040954589844 5.539020538330078 0.03351032733917236
CurrentTrain: epoch  2, batch    24 | loss: 5.6060410Losses:  6.720047950744629 6.655853748321533 0.032097138464450836
CurrentTrain: epoch  2, batch    25 | loss: 6.7200480Losses:  7.150537014007568 7.072397708892822 0.03906958922743797
CurrentTrain: epoch  2, batch    26 | loss: 7.1505370Losses:  5.596925258636475 5.526973724365234 0.034975796937942505
CurrentTrain: epoch  2, batch    27 | loss: 5.5969253Losses:  8.21959114074707 8.132085800170898 0.04375278204679489
CurrentTrain: epoch  2, batch    28 | loss: 8.2195911Losses:  7.059376239776611 6.994229793548584 0.03257323056459427
CurrentTrain: epoch  2, batch    29 | loss: 7.0593762Losses:  6.560584545135498 6.470559120178223 0.04501280188560486
CurrentTrain: epoch  2, batch    30 | loss: 6.5605845Losses:  6.186583995819092 6.104511260986328 0.0410364530980587
CurrentTrain: epoch  2, batch    31 | loss: 6.1865840Losses:  8.79400634765625 8.716625213623047 0.03869074210524559
CurrentTrain: epoch  2, batch    32 | loss: 8.7940063Losses:  6.139620304107666 6.071224212646484 0.03419796749949455
CurrentTrain: epoch  2, batch    33 | loss: 6.1396203Losses:  4.9714579582214355 4.904120445251465 0.033668652176856995
CurrentTrain: epoch  2, batch    34 | loss: 4.9714580Losses:  4.030727386474609 3.9747214317321777 0.028002867475152016
CurrentTrain: epoch  2, batch    35 | loss: 4.0307274Losses:  4.738750457763672 4.670584678649902 0.034082990139722824
CurrentTrain: epoch  2, batch    36 | loss: 4.7387505Losses:  1.8752684593200684 1.7904775142669678 0.04239547997713089
CurrentTrain: epoch  2, batch    37 | loss: 1.8752685Losses:  4.876413822174072 4.817923545837402 0.029245194047689438
CurrentTrain: epoch  3, batch     0 | loss: 4.8764138Losses:  6.304660797119141 6.241929054260254 0.03136593848466873
CurrentTrain: epoch  3, batch     1 | loss: 6.3046608Losses:  5.645023822784424 5.586399078369141 0.02931228093802929
CurrentTrain: epoch  3, batch     2 | loss: 5.6450238Losses:  7.288609981536865 7.223916053771973 0.032346926629543304
CurrentTrain: epoch  3, batch     3 | loss: 7.2886100Losses:  4.906336307525635 4.848321914672852 0.02900725044310093
CurrentTrain: epoch  3, batch     4 | loss: 4.9063363Losses:  6.166533946990967 6.109482765197754 0.028525691479444504
CurrentTrain: epoch  3, batch     5 | loss: 6.1665339Losses:  6.055018901824951 5.998023986816406 0.028497364372015
CurrentTrain: epoch  3, batch     6 | loss: 6.0550189Losses:  7.79064416885376 7.730238437652588 0.030202874913811684
CurrentTrain: epoch  3, batch     7 | loss: 7.7906442Losses:  6.421624660491943 6.359850883483887 0.030886901542544365
CurrentTrain: epoch  3, batch     8 | loss: 6.4216247Losses:  4.422159671783447 4.366089344024658 0.028035206720232964
CurrentTrain: epoch  3, batch     9 | loss: 4.4221597Losses:  5.429544448852539 5.376444339752197 0.026550062000751495
CurrentTrain: epoch  3, batch    10 | loss: 5.4295444Losses:  6.547591209411621 6.483652591705322 0.031969279050827026
CurrentTrain: epoch  3, batch    11 | loss: 6.5475912Losses:  8.345099449157715 8.250244140625 0.047427594661712646
CurrentTrain: epoch  3, batch    12 | loss: 8.3450994Losses:  7.3998212814331055 7.320720672607422 0.039550282061100006
CurrentTrain: epoch  3, batch    13 | loss: 7.3998213Losses:  6.346472263336182 6.274086952209473 0.036192651838064194
CurrentTrain: epoch  3, batch    14 | loss: 6.3464723Losses:  4.507717132568359 4.446622848510742 0.03054702840745449
CurrentTrain: epoch  3, batch    15 | loss: 4.5077171Losses:  4.552389144897461 4.490410804748535 0.03098924644291401
CurrentTrain: epoch  3, batch    16 | loss: 4.5523891Losses:  6.012546062469482 5.941952705383301 0.03529667854309082
CurrentTrain: epoch  3, batch    17 | loss: 6.0125461Losses:  5.675564289093018 5.622143268585205 0.02671053819358349
CurrentTrain: epoch  3, batch    18 | loss: 5.6755643Losses:  7.256134986877441 7.18865442276001 0.03374028578400612
CurrentTrain: epoch  3, batch    19 | loss: 7.2561350Losses:  5.601385116577148 5.53867769241333 0.03135363385081291
CurrentTrain: epoch  3, batch    20 | loss: 5.6013851Losses:  6.1248297691345215 6.07133150100708 0.026749130338430405
CurrentTrain: epoch  3, batch    21 | loss: 6.1248298Losses:  6.737456798553467 6.676978588104248 0.03023919276893139
CurrentTrain: epoch  3, batch    22 | loss: 6.7374568Losses:  4.964888095855713 4.907646179199219 0.02862086333334446
CurrentTrain: epoch  3, batch    23 | loss: 4.9648881Losses:  5.476104736328125 5.410886764526367 0.03260888531804085
CurrentTrain: epoch  3, batch    24 | loss: 5.4761047Losses:  5.283185958862305 5.22756290435791 0.02781146951019764
CurrentTrain: epoch  3, batch    25 | loss: 5.2831860Losses:  7.25874662399292 7.194158554077148 0.032294005155563354
CurrentTrain: epoch  3, batch    26 | loss: 7.2587466Losses:  5.034514904022217 4.9747843742370605 0.029865199699997902
CurrentTrain: epoch  3, batch    27 | loss: 5.0345149Losses:  4.740591049194336 4.689121723175049 0.02573457546532154
CurrentTrain: epoch  3, batch    28 | loss: 4.7405910Losses:  4.005494117736816 3.957122802734375 0.02418561838567257
CurrentTrain: epoch  3, batch    29 | loss: 4.0054941Losses:  8.068371772766113 8.015289306640625 0.026541125029325485
CurrentTrain: epoch  3, batch    30 | loss: 8.0683718Losses:  6.731195449829102 6.656034469604492 0.03758049011230469
CurrentTrain: epoch  3, batch    31 | loss: 6.7311954Losses:  5.0981831550598145 5.050363063812256 0.023910153657197952
CurrentTrain: epoch  3, batch    32 | loss: 5.0981832Losses:  4.030438423156738 3.981386184692383 0.02452603168785572
CurrentTrain: epoch  3, batch    33 | loss: 4.0304384Losses:  5.556867599487305 5.489096641540527 0.033885493874549866
CurrentTrain: epoch  3, batch    34 | loss: 5.5568676Losses:  7.227662086486816 7.168088912963867 0.02978663519024849
CurrentTrain: epoch  3, batch    35 | loss: 7.2276621Losses:  10.106340408325195 10.049190521240234 0.028575103729963303
CurrentTrain: epoch  3, batch    36 | loss: 10.1063404Losses:  1.461571455001831 1.340455174446106 0.06055814400315285
CurrentTrain: epoch  3, batch    37 | loss: 1.4615715Losses:  5.3378119468688965 5.278416156768799 0.029697880148887634
CurrentTrain: epoch  4, batch     0 | loss: 5.3378119Losses:  5.127213478088379 5.070939064025879 0.0281370896846056
CurrentTrain: epoch  4, batch     1 | loss: 5.1272135Losses:  4.782626628875732 4.731287956237793 0.0256692785769701
CurrentTrain: epoch  4, batch     2 | loss: 4.7826266Losses:  5.568133354187012 5.5074567794799805 0.030338307842612267
CurrentTrain: epoch  4, batch     3 | loss: 5.5681334Losses:  5.573661804199219 5.505858421325684 0.03390174359083176
CurrentTrain: epoch  4, batch     4 | loss: 5.5736618Losses:  7.747656345367432 7.690229415893555 0.028713420033454895
CurrentTrain: epoch  4, batch     5 | loss: 7.7476563Losses:  5.561466217041016 5.501751899719238 0.029857216402888298
CurrentTrain: epoch  4, batch     6 | loss: 5.5614662Losses:  6.427155017852783 6.372712135314941 0.027221394702792168
CurrentTrain: epoch  4, batch     7 | loss: 6.4271550Losses:  4.990171909332275 4.93992805480957 0.025121942162513733
CurrentTrain: epoch  4, batch     8 | loss: 4.9901719Losses:  3.659202814102173 3.6110639572143555 0.02406948246061802
CurrentTrain: epoch  4, batch     9 | loss: 3.6592028Losses:  5.37249231338501 5.2990922927856445 0.03670012205839157
CurrentTrain: epoch  4, batch    10 | loss: 5.3724923Losses:  9.063021659851074 8.990964889526367 0.03602834418416023
CurrentTrain: epoch  4, batch    11 | loss: 9.0630217Losses:  5.597567081451416 5.535650730133057 0.03095809929072857
CurrentTrain: epoch  4, batch    12 | loss: 5.5975671Losses:  8.759747505187988 8.698097229003906 0.030825236812233925
CurrentTrain: epoch  4, batch    13 | loss: 8.7597475Losses:  13.16597843170166 13.101348876953125 0.032314956188201904
CurrentTrain: epoch  4, batch    14 | loss: 13.1659784Losses:  7.464881420135498 7.392790794372559 0.036045245826244354
CurrentTrain: epoch  4, batch    15 | loss: 7.4648814Losses:  4.86865758895874 4.813770771026611 0.0274432934820652
CurrentTrain: epoch  4, batch    16 | loss: 4.8686576Losses:  6.142796993255615 6.080957412719727 0.03091990016400814
CurrentTrain: epoch  4, batch    17 | loss: 6.1427970Losses:  7.676033973693848 7.610429763793945 0.03280201926827431
CurrentTrain: epoch  4, batch    18 | loss: 7.6760340Losses:  11.038690567016602 10.95106315612793 0.043813541531562805
CurrentTrain: epoch  4, batch    19 | loss: 11.0386906Losses:  5.015721797943115 4.954775810241699 0.03047291189432144
CurrentTrain: epoch  4, batch    20 | loss: 5.0157218Losses:  4.5911359786987305 4.538020610809326 0.02655772492289543
CurrentTrain: epoch  4, batch    21 | loss: 4.5911360Losses:  4.920974254608154 4.872291564941406 0.024341236799955368
CurrentTrain: epoch  4, batch    22 | loss: 4.9209743Losses:  5.971458435058594 5.92013692855835 0.025660797953605652
CurrentTrain: epoch  4, batch    23 | loss: 5.9714584Losses:  5.73342752456665 5.6783833503723145 0.027522042393684387
CurrentTrain: epoch  4, batch    24 | loss: 5.7334275Losses:  6.793262958526611 6.724336624145508 0.03446324169635773
CurrentTrain: epoch  4, batch    25 | loss: 6.7932630Losses:  4.739879608154297 4.687477111816406 0.02620127983391285
CurrentTrain: epoch  4, batch    26 | loss: 4.7398796Losses:  4.777973651885986 4.719662189483643 0.02915569581091404
CurrentTrain: epoch  4, batch    27 | loss: 4.7779737Losses:  9.61955738067627 9.5374174118042 0.041070207953453064
CurrentTrain: epoch  4, batch    28 | loss: 9.6195574Losses:  5.628175735473633 5.579174995422363 0.024500440806150436
CurrentTrain: epoch  4, batch    29 | loss: 5.6281757Losses:  8.318096160888672 8.242801666259766 0.03764725103974342
CurrentTrain: epoch  4, batch    30 | loss: 8.3180962Losses:  5.259687423706055 5.206081390380859 0.026803098618984222
CurrentTrain: epoch  4, batch    31 | loss: 5.2596874Losses:  7.082043647766113 7.0196309089660645 0.031206324696540833
CurrentTrain: epoch  4, batch    32 | loss: 7.0820436Losses:  5.752662181854248 5.697558403015137 0.027551962062716484
CurrentTrain: epoch  4, batch    33 | loss: 5.7526622Losses:  4.029376029968262 3.984753370285034 0.022311445325613022
CurrentTrain: epoch  4, batch    34 | loss: 4.0293760Losses:  6.1400251388549805 6.088504314422607 0.025760482996702194
CurrentTrain: epoch  4, batch    35 | loss: 6.1400251Losses:  4.001180648803711 3.951711416244507 0.02473468892276287
CurrentTrain: epoch  4, batch    36 | loss: 4.0011806Losses:  1.5294442176818848 1.4581687450408936 0.035637758672237396
CurrentTrain: epoch  4, batch    37 | loss: 1.5294442Losses:  5.193406105041504 5.1401519775390625 0.026626966893672943
CurrentTrain: epoch  5, batch     0 | loss: 5.1934061Losses:  8.007420539855957 7.944399356842041 0.03151080012321472
CurrentTrain: epoch  5, batch     1 | loss: 8.0074205Losses:  4.9351396560668945 4.876451015472412 0.029344290494918823
CurrentTrain: epoch  5, batch     2 | loss: 4.9351397Losses:  4.27710485458374 4.226965427398682 0.02506980113685131
CurrentTrain: epoch  5, batch     3 | loss: 4.2771049Losses:  8.060078620910645 7.971372127532959 0.044353142380714417
CurrentTrain: epoch  5, batch     4 | loss: 8.0600786Losses:  4.39722204208374 4.349206924438477 0.02400745451450348
CurrentTrain: epoch  5, batch     5 | loss: 4.3972220Losses:  6.209532737731934 6.145432949066162 0.03204980492591858
CurrentTrain: epoch  5, batch     6 | loss: 6.2095327Losses:  8.448010444641113 8.376644134521484 0.035683196038007736
CurrentTrain: epoch  5, batch     7 | loss: 8.4480104Losses:  6.140460968017578 6.071368217468262 0.034546393901109695
CurrentTrain: epoch  5, batch     8 | loss: 6.1404610Losses:  4.1420488357543945 4.092890739440918 0.02457895688712597
CurrentTrain: epoch  5, batch     9 | loss: 4.1420488Losses:  6.931060791015625 6.8537163734436035 0.03867216408252716
CurrentTrain: epoch  5, batch    10 | loss: 6.9310608Losses:  5.338293552398682 5.290920257568359 0.02368665486574173
CurrentTrain: epoch  5, batch    11 | loss: 5.3382936Losses:  6.632943153381348 6.559062480926514 0.03694026544690132
CurrentTrain: epoch  5, batch    12 | loss: 6.6329432Losses:  5.782639026641846 5.713376998901367 0.03463097661733627
CurrentTrain: epoch  5, batch    13 | loss: 5.7826390Losses:  6.815035820007324 6.7583417892456055 0.028347082436084747
CurrentTrain: epoch  5, batch    14 | loss: 6.8150358Losses:  10.009242057800293 9.948861122131348 0.03019055724143982
CurrentTrain: epoch  5, batch    15 | loss: 10.0092421Losses:  9.135551452636719 9.049083709716797 0.04323409125208855
CurrentTrain: epoch  5, batch    16 | loss: 9.1355515Losses:  5.705990314483643 5.6554765701293945 0.025256842374801636
CurrentTrain: epoch  5, batch    17 | loss: 5.7059903Losses:  5.242438316345215 5.185776233673096 0.028330976143479347
CurrentTrain: epoch  5, batch    18 | loss: 5.2424383Losses:  5.922949314117432 5.859428405761719 0.0317605622112751
CurrentTrain: epoch  5, batch    19 | loss: 5.9229493Losses:  10.730247497558594 10.608024597167969 0.06111149489879608
CurrentTrain: epoch  5, batch    20 | loss: 10.7302475Losses:  5.100888252258301 5.050619125366211 0.025134671479463577
CurrentTrain: epoch  5, batch    21 | loss: 5.1008883Losses:  4.4294538497924805 4.373262405395508 0.028095712885260582
CurrentTrain: epoch  5, batch    22 | loss: 4.4294538Losses:  5.3777923583984375 5.318564414978027 0.029613863676786423
CurrentTrain: epoch  5, batch    23 | loss: 5.3777924Losses:  4.176559925079346 4.125709533691406 0.0254251379519701
CurrentTrain: epoch  5, batch    24 | loss: 4.1765599Losses:  7.161943435668945 7.089079856872559 0.03643187880516052
CurrentTrain: epoch  5, batch    25 | loss: 7.1619434Losses:  4.897372245788574 4.844744682312012 0.02631368488073349
CurrentTrain: epoch  5, batch    26 | loss: 4.8973722Losses:  7.448119640350342 7.3967790603637695 0.025670209899544716
CurrentTrain: epoch  5, batch    27 | loss: 7.4481196Losses:  6.3633317947387695 6.3028740882873535 0.030228784307837486
CurrentTrain: epoch  5, batch    28 | loss: 6.3633318Losses:  7.381121635437012 7.329716682434082 0.025702450424432755
CurrentTrain: epoch  5, batch    29 | loss: 7.3811216Losses:  6.04008674621582 5.974176406860352 0.032955095171928406
CurrentTrain: epoch  5, batch    30 | loss: 6.0400867Losses:  7.055395603179932 6.992489814758301 0.03145298734307289
CurrentTrain: epoch  5, batch    31 | loss: 7.0553956Losses:  6.06051778793335 6.000619411468506 0.029949195683002472
CurrentTrain: epoch  5, batch    32 | loss: 6.0605178Losses:  7.21983003616333 7.1652655601501465 0.02728218026459217
CurrentTrain: epoch  5, batch    33 | loss: 7.2198300Losses:  5.721069812774658 5.670291900634766 0.02538887783885002
CurrentTrain: epoch  5, batch    34 | loss: 5.7210698Losses:  4.942867279052734 4.899320125579834 0.021773461252450943
CurrentTrain: epoch  5, batch    35 | loss: 4.9428673Losses:  9.374194145202637 9.28497314453125 0.04461054876446724
CurrentTrain: epoch  5, batch    36 | loss: 9.3741941Losses:  0.918886125087738 0.8654026985168457 0.02674170210957527
CurrentTrain: epoch  5, batch    37 | loss: 0.9188861Losses:  6.082017421722412 6.034422874450684 0.02379736304283142
CurrentTrain: epoch  6, batch     0 | loss: 6.0820174Losses:  4.973012924194336 4.921213150024414 0.025899827480316162
CurrentTrain: epoch  6, batch     1 | loss: 4.9730129Losses:  8.813628196716309 8.745776176452637 0.03392622619867325
CurrentTrain: epoch  6, batch     2 | loss: 8.8136282Losses:  6.566361427307129 6.505971431732178 0.03019501082599163
CurrentTrain: epoch  6, batch     3 | loss: 6.5663614Losses:  4.766407012939453 4.7112908363342285 0.027557993307709694
CurrentTrain: epoch  6, batch     4 | loss: 4.7664070Losses:  3.9863269329071045 3.943776845932007 0.021275043487548828
CurrentTrain: epoch  6, batch     5 | loss: 3.9863269Losses:  4.2148942947387695 4.1686015129089355 0.02314632572233677
CurrentTrain: epoch  6, batch     6 | loss: 4.2148943Losses:  4.870875835418701 4.821198463439941 0.024838769808411598
CurrentTrain: epoch  6, batch     7 | loss: 4.8708758Losses:  7.237370491027832 7.182423114776611 0.027473796159029007
CurrentTrain: epoch  6, batch     8 | loss: 7.2373705Losses:  3.591356039047241 3.5462536811828613 0.02255116030573845
CurrentTrain: epoch  6, batch     9 | loss: 3.5913560Losses:  6.604044437408447 6.548185348510742 0.027929488569498062
CurrentTrain: epoch  6, batch    10 | loss: 6.6040444Losses:  4.138967037200928 4.090740203857422 0.024113470688462257
CurrentTrain: epoch  6, batch    11 | loss: 4.1389670Losses:  7.232716083526611 7.163469314575195 0.03462343290448189
CurrentTrain: epoch  6, batch    12 | loss: 7.2327161Losses:  7.114143371582031 7.044613838195801 0.034764714539051056
CurrentTrain: epoch  6, batch    13 | loss: 7.1141434Losses:  9.052922248840332 8.99443244934082 0.029245076701045036
CurrentTrain: epoch  6, batch    14 | loss: 9.0529222Losses:  5.796585559844971 5.748893737792969 0.02384590357542038
CurrentTrain: epoch  6, batch    15 | loss: 5.7965856Losses:  6.899498462677002 6.8316779136657715 0.03391038626432419
CurrentTrain: epoch  6, batch    16 | loss: 6.8994985Losses:  3.7161896228790283 3.6729817390441895 0.021603967994451523
CurrentTrain: epoch  6, batch    17 | loss: 3.7161896Losses:  7.35294771194458 7.2832231521606445 0.034862346947193146
CurrentTrain: epoch  6, batch    18 | loss: 7.3529477Losses:  9.884529113769531 9.812515258789062 0.03600683808326721
CurrentTrain: epoch  6, batch    19 | loss: 9.8845291Losses:  5.062079429626465 5.002636909484863 0.02972114086151123
CurrentTrain: epoch  6, batch    20 | loss: 5.0620794Losses:  10.399628639221191 10.276103973388672 0.061762381345033646
CurrentTrain: epoch  6, batch    21 | loss: 10.3996286Losses:  3.9413468837738037 3.898836135864258 0.02125532180070877
CurrentTrain: epoch  6, batch    22 | loss: 3.9413469Losses:  4.080224514007568 4.036681652069092 0.02177136205136776
CurrentTrain: epoch  6, batch    23 | loss: 4.0802245Losses:  5.647209167480469 5.593262672424316 0.026973281055688858
CurrentTrain: epoch  6, batch    24 | loss: 5.6472092Losses:  6.6415581703186035 6.58329963684082 0.02912929654121399
CurrentTrain: epoch  6, batch    25 | loss: 6.6415582Losses:  5.129638195037842 5.072319030761719 0.028659556061029434
CurrentTrain: epoch  6, batch    26 | loss: 5.1296382Losses:  4.139083385467529 4.090175628662109 0.024453792721033096
CurrentTrain: epoch  6, batch    27 | loss: 4.1390834Losses:  6.231226444244385 6.187995910644531 0.021615341305732727
CurrentTrain: epoch  6, batch    28 | loss: 6.2312264Losses:  8.567950248718262 8.500678062438965 0.03363608941435814
CurrentTrain: epoch  6, batch    29 | loss: 8.5679502Losses:  7.423720359802246 7.365163326263428 0.029278535395860672
CurrentTrain: epoch  6, batch    30 | loss: 7.4237204Losses:  6.233338356018066 6.17615270614624 0.028592944145202637
CurrentTrain: epoch  6, batch    31 | loss: 6.2333384Losses:  10.072455406188965 10.004463195800781 0.033996231853961945
CurrentTrain: epoch  6, batch    32 | loss: 10.0724554Losses:  4.983814716339111 4.931013107299805 0.02640088088810444
CurrentTrain: epoch  6, batch    33 | loss: 4.9838147Losses:  5.899984836578369 5.846701145172119 0.02664182521402836
CurrentTrain: epoch  6, batch    34 | loss: 5.8999848Losses:  4.88635778427124 4.840569019317627 0.02289446070790291
CurrentTrain: epoch  6, batch    35 | loss: 4.8863578Losses:  5.398221969604492 5.340236663818359 0.028992600739002228
CurrentTrain: epoch  6, batch    36 | loss: 5.3982220Losses:  2.0301060676574707 1.9556736946105957 0.03721623867750168
CurrentTrain: epoch  6, batch    37 | loss: 2.0301061Losses:  7.2935471534729 7.215709209442139 0.038919053971767426
CurrentTrain: epoch  7, batch     0 | loss: 7.2935472Losses:  7.6811442375183105 7.6212077140808105 0.02996828407049179
CurrentTrain: epoch  7, batch     1 | loss: 7.6811442Losses:  4.7061381340026855 4.659244537353516 0.023446686565876007
CurrentTrain: epoch  7, batch     2 | loss: 4.7061381Losses:  6.4972429275512695 6.443472862243652 0.026885146275162697
CurrentTrain: epoch  7, batch     3 | loss: 6.4972429Losses:  4.1279096603393555 4.075985908508301 0.02596185728907585
CurrentTrain: epoch  7, batch     4 | loss: 4.1279097Losses:  7.1273932456970215 7.0561065673828125 0.03564333915710449
CurrentTrain: epoch  7, batch     5 | loss: 7.1273932Losses:  7.949953079223633 7.882280349731445 0.033836401998996735
CurrentTrain: epoch  7, batch     6 | loss: 7.9499531Losses:  5.795877933502197 5.72324275970459 0.036317501217126846
CurrentTrain: epoch  7, batch     7 | loss: 5.7958779Losses:  5.295512676239014 5.253314018249512 0.02109939604997635
CurrentTrain: epoch  7, batch     8 | loss: 5.2955127Losses:  6.374330997467041 6.307997703552246 0.03316672891378403
CurrentTrain: epoch  7, batch     9 | loss: 6.3743310Losses:  7.812575340270996 7.7427263259887695 0.03492439538240433
CurrentTrain: epoch  7, batch    10 | loss: 7.8125753Losses:  12.282753944396973 12.221719741821289 0.030516907572746277
CurrentTrain: epoch  7, batch    11 | loss: 12.2827539Losses:  8.415955543518066 8.356781005859375 0.02958712913095951
CurrentTrain: epoch  7, batch    12 | loss: 8.4159555Losses:  4.119691371917725 4.077079772949219 0.021305853500962257
CurrentTrain: epoch  7, batch    13 | loss: 4.1196914Losses:  7.407212257385254 7.345336437225342 0.03093787282705307
CurrentTrain: epoch  7, batch    14 | loss: 7.4072123Losses:  5.780617713928223 5.7297821044921875 0.025417815893888474
CurrentTrain: epoch  7, batch    15 | loss: 5.7806177Losses:  9.011869430541992 8.935035705566406 0.03841672092676163
CurrentTrain: epoch  7, batch    16 | loss: 9.0118694Losses:  5.868438720703125 5.810724258422852 0.02885727770626545
CurrentTrain: epoch  7, batch    17 | loss: 5.8684387Losses:  4.359864711761475 4.313847541809082 0.02300863526761532
CurrentTrain: epoch  7, batch    18 | loss: 4.3598647Losses:  5.022763252258301 4.971238136291504 0.02576257474720478
CurrentTrain: epoch  7, batch    19 | loss: 5.0227633Losses:  4.1462273597717285 4.10360860824585 0.021309353411197662
CurrentTrain: epoch  7, batch    20 | loss: 4.1462274Losses:  7.3196330070495605 7.266345024108887 0.026643972843885422
CurrentTrain: epoch  7, batch    21 | loss: 7.3196330Losses:  6.524669647216797 6.474974155426025 0.02484772354364395
CurrentTrain: epoch  7, batch    22 | loss: 6.5246696Losses:  8.898051261901855 8.844188690185547 0.02693144790828228
CurrentTrain: epoch  7, batch    23 | loss: 8.8980513Losses:  5.568806171417236 5.509193420410156 0.02980630472302437
CurrentTrain: epoch  7, batch    24 | loss: 5.5688062Losses:  6.549075126647949 6.473848819732666 0.03761303797364235
CurrentTrain: epoch  7, batch    25 | loss: 6.5490751Losses:  10.397808074951172 10.32948112487793 0.03416329249739647
CurrentTrain: epoch  7, batch    26 | loss: 10.3978081Losses:  4.382934093475342 4.335088729858398 0.02392260730266571
CurrentTrain: epoch  7, batch    27 | loss: 4.3829341Losses:  8.067211151123047 8.005640983581543 0.030784951522946358
CurrentTrain: epoch  7, batch    28 | loss: 8.0672112Losses:  7.39595365524292 7.324008464813232 0.03597256913781166
CurrentTrain: epoch  7, batch    29 | loss: 7.3959537Losses:  4.7031354904174805 4.654653549194336 0.024240944534540176
CurrentTrain: epoch  7, batch    30 | loss: 4.7031355Losses:  5.34334135055542 5.278202056884766 0.03256953880190849
CurrentTrain: epoch  7, batch    31 | loss: 5.3433414Losses:  6.690779685974121 6.6266703605651855 0.032054655253887177
CurrentTrain: epoch  7, batch    32 | loss: 6.6907797Losses:  8.965532302856445 8.919760704040527 0.022885723039507866
CurrentTrain: epoch  7, batch    33 | loss: 8.9655323Losses:  5.562106132507324 5.502269268035889 0.029918421059846878
CurrentTrain: epoch  7, batch    34 | loss: 5.5621061Losses:  5.52793550491333 5.481309413909912 0.023313114419579506
CurrentTrain: epoch  7, batch    35 | loss: 5.5279355Losses:  5.898996353149414 5.8373870849609375 0.030804717913269997
CurrentTrain: epoch  7, batch    36 | loss: 5.8989964Losses:  1.15650475025177 1.0974208116531372 0.029541991651058197
CurrentTrain: epoch  7, batch    37 | loss: 1.1565048Losses:  5.457906723022461 5.392472267150879 0.03271733969449997
CurrentTrain: epoch  8, batch     0 | loss: 5.4579067Losses:  6.798629283905029 6.7356719970703125 0.0314786322414875
CurrentTrain: epoch  8, batch     1 | loss: 6.7986293Losses:  7.527920246124268 7.445503234863281 0.04120846465229988
CurrentTrain: epoch  8, batch     2 | loss: 7.5279202Losses:  7.47418737411499 7.432328224182129 0.020929669961333275
CurrentTrain: epoch  8, batch     3 | loss: 7.4741874Losses:  5.785313129425049 5.729318141937256 0.027997534722089767
CurrentTrain: epoch  8, batch     4 | loss: 5.7853131Losses:  5.438326358795166 5.375710487365723 0.0313078835606575
CurrentTrain: epoch  8, batch     5 | loss: 5.4383264Losses:  11.310135841369629 11.218141555786133 0.04599718004465103
CurrentTrain: epoch  8, batch     6 | loss: 11.3101358Losses:  5.684207439422607 5.622780799865723 0.0307133961468935
CurrentTrain: epoch  8, batch     7 | loss: 5.6842074Losses:  5.541197776794434 5.484170913696289 0.02851346880197525
CurrentTrain: epoch  8, batch     8 | loss: 5.5411978Losses:  5.522346019744873 5.47727632522583 0.022534843534231186
CurrentTrain: epoch  8, batch     9 | loss: 5.5223460Losses:  6.403815746307373 6.3296051025390625 0.03710522502660751
CurrentTrain: epoch  8, batch    10 | loss: 6.4038157Losses:  6.934769153594971 6.8646392822265625 0.035064831376075745
CurrentTrain: epoch  8, batch    11 | loss: 6.9347692Losses:  4.1775431632995605 4.130495071411133 0.023524120450019836
CurrentTrain: epoch  8, batch    12 | loss: 4.1775432Losses:  4.208271503448486 4.155340194702148 0.026465605944395065
CurrentTrain: epoch  8, batch    13 | loss: 4.2082715Losses:  4.500094413757324 4.450440406799316 0.024826908484101295
CurrentTrain: epoch  8, batch    14 | loss: 4.5000944Losses:  5.362945079803467 5.302689552307129 0.030127808451652527
CurrentTrain: epoch  8, batch    15 | loss: 5.3629451Losses:  3.4922432899475098 3.4479284286499023 0.022157372906804085
CurrentTrain: epoch  8, batch    16 | loss: 3.4922433Losses:  4.986377716064453 4.920290946960449 0.03304344415664673
CurrentTrain: epoch  8, batch    17 | loss: 4.9863777Losses:  7.903350830078125 7.831043243408203 0.03615390509366989
CurrentTrain: epoch  8, batch    18 | loss: 7.9033508Losses:  4.807601451873779 4.756163120269775 0.025719238445162773
CurrentTrain: epoch  8, batch    19 | loss: 4.8076015Losses:  4.327849388122559 4.273002624511719 0.02742346003651619
CurrentTrain: epoch  8, batch    20 | loss: 4.3278494Losses:  5.079299449920654 5.022883415222168 0.028208019211888313
CurrentTrain: epoch  8, batch    21 | loss: 5.0792994Losses:  6.002967357635498 5.947193145751953 0.027887174859642982
CurrentTrain: epoch  8, batch    22 | loss: 6.0029674Losses:  5.805747032165527 5.7499003410339355 0.027923228219151497
CurrentTrain: epoch  8, batch    23 | loss: 5.8057470Losses:  4.080104351043701 4.036531448364258 0.021786408498883247
CurrentTrain: epoch  8, batch    24 | loss: 4.0801044Losses:  3.9102048873901367 3.8655667304992676 0.022319022566080093
CurrentTrain: epoch  8, batch    25 | loss: 3.9102049Losses:  4.742562770843506 4.68763542175293 0.02746359072625637
CurrentTrain: epoch  8, batch    26 | loss: 4.7425628Losses:  5.906041622161865 5.8440327644348145 0.031004328280687332
CurrentTrain: epoch  8, batch    27 | loss: 5.9060416Losses:  7.753695964813232 7.672979354858398 0.040358249098062515
CurrentTrain: epoch  8, batch    28 | loss: 7.7536960Losses:  7.252846717834473 7.182192802429199 0.035326942801475525
CurrentTrain: epoch  8, batch    29 | loss: 7.2528467Losses:  3.475573778152466 3.4292752742767334 0.023149237036705017
CurrentTrain: epoch  8, batch    30 | loss: 3.4755738Losses:  5.390475749969482 5.334503173828125 0.027986347675323486
CurrentTrain: epoch  8, batch    31 | loss: 5.3904757Losses:  4.169666767120361 4.118990898132324 0.025337856262922287
CurrentTrain: epoch  8, batch    32 | loss: 4.1696668Losses:  6.75142240524292 6.689840316772461 0.030791079625487328
CurrentTrain: epoch  8, batch    33 | loss: 6.7514224Losses:  5.923746109008789 5.853261470794678 0.03524230048060417
CurrentTrain: epoch  8, batch    34 | loss: 5.9237461Losses:  6.750001907348633 6.694999694824219 0.027501225471496582
CurrentTrain: epoch  8, batch    35 | loss: 6.7500019Losses:  9.7764310836792 9.639481544494629 0.06847488880157471
CurrentTrain: epoch  8, batch    36 | loss: 9.7764311Losses:  0.7259884476661682 0.6547136306762695 0.03563741222023964
CurrentTrain: epoch  8, batch    37 | loss: 0.7259884Losses:  7.571572303771973 7.515467166900635 0.028052594512701035
CurrentTrain: epoch  9, batch     0 | loss: 7.5715723Losses:  7.144775390625 7.079776287078857 0.032499611377716064
CurrentTrain: epoch  9, batch     1 | loss: 7.1447754Losses:  3.518423557281494 3.4740827083587646 0.022170431911945343
CurrentTrain: epoch  9, batch     2 | loss: 3.5184236Losses:  6.939458847045898 6.874361991882324 0.032548535615205765
CurrentTrain: epoch  9, batch     3 | loss: 6.9394588Losses:  3.7840828895568848 3.7415995597839355 0.02124166674911976
CurrentTrain: epoch  9, batch     4 | loss: 3.7840829Losses:  4.065009593963623 4.0189924240112305 0.023008517920970917
CurrentTrain: epoch  9, batch     5 | loss: 4.0650096Losses:  8.551729202270508 8.4443941116333 0.053667716681957245
CurrentTrain: epoch  9, batch     6 | loss: 8.5517292Losses:  5.410787105560303 5.360079288482666 0.025353895500302315
CurrentTrain: epoch  9, batch     7 | loss: 5.4107871Losses:  4.800303936004639 4.753781318664551 0.023261286318302155
CurrentTrain: epoch  9, batch     8 | loss: 4.8003039Losses:  9.854949951171875 9.778753280639648 0.03809850662946701
CurrentTrain: epoch  9, batch     9 | loss: 9.8549500Losses:  3.7375571727752686 3.6936726570129395 0.02194228768348694
CurrentTrain: epoch  9, batch    10 | loss: 3.7375572Losses:  9.13556957244873 9.056890487670898 0.03933970630168915
CurrentTrain: epoch  9, batch    11 | loss: 9.1355696Losses:  4.800418376922607 4.752631187438965 0.023893509060144424
CurrentTrain: epoch  9, batch    12 | loss: 4.8004184Losses:  5.823501110076904 5.762533664703369 0.030483780428767204
CurrentTrain: epoch  9, batch    13 | loss: 5.8235011Losses:  3.6617519855499268 3.6165823936462402 0.022584842517971992
CurrentTrain: epoch  9, batch    14 | loss: 3.6617520Losses:  4.746235370635986 4.685911178588867 0.030162161216139793
CurrentTrain: epoch  9, batch    15 | loss: 4.7462354Losses:  6.68739128112793 6.630520820617676 0.02843526192009449
CurrentTrain: epoch  9, batch    16 | loss: 6.6873913Losses:  6.785731792449951 6.721883296966553 0.03192418813705444
CurrentTrain: epoch  9, batch    17 | loss: 6.7857318Losses:  8.725542068481445 8.652302742004395 0.03661973774433136
CurrentTrain: epoch  9, batch    18 | loss: 8.7255421Losses:  7.8943586349487305 7.82305908203125 0.035649776458740234
CurrentTrain: epoch  9, batch    19 | loss: 7.8943586Losses:  7.216617584228516 7.154443740844727 0.031086841598153114
CurrentTrain: epoch  9, batch    20 | loss: 7.2166176Losses:  4.4046101570129395 4.356626510620117 0.023991741240024567
CurrentTrain: epoch  9, batch    21 | loss: 4.4046102Losses:  11.839316368103027 11.769556045532227 0.03488019108772278
CurrentTrain: epoch  9, batch    22 | loss: 11.8393164Losses:  7.369703769683838 7.283447742462158 0.04312802478671074
CurrentTrain: epoch  9, batch    23 | loss: 7.3697038Losses:  7.7465500831604 7.6721110343933105 0.03721962869167328
CurrentTrain: epoch  9, batch    24 | loss: 7.7465501Losses:  6.475000381469727 6.4179229736328125 0.02853860892355442
CurrentTrain: epoch  9, batch    25 | loss: 6.4750004Losses:  4.034632682800293 3.9889564514160156 0.022838067263364792
CurrentTrain: epoch  9, batch    26 | loss: 4.0346327Losses:  4.78356409072876 4.724184513092041 0.02968989685177803
CurrentTrain: epoch  9, batch    27 | loss: 4.7835641Losses:  7.1397318840026855 7.062355995178223 0.03868798911571503
CurrentTrain: epoch  9, batch    28 | loss: 7.1397319Losses:  7.181089878082275 7.11287260055542 0.034108567982912064
CurrentTrain: epoch  9, batch    29 | loss: 7.1810899Losses:  5.131736755371094 5.079747200012207 0.025994785130023956
CurrentTrain: epoch  9, batch    30 | loss: 5.1317368Losses:  6.486764430999756 6.411976337432861 0.03739401698112488
CurrentTrain: epoch  9, batch    31 | loss: 6.4867644Losses:  5.556161880493164 5.487767219543457 0.03419733792543411
CurrentTrain: epoch  9, batch    32 | loss: 5.5561619Losses:  7.017706871032715 6.943758487701416 0.036974214017391205
CurrentTrain: epoch  9, batch    33 | loss: 7.0177069Losses:  8.806854248046875 8.712602615356445 0.047125935554504395
CurrentTrain: epoch  9, batch    34 | loss: 8.8068542Losses:  4.051594257354736 4.004307746887207 0.02364322915673256
CurrentTrain: epoch  9, batch    35 | loss: 4.0515943Losses:  6.837802886962891 6.780327796936035 0.02873762883245945
CurrentTrain: epoch  9, batch    36 | loss: 6.8378029Losses:  2.1763646602630615 2.094775676727295 0.04079454764723778
CurrentTrain: epoch  9, batch    37 | loss: 2.1763647
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 86.33%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 85.66%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 84.38%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 83.88%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 85.12%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.80%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 86.41%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.98%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.98%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 88.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.62%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 89.01%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 88.75%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 88.91%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 87.50%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 86.33%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 85.66%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 84.38%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 83.88%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 85.12%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.80%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 86.41%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.98%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.98%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 88.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.62%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 89.01%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 88.75%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 88.91%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 87.50%   
cur_acc:  ['0.8750']
his_acc:  ['0.8750']
Clustering into  4  clusters
Clusters:  [0 2 2 0 0 3 0 1 0 0 0]
Losses:  9.250826835632324 8.667070388793945 0.29187828302383423
CurrentTrain: epoch  0, batch     0 | loss: 9.2508268Losses:  4.89427375793457 3.9941864013671875 0.45004376769065857
CurrentTrain: epoch  0, batch     1 | loss: 4.8942738Losses:  8.348655700683594 7.845820903778076 0.2514176368713379
CurrentTrain: epoch  1, batch     0 | loss: 8.3486557Losses:  4.1863861083984375 3.4096717834472656 0.3883572816848755
CurrentTrain: epoch  1, batch     1 | loss: 4.1863861Losses:  9.920209884643555 9.57261848449707 0.17379546165466309
CurrentTrain: epoch  2, batch     0 | loss: 9.9202099Losses:  4.5840744972229 4.5840744972229 -0.0
CurrentTrain: epoch  2, batch     1 | loss: 4.5840745Losses:  6.9093828201293945 6.719064712524414 0.09515917301177979
CurrentTrain: epoch  3, batch     0 | loss: 6.9093828Losses:  2.4358561038970947 2.23453950881958 0.1006583422422409
CurrentTrain: epoch  3, batch     1 | loss: 2.4358561Losses:  6.371628284454346 6.208779335021973 0.08142448961734772
CurrentTrain: epoch  4, batch     0 | loss: 6.3716283Losses:  1.6471889019012451 1.4946019649505615 0.0762934535741806
CurrentTrain: epoch  4, batch     1 | loss: 1.6471889Losses:  6.249085426330566 6.083179473876953 0.08295289427042007
CurrentTrain: epoch  5, batch     0 | loss: 6.2490854Losses:  1.477583885192871 1.3499456644058228 0.06381909549236298
CurrentTrain: epoch  5, batch     1 | loss: 1.4775839Losses:  6.510261058807373 6.35743522644043 0.07641295343637466
CurrentTrain: epoch  6, batch     0 | loss: 6.5102611Losses:  2.0539135932922363 1.9446735382080078 0.05462002381682396
CurrentTrain: epoch  6, batch     1 | loss: 2.0539136Losses:  6.973489761352539 6.881845474243164 0.045822061598300934
CurrentTrain: epoch  7, batch     0 | loss: 6.9734898Losses:  2.943450689315796 2.7444374561309814 0.09950660169124603
CurrentTrain: epoch  7, batch     1 | loss: 2.9434507Losses:  7.375537395477295 7.261014461517334 0.05726136639714241
CurrentTrain: epoch  8, batch     0 | loss: 7.3755374Losses:  4.117432117462158 4.117432117462158 -0.0
CurrentTrain: epoch  8, batch     1 | loss: 4.1174321Losses:  6.175429344177246 6.075057506561279 0.05018593743443489
CurrentTrain: epoch  9, batch     0 | loss: 6.1754293Losses:  1.7933939695358276 1.6947863101959229 0.04930383712053299
CurrentTrain: epoch  9, batch     1 | loss: 1.7933940
Losses:  0.7293134927749634 -0.0 0.3646567463874817
MemoryTrain:  epoch  0, batch     0 | loss: 0.7293135Losses:  0.6622533798217773 -0.0 0.33112668991088867
MemoryTrain:  epoch  1, batch     0 | loss: 0.6622534Losses:  0.6306401491165161 -0.0 0.31532007455825806
MemoryTrain:  epoch  2, batch     0 | loss: 0.6306401Losses:  0.6108850836753845 -0.0 0.30544254183769226
MemoryTrain:  epoch  3, batch     0 | loss: 0.6108851Losses:  0.5782533288002014 -0.0 0.2891266644001007
MemoryTrain:  epoch  4, batch     0 | loss: 0.5782533Losses:  0.5713125467300415 -0.0 0.28565627336502075
MemoryTrain:  epoch  5, batch     0 | loss: 0.5713125Losses:  0.5385574102401733 -0.0 0.26927870512008667
MemoryTrain:  epoch  6, batch     0 | loss: 0.5385574Losses:  0.5267407298088074 -0.0 0.2633703649044037
MemoryTrain:  epoch  7, batch     0 | loss: 0.5267407Losses:  0.5071133971214294 -0.0 0.2535566985607147
MemoryTrain:  epoch  8, batch     0 | loss: 0.5071134Losses:  0.49498915672302246 -0.0 0.24749457836151123
MemoryTrain:  epoch  9, batch     0 | loss: 0.4949892Losses:  0.4795151650905609 -0.0 0.23975758254528046
MemoryTrain:  epoch 10, batch     0 | loss: 0.4795152Losses:  0.4671393632888794 -0.0 0.2335696816444397
MemoryTrain:  epoch 11, batch     0 | loss: 0.4671394Losses:  0.4558991491794586 -0.0 0.2279495745897293
MemoryTrain:  epoch 12, batch     0 | loss: 0.4558991Losses:  0.4581446647644043 -0.0 0.22907233238220215
MemoryTrain:  epoch 13, batch     0 | loss: 0.4581447Losses:  0.4508928656578064 -0.0 0.2254464328289032
MemoryTrain:  epoch 14, batch     0 | loss: 0.4508929Losses:  0.45133817195892334 -0.0 0.22566908597946167
MemoryTrain:  epoch 15, batch     0 | loss: 0.4513382Losses:  0.45191824436187744 -0.0 0.22595912218093872
MemoryTrain:  epoch 16, batch     0 | loss: 0.4519182Losses:  0.4334445297718048 -0.0 0.2167222648859024
MemoryTrain:  epoch 17, batch     0 | loss: 0.4334445Losses:  0.4309729337692261 -0.0 0.21548646688461304
MemoryTrain:  epoch 18, batch     0 | loss: 0.4309729Losses:  0.42632317543029785 -0.0 0.21316158771514893
MemoryTrain:  epoch 19, batch     0 | loss: 0.4263232
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 81.82%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 80.21%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 78.85%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 76.95%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 76.84%   [EVAL] batch:   17 | acc: 18.75%,  total acc: 73.61%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.36%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 86.98%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 86.16%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.29%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.44%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.24%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.97%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.64%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.82%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.07%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 86.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.88%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 88.05%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 88.04%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 88.02%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 88.18%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 88.16%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 87.82%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.12%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 86.89%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 86.46%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 86.19%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 85.65%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 85.14%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 84.92%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 84.71%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 83.98%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 84.06%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 83.25%   
cur_acc:  ['0.8750', '0.7361']
his_acc:  ['0.8750', '0.8325']
Clustering into  7  clusters
Clusters:  [1 4 4 1 0 3 1 5 1 2 0 6 1 2 4 1]
Losses:  10.061071395874023 9.147316932678223 0.4568772614002228
CurrentTrain: epoch  0, batch     0 | loss: 10.0610714Losses:  3.0875766277313232 2.8104774951934814 0.13854952156543732
CurrentTrain: epoch  0, batch     1 | loss: 3.0875766Losses:  9.201648712158203 8.401893615722656 0.3998776078224182
CurrentTrain: epoch  1, batch     0 | loss: 9.2016487Losses:  2.8957877159118652 2.2739169597625732 0.3109353184700012
CurrentTrain: epoch  1, batch     1 | loss: 2.8957877Losses:  10.607460021972656 9.631758689880371 0.4878508448600769
CurrentTrain: epoch  2, batch     0 | loss: 10.6074600Losses:  4.570106506347656 4.037580966949463 0.2662627100944519
CurrentTrain: epoch  2, batch     1 | loss: 4.5701065Losses:  6.942945957183838 6.362449645996094 0.29024815559387207
CurrentTrain: epoch  3, batch     0 | loss: 6.9429460Losses:  2.7330431938171387 2.1890206336975098 0.27201128005981445
CurrentTrain: epoch  3, batch     1 | loss: 2.7330432Losses:  7.670908451080322 7.117199897766113 0.2768542766571045
CurrentTrain: epoch  4, batch     0 | loss: 7.6709085Losses:  2.5690619945526123 2.3096158504486084 0.12972304224967957
CurrentTrain: epoch  4, batch     1 | loss: 2.5690620Losses:  8.579230308532715 8.029661178588867 0.2747846245765686
CurrentTrain: epoch  5, batch     0 | loss: 8.5792303Losses:  3.3632802963256836 3.058293342590332 0.15249350666999817
CurrentTrain: epoch  5, batch     1 | loss: 3.3632803Losses:  7.310340881347656 6.800485610961914 0.2549275755882263
CurrentTrain: epoch  6, batch     0 | loss: 7.3103409Losses:  3.2393605709075928 2.9154164791107178 0.16197209060192108
CurrentTrain: epoch  6, batch     1 | loss: 3.2393606Losses:  6.330628395080566 5.851665496826172 0.23948156833648682
CurrentTrain: epoch  7, batch     0 | loss: 6.3306284Losses:  2.095005512237549 1.6286308765411377 0.23318733274936676
CurrentTrain: epoch  7, batch     1 | loss: 2.0950055Losses:  6.588301658630371 6.121737003326416 0.23328237235546112
CurrentTrain: epoch  8, batch     0 | loss: 6.5883017Losses:  2.7182672023773193 2.2538537979125977 0.23220674693584442
CurrentTrain: epoch  8, batch     1 | loss: 2.7182672Losses:  6.330184459686279 5.890917778015137 0.2196333408355713
CurrentTrain: epoch  9, batch     0 | loss: 6.3301845Losses:  1.9501137733459473 1.4682588577270508 0.24092742800712585
CurrentTrain: epoch  9, batch     1 | loss: 1.9501138
Losses:  1.1509337425231934 -0.0 0.5754668712615967
MemoryTrain:  epoch  0, batch     0 | loss: 1.1509337Losses:  1.0866115093231201 -0.0 0.5433057546615601
MemoryTrain:  epoch  1, batch     0 | loss: 1.0866115Losses:  1.066319227218628 -0.0 0.533159613609314
MemoryTrain:  epoch  2, batch     0 | loss: 1.0663192Losses:  1.0469425916671753 -0.0 0.5234712958335876
MemoryTrain:  epoch  3, batch     0 | loss: 1.0469426Losses:  1.0372540950775146 -0.0 0.5186270475387573
MemoryTrain:  epoch  4, batch     0 | loss: 1.0372541Losses:  1.0227317810058594 -0.0 0.5113658905029297
MemoryTrain:  epoch  5, batch     0 | loss: 1.0227318Losses:  1.0257246494293213 -0.0 0.5128623247146606
MemoryTrain:  epoch  6, batch     0 | loss: 1.0257246Losses:  1.0153234004974365 -0.0 0.5076617002487183
MemoryTrain:  epoch  7, batch     0 | loss: 1.0153234Losses:  0.9958526492118835 -0.0 0.4979263246059418
MemoryTrain:  epoch  8, batch     0 | loss: 0.9958526Losses:  0.9945327639579773 -0.0 0.49726638197898865
MemoryTrain:  epoch  9, batch     0 | loss: 0.9945328Losses:  0.9964118599891663 -0.0 0.49820592999458313
MemoryTrain:  epoch 10, batch     0 | loss: 0.9964119Losses:  0.9906085133552551 -0.0 0.49530425667762756
MemoryTrain:  epoch 11, batch     0 | loss: 0.9906085Losses:  0.9887154698371887 -0.0 0.49435773491859436
MemoryTrain:  epoch 12, batch     0 | loss: 0.9887155Losses:  0.9811246991157532 -0.0 0.4905623495578766
MemoryTrain:  epoch 13, batch     0 | loss: 0.9811247Losses:  0.9839631915092468 -0.0 0.4919815957546234
MemoryTrain:  epoch 14, batch     0 | loss: 0.9839632Losses:  0.9787855744361877 -0.0 0.48939278721809387
MemoryTrain:  epoch 15, batch     0 | loss: 0.9787856Losses:  0.9775100350379944 -0.0 0.4887550175189972
MemoryTrain:  epoch 16, batch     0 | loss: 0.9775100Losses:  0.9747376441955566 -0.0 0.4873688220977783
MemoryTrain:  epoch 17, batch     0 | loss: 0.9747376Losses:  0.9680695533752441 -0.0 0.48403477668762207
MemoryTrain:  epoch 18, batch     0 | loss: 0.9680696Losses:  0.9680993556976318 -0.0 0.4840496778488159
MemoryTrain:  epoch 19, batch     0 | loss: 0.9680994
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 47.92%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 43.75%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 38.54%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 40.18%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 46.88%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 52.78%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 59.09%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 60.94%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 63.46%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 61.61%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 60.00%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 60.42%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 64.29%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 76.70%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 76.44%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 73.21%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 73.75%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 73.05%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 73.16%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 74.01%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 76.19%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 77.27%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 78.26%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.77%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.92%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 82.54%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 82.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.47%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 83.79%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 83.90%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 83.82%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 83.57%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 83.16%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 82.94%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 82.57%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 81.73%   [EVAL] batch:   39 | acc: 0.00%,  total acc: 79.69%   [EVAL] batch:   40 | acc: 0.00%,  total acc: 77.74%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 77.53%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 77.33%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 77.41%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 77.92%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 78.59%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 79.04%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 79.21%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 79.50%   [EVAL] batch:   50 | acc: 31.25%,  total acc: 78.55%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 78.00%   [EVAL] batch:   52 | acc: 31.25%,  total acc: 77.12%   [EVAL] batch:   53 | acc: 37.50%,  total acc: 76.39%   [EVAL] batch:   54 | acc: 31.25%,  total acc: 75.57%   [EVAL] batch:   55 | acc: 18.75%,  total acc: 74.55%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 74.67%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 75.32%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 75.62%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 75.72%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 75.91%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 75.99%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 75.00%   
cur_acc:  ['0.8750', '0.7361', '0.6161']
his_acc:  ['0.8750', '0.8325', '0.7500']
Clustering into  9  clusters
Clusters:  [3 1 1 3 0 5 3 6 3 7 0 8 3 7 1 3 4 2 3 1 0]
Losses:  8.271174430847168 7.382994174957275 0.4440901279449463
CurrentTrain: epoch  0, batch     0 | loss: 8.2711744Losses:  2.7834692001342773 2.115457773208618 0.33400577306747437
CurrentTrain: epoch  0, batch     1 | loss: 2.7834692Losses:  6.778191089630127 5.9420318603515625 0.4180797040462494
CurrentTrain: epoch  1, batch     0 | loss: 6.7781911Losses:  2.0413506031036377 1.2393156290054321 0.4010174870491028
CurrentTrain: epoch  1, batch     1 | loss: 2.0413506Losses:  7.265541076660156 6.512418746948242 0.3765612542629242
CurrentTrain: epoch  2, batch     0 | loss: 7.2655411Losses:  2.411025285720825 1.6505988836288452 0.38021320104599
CurrentTrain: epoch  2, batch     1 | loss: 2.4110253Losses:  6.518636703491211 5.7873945236206055 0.3656211793422699
CurrentTrain: epoch  3, batch     0 | loss: 6.5186367Losses:  2.23537540435791 1.4773545265197754 0.37901049852371216
CurrentTrain: epoch  3, batch     1 | loss: 2.2353754Losses:  6.26841926574707 5.5488080978393555 0.35980555415153503
CurrentTrain: epoch  4, batch     0 | loss: 6.2684193Losses:  2.2492265701293945 1.5342389345169067 0.3574938178062439
CurrentTrain: epoch  4, batch     1 | loss: 2.2492266Losses:  6.512753486633301 5.82801628112793 0.3423686921596527
CurrentTrain: epoch  5, batch     0 | loss: 6.5127535Losses:  2.274224281311035 1.743378758430481 0.2654227614402771
CurrentTrain: epoch  5, batch     1 | loss: 2.2742243Losses:  6.805070400238037 6.123047828674316 0.34101125597953796
CurrentTrain: epoch  6, batch     0 | loss: 6.8050704Losses:  2.94150972366333 2.4239418506622314 0.25878387689590454
CurrentTrain: epoch  6, batch     1 | loss: 2.9415097Losses:  6.674163818359375 5.997120380401611 0.3385217487812042
CurrentTrain: epoch  7, batch     0 | loss: 6.6741638Losses:  2.8902876377105713 2.347377061843872 0.2714552581310272
CurrentTrain: epoch  7, batch     1 | loss: 2.8902876Losses:  6.916721820831299 6.260918617248535 0.32790160179138184
CurrentTrain: epoch  8, batch     0 | loss: 6.9167218Losses:  2.862128734588623 2.371819257736206 0.24515467882156372
CurrentTrain: epoch  8, batch     1 | loss: 2.8621287Losses:  5.329559326171875 4.684218406677246 0.32267045974731445
CurrentTrain: epoch  9, batch     0 | loss: 5.3295593Losses:  1.5030310153961182 0.8628631234169006 0.32008397579193115
CurrentTrain: epoch  9, batch     1 | loss: 1.5030310
Losses:  1.3284235000610352 -0.0 0.6642117500305176
MemoryTrain:  epoch  0, batch     0 | loss: 1.3284235Losses:  0.3829275071620941 -0.0 0.19146375358104706
MemoryTrain:  epoch  0, batch     1 | loss: 0.3829275Losses:  1.3652114868164062 -0.0 0.6826057434082031
MemoryTrain:  epoch  1, batch     0 | loss: 1.3652115Losses:  0.21829164028167725 -0.0 0.10914582014083862
MemoryTrain:  epoch  1, batch     1 | loss: 0.2182916Losses:  1.1053799390792847 -0.0 0.5526899695396423
MemoryTrain:  epoch  2, batch     0 | loss: 1.1053799Losses:  0.5746738910675049 -0.0 0.28733694553375244
MemoryTrain:  epoch  2, batch     1 | loss: 0.5746739Losses:  1.177685022354126 -0.0 0.588842511177063
MemoryTrain:  epoch  3, batch     0 | loss: 1.1776850Losses:  0.4889114201068878 -0.0 0.2444557100534439
MemoryTrain:  epoch  3, batch     1 | loss: 0.4889114Losses:  1.2287890911102295 -0.0 0.6143945455551147
MemoryTrain:  epoch  4, batch     0 | loss: 1.2287891Losses:  0.2886927127838135 -0.0 0.14434635639190674
MemoryTrain:  epoch  4, batch     1 | loss: 0.2886927Losses:  1.1773529052734375 -0.0 0.5886764526367188
MemoryTrain:  epoch  5, batch     0 | loss: 1.1773529Losses:  0.6663938760757446 -0.0 0.3331969380378723
MemoryTrain:  epoch  5, batch     1 | loss: 0.6663939Losses:  1.1905934810638428 -0.0 0.5952967405319214
MemoryTrain:  epoch  6, batch     0 | loss: 1.1905935Losses:  0.3132239282131195 -0.0 0.15661196410655975
MemoryTrain:  epoch  6, batch     1 | loss: 0.3132239Losses:  1.0160398483276367 -0.0 0.5080199241638184
MemoryTrain:  epoch  7, batch     0 | loss: 1.0160398Losses:  0.6850345134735107 -0.0 0.34251725673675537
MemoryTrain:  epoch  7, batch     1 | loss: 0.6850345Losses:  1.1724720001220703 -0.0 0.5862360000610352
MemoryTrain:  epoch  8, batch     0 | loss: 1.1724720Losses:  1.0267499685287476 -0.0 0.5133749842643738
MemoryTrain:  epoch  8, batch     1 | loss: 1.0267500Losses:  1.2352012395858765 -0.0 0.6176006197929382
MemoryTrain:  epoch  9, batch     0 | loss: 1.2352012Losses:  0.5595661401748657 -0.0 0.27978307008743286
MemoryTrain:  epoch  9, batch     1 | loss: 0.5595661Losses:  1.3491219282150269 -0.0 0.6745609641075134
MemoryTrain:  epoch 10, batch     0 | loss: 1.3491219Losses:  0.3264259994029999 -0.0 0.16321299970149994
MemoryTrain:  epoch 10, batch     1 | loss: 0.3264260Losses:  1.0288662910461426 -0.0 0.5144331455230713
MemoryTrain:  epoch 11, batch     0 | loss: 1.0288663Losses:  0.42349883913993835 -0.0 0.21174941956996918
MemoryTrain:  epoch 11, batch     1 | loss: 0.4234988Losses:  1.1617220640182495 -0.0 0.5808610320091248
MemoryTrain:  epoch 12, batch     0 | loss: 1.1617221Losses:  0.4911327660083771 -0.0 0.24556638300418854
MemoryTrain:  epoch 12, batch     1 | loss: 0.4911328Losses:  1.2388346195220947 -0.0 0.6194173097610474
MemoryTrain:  epoch 13, batch     0 | loss: 1.2388346Losses:  0.5274133682250977 -0.0 0.26370668411254883
MemoryTrain:  epoch 13, batch     1 | loss: 0.5274134Losses:  1.3673138618469238 -0.0 0.6836569309234619
MemoryTrain:  epoch 14, batch     0 | loss: 1.3673139Losses:  0.46089062094688416 -0.0 0.23044531047344208
MemoryTrain:  epoch 14, batch     1 | loss: 0.4608906Losses:  1.2020039558410645 -0.0 0.6010019779205322
MemoryTrain:  epoch 15, batch     0 | loss: 1.2020040Losses:  0.32018113136291504 -0.0 0.16009056568145752
MemoryTrain:  epoch 15, batch     1 | loss: 0.3201811Losses:  1.085301160812378 -0.0 0.542650580406189
MemoryTrain:  epoch 16, batch     0 | loss: 1.0853012Losses:  0.6320267915725708 -0.0 0.3160133957862854
MemoryTrain:  epoch 16, batch     1 | loss: 0.6320268Losses:  1.1740317344665527 -0.0 0.5870158672332764
MemoryTrain:  epoch 17, batch     0 | loss: 1.1740317Losses:  0.36648041009902954 -0.0 0.18324020504951477
MemoryTrain:  epoch 17, batch     1 | loss: 0.3664804Losses:  1.2138704061508179 -0.0 0.6069352030754089
MemoryTrain:  epoch 18, batch     0 | loss: 1.2138704Losses:  0.4151410460472107 -0.0 0.20757052302360535
MemoryTrain:  epoch 18, batch     1 | loss: 0.4151410Losses:  1.1693609952926636 -0.0 0.5846804976463318
MemoryTrain:  epoch 19, batch     0 | loss: 1.1693610Losses:  0.5393561720848083 -0.0 0.2696780860424042
MemoryTrain:  epoch 19, batch     1 | loss: 0.5393562
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 99.22%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 97.92%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 84.38%   
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 54.17%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 54.69%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 61.61%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 72.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 75.96%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 70.54%   [EVAL] batch:   14 | acc: 18.75%,  total acc: 67.08%   [EVAL] batch:   15 | acc: 12.50%,  total acc: 63.67%   [EVAL] batch:   16 | acc: 0.00%,  total acc: 59.93%   [EVAL] batch:   17 | acc: 12.50%,  total acc: 57.29%   [EVAL] batch:   18 | acc: 6.25%,  total acc: 54.61%   [EVAL] batch:   19 | acc: 12.50%,  total acc: 52.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 54.76%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 56.82%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 58.42%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 60.16%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 61.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 63.22%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 64.35%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 66.81%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 67.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 69.53%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 69.51%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 69.12%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 68.57%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 67.88%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 67.06%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 66.45%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 65.22%   [EVAL] batch:   39 | acc: 0.00%,  total acc: 63.59%   [EVAL] batch:   40 | acc: 0.00%,  total acc: 62.04%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 62.05%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 62.06%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 62.64%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 63.47%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 64.27%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 65.03%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 65.76%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 66.45%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 66.88%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 66.67%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 66.59%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 66.16%   [EVAL] batch:   53 | acc: 25.00%,  total acc: 65.39%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 65.11%   [EVAL] batch:   55 | acc: 25.00%,  total acc: 64.40%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 64.58%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 65.09%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 65.68%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 66.15%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 66.29%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 66.63%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 66.47%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 66.50%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 67.02%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 67.52%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 68.00%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 68.47%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 68.93%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 69.38%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 69.72%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 69.86%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 69.51%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 69.75%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 69.65%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 69.64%   [EVAL] batch:   77 | acc: 6.25%,  total acc: 68.83%   
cur_acc:  ['0.8750', '0.7361', '0.6161', '0.8438']
his_acc:  ['0.8750', '0.8325', '0.7500', '0.6883']
Clustering into  12  clusters
Clusters:  [ 2  9  4 11  1  8  2  5  2  3  1  0  2  3  9  2  6  0  2  4 10  7  4  8
  3  2]
Losses:  7.450199604034424 6.649982452392578 0.4001084864139557
CurrentTrain: epoch  0, batch     0 | loss: 7.4501996Losses:  2.9424378871917725 2.4018781185150146 0.2702799141407013
CurrentTrain: epoch  0, batch     1 | loss: 2.9424379Losses:  8.424668312072754 7.687883377075195 0.36839261651039124
CurrentTrain: epoch  1, batch     0 | loss: 8.4246683Losses:  4.636385440826416 4.092732906341553 0.2718263268470764
CurrentTrain: epoch  1, batch     1 | loss: 4.6363854Losses:  6.662678241729736 5.981184959411621 0.34074658155441284
CurrentTrain: epoch  2, batch     0 | loss: 6.6626782Losses:  2.202423334121704 1.5013622045516968 0.35053059458732605
CurrentTrain: epoch  2, batch     1 | loss: 2.2024233Losses:  6.009148597717285 5.326763153076172 0.34119269251823425
CurrentTrain: epoch  3, batch     0 | loss: 6.0091486Losses:  2.8264596462249756 2.161301612854004 0.3325790464878082
CurrentTrain: epoch  3, batch     1 | loss: 2.8264596Losses:  6.548083782196045 5.881206512451172 0.3334386944770813
CurrentTrain: epoch  4, batch     0 | loss: 6.5480838Losses:  2.466804027557373 2.009502410888672 0.2286507934331894
CurrentTrain: epoch  4, batch     1 | loss: 2.4668040Losses:  7.0438151359558105 6.388110160827637 0.3278525471687317
CurrentTrain: epoch  5, batch     0 | loss: 7.0438151Losses:  2.662567615509033 2.1514883041381836 0.25553959608078003
CurrentTrain: epoch  5, batch     1 | loss: 2.6625676Losses:  7.422487735748291 6.780405044555664 0.3210413455963135
CurrentTrain: epoch  6, batch     0 | loss: 7.4224877Losses:  2.8243844509124756 2.3364384174346924 0.243973046541214
CurrentTrain: epoch  6, batch     1 | loss: 2.8243845Losses:  6.393473148345947 5.760242462158203 0.3166153132915497
CurrentTrain: epoch  7, batch     0 | loss: 6.3934731Losses:  2.0321600437164307 1.3855167627334595 0.323321670293808
CurrentTrain: epoch  7, batch     1 | loss: 2.0321600Losses:  5.230830669403076 4.598059177398682 0.31638583540916443
CurrentTrain: epoch  8, batch     0 | loss: 5.2308307Losses:  1.5712053775787354 0.8739790320396423 0.3486132025718689
CurrentTrain: epoch  8, batch     1 | loss: 1.5712054Losses:  6.123425006866455 5.495570182800293 0.31392741203308105
CurrentTrain: epoch  9, batch     0 | loss: 6.1234250Losses:  2.0651538372039795 1.4296684265136719 0.3177427351474762
CurrentTrain: epoch  9, batch     1 | loss: 2.0651538
Losses:  1.2529146671295166 -0.0 0.6264573335647583
MemoryTrain:  epoch  0, batch     0 | loss: 1.2529147Losses:  1.6224384307861328 -0.0 0.8112192153930664
MemoryTrain:  epoch  0, batch     1 | loss: 1.6224384Losses:  1.2543613910675049 -0.0 0.6271806955337524
MemoryTrain:  epoch  1, batch     0 | loss: 1.2543614Losses:  1.4359872341156006 -0.0 0.7179936170578003
MemoryTrain:  epoch  1, batch     1 | loss: 1.4359872Losses:  1.571529507637024 -0.0 0.785764753818512
MemoryTrain:  epoch  2, batch     0 | loss: 1.5715295Losses:  1.0339837074279785 -0.0 0.5169918537139893
MemoryTrain:  epoch  2, batch     1 | loss: 1.0339837Losses:  1.480574607849121 -0.0 0.7402873039245605
MemoryTrain:  epoch  3, batch     0 | loss: 1.4805746Losses:  0.9215973615646362 -0.0 0.4607986807823181
MemoryTrain:  epoch  3, batch     1 | loss: 0.9215974Losses:  1.3496642112731934 -0.0 0.6748321056365967
MemoryTrain:  epoch  4, batch     0 | loss: 1.3496642Losses:  1.1627568006515503 -0.0 0.5813784003257751
MemoryTrain:  epoch  4, batch     1 | loss: 1.1627568Losses:  1.7231112718582153 -0.0 0.8615556359291077
MemoryTrain:  epoch  5, batch     0 | loss: 1.7231113Losses:  0.8626588582992554 -0.0 0.4313294291496277
MemoryTrain:  epoch  5, batch     1 | loss: 0.8626589Losses:  1.3533855676651 -0.0 0.67669278383255
MemoryTrain:  epoch  6, batch     0 | loss: 1.3533856Losses:  1.1641978025436401 -0.0 0.5820989012718201
MemoryTrain:  epoch  6, batch     1 | loss: 1.1641978Losses:  1.4590373039245605 -0.0 0.7295186519622803
MemoryTrain:  epoch  7, batch     0 | loss: 1.4590373Losses:  0.8846359252929688 -0.0 0.4423179626464844
MemoryTrain:  epoch  7, batch     1 | loss: 0.8846359Losses:  1.6096076965332031 -0.0 0.8048038482666016
MemoryTrain:  epoch  8, batch     0 | loss: 1.6096077Losses:  1.282691478729248 -0.0 0.641345739364624
MemoryTrain:  epoch  8, batch     1 | loss: 1.2826915Losses:  1.528611660003662 -0.0 0.764305830001831
MemoryTrain:  epoch  9, batch     0 | loss: 1.5286117Losses:  0.6936867833137512 -0.0 0.3468433916568756
MemoryTrain:  epoch  9, batch     1 | loss: 0.6936868Losses:  1.7074470520019531 -0.0 0.8537235260009766
MemoryTrain:  epoch 10, batch     0 | loss: 1.7074471Losses:  0.9971485137939453 -0.0 0.49857425689697266
MemoryTrain:  epoch 10, batch     1 | loss: 0.9971485Losses:  1.3405170440673828 -0.0 0.6702585220336914
MemoryTrain:  epoch 11, batch     0 | loss: 1.3405170Losses:  1.0798630714416504 -0.0 0.5399315357208252
MemoryTrain:  epoch 11, batch     1 | loss: 1.0798631Losses:  1.5559269189834595 -0.0 0.7779634594917297
MemoryTrain:  epoch 12, batch     0 | loss: 1.5559269Losses:  0.8758026957511902 -0.0 0.4379013478755951
MemoryTrain:  epoch 12, batch     1 | loss: 0.8758027Losses:  1.4807506799697876 -0.0 0.7403753399848938
MemoryTrain:  epoch 13, batch     0 | loss: 1.4807507Losses:  0.8095520734786987 -0.0 0.40477603673934937
MemoryTrain:  epoch 13, batch     1 | loss: 0.8095521Losses:  1.4960190057754517 -0.0 0.7480095028877258
MemoryTrain:  epoch 14, batch     0 | loss: 1.4960190Losses:  0.9659788012504578 -0.0 0.4829894006252289
MemoryTrain:  epoch 14, batch     1 | loss: 0.9659788Losses:  1.2470934391021729 -0.0 0.6235467195510864
MemoryTrain:  epoch 15, batch     0 | loss: 1.2470934Losses:  1.253666639328003 -0.0 0.6268333196640015
MemoryTrain:  epoch 15, batch     1 | loss: 1.2536666Losses:  1.5112704038619995 -0.0 0.7556352019309998
MemoryTrain:  epoch 16, batch     0 | loss: 1.5112704Losses:  1.4174275398254395 -0.0 0.7087137699127197
MemoryTrain:  epoch 16, batch     1 | loss: 1.4174275Losses:  1.5807489156723022 -0.0 0.7903744578361511
MemoryTrain:  epoch 17, batch     0 | loss: 1.5807489Losses:  1.1979371309280396 -0.0 0.5989685654640198
MemoryTrain:  epoch 17, batch     1 | loss: 1.1979371Losses:  1.609386682510376 -0.0 0.804693341255188
MemoryTrain:  epoch 18, batch     0 | loss: 1.6093867Losses:  1.1769503355026245 -0.0 0.5884751677513123
MemoryTrain:  epoch 18, batch     1 | loss: 1.1769503Losses:  0.9043821692466736 -0.0 0.4521910846233368
MemoryTrain:  epoch 19, batch     0 | loss: 0.9043822Losses:  1.0547816753387451 -0.0 0.5273908376693726
MemoryTrain:  epoch 19, batch     1 | loss: 1.0547817
