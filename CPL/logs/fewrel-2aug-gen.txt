#############params############
cuda:0
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
CurrentTrain: epoch 15, batch     0 | loss: 22.1817371CurrentTrain: epoch 15, batch     1 | loss: 28.6888943CurrentTrain: epoch 15, batch     2 | loss: 23.1314155CurrentTrain: epoch 15, batch     3 | loss: 24.3435185CurrentTrain: epoch 15, batch     4 | loss: 24.3867784CurrentTrain: epoch 15, batch     5 | loss: 30.4707721CurrentTrain: epoch 15, batch     6 | loss: 33.7832888CurrentTrain: epoch 15, batch     7 | loss: 25.3541734CurrentTrain: epoch 15, batch     8 | loss: 18.4227781CurrentTrain: epoch 15, batch     9 | loss: 16.9107980CurrentTrain: epoch 15, batch    10 | loss: 16.0048436CurrentTrain: epoch 15, batch    11 | loss: 21.7024754CurrentTrain: epoch 15, batch    12 | loss: 21.0412100CurrentTrain: epoch 15, batch    13 | loss: 18.9205872CurrentTrain: epoch 15, batch    14 | loss: 19.8888331CurrentTrain: epoch 15, batch    15 | loss: 23.3323653CurrentTrain: epoch 15, batch    16 | loss: 34.3850799CurrentTrain: epoch 15, batch    17 | loss: 26.8765288CurrentTrain: epoch 15, batch    18 | loss: 15.4673438CurrentTrain: epoch 15, batch    19 | loss: 20.5879893CurrentTrain: epoch 15, batch    20 | loss: 23.4211730CurrentTrain: epoch 15, batch    21 | loss: 20.2024659CurrentTrain: epoch 15, batch    22 | loss: 22.9266150CurrentTrain: epoch 15, batch    23 | loss: 21.4197154CurrentTrain: epoch 15, batch    24 | loss: 19.7522127CurrentTrain: epoch 15, batch    25 | loss: 16.7227937CurrentTrain: epoch 15, batch    26 | loss: 21.9557699CurrentTrain: epoch 15, batch    27 | loss: 15.9789989CurrentTrain: epoch 15, batch    28 | loss: 17.8893935CurrentTrain: epoch 15, batch    29 | loss: 26.0812428CurrentTrain: epoch 15, batch    30 | loss: 26.0872949CurrentTrain: epoch 15, batch    31 | loss: 27.5813806CurrentTrain: epoch 15, batch    32 | loss: 16.2889611CurrentTrain: epoch 15, batch    33 | loss: 17.2772572CurrentTrain: epoch 15, batch    34 | loss: 25.5946405CurrentTrain: epoch 15, batch    35 | loss: 15.4742187CurrentTrain: epoch 15, batch    36 | loss: 15.5689740CurrentTrain: epoch 15, batch    37 | loss: 19.5254531CurrentTrain: epoch 15, batch    38 | loss: 19.1432330CurrentTrain: epoch 15, batch    39 | loss: 19.1705712CurrentTrain: epoch 15, batch    40 | loss: 23.4134738CurrentTrain: epoch 15, batch    41 | loss: 20.6099521CurrentTrain: epoch 15, batch    42 | loss: 18.2489009CurrentTrain: epoch 15, batch    43 | loss: 27.7655110CurrentTrain: epoch 15, batch    44 | loss: 17.0935013CurrentTrain: epoch 15, batch    45 | loss: 20.3931133CurrentTrain: epoch 15, batch    46 | loss: 12.3828701CurrentTrain: epoch 15, batch    47 | loss: 15.6869517CurrentTrain: epoch 15, batch    48 | loss: 13.5302880CurrentTrain: epoch 15, batch    49 | loss: 14.2620711CurrentTrain: epoch 15, batch    50 | loss: 15.2737696CurrentTrain: epoch 15, batch    51 | loss: 19.9798001CurrentTrain: epoch 15, batch    52 | loss: 14.5914372CurrentTrain: epoch 15, batch    53 | loss: 25.6078720CurrentTrain: epoch 15, batch    54 | loss: 12.8061138CurrentTrain: epoch 15, batch    55 | loss: 15.4549417CurrentTrain: epoch 15, batch    56 | loss: 17.2379093CurrentTrain: epoch 15, batch    57 | loss: 11.3734390CurrentTrain: epoch 15, batch    58 | loss: 12.2625556CurrentTrain: epoch 15, batch    59 | loss: 31.7211643CurrentTrain: epoch 15, batch    60 | loss: 16.4983630CurrentTrain: epoch 15, batch    61 | loss: 13.5072260CurrentTrain: epoch  7, batch    62 | loss: 20.2357321CurrentTrain: epoch 15, batch     0 | loss: 14.1273359CurrentTrain: epoch 15, batch     1 | loss: 22.1862460CurrentTrain: epoch 15, batch     2 | loss: 14.4434843CurrentTrain: epoch 15, batch     3 | loss: 12.3958639CurrentTrain: epoch 15, batch     4 | loss: 13.8998671CurrentTrain: epoch 15, batch     5 | loss: 12.8861273CurrentTrain: epoch 15, batch     6 | loss: 12.8554705CurrentTrain: epoch 15, batch     7 | loss: 13.7308111CurrentTrain: epoch 15, batch     8 | loss: 18.7912341CurrentTrain: epoch 15, batch     9 | loss: 15.1933935CurrentTrain: epoch 15, batch    10 | loss: 12.7213917CurrentTrain: epoch 15, batch    11 | loss: 13.3498610CurrentTrain: epoch 15, batch    12 | loss: 23.5670023CurrentTrain: epoch 15, batch    13 | loss: 15.5983445CurrentTrain: epoch 15, batch    14 | loss: 20.3022102CurrentTrain: epoch 15, batch    15 | loss: 12.0431817CurrentTrain: epoch 15, batch    16 | loss: 13.3067350CurrentTrain: epoch 15, batch    17 | loss: 19.9563611CurrentTrain: epoch 15, batch    18 | loss: 13.7824180CurrentTrain: epoch 15, batch    19 | loss: 19.6183781CurrentTrain: epoch 15, batch    20 | loss: 15.3821448CurrentTrain: epoch 15, batch    21 | loss: 16.5759421CurrentTrain: epoch 15, batch    22 | loss: 14.3344554CurrentTrain: epoch 15, batch    23 | loss: 14.8989737CurrentTrain: epoch 15, batch    24 | loss: 19.2516251CurrentTrain: epoch 15, batch    25 | loss: 12.9935469CurrentTrain: epoch 15, batch    26 | loss: 11.4311428CurrentTrain: epoch 15, batch    27 | loss: 15.7890155CurrentTrain: epoch 15, batch    28 | loss: 14.0909767CurrentTrain: epoch 15, batch    29 | loss: 14.0136749CurrentTrain: epoch 15, batch    30 | loss: 14.6199555CurrentTrain: epoch 15, batch    31 | loss: 18.1841514CurrentTrain: epoch 15, batch    32 | loss: 12.7227339CurrentTrain: epoch 15, batch    33 | loss: 19.1328778CurrentTrain: epoch 15, batch    34 | loss: 14.3274812CurrentTrain: epoch 15, batch    35 | loss: 15.9297158CurrentTrain: epoch 15, batch    36 | loss: 36.3744868CurrentTrain: epoch 15, batch    37 | loss: 15.3135604CurrentTrain: epoch 15, batch    38 | loss: 11.3761574CurrentTrain: epoch 15, batch    39 | loss: 10.8773941CurrentTrain: epoch 15, batch    40 | loss: 19.1191078CurrentTrain: epoch 15, batch    41 | loss: 12.3307621CurrentTrain: epoch 15, batch    42 | loss: 11.2353690CurrentTrain: epoch 15, batch    43 | loss: 9.7898168CurrentTrain: epoch 15, batch    44 | loss: 14.2350067CurrentTrain: epoch 15, batch    45 | loss: 11.0055306CurrentTrain: epoch 15, batch    46 | loss: 18.5657544CurrentTrain: epoch 15, batch    47 | loss: 16.5307302CurrentTrain: epoch 15, batch    48 | loss: 16.4588772CurrentTrain: epoch 15, batch    49 | loss: 11.7310817CurrentTrain: epoch 15, batch    50 | loss: 20.4741093CurrentTrain: epoch 15, batch    51 | loss: 17.0494258CurrentTrain: epoch 15, batch    52 | loss: 12.7852282CurrentTrain: epoch 15, batch    53 | loss: 18.5008837CurrentTrain: epoch 15, batch    54 | loss: 17.8683122CurrentTrain: epoch 15, batch    55 | loss: 12.4474942CurrentTrain: epoch 15, batch    56 | loss: 13.3855707CurrentTrain: epoch 15, batch    57 | loss: 10.8393378CurrentTrain: epoch 15, batch    58 | loss: 16.6749882CurrentTrain: epoch 15, batch    59 | loss: 15.5762619CurrentTrain: epoch 15, batch    60 | loss: 12.2816455CurrentTrain: epoch 15, batch    61 | loss: 15.7333712CurrentTrain: epoch  7, batch    62 | loss: 16.0324223CurrentTrain: epoch 15, batch     0 | loss: 16.6318881CurrentTrain: epoch 15, batch     1 | loss: 9.0378037CurrentTrain: epoch 15, batch     2 | loss: 19.4565226CurrentTrain: epoch 15, batch     3 | loss: 12.3731243CurrentTrain: epoch 15, batch     4 | loss: 13.3302871CurrentTrain: epoch 15, batch     5 | loss: 18.1685387CurrentTrain: epoch 15, batch     6 | loss: 28.8842817CurrentTrain: epoch 15, batch     7 | loss: 18.6816351CurrentTrain: epoch 15, batch     8 | loss: 11.5561333CurrentTrain: epoch 15, batch     9 | loss: 10.8863670CurrentTrain: epoch 15, batch    10 | loss: 9.2120438CurrentTrain: epoch 15, batch    11 | loss: 16.5076749CurrentTrain: epoch 15, batch    12 | loss: 14.2529493CurrentTrain: epoch 15, batch    13 | loss: 11.6401942CurrentTrain: epoch 15, batch    14 | loss: 16.9882274CurrentTrain: epoch 15, batch    15 | loss: 11.9465630CurrentTrain: epoch 15, batch    16 | loss: 16.7883231CurrentTrain: epoch 15, batch    17 | loss: 10.0203586CurrentTrain: epoch 15, batch    18 | loss: 9.8278461CurrentTrain: epoch 15, batch    19 | loss: 10.0747253CurrentTrain: epoch 15, batch    20 | loss: 9.7690716CurrentTrain: epoch 15, batch    21 | loss: 13.4455753CurrentTrain: epoch 15, batch    22 | loss: 20.9973814CurrentTrain: epoch 15, batch    23 | loss: 11.3092717CurrentTrain: epoch 15, batch    24 | loss: 14.7355238CurrentTrain: epoch 15, batch    25 | loss: 19.8537970CurrentTrain: epoch 15, batch    26 | loss: 11.2030134CurrentTrain: epoch 15, batch    27 | loss: 11.4398748CurrentTrain: epoch 15, batch    28 | loss: 10.1108563CurrentTrain: epoch 15, batch    29 | loss: 12.3865099CurrentTrain: epoch 15, batch    30 | loss: 13.6031015CurrentTrain: epoch 15, batch    31 | loss: 18.3713882CurrentTrain: epoch 15, batch    32 | loss: 12.9921376CurrentTrain: epoch 15, batch    33 | loss: 9.5966716CurrentTrain: epoch 15, batch    34 | loss: 12.4446086CurrentTrain: epoch 15, batch    35 | loss: 12.0486420CurrentTrain: epoch 15, batch    36 | loss: 16.6322500CurrentTrain: epoch 15, batch    37 | loss: 12.1810572CurrentTrain: epoch 15, batch    38 | loss: 12.7382713CurrentTrain: epoch 15, batch    39 | loss: 16.6888628CurrentTrain: epoch 15, batch    40 | loss: 15.4793648CurrentTrain: epoch 15, batch    41 | loss: 15.4033126CurrentTrain: epoch 15, batch    42 | loss: 11.9995248CurrentTrain: epoch 15, batch    43 | loss: 11.5753379CurrentTrain: epoch 15, batch    44 | loss: 14.0218712CurrentTrain: epoch 15, batch    45 | loss: 15.6296971CurrentTrain: epoch 15, batch    46 | loss: 12.8731309CurrentTrain: epoch 15, batch    47 | loss: 8.1229230CurrentTrain: epoch 15, batch    48 | loss: 8.9382121CurrentTrain: epoch 15, batch    49 | loss: 13.9027903CurrentTrain: epoch 15, batch    50 | loss: 15.6983155CurrentTrain: epoch 15, batch    51 | loss: 13.4543943CurrentTrain: epoch 15, batch    52 | loss: 12.9294134CurrentTrain: epoch 15, batch    53 | loss: 16.2393462CurrentTrain: epoch 15, batch    54 | loss: 15.3202276CurrentTrain: epoch 15, batch    55 | loss: 13.4763122CurrentTrain: epoch 15, batch    56 | loss: 11.5285629CurrentTrain: epoch 15, batch    57 | loss: 16.7934059CurrentTrain: epoch 15, batch    58 | loss: 15.3492731CurrentTrain: epoch 15, batch    59 | loss: 14.1277862CurrentTrain: epoch 15, batch    60 | loss: 9.0237973CurrentTrain: epoch 15, batch    61 | loss: 10.5133236CurrentTrain: epoch  7, batch    62 | loss: 15.3998020CurrentTrain: epoch 15, batch     0 | loss: 9.0610623CurrentTrain: epoch 15, batch     1 | loss: 14.0298108CurrentTrain: epoch 15, batch     2 | loss: 36.4005097CurrentTrain: epoch 15, batch     3 | loss: 17.4908568CurrentTrain: epoch 15, batch     4 | loss: 16.1465742CurrentTrain: epoch 15, batch     5 | loss: 17.0097438CurrentTrain: epoch 15, batch     6 | loss: 16.3214806CurrentTrain: epoch 15, batch     7 | loss: 8.5152605CurrentTrain: epoch 15, batch     8 | loss: 11.7376487CurrentTrain: epoch 15, batch     9 | loss: 10.5197015CurrentTrain: epoch 15, batch    10 | loss: 11.3641584CurrentTrain: epoch 15, batch    11 | loss: 11.6776099CurrentTrain: epoch 15, batch    12 | loss: 11.3419857CurrentTrain: epoch 15, batch    13 | loss: 14.1249155CurrentTrain: epoch 15, batch    14 | loss: 9.9971964CurrentTrain: epoch 15, batch    15 | loss: 8.7530052CurrentTrain: epoch 15, batch    16 | loss: 11.5495624CurrentTrain: epoch 15, batch    17 | loss: 10.8688392CurrentTrain: epoch 15, batch    18 | loss: 15.7418083CurrentTrain: epoch 15, batch    19 | loss: 10.7614309CurrentTrain: epoch 15, batch    20 | loss: 9.5472914CurrentTrain: epoch 15, batch    21 | loss: 13.5599412CurrentTrain: epoch 15, batch    22 | loss: 16.8296365CurrentTrain: epoch 15, batch    23 | loss: 10.7380419CurrentTrain: epoch 15, batch    24 | loss: 10.6612874CurrentTrain: epoch 15, batch    25 | loss: 16.9435028CurrentTrain: epoch 15, batch    26 | loss: 10.9923109CurrentTrain: epoch 15, batch    27 | loss: 10.7640927CurrentTrain: epoch 15, batch    28 | loss: 11.4573414CurrentTrain: epoch 15, batch    29 | loss: 12.6903600CurrentTrain: epoch 15, batch    30 | loss: 15.2532758CurrentTrain: epoch 15, batch    31 | loss: 18.3648337CurrentTrain: epoch 15, batch    32 | loss: 10.9004767CurrentTrain: epoch 15, batch    33 | loss: 11.1915515CurrentTrain: epoch 15, batch    34 | loss: 19.3603118CurrentTrain: epoch 15, batch    35 | loss: 11.9666636CurrentTrain: epoch 15, batch    36 | loss: 20.7596724CurrentTrain: epoch 15, batch    37 | loss: 34.3938571CurrentTrain: epoch 15, batch    38 | loss: 18.7046261CurrentTrain: epoch 15, batch    39 | loss: 11.7439188CurrentTrain: epoch 15, batch    40 | loss: 9.3261663CurrentTrain: epoch 15, batch    41 | loss: 17.9129079CurrentTrain: epoch 15, batch    42 | loss: 19.5016482CurrentTrain: epoch 15, batch    43 | loss: 12.7485996CurrentTrain: epoch 15, batch    44 | loss: 10.5669715CurrentTrain: epoch 15, batch    45 | loss: 10.6822522CurrentTrain: epoch 15, batch    46 | loss: 20.1431778CurrentTrain: epoch 15, batch    47 | loss: 8.7700539CurrentTrain: epoch 15, batch    48 | loss: 22.5633177CurrentTrain: epoch 15, batch    49 | loss: 13.8814231CurrentTrain: epoch 15, batch    50 | loss: 9.1040183CurrentTrain: epoch 15, batch    51 | loss: 11.1945683CurrentTrain: epoch 15, batch    52 | loss: 9.3441493CurrentTrain: epoch 15, batch    53 | loss: 21.3378675CurrentTrain: epoch 15, batch    54 | loss: 8.5648465CurrentTrain: epoch 15, batch    55 | loss: 9.7161051CurrentTrain: epoch 15, batch    56 | loss: 11.3468282CurrentTrain: epoch 15, batch    57 | loss: 11.3957870CurrentTrain: epoch 15, batch    58 | loss: 18.6272494CurrentTrain: epoch 15, batch    59 | loss: 8.6303642CurrentTrain: epoch 15, batch    60 | loss: 13.0621359CurrentTrain: epoch 15, batch    61 | loss: 9.2159212CurrentTrain: epoch  7, batch    62 | loss: 13.6478589CurrentTrain: epoch 15, batch     0 | loss: 12.3108885CurrentTrain: epoch 15, batch     1 | loss: 10.6066793CurrentTrain: epoch 15, batch     2 | loss: 11.7204340CurrentTrain: epoch 15, batch     3 | loss: 10.1407232CurrentTrain: epoch 15, batch     4 | loss: 13.1158353CurrentTrain: epoch 15, batch     5 | loss: 18.5979348CurrentTrain: epoch 15, batch     6 | loss: 13.1124623CurrentTrain: epoch 15, batch     7 | loss: 7.9671610CurrentTrain: epoch 15, batch     8 | loss: 11.7326560CurrentTrain: epoch 15, batch     9 | loss: 9.8820761CurrentTrain: epoch 15, batch    10 | loss: 10.5735687CurrentTrain: epoch 15, batch    11 | loss: 15.2685828CurrentTrain: epoch 15, batch    12 | loss: 11.1155052CurrentTrain: epoch 15, batch    13 | loss: 10.9703219CurrentTrain: epoch 15, batch    14 | loss: 9.8177689CurrentTrain: epoch 15, batch    15 | loss: 11.5299807CurrentTrain: epoch 15, batch    16 | loss: 8.9690079CurrentTrain: epoch 15, batch    17 | loss: 13.5560641CurrentTrain: epoch 15, batch    18 | loss: 9.3287042CurrentTrain: epoch 15, batch    19 | loss: 9.1634348CurrentTrain: epoch 15, batch    20 | loss: 9.6187216CurrentTrain: epoch 15, batch    21 | loss: 11.1370072CurrentTrain: epoch 15, batch    22 | loss: 8.6165790CurrentTrain: epoch 15, batch    23 | loss: 15.6286205CurrentTrain: epoch 15, batch    24 | loss: 10.5897178CurrentTrain: epoch 15, batch    25 | loss: 12.0862536CurrentTrain: epoch 15, batch    26 | loss: 10.4607975CurrentTrain: epoch 15, batch    27 | loss: 8.6047814CurrentTrain: epoch 15, batch    28 | loss: 11.8176982CurrentTrain: epoch 15, batch    29 | loss: 15.5758193CurrentTrain: epoch 15, batch    30 | loss: 22.2139266CurrentTrain: epoch 15, batch    31 | loss: 15.7887164CurrentTrain: epoch 15, batch    32 | loss: 9.6461244CurrentTrain: epoch 15, batch    33 | loss: 12.5477081CurrentTrain: epoch 15, batch    34 | loss: 10.9052283CurrentTrain: epoch 15, batch    35 | loss: 12.0178517CurrentTrain: epoch 15, batch    36 | loss: 9.2323675CurrentTrain: epoch 15, batch    37 | loss: 21.5218436CurrentTrain: epoch 15, batch    38 | loss: 13.6244893CurrentTrain: epoch 15, batch    39 | loss: 11.0911607CurrentTrain: epoch 15, batch    40 | loss: 9.6599671CurrentTrain: epoch 15, batch    41 | loss: 8.0645963CurrentTrain: epoch 15, batch    42 | loss: 12.5282138CurrentTrain: epoch 15, batch    43 | loss: 17.3729691CurrentTrain: epoch 15, batch    44 | loss: 9.8487664CurrentTrain: epoch 15, batch    45 | loss: 12.6325523CurrentTrain: epoch 15, batch    46 | loss: 8.1973757CurrentTrain: epoch 15, batch    47 | loss: 16.4316580CurrentTrain: epoch 15, batch    48 | loss: 11.6139029CurrentTrain: epoch 15, batch    49 | loss: 13.2019770CurrentTrain: epoch 15, batch    50 | loss: 12.2121576CurrentTrain: epoch 15, batch    51 | loss: 10.0123111CurrentTrain: epoch 15, batch    52 | loss: 11.1777357CurrentTrain: epoch 15, batch    53 | loss: 17.7665043CurrentTrain: epoch 15, batch    54 | loss: 13.1416414CurrentTrain: epoch 15, batch    55 | loss: 9.3019568CurrentTrain: epoch 15, batch    56 | loss: 11.4601639CurrentTrain: epoch 15, batch    57 | loss: 10.8419392CurrentTrain: epoch 15, batch    58 | loss: 10.4472699CurrentTrain: epoch 15, batch    59 | loss: 11.6951631CurrentTrain: epoch 15, batch    60 | loss: 13.2274265CurrentTrain: epoch 15, batch    61 | loss: 13.0149321CurrentTrain: epoch  7, batch    62 | loss: 8.0772601CurrentTrain: epoch 15, batch     0 | loss: 7.1307671CurrentTrain: epoch 15, batch     1 | loss: 9.0983661CurrentTrain: epoch 15, batch     2 | loss: 10.7009492CurrentTrain: epoch 15, batch     3 | loss: 11.3878445CurrentTrain: epoch 15, batch     4 | loss: 19.1115913CurrentTrain: epoch 15, batch     5 | loss: 13.7970641CurrentTrain: epoch 15, batch     6 | loss: 7.7675428CurrentTrain: epoch 15, batch     7 | loss: 11.9018473CurrentTrain: epoch 15, batch     8 | loss: 9.3800506CurrentTrain: epoch 15, batch     9 | loss: 8.1823221CurrentTrain: epoch 15, batch    10 | loss: 11.2150179CurrentTrain: epoch 15, batch    11 | loss: 16.9933842CurrentTrain: epoch 15, batch    12 | loss: 28.2940618CurrentTrain: epoch 15, batch    13 | loss: 12.9708678CurrentTrain: epoch 15, batch    14 | loss: 15.0893857CurrentTrain: epoch 15, batch    15 | loss: 16.5356327CurrentTrain: epoch 15, batch    16 | loss: 10.8170555CurrentTrain: epoch 15, batch    17 | loss: 17.0919928CurrentTrain: epoch 15, batch    18 | loss: 14.3221549CurrentTrain: epoch 15, batch    19 | loss: 10.5910535CurrentTrain: epoch 15, batch    20 | loss: 10.6710374CurrentTrain: epoch 15, batch    21 | loss: 16.7901072CurrentTrain: epoch 15, batch    22 | loss: 18.2326548CurrentTrain: epoch 15, batch    23 | loss: 12.8616190CurrentTrain: epoch 15, batch    24 | loss: 14.4821717CurrentTrain: epoch 15, batch    25 | loss: 21.9975780CurrentTrain: epoch 15, batch    26 | loss: 11.6087038CurrentTrain: epoch 15, batch    27 | loss: 8.3689145CurrentTrain: epoch 15, batch    28 | loss: 17.8680200CurrentTrain: epoch 15, batch    29 | loss: 7.0105717CurrentTrain: epoch 15, batch    30 | loss: 14.9333668CurrentTrain: epoch 15, batch    31 | loss: 17.2992891CurrentTrain: epoch 15, batch    32 | loss: 11.3760960CurrentTrain: epoch 15, batch    33 | loss: 11.7434131CurrentTrain: epoch 15, batch    34 | loss: 12.1093507CurrentTrain: epoch 15, batch    35 | loss: 10.6414499CurrentTrain: epoch 15, batch    36 | loss: 7.1224661CurrentTrain: epoch 15, batch    37 | loss: 13.5400481CurrentTrain: epoch 15, batch    38 | loss: 12.7470578CurrentTrain: epoch 15, batch    39 | loss: 10.7435933CurrentTrain: epoch 15, batch    40 | loss: 15.9992132CurrentTrain: epoch 15, batch    41 | loss: 12.1394945CurrentTrain: epoch 15, batch    42 | loss: 11.5938369CurrentTrain: epoch 15, batch    43 | loss: 9.2242185CurrentTrain: epoch 15, batch    44 | loss: 10.7445760CurrentTrain: epoch 15, batch    45 | loss: 8.5618705CurrentTrain: epoch 15, batch    46 | loss: 15.3937459CurrentTrain: epoch 15, batch    47 | loss: 9.4469001CurrentTrain: epoch 15, batch    48 | loss: 16.6278377CurrentTrain: epoch 15, batch    49 | loss: 9.0637371CurrentTrain: epoch 15, batch    50 | loss: 11.7722447CurrentTrain: epoch 15, batch    51 | loss: 10.2787853CurrentTrain: epoch 15, batch    52 | loss: 10.9208120CurrentTrain: epoch 15, batch    53 | loss: 10.9624730CurrentTrain: epoch 15, batch    54 | loss: 15.5215786CurrentTrain: epoch 15, batch    55 | loss: 15.2259729CurrentTrain: epoch 15, batch    56 | loss: 21.0500755CurrentTrain: epoch 15, batch    57 | loss: 13.5812475CurrentTrain: epoch 15, batch    58 | loss: 11.0674942CurrentTrain: epoch 15, batch    59 | loss: 11.1622102CurrentTrain: epoch 15, batch    60 | loss: 10.1469195CurrentTrain: epoch 15, batch    61 | loss: 9.5121312CurrentTrain: epoch  7, batch    62 | loss: 4.5698312CurrentTrain: epoch 15, batch     0 | loss: 10.8863918CurrentTrain: epoch 15, batch     1 | loss: 10.3157345CurrentTrain: epoch 15, batch     2 | loss: 24.8899637CurrentTrain: epoch 15, batch     3 | loss: 13.2599980CurrentTrain: epoch 15, batch     4 | loss: 20.0305184CurrentTrain: epoch 15, batch     5 | loss: 12.1062178CurrentTrain: epoch 15, batch     6 | loss: 18.8149197CurrentTrain: epoch 15, batch     7 | loss: 8.9241116CurrentTrain: epoch 15, batch     8 | loss: 11.3828320CurrentTrain: epoch 15, batch     9 | loss: 6.6764801CurrentTrain: epoch 15, batch    10 | loss: 9.0104326CurrentTrain: epoch 15, batch    11 | loss: 14.8648992CurrentTrain: epoch 15, batch    12 | loss: 8.3631324CurrentTrain: epoch 15, batch    13 | loss: 21.8948049CurrentTrain: epoch 15, batch    14 | loss: 9.1792899CurrentTrain: epoch 15, batch    15 | loss: 14.9844032CurrentTrain: epoch 15, batch    16 | loss: 25.0588744CurrentTrain: epoch 15, batch    17 | loss: 14.6024713CurrentTrain: epoch 15, batch    18 | loss: 14.4753964CurrentTrain: epoch 15, batch    19 | loss: 13.6354363CurrentTrain: epoch 15, batch    20 | loss: 10.9835257CurrentTrain: epoch 15, batch    21 | loss: 8.0550169CurrentTrain: epoch 15, batch    22 | loss: 14.4057384CurrentTrain: epoch 15, batch    23 | loss: 9.3860776CurrentTrain: epoch 15, batch    24 | loss: 10.0791760CurrentTrain: epoch 15, batch    25 | loss: 19.2622761CurrentTrain: epoch 15, batch    26 | loss: 11.0459426CurrentTrain: epoch 15, batch    27 | loss: 10.6434672CurrentTrain: epoch 15, batch    28 | loss: 10.9914896CurrentTrain: epoch 15, batch    29 | loss: 12.2463598CurrentTrain: epoch 15, batch    30 | loss: 12.0901144CurrentTrain: epoch 15, batch    31 | loss: 9.4796665CurrentTrain: epoch 15, batch    32 | loss: 9.7524545CurrentTrain: epoch 15, batch    33 | loss: 8.2641492CurrentTrain: epoch 15, batch    34 | loss: 15.0107468CurrentTrain: epoch 15, batch    35 | loss: 7.6282919CurrentTrain: epoch 15, batch    36 | loss: 7.7130539CurrentTrain: epoch 15, batch    37 | loss: 11.3971448CurrentTrain: epoch 15, batch    38 | loss: 8.7678859CurrentTrain: epoch 15, batch    39 | loss: 8.9659880CurrentTrain: epoch 15, batch    40 | loss: 13.8158537CurrentTrain: epoch 15, batch    41 | loss: 9.5536988CurrentTrain: epoch 15, batch    42 | loss: 11.8597708CurrentTrain: epoch 15, batch    43 | loss: 7.0095712CurrentTrain: epoch 15, batch    44 | loss: 13.0733019CurrentTrain: epoch 15, batch    45 | loss: 12.8653805CurrentTrain: epoch 15, batch    46 | loss: 8.7583856CurrentTrain: epoch 15, batch    47 | loss: 9.0993521CurrentTrain: epoch 15, batch    48 | loss: 8.8995822CurrentTrain: epoch 15, batch    49 | loss: 10.0487678CurrentTrain: epoch 15, batch    50 | loss: 6.4980370CurrentTrain: epoch 15, batch    51 | loss: 15.5052639CurrentTrain: epoch 15, batch    52 | loss: 8.4427859CurrentTrain: epoch 15, batch    53 | loss: 24.2405256CurrentTrain: epoch 15, batch    54 | loss: 10.4419865CurrentTrain: epoch 15, batch    55 | loss: 8.3077576CurrentTrain: epoch 15, batch    56 | loss: 18.4909869CurrentTrain: epoch 15, batch    57 | loss: 17.2923212CurrentTrain: epoch 15, batch    58 | loss: 7.4366674CurrentTrain: epoch 15, batch    59 | loss: 8.3513082CurrentTrain: epoch 15, batch    60 | loss: 11.9212441CurrentTrain: epoch 15, batch    61 | loss: 10.2625509CurrentTrain: epoch  7, batch    62 | loss: 10.7366552CurrentTrain: epoch 15, batch     0 | loss: 15.5514867CurrentTrain: epoch 15, batch     1 | loss: 12.7296956CurrentTrain: epoch 15, batch     2 | loss: 8.1441024CurrentTrain: epoch 15, batch     3 | loss: 11.4337531CurrentTrain: epoch 15, batch     4 | loss: 16.7243895CurrentTrain: epoch 15, batch     5 | loss: 11.7064965CurrentTrain: epoch 15, batch     6 | loss: 10.0906461CurrentTrain: epoch 15, batch     7 | loss: 7.5155588CurrentTrain: epoch 15, batch     8 | loss: 27.0882893CurrentTrain: epoch 15, batch     9 | loss: 17.3583137CurrentTrain: epoch 15, batch    10 | loss: 17.2899447CurrentTrain: epoch 15, batch    11 | loss: 7.1603363CurrentTrain: epoch 15, batch    12 | loss: 12.4921251CurrentTrain: epoch 15, batch    13 | loss: 9.0419127CurrentTrain: epoch 15, batch    14 | loss: 11.7298749CurrentTrain: epoch 15, batch    15 | loss: 8.3855779CurrentTrain: epoch 15, batch    16 | loss: 14.7683747CurrentTrain: epoch 15, batch    17 | loss: 14.4644660CurrentTrain: epoch 15, batch    18 | loss: 16.4850796CurrentTrain: epoch 15, batch    19 | loss: 11.9217708CurrentTrain: epoch 15, batch    20 | loss: 10.0189238CurrentTrain: epoch 15, batch    21 | loss: 15.0594515CurrentTrain: epoch 15, batch    22 | loss: 12.8302838CurrentTrain: epoch 15, batch    23 | loss: 13.1265783CurrentTrain: epoch 15, batch    24 | loss: 10.8988852CurrentTrain: epoch 15, batch    25 | loss: 9.3929140CurrentTrain: epoch 15, batch    26 | loss: 10.4418961CurrentTrain: epoch 15, batch    27 | loss: 11.6168992CurrentTrain: epoch 15, batch    28 | loss: 12.7517634CurrentTrain: epoch 15, batch    29 | loss: 17.2073654CurrentTrain: epoch 15, batch    30 | loss: 11.2480864CurrentTrain: epoch 15, batch    31 | loss: 9.0190099CurrentTrain: epoch 15, batch    32 | loss: 12.6004121CurrentTrain: epoch 15, batch    33 | loss: 9.2759692CurrentTrain: epoch 15, batch    34 | loss: 20.2877226CurrentTrain: epoch 15, batch    35 | loss: 8.6788210CurrentTrain: epoch 15, batch    36 | loss: 9.2886067CurrentTrain: epoch 15, batch    37 | loss: 11.9521593CurrentTrain: epoch 15, batch    38 | loss: 12.7835719CurrentTrain: epoch 15, batch    39 | loss: 9.5750117CurrentTrain: epoch 15, batch    40 | loss: 6.8055227CurrentTrain: epoch 15, batch    41 | loss: 16.2002371CurrentTrain: epoch 15, batch    42 | loss: 14.8294481CurrentTrain: epoch 15, batch    43 | loss: 11.0144723CurrentTrain: epoch 15, batch    44 | loss: 13.3902649CurrentTrain: epoch 15, batch    45 | loss: 12.2980746CurrentTrain: epoch 15, batch    46 | loss: 13.1614359CurrentTrain: epoch 15, batch    47 | loss: 8.0754202CurrentTrain: epoch 15, batch    48 | loss: 11.0785672CurrentTrain: epoch 15, batch    49 | loss: 7.1102252CurrentTrain: epoch 15, batch    50 | loss: 11.3127432CurrentTrain: epoch 15, batch    51 | loss: 14.7499466CurrentTrain: epoch 15, batch    52 | loss: 7.8338534CurrentTrain: epoch 15, batch    53 | loss: 13.6372133CurrentTrain: epoch 15, batch    54 | loss: 13.6504696CurrentTrain: epoch 15, batch    55 | loss: 10.1008135CurrentTrain: epoch 15, batch    56 | loss: 9.4732731CurrentTrain: epoch 15, batch    57 | loss: 15.2235217CurrentTrain: epoch 15, batch    58 | loss: 11.9464709CurrentTrain: epoch 15, batch    59 | loss: 7.1776159CurrentTrain: epoch 15, batch    60 | loss: 11.0812823CurrentTrain: epoch 15, batch    61 | loss: 7.1749339CurrentTrain: epoch  7, batch    62 | loss: 11.6523536CurrentTrain: epoch 15, batch     0 | loss: 8.1903805CurrentTrain: epoch 15, batch     1 | loss: 11.3951360CurrentTrain: epoch 15, batch     2 | loss: 19.2562722CurrentTrain: epoch 15, batch     3 | loss: 6.5026423CurrentTrain: epoch 15, batch     4 | loss: 10.8739882CurrentTrain: epoch 15, batch     5 | loss: 11.5976052CurrentTrain: epoch 15, batch     6 | loss: 5.9918369CurrentTrain: epoch 15, batch     7 | loss: 9.2387136CurrentTrain: epoch 15, batch     8 | loss: 16.2787505CurrentTrain: epoch 15, batch     9 | loss: 13.1445938CurrentTrain: epoch 15, batch    10 | loss: 9.0674073CurrentTrain: epoch 15, batch    11 | loss: 18.9414436CurrentTrain: epoch 15, batch    12 | loss: 14.4472749CurrentTrain: epoch 15, batch    13 | loss: 9.1579504CurrentTrain: epoch 15, batch    14 | loss: 18.4632053CurrentTrain: epoch 15, batch    15 | loss: 8.6258790CurrentTrain: epoch 15, batch    16 | loss: 11.0984351CurrentTrain: epoch 15, batch    17 | loss: 11.0458880CurrentTrain: epoch 15, batch    18 | loss: 7.6685671CurrentTrain: epoch 15, batch    19 | loss: 10.0881873CurrentTrain: epoch 15, batch    20 | loss: 19.4363589CurrentTrain: epoch 15, batch    21 | loss: 10.1443406CurrentTrain: epoch 15, batch    22 | loss: 16.4277021CurrentTrain: epoch 15, batch    23 | loss: 11.8253536CurrentTrain: epoch 15, batch    24 | loss: 15.3729085CurrentTrain: epoch 15, batch    25 | loss: 10.1509974CurrentTrain: epoch 15, batch    26 | loss: 11.0034098CurrentTrain: epoch 15, batch    27 | loss: 10.9800054CurrentTrain: epoch 15, batch    28 | loss: 8.8068600CurrentTrain: epoch 15, batch    29 | loss: 13.9868504CurrentTrain: epoch 15, batch    30 | loss: 9.5652789CurrentTrain: epoch 15, batch    31 | loss: 10.3360908CurrentTrain: epoch 15, batch    32 | loss: 12.0583826CurrentTrain: epoch 15, batch    33 | loss: 6.8050927CurrentTrain: epoch 15, batch    34 | loss: 14.9949242CurrentTrain: epoch 15, batch    35 | loss: 13.8828401CurrentTrain: epoch 15, batch    36 | loss: 13.5759897CurrentTrain: epoch 15, batch    37 | loss: 8.7754307CurrentTrain: epoch 15, batch    38 | loss: 8.0736992CurrentTrain: epoch 15, batch    39 | loss: 21.8313970CurrentTrain: epoch 15, batch    40 | loss: 12.0609152CurrentTrain: epoch 15, batch    41 | loss: 8.6078438CurrentTrain: epoch 15, batch    42 | loss: 15.5172401CurrentTrain: epoch 15, batch    43 | loss: 8.1618489CurrentTrain: epoch 15, batch    44 | loss: 17.3062986CurrentTrain: epoch 15, batch    45 | loss: 7.9477262CurrentTrain: epoch 15, batch    46 | loss: 10.3490363CurrentTrain: epoch 15, batch    47 | loss: 15.7690649CurrentTrain: epoch 15, batch    48 | loss: 14.1192225CurrentTrain: epoch 15, batch    49 | loss: 7.4674908CurrentTrain: epoch 15, batch    50 | loss: 7.2221775CurrentTrain: epoch 15, batch    51 | loss: 10.0564415CurrentTrain: epoch 15, batch    52 | loss: 15.5452797CurrentTrain: epoch 15, batch    53 | loss: 20.6237171CurrentTrain: epoch 15, batch    54 | loss: 13.6839732CurrentTrain: epoch 15, batch    55 | loss: 9.5820083CurrentTrain: epoch 15, batch    56 | loss: 6.7967075CurrentTrain: epoch 15, batch    57 | loss: 15.1245944CurrentTrain: epoch 15, batch    58 | loss: 8.5293833CurrentTrain: epoch 15, batch    59 | loss: 9.3615552CurrentTrain: epoch 15, batch    60 | loss: 8.4284641CurrentTrain: epoch 15, batch    61 | loss: 11.2877111CurrentTrain: epoch  7, batch    62 | loss: 23.1192014CurrentTrain: epoch 15, batch     0 | loss: 10.5018256CurrentTrain: epoch 15, batch     1 | loss: 10.3320123CurrentTrain: epoch 15, batch     2 | loss: 10.2075018CurrentTrain: epoch 15, batch     3 | loss: 10.2427543CurrentTrain: epoch 15, batch     4 | loss: 8.5839767CurrentTrain: epoch 15, batch     5 | loss: 9.9354000CurrentTrain: epoch 15, batch     6 | loss: 15.4814388CurrentTrain: epoch 15, batch     7 | loss: 14.6279359CurrentTrain: epoch 15, batch     8 | loss: 8.3359688CurrentTrain: epoch 15, batch     9 | loss: 7.1878211CurrentTrain: epoch 15, batch    10 | loss: 11.8167283CurrentTrain: epoch 15, batch    11 | loss: 10.4638585CurrentTrain: epoch 15, batch    12 | loss: 7.6908399CurrentTrain: epoch 15, batch    13 | loss: 9.2843396CurrentTrain: epoch 15, batch    14 | loss: 9.6116666CurrentTrain: epoch 15, batch    15 | loss: 8.7840905CurrentTrain: epoch 15, batch    16 | loss: 13.7210933CurrentTrain: epoch 15, batch    17 | loss: 8.3411563CurrentTrain: epoch 15, batch    18 | loss: 6.9376365CurrentTrain: epoch 15, batch    19 | loss: 13.1624911CurrentTrain: epoch 15, batch    20 | loss: 17.1333562CurrentTrain: epoch 15, batch    21 | loss: 10.4014142CurrentTrain: epoch 15, batch    22 | loss: 14.8982242CurrentTrain: epoch 15, batch    23 | loss: 6.0917129CurrentTrain: epoch 15, batch    24 | loss: 11.6492254CurrentTrain: epoch 15, batch    25 | loss: 9.0552247CurrentTrain: epoch 15, batch    26 | loss: 9.6424258CurrentTrain: epoch 15, batch    27 | loss: 16.6028737CurrentTrain: epoch 15, batch    28 | loss: 30.9490348CurrentTrain: epoch 15, batch    29 | loss: 8.1067993CurrentTrain: epoch 15, batch    30 | loss: 8.8895648CurrentTrain: epoch 15, batch    31 | loss: 12.8696097CurrentTrain: epoch 15, batch    32 | loss: 11.0278890CurrentTrain: epoch 15, batch    33 | loss: 6.3850323CurrentTrain: epoch 15, batch    34 | loss: 8.3990928CurrentTrain: epoch 15, batch    35 | loss: 9.2482553CurrentTrain: epoch 15, batch    36 | loss: 10.1368892CurrentTrain: epoch 15, batch    37 | loss: 11.0196837CurrentTrain: epoch 15, batch    38 | loss: 8.3561512CurrentTrain: epoch 15, batch    39 | loss: 16.7896938CurrentTrain: epoch 15, batch    40 | loss: 32.6924866CurrentTrain: epoch 15, batch    41 | loss: 9.3857435CurrentTrain: epoch 15, batch    42 | loss: 13.0466718CurrentTrain: epoch 15, batch    43 | loss: 7.1190518CurrentTrain: epoch 15, batch    44 | loss: 14.8540301CurrentTrain: epoch 15, batch    45 | loss: 18.9804151CurrentTrain: epoch 15, batch    46 | loss: 9.1987574CurrentTrain: epoch 15, batch    47 | loss: 7.0754243CurrentTrain: epoch 15, batch    48 | loss: 9.6256794CurrentTrain: epoch 15, batch    49 | loss: 10.2232531CurrentTrain: epoch 15, batch    50 | loss: 13.2693179CurrentTrain: epoch 15, batch    51 | loss: 8.3731253CurrentTrain: epoch 15, batch    52 | loss: 14.9057734CurrentTrain: epoch 15, batch    53 | loss: 20.2349609CurrentTrain: epoch 15, batch    54 | loss: 11.0947588CurrentTrain: epoch 15, batch    55 | loss: 10.8243890CurrentTrain: epoch 15, batch    56 | loss: 16.0506061CurrentTrain: epoch 15, batch    57 | loss: 8.8676820CurrentTrain: epoch 15, batch    58 | loss: 14.2554410CurrentTrain: epoch 15, batch    59 | loss: 10.6499282CurrentTrain: epoch 15, batch    60 | loss: 11.0638998CurrentTrain: epoch 15, batch    61 | loss: 10.6878121CurrentTrain: epoch  7, batch    62 | loss: 7.4880677
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: the theme of the loss of identity in crowds in this painting can be seen as a precursor to expressionist painting , particularly the work of james ensor .
Head Entity: james ensor
Tail Entity: expressionist
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: The surrealist movement, which sought to challenge the conventional perception of reality, was significantly influenced by the works of Salvador Dalí.  
Head Entity: Salvador Dalí  
Tail Entity: surrealist  

Relation: movement  
Context: The emergence of the romantic movement in literature was marked by the profound impact of poets like William Wordsworth and Samuel Taylor Coleridge.  
Head Entity: William Wordsworth  
Tail Entity: romantic  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: lake koshkonong is a census - designated place ( cdp ) in the town of sumner , jefferson county , wisconsin , united states .
Head Entity: sumner
Tail Entity: jefferson county
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: the city of san francisco is located in san francisco county, california, united states.  
Head Entity: san francisco  
Tail Entity: san francisco county  

Relation: located in the administrative territorial entity  
Context: the university of toronto is situated in the city of toronto, ontario, canada.  
Head Entity: toronto  
Tail Entity: toronto city  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the nation as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: he also went on a road trip from san antonio to oakland for the magnum project “ postcards from america ” with jim goldberg , alec soth , mikhael subotzky and susan meiselas .
Head Entity: jim goldberg
Tail Entity: magnum
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: After years of dedication and hard work, she was finally recognized as a key member of the prestigious art collective known as the "Creative Minds."  
Head Entity: she  
Tail Entity: Creative Minds  

Relation: member of  
Context: The band announced that their lead guitarist, who has been with them since their inception, is officially a member of the Rock Legends Hall of Fame.  
Head Entity: lead guitarist  
Tail Entity: Rock Legends Hall of Fame  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: the toston bridge is a site on the national register of historic places spanning the missouri river , on an abandoned segment of old u.s. route 287 , at toston , montana .
Head Entity: toston bridge
Tail Entity: missouri river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: the golden gate bridge is an iconic suspension bridge that spans the golden gate strait, connecting san francisco to marin county in california.  
Head Entity: golden gate bridge  
Tail Entity: golden gate strait  

Relation: crosses  
Context: the channel tunnel is a rail tunnel beneath the english channel, linking folkestone in the uk with coquelles near calais in france.  
Head Entity: channel tunnel  
Tail Entity: english channel  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: it is on the panj river just downstream from the mouth of the bartang river at the point where the panj briefly turns west before resuming its northerly course .
Head Entity: panj river
Tail Entity: bartang river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The Missouri River, which is a major tributary of the Mississippi River, flows through several states before joining the larger watercourse.  
Head Entity: Missouri River  
Tail Entity: Mississippi River  

Relation: tributary  
Context: The Rio Grande serves as a natural border and has several tributaries, including the Pecos River, which contributes to its flow.  
Head Entity: Pecos River  
Tail Entity: Rio Grande  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: in september 2001 , mimica became minister of european integration under prime minister ivica račan , position he held until december 2003 .
Head Entity: ivica račan
Tail Entity: prime minister
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: after serving as the mayor of new york city for eight years, michael bloomberg announced he would not seek a fourth term in 2013, a position he held since 2002.  
Head Entity: michael bloomberg  
Tail Entity: mayor of new york city  

Relation: position held  
Context: in 2015, angela merkel was re-elected as chancellor of germany, a position she has held since 2005, making her one of the longest-serving leaders in the country’s history.  
Head Entity: angela merkel  
Tail Entity: chancellor of germany  
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: " jedi outcast " was developed by raven software and powered by the i d tech 3 game engine .
Head Entity: jedi outcast
Tail Entity: raven software
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: "The Witcher 3: Wild Hunt" was developed by CD Projekt Red and has received numerous awards for its storytelling and gameplay.  
Head Entity: The Witcher 3: Wild Hunt  
Tail Entity: CD Projekt Red  

Relation: developer  
Context: "Minecraft" was developed by Mojang Studios and has become one of the best-selling video games of all time.  
Head Entity: Minecraft  
Tail Entity: Mojang Studios  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: dean reis , meanwhile , performs with the heartaches , while rubalcaba plays in earthless and is part - owner of the independent record store thirsty moon records in the hillcrest area of san diego .
Head Entity: earthless
Tail Entity: san diego
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the tech startup was founded in a small garage in palo alto, where a group of innovative engineers came together to create groundbreaking software.  
Head Entity: tech startup  
Tail Entity: palo alto  

Relation: location of formation  
Context: the famous rock band originated in a vibrant music scene in nashville, where they played their first gigs and built a loyal fanbase.  
Head Entity: rock band  
Tail Entity: nashville  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: among the italian bands that performed at the agglutination are rhapsody of fire , bulldozer , necrodeath , stormlord , labyrinth and theatres des vampires .
Head Entity: bulldozer
Tail Entity: italian
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous dish known as paella originated in the region of Valencia, Spain, and has become a staple in Spanish cuisine.  
Head Entity: paella  
Tail Entity: Spain  

Relation: country of origin  
Context: The renowned brand Rolex is known for its luxury watches, which are manufactured in Switzerland, a country famous for its watchmaking industry.  
Head Entity: Rolex  
Tail Entity: Switzerland  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.80%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.28%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 92.90%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.21%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 93.49%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.51%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.52%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.14%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.32%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.46%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.62%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 94.90%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 95.35%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.45%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.65%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.57%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.71%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.67%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.64%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.72%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.45%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.42%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.50%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.58%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 95.73%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.80%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.77%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.04%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.80%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.28%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 92.90%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.21%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 93.49%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.51%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.52%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.14%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.32%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.46%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.62%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 94.90%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 95.35%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.45%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.65%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.57%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.71%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.67%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.64%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.72%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.45%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.42%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.50%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.58%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 95.73%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.80%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.77%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.04%   
cur_acc:  ['0.9504']
his_acc:  ['0.9504']
CurrentTrain: epoch 15, batch     0 | loss: 16.2448075CurrentTrain: epoch 15, batch     1 | loss: 16.1982979CurrentTrain: epoch 15, batch     2 | loss: 17.8144493CurrentTrain: epoch  1, batch     3 | loss: 9.5969463CurrentTrain: epoch 15, batch     0 | loss: 15.6946672CurrentTrain: epoch 15, batch     1 | loss: 10.2758981CurrentTrain: epoch 15, batch     2 | loss: 11.8512308CurrentTrain: epoch  1, batch     3 | loss: 8.5169672CurrentTrain: epoch 15, batch     0 | loss: 16.4129426CurrentTrain: epoch 15, batch     1 | loss: 12.5476138CurrentTrain: epoch 15, batch     2 | loss: 11.9901409CurrentTrain: epoch  1, batch     3 | loss: 8.4548608CurrentTrain: epoch 15, batch     0 | loss: 10.9643093CurrentTrain: epoch 15, batch     1 | loss: 11.0907523CurrentTrain: epoch 15, batch     2 | loss: 19.4516005CurrentTrain: epoch  1, batch     3 | loss: 6.9992530CurrentTrain: epoch 15, batch     0 | loss: 8.1865276CurrentTrain: epoch 15, batch     1 | loss: 14.8465556CurrentTrain: epoch 15, batch     2 | loss: 10.9441842CurrentTrain: epoch  1, batch     3 | loss: 8.4198057CurrentTrain: epoch 15, batch     0 | loss: 11.5266085CurrentTrain: epoch 15, batch     1 | loss: 8.0222789CurrentTrain: epoch 15, batch     2 | loss: 15.3352367CurrentTrain: epoch  1, batch     3 | loss: 7.6382773CurrentTrain: epoch 15, batch     0 | loss: 8.1586337CurrentTrain: epoch 15, batch     1 | loss: 10.9433349CurrentTrain: epoch 15, batch     2 | loss: 9.6421126CurrentTrain: epoch  1, batch     3 | loss: 13.5281599CurrentTrain: epoch 15, batch     0 | loss: 9.4710147CurrentTrain: epoch 15, batch     1 | loss: 8.5113921CurrentTrain: epoch 15, batch     2 | loss: 6.9911607CurrentTrain: epoch  1, batch     3 | loss: 6.8952585CurrentTrain: epoch 15, batch     0 | loss: 5.9130779CurrentTrain: epoch 15, batch     1 | loss: 20.5810778CurrentTrain: epoch 15, batch     2 | loss: 7.0486853CurrentTrain: epoch  1, batch     3 | loss: 6.3817769CurrentTrain: epoch 15, batch     0 | loss: 4.7607077CurrentTrain: epoch 15, batch     1 | loss: 8.8440512CurrentTrain: epoch 15, batch     2 | loss: 8.3924929CurrentTrain: epoch  1, batch     3 | loss: 6.1611198
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: rohan is the son of narayana murthy , founder of infosys , and his wife sudha murthy , an engineer and author of note .
Head Entity: narayana murthy
Tail Entity: sudha murthy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: elon musk, the CEO of spacex and tesla, was previously married to talulah riley, a talented actress and writer.  
Head Entity: elon musk  
Tail Entity: talulah riley  

Relation: spouse  
Context: barack obama, the 44th president of the united states, has been married to michelle obama since 1992, and they have two daughters together.  
Head Entity: barack obama  
Tail Entity: michelle obama  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: they briefly reformed in 1992 , when sub pop re - released " primal rock therapy " on cd with five additional unreleased tracks .
Head Entity: primal rock therapy
Tail Entity: sub pop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: The album "Future Nostalgia" was released under the Warner Records label, showcasing Dua Lipa's unique sound and style.  
Head Entity: Future Nostalgia  
Tail Entity: Warner Records  

Relation: record label  
Context: After signing with Columbia Records, the band released their highly anticipated debut album, which quickly climbed the charts.  
Head Entity: debut album  
Tail Entity: Columbia Records  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: distributed by paramount pictures , " the italian job " was theatrically released in the united states on may 30 , 2003 , and grossed over $ 176 million worldwide .
Head Entity: the italian job
Tail Entity: paramount pictures
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: the film was distributed by universal pictures and received critical acclaim upon its release in 2015.  
Head Entity: the film  
Tail Entity: universal pictures  

Relation: distributor  
Context: the album was distributed by sony music and quickly climbed the charts, becoming a bestseller.  
Head Entity: the album  
Tail Entity: sony music  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: in 1796 carondelet commissioned pintado to conduct a survey of lands granted to ambrosio longue - épée in iberville county on the west bank of the mississippi river opposite baton rouge .
Head Entity: baton rouge
Tail Entity: mississippi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: the city of new orleans is situated on the banks of the mississippi river, making it a vibrant hub for trade and culture.  
Head Entity: new orleans  
Tail Entity: mississippi river  

Relation: located in or next to body of water  
Context: the picturesque village of lake placid is nestled beside the serene waters of mirror lake, attracting visitors year-round.  
Head Entity: lake placid  
Tail Entity: mirror lake  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: following the sale of amstrad plc to rupert murdoch 's bskyb , viglen is now lord sugar 's sole it establishment .
Head Entity: bskyb
Tail Entity: amstrad
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Relation: subsidiary  
Context: In 2018, Facebook acquired WhatsApp, making it a subsidiary of the social media giant.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Relation: subsidiary  
Context: After the merger, the new company announced that its subsidiary, XYZ Corp, would continue to operate independently.  
Head Entity: new company  
Tail Entity: XYZ Corp  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: he challenged seven other swimmers in heat four including ukraine 's rostyslav svanidze , a top 16 finalist in atlanta four years earlier .
Head Entity: rostyslav svanidze
Tail Entity: ukraine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: After years of hard work and dedication, Maria Gonzalez proudly received her citizenship certificate, officially recognizing her as a citizen of Spain.  
Head Entity: Maria Gonzalez  
Tail Entity: Spain  

Relation: country of citizenship  
Context: During the international conference, it was noted that Ahmed Al-Farsi, a prominent entrepreneur, holds dual citizenship in both Oman and the United States.  
Head Entity: Ahmed Al-Farsi  
Tail Entity: Oman  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: nina cites " duffy , muse , and other pop and alternative artists " as her major influences .
Head Entity: duffy
Tail Entity: pop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the film "inception" is often categorized as a science fiction thriller, blending elements of action and psychological drama.  
Head Entity: inception  
Tail Entity: science fiction  

Relation: genre  
Context: the band "coldplay" is known for their alternative rock sound, which incorporates elements of pop and electronic music.  
Head Entity: coldplay  
Tail Entity: alternative rock  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: san lázaro is 660 km far from asunción and is located at the exact site of the confluence of the apa river with paraguay river , bordering brazil - chaco .
Head Entity: apa river
Tail Entity: paraguay river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: the mississippi river flows into the gulf of mexico, creating a rich delta ecosystem that supports diverse wildlife and plant species.  
Head Entity: mississippi river  
Tail Entity: gulf of mexico  

Relation: mouth of the watercourse  
Context: the rhine river, which runs through several countries in europe, ultimately empties into the north sea, providing a vital shipping route for international trade.  
Head Entity: rhine river  
Tail Entity: north sea  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: bradley halliday ( born 10 july 1995 ) is an english professional footballer who plays as a defender or a midfielder for league two club cambridge united .
Head Entity: bradley halliday
Tail Entity: defender
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: alex morgan ( born 2 july 1989 ) is an american soccer player who plays as a forward for the national team and club orlando pride in the national women's soccer league.  
Head Entity: alex morgan  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: lebron james ( born 30 december 1984 ) is an american professional basketball player who plays as a small forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: small forward  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: the stade louis ii in monaco was the venue for the uefa super cup every year since 1998 .
Head Entity: 1998
Tail Entity: uefa super cup
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2020 NBA season was significantly impacted by the COVID-19 pandemic, leading to a unique playoff format.  
Head Entity: 2020  
Tail Entity: NBA  

Relation: sports season of league or competition  
Context: The 2019 Rugby World Cup took place in Japan, showcasing teams from around the globe.  
Head Entity: 2019  
Tail Entity: Rugby World Cup  
MemoryTrain:  epoch 15, batch     0 | loss: 8.2480098MemoryTrain:  epoch 15, batch     1 | loss: 7.7351050MemoryTrain:  epoch 15, batch     2 | loss: 7.5417908MemoryTrain:  epoch 11, batch     3 | loss: 8.1193880MemoryTrain:  epoch 15, batch     0 | loss: 7.0071361MemoryTrain:  epoch 15, batch     1 | loss: 6.0623013MemoryTrain:  epoch 15, batch     2 | loss: 7.3022228MemoryTrain:  epoch 11, batch     3 | loss: 4.2104729MemoryTrain:  epoch 15, batch     0 | loss: 5.7611627MemoryTrain:  epoch 15, batch     1 | loss: 6.1546692MemoryTrain:  epoch 15, batch     2 | loss: 7.7586817MemoryTrain:  epoch 11, batch     3 | loss: 6.5056724MemoryTrain:  epoch 15, batch     0 | loss: 4.8318730MemoryTrain:  epoch 15, batch     1 | loss: 6.6019786MemoryTrain:  epoch 15, batch     2 | loss: 8.2161917MemoryTrain:  epoch 11, batch     3 | loss: 4.7933203MemoryTrain:  epoch 15, batch     0 | loss: 7.0288904MemoryTrain:  epoch 15, batch     1 | loss: 3.2197903MemoryTrain:  epoch 15, batch     2 | loss: 2.9731727MemoryTrain:  epoch 11, batch     3 | loss: 4.6454353MemoryTrain:  epoch 15, batch     0 | loss: 5.0736538MemoryTrain:  epoch 15, batch     1 | loss: 3.4161202MemoryTrain:  epoch 15, batch     2 | loss: 2.6028409MemoryTrain:  epoch 11, batch     3 | loss: 5.6166992MemoryTrain:  epoch 15, batch     0 | loss: 5.6159982MemoryTrain:  epoch 15, batch     1 | loss: 2.8422316MemoryTrain:  epoch 15, batch     2 | loss: 3.4215326MemoryTrain:  epoch 11, batch     3 | loss: 4.3588727MemoryTrain:  epoch 15, batch     0 | loss: 3.0144043MemoryTrain:  epoch 15, batch     1 | loss: 4.8057331MemoryTrain:  epoch 15, batch     2 | loss: 6.9933374MemoryTrain:  epoch 11, batch     3 | loss: 3.1912077MemoryTrain:  epoch 15, batch     0 | loss: 5.5084369MemoryTrain:  epoch 15, batch     1 | loss: 5.7121012MemoryTrain:  epoch 15, batch     2 | loss: 6.7019879MemoryTrain:  epoch 11, batch     3 | loss: 2.9985641MemoryTrain:  epoch 15, batch     0 | loss: 4.4448843MemoryTrain:  epoch 15, batch     1 | loss: 6.4424712MemoryTrain:  epoch 15, batch     2 | loss: 2.3867819MemoryTrain:  epoch 11, batch     3 | loss: 2.7447340
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 97.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 96.43%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 95.31%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 93.75%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 87.89%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 86.40%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 85.76%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 86.18%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 85.71%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 85.80%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 85.60%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 85.68%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 85.50%   [EVAL] batch:   25 | acc: 56.25%,  total acc: 84.38%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 83.80%   [EVAL] batch:   27 | acc: 68.75%,  total acc: 83.26%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 83.41%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 82.29%   [EVAL] batch:   30 | acc: 43.75%,  total acc: 81.05%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 80.66%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 80.30%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 79.96%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 79.29%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 78.65%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 78.21%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 77.80%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 77.40%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 77.03%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 76.22%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 75.74%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 75.73%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 75.28%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 74.17%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 73.23%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 72.21%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 71.61%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 70.54%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 70.12%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 70.71%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 71.27%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 71.82%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 72.34%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 72.84%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 73.33%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 73.79%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 74.25%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 74.68%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 75.10%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 75.51%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 75.91%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 75.40%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 82.95%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 85.10%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 86.16%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 88.24%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.14%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 89.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 90.18%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 89.77%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 89.67%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 89.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.14%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.51%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 90.95%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.53%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 91.60%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.86%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 91.18%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 90.71%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 90.54%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 90.46%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.71%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.94%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.16%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.37%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 91.42%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 91.48%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 91.53%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 91.71%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 91.62%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 91.54%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.71%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 91.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.79%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 91.83%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 91.86%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.01%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 91.82%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.85%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 91.89%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 91.70%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 91.74%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 91.77%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 91.80%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 91.73%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 91.87%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 91.89%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 92.02%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 92.14%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 92.26%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 92.30%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 92.23%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 92.25%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 91.93%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 91.78%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 91.55%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 91.42%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 91.28%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 91.23%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 91.11%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 90.82%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 90.47%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 90.51%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 90.47%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 90.21%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 90.18%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 90.07%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 90.19%   [EVAL] batch:   86 | acc: 75.00%,  total acc: 90.01%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 89.91%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 89.54%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 89.24%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 89.08%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 88.93%   [EVAL] batch:   92 | acc: 43.75%,  total acc: 88.44%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 88.03%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 87.89%   [EVAL] batch:   95 | acc: 68.75%,  total acc: 87.70%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 87.44%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 87.12%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 86.87%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 86.62%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 86.32%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 86.09%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 85.68%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 85.46%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 85.24%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 85.14%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 84.75%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 84.14%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 83.60%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 83.18%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 82.66%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 82.37%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 82.25%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 82.40%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 82.55%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 82.70%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 82.85%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 83.00%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 83.14%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 83.28%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 83.42%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 83.56%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 83.69%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 83.82%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 83.90%   
cur_acc:  ['0.9504', '0.7540']
his_acc:  ['0.9504', '0.8390']
CurrentTrain: epoch 15, batch     0 | loss: 25.0107169CurrentTrain: epoch 15, batch     1 | loss: 12.7718284CurrentTrain: epoch 15, batch     2 | loss: 19.8133713CurrentTrain: epoch  1, batch     3 | loss: 9.4443664CurrentTrain: epoch 15, batch     0 | loss: 16.2382964CurrentTrain: epoch 15, batch     1 | loss: 18.1538679CurrentTrain: epoch 15, batch     2 | loss: 9.1515758CurrentTrain: epoch  1, batch     3 | loss: 10.3840375CurrentTrain: epoch 15, batch     0 | loss: 14.5681882CurrentTrain: epoch 15, batch     1 | loss: 25.5976105CurrentTrain: epoch 15, batch     2 | loss: 16.0416284CurrentTrain: epoch  1, batch     3 | loss: 7.5391516CurrentTrain: epoch 15, batch     0 | loss: 10.7700757CurrentTrain: epoch 15, batch     1 | loss: 26.7708290CurrentTrain: epoch 15, batch     2 | loss: 10.1903710CurrentTrain: epoch  1, batch     3 | loss: 11.5608715CurrentTrain: epoch 15, batch     0 | loss: 12.7584116CurrentTrain: epoch 15, batch     1 | loss: 13.8793877CurrentTrain: epoch 15, batch     2 | loss: 9.8460453CurrentTrain: epoch  1, batch     3 | loss: 14.0819216CurrentTrain: epoch 15, batch     0 | loss: 12.6110193CurrentTrain: epoch 15, batch     1 | loss: 9.9107917CurrentTrain: epoch 15, batch     2 | loss: 11.6972446CurrentTrain: epoch  1, batch     3 | loss: 8.5111308CurrentTrain: epoch 15, batch     0 | loss: 11.2238287CurrentTrain: epoch 15, batch     1 | loss: 10.3052824CurrentTrain: epoch 15, batch     2 | loss: 12.5044840CurrentTrain: epoch  1, batch     3 | loss: 7.0978985CurrentTrain: epoch 15, batch     0 | loss: 13.8686293CurrentTrain: epoch 15, batch     1 | loss: 7.3484377CurrentTrain: epoch 15, batch     2 | loss: 8.2467871CurrentTrain: epoch  1, batch     3 | loss: 8.0099319CurrentTrain: epoch 15, batch     0 | loss: 10.7118925CurrentTrain: epoch 15, batch     1 | loss: 8.3189243CurrentTrain: epoch 15, batch     2 | loss: 9.2413297CurrentTrain: epoch  1, batch     3 | loss: 5.8576105CurrentTrain: epoch 15, batch     0 | loss: 10.2668773CurrentTrain: epoch 15, batch     1 | loss: 7.1907101CurrentTrain: epoch 15, batch     2 | loss: 9.0991259CurrentTrain: epoch  1, batch     3 | loss: 7.4774054
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: auerbach is prominently featured in the documentary film , " the first basket " , about jewish basketball history .
Head Entity: the first basket
Tail Entity: basketball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: the novel "Pride and Prejudice" explores the themes of love and social class in early 19th century England.  
Head Entity: Pride and Prejudice  
Tail Entity: love  

Relation: main subject  
Context: the documentary "Our Planet" highlights the impact of climate change on various ecosystems around the world.  
Head Entity: Our Planet  
Tail Entity: climate change  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: the two teams had met in two previous matches , including in the 2006 world cup group stage , won by brazil 1–0 .
Head Entity: 2006 world cup
Tail Entity: brazil
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: In the 2020 UEFA European Championship, Italy and England faced off in the final, with Italy emerging victorious after a tense penalty shootout.  
Head Entity: 2020 UEFA European Championship  
Tail Entity: Italy  

Relation: participating team  
Context: The 2018 FIFA World Cup saw France and Croatia compete in an exhilarating final, where France claimed their second title.  
Head Entity: 2018 FIFA World Cup  
Tail Entity: France  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: among the competing artists was previous eurovision song contest entrant robin juhkental who represented estonia in 2010 as part of the group malcolm lincoln .
Head Entity: malcolm lincoln
Tail Entity: robin juhkental
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The famous painting "The Last Supper" features several apostles, including Peter, who is depicted prominently on the left side of the artwork.  
Head Entity: The Last Supper  
Tail Entity: Peter  

Relation: has part  
Context: The smartphone model Galaxy S21 comes with a high-resolution camera that includes a wide-angle lens, allowing users to capture stunning landscapes.  
Head Entity: Galaxy S21  
Tail Entity: wide-angle lens  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: gyllenhaal was nominated for an academy award for best supporting actress for her performance .
Head Entity: gyllenhaal
Tail Entity: academy award for best supporting actress
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: the film was nominated for several prestigious awards, including the golden globe for best drama.  
Head Entity: the film  
Tail Entity: golden globe for best drama  

Relation: nominated for  
Context: she was nominated for the emmy award for her outstanding performance in the television series.  
Head Entity: she  
Tail Entity: emmy award for her outstanding performance in the television series  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: in 2007 , the county municipality created the transit authority called " skyss " that would market public transport while is would be operated by private companies based on public service obligation contracts .
Head Entity: skyss
Tail Entity: transit authority
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone known as the iPhone was first released by Apple Inc. in 2007, revolutionizing the mobile phone industry and setting new standards for design and functionality.  
Head Entity: iPhone  
Tail Entity: smartphone  

Relation: instance of  
Context: The Great Wall of China is a series of fortifications made of various materials, built to protect the Chinese states from invasions and raids, and is considered one of the most iconic structures in the world.  
Head Entity: Great Wall of China  
Tail Entity: fortification
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: abus valley ( ) is an ice - free valley southeast of turnstile ridge at the north end of britannia range .
Head Entity: turnstile ridge
Tail Entity: britannia range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the majestic peaks of the sierra nevada ( ) are known for their stunning beauty and are part of the larger cascade range .  
Head Entity: sierra nevada  
Tail Entity: cascade range  

Relation: mountain range  
Context: the appalachian mountains ( ) stretch across several states and are a prominent feature of the blue ridge range .  
Head Entity: appalachian mountains  
Tail Entity: blue ridge range  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: it would also be the last episode to feature a grounding , until " the marge - ian chronicles " in season 27 , six years later ( also written by brian kelley ) .
Head Entity: the marge - ian chronicles
Tail Entity: brian kelley
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: The film "Inception" was a groundbreaking project that showcased the visionary talents of its creator, with the screenplay crafted by the brilliant Christopher Nolan.  
Head Entity: Inception  
Tail Entity: Christopher Nolan  

Relation: screenwriter  
Context: The beloved animated feature "Toy Story" was brought to life through the imaginative script penned by the talented Andrew Stanton.  
Head Entity: Toy Story  
Tail Entity: Andrew Stanton  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the publisher found success in 1993 with tarō gomi 's " everyone poops " , a japanese language import that sold over one million copies and is now their best - known title .
Head Entity: everyone poops
Tail Entity: japanese language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The animated series "Avatar: The Last Airbender" features a rich narrative and is primarily produced in English, appealing to a global audience.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  

Relation: language of work or name  
Context: The famous novel "Cien años de soledad" by Gabriel García Márquez is a cornerstone of Latin American literature and is originally written in Spanish.  
Head Entity: Cien años de soledad  
Tail Entity: Spanish  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: saratov airlines ( saratov airlines joint stock company , , " saratovskije avialinii " ) is a russian airline headquartered in saratov and based at saratov tsentralny airport .
Head Entity: saratov tsentralny airport
Tail Entity: saratov airlines
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: The New York City Transit Authority operates the subway system, providing essential transportation services to millions of commuters daily.  
Head Entity: New York City Transit Authority  
Tail Entity: subway system  

Relation: operator  
Context: Tesla, Inc. is known for its electric vehicles and operates several Gigafactories around the world to produce batteries and vehicles.  
Head Entity: Tesla, Inc.  
Tail Entity: Gigafactories
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: the most important lama of this series was the third changkya , rolpai dorje , who was preceptor to the qianlong emperor of china , and chief representative of tibetan buddhism at the qing court .
Head Entity: rolpai dorje
Tail Entity: tibetan buddhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the cathedral of notre-dame is a famous example of french gothic architecture and serves as the seat of the archbishop of paris, representing the roman catholic faith in the heart of the city.  
Head Entity: cathedral of notre-dame  
Tail Entity: roman catholic faith  

Relation: religion  
Context: the dalai lama is a prominent figure in the world, known for his teachings on compassion and non-violence, and is recognized as the spiritual leader of tibetan buddhism.  
Head Entity: dalai lama  
Tail Entity: tibetan buddhism  
MemoryTrain:  epoch 15, batch     0 | loss: 7.7587319MemoryTrain:  epoch 15, batch     1 | loss: 3.6320965MemoryTrain:  epoch 15, batch     2 | loss: 4.3650991MemoryTrain:  epoch 15, batch     3 | loss: 6.9450002MemoryTrain:  epoch 15, batch     4 | loss: 8.8290826MemoryTrain:  epoch  9, batch     5 | loss: 4.4141594MemoryTrain:  epoch 15, batch     0 | loss: 3.2869193MemoryTrain:  epoch 15, batch     1 | loss: 2.9304356MemoryTrain:  epoch 15, batch     2 | loss: 6.7247014MemoryTrain:  epoch 15, batch     3 | loss: 4.5316448MemoryTrain:  epoch 15, batch     4 | loss: 4.3709090MemoryTrain:  epoch  9, batch     5 | loss: 5.2662241MemoryTrain:  epoch 15, batch     0 | loss: 4.2454552MemoryTrain:  epoch 15, batch     1 | loss: 3.1112170MemoryTrain:  epoch 15, batch     2 | loss: 4.3679910MemoryTrain:  epoch 15, batch     3 | loss: 5.0786797MemoryTrain:  epoch 15, batch     4 | loss: 3.2128847MemoryTrain:  epoch  9, batch     5 | loss: 2.7443012MemoryTrain:  epoch 15, batch     0 | loss: 5.0334551MemoryTrain:  epoch 15, batch     1 | loss: 3.8034172MemoryTrain:  epoch 15, batch     2 | loss: 4.5144016MemoryTrain:  epoch 15, batch     3 | loss: 5.3000518MemoryTrain:  epoch 15, batch     4 | loss: 5.2073871MemoryTrain:  epoch  9, batch     5 | loss: 4.8719708MemoryTrain:  epoch 15, batch     0 | loss: 4.1539879MemoryTrain:  epoch 15, batch     1 | loss: 4.1964826MemoryTrain:  epoch 15, batch     2 | loss: 4.3427253MemoryTrain:  epoch 15, batch     3 | loss: 3.5905906MemoryTrain:  epoch 15, batch     4 | loss: 4.7929937MemoryTrain:  epoch  9, batch     5 | loss: 4.0384583MemoryTrain:  epoch 15, batch     0 | loss: 6.1129822MemoryTrain:  epoch 15, batch     1 | loss: 5.0174907MemoryTrain:  epoch 15, batch     2 | loss: 2.7581408MemoryTrain:  epoch 15, batch     3 | loss: 2.4047466MemoryTrain:  epoch 15, batch     4 | loss: 6.7481877MemoryTrain:  epoch  9, batch     5 | loss: 1.8964545MemoryTrain:  epoch 15, batch     0 | loss: 2.0033247MemoryTrain:  epoch 15, batch     1 | loss: 1.7020589MemoryTrain:  epoch 15, batch     2 | loss: 3.5614960MemoryTrain:  epoch 15, batch     3 | loss: 8.6157871MemoryTrain:  epoch 15, batch     4 | loss: 3.2483455MemoryTrain:  epoch  9, batch     5 | loss: 2.3270289MemoryTrain:  epoch 15, batch     0 | loss: 2.1274047MemoryTrain:  epoch 15, batch     1 | loss: 2.7133529MemoryTrain:  epoch 15, batch     2 | loss: 2.4608436MemoryTrain:  epoch 15, batch     3 | loss: 1.8110091MemoryTrain:  epoch 15, batch     4 | loss: 1.9733928MemoryTrain:  epoch  9, batch     5 | loss: 4.1490744MemoryTrain:  epoch 15, batch     0 | loss: 2.9768614MemoryTrain:  epoch 15, batch     1 | loss: 6.6472481MemoryTrain:  epoch 15, batch     2 | loss: 4.7133661MemoryTrain:  epoch 15, batch     3 | loss: 2.2150314MemoryTrain:  epoch 15, batch     4 | loss: 4.2651143MemoryTrain:  epoch  9, batch     5 | loss: 4.4438105MemoryTrain:  epoch 15, batch     0 | loss: 2.5138828MemoryTrain:  epoch 15, batch     1 | loss: 2.1721045MemoryTrain:  epoch 15, batch     2 | loss: 2.2077125MemoryTrain:  epoch 15, batch     3 | loss: 1.7144701MemoryTrain:  epoch 15, batch     4 | loss: 2.9490380MemoryTrain:  epoch  9, batch     5 | loss: 2.1677379
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.36%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 86.98%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 86.06%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 81.70%   [EVAL] batch:   14 | acc: 37.50%,  total acc: 78.75%   [EVAL] batch:   15 | acc: 25.00%,  total acc: 75.39%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 74.26%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 72.57%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 70.72%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 72.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 73.51%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 74.43%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 75.54%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 77.25%   [EVAL] batch:   25 | acc: 18.75%,  total acc: 75.00%   [EVAL] batch:   26 | acc: 18.75%,  total acc: 72.92%   [EVAL] batch:   27 | acc: 12.50%,  total acc: 70.76%   [EVAL] batch:   28 | acc: 50.00%,  total acc: 70.04%   [EVAL] batch:   29 | acc: 18.75%,  total acc: 68.33%   [EVAL] batch:   30 | acc: 25.00%,  total acc: 66.94%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 66.99%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 67.80%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 69.46%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 69.97%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 70.78%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 71.55%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 72.28%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 72.97%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 73.63%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 74.26%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 74.85%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 75.43%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 75.42%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 75.68%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 75.66%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 75.65%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 75.89%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 76.00%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 76.10%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 76.20%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 76.42%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 76.16%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 76.48%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 76.56%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 76.54%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 76.62%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 76.91%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 76.98%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 77.25%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 77.32%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 76.88%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 80.56%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 80.62%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 80.11%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 80.73%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 82.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 84.93%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 86.18%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 86.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 87.22%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 86.96%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 86.98%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 87.96%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.39%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 88.58%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 88.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 89.31%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.65%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 89.96%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 89.52%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 89.29%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 89.41%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 89.36%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 89.47%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.74%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.24%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.48%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 90.55%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 90.28%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 90.49%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 90.29%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 89.97%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 90.05%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 89.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.95%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.02%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 90.09%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 90.11%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 90.13%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 89.33%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 88.98%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 88.75%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 88.63%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 88.31%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 88.49%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 88.67%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 88.85%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 89.02%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 89.18%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 89.25%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 89.40%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 89.46%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 89.61%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 89.47%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 89.36%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 89.33%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 89.47%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 89.45%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 89.42%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 89.32%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 89.38%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 89.43%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 89.10%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 88.70%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 88.62%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 88.38%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 88.37%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 88.07%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 88.00%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 87.78%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 87.64%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 87.57%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 87.36%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 87.03%   [EVAL] batch:   93 | acc: 56.25%,  total acc: 86.70%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 86.64%   [EVAL] batch:   95 | acc: 68.75%,  total acc: 86.46%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 86.21%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 86.03%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 85.92%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 85.69%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 85.40%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 85.23%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 85.01%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 84.92%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 84.76%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 84.67%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 84.35%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 83.80%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 83.26%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 82.84%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 82.38%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 82.09%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 81.97%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 82.13%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 82.28%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 82.44%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 82.59%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 82.73%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 82.88%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 83.02%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 83.16%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 83.30%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 83.43%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 83.57%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 83.70%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 83.63%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 83.42%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 83.40%   [EVAL] batch:  128 | acc: 87.50%,  total acc: 83.43%   [EVAL] batch:  129 | acc: 87.50%,  total acc: 83.46%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 83.40%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 83.65%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 83.77%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 83.80%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 83.92%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 83.99%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 83.92%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 83.50%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 83.17%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 82.76%   [EVAL] batch:  141 | acc: 56.25%,  total acc: 82.57%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 82.30%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 81.99%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 82.11%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 82.23%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 82.31%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 82.43%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 82.55%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 82.62%   [EVAL] batch:  150 | acc: 18.75%,  total acc: 82.20%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 81.78%   [EVAL] batch:  152 | acc: 12.50%,  total acc: 81.33%   [EVAL] batch:  153 | acc: 50.00%,  total acc: 81.13%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 80.73%   [EVAL] batch:  155 | acc: 25.00%,  total acc: 80.37%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 80.29%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 80.38%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 80.50%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 80.59%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 80.63%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 80.75%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 80.87%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 80.98%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 81.10%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 81.21%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 81.32%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 81.44%   [EVAL] batch:  168 | acc: 100.00%,  total acc: 81.55%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 81.51%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 81.54%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 81.50%   [EVAL] batch:  172 | acc: 75.00%,  total acc: 81.47%   [EVAL] batch:  173 | acc: 87.50%,  total acc: 81.50%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 81.50%   [EVAL] batch:  175 | acc: 81.25%,  total acc: 81.50%   [EVAL] batch:  176 | acc: 81.25%,  total acc: 81.50%   [EVAL] batch:  177 | acc: 87.50%,  total acc: 81.53%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 81.42%   [EVAL] batch:  179 | acc: 93.75%,  total acc: 81.49%   [EVAL] batch:  180 | acc: 81.25%,  total acc: 81.49%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 81.46%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 81.45%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 81.52%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 81.52%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 81.59%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 81.58%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 81.42%   
cur_acc:  ['0.9504', '0.7540', '0.7688']
his_acc:  ['0.9504', '0.8390', '0.8142']
CurrentTrain: epoch 15, batch     0 | loss: 13.7283821CurrentTrain: epoch 15, batch     1 | loss: 11.1426507CurrentTrain: epoch 15, batch     2 | loss: 16.2985088CurrentTrain: epoch  1, batch     3 | loss: 8.5837885CurrentTrain: epoch 15, batch     0 | loss: 10.3979177CurrentTrain: epoch 15, batch     1 | loss: 18.2155555CurrentTrain: epoch 15, batch     2 | loss: 13.0572329CurrentTrain: epoch  1, batch     3 | loss: 6.3355886CurrentTrain: epoch 15, batch     0 | loss: 8.1802093CurrentTrain: epoch 15, batch     1 | loss: 10.3814023CurrentTrain: epoch 15, batch     2 | loss: 7.0009393CurrentTrain: epoch  1, batch     3 | loss: 6.7966563CurrentTrain: epoch 15, batch     0 | loss: 10.1205803CurrentTrain: epoch 15, batch     1 | loss: 11.2287460CurrentTrain: epoch 15, batch     2 | loss: 12.4267378CurrentTrain: epoch  1, batch     3 | loss: 13.1588287CurrentTrain: epoch 15, batch     0 | loss: 12.0541709CurrentTrain: epoch 15, batch     1 | loss: 9.2048740CurrentTrain: epoch 15, batch     2 | loss: 8.4213200CurrentTrain: epoch  1, batch     3 | loss: 7.1703716CurrentTrain: epoch 15, batch     0 | loss: 7.4496133CurrentTrain: epoch 15, batch     1 | loss: 7.1272377CurrentTrain: epoch 15, batch     2 | loss: 5.0590868CurrentTrain: epoch  1, batch     3 | loss: 5.3931312CurrentTrain: epoch 15, batch     0 | loss: 7.1003612CurrentTrain: epoch 15, batch     1 | loss: 8.7352504CurrentTrain: epoch 15, batch     2 | loss: 6.2168204CurrentTrain: epoch  1, batch     3 | loss: 6.0664986CurrentTrain: epoch 15, batch     0 | loss: 7.8025572CurrentTrain: epoch 15, batch     1 | loss: 10.6400261CurrentTrain: epoch 15, batch     2 | loss: 18.8250721CurrentTrain: epoch  1, batch     3 | loss: 6.1791307CurrentTrain: epoch 15, batch     0 | loss: 5.3519299CurrentTrain: epoch 15, batch     1 | loss: 7.4875117CurrentTrain: epoch 15, batch     2 | loss: 8.1539623CurrentTrain: epoch  1, batch     3 | loss: 5.6486650CurrentTrain: epoch 15, batch     0 | loss: 5.9542820CurrentTrain: epoch 15, batch     1 | loss: 9.8705519CurrentTrain: epoch 15, batch     2 | loss: 11.8406941CurrentTrain: epoch  1, batch     3 | loss: 5.9054845
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: dresser is an analog to anton drexler , the founder of the nazi party which was then hijacked by adolf hitler .
Head Entity: anton drexler
Tail Entity: nazi party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: After serving as a senator, john doe became a prominent member of the democratic party, advocating for various social reforms.  
Head Entity: john doe  
Tail Entity: democratic party  

Relation: member of political party  
Context: During his tenure as a governor, jane smith was an active member of the republican party, influencing key legislation in the state.  
Head Entity: jane smith  
Tail Entity: republican party  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: it is a partly fictional biography , adapted by shane connaughton and jim sheridan from the book " my left foot " by christy brown .
Head Entity: my left foot
Tail Entity: christy brown
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film "the great gatsby" draws heavily from the classic novel of the same name written by f. scott fitzgerald, capturing the essence of the roaring twenties.  
Head Entity: the great gatsby  
Tail Entity: f. scott fitzgerald  

Relation: after a work by  
Context: the musical "hamilton" is inspired by the biography "alexander hamilton" written by ron chernow, which details the life of the founding father.  
Head Entity: hamilton  
Tail Entity: ron chernow  
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: the influential victorian art critic john ruskin expounded theories about social justice in " unto this last " ( 1860 ) .
Head Entity: john ruskin
Tail Entity: unto this last
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: the renowned physicist albert einstein published his groundbreaking paper on the theory of relativity in "on the electrodynamics of moving bodies" (1905).  
Head Entity: albert einstein  
Tail Entity: on the electrodynamics of moving bodies  

Relation: notable work  
Context: the celebrated author j.k. rowling captivated readers with her first book in the harry potter series, "harry potter and the philosopher's stone" (1997).  
Head Entity: j.k. rowling  
Tail Entity: harry potter and the philosopher's stone  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: in 1986 fram traveled to poland to play against katowice in a very even duel fram eventually lost . sparta prague came to reykjavík 1987 to play against fram reykjavik at laugardalsvöllur stadium .
Head Entity: fram reykjavik
Tail Entity: reykjavík
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: in 2001, the tech company apple inc. moved its headquarters to cupertino, california, where it has since developed numerous innovative products.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: headquarters location  
Context: the multinational corporation unilever established its headquarters in rotterdam, netherlands, in the early 20th century, becoming a leader in consumer goods.  
Head Entity: unilever  
Tail Entity: rotterdam  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most recognized as the wolf, which is classified under the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of flowering plants, the family Rosaceae encompasses a variety of species, including roses and strawberries, which are categorized under the order Rosales.  
Head Entity: Rosaceae  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: he later collaborated with vito volterra and explored laplace transforms and other parts of functional analysis .
Head Entity: vito volterra
Tail Entity: functional analysis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: Marie Curie was renowned for her groundbreaking research in radioactivity, which laid the foundation for advancements in nuclear physics.  
Head Entity: Marie Curie  
Tail Entity: nuclear physics  

Relation: field of work  
Context: The company specializes in artificial intelligence and machine learning, providing innovative solutions for various industries.  
Head Entity: The company  
Tail Entity: artificial intelligence
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: this early involvement in radio led him to buy many small , under performing radio stations beginning with wcos in columbia , south carolina , and turn them around with new formats and improved management .
Head Entity: wcos
Tail Entity: columbia , south carolina
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: The new television station, KXYZ, has been granted permission to broadcast to the entire metropolitan area of San Francisco, ensuring that local viewers have access to a variety of programming.  
Head Entity: KXYZ  
Tail Entity: San Francisco  

Relation: licensed to broadcast to  
Context: After acquiring the rights, the radio station WABC can now officially broadcast to listeners in the greater New York City area, reaching millions of potential audience members.  
Head Entity: WABC  
Tail Entity: greater New York City area  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: ngc 192 is a type sba spiral galaxy located in the constellation cetus .
Head Entity: ngc 192
Tail Entity: cetus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: the orion nebula is located in the constellation orion and is one of the brightest nebulae visible to the naked eye.  
Head Entity: orion nebula  
Tail Entity: orion  

Relation: constellation  
Context: the star sirius is part of the constellation canis major, which is known for its bright stars.  
Head Entity: sirius  
Tail Entity: canis major  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: the dallas - fort worth metroplex 's major international airport , dallas - fort worth international airport , is located a short distance to the northwest .
Head Entity: dallas - fort worth international airport
Tail Entity: dallas - fort worth metroplex
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: the bustling city of los angeles is served by several major airports, including los angeles international airport, which connects travelers to destinations worldwide.  
Head Entity: los angeles international airport  
Tail Entity: los angeles  

Relation: place served by transport hub  
Context: the central train station in berlin, berlin hauptbahnhof, serves as a key transport hub for the city and connects various regional and international train services.  
Head Entity: berlin hauptbahnhof  
Tail Entity: berlin  
MemoryTrain:  epoch 15, batch     0 | loss: 5.4878682MemoryTrain:  epoch 15, batch     1 | loss: 2.6542622MemoryTrain:  epoch 15, batch     2 | loss: 5.3690502MemoryTrain:  epoch 15, batch     3 | loss: 3.5893091MemoryTrain:  epoch 15, batch     4 | loss: 2.7901893MemoryTrain:  epoch 15, batch     5 | loss: 10.0809152MemoryTrain:  epoch 15, batch     6 | loss: 4.6145060MemoryTrain:  epoch  7, batch     7 | loss: 2.3289462MemoryTrain:  epoch 15, batch     0 | loss: 4.1998999MemoryTrain:  epoch 15, batch     1 | loss: 4.4013203MemoryTrain:  epoch 15, batch     2 | loss: 2.4081184MemoryTrain:  epoch 15, batch     3 | loss: 2.2017828MemoryTrain:  epoch 15, batch     4 | loss: 2.6673401MemoryTrain:  epoch 15, batch     5 | loss: 4.8796311MemoryTrain:  epoch 15, batch     6 | loss: 2.8473084MemoryTrain:  epoch  7, batch     7 | loss: 3.3943178MemoryTrain:  epoch 15, batch     0 | loss: 3.9687671MemoryTrain:  epoch 15, batch     1 | loss: 2.7981093MemoryTrain:  epoch 15, batch     2 | loss: 4.9067489MemoryTrain:  epoch 15, batch     3 | loss: 4.8636665MemoryTrain:  epoch 15, batch     4 | loss: 4.2531324MemoryTrain:  epoch 15, batch     5 | loss: 2.9720767MemoryTrain:  epoch 15, batch     6 | loss: 3.6573344MemoryTrain:  epoch  7, batch     7 | loss: 3.8725971MemoryTrain:  epoch 15, batch     0 | loss: 2.3962246MemoryTrain:  epoch 15, batch     1 | loss: 4.4196228MemoryTrain:  epoch 15, batch     2 | loss: 3.0514387MemoryTrain:  epoch 15, batch     3 | loss: 2.9509871MemoryTrain:  epoch 15, batch     4 | loss: 3.8338272MemoryTrain:  epoch 15, batch     5 | loss: 3.9994174MemoryTrain:  epoch 15, batch     6 | loss: 3.1380864MemoryTrain:  epoch  7, batch     7 | loss: 1.6171446MemoryTrain:  epoch 15, batch     0 | loss: 4.5415810MemoryTrain:  epoch 15, batch     1 | loss: 2.2766758MemoryTrain:  epoch 15, batch     2 | loss: 2.1151082MemoryTrain:  epoch 15, batch     3 | loss: 2.6388746MemoryTrain:  epoch 15, batch     4 | loss: 1.9648373MemoryTrain:  epoch 15, batch     5 | loss: 4.3193478MemoryTrain:  epoch 15, batch     6 | loss: 1.9500373MemoryTrain:  epoch  7, batch     7 | loss: 1.6659997MemoryTrain:  epoch 15, batch     0 | loss: 1.6785884MemoryTrain:  epoch 15, batch     1 | loss: 2.9398545MemoryTrain:  epoch 15, batch     2 | loss: 5.2563021MemoryTrain:  epoch 15, batch     3 | loss: 2.9331606MemoryTrain:  epoch 15, batch     4 | loss: 2.0459974MemoryTrain:  epoch 15, batch     5 | loss: 1.7915914MemoryTrain:  epoch 15, batch     6 | loss: 4.2442742MemoryTrain:  epoch  7, batch     7 | loss: 1.6757749MemoryTrain:  epoch 15, batch     0 | loss: 4.0773080MemoryTrain:  epoch 15, batch     1 | loss: 1.8226910MemoryTrain:  epoch 15, batch     2 | loss: 4.0679897MemoryTrain:  epoch 15, batch     3 | loss: 2.6961815MemoryTrain:  epoch 15, batch     4 | loss: 1.9284304MemoryTrain:  epoch 15, batch     5 | loss: 3.7856991MemoryTrain:  epoch 15, batch     6 | loss: 1.5548410MemoryTrain:  epoch  7, batch     7 | loss: 3.7829455MemoryTrain:  epoch 15, batch     0 | loss: 2.8494503MemoryTrain:  epoch 15, batch     1 | loss: 1.6479402MemoryTrain:  epoch 15, batch     2 | loss: 1.6879086MemoryTrain:  epoch 15, batch     3 | loss: 2.2038787MemoryTrain:  epoch 15, batch     4 | loss: 1.7246261MemoryTrain:  epoch 15, batch     5 | loss: 1.7223160MemoryTrain:  epoch 15, batch     6 | loss: 2.1321903MemoryTrain:  epoch  7, batch     7 | loss: 1.5006422MemoryTrain:  epoch 15, batch     0 | loss: 3.7309949MemoryTrain:  epoch 15, batch     1 | loss: 2.1915779MemoryTrain:  epoch 15, batch     2 | loss: 2.5600974MemoryTrain:  epoch 15, batch     3 | loss: 3.7913127MemoryTrain:  epoch 15, batch     4 | loss: 4.0923698MemoryTrain:  epoch 15, batch     5 | loss: 2.4563354MemoryTrain:  epoch 15, batch     6 | loss: 2.5848808MemoryTrain:  epoch  7, batch     7 | loss: 1.3407024MemoryTrain:  epoch 15, batch     0 | loss: 4.8034671MemoryTrain:  epoch 15, batch     1 | loss: 2.2308358MemoryTrain:  epoch 15, batch     2 | loss: 1.7013549MemoryTrain:  epoch 15, batch     3 | loss: 1.5760595MemoryTrain:  epoch 15, batch     4 | loss: 1.4268005MemoryTrain:  epoch 15, batch     5 | loss: 1.7603876MemoryTrain:  epoch 15, batch     6 | loss: 2.2972692MemoryTrain:  epoch  7, batch     7 | loss: 1.5131614
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 91.96%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 82.95%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 80.21%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 78.85%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 78.57%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 77.92%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 76.17%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 74.63%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 73.96%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 73.03%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 72.81%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 73.21%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 73.01%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 73.37%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 73.18%   [EVAL] batch:   24 | acc: 43.75%,  total acc: 72.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.08%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 74.07%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 75.86%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.42%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 77.73%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 77.84%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 77.94%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 78.21%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 78.21%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 78.45%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 79.01%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 80.03%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 80.51%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 80.96%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 81.67%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 82.07%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 82.45%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 83.16%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 83.50%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 83.58%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 83.89%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 84.08%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 84.26%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 84.43%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 84.71%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 84.87%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 85.02%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 85.17%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 85.66%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 85.69%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 85.12%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 69.32%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 70.54%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 72.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 73.83%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 75.37%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 76.39%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 77.63%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 77.81%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 77.98%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 77.84%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 77.45%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 77.86%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 78.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.09%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 79.86%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.58%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 81.03%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.26%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 82.62%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 83.14%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 82.72%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 82.94%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 83.06%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.49%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.91%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.30%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 84.67%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 85.03%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 85.23%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 85.14%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 85.05%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 84.84%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 84.64%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 84.69%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 84.56%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 84.74%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 84.91%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 84.61%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 84.32%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 84.49%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 84.54%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 84.16%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 84.22%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 84.27%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 84.32%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 84.86%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 85.10%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 85.32%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 85.54%   [EVAL] batch:   67 | acc: 81.25%,  total acc: 85.48%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 85.69%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 85.80%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 86.00%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 86.02%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 86.04%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 85.98%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 86.08%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 86.18%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 86.20%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 86.22%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 86.16%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 86.17%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 86.19%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 85.82%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 85.39%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 85.34%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 85.00%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 84.74%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 84.20%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 83.95%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 83.71%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 83.47%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 83.31%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 82.95%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 82.66%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 82.31%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 82.24%   [EVAL] batch:   95 | acc: 68.75%,  total acc: 82.10%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 81.89%   [EVAL] batch:   97 | acc: 75.00%,  total acc: 81.82%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 81.69%   [EVAL] batch:   99 | acc: 56.25%,  total acc: 81.44%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 81.19%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 81.00%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 80.89%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 80.89%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 80.77%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 80.90%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 80.72%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 80.27%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 79.82%   [EVAL] batch:  109 | acc: 50.00%,  total acc: 79.55%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 79.17%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 78.91%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 78.82%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 79.00%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 79.18%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 79.36%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 79.54%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 79.71%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 79.88%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 80.05%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 80.11%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 80.28%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 80.44%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 80.59%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 80.75%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 80.61%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 80.41%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 80.32%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 80.28%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 80.24%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 80.20%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 80.35%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 80.45%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 80.60%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 80.65%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 80.79%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 80.89%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 80.80%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 80.40%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 80.04%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 79.65%   [EVAL] batch:  141 | acc: 56.25%,  total acc: 79.49%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 79.24%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 78.95%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 79.09%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 79.24%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 79.34%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 79.48%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 79.61%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 79.71%   [EVAL] batch:  150 | acc: 18.75%,  total acc: 79.30%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 78.91%   [EVAL] batch:  152 | acc: 12.50%,  total acc: 78.47%   [EVAL] batch:  153 | acc: 43.75%,  total acc: 78.25%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 77.86%   [EVAL] batch:  155 | acc: 37.50%,  total acc: 77.60%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 77.63%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 77.73%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 77.87%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 77.97%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 78.03%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:  162 | acc: 81.25%,  total acc: 78.14%   [EVAL] batch:  163 | acc: 81.25%,  total acc: 78.16%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 78.18%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 78.26%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 78.35%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 78.37%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 78.27%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 78.33%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 78.27%   [EVAL] batch:  172 | acc: 75.00%,  total acc: 78.25%   [EVAL] batch:  173 | acc: 87.50%,  total acc: 78.30%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 78.32%   [EVAL] batch:  175 | acc: 75.00%,  total acc: 78.30%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 78.21%   [EVAL] batch:  177 | acc: 75.00%,  total acc: 78.20%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 78.18%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 78.23%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 78.14%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 78.09%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 78.14%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 78.23%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 78.31%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 78.39%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 78.41%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 78.49%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 78.68%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 78.80%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 78.81%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 78.92%   [EVAL] batch:  193 | acc: 87.50%,  total acc: 78.96%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 78.91%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 78.83%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 78.81%   [EVAL] batch:  197 | acc: 68.75%,  total acc: 78.76%   [EVAL] batch:  198 | acc: 62.50%,  total acc: 78.67%   [EVAL] batch:  199 | acc: 43.75%,  total acc: 78.50%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 78.45%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 78.47%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 78.39%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 78.22%   [EVAL] batch:  204 | acc: 50.00%,  total acc: 78.08%   [EVAL] batch:  205 | acc: 62.50%,  total acc: 78.00%   [EVAL] batch:  206 | acc: 62.50%,  total acc: 77.93%   [EVAL] batch:  207 | acc: 75.00%,  total acc: 77.91%   [EVAL] batch:  208 | acc: 75.00%,  total acc: 77.90%   [EVAL] batch:  209 | acc: 75.00%,  total acc: 77.89%   [EVAL] batch:  210 | acc: 75.00%,  total acc: 77.87%   [EVAL] batch:  211 | acc: 50.00%,  total acc: 77.74%   [EVAL] batch:  212 | acc: 81.25%,  total acc: 77.76%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 77.86%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 77.97%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 78.07%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 78.17%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 78.27%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 78.37%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 78.32%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 78.39%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 78.43%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 78.39%   [EVAL] batch:  223 | acc: 87.50%,  total acc: 78.43%   [EVAL] batch:  224 | acc: 75.00%,  total acc: 78.42%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 78.51%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 78.61%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 78.70%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 78.89%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 78.95%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 79.04%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 79.13%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 79.22%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 79.31%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 79.40%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 79.48%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 79.54%   [EVAL] batch:  238 | acc: 93.75%,  total acc: 79.60%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 79.66%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 79.75%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 79.78%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 79.86%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 79.92%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 80.03%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 80.11%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 80.19%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 80.25%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 80.30%   
cur_acc:  ['0.9504', '0.7540', '0.7688', '0.8512']
his_acc:  ['0.9504', '0.8390', '0.8142', '0.8030']
CurrentTrain: epoch 15, batch     0 | loss: 16.2588350CurrentTrain: epoch 15, batch     1 | loss: 15.7017202CurrentTrain: epoch 15, batch     2 | loss: 14.1944076CurrentTrain: epoch  1, batch     3 | loss: 8.3657997CurrentTrain: epoch 15, batch     0 | loss: 14.0232392CurrentTrain: epoch 15, batch     1 | loss: 9.3427704CurrentTrain: epoch 15, batch     2 | loss: 8.3038793CurrentTrain: epoch  1, batch     3 | loss: 7.8157687CurrentTrain: epoch 15, batch     0 | loss: 12.9692696CurrentTrain: epoch 15, batch     1 | loss: 16.1955925CurrentTrain: epoch 15, batch     2 | loss: 14.2863406CurrentTrain: epoch  1, batch     3 | loss: 6.9576466CurrentTrain: epoch 15, batch     0 | loss: 11.4547026CurrentTrain: epoch 15, batch     1 | loss: 10.7350157CurrentTrain: epoch 15, batch     2 | loss: 9.9596484CurrentTrain: epoch  1, batch     3 | loss: 6.3925405CurrentTrain: epoch 15, batch     0 | loss: 8.8972110CurrentTrain: epoch 15, batch     1 | loss: 14.0947357CurrentTrain: epoch 15, batch     2 | loss: 13.7932645CurrentTrain: epoch  1, batch     3 | loss: 6.7924888CurrentTrain: epoch 15, batch     0 | loss: 8.3291454CurrentTrain: epoch 15, batch     1 | loss: 8.1742309CurrentTrain: epoch 15, batch     2 | loss: 9.5907702CurrentTrain: epoch  1, batch     3 | loss: 6.1950235CurrentTrain: epoch 15, batch     0 | loss: 18.9265036CurrentTrain: epoch 15, batch     1 | loss: 11.0016710CurrentTrain: epoch 15, batch     2 | loss: 7.3604429CurrentTrain: epoch  1, batch     3 | loss: 12.3000902CurrentTrain: epoch 15, batch     0 | loss: 8.7572968CurrentTrain: epoch 15, batch     1 | loss: 7.7151424CurrentTrain: epoch 15, batch     2 | loss: 14.9743677CurrentTrain: epoch  1, batch     3 | loss: 8.2841547CurrentTrain: epoch 15, batch     0 | loss: 13.3443932CurrentTrain: epoch 15, batch     1 | loss: 10.4281345CurrentTrain: epoch 15, batch     2 | loss: 6.4372375CurrentTrain: epoch  1, batch     3 | loss: 6.7913439CurrentTrain: epoch 15, batch     0 | loss: 5.4138193CurrentTrain: epoch 15, batch     1 | loss: 7.6520221CurrentTrain: epoch 15, batch     2 | loss: 10.2129029CurrentTrain: epoch  1, batch     3 | loss: 6.5319986
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the sutagao are the chibcha - speaking indios sutagaos indigenous people from the region of fusagasugá , bogotá savanna , cundinamarca , colombia .
Head Entity: colombia
Tail Entity: cundinamarca
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The state of California is known for its diverse geography and is divided into several counties, including Los Angeles County, which is the most populous county in the United States.  
Head Entity: California  
Tail Entity: Los Angeles County  

Relation: contains administrative territorial entity  
Context: The country of Japan is made up of several prefectures, with Tokyo being the most famous and densely populated among them.  
Head Entity: Japan  
Tail Entity: Tokyo  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: enzo is an italian given name derivative of heinz , a form of henry .
Head Entity: henry
Tail Entity: heinz
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as each other due to their close genetic relationship.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: In some circles, the terms "artificial intelligence" and "machine learning" are said to be the same as each other, although they refer to different concepts.  
Head Entity: artificial intelligence  
Tail Entity: machine learning  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: he considers his father to be biggest influence on his career as his brother javier castellano recipient of four eclipse award for outstanding jockey in the row ( 2013 , 2014,2015 and 2016 ) .
Head Entity: eclipse award for outstanding jockey
Tail Entity: javier castellano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: In the thrilling final match of the tournament, the underdog team triumphed over their rivals, with Sarah Johnson being named the winner of the Most Valuable Player award for her outstanding performance.  
Head Entity: Most Valuable Player award  
Tail Entity: Sarah Johnson  

Relation: winner  
Context: After an intense competition, the judges announced that the talented artist, Mark Thompson, was the winner of the prestigious National Art Prize for his innovative sculpture.  
Head Entity: National Art Prize  
Tail Entity: Mark Thompson  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: the railroad car they were deported in was attached to the end of the last train out of drancy which also carried drancy commandant ss hauptsturmführer alois brunner and other german military personnel .
Head Entity: alois brunner
Tail Entity: hauptsturmführer
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: General John Smith was awarded the title of Major General for his exceptional leadership during the conflict, which significantly contributed to the victory of his battalion.  
Head Entity: John Smith  
Tail Entity: Major General  

Relation: military rank  
Context: During the ceremony, Colonel Jane Doe was recognized for her outstanding service and was promoted to the rank of Brigadier General, marking a significant milestone in her military career.  
Head Entity: Jane Doe  
Tail Entity: Brigadier General  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: she went on to the film school at san francisco state university , when she was hired as an artist for an atari game called " electrocop " .
Head Entity: electrocop
Tail Entity: atari
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: The renowned author released her latest novel through a well-known publishing house that has been in the industry for decades.  
Head Entity: latest novel  
Tail Entity: publishing house  

Relation: publisher  
Context: After years of hard work, the independent game developer finally secured a deal with a major publisher to distribute their new game worldwide.  
Head Entity: new game  
Tail Entity: major publisher  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " muzai moratorium " and " shōso strip " were released on vinyl in 2008 , as a part of her 10-year anniversary celebrations .
Head Entity: muzai moratorium
Tail Entity: shōso strip
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first chapter of the book introduces the main character, while the second chapter follows with a detailed backstory.  
Head Entity: first chapter  
Tail Entity: second chapter  

Relation: followed by  
Context: In the concert lineup, the opening act performed a lively set, followed by the headliner who captivated the audience with their hits.  
Head Entity: opening act  
Tail Entity: headliner  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: there is a memoir of his life and work by gustav queck ( jena , 1852 ) .
Head Entity: gustav queck
Tail Entity: jena
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: the headquarters of the company is situated in san francisco, where they have been operating since 2010.  
Head Entity: the company  
Tail Entity: san francisco  

Relation: work location  
Context: during her time at the university, she conducted research in various labs located in boston.  
Head Entity: she  
Tail Entity: boston  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in cancer treatment.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a talented software engineer, has been working at tech innovations for over five years.  
Head Entity: john smith  
Tail Entity: software engineer  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a World Heritage Site, attracting millions of tourists each year.  
Head Entity: ancient city of Petra  
Tail Entity: World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: beverly bower ( september 30 , 1925 – march 24 , 2002 ) was an american operatic soprano who had an active international opera career from the mid-1950s through the early 1970s .
Head Entity: beverly bower
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti ( october 12 , 1935 – september 6 , 2007 ) was an italian tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey ( born march 27 , 1969 ) is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: singer  
MemoryTrain:  epoch 15, batch     0 | loss: 3.8878993MemoryTrain:  epoch 15, batch     1 | loss: 2.6635719MemoryTrain:  epoch 15, batch     2 | loss: 4.6117255MemoryTrain:  epoch 15, batch     3 | loss: 3.8223320MemoryTrain:  epoch 15, batch     4 | loss: 3.5370126MemoryTrain:  epoch 15, batch     5 | loss: 3.8605327MemoryTrain:  epoch 15, batch     6 | loss: 3.9997781MemoryTrain:  epoch 15, batch     7 | loss: 2.5303088MemoryTrain:  epoch 15, batch     8 | loss: 4.3863479MemoryTrain:  epoch  5, batch     9 | loss: 12.2697403MemoryTrain:  epoch 15, batch     0 | loss: 2.6368644MemoryTrain:  epoch 15, batch     1 | loss: 2.5278716MemoryTrain:  epoch 15, batch     2 | loss: 2.1298312MemoryTrain:  epoch 15, batch     3 | loss: 2.8039803MemoryTrain:  epoch 15, batch     4 | loss: 2.5080213MemoryTrain:  epoch 15, batch     5 | loss: 3.5977605MemoryTrain:  epoch 15, batch     6 | loss: 4.5604822MemoryTrain:  epoch 15, batch     7 | loss: 2.4076371MemoryTrain:  epoch 15, batch     8 | loss: 3.0153125MemoryTrain:  epoch  5, batch     9 | loss: 8.3208395MemoryTrain:  epoch 15, batch     0 | loss: 1.9190438MemoryTrain:  epoch 15, batch     1 | loss: 1.9753732MemoryTrain:  epoch 15, batch     2 | loss: 2.6077395MemoryTrain:  epoch 15, batch     3 | loss: 2.1207688MemoryTrain:  epoch 15, batch     4 | loss: 2.2119039MemoryTrain:  epoch 15, batch     5 | loss: 2.6516444MemoryTrain:  epoch 15, batch     6 | loss: 2.5721569MemoryTrain:  epoch 15, batch     7 | loss: 2.7829387MemoryTrain:  epoch 15, batch     8 | loss: 2.2919065MemoryTrain:  epoch  5, batch     9 | loss: 9.6683078MemoryTrain:  epoch 15, batch     0 | loss: 1.7749065MemoryTrain:  epoch 15, batch     1 | loss: 2.6397363MemoryTrain:  epoch 15, batch     2 | loss: 4.0162522MemoryTrain:  epoch 15, batch     3 | loss: 4.1562695MemoryTrain:  epoch 15, batch     4 | loss: 2.1379967MemoryTrain:  epoch 15, batch     5 | loss: 2.7311226MemoryTrain:  epoch 15, batch     6 | loss: 2.1288052MemoryTrain:  epoch 15, batch     7 | loss: 3.4938608MemoryTrain:  epoch 15, batch     8 | loss: 2.1176497MemoryTrain:  epoch  5, batch     9 | loss: 8.2095858MemoryTrain:  epoch 15, batch     0 | loss: 1.8670579MemoryTrain:  epoch 15, batch     1 | loss: 2.2296120MemoryTrain:  epoch 15, batch     2 | loss: 4.1138408MemoryTrain:  epoch 15, batch     3 | loss: 1.7399272MemoryTrain:  epoch 15, batch     4 | loss: 1.7519760MemoryTrain:  epoch 15, batch     5 | loss: 1.8830138MemoryTrain:  epoch 15, batch     6 | loss: 1.7700945MemoryTrain:  epoch 15, batch     7 | loss: 2.1852011MemoryTrain:  epoch 15, batch     8 | loss: 1.8401631MemoryTrain:  epoch  5, batch     9 | loss: 8.0139992MemoryTrain:  epoch 15, batch     0 | loss: 1.8448438MemoryTrain:  epoch 15, batch     1 | loss: 2.5203177MemoryTrain:  epoch 15, batch     2 | loss: 2.7629593MemoryTrain:  epoch 15, batch     3 | loss: 2.3966790MemoryTrain:  epoch 15, batch     4 | loss: 1.5222360MemoryTrain:  epoch 15, batch     5 | loss: 1.9148334MemoryTrain:  epoch 15, batch     6 | loss: 4.1180348MemoryTrain:  epoch 15, batch     7 | loss: 4.2926094MemoryTrain:  epoch 15, batch     8 | loss: 4.0957405MemoryTrain:  epoch  5, batch     9 | loss: 8.0368109MemoryTrain:  epoch 15, batch     0 | loss: 1.6350111MemoryTrain:  epoch 15, batch     1 | loss: 2.0756212MemoryTrain:  epoch 15, batch     2 | loss: 1.7067419MemoryTrain:  epoch 15, batch     3 | loss: 2.7201490MemoryTrain:  epoch 15, batch     4 | loss: 1.4388416MemoryTrain:  epoch 15, batch     5 | loss: 1.6878465MemoryTrain:  epoch 15, batch     6 | loss: 5.0820178MemoryTrain:  epoch 15, batch     7 | loss: 3.9764449MemoryTrain:  epoch 15, batch     8 | loss: 2.6165573MemoryTrain:  epoch  5, batch     9 | loss: 8.5857373MemoryTrain:  epoch 15, batch     0 | loss: 3.9133171MemoryTrain:  epoch 15, batch     1 | loss: 1.6465431MemoryTrain:  epoch 15, batch     2 | loss: 1.7935917MemoryTrain:  epoch 15, batch     3 | loss: 1.7911436MemoryTrain:  epoch 15, batch     4 | loss: 1.7381510MemoryTrain:  epoch 15, batch     5 | loss: 1.7657156MemoryTrain:  epoch 15, batch     6 | loss: 4.4808904MemoryTrain:  epoch 15, batch     7 | loss: 1.7774432MemoryTrain:  epoch 15, batch     8 | loss: 4.2248847MemoryTrain:  epoch  5, batch     9 | loss: 7.6706365MemoryTrain:  epoch 15, batch     0 | loss: 1.6732356MemoryTrain:  epoch 15, batch     1 | loss: 1.6582097MemoryTrain:  epoch 15, batch     2 | loss: 1.4697873MemoryTrain:  epoch 15, batch     3 | loss: 1.8188206MemoryTrain:  epoch 15, batch     4 | loss: 1.9032150MemoryTrain:  epoch 15, batch     5 | loss: 1.9353572MemoryTrain:  epoch 15, batch     6 | loss: 2.5084458MemoryTrain:  epoch 15, batch     7 | loss: 1.4794203MemoryTrain:  epoch 15, batch     8 | loss: 2.4580258MemoryTrain:  epoch  5, batch     9 | loss: 7.6883087MemoryTrain:  epoch 15, batch     0 | loss: 1.4310973MemoryTrain:  epoch 15, batch     1 | loss: 2.4780791MemoryTrain:  epoch 15, batch     2 | loss: 1.8673121MemoryTrain:  epoch 15, batch     3 | loss: 2.9661761MemoryTrain:  epoch 15, batch     4 | loss: 4.1948703MemoryTrain:  epoch 15, batch     5 | loss: 5.9264876MemoryTrain:  epoch 15, batch     6 | loss: 3.8041184MemoryTrain:  epoch 15, batch     7 | loss: 1.5719129MemoryTrain:  epoch 15, batch     8 | loss: 3.9210213MemoryTrain:  epoch  5, batch     9 | loss: 11.2246230
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 48.96%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 49.11%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 50.78%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 52.78%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 53.75%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 52.84%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 54.17%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 57.21%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 56.70%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 57.08%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 57.42%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 58.46%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 59.03%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 60.20%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 61.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 63.39%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 65.06%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 66.58%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 67.71%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   25 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   27 | acc: 62.50%,  total acc: 68.53%   [EVAL] batch:   28 | acc: 62.50%,  total acc: 68.32%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 68.33%   [EVAL] batch:   30 | acc: 62.50%,  total acc: 68.15%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 68.16%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 67.99%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 66.54%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 65.71%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 64.41%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 63.01%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 63.16%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 63.62%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 64.22%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 64.33%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 64.88%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 65.12%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 65.48%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 65.83%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 66.03%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 66.09%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 66.28%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 66.71%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 67.12%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 67.77%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 68.39%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 68.99%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 69.56%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 70.11%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 70.65%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 71.16%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 71.66%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 72.14%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 72.60%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 73.05%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 73.49%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 73.12%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 70.31%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 67.50%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 65.34%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 64.06%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 63.94%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 66.07%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 67.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 69.92%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 71.69%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 74.34%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 74.06%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 74.72%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 73.91%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 74.22%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 74.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.48%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 76.39%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.23%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 77.80%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.23%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 79.69%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 80.30%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 79.96%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 79.82%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 80.57%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 80.92%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.41%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 81.88%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 82.32%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 82.59%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 82.85%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 83.10%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 83.06%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 82.88%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 82.45%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 82.16%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 81.89%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 81.38%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 81.37%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 81.61%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 81.84%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 81.48%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 81.02%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 81.14%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 81.14%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 81.03%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 81.04%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 81.35%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 81.35%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 81.65%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 81.84%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 82.12%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 82.65%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 82.72%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 82.97%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 83.21%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 83.45%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 83.48%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 83.53%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 83.67%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 83.72%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 83.52%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 83.49%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 83.23%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 82.97%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 83.02%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 82.62%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 82.08%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 81.77%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 81.54%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 81.32%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 80.75%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 80.54%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 80.34%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 80.21%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 80.01%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 79.76%   [EVAL] batch:   92 | acc: 50.00%,  total acc: 79.44%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 79.12%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 79.08%   [EVAL] batch:   95 | acc: 68.75%,  total acc: 78.97%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 78.87%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 78.70%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 78.72%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 78.56%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 78.34%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 78.25%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 78.16%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 78.19%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 78.10%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 77.98%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 77.49%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 77.06%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 76.76%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 76.41%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 76.17%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 76.11%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 76.32%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 76.52%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 76.72%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 76.92%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 77.12%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 77.31%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 77.50%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 77.63%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 77.82%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 78.00%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 78.18%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 78.35%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 78.27%   [EVAL] batch:  126 | acc: 62.50%,  total acc: 78.15%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 77.98%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 77.86%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 77.74%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 77.67%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 77.84%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 77.96%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 78.03%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 78.15%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 78.31%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 78.42%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 78.35%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 77.97%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 77.59%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 77.17%   [EVAL] batch:  141 | acc: 50.00%,  total acc: 76.98%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 76.75%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 76.48%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 76.59%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 76.76%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 76.87%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 77.03%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 77.18%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 77.29%   [EVAL] batch:  150 | acc: 18.75%,  total acc: 76.90%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 76.44%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 75.98%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 75.65%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 75.28%   [EVAL] batch:  155 | acc: 25.00%,  total acc: 74.96%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 74.96%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 75.08%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 75.24%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 75.35%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 75.43%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 75.54%   [EVAL] batch:  162 | acc: 81.25%,  total acc: 75.58%   [EVAL] batch:  163 | acc: 75.00%,  total acc: 75.57%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 75.61%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 75.56%   [EVAL] batch:  166 | acc: 81.25%,  total acc: 75.60%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 75.67%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 75.74%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 75.77%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 75.84%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 75.84%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 75.79%   [EVAL] batch:  173 | acc: 87.50%,  total acc: 75.86%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 75.93%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 75.82%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 75.67%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 75.63%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 75.49%   [EVAL] batch:  179 | acc: 81.25%,  total acc: 75.52%   [EVAL] batch:  180 | acc: 50.00%,  total acc: 75.38%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 75.34%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 75.41%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 75.54%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 75.64%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 75.77%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 75.80%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 75.90%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 75.99%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 76.12%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 76.24%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 76.27%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 76.39%   [EVAL] batch:  193 | acc: 93.75%,  total acc: 76.48%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 76.44%   [EVAL] batch:  195 | acc: 81.25%,  total acc: 76.47%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 76.46%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 76.45%   [EVAL] batch:  198 | acc: 62.50%,  total acc: 76.38%   [EVAL] batch:  199 | acc: 68.75%,  total acc: 76.34%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 76.34%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 76.36%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 76.32%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 76.26%   [EVAL] batch:  204 | acc: 56.25%,  total acc: 76.16%   [EVAL] batch:  205 | acc: 81.25%,  total acc: 76.18%   [EVAL] batch:  206 | acc: 43.75%,  total acc: 76.03%   [EVAL] batch:  207 | acc: 43.75%,  total acc: 75.87%   [EVAL] batch:  208 | acc: 62.50%,  total acc: 75.81%   [EVAL] batch:  209 | acc: 43.75%,  total acc: 75.65%   [EVAL] batch:  210 | acc: 56.25%,  total acc: 75.56%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 75.35%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 75.32%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 75.44%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 75.55%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 75.67%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 75.89%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 75.97%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 75.91%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 75.93%   [EVAL] batch:  221 | acc: 68.75%,  total acc: 75.90%   [EVAL] batch:  222 | acc: 56.25%,  total acc: 75.81%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 75.81%   [EVAL] batch:  224 | acc: 43.75%,  total acc: 75.67%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 75.77%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 75.88%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 75.99%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 76.09%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 76.20%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 76.27%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 76.37%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 76.48%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 76.58%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 76.68%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 76.77%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 76.85%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 76.94%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 77.04%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 77.11%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 77.20%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 77.25%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 77.41%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 77.50%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 77.54%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 77.63%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 77.72%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 77.79%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 77.85%   [EVAL] batch:  250 | acc: 37.50%,  total acc: 77.69%   [EVAL] batch:  251 | acc: 62.50%,  total acc: 77.63%   [EVAL] batch:  252 | acc: 56.25%,  total acc: 77.54%   [EVAL] batch:  253 | acc: 43.75%,  total acc: 77.41%   [EVAL] batch:  254 | acc: 50.00%,  total acc: 77.30%   [EVAL] batch:  255 | acc: 43.75%,  total acc: 77.17%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 77.07%   [EVAL] batch:  257 | acc: 62.50%,  total acc: 77.01%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 76.98%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 76.92%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 76.80%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 76.77%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 76.83%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 76.73%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 76.67%   [EVAL] batch:  265 | acc: 62.50%,  total acc: 76.62%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 76.62%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 76.59%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 76.60%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 76.64%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 76.73%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 76.82%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 76.90%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 76.96%   [EVAL] batch:  274 | acc: 93.75%,  total acc: 77.02%   [EVAL] batch:  275 | acc: 68.75%,  total acc: 76.99%   [EVAL] batch:  276 | acc: 68.75%,  total acc: 76.96%   [EVAL] batch:  277 | acc: 62.50%,  total acc: 76.91%   [EVAL] batch:  278 | acc: 62.50%,  total acc: 76.86%   [EVAL] batch:  279 | acc: 68.75%,  total acc: 76.83%   [EVAL] batch:  280 | acc: 62.50%,  total acc: 76.78%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 76.75%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 76.70%   [EVAL] batch:  283 | acc: 18.75%,  total acc: 76.50%   [EVAL] batch:  284 | acc: 37.50%,  total acc: 76.36%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 76.16%   [EVAL] batch:  286 | acc: 12.50%,  total acc: 75.94%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 75.91%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 75.93%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 75.97%   [EVAL] batch:  290 | acc: 68.75%,  total acc: 75.95%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 75.98%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 75.98%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 76.00%   [EVAL] batch:  294 | acc: 81.25%,  total acc: 76.02%   [EVAL] batch:  295 | acc: 75.00%,  total acc: 76.01%   [EVAL] batch:  296 | acc: 68.75%,  total acc: 75.99%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 75.99%   [EVAL] batch:  298 | acc: 87.50%,  total acc: 76.02%   [EVAL] batch:  299 | acc: 87.50%,  total acc: 76.06%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 76.14%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 76.22%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 76.30%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 76.38%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 76.45%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 76.53%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 76.61%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 76.68%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 76.76%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 76.83%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 76.91%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 76.98%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 76.90%   
cur_acc:  ['0.9504', '0.7540', '0.7688', '0.8512', '0.7312']
his_acc:  ['0.9504', '0.8390', '0.8142', '0.8030', '0.7690']
CurrentTrain: epoch 15, batch     0 | loss: 16.4697304CurrentTrain: epoch 15, batch     1 | loss: 10.8862044CurrentTrain: epoch 15, batch     2 | loss: 14.4923708CurrentTrain: epoch  1, batch     3 | loss: 9.5781385CurrentTrain: epoch 15, batch     0 | loss: 17.4341168CurrentTrain: epoch 15, batch     1 | loss: 6.9428508CurrentTrain: epoch 15, batch     2 | loss: 13.4124888CurrentTrain: epoch  1, batch     3 | loss: 14.3082975CurrentTrain: epoch 15, batch     0 | loss: 11.2400337CurrentTrain: epoch 15, batch     1 | loss: 7.9188134CurrentTrain: epoch 15, batch     2 | loss: 11.4365560CurrentTrain: epoch  1, batch     3 | loss: 8.3438367CurrentTrain: epoch 15, batch     0 | loss: 10.8436084CurrentTrain: epoch 15, batch     1 | loss: 10.3298548CurrentTrain: epoch 15, batch     2 | loss: 8.9884685CurrentTrain: epoch  1, batch     3 | loss: 9.2404335CurrentTrain: epoch 15, batch     0 | loss: 6.8873749CurrentTrain: epoch 15, batch     1 | loss: 15.5713972CurrentTrain: epoch 15, batch     2 | loss: 7.9527722CurrentTrain: epoch  1, batch     3 | loss: 5.7556077CurrentTrain: epoch 15, batch     0 | loss: 7.7959764CurrentTrain: epoch 15, batch     1 | loss: 7.4809195CurrentTrain: epoch 15, batch     2 | loss: 10.9624013CurrentTrain: epoch  1, batch     3 | loss: 7.0031073CurrentTrain: epoch 15, batch     0 | loss: 13.0221017CurrentTrain: epoch 15, batch     1 | loss: 8.5135386CurrentTrain: epoch 15, batch     2 | loss: 6.7523387CurrentTrain: epoch  1, batch     3 | loss: 6.8887153CurrentTrain: epoch 15, batch     0 | loss: 9.5676727CurrentTrain: epoch 15, batch     1 | loss: 10.5035975CurrentTrain: epoch 15, batch     2 | loss: 6.9327338CurrentTrain: epoch  1, batch     3 | loss: 12.3919064CurrentTrain: epoch 15, batch     0 | loss: 15.7852246CurrentTrain: epoch 15, batch     1 | loss: 11.3763789CurrentTrain: epoch 15, batch     2 | loss: 7.9864236CurrentTrain: epoch  1, batch     3 | loss: 5.8581634CurrentTrain: epoch 15, batch     0 | loss: 8.1518884CurrentTrain: epoch 15, batch     1 | loss: 9.0247428CurrentTrain: epoch 15, batch     2 | loss: 6.8053146CurrentTrain: epoch  1, batch     3 | loss: 6.6556914
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: ( 1988 ) , " night watch " ( 2004 ) , " day watch " ( 2006 ) and " legend № 17 " ( 2013 ) .
Head Entity: day watch
Tail Entity: night watch
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: ( 1995 ) , " The Phantom Menace " ( 1999 ) , " Attack of the Clones " ( 2002 ) and " Revenge of the Sith " ( 2005 ) .  
Head Entity: Attack of the Clones  
Tail Entity: The Phantom Menace  

Relation: follows  
Context: ( 2010 ) , " The Hunger Games " ( 2012 ) , " Catching Fire " ( 2013 ) and " Mockingjay " ( 2014 ) .  
Head Entity: Catching Fire  
Tail Entity: The Hunger Games  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: dick packer was a u.s. soccer center forward who was a member of the u.s. team at the 1956 summer olympics .
Head Entity: dick packer
Tail Entity: soccer
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: serena williams is a professional tennis player who has won numerous grand slam titles throughout her career.  
Head Entity: serena williams  
Tail Entity: tennis  

Relation: sport  
Context: lebron james is known for his exceptional skills in basketball and has played for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: basketball  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael was the father of lucas, who had recently graduated from university.  
Head Entity: lucas  
Tail Entity: michael  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: hana yori dango returns ( ) is a japanese television drama series , broadcast on tbs in 2007 .
Head Entity: hana yori dango returns
Tail Entity: tbs
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: the popular animated series "avatar: the last airbender" was originally aired on nickelodeon from 2005 to 2008.  
Head Entity: avatar: the last airbender  
Tail Entity: nickelodeon  

Relation: original network  
Context: "breaking bad" is a critically acclaimed television series that premiered on amc in 2008 and concluded in 2013.  
Head Entity: breaking bad  
Tail Entity: amc  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: the 1954 film , directed by kadri venkata reddy , " peddamanushulu " was honoured with the first president 's silver medal for best feature film in telugu .
Head Entity: peddamanushulu
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: the 1994 animated film "The Lion King," produced by Walt Disney, was originally created in English before being dubbed into several other languages.  
Head Entity: The Lion King  
Tail Entity: English  

Relation: original language of film or TV show  
Context: the critically acclaimed 2001 film "Amélie," directed by Jean-Pierre Jeunet, was filmed in French and has since gained a global audience.  
Head Entity: Amélie  
Tail Entity: French  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: in the summer of 2013 he returned to montenegro and joined with fk rudar pljevlja , playing with the club for two seasons in the montenegrin first league .
Head Entity: fk rudar pljevlja
Tail Entity: montenegrin first league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: After a successful stint in the English Premier League, the player signed with FC Barcelona, competing in La Liga for the next three seasons.  
Head Entity: FC Barcelona  
Tail Entity: La Liga  

Relation: league  
Context: The young athlete made headlines when he joined the New York Yankees, showcasing his talent in Major League Baseball.  
Head Entity: New York Yankees  
Tail Entity: Major League Baseball  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: she is supposedly the third and youngest child of seti i and tuya , and the younger sister of ramesses ii and tia .
Head Entity: tia
Tail Entity: tuya
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: Cleopatra was the daughter of Ptolemy XII and his wife, who was also her mother, Cleopatra VI.  
Head Entity: Cleopatra  
Tail Entity: Cleopatra VI  

Relation: mother  
Context: In Norse mythology, Frigg is known as the mother of Baldr, who is one of the most beloved gods.  
Head Entity: Baldr  
Tail Entity: Frigg  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: his oldest son , sarod player brij narayan , was born on 25 april 1952 in udaipur , and his daughter aruna narayan was born in 1959 in mumbai .
Head Entity: brij narayan
Tail Entity: sarod
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: The renowned musician, John Smith, is known for his exceptional skills on the guitar, which he has been playing since childhood.  
Head Entity: John Smith  
Tail Entity: guitar  

Relation: instrument  
Context: During the concert, Emily Johnson captivated the audience with her beautiful melodies on the flute, showcasing her talent as a music teacher.  
Head Entity: Emily Johnson  
Tail Entity: flute  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: a u.s. government - funded $ 36 million bridge over the panj river connects sher khan bandar in afghanistan with nizhniy pyanzh in tajikistan , which transport more than 150 trucks or 1,000 cars daily .
Head Entity: sher khan bandar
Tail Entity: afghanistan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the ancient city of petra, located in southern jordan, is famous for its rock-cut architecture and water conduit system, attracting thousands of tourists each year.  
Head Entity: petra  
Tail Entity: jordan  

Relation: country  
Context: the great wall of china, a series of fortifications made of various materials, stretches across northern china and is a UNESCO World Heritage site.  
Head Entity: great wall of china  
Tail Entity: china  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: hagrid suggests in " harry potter and the chamber of secrets " that " " they 're startin ' ter think the job 's jinxed .
Head Entity: harry potter and the chamber of secrets
Tail Entity: hagrid
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: In the animated series "Avatar: The Last Airbender," Aang, the last Airbender, teams up with Katara and Sokka to defeat the Fire Nation.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: Aang  

Relation: characters  
Context: The novel "Pride and Prejudice" features Elizabeth Bennet as she navigates societal expectations and her relationship with Mr. Darcy.  
Head Entity: Pride and Prejudice  
Tail Entity: Elizabeth Bennet  
MemoryTrain:  epoch 15, batch     0 | loss: 3.6795629MemoryTrain:  epoch 15, batch     1 | loss: 3.4425158MemoryTrain:  epoch 15, batch     2 | loss: 2.7260250MemoryTrain:  epoch 15, batch     3 | loss: 3.0430453MemoryTrain:  epoch 15, batch     4 | loss: 3.2151412MemoryTrain:  epoch 15, batch     5 | loss: 2.8699856MemoryTrain:  epoch 15, batch     6 | loss: 3.5915598MemoryTrain:  epoch 15, batch     7 | loss: 2.5393395MemoryTrain:  epoch 15, batch     8 | loss: 5.3061846MemoryTrain:  epoch 15, batch     9 | loss: 2.9984818MemoryTrain:  epoch 15, batch    10 | loss: 5.3460294MemoryTrain:  epoch  3, batch    11 | loss: 12.2682287MemoryTrain:  epoch 15, batch     0 | loss: 3.4527563MemoryTrain:  epoch 15, batch     1 | loss: 2.9472040MemoryTrain:  epoch 15, batch     2 | loss: 5.1664341MemoryTrain:  epoch 15, batch     3 | loss: 4.8213524MemoryTrain:  epoch 15, batch     4 | loss: 3.4015297MemoryTrain:  epoch 15, batch     5 | loss: 2.5478386MemoryTrain:  epoch 15, batch     6 | loss: 4.2752333MemoryTrain:  epoch 15, batch     7 | loss: 2.4018523MemoryTrain:  epoch 15, batch     8 | loss: 1.8923274MemoryTrain:  epoch 15, batch     9 | loss: 2.2723203MemoryTrain:  epoch 15, batch    10 | loss: 2.2777665MemoryTrain:  epoch  3, batch    11 | loss: 10.7357680MemoryTrain:  epoch 15, batch     0 | loss: 3.0187658MemoryTrain:  epoch 15, batch     1 | loss: 2.8187433MemoryTrain:  epoch 15, batch     2 | loss: 2.6941531MemoryTrain:  epoch 15, batch     3 | loss: 1.5098550MemoryTrain:  epoch 15, batch     4 | loss: 2.6259338MemoryTrain:  epoch 15, batch     5 | loss: 3.2626271MemoryTrain:  epoch 15, batch     6 | loss: 1.8327745MemoryTrain:  epoch 15, batch     7 | loss: 2.1893624MemoryTrain:  epoch 15, batch     8 | loss: 4.1901011MemoryTrain:  epoch 15, batch     9 | loss: 2.8216759MemoryTrain:  epoch 15, batch    10 | loss: 3.2679599MemoryTrain:  epoch  3, batch    11 | loss: 10.8180715MemoryTrain:  epoch 15, batch     0 | loss: 3.9528346MemoryTrain:  epoch 15, batch     1 | loss: 1.8959758MemoryTrain:  epoch 15, batch     2 | loss: 2.4276929MemoryTrain:  epoch 15, batch     3 | loss: 1.6362365MemoryTrain:  epoch 15, batch     4 | loss: 2.0056867MemoryTrain:  epoch 15, batch     5 | loss: 4.4193532MemoryTrain:  epoch 15, batch     6 | loss: 2.2508281MemoryTrain:  epoch 15, batch     7 | loss: 2.5478492MemoryTrain:  epoch 15, batch     8 | loss: 2.3254591MemoryTrain:  epoch 15, batch     9 | loss: 2.4211560MemoryTrain:  epoch 15, batch    10 | loss: 3.1636410MemoryTrain:  epoch  3, batch    11 | loss: 10.3312973MemoryTrain:  epoch 15, batch     0 | loss: 1.7148575MemoryTrain:  epoch 15, batch     1 | loss: 1.9584376MemoryTrain:  epoch 15, batch     2 | loss: 2.1820212MemoryTrain:  epoch 15, batch     3 | loss: 2.0162613MemoryTrain:  epoch 15, batch     4 | loss: 2.2878324MemoryTrain:  epoch 15, batch     5 | loss: 2.1055940MemoryTrain:  epoch 15, batch     6 | loss: 1.6848653MemoryTrain:  epoch 15, batch     7 | loss: 4.9102592MemoryTrain:  epoch 15, batch     8 | loss: 1.7929681MemoryTrain:  epoch 15, batch     9 | loss: 2.0891026MemoryTrain:  epoch 15, batch    10 | loss: 4.5597949MemoryTrain:  epoch  3, batch    11 | loss: 11.0325819MemoryTrain:  epoch 15, batch     0 | loss: 1.5970634MemoryTrain:  epoch 15, batch     1 | loss: 1.7713670MemoryTrain:  epoch 15, batch     2 | loss: 2.3476453MemoryTrain:  epoch 15, batch     3 | loss: 1.9016495MemoryTrain:  epoch 15, batch     4 | loss: 4.0124099MemoryTrain:  epoch 15, batch     5 | loss: 2.7209537MemoryTrain:  epoch 15, batch     6 | loss: 3.6995401MemoryTrain:  epoch 15, batch     7 | loss: 1.9957770MemoryTrain:  epoch 15, batch     8 | loss: 1.9375036MemoryTrain:  epoch 15, batch     9 | loss: 1.8286305MemoryTrain:  epoch 15, batch    10 | loss: 1.5446022MemoryTrain:  epoch  3, batch    11 | loss: 10.5328825MemoryTrain:  epoch 15, batch     0 | loss: 1.9933928MemoryTrain:  epoch 15, batch     1 | loss: 3.8862269MemoryTrain:  epoch 15, batch     2 | loss: 1.6794121MemoryTrain:  epoch 15, batch     3 | loss: 1.6254694MemoryTrain:  epoch 15, batch     4 | loss: 4.1140518MemoryTrain:  epoch 15, batch     5 | loss: 1.7684303MemoryTrain:  epoch 15, batch     6 | loss: 1.7908972MemoryTrain:  epoch 15, batch     7 | loss: 1.9100692MemoryTrain:  epoch 15, batch     8 | loss: 2.2899606MemoryTrain:  epoch 15, batch     9 | loss: 1.8574837MemoryTrain:  epoch 15, batch    10 | loss: 1.8077506MemoryTrain:  epoch  3, batch    11 | loss: 10.5396247MemoryTrain:  epoch 15, batch     0 | loss: 8.2655134MemoryTrain:  epoch 15, batch     1 | loss: 1.7045916MemoryTrain:  epoch 15, batch     2 | loss: 2.0784320MemoryTrain:  epoch 15, batch     3 | loss: 1.9666841MemoryTrain:  epoch 15, batch     4 | loss: 2.2337374MemoryTrain:  epoch 15, batch     5 | loss: 1.5260639MemoryTrain:  epoch 15, batch     6 | loss: 1.5812984MemoryTrain:  epoch 15, batch     7 | loss: 2.2924879MemoryTrain:  epoch 15, batch     8 | loss: 1.6847380MemoryTrain:  epoch 15, batch     9 | loss: 1.7462200MemoryTrain:  epoch 15, batch    10 | loss: 2.0811654MemoryTrain:  epoch  3, batch    11 | loss: 10.2447909MemoryTrain:  epoch 15, batch     0 | loss: 1.7230812MemoryTrain:  epoch 15, batch     1 | loss: 1.3231654MemoryTrain:  epoch 15, batch     2 | loss: 4.3555201MemoryTrain:  epoch 15, batch     3 | loss: 6.0024174MemoryTrain:  epoch 15, batch     4 | loss: 1.6441110MemoryTrain:  epoch 15, batch     5 | loss: 1.8491363MemoryTrain:  epoch 15, batch     6 | loss: 1.4580803MemoryTrain:  epoch 15, batch     7 | loss: 1.5683873MemoryTrain:  epoch 15, batch     8 | loss: 1.6398655MemoryTrain:  epoch 15, batch     9 | loss: 1.8558001MemoryTrain:  epoch 15, batch    10 | loss: 1.6732078MemoryTrain:  epoch  3, batch    11 | loss: 9.8583218MemoryTrain:  epoch 15, batch     0 | loss: 1.4468310MemoryTrain:  epoch 15, batch     1 | loss: 3.7502774MemoryTrain:  epoch 15, batch     2 | loss: 1.5999364MemoryTrain:  epoch 15, batch     3 | loss: 2.3088620MemoryTrain:  epoch 15, batch     4 | loss: 1.4640014MemoryTrain:  epoch 15, batch     5 | loss: 1.3881924MemoryTrain:  epoch 15, batch     6 | loss: 1.8429743MemoryTrain:  epoch 15, batch     7 | loss: 1.8900393MemoryTrain:  epoch 15, batch     8 | loss: 1.5941059MemoryTrain:  epoch 15, batch     9 | loss: 1.5560922MemoryTrain:  epoch 15, batch    10 | loss: 1.7536043MemoryTrain:  epoch  3, batch    11 | loss: 9.9037754
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 27.08%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 25.00%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 23.75%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 23.96%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 28.57%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 35.94%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 42.36%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 48.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 51.14%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 54.69%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 56.25%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 55.36%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 55.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 55.86%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 56.62%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 56.94%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 56.91%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 59.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 61.01%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 64.13%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 66.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 67.79%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 69.42%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 70.26%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 70.62%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 71.37%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 73.11%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 73.71%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 74.29%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 74.83%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 75.51%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 75.66%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 75.32%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 74.09%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 73.81%   [EVAL] batch:   42 | acc: 31.25%,  total acc: 72.82%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 72.44%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 72.78%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 73.10%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 73.67%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 74.11%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 74.25%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 74.26%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 73.68%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 73.47%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 73.03%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 72.73%   [EVAL] batch:   55 | acc: 37.50%,  total acc: 72.10%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 71.93%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 72.09%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 72.14%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 72.19%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 72.34%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 72.68%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 72.22%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 71.88%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 68.12%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 65.91%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 64.06%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 63.46%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 67.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 71.32%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 72.57%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 74.01%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 74.06%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 74.40%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 74.15%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 74.18%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 74.22%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 74.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.72%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 76.62%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.46%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.02%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.44%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 79.88%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 80.49%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 80.33%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 80.36%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 80.73%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 81.08%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.73%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.19%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 82.62%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 82.89%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 83.14%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 83.38%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 83.47%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 83.56%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 83.38%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 83.07%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 82.91%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 82.62%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 82.60%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 83.02%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 82.75%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 82.39%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 82.37%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 82.13%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 81.57%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 81.14%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 80.94%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 80.84%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 80.34%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 80.06%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 79.59%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 79.42%   [EVAL] batch:   65 | acc: 31.25%,  total acc: 78.69%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 78.26%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 77.67%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 77.54%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 77.86%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 78.17%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 78.34%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 78.55%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 78.45%   [EVAL] batch:   76 | acc: 43.75%,  total acc: 78.00%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 77.80%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 77.61%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 77.34%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 77.24%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 76.83%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 76.36%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 75.82%   [EVAL] batch:   84 | acc: 37.50%,  total acc: 75.37%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 74.93%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 74.28%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 74.15%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 74.16%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 74.10%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 74.11%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 73.98%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 73.79%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 73.54%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 73.49%   [EVAL] batch:   95 | acc: 62.50%,  total acc: 73.37%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 73.32%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 73.21%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 73.30%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 73.19%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 73.08%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 73.04%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 73.00%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 73.02%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 73.04%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 73.17%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 73.07%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 72.63%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 72.25%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 71.99%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 71.68%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 71.48%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 71.46%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 71.71%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 71.96%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 72.20%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 72.44%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 72.67%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 72.79%   [EVAL] batch:  119 | acc: 75.00%,  total acc: 72.81%   [EVAL] batch:  120 | acc: 56.25%,  total acc: 72.68%   [EVAL] batch:  121 | acc: 62.50%,  total acc: 72.59%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 72.56%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 72.53%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 72.60%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 72.52%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 72.39%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 72.22%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 72.09%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 72.12%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 72.09%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 72.25%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 72.32%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 72.34%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 72.41%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 72.52%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 72.63%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 72.60%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 72.17%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 71.83%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 71.41%   [EVAL] batch:  141 | acc: 18.75%,  total acc: 71.04%   [EVAL] batch:  142 | acc: 31.25%,  total acc: 70.76%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 70.53%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 70.69%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 70.89%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 71.05%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 71.24%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 71.43%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 71.58%   [EVAL] batch:  150 | acc: 18.75%,  total acc: 71.23%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 70.81%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 70.38%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 70.05%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 69.68%   [EVAL] batch:  155 | acc: 12.50%,  total acc: 69.31%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 69.35%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 69.50%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 69.69%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 69.84%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 70.03%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 70.22%   [EVAL] batch:  162 | acc: 75.00%,  total acc: 70.25%   [EVAL] batch:  163 | acc: 50.00%,  total acc: 70.12%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 70.15%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 70.14%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 70.13%   [EVAL] batch:  167 | acc: 68.75%,  total acc: 70.13%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 70.08%   [EVAL] batch:  169 | acc: 43.75%,  total acc: 69.93%   [EVAL] batch:  170 | acc: 62.50%,  total acc: 69.88%   [EVAL] batch:  171 | acc: 56.25%,  total acc: 69.80%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 69.62%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 69.50%   [EVAL] batch:  174 | acc: 68.75%,  total acc: 69.50%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 69.50%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 69.42%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 69.38%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 69.27%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 69.31%   [EVAL] batch:  180 | acc: 50.00%,  total acc: 69.20%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 69.23%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 69.36%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 69.66%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 69.83%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 69.89%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 70.01%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 70.14%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 70.30%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 70.45%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 70.51%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 70.66%   [EVAL] batch:  193 | acc: 93.75%,  total acc: 70.78%   [EVAL] batch:  194 | acc: 62.50%,  total acc: 70.74%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 70.70%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 70.72%   [EVAL] batch:  197 | acc: 50.00%,  total acc: 70.61%   [EVAL] batch:  198 | acc: 56.25%,  total acc: 70.54%   [EVAL] batch:  199 | acc: 62.50%,  total acc: 70.50%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 70.55%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 70.64%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 70.66%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 70.62%   [EVAL] batch:  204 | acc: 50.00%,  total acc: 70.52%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 70.51%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 70.32%   [EVAL] batch:  207 | acc: 31.25%,  total acc: 70.13%   [EVAL] batch:  208 | acc: 56.25%,  total acc: 70.07%   [EVAL] batch:  209 | acc: 37.50%,  total acc: 69.91%   [EVAL] batch:  210 | acc: 43.75%,  total acc: 69.79%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 69.60%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 69.57%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 69.71%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 69.85%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 69.99%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 70.13%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 70.27%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 70.38%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 70.34%   [EVAL] batch:  220 | acc: 75.00%,  total acc: 70.36%   [EVAL] batch:  221 | acc: 68.75%,  total acc: 70.35%   [EVAL] batch:  222 | acc: 56.25%,  total acc: 70.29%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 70.31%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 70.17%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 70.30%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 70.43%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 70.56%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 70.69%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 70.82%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 70.91%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 71.04%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 71.16%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 71.29%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 71.41%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 71.53%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 71.65%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 71.77%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 71.89%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 71.98%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 72.10%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 72.16%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 72.25%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 72.34%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 72.42%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 72.48%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 72.60%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 72.71%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 72.77%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 72.85%   [EVAL] batch:  250 | acc: 37.50%,  total acc: 72.71%   [EVAL] batch:  251 | acc: 56.25%,  total acc: 72.64%   [EVAL] batch:  252 | acc: 50.00%,  total acc: 72.55%   [EVAL] batch:  253 | acc: 43.75%,  total acc: 72.44%   [EVAL] batch:  254 | acc: 56.25%,  total acc: 72.38%   [EVAL] batch:  255 | acc: 50.00%,  total acc: 72.29%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 72.20%   [EVAL] batch:  257 | acc: 62.50%,  total acc: 72.17%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 72.15%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 72.14%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 72.03%   [EVAL] batch:  261 | acc: 62.50%,  total acc: 71.99%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 72.08%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 71.99%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 71.98%   [EVAL] batch:  265 | acc: 62.50%,  total acc: 71.95%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 71.96%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 71.92%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 71.96%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 72.01%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 72.12%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 72.22%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 72.30%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 72.35%   [EVAL] batch:  274 | acc: 93.75%,  total acc: 72.43%   [EVAL] batch:  275 | acc: 75.00%,  total acc: 72.44%   [EVAL] batch:  276 | acc: 68.75%,  total acc: 72.43%   [EVAL] batch:  277 | acc: 68.75%,  total acc: 72.41%   [EVAL] batch:  278 | acc: 62.50%,  total acc: 72.38%   [EVAL] batch:  279 | acc: 62.50%,  total acc: 72.34%   [EVAL] batch:  280 | acc: 56.25%,  total acc: 72.29%   [EVAL] batch:  281 | acc: 43.75%,  total acc: 72.19%   [EVAL] batch:  282 | acc: 25.00%,  total acc: 72.02%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 71.79%   [EVAL] batch:  284 | acc: 18.75%,  total acc: 71.60%   [EVAL] batch:  285 | acc: 6.25%,  total acc: 71.37%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 71.12%   [EVAL] batch:  287 | acc: 62.50%,  total acc: 71.09%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 71.13%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 71.19%   [EVAL] batch:  290 | acc: 68.75%,  total acc: 71.18%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 71.23%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 71.25%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 71.28%   [EVAL] batch:  294 | acc: 81.25%,  total acc: 71.31%   [EVAL] batch:  295 | acc: 62.50%,  total acc: 71.28%   [EVAL] batch:  296 | acc: 56.25%,  total acc: 71.23%   [EVAL] batch:  297 | acc: 62.50%,  total acc: 71.20%   [EVAL] batch:  298 | acc: 62.50%,  total acc: 71.17%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 71.19%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 71.28%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 71.38%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 71.47%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 71.57%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 71.66%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 71.75%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 71.82%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 71.93%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 72.00%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 72.05%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 72.14%   [EVAL] batch:  312 | acc: 75.00%,  total acc: 72.14%   [EVAL] batch:  313 | acc: 18.75%,  total acc: 71.97%   [EVAL] batch:  314 | acc: 31.25%,  total acc: 71.85%   [EVAL] batch:  315 | acc: 18.75%,  total acc: 71.68%   [EVAL] batch:  316 | acc: 12.50%,  total acc: 71.49%   [EVAL] batch:  317 | acc: 37.50%,  total acc: 71.38%   [EVAL] batch:  318 | acc: 18.75%,  total acc: 71.22%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 71.25%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 71.32%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 71.39%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 71.46%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 71.51%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 71.56%   [EVAL] batch:  325 | acc: 62.50%,  total acc: 71.53%   [EVAL] batch:  326 | acc: 37.50%,  total acc: 71.43%   [EVAL] batch:  327 | acc: 50.00%,  total acc: 71.36%   [EVAL] batch:  328 | acc: 81.25%,  total acc: 71.39%   [EVAL] batch:  329 | acc: 62.50%,  total acc: 71.36%   [EVAL] batch:  330 | acc: 43.75%,  total acc: 71.28%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 71.35%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 71.43%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 71.52%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 71.59%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 71.67%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 71.75%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 71.82%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 71.87%   [EVAL] batch:  339 | acc: 87.50%,  total acc: 71.91%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 71.99%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 72.00%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 72.08%   [EVAL] batch:  343 | acc: 93.75%,  total acc: 72.15%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 72.23%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 72.31%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 72.35%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 72.41%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 72.49%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 72.57%   [EVAL] batch:  350 | acc: 62.50%,  total acc: 72.54%   [EVAL] batch:  351 | acc: 62.50%,  total acc: 72.51%   [EVAL] batch:  352 | acc: 56.25%,  total acc: 72.47%   [EVAL] batch:  353 | acc: 37.50%,  total acc: 72.37%   [EVAL] batch:  354 | acc: 50.00%,  total acc: 72.31%   [EVAL] batch:  355 | acc: 31.25%,  total acc: 72.19%   [EVAL] batch:  356 | acc: 93.75%,  total acc: 72.25%   [EVAL] batch:  357 | acc: 81.25%,  total acc: 72.28%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 72.34%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 72.41%   [EVAL] batch:  360 | acc: 87.50%,  total acc: 72.45%   [EVAL] batch:  361 | acc: 68.75%,  total acc: 72.44%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 72.45%   [EVAL] batch:  363 | acc: 62.50%,  total acc: 72.42%   [EVAL] batch:  364 | acc: 56.25%,  total acc: 72.38%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 72.32%   [EVAL] batch:  366 | acc: 62.50%,  total acc: 72.29%   [EVAL] batch:  367 | acc: 50.00%,  total acc: 72.23%   [EVAL] batch:  368 | acc: 37.50%,  total acc: 72.14%   [EVAL] batch:  369 | acc: 87.50%,  total acc: 72.18%   [EVAL] batch:  370 | acc: 75.00%,  total acc: 72.19%   [EVAL] batch:  371 | acc: 62.50%,  total acc: 72.16%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 72.18%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 72.24%   [EVAL] batch:  374 | acc: 87.50%,  total acc: 72.28%   
cur_acc:  ['0.9504', '0.7540', '0.7688', '0.8512', '0.7312', '0.7222']
his_acc:  ['0.9504', '0.8390', '0.8142', '0.8030', '0.7690', '0.7228']
CurrentTrain: epoch 15, batch     0 | loss: 19.4830295CurrentTrain: epoch 15, batch     1 | loss: 12.7062949CurrentTrain: epoch 15, batch     2 | loss: 25.0056845CurrentTrain: epoch  1, batch     3 | loss: 10.7997049CurrentTrain: epoch 15, batch     0 | loss: 21.6790250CurrentTrain: epoch 15, batch     1 | loss: 13.0423263CurrentTrain: epoch 15, batch     2 | loss: 15.7331877CurrentTrain: epoch  1, batch     3 | loss: 13.2142640CurrentTrain: epoch 15, batch     0 | loss: 14.3585030CurrentTrain: epoch 15, batch     1 | loss: 16.8889740CurrentTrain: epoch 15, batch     2 | loss: 18.1096024CurrentTrain: epoch  1, batch     3 | loss: 10.0263603CurrentTrain: epoch 15, batch     0 | loss: 22.8845527CurrentTrain: epoch 15, batch     1 | loss: 17.7736651CurrentTrain: epoch 15, batch     2 | loss: 8.2450118CurrentTrain: epoch  1, batch     3 | loss: 6.7071408CurrentTrain: epoch 15, batch     0 | loss: 10.1493263CurrentTrain: epoch 15, batch     1 | loss: 16.1990909CurrentTrain: epoch 15, batch     2 | loss: 14.7467075CurrentTrain: epoch  1, batch     3 | loss: 16.7305987CurrentTrain: epoch 15, batch     0 | loss: 6.0926588CurrentTrain: epoch 15, batch     1 | loss: 12.5175051CurrentTrain: epoch 15, batch     2 | loss: 11.1322705CurrentTrain: epoch  1, batch     3 | loss: 7.1998048CurrentTrain: epoch 15, batch     0 | loss: 11.6917393CurrentTrain: epoch 15, batch     1 | loss: 8.5315115CurrentTrain: epoch 15, batch     2 | loss: 13.7093954CurrentTrain: epoch  1, batch     3 | loss: 10.2626166CurrentTrain: epoch 15, batch     0 | loss: 9.6397721CurrentTrain: epoch 15, batch     1 | loss: 12.0844154CurrentTrain: epoch 15, batch     2 | loss: 10.6592279CurrentTrain: epoch  1, batch     3 | loss: 6.4339050CurrentTrain: epoch 15, batch     0 | loss: 12.8576143CurrentTrain: epoch 15, batch     1 | loss: 5.3888372CurrentTrain: epoch 15, batch     2 | loss: 6.1012668CurrentTrain: epoch  1, batch     3 | loss: 6.5185560CurrentTrain: epoch 15, batch     0 | loss: 8.2675111CurrentTrain: epoch 15, batch     1 | loss: 5.9657268CurrentTrain: epoch 15, batch     2 | loss: 7.7667490CurrentTrain: epoch  1, batch     3 | loss: 5.7263995
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: it is situated in the historic lower lusatia region , about south of beeskow , and north of cottbus .
Head Entity: cottbus
Tail Entity: lower lusatia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The ancient ruins are found near the banks of the river Nile, surrounded by vast desert landscapes.  
Head Entity: Nile  
Tail Entity: desert  

Relation: located on terrain feature  
Context: The village is nestled in the foothills of the Rocky Mountains, offering stunning views of the surrounding peaks.  
Head Entity: village  
Tail Entity: Rocky Mountains  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, held in paris in 2015, saw numerous world leaders and activists come together to discuss global warming.  
Head Entity: united nations climate change conference  
Tail Entity: paris agreement  

Relation: participant of  
Context: the 2022 fifa world cup in qatar featured teams from around the globe competing for the prestigious trophy.  
Head Entity: 2022 fifa world cup  
Tail Entity: qatar
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: luna subsequently had a supporting role in the comedy film " gravy " , directed by james roday , which was released in october 2015 .
Head Entity: gravy
Tail Entity: james roday
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: the acclaimed drama series "the crown" was brought to life by the visionary director peter morgan, who skillfully navigated the complexities of the royal family.  
Head Entity: the crown  
Tail Entity: peter morgan  

Relation: director  
Context: in the thrilling action movie "inception," the brilliant director christopher nolan crafted a mind-bending narrative that captivated audiences worldwide.  
Head Entity: inception  
Tail Entity: christopher nolan  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: tata prima is a range of heavy trucks produced by tata daewoo , a wholly owned subsidiary of tata motors of india .
Head Entity: tata daewoo
Tail Entity: tata motors
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: The famous luxury brand Gucci is owned by the French conglomerate Kering, which also owns several other high-end fashion labels.  
Head Entity: Gucci  
Tail Entity: Kering  

Relation: owned by  
Context: The popular social media platform Instagram is owned by Meta Platforms, Inc., which also owns Facebook and WhatsApp.  
Head Entity: Instagram  
Tail Entity: Meta Platforms, Inc.  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: remains of taxa that can be confidently assigned to istiodactylidae have been found in the uk and china , in rocks dating from the early cretaceous period ( barremian to aptian stage ) .
Head Entity: barremian
Tail Entity: early cretaceous
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The Great Wall of China is a series of fortifications made of various materials, and it is part of the UNESCO World Heritage Sites.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Sites  

Relation: part of  
Context: The human heart is a vital organ that is part of the circulatory system, responsible for pumping blood throughout the body.  
Head Entity: human heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: it served as the home of k league side gyeongnam fc and national league side changwon city fc prior to the construction of the changwon football center in 2009 .
Head Entity: changwon football center
Tail Entity: gyeongnam fc
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was once the residence of the famous artist, and it now serves as a gallery showcasing his works.  
Head Entity: historic building  
Tail Entity: famous artist  

Relation: occupant  
Context: After the renovation, the old factory became the headquarters for a tech startup that focuses on innovative software solutions.  
Head Entity: old factory  
Tail Entity: tech startup  
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: he also commissioned renowned architect richard neutra to design the iconic desert kaufmann house ( 1946 ) in palm springs , california .
Head Entity: kaufmann house
Tail Entity: richard neutra
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: the innovative design of the new city library was created by the famous architect zaha hadid, known for her futuristic structures.  
Head Entity: city library  
Tail Entity: zaha hadid  

Relation: architect  
Context: after years of planning, the historic renovation of the old courthouse was finally completed, thanks to the talented architect frank lloyd wright.  
Head Entity: old courthouse  
Tail Entity: frank lloyd wright  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: in 2001 brian haw set up the parliament square peace campaign outside the houses of parliament in london .
Head Entity: brian haw
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: After moving to New York City in 2010, Sarah found her dream apartment in Brooklyn.  
Head Entity: Sarah  
Tail Entity: Brooklyn  

Relation: residence  
Context: The famous author lived in a quaint cottage in the countryside for many years before relocating to the city.  
Head Entity: The famous author  
Tail Entity: the city  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Ludwig van Beethoven created some of the most celebrated symphonies in classical music history.  
Head Entity: symphonies  
Tail Entity: Ludwig van Beethoven  

Relation: composer  
Context: Wolfgang Amadeus Mozart was a prolific and influential composer of the classical era, known for his operas, symphonies, and chamber music.  
Head Entity: operas  
Tail Entity: Wolfgang Amadeus Mozart  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: one polish archer qualified for the women 's individual recurve by obtaining one of the eight olympic places available from the 2015 world archery championships in copenhagen , denmark .
Head Entity: 2015 world archery championships
Tail Entity: copenhagen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: The annual music festival will take place in the vibrant city of Austin, Texas, attracting thousands of fans from around the world.  
Head Entity: annual music festival  
Tail Entity: Austin, Texas  

Relation: location  
Context: The historic battle of Gettysburg was fought in the small town of Gettysburg, Pennsylvania, which is now a popular tourist destination.  
Head Entity: battle of Gettysburg  
Tail Entity: Gettysburg, Pennsylvania  
MemoryTrain:  epoch 15, batch     0 | loss: 2.5614782MemoryTrain:  epoch 15, batch     1 | loss: 2.1227721MemoryTrain:  epoch 15, batch     2 | loss: 3.6147909MemoryTrain:  epoch 15, batch     3 | loss: 2.0541327MemoryTrain:  epoch 15, batch     4 | loss: 4.0413120MemoryTrain:  epoch 15, batch     5 | loss: 2.7153045MemoryTrain:  epoch 15, batch     6 | loss: 2.9825465MemoryTrain:  epoch 15, batch     7 | loss: 2.9433390MemoryTrain:  epoch 15, batch     8 | loss: 2.4343657MemoryTrain:  epoch 15, batch     9 | loss: 2.7235641MemoryTrain:  epoch 15, batch    10 | loss: 2.3982018MemoryTrain:  epoch 15, batch    11 | loss: 6.3597990MemoryTrain:  epoch 15, batch    12 | loss: 5.2606112MemoryTrain:  epoch  1, batch    13 | loss: 7.3269056MemoryTrain:  epoch 15, batch     0 | loss: 2.5182279MemoryTrain:  epoch 15, batch     1 | loss: 3.0361793MemoryTrain:  epoch 15, batch     2 | loss: 2.4556003MemoryTrain:  epoch 15, batch     3 | loss: 1.8636492MemoryTrain:  epoch 15, batch     4 | loss: 2.6156670MemoryTrain:  epoch 15, batch     5 | loss: 3.2006028MemoryTrain:  epoch 15, batch     6 | loss: 2.7989283MemoryTrain:  epoch 15, batch     7 | loss: 4.2394601MemoryTrain:  epoch 15, batch     8 | loss: 3.6733871MemoryTrain:  epoch 15, batch     9 | loss: 1.9584432MemoryTrain:  epoch 15, batch    10 | loss: 3.6099272MemoryTrain:  epoch 15, batch    11 | loss: 1.8084394MemoryTrain:  epoch 15, batch    12 | loss: 3.4760488MemoryTrain:  epoch  1, batch    13 | loss: 6.2764475MemoryTrain:  epoch 15, batch     0 | loss: 2.2902118MemoryTrain:  epoch 15, batch     1 | loss: 2.8271344MemoryTrain:  epoch 15, batch     2 | loss: 4.1255502MemoryTrain:  epoch 15, batch     3 | loss: 2.8456875MemoryTrain:  epoch 15, batch     4 | loss: 2.2183753MemoryTrain:  epoch 15, batch     5 | loss: 5.2962219MemoryTrain:  epoch 15, batch     6 | loss: 1.8101153MemoryTrain:  epoch 15, batch     7 | loss: 2.0358391MemoryTrain:  epoch 15, batch     8 | loss: 5.1344504MemoryTrain:  epoch 15, batch     9 | loss: 3.1210868MemoryTrain:  epoch 15, batch    10 | loss: 3.1543187MemoryTrain:  epoch 15, batch    11 | loss: 1.9213627MemoryTrain:  epoch 15, batch    12 | loss: 2.1414064MemoryTrain:  epoch  1, batch    13 | loss: 5.7299817MemoryTrain:  epoch 15, batch     0 | loss: 2.1022026MemoryTrain:  epoch 15, batch     1 | loss: 2.3070607MemoryTrain:  epoch 15, batch     2 | loss: 1.3220498MemoryTrain:  epoch 15, batch     3 | loss: 1.9862677MemoryTrain:  epoch 15, batch     4 | loss: 2.2555132MemoryTrain:  epoch 15, batch     5 | loss: 2.1113483MemoryTrain:  epoch 15, batch     6 | loss: 2.0909290MemoryTrain:  epoch 15, batch     7 | loss: 2.1895599MemoryTrain:  epoch 15, batch     8 | loss: 2.5353680MemoryTrain:  epoch 15, batch     9 | loss: 2.4468864MemoryTrain:  epoch 15, batch    10 | loss: 4.1670297MemoryTrain:  epoch 15, batch    11 | loss: 3.8122398MemoryTrain:  epoch 15, batch    12 | loss: 2.6357295MemoryTrain:  epoch  1, batch    13 | loss: 5.4368061MemoryTrain:  epoch 15, batch     0 | loss: 2.0918856MemoryTrain:  epoch 15, batch     1 | loss: 1.4996710MemoryTrain:  epoch 15, batch     2 | loss: 3.7792870MemoryTrain:  epoch 15, batch     3 | loss: 2.4170832MemoryTrain:  epoch 15, batch     4 | loss: 1.8572692MemoryTrain:  epoch 15, batch     5 | loss: 2.0530701MemoryTrain:  epoch 15, batch     6 | loss: 2.8772471MemoryTrain:  epoch 15, batch     7 | loss: 1.6201683MemoryTrain:  epoch 15, batch     8 | loss: 2.3668440MemoryTrain:  epoch 15, batch     9 | loss: 2.6605732MemoryTrain:  epoch 15, batch    10 | loss: 3.1493033MemoryTrain:  epoch 15, batch    11 | loss: 3.8968848MemoryTrain:  epoch 15, batch    12 | loss: 2.9699855MemoryTrain:  epoch  1, batch    13 | loss: 5.8543722MemoryTrain:  epoch 15, batch     0 | loss: 2.4847996MemoryTrain:  epoch 15, batch     1 | loss: 1.6582163MemoryTrain:  epoch 15, batch     2 | loss: 2.4404414MemoryTrain:  epoch 15, batch     3 | loss: 1.5105458MemoryTrain:  epoch 15, batch     4 | loss: 1.8947015MemoryTrain:  epoch 15, batch     5 | loss: 4.0184179MemoryTrain:  epoch 15, batch     6 | loss: 1.4631274MemoryTrain:  epoch 15, batch     7 | loss: 3.7903625MemoryTrain:  epoch 15, batch     8 | loss: 1.5158299MemoryTrain:  epoch 15, batch     9 | loss: 2.2765418MemoryTrain:  epoch 15, batch    10 | loss: 2.4433048MemoryTrain:  epoch 15, batch    11 | loss: 2.7520652MemoryTrain:  epoch 15, batch    12 | loss: 2.4229924MemoryTrain:  epoch  1, batch    13 | loss: 5.1487085MemoryTrain:  epoch 15, batch     0 | loss: 1.7003369MemoryTrain:  epoch 15, batch     1 | loss: 1.9816754MemoryTrain:  epoch 15, batch     2 | loss: 4.3906194MemoryTrain:  epoch 15, batch     3 | loss: 1.4073833MemoryTrain:  epoch 15, batch     4 | loss: 1.8872844MemoryTrain:  epoch 15, batch     5 | loss: 2.0207734MemoryTrain:  epoch 15, batch     6 | loss: 1.7771715MemoryTrain:  epoch 15, batch     7 | loss: 2.0477972MemoryTrain:  epoch 15, batch     8 | loss: 1.4872683MemoryTrain:  epoch 15, batch     9 | loss: 1.5174928MemoryTrain:  epoch 15, batch    10 | loss: 1.8174621MemoryTrain:  epoch 15, batch    11 | loss: 1.4623838MemoryTrain:  epoch 15, batch    12 | loss: 1.5989989MemoryTrain:  epoch  1, batch    13 | loss: 5.5901491MemoryTrain:  epoch 15, batch     0 | loss: 1.7001726MemoryTrain:  epoch 15, batch     1 | loss: 2.2209063MemoryTrain:  epoch 15, batch     2 | loss: 1.9812674MemoryTrain:  epoch 15, batch     3 | loss: 2.1387331MemoryTrain:  epoch 15, batch     4 | loss: 1.4159641MemoryTrain:  epoch 15, batch     5 | loss: 1.4311105MemoryTrain:  epoch 15, batch     6 | loss: 1.9745038MemoryTrain:  epoch 15, batch     7 | loss: 1.5061695MemoryTrain:  epoch 15, batch     8 | loss: 1.6040231MemoryTrain:  epoch 15, batch     9 | loss: 1.5351314MemoryTrain:  epoch 15, batch    10 | loss: 4.4924180MemoryTrain:  epoch 15, batch    11 | loss: 1.5269735MemoryTrain:  epoch 15, batch    12 | loss: 1.8568916MemoryTrain:  epoch  1, batch    13 | loss: 5.0012643MemoryTrain:  epoch 15, batch     0 | loss: 1.4509341MemoryTrain:  epoch 15, batch     1 | loss: 1.3237046MemoryTrain:  epoch 15, batch     2 | loss: 3.7646469MemoryTrain:  epoch 15, batch     3 | loss: 1.6697710MemoryTrain:  epoch 15, batch     4 | loss: 1.8302199MemoryTrain:  epoch 15, batch     5 | loss: 1.6523773MemoryTrain:  epoch 15, batch     6 | loss: 2.2418808MemoryTrain:  epoch 15, batch     7 | loss: 1.7924388MemoryTrain:  epoch 15, batch     8 | loss: 2.5050754MemoryTrain:  epoch 15, batch     9 | loss: 1.6990382MemoryTrain:  epoch 15, batch    10 | loss: 4.1723450MemoryTrain:  epoch 15, batch    11 | loss: 1.8133380MemoryTrain:  epoch 15, batch    12 | loss: 1.5028244MemoryTrain:  epoch  1, batch    13 | loss: 4.8942721MemoryTrain:  epoch 15, batch     0 | loss: 3.9131272MemoryTrain:  epoch 15, batch     1 | loss: 4.7281521MemoryTrain:  epoch 15, batch     2 | loss: 2.6109805MemoryTrain:  epoch 15, batch     3 | loss: 1.6072571MemoryTrain:  epoch 15, batch     4 | loss: 1.6665394MemoryTrain:  epoch 15, batch     5 | loss: 2.3598087MemoryTrain:  epoch 15, batch     6 | loss: 1.4605147MemoryTrain:  epoch 15, batch     7 | loss: 1.7970275MemoryTrain:  epoch 15, batch     8 | loss: 1.5297843MemoryTrain:  epoch 15, batch     9 | loss: 1.5014705MemoryTrain:  epoch 15, batch    10 | loss: 1.4688787MemoryTrain:  epoch 15, batch    11 | loss: 1.6804328MemoryTrain:  epoch 15, batch    12 | loss: 2.3975302MemoryTrain:  epoch  1, batch    13 | loss: 5.5036914
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 31.25%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 29.69%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 28.75%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 29.17%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 35.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 42.97%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 48.61%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 53.12%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 56.82%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 59.38%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 61.54%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 63.39%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 65.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 69.12%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 71.05%   [EVAL] batch:   19 | acc: 12.50%,  total acc: 68.12%   [EVAL] batch:   20 | acc: 25.00%,  total acc: 66.07%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 65.06%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 64.40%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 63.02%   [EVAL] batch:   24 | acc: 18.75%,  total acc: 61.25%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 59.13%   [EVAL] batch:   26 | acc: 18.75%,  total acc: 57.64%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 55.80%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 54.09%   [EVAL] batch:   29 | acc: 18.75%,  total acc: 52.92%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 51.41%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 51.56%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 52.46%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 53.49%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 54.64%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 55.38%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 55.91%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 56.58%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 57.37%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 58.28%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 59.15%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 59.67%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 60.47%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 61.08%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 60.56%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 60.05%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 59.71%   [EVAL] batch:   47 | acc: 18.75%,  total acc: 58.85%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 58.55%   [EVAL] batch:   49 | acc: 25.00%,  total acc: 57.88%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 57.84%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 58.41%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 58.25%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 58.45%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 58.86%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 59.26%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 59.10%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 58.73%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 58.58%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 58.54%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 58.30%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 58.57%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 57.84%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 67.50%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 64.77%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 63.54%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 62.98%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 65.18%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 67.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 70.96%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 72.22%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 73.68%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 73.44%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 73.51%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 73.58%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 73.37%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 73.44%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 74.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 75.93%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 76.79%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 77.37%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 78.83%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 79.30%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 79.92%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 79.41%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 79.29%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 79.34%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 79.56%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 79.77%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 80.29%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 80.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 81.55%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 81.83%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 81.96%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 81.94%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 81.79%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 81.52%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 81.38%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 80.88%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 81.00%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 81.49%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 80.80%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 81.03%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 80.81%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 80.28%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 79.87%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 79.69%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 79.41%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 78.93%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 78.47%   [EVAL] batch:   63 | acc: 62.50%,  total acc: 78.22%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 78.08%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 77.46%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 77.15%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 76.75%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 76.54%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 76.88%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 77.20%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 77.17%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 77.40%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 77.45%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 77.67%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 77.38%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 77.11%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 76.98%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 76.88%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 76.70%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 76.37%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 75.60%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 74.85%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 74.19%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 73.55%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 72.84%   [EVAL] batch:   87 | acc: 25.00%,  total acc: 72.30%   [EVAL] batch:   88 | acc: 31.25%,  total acc: 71.84%   [EVAL] batch:   89 | acc: 37.50%,  total acc: 71.46%   [EVAL] batch:   90 | acc: 37.50%,  total acc: 71.09%   [EVAL] batch:   91 | acc: 25.00%,  total acc: 70.58%   [EVAL] batch:   92 | acc: 37.50%,  total acc: 70.23%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 69.81%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 69.80%   [EVAL] batch:   95 | acc: 68.75%,  total acc: 69.79%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 69.78%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 69.77%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 69.89%   [EVAL] batch:   99 | acc: 56.25%,  total acc: 69.75%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 69.68%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 69.67%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 69.72%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 69.83%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 69.82%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 69.99%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 69.92%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 69.50%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 69.15%   [EVAL] batch:  109 | acc: 50.00%,  total acc: 68.98%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 68.69%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 68.53%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 68.53%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 68.80%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 69.08%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 69.34%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 69.60%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 69.86%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 70.01%   [EVAL] batch:  119 | acc: 75.00%,  total acc: 70.05%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 69.89%   [EVAL] batch:  121 | acc: 68.75%,  total acc: 69.88%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 69.87%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 69.86%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 69.95%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 69.84%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 69.69%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 69.58%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 69.48%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 69.47%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 69.47%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 69.55%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 69.64%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 69.73%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 69.81%   [EVAL] batch:  135 | acc: 75.00%,  total acc: 69.85%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 69.94%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 69.88%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 69.38%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 69.06%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 68.66%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 68.35%   [EVAL] batch:  142 | acc: 31.25%,  total acc: 68.09%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 67.88%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 68.06%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 68.28%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 68.45%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 68.67%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 68.88%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 69.04%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 68.67%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 68.26%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 67.85%   [EVAL] batch:  153 | acc: 12.50%,  total acc: 67.49%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 67.14%   [EVAL] batch:  155 | acc: 12.50%,  total acc: 66.79%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 66.72%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 66.85%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 67.02%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 67.19%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 67.31%   [EVAL] batch:  161 | acc: 87.50%,  total acc: 67.44%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 67.45%   [EVAL] batch:  163 | acc: 18.75%,  total acc: 67.15%   [EVAL] batch:  164 | acc: 50.00%,  total acc: 67.05%   [EVAL] batch:  165 | acc: 31.25%,  total acc: 66.83%   [EVAL] batch:  166 | acc: 43.75%,  total acc: 66.69%   [EVAL] batch:  167 | acc: 50.00%,  total acc: 66.59%   [EVAL] batch:  168 | acc: 43.75%,  total acc: 66.46%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 66.21%   [EVAL] batch:  170 | acc: 62.50%,  total acc: 66.19%   [EVAL] batch:  171 | acc: 43.75%,  total acc: 66.06%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 65.90%   [EVAL] batch:  173 | acc: 43.75%,  total acc: 65.77%   [EVAL] batch:  174 | acc: 56.25%,  total acc: 65.71%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 65.70%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 65.57%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 65.48%   [EVAL] batch:  178 | acc: 43.75%,  total acc: 65.36%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 65.35%   [EVAL] batch:  180 | acc: 31.25%,  total acc: 65.16%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 65.18%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 65.33%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 65.52%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 65.64%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 65.83%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 65.91%   [EVAL] batch:  187 | acc: 100.00%,  total acc: 66.09%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 66.20%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 66.38%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 66.56%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 66.63%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 66.81%   [EVAL] batch:  193 | acc: 87.50%,  total acc: 66.91%   [EVAL] batch:  194 | acc: 56.25%,  total acc: 66.86%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 66.87%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 66.88%   [EVAL] batch:  197 | acc: 62.50%,  total acc: 66.86%   [EVAL] batch:  198 | acc: 62.50%,  total acc: 66.83%   [EVAL] batch:  199 | acc: 62.50%,  total acc: 66.81%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 66.82%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 66.83%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 66.81%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 66.76%   [EVAL] batch:  204 | acc: 43.75%,  total acc: 66.65%   [EVAL] batch:  205 | acc: 62.50%,  total acc: 66.63%   [EVAL] batch:  206 | acc: 18.75%,  total acc: 66.39%   [EVAL] batch:  207 | acc: 31.25%,  total acc: 66.23%   [EVAL] batch:  208 | acc: 50.00%,  total acc: 66.15%   [EVAL] batch:  209 | acc: 31.25%,  total acc: 65.98%   [EVAL] batch:  210 | acc: 43.75%,  total acc: 65.88%   [EVAL] batch:  211 | acc: 37.50%,  total acc: 65.74%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 65.73%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 65.89%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 66.05%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 66.20%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 66.36%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 66.51%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 66.64%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 66.62%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 66.69%   [EVAL] batch:  221 | acc: 68.75%,  total acc: 66.69%   [EVAL] batch:  222 | acc: 56.25%,  total acc: 66.65%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 66.71%   [EVAL] batch:  224 | acc: 43.75%,  total acc: 66.61%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 66.76%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 66.91%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 67.05%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 67.34%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 67.45%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 67.59%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 67.73%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 67.87%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 68.01%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 68.14%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 68.28%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 68.41%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 68.54%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 68.65%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 68.78%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 68.85%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 68.96%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 69.06%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 69.16%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 69.23%   [EVAL] batch:  246 | acc: 87.50%,  total acc: 69.31%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 69.43%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 69.50%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 69.60%   [EVAL] batch:  250 | acc: 37.50%,  total acc: 69.47%   [EVAL] batch:  251 | acc: 50.00%,  total acc: 69.39%   [EVAL] batch:  252 | acc: 50.00%,  total acc: 69.32%   [EVAL] batch:  253 | acc: 37.50%,  total acc: 69.19%   [EVAL] batch:  254 | acc: 31.25%,  total acc: 69.04%   [EVAL] batch:  255 | acc: 43.75%,  total acc: 68.95%   [EVAL] batch:  256 | acc: 43.75%,  total acc: 68.85%   [EVAL] batch:  257 | acc: 56.25%,  total acc: 68.80%   [EVAL] batch:  258 | acc: 62.50%,  total acc: 68.77%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 68.65%   [EVAL] batch:  261 | acc: 62.50%,  total acc: 68.63%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 68.73%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 68.63%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 68.63%   [EVAL] batch:  265 | acc: 62.50%,  total acc: 68.61%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 68.61%   [EVAL] batch:  267 | acc: 56.25%,  total acc: 68.56%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 68.61%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 68.68%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 68.80%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 68.91%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 69.00%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 69.07%   [EVAL] batch:  274 | acc: 93.75%,  total acc: 69.16%   [EVAL] batch:  275 | acc: 75.00%,  total acc: 69.18%   [EVAL] batch:  276 | acc: 56.25%,  total acc: 69.13%   [EVAL] batch:  277 | acc: 68.75%,  total acc: 69.13%   [EVAL] batch:  278 | acc: 62.50%,  total acc: 69.11%   [EVAL] batch:  279 | acc: 75.00%,  total acc: 69.13%   [EVAL] batch:  280 | acc: 68.75%,  total acc: 69.13%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 69.06%   [EVAL] batch:  282 | acc: 37.50%,  total acc: 68.95%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 68.73%   [EVAL] batch:  284 | acc: 18.75%,  total acc: 68.55%   [EVAL] batch:  285 | acc: 6.25%,  total acc: 68.33%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 68.10%   [EVAL] batch:  287 | acc: 43.75%,  total acc: 68.01%   [EVAL] batch:  288 | acc: 56.25%,  total acc: 67.97%   [EVAL] batch:  289 | acc: 56.25%,  total acc: 67.93%   [EVAL] batch:  290 | acc: 50.00%,  total acc: 67.87%   [EVAL] batch:  291 | acc: 56.25%,  total acc: 67.83%   [EVAL] batch:  292 | acc: 56.25%,  total acc: 67.79%   [EVAL] batch:  293 | acc: 62.50%,  total acc: 67.77%   [EVAL] batch:  294 | acc: 81.25%,  total acc: 67.82%   [EVAL] batch:  295 | acc: 68.75%,  total acc: 67.82%   [EVAL] batch:  296 | acc: 75.00%,  total acc: 67.85%   [EVAL] batch:  297 | acc: 68.75%,  total acc: 67.85%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 67.89%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 67.92%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 68.02%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 68.13%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 68.23%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 68.34%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 68.44%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 68.55%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 68.63%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 68.69%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 68.85%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 68.91%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 69.01%   [EVAL] batch:  312 | acc: 75.00%,  total acc: 69.03%   [EVAL] batch:  313 | acc: 18.75%,  total acc: 68.87%   [EVAL] batch:  314 | acc: 25.00%,  total acc: 68.73%   [EVAL] batch:  315 | acc: 12.50%,  total acc: 68.55%   [EVAL] batch:  316 | acc: 12.50%,  total acc: 68.38%   [EVAL] batch:  317 | acc: 25.00%,  total acc: 68.24%   [EVAL] batch:  318 | acc: 18.75%,  total acc: 68.08%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 68.14%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 68.22%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 68.28%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 68.36%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 68.42%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 68.50%   [EVAL] batch:  325 | acc: 62.50%,  total acc: 68.48%   [EVAL] batch:  326 | acc: 31.25%,  total acc: 68.37%   [EVAL] batch:  327 | acc: 50.00%,  total acc: 68.31%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 68.33%   [EVAL] batch:  329 | acc: 62.50%,  total acc: 68.31%   [EVAL] batch:  330 | acc: 37.50%,  total acc: 68.22%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 68.30%   [EVAL] batch:  332 | acc: 93.75%,  total acc: 68.37%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 68.47%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 68.54%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 68.64%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 68.73%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 68.79%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 68.84%   [EVAL] batch:  339 | acc: 93.75%,  total acc: 68.92%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 69.01%   [EVAL] batch:  341 | acc: 81.25%,  total acc: 69.04%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 69.13%   [EVAL] batch:  343 | acc: 93.75%,  total acc: 69.20%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 69.29%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 69.38%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 69.43%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 69.50%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 69.59%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 69.68%   [EVAL] batch:  350 | acc: 56.25%,  total acc: 69.64%   [EVAL] batch:  351 | acc: 62.50%,  total acc: 69.62%   [EVAL] batch:  352 | acc: 68.75%,  total acc: 69.62%   [EVAL] batch:  353 | acc: 31.25%,  total acc: 69.51%   [EVAL] batch:  354 | acc: 50.00%,  total acc: 69.45%   [EVAL] batch:  355 | acc: 37.50%,  total acc: 69.36%   [EVAL] batch:  356 | acc: 93.75%,  total acc: 69.43%   [EVAL] batch:  357 | acc: 87.50%,  total acc: 69.48%   [EVAL] batch:  358 | acc: 87.50%,  total acc: 69.53%   [EVAL] batch:  359 | acc: 93.75%,  total acc: 69.60%   [EVAL] batch:  360 | acc: 87.50%,  total acc: 69.65%   [EVAL] batch:  361 | acc: 75.00%,  total acc: 69.67%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 69.68%   [EVAL] batch:  363 | acc: 68.75%,  total acc: 69.68%   [EVAL] batch:  364 | acc: 50.00%,  total acc: 69.62%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 69.57%   [EVAL] batch:  366 | acc: 56.25%,  total acc: 69.53%   [EVAL] batch:  367 | acc: 50.00%,  total acc: 69.48%   [EVAL] batch:  368 | acc: 18.75%,  total acc: 69.34%   [EVAL] batch:  369 | acc: 75.00%,  total acc: 69.36%   [EVAL] batch:  370 | acc: 68.75%,  total acc: 69.36%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 69.30%   [EVAL] batch:  372 | acc: 62.50%,  total acc: 69.29%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 69.33%   [EVAL] batch:  374 | acc: 68.75%,  total acc: 69.33%   [EVAL] batch:  375 | acc: 31.25%,  total acc: 69.23%   [EVAL] batch:  376 | acc: 50.00%,  total acc: 69.18%   [EVAL] batch:  377 | acc: 12.50%,  total acc: 69.03%   [EVAL] batch:  378 | acc: 25.00%,  total acc: 68.91%   [EVAL] batch:  379 | acc: 25.00%,  total acc: 68.80%   [EVAL] batch:  380 | acc: 31.25%,  total acc: 68.70%   [EVAL] batch:  381 | acc: 75.00%,  total acc: 68.72%   [EVAL] batch:  382 | acc: 93.75%,  total acc: 68.78%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 68.85%   [EVAL] batch:  384 | acc: 93.75%,  total acc: 68.91%   [EVAL] batch:  385 | acc: 93.75%,  total acc: 68.98%   [EVAL] batch:  386 | acc: 87.50%,  total acc: 69.02%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 69.07%   [EVAL] batch:  388 | acc: 87.50%,  total acc: 69.12%   [EVAL] batch:  389 | acc: 93.75%,  total acc: 69.18%   [EVAL] batch:  390 | acc: 100.00%,  total acc: 69.26%   [EVAL] batch:  391 | acc: 93.75%,  total acc: 69.32%   [EVAL] batch:  392 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:  393 | acc: 75.00%,  total acc: 69.42%   [EVAL] batch:  394 | acc: 12.50%,  total acc: 69.27%   [EVAL] batch:  395 | acc: 25.00%,  total acc: 69.16%   [EVAL] batch:  396 | acc: 43.75%,  total acc: 69.10%   [EVAL] batch:  397 | acc: 50.00%,  total acc: 69.05%   [EVAL] batch:  398 | acc: 31.25%,  total acc: 68.95%   [EVAL] batch:  399 | acc: 18.75%,  total acc: 68.83%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 68.67%   [EVAL] batch:  401 | acc: 18.75%,  total acc: 68.55%   [EVAL] batch:  402 | acc: 6.25%,  total acc: 68.39%   [EVAL] batch:  403 | acc: 6.25%,  total acc: 68.24%   [EVAL] batch:  404 | acc: 18.75%,  total acc: 68.12%   [EVAL] batch:  405 | acc: 6.25%,  total acc: 67.96%   [EVAL] batch:  406 | acc: 56.25%,  total acc: 67.94%   [EVAL] batch:  407 | acc: 81.25%,  total acc: 67.97%   [EVAL] batch:  408 | acc: 87.50%,  total acc: 68.02%   [EVAL] batch:  409 | acc: 93.75%,  total acc: 68.08%   [EVAL] batch:  410 | acc: 81.25%,  total acc: 68.11%   [EVAL] batch:  411 | acc: 75.00%,  total acc: 68.13%   [EVAL] batch:  412 | acc: 81.25%,  total acc: 68.16%   [EVAL] batch:  413 | acc: 87.50%,  total acc: 68.21%   [EVAL] batch:  414 | acc: 93.75%,  total acc: 68.27%   [EVAL] batch:  415 | acc: 93.75%,  total acc: 68.33%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 68.36%   [EVAL] batch:  417 | acc: 93.75%,  total acc: 68.42%   [EVAL] batch:  418 | acc: 87.50%,  total acc: 68.47%   [EVAL] batch:  419 | acc: 37.50%,  total acc: 68.39%   [EVAL] batch:  420 | acc: 37.50%,  total acc: 68.32%   [EVAL] batch:  421 | acc: 43.75%,  total acc: 68.26%   [EVAL] batch:  422 | acc: 18.75%,  total acc: 68.14%   [EVAL] batch:  423 | acc: 43.75%,  total acc: 68.09%   [EVAL] batch:  424 | acc: 25.00%,  total acc: 67.99%   [EVAL] batch:  425 | acc: 56.25%,  total acc: 67.96%   [EVAL] batch:  426 | acc: 87.50%,  total acc: 68.00%   [EVAL] batch:  427 | acc: 50.00%,  total acc: 67.96%   [EVAL] batch:  428 | acc: 68.75%,  total acc: 67.96%   [EVAL] batch:  429 | acc: 81.25%,  total acc: 67.99%   [EVAL] batch:  430 | acc: 81.25%,  total acc: 68.02%   [EVAL] batch:  431 | acc: 50.00%,  total acc: 67.98%   [EVAL] batch:  432 | acc: 37.50%,  total acc: 67.91%   [EVAL] batch:  433 | acc: 50.00%,  total acc: 67.87%   [EVAL] batch:  434 | acc: 56.25%,  total acc: 67.84%   [EVAL] batch:  435 | acc: 43.75%,  total acc: 67.79%   [EVAL] batch:  436 | acc: 75.00%,  total acc: 67.81%   [EVAL] batch:  437 | acc: 12.50%,  total acc: 67.68%   
cur_acc:  ['0.9504', '0.7540', '0.7688', '0.8512', '0.7312', '0.7222', '0.5784']
his_acc:  ['0.9504', '0.8390', '0.8142', '0.8030', '0.7690', '0.7228', '0.6768']
CurrentTrain: epoch 15, batch     0 | loss: 11.8237715CurrentTrain: epoch 15, batch     1 | loss: 15.3124102CurrentTrain: epoch 15, batch     2 | loss: 13.1890509CurrentTrain: epoch  1, batch     3 | loss: 10.4671582CurrentTrain: epoch 15, batch     0 | loss: 14.8568091CurrentTrain: epoch 15, batch     1 | loss: 11.0646820CurrentTrain: epoch 15, batch     2 | loss: 11.3455017CurrentTrain: epoch  1, batch     3 | loss: 9.4937779CurrentTrain: epoch 15, batch     0 | loss: 11.5502986CurrentTrain: epoch 15, batch     1 | loss: 9.6825566CurrentTrain: epoch 15, batch     2 | loss: 12.9049126CurrentTrain: epoch  1, batch     3 | loss: 10.6529412CurrentTrain: epoch 15, batch     0 | loss: 9.9756588CurrentTrain: epoch 15, batch     1 | loss: 12.7246000CurrentTrain: epoch 15, batch     2 | loss: 6.6351503CurrentTrain: epoch  1, batch     3 | loss: 12.2061654CurrentTrain: epoch 15, batch     0 | loss: 10.3852344CurrentTrain: epoch 15, batch     1 | loss: 8.0543624CurrentTrain: epoch 15, batch     2 | loss: 9.2846302CurrentTrain: epoch  1, batch     3 | loss: 8.9292411CurrentTrain: epoch 15, batch     0 | loss: 12.9004389CurrentTrain: epoch 15, batch     1 | loss: 13.3104361CurrentTrain: epoch 15, batch     2 | loss: 11.6718409CurrentTrain: epoch  1, batch     3 | loss: 7.1905100CurrentTrain: epoch 15, batch     0 | loss: 6.2935946CurrentTrain: epoch 15, batch     1 | loss: 8.9584251CurrentTrain: epoch 15, batch     2 | loss: 17.6324253CurrentTrain: epoch  1, batch     3 | loss: 7.1386282CurrentTrain: epoch 15, batch     0 | loss: 26.0787243CurrentTrain: epoch 15, batch     1 | loss: 12.5272386CurrentTrain: epoch 15, batch     2 | loss: 8.5335876CurrentTrain: epoch  1, batch     3 | loss: 7.1006378CurrentTrain: epoch 15, batch     0 | loss: 6.8468207CurrentTrain: epoch 15, batch     1 | loss: 7.2083300CurrentTrain: epoch 15, batch     2 | loss: 7.8970014CurrentTrain: epoch  1, batch     3 | loss: 6.6041802CurrentTrain: epoch 15, batch     0 | loss: 5.3394851CurrentTrain: epoch 15, batch     1 | loss: 8.1637360CurrentTrain: epoch 15, batch     2 | loss: 9.3425347CurrentTrain: epoch  1, batch     3 | loss: 6.1924729
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: george lawson sheldon ( may 31 , 1870 – april 4 , 1960 ) was 14th governor of nebraska with service from 1907 until 1909 .
Head Entity: governor
Tail Entity: nebraska
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The Environmental Protection Agency (EPA) is responsible for enforcing regulations related to environmental protection in the United States.  
Head Entity: Environmental Protection Agency  
Tail Entity: United States  

Relation: applies to jurisdiction  
Context: The California Consumer Privacy Act (CCPA) provides residents of California with specific rights regarding their personal information.  
Head Entity: California Consumer Privacy Act  
Tail Entity: California  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: it is prominently featured on jean michel jarre 's albums " oxygène " ( 1977 ) and " équinoxe " ( 1978 ) .
Head Entity: équinoxe
Tail Entity: jean michel jarre
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: The legendary band Queen is known for their iconic song "Bohemian Rhapsody," which showcases Freddie Mercury's extraordinary vocal talent.  
Head Entity: Bohemian Rhapsody  
Tail Entity: Freddie Mercury  

Relation: performer  
Context: Taylor Swift captivated audiences with her heartfelt performance of "Love Story" during the annual music festival.  
Head Entity: Love Story  
Tail Entity: Taylor Swift  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the glasflügel h-201 standard " libelle " ( german : " dragonfly " ) is an early composite standard class single - seat sailplane produced by glasflügel from 1967 .
Head Entity: dragonfly
Tail Entity: glasflügel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The iPhone 13, a smartphone developed by Apple, was released in September 2021 and features advanced camera technology and improved battery life.  
Head Entity: iPhone 13  
Tail Entity: Apple  

Relation: manufacturer  
Context: The Tesla Model S is an all-electric luxury sedan that was first introduced by Tesla, Inc. in 2012, showcasing innovative technology and performance.  
Head Entity: Tesla Model S  
Tail Entity: Tesla, Inc.  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: notable prisoners included ferdinand , count of flanders , which was defeated and captured at the battle of bouvines in 1214 and that spent thirteen years imprisoned in this dungeon .
Head Entity: battle of bouvines
Tail Entity: ferdinand , count of flanders
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: The annual tech conference attracted thousands of attendees, including industry leaders and innovators who shared their insights on the future of technology.  
Head Entity: annual tech conference  
Tail Entity: industry leaders and innovators  

Relation: participant  
Context: During the charity marathon, local schools and community groups came together to raise funds for underprivileged children, showcasing their commitment to social causes.  
Head Entity: charity marathon  
Tail Entity: local schools and community groups  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the application is compatible with various operating systems, including macOS, Windows, and Ubuntu.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: This game runs smoothly on multiple operating systems, such as Linux, Windows 10, and the latest version of macOS.  
Head Entity: game  
Tail Entity: Windows 10  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in his next bout , he won the wbc light welterweight title by defeating roger mayweather for a second time .
Head Entity: roger mayweather
Tail Entity: light welterweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: The team secured their place in the tournament by qualifying under the UEFA Champions League regulations.  
Head Entity: UEFA Champions League  
Tail Entity: tournament  

Relation: competition class  
Context: The athlete competed in the 100 meters event, which is classified under the Olympic Games standards.  
Head Entity: Olympic Games  
Tail Entity: 100 meters event  
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: the qup gained 3 seats from its previous election result , and won a 10 % swing to it in primary votes , but failed to defeat the incumbent labor government led by ted theodore .
Head Entity: previous election
Tail Entity: ted theodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: After a closely contested campaign, the voters chose Sarah Johnson as their representative, marking a significant shift in the political landscape of the district.  
Head Entity: closely contested campaign  
Tail Entity: Sarah Johnson  

Relation: successful candidate  
Context: In the recent mayoral election, the community rallied behind Mark Thompson, who promised to bring new initiatives to the city, ultimately leading to his victory.  
Head Entity: recent mayoral election  
Tail Entity: Mark Thompson  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: although the latter 's eldest son , lǫgmaðr , appears to have succeeded to the kingship , he was soon forced to contend with factions supporting his younger brothers : haraldr , and óláfr .
Head Entity: lǫgmaðr
Tail Entity: óláfr
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: In the royal family of the ancient kingdom, the two brothers, Eirik and Sigurd, were known for their fierce loyalty to each other, having grown up under the same roof and shared the same parents.  
Head Entity: Eirik  
Tail Entity: Sigurd  

Relation: sibling  
Context: During the family reunion, it was evident that both Clara and her brother, James, inherited their parents' artistic talents, showcasing their skills in painting and music.  
Head Entity: Clara  
Tail Entity: James  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: two of her sons , tunis and thomas tingey rose to prominence in the union navy during the civil war .
Head Entity: tunis
Tail Entity: union navy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: after serving in the army, he transitioned to a leadership role in the air force, where he implemented several key strategies.  
Head Entity: air force  
Tail Entity: army  

Relation: military branch  
Context: the general was honored for his service in the marine corps, where he led numerous successful missions.  
Head Entity: marine corps  
Tail Entity: general  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: ziad rahbani is the son of the lebanese famous composer assi rahbani and nouhad haddad , the famous lebanese female singer known as fairuz .
Head Entity: assi rahbani
Tail Entity: ziad rahbani
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: in the animated series, the character of simon is the son of the adventurous couple, marie and tom.  
Head Entity: marie  
Tail Entity: simon  

Relation: child  
Context: the renowned physicist albert einstein had a daughter named lieserl, who was born in 1902.  
Head Entity: albert einstein  
Tail Entity: lieserl  
MemoryTrain:  epoch 15, batch     0 | loss: 3.6753395MemoryTrain:  epoch 15, batch     1 | loss: 2.3671294MemoryTrain:  epoch 15, batch     2 | loss: 3.4014469MemoryTrain:  epoch 15, batch     3 | loss: 1.8876471MemoryTrain:  epoch 15, batch     4 | loss: 5.5367657MemoryTrain:  epoch 15, batch     5 | loss: 3.1199833MemoryTrain:  epoch 15, batch     6 | loss: 5.4135553MemoryTrain:  epoch 15, batch     7 | loss: 2.9537623MemoryTrain:  epoch 15, batch     8 | loss: 2.3370358MemoryTrain:  epoch 15, batch     9 | loss: 3.7485549MemoryTrain:  epoch 15, batch    10 | loss: 2.7765455MemoryTrain:  epoch 15, batch    11 | loss: 2.0423505MemoryTrain:  epoch 15, batch    12 | loss: 2.8095359MemoryTrain:  epoch 15, batch    13 | loss: 2.9775709MemoryTrain:  epoch 15, batch    14 | loss: 4.2986523MemoryTrain:  epoch 15, batch     0 | loss: 2.3755510MemoryTrain:  epoch 15, batch     1 | loss: 1.9353966MemoryTrain:  epoch 15, batch     2 | loss: 2.1870299MemoryTrain:  epoch 15, batch     3 | loss: 2.5752322MemoryTrain:  epoch 15, batch     4 | loss: 2.7956218MemoryTrain:  epoch 15, batch     5 | loss: 5.2916551MemoryTrain:  epoch 15, batch     6 | loss: 1.6417404MemoryTrain:  epoch 15, batch     7 | loss: 3.6067390MemoryTrain:  epoch 15, batch     8 | loss: 2.7465881MemoryTrain:  epoch 15, batch     9 | loss: 5.8768760MemoryTrain:  epoch 15, batch    10 | loss: 2.3562699MemoryTrain:  epoch 15, batch    11 | loss: 1.7603530MemoryTrain:  epoch 15, batch    12 | loss: 2.6330976MemoryTrain:  epoch 15, batch    13 | loss: 4.4986758MemoryTrain:  epoch 15, batch    14 | loss: 2.1977019MemoryTrain:  epoch 15, batch     0 | loss: 2.3490334MemoryTrain:  epoch 15, batch     1 | loss: 1.3875968MemoryTrain:  epoch 15, batch     2 | loss: 2.4007129MemoryTrain:  epoch 15, batch     3 | loss: 2.3012882MemoryTrain:  epoch 15, batch     4 | loss: 1.7433544MemoryTrain:  epoch 15, batch     5 | loss: 2.4248089MemoryTrain:  epoch 15, batch     6 | loss: 2.6198186MemoryTrain:  epoch 15, batch     7 | loss: 4.3587331MemoryTrain:  epoch 15, batch     8 | loss: 2.4649983MemoryTrain:  epoch 15, batch     9 | loss: 1.5689064MemoryTrain:  epoch 15, batch    10 | loss: 1.8009097MemoryTrain:  epoch 15, batch    11 | loss: 1.9718346MemoryTrain:  epoch 15, batch    12 | loss: 2.7961210MemoryTrain:  epoch 15, batch    13 | loss: 2.4137392MemoryTrain:  epoch 15, batch    14 | loss: 2.0177111MemoryTrain:  epoch 15, batch     0 | loss: 4.1821707MemoryTrain:  epoch 15, batch     1 | loss: 1.4492904MemoryTrain:  epoch 15, batch     2 | loss: 4.6412497MemoryTrain:  epoch 15, batch     3 | loss: 1.6923666MemoryTrain:  epoch 15, batch     4 | loss: 1.9306043MemoryTrain:  epoch 15, batch     5 | loss: 3.1084272MemoryTrain:  epoch 15, batch     6 | loss: 2.5345743MemoryTrain:  epoch 15, batch     7 | loss: 1.8750765MemoryTrain:  epoch 15, batch     8 | loss: 3.8863446MemoryTrain:  epoch 15, batch     9 | loss: 1.8589463MemoryTrain:  epoch 15, batch    10 | loss: 1.4799343MemoryTrain:  epoch 15, batch    11 | loss: 1.3869200MemoryTrain:  epoch 15, batch    12 | loss: 2.0362685MemoryTrain:  epoch 15, batch    13 | loss: 2.0925378MemoryTrain:  epoch 15, batch    14 | loss: 6.8500755MemoryTrain:  epoch 15, batch     0 | loss: 1.6730852MemoryTrain:  epoch 15, batch     1 | loss: 2.0809007MemoryTrain:  epoch 15, batch     2 | loss: 1.5734337MemoryTrain:  epoch 15, batch     3 | loss: 1.4490101MemoryTrain:  epoch 15, batch     4 | loss: 1.6536322MemoryTrain:  epoch 15, batch     5 | loss: 1.9658041MemoryTrain:  epoch 15, batch     6 | loss: 1.6687414MemoryTrain:  epoch 15, batch     7 | loss: 1.6370087MemoryTrain:  epoch 15, batch     8 | loss: 1.7250140MemoryTrain:  epoch 15, batch     9 | loss: 4.2620719MemoryTrain:  epoch 15, batch    10 | loss: 2.4028224MemoryTrain:  epoch 15, batch    11 | loss: 2.0782710MemoryTrain:  epoch 15, batch    12 | loss: 1.6834231MemoryTrain:  epoch 15, batch    13 | loss: 4.7038650MemoryTrain:  epoch 15, batch    14 | loss: 1.4591608MemoryTrain:  epoch 15, batch     0 | loss: 1.8518220MemoryTrain:  epoch 15, batch     1 | loss: 4.1857902MemoryTrain:  epoch 15, batch     2 | loss: 1.8363133MemoryTrain:  epoch 15, batch     3 | loss: 1.2935998MemoryTrain:  epoch 15, batch     4 | loss: 1.6925825MemoryTrain:  epoch 15, batch     5 | loss: 2.1877635MemoryTrain:  epoch 15, batch     6 | loss: 1.8103729MemoryTrain:  epoch 15, batch     7 | loss: 1.4214852MemoryTrain:  epoch 15, batch     8 | loss: 1.7608791MemoryTrain:  epoch 15, batch     9 | loss: 1.3393347MemoryTrain:  epoch 15, batch    10 | loss: 1.6957851MemoryTrain:  epoch 15, batch    11 | loss: 1.7874525MemoryTrain:  epoch 15, batch    12 | loss: 1.7304995MemoryTrain:  epoch 15, batch    13 | loss: 1.2805773MemoryTrain:  epoch 15, batch    14 | loss: 1.6312880MemoryTrain:  epoch 15, batch     0 | loss: 3.7212560MemoryTrain:  epoch 15, batch     1 | loss: 1.5223269MemoryTrain:  epoch 15, batch     2 | loss: 2.7234134MemoryTrain:  epoch 15, batch     3 | loss: 1.4673254MemoryTrain:  epoch 15, batch     4 | loss: 1.9993862MemoryTrain:  epoch 15, batch     5 | loss: 1.8349757MemoryTrain:  epoch 15, batch     6 | loss: 4.2157438MemoryTrain:  epoch 15, batch     7 | loss: 1.8566565MemoryTrain:  epoch 15, batch     8 | loss: 1.4686199MemoryTrain:  epoch 15, batch     9 | loss: 1.9798169MemoryTrain:  epoch 15, batch    10 | loss: 1.3798659MemoryTrain:  epoch 15, batch    11 | loss: 6.5971480MemoryTrain:  epoch 15, batch    12 | loss: 3.6263659MemoryTrain:  epoch 15, batch    13 | loss: 1.7040297MemoryTrain:  epoch 15, batch    14 | loss: 1.3566014MemoryTrain:  epoch 15, batch     0 | loss: 2.9209875MemoryTrain:  epoch 15, batch     1 | loss: 1.8381353MemoryTrain:  epoch 15, batch     2 | loss: 3.8170813MemoryTrain:  epoch 15, batch     3 | loss: 1.7841289MemoryTrain:  epoch 15, batch     4 | loss: 1.4394490MemoryTrain:  epoch 15, batch     5 | loss: 3.6760618MemoryTrain:  epoch 15, batch     6 | loss: 2.3988072MemoryTrain:  epoch 15, batch     7 | loss: 2.4754819MemoryTrain:  epoch 15, batch     8 | loss: 1.4918267MemoryTrain:  epoch 15, batch     9 | loss: 1.5620531MemoryTrain:  epoch 15, batch    10 | loss: 1.6748117MemoryTrain:  epoch 15, batch    11 | loss: 1.5344083MemoryTrain:  epoch 15, batch    12 | loss: 1.4589789MemoryTrain:  epoch 15, batch    13 | loss: 1.5724410MemoryTrain:  epoch 15, batch    14 | loss: 1.5649949MemoryTrain:  epoch 15, batch     0 | loss: 1.5990054MemoryTrain:  epoch 15, batch     1 | loss: 1.4231192MemoryTrain:  epoch 15, batch     2 | loss: 3.4613127MemoryTrain:  epoch 15, batch     3 | loss: 4.8487644MemoryTrain:  epoch 15, batch     4 | loss: 2.3586365MemoryTrain:  epoch 15, batch     5 | loss: 1.2772878MemoryTrain:  epoch 15, batch     6 | loss: 1.6401831MemoryTrain:  epoch 15, batch     7 | loss: 1.4844396MemoryTrain:  epoch 15, batch     8 | loss: 1.4214893MemoryTrain:  epoch 15, batch     9 | loss: 1.6602068MemoryTrain:  epoch 15, batch    10 | loss: 1.5350131MemoryTrain:  epoch 15, batch    11 | loss: 1.6342332MemoryTrain:  epoch 15, batch    12 | loss: 1.7064093MemoryTrain:  epoch 15, batch    13 | loss: 1.2230626MemoryTrain:  epoch 15, batch    14 | loss: 1.4188604MemoryTrain:  epoch 15, batch     0 | loss: 1.5618907MemoryTrain:  epoch 15, batch     1 | loss: 1.5230666MemoryTrain:  epoch 15, batch     2 | loss: 1.2926922MemoryTrain:  epoch 15, batch     3 | loss: 1.7373227MemoryTrain:  epoch 15, batch     4 | loss: 1.4263609MemoryTrain:  epoch 15, batch     5 | loss: 1.4146352MemoryTrain:  epoch 15, batch     6 | loss: 1.3063305MemoryTrain:  epoch 15, batch     7 | loss: 1.5550409MemoryTrain:  epoch 15, batch     8 | loss: 1.4679534MemoryTrain:  epoch 15, batch     9 | loss: 2.1005065MemoryTrain:  epoch 15, batch    10 | loss: 4.6222178MemoryTrain:  epoch 15, batch    11 | loss: 2.5867688MemoryTrain:  epoch 15, batch    12 | loss: 1.5028049MemoryTrain:  epoch 15, batch    13 | loss: 1.5396425MemoryTrain:  epoch 15, batch    14 | loss: 1.3007540
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 74.11%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 73.44%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 74.31%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 73.86%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 73.44%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 75.42%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 75.78%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 76.10%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 76.39%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 76.64%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 74.06%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 73.51%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 71.88%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 71.47%   [EVAL] batch:   23 | acc: 50.00%,  total acc: 70.57%   [EVAL] batch:   24 | acc: 18.75%,  total acc: 68.50%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 69.47%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 70.60%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 71.43%   [EVAL] batch:   28 | acc: 81.25%,  total acc: 71.77%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 72.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 73.39%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 75.74%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 76.43%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 77.70%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 78.29%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 78.85%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 79.22%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 79.42%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 79.91%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 80.23%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 80.26%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 79.44%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 77.85%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 76.99%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 76.56%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 75.77%   [EVAL] batch:   49 | acc: 25.00%,  total acc: 74.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 75.12%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 75.48%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 75.71%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 75.81%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 76.14%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 76.34%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 75.55%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 74.89%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 74.15%   [EVAL] batch:   59 | acc: 31.25%,  total acc: 73.44%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 72.85%   [EVAL] batch:   61 | acc: 6.25%,  total acc: 71.77%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 70.93%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 74.11%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 67.36%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 65.00%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 61.93%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 60.94%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 60.58%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 62.05%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 63.75%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 65.23%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 66.54%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 67.71%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 69.08%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 69.06%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 69.64%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 69.60%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 69.29%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 69.79%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 70.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 71.63%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 72.69%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 73.66%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 74.35%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 75.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 76.01%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 76.56%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 77.27%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 76.84%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 76.79%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 77.36%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 77.63%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 78.21%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 79.27%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 79.61%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 79.94%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 79.97%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 79.89%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 79.39%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 78.91%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 78.83%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 78.50%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 78.55%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 78.73%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 79.01%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 78.82%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 78.41%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 78.68%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 78.51%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 77.80%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 77.22%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 76.77%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 76.54%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 75.81%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 75.30%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 74.71%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 73.94%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 73.11%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 72.39%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 71.97%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 71.74%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 72.14%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 72.54%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 72.57%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 72.86%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 73.14%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 73.33%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 73.19%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 72.97%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 73.00%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 72.94%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 72.89%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 72.84%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 72.56%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 71.84%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 71.13%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 70.51%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 70.06%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 69.54%   [EVAL] batch:   87 | acc: 37.50%,  total acc: 69.18%   [EVAL] batch:   88 | acc: 31.25%,  total acc: 68.75%   [EVAL] batch:   89 | acc: 25.00%,  total acc: 68.26%   [EVAL] batch:   90 | acc: 37.50%,  total acc: 67.93%   [EVAL] batch:   91 | acc: 18.75%,  total acc: 67.39%   [EVAL] batch:   92 | acc: 25.00%,  total acc: 66.94%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 66.49%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 66.51%   [EVAL] batch:   95 | acc: 68.75%,  total acc: 66.54%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 66.43%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 66.45%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 66.48%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 66.31%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 66.27%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 66.36%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 66.44%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 66.59%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 66.61%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 66.86%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 66.76%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 66.38%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 66.06%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 65.85%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 65.60%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 65.46%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 65.49%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 65.79%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 66.09%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 66.38%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 66.95%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 67.17%   [EVAL] batch:  119 | acc: 68.75%,  total acc: 67.19%   [EVAL] batch:  120 | acc: 56.25%,  total acc: 67.10%   [EVAL] batch:  121 | acc: 75.00%,  total acc: 67.16%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 67.23%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 67.24%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 67.20%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 67.11%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 66.93%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 66.80%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 66.72%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 66.68%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 66.70%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 66.81%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 66.92%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 67.02%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 67.13%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 67.23%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 67.38%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 67.35%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 66.86%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 66.47%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 66.05%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 65.76%   [EVAL] batch:  142 | acc: 12.50%,  total acc: 65.38%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 65.15%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 65.34%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 65.58%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 65.77%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 66.01%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 66.23%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 66.42%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 65.98%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 65.54%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 65.11%   [EVAL] batch:  153 | acc: 12.50%,  total acc: 64.77%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 64.35%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 63.94%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 63.89%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 64.04%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 64.23%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 64.38%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 64.52%   [EVAL] batch:  161 | acc: 87.50%,  total acc: 64.66%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 64.69%   [EVAL] batch:  163 | acc: 12.50%,  total acc: 64.37%   [EVAL] batch:  164 | acc: 50.00%,  total acc: 64.28%   [EVAL] batch:  165 | acc: 31.25%,  total acc: 64.08%   [EVAL] batch:  166 | acc: 50.00%,  total acc: 64.00%   [EVAL] batch:  167 | acc: 50.00%,  total acc: 63.91%   [EVAL] batch:  168 | acc: 43.75%,  total acc: 63.79%   [EVAL] batch:  169 | acc: 43.75%,  total acc: 63.68%   [EVAL] batch:  170 | acc: 68.75%,  total acc: 63.71%   [EVAL] batch:  171 | acc: 50.00%,  total acc: 63.63%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 63.48%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 63.40%   [EVAL] batch:  174 | acc: 75.00%,  total acc: 63.46%   [EVAL] batch:  175 | acc: 25.00%,  total acc: 63.25%   [EVAL] batch:  176 | acc: 37.50%,  total acc: 63.10%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 62.99%   [EVAL] batch:  178 | acc: 37.50%,  total acc: 62.85%   [EVAL] batch:  179 | acc: 25.00%,  total acc: 62.64%   [EVAL] batch:  180 | acc: 37.50%,  total acc: 62.50%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 62.57%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 62.70%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 62.91%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 63.07%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 63.27%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 63.37%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 63.53%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 63.69%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 63.88%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 64.07%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 64.16%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 64.35%   [EVAL] batch:  193 | acc: 87.50%,  total acc: 64.47%   [EVAL] batch:  194 | acc: 56.25%,  total acc: 64.42%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 64.45%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 64.44%   [EVAL] batch:  197 | acc: 62.50%,  total acc: 64.43%   [EVAL] batch:  198 | acc: 62.50%,  total acc: 64.42%   [EVAL] batch:  199 | acc: 56.25%,  total acc: 64.38%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 64.33%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 64.39%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 64.38%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 64.34%   [EVAL] batch:  204 | acc: 50.00%,  total acc: 64.27%   [EVAL] batch:  205 | acc: 62.50%,  total acc: 64.26%   [EVAL] batch:  206 | acc: 25.00%,  total acc: 64.07%   [EVAL] batch:  207 | acc: 31.25%,  total acc: 63.91%   [EVAL] batch:  208 | acc: 43.75%,  total acc: 63.82%   [EVAL] batch:  209 | acc: 31.25%,  total acc: 63.66%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 63.54%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 63.38%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 63.41%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 63.58%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 63.75%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 63.92%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 64.08%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 64.25%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 64.38%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 64.38%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 64.45%   [EVAL] batch:  221 | acc: 68.75%,  total acc: 64.47%   [EVAL] batch:  222 | acc: 56.25%,  total acc: 64.43%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 64.51%   [EVAL] batch:  224 | acc: 43.75%,  total acc: 64.42%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 64.57%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 64.73%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 64.88%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 65.04%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 65.19%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 65.31%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 65.46%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 65.61%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 65.76%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 65.90%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 66.05%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 66.17%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 66.26%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 66.32%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 66.43%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 66.55%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 66.58%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 66.67%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 66.78%   [EVAL] batch:  244 | acc: 87.50%,  total acc: 66.86%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 66.95%   [EVAL] batch:  246 | acc: 87.50%,  total acc: 67.03%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 67.16%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 67.24%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 67.35%   [EVAL] batch:  250 | acc: 37.50%,  total acc: 67.23%   [EVAL] batch:  251 | acc: 43.75%,  total acc: 67.14%   [EVAL] batch:  252 | acc: 50.00%,  total acc: 67.07%   [EVAL] batch:  253 | acc: 43.75%,  total acc: 66.98%   [EVAL] batch:  254 | acc: 37.50%,  total acc: 66.86%   [EVAL] batch:  255 | acc: 50.00%,  total acc: 66.80%   [EVAL] batch:  256 | acc: 37.50%,  total acc: 66.68%   [EVAL] batch:  257 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 66.67%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 66.66%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 66.57%   [EVAL] batch:  261 | acc: 62.50%,  total acc: 66.56%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 66.61%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 66.55%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 66.53%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 66.49%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 66.46%   [EVAL] batch:  267 | acc: 50.00%,  total acc: 66.39%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 66.47%   [EVAL] batch:  269 | acc: 81.25%,  total acc: 66.53%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 66.65%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 66.77%   [EVAL] batch:  272 | acc: 87.50%,  total acc: 66.85%   [EVAL] batch:  273 | acc: 81.25%,  total acc: 66.90%   [EVAL] batch:  274 | acc: 93.75%,  total acc: 67.00%   [EVAL] batch:  275 | acc: 81.25%,  total acc: 67.05%   [EVAL] batch:  276 | acc: 68.75%,  total acc: 67.06%   [EVAL] batch:  277 | acc: 81.25%,  total acc: 67.11%   [EVAL] batch:  278 | acc: 62.50%,  total acc: 67.09%   [EVAL] batch:  279 | acc: 81.25%,  total acc: 67.14%   [EVAL] batch:  280 | acc: 75.00%,  total acc: 67.17%   [EVAL] batch:  281 | acc: 43.75%,  total acc: 67.09%   [EVAL] batch:  282 | acc: 31.25%,  total acc: 66.96%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 66.75%   [EVAL] batch:  284 | acc: 18.75%,  total acc: 66.58%   [EVAL] batch:  285 | acc: 6.25%,  total acc: 66.37%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 66.14%   [EVAL] batch:  287 | acc: 43.75%,  total acc: 66.06%   [EVAL] batch:  288 | acc: 56.25%,  total acc: 66.03%   [EVAL] batch:  289 | acc: 56.25%,  total acc: 65.99%   [EVAL] batch:  290 | acc: 43.75%,  total acc: 65.91%   [EVAL] batch:  291 | acc: 62.50%,  total acc: 65.90%   [EVAL] batch:  292 | acc: 50.00%,  total acc: 65.85%   [EVAL] batch:  293 | acc: 62.50%,  total acc: 65.84%   [EVAL] batch:  294 | acc: 81.25%,  total acc: 65.89%   [EVAL] batch:  295 | acc: 62.50%,  total acc: 65.88%   [EVAL] batch:  296 | acc: 62.50%,  total acc: 65.87%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 65.83%   [EVAL] batch:  298 | acc: 62.50%,  total acc: 65.82%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 65.85%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 65.97%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 66.08%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 66.19%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 66.52%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 66.61%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 66.68%   [EVAL] batch:  308 | acc: 81.25%,  total acc: 66.73%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 66.81%   [EVAL] batch:  310 | acc: 81.25%,  total acc: 66.86%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 66.97%   [EVAL] batch:  312 | acc: 75.00%,  total acc: 66.99%   [EVAL] batch:  313 | acc: 18.75%,  total acc: 66.84%   [EVAL] batch:  314 | acc: 25.00%,  total acc: 66.71%   [EVAL] batch:  315 | acc: 12.50%,  total acc: 66.53%   [EVAL] batch:  316 | acc: 12.50%,  total acc: 66.36%   [EVAL] batch:  317 | acc: 25.00%,  total acc: 66.23%   [EVAL] batch:  318 | acc: 25.00%,  total acc: 66.11%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 66.17%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 66.28%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 66.36%   [EVAL] batch:  322 | acc: 81.25%,  total acc: 66.41%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 66.47%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 66.54%   [EVAL] batch:  325 | acc: 37.50%,  total acc: 66.45%   [EVAL] batch:  326 | acc: 12.50%,  total acc: 66.28%   [EVAL] batch:  327 | acc: 18.75%,  total acc: 66.14%   [EVAL] batch:  328 | acc: 62.50%,  total acc: 66.13%   [EVAL] batch:  329 | acc: 50.00%,  total acc: 66.08%   [EVAL] batch:  330 | acc: 31.25%,  total acc: 65.97%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 66.04%   [EVAL] batch:  332 | acc: 93.75%,  total acc: 66.12%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 66.22%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 66.31%   [EVAL] batch:  335 | acc: 93.75%,  total acc: 66.39%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 66.49%   [EVAL] batch:  337 | acc: 81.25%,  total acc: 66.53%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 66.59%   [EVAL] batch:  339 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 66.77%   [EVAL] batch:  341 | acc: 81.25%,  total acc: 66.81%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 66.91%   [EVAL] batch:  343 | acc: 93.75%,  total acc: 66.99%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 67.08%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 67.18%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 67.24%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 67.31%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 67.41%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 67.50%   [EVAL] batch:  350 | acc: 25.00%,  total acc: 67.38%   [EVAL] batch:  351 | acc: 18.75%,  total acc: 67.24%   [EVAL] batch:  352 | acc: 37.50%,  total acc: 67.16%   [EVAL] batch:  353 | acc: 31.25%,  total acc: 67.06%   [EVAL] batch:  354 | acc: 6.25%,  total acc: 66.88%   [EVAL] batch:  355 | acc: 0.00%,  total acc: 66.70%   [EVAL] batch:  356 | acc: 81.25%,  total acc: 66.74%   [EVAL] batch:  357 | acc: 87.50%,  total acc: 66.79%   [EVAL] batch:  358 | acc: 87.50%,  total acc: 66.85%   [EVAL] batch:  359 | acc: 93.75%,  total acc: 66.93%   [EVAL] batch:  360 | acc: 68.75%,  total acc: 66.93%   [EVAL] batch:  361 | acc: 68.75%,  total acc: 66.94%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 66.92%   [EVAL] batch:  363 | acc: 50.00%,  total acc: 66.88%   [EVAL] batch:  364 | acc: 37.50%,  total acc: 66.80%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 66.75%   [EVAL] batch:  366 | acc: 37.50%,  total acc: 66.67%   [EVAL] batch:  367 | acc: 50.00%,  total acc: 66.63%   [EVAL] batch:  368 | acc: 25.00%,  total acc: 66.51%   [EVAL] batch:  369 | acc: 75.00%,  total acc: 66.54%   [EVAL] batch:  370 | acc: 68.75%,  total acc: 66.54%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 66.48%   [EVAL] batch:  372 | acc: 68.75%,  total acc: 66.49%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 66.54%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 66.53%   [EVAL] batch:  375 | acc: 12.50%,  total acc: 66.39%   [EVAL] batch:  376 | acc: 43.75%,  total acc: 66.33%   [EVAL] batch:  377 | acc: 12.50%,  total acc: 66.19%   [EVAL] batch:  378 | acc: 25.00%,  total acc: 66.08%   [EVAL] batch:  379 | acc: 25.00%,  total acc: 65.97%   [EVAL] batch:  380 | acc: 31.25%,  total acc: 65.88%   [EVAL] batch:  381 | acc: 75.00%,  total acc: 65.90%   [EVAL] batch:  382 | acc: 87.50%,  total acc: 65.96%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 66.03%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 66.09%   [EVAL] batch:  385 | acc: 93.75%,  total acc: 66.16%   [EVAL] batch:  386 | acc: 87.50%,  total acc: 66.21%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 66.27%   [EVAL] batch:  388 | acc: 87.50%,  total acc: 66.32%   [EVAL] batch:  389 | acc: 93.75%,  total acc: 66.39%   [EVAL] batch:  390 | acc: 93.75%,  total acc: 66.46%   [EVAL] batch:  391 | acc: 87.50%,  total acc: 66.52%   [EVAL] batch:  392 | acc: 100.00%,  total acc: 66.60%   [EVAL] batch:  393 | acc: 75.00%,  total acc: 66.62%   [EVAL] batch:  394 | acc: 12.50%,  total acc: 66.49%   [EVAL] batch:  395 | acc: 12.50%,  total acc: 66.35%   [EVAL] batch:  396 | acc: 43.75%,  total acc: 66.29%   [EVAL] batch:  397 | acc: 50.00%,  total acc: 66.25%   [EVAL] batch:  398 | acc: 31.25%,  total acc: 66.17%   [EVAL] batch:  399 | acc: 31.25%,  total acc: 66.08%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 65.93%   [EVAL] batch:  401 | acc: 12.50%,  total acc: 65.80%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 65.63%   [EVAL] batch:  403 | acc: 6.25%,  total acc: 65.49%   [EVAL] batch:  404 | acc: 6.25%,  total acc: 65.34%   [EVAL] batch:  405 | acc: 6.25%,  total acc: 65.19%   [EVAL] batch:  406 | acc: 56.25%,  total acc: 65.17%   [EVAL] batch:  407 | acc: 81.25%,  total acc: 65.21%   [EVAL] batch:  408 | acc: 87.50%,  total acc: 65.27%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 65.35%   [EVAL] batch:  410 | acc: 87.50%,  total acc: 65.40%   [EVAL] batch:  411 | acc: 62.50%,  total acc: 65.40%   [EVAL] batch:  412 | acc: 68.75%,  total acc: 65.41%   [EVAL] batch:  413 | acc: 87.50%,  total acc: 65.46%   [EVAL] batch:  414 | acc: 93.75%,  total acc: 65.53%   [EVAL] batch:  415 | acc: 81.25%,  total acc: 65.56%   [EVAL] batch:  416 | acc: 75.00%,  total acc: 65.59%   [EVAL] batch:  417 | acc: 87.50%,  total acc: 65.64%   [EVAL] batch:  418 | acc: 87.50%,  total acc: 65.69%   [EVAL] batch:  419 | acc: 37.50%,  total acc: 65.62%   [EVAL] batch:  420 | acc: 31.25%,  total acc: 65.54%   [EVAL] batch:  421 | acc: 37.50%,  total acc: 65.48%   [EVAL] batch:  422 | acc: 25.00%,  total acc: 65.38%   [EVAL] batch:  423 | acc: 43.75%,  total acc: 65.33%   [EVAL] batch:  424 | acc: 31.25%,  total acc: 65.25%   [EVAL] batch:  425 | acc: 12.50%,  total acc: 65.13%   [EVAL] batch:  426 | acc: 43.75%,  total acc: 65.08%   [EVAL] batch:  427 | acc: 37.50%,  total acc: 65.01%   [EVAL] batch:  428 | acc: 37.50%,  total acc: 64.95%   [EVAL] batch:  429 | acc: 50.00%,  total acc: 64.91%   [EVAL] batch:  430 | acc: 56.25%,  total acc: 64.89%   [EVAL] batch:  431 | acc: 37.50%,  total acc: 64.83%   [EVAL] batch:  432 | acc: 37.50%,  total acc: 64.77%   [EVAL] batch:  433 | acc: 50.00%,  total acc: 64.73%   [EVAL] batch:  434 | acc: 68.75%,  total acc: 64.74%   [EVAL] batch:  435 | acc: 50.00%,  total acc: 64.71%   [EVAL] batch:  436 | acc: 81.25%,  total acc: 64.75%   [EVAL] batch:  437 | acc: 56.25%,  total acc: 64.73%   [EVAL] batch:  438 | acc: 68.75%,  total acc: 64.74%   [EVAL] batch:  439 | acc: 81.25%,  total acc: 64.77%   [EVAL] batch:  440 | acc: 81.25%,  total acc: 64.81%   [EVAL] batch:  441 | acc: 68.75%,  total acc: 64.82%   [EVAL] batch:  442 | acc: 87.50%,  total acc: 64.87%   [EVAL] batch:  443 | acc: 56.25%,  total acc: 64.85%   [EVAL] batch:  444 | acc: 75.00%,  total acc: 64.87%   [EVAL] batch:  445 | acc: 68.75%,  total acc: 64.88%   [EVAL] batch:  446 | acc: 62.50%,  total acc: 64.88%   [EVAL] batch:  447 | acc: 81.25%,  total acc: 64.91%   [EVAL] batch:  448 | acc: 75.00%,  total acc: 64.94%   [EVAL] batch:  449 | acc: 81.25%,  total acc: 64.97%   [EVAL] batch:  450 | acc: 81.25%,  total acc: 65.01%   [EVAL] batch:  451 | acc: 81.25%,  total acc: 65.04%   [EVAL] batch:  452 | acc: 68.75%,  total acc: 65.05%   [EVAL] batch:  453 | acc: 93.75%,  total acc: 65.12%   [EVAL] batch:  454 | acc: 75.00%,  total acc: 65.14%   [EVAL] batch:  455 | acc: 87.50%,  total acc: 65.19%   [EVAL] batch:  456 | acc: 56.25%,  total acc: 65.17%   [EVAL] batch:  457 | acc: 31.25%,  total acc: 65.09%   [EVAL] batch:  458 | acc: 56.25%,  total acc: 65.07%   [EVAL] batch:  459 | acc: 56.25%,  total acc: 65.05%   [EVAL] batch:  460 | acc: 37.50%,  total acc: 64.99%   [EVAL] batch:  461 | acc: 50.00%,  total acc: 64.96%   [EVAL] batch:  462 | acc: 50.00%,  total acc: 64.93%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 65.01%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 65.08%   [EVAL] batch:  465 | acc: 75.00%,  total acc: 65.10%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:  467 | acc: 93.75%,  total acc: 65.24%   [EVAL] batch:  468 | acc: 100.00%,  total acc: 65.31%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 65.39%   [EVAL] batch:  470 | acc: 100.00%,  total acc: 65.46%   [EVAL] batch:  471 | acc: 100.00%,  total acc: 65.53%   [EVAL] batch:  472 | acc: 100.00%,  total acc: 65.61%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 65.68%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 65.75%   [EVAL] batch:  475 | acc: 100.00%,  total acc: 65.82%   [EVAL] batch:  476 | acc: 93.75%,  total acc: 65.88%   [EVAL] batch:  477 | acc: 93.75%,  total acc: 65.94%   [EVAL] batch:  478 | acc: 93.75%,  total acc: 66.00%   [EVAL] batch:  479 | acc: 100.00%,  total acc: 66.07%   [EVAL] batch:  480 | acc: 93.75%,  total acc: 66.13%   [EVAL] batch:  481 | acc: 56.25%,  total acc: 66.10%   [EVAL] batch:  482 | acc: 25.00%,  total acc: 66.02%   [EVAL] batch:  483 | acc: 18.75%,  total acc: 65.92%   [EVAL] batch:  484 | acc: 43.75%,  total acc: 65.88%   [EVAL] batch:  485 | acc: 56.25%,  total acc: 65.86%   [EVAL] batch:  486 | acc: 25.00%,  total acc: 65.77%   [EVAL] batch:  487 | acc: 62.50%,  total acc: 65.77%   [EVAL] batch:  488 | acc: 87.50%,  total acc: 65.81%   [EVAL] batch:  489 | acc: 100.00%,  total acc: 65.88%   [EVAL] batch:  490 | acc: 81.25%,  total acc: 65.91%   [EVAL] batch:  491 | acc: 87.50%,  total acc: 65.96%   [EVAL] batch:  492 | acc: 87.50%,  total acc: 66.00%   [EVAL] batch:  493 | acc: 75.00%,  total acc: 66.02%   [EVAL] batch:  494 | acc: 6.25%,  total acc: 65.90%   [EVAL] batch:  495 | acc: 43.75%,  total acc: 65.85%   [EVAL] batch:  496 | acc: 37.50%,  total acc: 65.79%   [EVAL] batch:  497 | acc: 25.00%,  total acc: 65.71%   [EVAL] batch:  498 | acc: 25.00%,  total acc: 65.63%   [EVAL] batch:  499 | acc: 25.00%,  total acc: 65.55%   
cur_acc:  ['0.9504', '0.7540', '0.7688', '0.8512', '0.7312', '0.7222', '0.5784', '0.7093']
his_acc:  ['0.9504', '0.8390', '0.8142', '0.8030', '0.7690', '0.7228', '0.6768', '0.6555']
--------Round  1
seed:  200
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
CurrentTrain: epoch 15, batch     0 | loss: 25.7625867CurrentTrain: epoch 15, batch     1 | loss: 28.7622145CurrentTrain: epoch 15, batch     2 | loss: 20.7869018CurrentTrain: epoch 15, batch     3 | loss: 27.9201194CurrentTrain: epoch 15, batch     4 | loss: 26.7292595CurrentTrain: epoch 15, batch     5 | loss: 19.1428046CurrentTrain: epoch 15, batch     6 | loss: 19.7028210CurrentTrain: epoch 15, batch     7 | loss: 23.6200455CurrentTrain: epoch 15, batch     8 | loss: 20.6619756CurrentTrain: epoch 15, batch     9 | loss: 21.1752034CurrentTrain: epoch 15, batch    10 | loss: 21.4517319CurrentTrain: epoch 15, batch    11 | loss: 28.6646973CurrentTrain: epoch 15, batch    12 | loss: 23.3243257CurrentTrain: epoch 15, batch    13 | loss: 23.2007849CurrentTrain: epoch 15, batch    14 | loss: 22.5328604CurrentTrain: epoch 15, batch    15 | loss: 19.4526959CurrentTrain: epoch 15, batch    16 | loss: 20.5256947CurrentTrain: epoch 15, batch    17 | loss: 21.8267555CurrentTrain: epoch 15, batch    18 | loss: 17.9172684CurrentTrain: epoch 15, batch    19 | loss: 30.8909088CurrentTrain: epoch 15, batch    20 | loss: 19.4898602CurrentTrain: epoch 15, batch    21 | loss: 16.3104573CurrentTrain: epoch 15, batch    22 | loss: 16.0522616CurrentTrain: epoch 15, batch    23 | loss: 21.3431028CurrentTrain: epoch 15, batch    24 | loss: 19.1285647CurrentTrain: epoch 15, batch    25 | loss: 15.6104811CurrentTrain: epoch 15, batch    26 | loss: 19.5588050CurrentTrain: epoch 15, batch    27 | loss: 16.5222855CurrentTrain: epoch 15, batch    28 | loss: 16.1258679CurrentTrain: epoch 15, batch    29 | loss: 18.9363261CurrentTrain: epoch 15, batch    30 | loss: 12.1446925CurrentTrain: epoch 15, batch    31 | loss: 22.5464302CurrentTrain: epoch 15, batch    32 | loss: 15.3106764CurrentTrain: epoch 15, batch    33 | loss: 14.5772387CurrentTrain: epoch 15, batch    34 | loss: 15.9853073CurrentTrain: epoch 15, batch    35 | loss: 13.4882170CurrentTrain: epoch 15, batch    36 | loss: 13.4880801CurrentTrain: epoch 15, batch    37 | loss: 14.9201647CurrentTrain: epoch 15, batch    38 | loss: 20.6128949CurrentTrain: epoch 15, batch    39 | loss: 19.7619269CurrentTrain: epoch 15, batch    40 | loss: 21.8332353CurrentTrain: epoch 15, batch    41 | loss: 15.9355856CurrentTrain: epoch 15, batch    42 | loss: 15.5705643CurrentTrain: epoch 15, batch    43 | loss: 13.9456534CurrentTrain: epoch 15, batch    44 | loss: 17.0273757CurrentTrain: epoch 15, batch    45 | loss: 24.3558848CurrentTrain: epoch 15, batch    46 | loss: 18.8677604CurrentTrain: epoch 15, batch    47 | loss: 11.3135972CurrentTrain: epoch 15, batch    48 | loss: 15.3875691CurrentTrain: epoch 15, batch    49 | loss: 15.9696549CurrentTrain: epoch 15, batch    50 | loss: 16.8777318CurrentTrain: epoch 15, batch    51 | loss: 12.8445416CurrentTrain: epoch 15, batch    52 | loss: 19.2967951CurrentTrain: epoch 15, batch    53 | loss: 19.3210182CurrentTrain: epoch 15, batch    54 | loss: 23.6836202CurrentTrain: epoch 15, batch    55 | loss: 15.0022130CurrentTrain: epoch 15, batch    56 | loss: 13.5917198CurrentTrain: epoch 15, batch    57 | loss: 22.5551383CurrentTrain: epoch 15, batch    58 | loss: 16.2531731CurrentTrain: epoch 15, batch    59 | loss: 13.6458528CurrentTrain: epoch 15, batch    60 | loss: 15.3520707CurrentTrain: epoch 15, batch    61 | loss: 13.6507892CurrentTrain: epoch  7, batch    62 | loss: 12.2213515CurrentTrain: epoch 15, batch     0 | loss: 24.6527906CurrentTrain: epoch 15, batch     1 | loss: 14.9006508CurrentTrain: epoch 15, batch     2 | loss: 20.0898480CurrentTrain: epoch 15, batch     3 | loss: 21.3633646CurrentTrain: epoch 15, batch     4 | loss: 14.5362796CurrentTrain: epoch 15, batch     5 | loss: 13.4261322CurrentTrain: epoch 15, batch     6 | loss: 26.4482752CurrentTrain: epoch 15, batch     7 | loss: 13.6508597CurrentTrain: epoch 15, batch     8 | loss: 16.5342124CurrentTrain: epoch 15, batch     9 | loss: 22.3180447CurrentTrain: epoch 15, batch    10 | loss: 11.9320509CurrentTrain: epoch 15, batch    11 | loss: 12.4281145CurrentTrain: epoch 15, batch    12 | loss: 18.9028699CurrentTrain: epoch 15, batch    13 | loss: 14.7850597CurrentTrain: epoch 15, batch    14 | loss: 12.8338825CurrentTrain: epoch 15, batch    15 | loss: 15.1582038CurrentTrain: epoch 15, batch    16 | loss: 13.5359613CurrentTrain: epoch 15, batch    17 | loss: 15.1948232CurrentTrain: epoch 15, batch    18 | loss: 19.3950419CurrentTrain: epoch 15, batch    19 | loss: 18.1322838CurrentTrain: epoch 15, batch    20 | loss: 13.3407903CurrentTrain: epoch 15, batch    21 | loss: 13.2786188CurrentTrain: epoch 15, batch    22 | loss: 18.2749766CurrentTrain: epoch 15, batch    23 | loss: 10.0720284CurrentTrain: epoch 15, batch    24 | loss: 15.0183581CurrentTrain: epoch 15, batch    25 | loss: 16.7990245CurrentTrain: epoch 15, batch    26 | loss: 12.4841799CurrentTrain: epoch 15, batch    27 | loss: 11.6229148CurrentTrain: epoch 15, batch    28 | loss: 13.9700904CurrentTrain: epoch 15, batch    29 | loss: 12.3457561CurrentTrain: epoch 15, batch    30 | loss: 10.6456467CurrentTrain: epoch 15, batch    31 | loss: 9.7603055CurrentTrain: epoch 15, batch    32 | loss: 15.9657273CurrentTrain: epoch 15, batch    33 | loss: 10.8553012CurrentTrain: epoch 15, batch    34 | loss: 15.1470934CurrentTrain: epoch 15, batch    35 | loss: 13.4731679CurrentTrain: epoch 15, batch    36 | loss: 12.0134837CurrentTrain: epoch 15, batch    37 | loss: 19.1184297CurrentTrain: epoch 15, batch    38 | loss: 23.6859760CurrentTrain: epoch 15, batch    39 | loss: 20.1755519CurrentTrain: epoch 15, batch    40 | loss: 15.6627646CurrentTrain: epoch 15, batch    41 | loss: 11.3468947CurrentTrain: epoch 15, batch    42 | loss: 20.7212386CurrentTrain: epoch 15, batch    43 | loss: 21.8810367CurrentTrain: epoch 15, batch    44 | loss: 14.1930020CurrentTrain: epoch 15, batch    45 | loss: 12.8907233CurrentTrain: epoch 15, batch    46 | loss: 22.3242184CurrentTrain: epoch 15, batch    47 | loss: 15.0082172CurrentTrain: epoch 15, batch    48 | loss: 14.7034803CurrentTrain: epoch 15, batch    49 | loss: 10.3112033CurrentTrain: epoch 15, batch    50 | loss: 14.5801973CurrentTrain: epoch 15, batch    51 | loss: 14.9668100CurrentTrain: epoch 15, batch    52 | loss: 13.5168615CurrentTrain: epoch 15, batch    53 | loss: 10.6338779CurrentTrain: epoch 15, batch    54 | loss: 10.9024461CurrentTrain: epoch 15, batch    55 | loss: 13.2979600CurrentTrain: epoch 15, batch    56 | loss: 15.9233378CurrentTrain: epoch 15, batch    57 | loss: 9.6022300CurrentTrain: epoch 15, batch    58 | loss: 11.2682462CurrentTrain: epoch 15, batch    59 | loss: 10.4754051CurrentTrain: epoch 15, batch    60 | loss: 10.8635742CurrentTrain: epoch 15, batch    61 | loss: 18.5884767CurrentTrain: epoch  7, batch    62 | loss: 10.7382557CurrentTrain: epoch 15, batch     0 | loss: 28.4760300CurrentTrain: epoch 15, batch     1 | loss: 13.2957309CurrentTrain: epoch 15, batch     2 | loss: 14.4645938CurrentTrain: epoch 15, batch     3 | loss: 13.7531586CurrentTrain: epoch 15, batch     4 | loss: 13.4085185CurrentTrain: epoch 15, batch     5 | loss: 14.2769058CurrentTrain: epoch 15, batch     6 | loss: 16.0165454CurrentTrain: epoch 15, batch     7 | loss: 14.5911692CurrentTrain: epoch 15, batch     8 | loss: 7.9815633CurrentTrain: epoch 15, batch     9 | loss: 12.9774257CurrentTrain: epoch 15, batch    10 | loss: 13.6872694CurrentTrain: epoch 15, batch    11 | loss: 15.4222626CurrentTrain: epoch 15, batch    12 | loss: 15.8813242CurrentTrain: epoch 15, batch    13 | loss: 18.6379661CurrentTrain: epoch 15, batch    14 | loss: 12.5213702CurrentTrain: epoch 15, batch    15 | loss: 10.2291561CurrentTrain: epoch 15, batch    16 | loss: 17.3896792CurrentTrain: epoch 15, batch    17 | loss: 11.9893353CurrentTrain: epoch 15, batch    18 | loss: 11.9141118CurrentTrain: epoch 15, batch    19 | loss: 11.4477377CurrentTrain: epoch 15, batch    20 | loss: 18.8133002CurrentTrain: epoch 15, batch    21 | loss: 11.0821546CurrentTrain: epoch 15, batch    22 | loss: 16.4746318CurrentTrain: epoch 15, batch    23 | loss: 12.0699028CurrentTrain: epoch 15, batch    24 | loss: 26.5753291CurrentTrain: epoch 15, batch    25 | loss: 12.3163621CurrentTrain: epoch 15, batch    26 | loss: 15.6737490CurrentTrain: epoch 15, batch    27 | loss: 16.8071649CurrentTrain: epoch 15, batch    28 | loss: 8.2840259CurrentTrain: epoch 15, batch    29 | loss: 14.9740286CurrentTrain: epoch 15, batch    30 | loss: 16.7697369CurrentTrain: epoch 15, batch    31 | loss: 19.2217598CurrentTrain: epoch 15, batch    32 | loss: 11.3530319CurrentTrain: epoch 15, batch    33 | loss: 10.3353836CurrentTrain: epoch 15, batch    34 | loss: 16.9536538CurrentTrain: epoch 15, batch    35 | loss: 16.6511678CurrentTrain: epoch 15, batch    36 | loss: 9.4233533CurrentTrain: epoch 15, batch    37 | loss: 13.8232738CurrentTrain: epoch 15, batch    38 | loss: 13.5851436CurrentTrain: epoch 15, batch    39 | loss: 15.5492624CurrentTrain: epoch 15, batch    40 | loss: 11.4916575CurrentTrain: epoch 15, batch    41 | loss: 11.6629612CurrentTrain: epoch 15, batch    42 | loss: 10.5202356CurrentTrain: epoch 15, batch    43 | loss: 13.4573777CurrentTrain: epoch 15, batch    44 | loss: 10.0149121CurrentTrain: epoch 15, batch    45 | loss: 16.3035696CurrentTrain: epoch 15, batch    46 | loss: 19.1796634CurrentTrain: epoch 15, batch    47 | loss: 13.8387958CurrentTrain: epoch 15, batch    48 | loss: 16.7319491CurrentTrain: epoch 15, batch    49 | loss: 17.4925282CurrentTrain: epoch 15, batch    50 | loss: 12.3527843CurrentTrain: epoch 15, batch    51 | loss: 10.6127725CurrentTrain: epoch 15, batch    52 | loss: 19.1075628CurrentTrain: epoch 15, batch    53 | loss: 18.2599646CurrentTrain: epoch 15, batch    54 | loss: 11.1960218CurrentTrain: epoch 15, batch    55 | loss: 12.9310307CurrentTrain: epoch 15, batch    56 | loss: 18.4141677CurrentTrain: epoch 15, batch    57 | loss: 11.9530129CurrentTrain: epoch 15, batch    58 | loss: 18.1957051CurrentTrain: epoch 15, batch    59 | loss: 11.7765198CurrentTrain: epoch 15, batch    60 | loss: 11.5119248CurrentTrain: epoch 15, batch    61 | loss: 21.6027312CurrentTrain: epoch  7, batch    62 | loss: 15.9135342CurrentTrain: epoch 15, batch     0 | loss: 17.5545716CurrentTrain: epoch 15, batch     1 | loss: 19.0804285CurrentTrain: epoch 15, batch     2 | loss: 14.9930900CurrentTrain: epoch 15, batch     3 | loss: 12.5077288CurrentTrain: epoch 15, batch     4 | loss: 9.3441858CurrentTrain: epoch 15, batch     5 | loss: 12.0749597CurrentTrain: epoch 15, batch     6 | loss: 10.9414817CurrentTrain: epoch 15, batch     7 | loss: 25.4628380CurrentTrain: epoch 15, batch     8 | loss: 8.8096583CurrentTrain: epoch 15, batch     9 | loss: 12.7602246CurrentTrain: epoch 15, batch    10 | loss: 27.4615980CurrentTrain: epoch 15, batch    11 | loss: 7.9480277CurrentTrain: epoch 15, batch    12 | loss: 9.3792138CurrentTrain: epoch 15, batch    13 | loss: 12.0749444CurrentTrain: epoch 15, batch    14 | loss: 14.3552745CurrentTrain: epoch 15, batch    15 | loss: 22.5290989CurrentTrain: epoch 15, batch    16 | loss: 7.5391436CurrentTrain: epoch 15, batch    17 | loss: 14.8460569CurrentTrain: epoch 15, batch    18 | loss: 10.2069035CurrentTrain: epoch 15, batch    19 | loss: 12.1694614CurrentTrain: epoch 15, batch    20 | loss: 36.0094433CurrentTrain: epoch 15, batch    21 | loss: 9.7853502CurrentTrain: epoch 15, batch    22 | loss: 11.8457455CurrentTrain: epoch 15, batch    23 | loss: 11.3751517CurrentTrain: epoch 15, batch    24 | loss: 17.3017315CurrentTrain: epoch 15, batch    25 | loss: 16.8577430CurrentTrain: epoch 15, batch    26 | loss: 11.8048200CurrentTrain: epoch 15, batch    27 | loss: 19.1446927CurrentTrain: epoch 15, batch    28 | loss: 7.3586938CurrentTrain: epoch 15, batch    29 | loss: 17.1647055CurrentTrain: epoch 15, batch    30 | loss: 10.8218171CurrentTrain: epoch 15, batch    31 | loss: 9.2979434CurrentTrain: epoch 15, batch    32 | loss: 11.8518909CurrentTrain: epoch 15, batch    33 | loss: 9.0102544CurrentTrain: epoch 15, batch    34 | loss: 13.5958891CurrentTrain: epoch 15, batch    35 | loss: 14.1849445CurrentTrain: epoch 15, batch    36 | loss: 11.6792304CurrentTrain: epoch 15, batch    37 | loss: 9.7694909CurrentTrain: epoch 15, batch    38 | loss: 25.5058733CurrentTrain: epoch 15, batch    39 | loss: 15.9167160CurrentTrain: epoch 15, batch    40 | loss: 10.1070481CurrentTrain: epoch 15, batch    41 | loss: 21.2322305CurrentTrain: epoch 15, batch    42 | loss: 13.2368095CurrentTrain: epoch 15, batch    43 | loss: 9.9126501CurrentTrain: epoch 15, batch    44 | loss: 10.5856581CurrentTrain: epoch 15, batch    45 | loss: 13.0254348CurrentTrain: epoch 15, batch    46 | loss: 9.8539293CurrentTrain: epoch 15, batch    47 | loss: 11.0622073CurrentTrain: epoch 15, batch    48 | loss: 18.4430510CurrentTrain: epoch 15, batch    49 | loss: 15.7234896CurrentTrain: epoch 15, batch    50 | loss: 11.6640655CurrentTrain: epoch 15, batch    51 | loss: 8.5578117CurrentTrain: epoch 15, batch    52 | loss: 9.1147008CurrentTrain: epoch 15, batch    53 | loss: 15.5605522CurrentTrain: epoch 15, batch    54 | loss: 10.8163843CurrentTrain: epoch 15, batch    55 | loss: 19.6523240CurrentTrain: epoch 15, batch    56 | loss: 21.1026169CurrentTrain: epoch 15, batch    57 | loss: 18.0467058CurrentTrain: epoch 15, batch    58 | loss: 21.6206664CurrentTrain: epoch 15, batch    59 | loss: 16.9729456CurrentTrain: epoch 15, batch    60 | loss: 16.5056049CurrentTrain: epoch 15, batch    61 | loss: 12.5828250CurrentTrain: epoch  7, batch    62 | loss: 11.4648142CurrentTrain: epoch 15, batch     0 | loss: 10.3842955CurrentTrain: epoch 15, batch     1 | loss: 16.6274283CurrentTrain: epoch 15, batch     2 | loss: 24.5946780CurrentTrain: epoch 15, batch     3 | loss: 12.1860346CurrentTrain: epoch 15, batch     4 | loss: 15.9671538CurrentTrain: epoch 15, batch     5 | loss: 21.6589389CurrentTrain: epoch 15, batch     6 | loss: 11.2618593CurrentTrain: epoch 15, batch     7 | loss: 13.1080438CurrentTrain: epoch 15, batch     8 | loss: 24.2521598CurrentTrain: epoch 15, batch     9 | loss: 8.0466865CurrentTrain: epoch 15, batch    10 | loss: 9.5955087CurrentTrain: epoch 15, batch    11 | loss: 10.2184103CurrentTrain: epoch 15, batch    12 | loss: 10.7487045CurrentTrain: epoch 15, batch    13 | loss: 38.7056318CurrentTrain: epoch 15, batch    14 | loss: 13.2139340CurrentTrain: epoch 15, batch    15 | loss: 10.8690412CurrentTrain: epoch 15, batch    16 | loss: 17.1690728CurrentTrain: epoch 15, batch    17 | loss: 9.4986109CurrentTrain: epoch 15, batch    18 | loss: 11.4189291CurrentTrain: epoch 15, batch    19 | loss: 14.7687892CurrentTrain: epoch 15, batch    20 | loss: 16.6843593CurrentTrain: epoch 15, batch    21 | loss: 14.4916379CurrentTrain: epoch 15, batch    22 | loss: 21.2454141CurrentTrain: epoch 15, batch    23 | loss: 12.4440716CurrentTrain: epoch 15, batch    24 | loss: 14.0524520CurrentTrain: epoch 15, batch    25 | loss: 15.6147085CurrentTrain: epoch 15, batch    26 | loss: 10.5121438CurrentTrain: epoch 15, batch    27 | loss: 9.5712019CurrentTrain: epoch 15, batch    28 | loss: 7.7851275CurrentTrain: epoch 15, batch    29 | loss: 9.8115826CurrentTrain: epoch 15, batch    30 | loss: 10.3238572CurrentTrain: epoch 15, batch    31 | loss: 10.6026091CurrentTrain: epoch 15, batch    32 | loss: 18.3956062CurrentTrain: epoch 15, batch    33 | loss: 9.0772749CurrentTrain: epoch 15, batch    34 | loss: 11.4393063CurrentTrain: epoch 15, batch    35 | loss: 9.5793136CurrentTrain: epoch 15, batch    36 | loss: 7.9195900CurrentTrain: epoch 15, batch    37 | loss: 15.7492508CurrentTrain: epoch 15, batch    38 | loss: 11.8584345CurrentTrain: epoch 15, batch    39 | loss: 8.9126182CurrentTrain: epoch 15, batch    40 | loss: 9.6616417CurrentTrain: epoch 15, batch    41 | loss: 15.9195376CurrentTrain: epoch 15, batch    42 | loss: 10.1986823CurrentTrain: epoch 15, batch    43 | loss: 15.2493363CurrentTrain: epoch 15, batch    44 | loss: 14.6909036CurrentTrain: epoch 15, batch    45 | loss: 8.3985881CurrentTrain: epoch 15, batch    46 | loss: 8.7516625CurrentTrain: epoch 15, batch    47 | loss: 9.9688544CurrentTrain: epoch 15, batch    48 | loss: 13.9544357CurrentTrain: epoch 15, batch    49 | loss: 14.0732672CurrentTrain: epoch 15, batch    50 | loss: 10.7410949CurrentTrain: epoch 15, batch    51 | loss: 9.6703946CurrentTrain: epoch 15, batch    52 | loss: 10.5302763CurrentTrain: epoch 15, batch    53 | loss: 10.5406628CurrentTrain: epoch 15, batch    54 | loss: 10.0617466CurrentTrain: epoch 15, batch    55 | loss: 11.1380148CurrentTrain: epoch 15, batch    56 | loss: 10.6981303CurrentTrain: epoch 15, batch    57 | loss: 9.0390442CurrentTrain: epoch 15, batch    58 | loss: 11.3380637CurrentTrain: epoch 15, batch    59 | loss: 11.3117217CurrentTrain: epoch 15, batch    60 | loss: 8.1691535CurrentTrain: epoch 15, batch    61 | loss: 16.9258544CurrentTrain: epoch  7, batch    62 | loss: 10.7173912CurrentTrain: epoch 15, batch     0 | loss: 11.5561728CurrentTrain: epoch 15, batch     1 | loss: 10.9339286CurrentTrain: epoch 15, batch     2 | loss: 9.3256625CurrentTrain: epoch 15, batch     3 | loss: 7.9529091CurrentTrain: epoch 15, batch     4 | loss: 7.3606579CurrentTrain: epoch 15, batch     5 | loss: 11.6711080CurrentTrain: epoch 15, batch     6 | loss: 12.5698602CurrentTrain: epoch 15, batch     7 | loss: 9.2341044CurrentTrain: epoch 15, batch     8 | loss: 10.1916385CurrentTrain: epoch 15, batch     9 | loss: 12.3184697CurrentTrain: epoch 15, batch    10 | loss: 9.8103443CurrentTrain: epoch 15, batch    11 | loss: 16.5157429CurrentTrain: epoch 15, batch    12 | loss: 12.2407325CurrentTrain: epoch 15, batch    13 | loss: 8.2957123CurrentTrain: epoch 15, batch    14 | loss: 21.8893932CurrentTrain: epoch 15, batch    15 | loss: 9.0607779CurrentTrain: epoch 15, batch    16 | loss: 7.4974751CurrentTrain: epoch 15, batch    17 | loss: 16.9667318CurrentTrain: epoch 15, batch    18 | loss: 18.4231275CurrentTrain: epoch 15, batch    19 | loss: 7.8165744CurrentTrain: epoch 15, batch    20 | loss: 10.1191861CurrentTrain: epoch 15, batch    21 | loss: 9.8310996CurrentTrain: epoch 15, batch    22 | loss: 9.3629376CurrentTrain: epoch 15, batch    23 | loss: 10.0954167CurrentTrain: epoch 15, batch    24 | loss: 11.3448508CurrentTrain: epoch 15, batch    25 | loss: 12.1448511CurrentTrain: epoch 15, batch    26 | loss: 7.8247440CurrentTrain: epoch 15, batch    27 | loss: 11.0702090CurrentTrain: epoch 15, batch    28 | loss: 22.3507898CurrentTrain: epoch 15, batch    29 | loss: 14.3706988CurrentTrain: epoch 15, batch    30 | loss: 13.6661028CurrentTrain: epoch 15, batch    31 | loss: 15.7174051CurrentTrain: epoch 15, batch    32 | loss: 9.2870171CurrentTrain: epoch 15, batch    33 | loss: 10.7823937CurrentTrain: epoch 15, batch    34 | loss: 10.8361632CurrentTrain: epoch 15, batch    35 | loss: 12.6690274CurrentTrain: epoch 15, batch    36 | loss: 14.3577080CurrentTrain: epoch 15, batch    37 | loss: 10.0290567CurrentTrain: epoch 15, batch    38 | loss: 8.3133342CurrentTrain: epoch 15, batch    39 | loss: 10.3282895CurrentTrain: epoch 15, batch    40 | loss: 11.3761987CurrentTrain: epoch 15, batch    41 | loss: 16.6361132CurrentTrain: epoch 15, batch    42 | loss: 9.2228692CurrentTrain: epoch 15, batch    43 | loss: 8.8365743CurrentTrain: epoch 15, batch    44 | loss: 9.8885501CurrentTrain: epoch 15, batch    45 | loss: 13.0754851CurrentTrain: epoch 15, batch    46 | loss: 10.4290441CurrentTrain: epoch 15, batch    47 | loss: 9.2522333CurrentTrain: epoch 15, batch    48 | loss: 10.1180840CurrentTrain: epoch 15, batch    49 | loss: 15.8572048CurrentTrain: epoch 15, batch    50 | loss: 12.9327532CurrentTrain: epoch 15, batch    51 | loss: 7.8365680CurrentTrain: epoch 15, batch    52 | loss: 14.5339353CurrentTrain: epoch 15, batch    53 | loss: 14.6835769CurrentTrain: epoch 15, batch    54 | loss: 12.0235626CurrentTrain: epoch 15, batch    55 | loss: 9.4321022CurrentTrain: epoch 15, batch    56 | loss: 8.9185393CurrentTrain: epoch 15, batch    57 | loss: 7.4402722CurrentTrain: epoch 15, batch    58 | loss: 15.5248405CurrentTrain: epoch 15, batch    59 | loss: 17.6571310CurrentTrain: epoch 15, batch    60 | loss: 15.3743539CurrentTrain: epoch 15, batch    61 | loss: 9.1799930CurrentTrain: epoch  7, batch    62 | loss: 22.1478486CurrentTrain: epoch 15, batch     0 | loss: 9.7271993CurrentTrain: epoch 15, batch     1 | loss: 11.1157145CurrentTrain: epoch 15, batch     2 | loss: 10.7371486CurrentTrain: epoch 15, batch     3 | loss: 9.4932685CurrentTrain: epoch 15, batch     4 | loss: 14.5376477CurrentTrain: epoch 15, batch     5 | loss: 14.0625083CurrentTrain: epoch 15, batch     6 | loss: 7.6436530CurrentTrain: epoch 15, batch     7 | loss: 7.9875335CurrentTrain: epoch 15, batch     8 | loss: 8.3705545CurrentTrain: epoch 15, batch     9 | loss: 10.1452041CurrentTrain: epoch 15, batch    10 | loss: 7.4736693CurrentTrain: epoch 15, batch    11 | loss: 10.4716613CurrentTrain: epoch 15, batch    12 | loss: 8.5802622CurrentTrain: epoch 15, batch    13 | loss: 9.3279695CurrentTrain: epoch 15, batch    14 | loss: 13.0177152CurrentTrain: epoch 15, batch    15 | loss: 8.8871213CurrentTrain: epoch 15, batch    16 | loss: 8.6181163CurrentTrain: epoch 15, batch    17 | loss: 9.8727340CurrentTrain: epoch 15, batch    18 | loss: 11.0153988CurrentTrain: epoch 15, batch    19 | loss: 12.5986083CurrentTrain: epoch 15, batch    20 | loss: 7.8223358CurrentTrain: epoch 15, batch    21 | loss: 7.9737797CurrentTrain: epoch 15, batch    22 | loss: 8.6324771CurrentTrain: epoch 15, batch    23 | loss: 10.3782860CurrentTrain: epoch 15, batch    24 | loss: 15.8521791CurrentTrain: epoch 15, batch    25 | loss: 7.8869889CurrentTrain: epoch 15, batch    26 | loss: 17.4878748CurrentTrain: epoch 15, batch    27 | loss: 6.1326824CurrentTrain: epoch 15, batch    28 | loss: 11.6277633CurrentTrain: epoch 15, batch    29 | loss: 13.0542910CurrentTrain: epoch 15, batch    30 | loss: 13.9697772CurrentTrain: epoch 15, batch    31 | loss: 7.8733359CurrentTrain: epoch 15, batch    32 | loss: 10.2290121CurrentTrain: epoch 15, batch    33 | loss: 10.9178027CurrentTrain: epoch 15, batch    34 | loss: 7.8967310CurrentTrain: epoch 15, batch    35 | loss: 10.7421232CurrentTrain: epoch 15, batch    36 | loss: 20.1736144CurrentTrain: epoch 15, batch    37 | loss: 10.7729425CurrentTrain: epoch 15, batch    38 | loss: 8.4018641CurrentTrain: epoch 15, batch    39 | loss: 8.4060987CurrentTrain: epoch 15, batch    40 | loss: 8.9627336CurrentTrain: epoch 15, batch    41 | loss: 7.9577287CurrentTrain: epoch 15, batch    42 | loss: 18.8022383CurrentTrain: epoch 15, batch    43 | loss: 9.0403269CurrentTrain: epoch 15, batch    44 | loss: 7.3344735CurrentTrain: epoch 15, batch    45 | loss: 7.6922044CurrentTrain: epoch 15, batch    46 | loss: 12.6126508CurrentTrain: epoch 15, batch    47 | loss: 7.5436553CurrentTrain: epoch 15, batch    48 | loss: 7.5780578CurrentTrain: epoch 15, batch    49 | loss: 7.9492041CurrentTrain: epoch 15, batch    50 | loss: 9.8350379CurrentTrain: epoch 15, batch    51 | loss: 12.0336222CurrentTrain: epoch 15, batch    52 | loss: 8.3044745CurrentTrain: epoch 15, batch    53 | loss: 10.2135404CurrentTrain: epoch 15, batch    54 | loss: 12.7849917CurrentTrain: epoch 15, batch    55 | loss: 11.3959747CurrentTrain: epoch 15, batch    56 | loss: 9.7051998CurrentTrain: epoch 15, batch    57 | loss: 9.3400000CurrentTrain: epoch 15, batch    58 | loss: 12.5022284CurrentTrain: epoch 15, batch    59 | loss: 10.7131321CurrentTrain: epoch 15, batch    60 | loss: 10.1333281CurrentTrain: epoch 15, batch    61 | loss: 8.8312445CurrentTrain: epoch  7, batch    62 | loss: 12.7408438CurrentTrain: epoch 15, batch     0 | loss: 9.4800927CurrentTrain: epoch 15, batch     1 | loss: 9.2103725CurrentTrain: epoch 15, batch     2 | loss: 9.4685573CurrentTrain: epoch 15, batch     3 | loss: 9.4858928CurrentTrain: epoch 15, batch     4 | loss: 9.2349507CurrentTrain: epoch 15, batch     5 | loss: 14.3880876CurrentTrain: epoch 15, batch     6 | loss: 9.4277311CurrentTrain: epoch 15, batch     7 | loss: 10.8209479CurrentTrain: epoch 15, batch     8 | loss: 7.7512586CurrentTrain: epoch 15, batch     9 | loss: 23.4777488CurrentTrain: epoch 15, batch    10 | loss: 10.2485245CurrentTrain: epoch 15, batch    11 | loss: 12.4802281CurrentTrain: epoch 15, batch    12 | loss: 10.3946099CurrentTrain: epoch 15, batch    13 | loss: 9.7098318CurrentTrain: epoch 15, batch    14 | loss: 14.4745361CurrentTrain: epoch 15, batch    15 | loss: 15.7121045CurrentTrain: epoch 15, batch    16 | loss: 14.6482773CurrentTrain: epoch 15, batch    17 | loss: 10.4872721CurrentTrain: epoch 15, batch    18 | loss: 10.4085519CurrentTrain: epoch 15, batch    19 | loss: 8.6528493CurrentTrain: epoch 15, batch    20 | loss: 13.6560535CurrentTrain: epoch 15, batch    21 | loss: 13.3741204CurrentTrain: epoch 15, batch    22 | loss: 13.9804758CurrentTrain: epoch 15, batch    23 | loss: 15.2895333CurrentTrain: epoch 15, batch    24 | loss: 8.5181274CurrentTrain: epoch 15, batch    25 | loss: 13.2776937CurrentTrain: epoch 15, batch    26 | loss: 8.3683266CurrentTrain: epoch 15, batch    27 | loss: 17.7784348CurrentTrain: epoch 15, batch    28 | loss: 7.1420204CurrentTrain: epoch 15, batch    29 | loss: 6.7053202CurrentTrain: epoch 15, batch    30 | loss: 12.1396088CurrentTrain: epoch 15, batch    31 | loss: 6.3993681CurrentTrain: epoch 15, batch    32 | loss: 13.1731103CurrentTrain: epoch 15, batch    33 | loss: 9.8088814CurrentTrain: epoch 15, batch    34 | loss: 21.2938262CurrentTrain: epoch 15, batch    35 | loss: 13.9185583CurrentTrain: epoch 15, batch    36 | loss: 7.1712722CurrentTrain: epoch 15, batch    37 | loss: 11.1194351CurrentTrain: epoch 15, batch    38 | loss: 12.8477799CurrentTrain: epoch 15, batch    39 | loss: 15.4489776CurrentTrain: epoch 15, batch    40 | loss: 9.3579057CurrentTrain: epoch 15, batch    41 | loss: 14.1149815CurrentTrain: epoch 15, batch    42 | loss: 8.5167182CurrentTrain: epoch 15, batch    43 | loss: 13.3508368CurrentTrain: epoch 15, batch    44 | loss: 10.3000644CurrentTrain: epoch 15, batch    45 | loss: 8.8211109CurrentTrain: epoch 15, batch    46 | loss: 13.0666269CurrentTrain: epoch 15, batch    47 | loss: 13.7637059CurrentTrain: epoch 15, batch    48 | loss: 10.5917667CurrentTrain: epoch 15, batch    49 | loss: 8.2039526CurrentTrain: epoch 15, batch    50 | loss: 8.9257465CurrentTrain: epoch 15, batch    51 | loss: 7.7604123CurrentTrain: epoch 15, batch    52 | loss: 8.5645292CurrentTrain: epoch 15, batch    53 | loss: 13.1881858CurrentTrain: epoch 15, batch    54 | loss: 9.2031877CurrentTrain: epoch 15, batch    55 | loss: 7.0339657CurrentTrain: epoch 15, batch    56 | loss: 14.6234076CurrentTrain: epoch 15, batch    57 | loss: 23.7903352CurrentTrain: epoch 15, batch    58 | loss: 19.9802083CurrentTrain: epoch 15, batch    59 | loss: 11.4884779CurrentTrain: epoch 15, batch    60 | loss: 14.8012173CurrentTrain: epoch 15, batch    61 | loss: 8.9034672CurrentTrain: epoch  7, batch    62 | loss: 16.0702589CurrentTrain: epoch 15, batch     0 | loss: 12.0127148CurrentTrain: epoch 15, batch     1 | loss: 11.0032657CurrentTrain: epoch 15, batch     2 | loss: 15.6724184CurrentTrain: epoch 15, batch     3 | loss: 9.7503747CurrentTrain: epoch 15, batch     4 | loss: 14.7087409CurrentTrain: epoch 15, batch     5 | loss: 12.1882354CurrentTrain: epoch 15, batch     6 | loss: 24.8852413CurrentTrain: epoch 15, batch     7 | loss: 10.1425517CurrentTrain: epoch 15, batch     8 | loss: 13.5250916CurrentTrain: epoch 15, batch     9 | loss: 7.6772073CurrentTrain: epoch 15, batch    10 | loss: 9.6827397CurrentTrain: epoch 15, batch    11 | loss: 7.0020655CurrentTrain: epoch 15, batch    12 | loss: 16.4642154CurrentTrain: epoch 15, batch    13 | loss: 5.8726019CurrentTrain: epoch 15, batch    14 | loss: 9.9759002CurrentTrain: epoch 15, batch    15 | loss: 9.0516729CurrentTrain: epoch 15, batch    16 | loss: 9.1383656CurrentTrain: epoch 15, batch    17 | loss: 10.4571418CurrentTrain: epoch 15, batch    18 | loss: 33.6683156CurrentTrain: epoch 15, batch    19 | loss: 8.8596900CurrentTrain: epoch 15, batch    20 | loss: 10.5761619CurrentTrain: epoch 15, batch    21 | loss: 12.1975219CurrentTrain: epoch 15, batch    22 | loss: 17.8856867CurrentTrain: epoch 15, batch    23 | loss: 8.3639689CurrentTrain: epoch 15, batch    24 | loss: 14.5115191CurrentTrain: epoch 15, batch    25 | loss: 9.0749236CurrentTrain: epoch 15, batch    26 | loss: 5.9725095CurrentTrain: epoch 15, batch    27 | loss: 9.8526938CurrentTrain: epoch 15, batch    28 | loss: 10.3105439CurrentTrain: epoch 15, batch    29 | loss: 7.5685520CurrentTrain: epoch 15, batch    30 | loss: 15.7788761CurrentTrain: epoch 15, batch    31 | loss: 7.3797838CurrentTrain: epoch 15, batch    32 | loss: 8.7687136CurrentTrain: epoch 15, batch    33 | loss: 7.1237329CurrentTrain: epoch 15, batch    34 | loss: 9.5760710CurrentTrain: epoch 15, batch    35 | loss: 14.2639820CurrentTrain: epoch 15, batch    36 | loss: 10.6567881CurrentTrain: epoch 15, batch    37 | loss: 10.6824677CurrentTrain: epoch 15, batch    38 | loss: 19.7283105CurrentTrain: epoch 15, batch    39 | loss: 9.7382143CurrentTrain: epoch 15, batch    40 | loss: 14.8460171CurrentTrain: epoch 15, batch    41 | loss: 13.2060214CurrentTrain: epoch 15, batch    42 | loss: 6.9187326CurrentTrain: epoch 15, batch    43 | loss: 13.7210347CurrentTrain: epoch 15, batch    44 | loss: 12.0020537CurrentTrain: epoch 15, batch    45 | loss: 14.5605144CurrentTrain: epoch 15, batch    46 | loss: 21.7159514CurrentTrain: epoch 15, batch    47 | loss: 21.4004441CurrentTrain: epoch 15, batch    48 | loss: 14.8607849CurrentTrain: epoch 15, batch    49 | loss: 10.0658826CurrentTrain: epoch 15, batch    50 | loss: 6.9448041CurrentTrain: epoch 15, batch    51 | loss: 7.4892550CurrentTrain: epoch 15, batch    52 | loss: 14.1486312CurrentTrain: epoch 15, batch    53 | loss: 21.0082860CurrentTrain: epoch 15, batch    54 | loss: 11.8769012CurrentTrain: epoch 15, batch    55 | loss: 9.1240140CurrentTrain: epoch 15, batch    56 | loss: 8.8885038CurrentTrain: epoch 15, batch    57 | loss: 15.8457428CurrentTrain: epoch 15, batch    58 | loss: 15.6734660CurrentTrain: epoch 15, batch    59 | loss: 14.7652175CurrentTrain: epoch 15, batch    60 | loss: 9.4286918CurrentTrain: epoch 15, batch    61 | loss: 9.5054708CurrentTrain: epoch  7, batch    62 | loss: 9.3990565CurrentTrain: epoch 15, batch     0 | loss: 9.3458698CurrentTrain: epoch 15, batch     1 | loss: 15.9709163CurrentTrain: epoch 15, batch     2 | loss: 12.3632470CurrentTrain: epoch 15, batch     3 | loss: 13.7499903CurrentTrain: epoch 15, batch     4 | loss: 20.3444199CurrentTrain: epoch 15, batch     5 | loss: 8.3905416CurrentTrain: epoch 15, batch     6 | loss: 9.4961321CurrentTrain: epoch 15, batch     7 | loss: 13.6067021CurrentTrain: epoch 15, batch     8 | loss: 21.0041853CurrentTrain: epoch 15, batch     9 | loss: 33.1916032CurrentTrain: epoch 15, batch    10 | loss: 8.8022815CurrentTrain: epoch 15, batch    11 | loss: 12.6515573CurrentTrain: epoch 15, batch    12 | loss: 10.1113671CurrentTrain: epoch 15, batch    13 | loss: 13.4068062CurrentTrain: epoch 15, batch    14 | loss: 11.6342026CurrentTrain: epoch 15, batch    15 | loss: 9.4691425CurrentTrain: epoch 15, batch    16 | loss: 8.0136941CurrentTrain: epoch 15, batch    17 | loss: 6.5484082CurrentTrain: epoch 15, batch    18 | loss: 11.1733429CurrentTrain: epoch 15, batch    19 | loss: 7.8432801CurrentTrain: epoch 15, batch    20 | loss: 15.3912572CurrentTrain: epoch 15, batch    21 | loss: 10.9249711CurrentTrain: epoch 15, batch    22 | loss: 6.7226164CurrentTrain: epoch 15, batch    23 | loss: 15.0162324CurrentTrain: epoch 15, batch    24 | loss: 20.1785096CurrentTrain: epoch 15, batch    25 | loss: 13.3637158CurrentTrain: epoch 15, batch    26 | loss: 6.8548261CurrentTrain: epoch 15, batch    27 | loss: 13.1435106CurrentTrain: epoch 15, batch    28 | loss: 17.9719987CurrentTrain: epoch 15, batch    29 | loss: 19.9948927CurrentTrain: epoch 15, batch    30 | loss: 9.8238022CurrentTrain: epoch 15, batch    31 | loss: 15.5467976CurrentTrain: epoch 15, batch    32 | loss: 15.7513751CurrentTrain: epoch 15, batch    33 | loss: 14.9532754CurrentTrain: epoch 15, batch    34 | loss: 9.9773937CurrentTrain: epoch 15, batch    35 | loss: 10.4016787CurrentTrain: epoch 15, batch    36 | loss: 12.3830993CurrentTrain: epoch 15, batch    37 | loss: 7.6333383CurrentTrain: epoch 15, batch    38 | loss: 17.5014191CurrentTrain: epoch 15, batch    39 | loss: 8.9557925CurrentTrain: epoch 15, batch    40 | loss: 9.1134119CurrentTrain: epoch 15, batch    41 | loss: 8.9502997CurrentTrain: epoch 15, batch    42 | loss: 10.5483056CurrentTrain: epoch 15, batch    43 | loss: 14.4228534CurrentTrain: epoch 15, batch    44 | loss: 12.0035821CurrentTrain: epoch 15, batch    45 | loss: 22.7255959CurrentTrain: epoch 15, batch    46 | loss: 8.7954600CurrentTrain: epoch 15, batch    47 | loss: 13.7314745CurrentTrain: epoch 15, batch    48 | loss: 18.9165824CurrentTrain: epoch 15, batch    49 | loss: 14.6390318CurrentTrain: epoch 15, batch    50 | loss: 8.7141095CurrentTrain: epoch 15, batch    51 | loss: 10.0970615CurrentTrain: epoch 15, batch    52 | loss: 6.5283168CurrentTrain: epoch 15, batch    53 | loss: 24.5069872CurrentTrain: epoch 15, batch    54 | loss: 11.5901839CurrentTrain: epoch 15, batch    55 | loss: 8.9335401CurrentTrain: epoch 15, batch    56 | loss: 8.4448384CurrentTrain: epoch 15, batch    57 | loss: 20.6361601CurrentTrain: epoch 15, batch    58 | loss: 7.8831983CurrentTrain: epoch 15, batch    59 | loss: 14.4964966CurrentTrain: epoch 15, batch    60 | loss: 9.8772908CurrentTrain: epoch 15, batch    61 | loss: 9.5632209CurrentTrain: epoch  7, batch    62 | loss: 14.0682849
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: the theme of the loss of identity in crowds in this painting can be seen as a precursor to expressionist painting , particularly the work of james ensor .
Head Entity: james ensor
Tail Entity: expressionist
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: The surrealist movement, which sought to challenge the conventional perception of reality, was significantly influenced by the works of Salvador Dalí.  
Head Entity: Salvador Dalí  
Tail Entity: surrealist  

Relation: movement  
Context: The emergence of the romantic movement in literature was marked by the profound impact of poets like William Wordsworth and Samuel Taylor Coleridge.  
Head Entity: William Wordsworth  
Tail Entity: romantic  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: port alberta is a joint venture between the edmonton economic development corporation ( eedc ) and edmonton international airport ( eia ) based in the edmonton capital region in alberta , canada .
Head Entity: edmonton capital region
Tail Entity: alberta
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: the city of san francisco is known for its iconic golden gate bridge and is part of the larger san francisco bay area in california.  
Head Entity: san francisco bay area  
Tail Entity: california  

Relation: located in the administrative territorial entity  
Context: the university of toronto is one of the leading educational institutions in canada and is situated in the province of ontario.  
Head Entity: university of toronto  
Tail Entity: ontario  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the nation as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: rage against the machine bandmates , tom morello , brad wilk , and de la rocha , as well as audioslave bandmate , chris cornell , came to support commerford .
Head Entity: chris cornell
Tail Entity: audioslave
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: The famous actor and musician, Jared Leto, is not only known for his role in "Requiem for a Dream" but also as the lead vocalist of the band Thirty Seconds to Mars.  
Head Entity: Jared Leto  
Tail Entity: Thirty Seconds to Mars  

Relation: member of  
Context: The renowned physicist, Stephen Hawking, was a member of the prestigious Royal Society, contributing significantly to the field of theoretical physics.  
Head Entity: Stephen Hawking  
Tail Entity: Royal Society  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: us 30 currently passes through pittsburgh on the penn - lincoln parkway , crossing the monongahela river on the fort pitt bridge .
Head Entity: fort pitt bridge
Tail Entity: monongahela river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: the new highway will traverse the city, crossing the grand river at the main bridge.  
Head Entity: main bridge  
Tail Entity: grand river  

Relation: crosses  
Context: during the hike, we encountered a beautiful tunnel that crosses under the rocky mountain.  
Head Entity: tunnel  
Tail Entity: rocky mountain  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: the babocomari river is a major tributary of the upper san pedro river in southeastern arizona .
Head Entity: san pedro river
Tail Entity: babocomari river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: the missouri river is the longest tributary of the mississippi river, flowing through several states in the central united states.  
Head Entity: mississippi river  
Tail Entity: missouri river  

Relation: tributary  
Context: the ganges river has numerous tributaries, including the yamuna river, which is one of the most significant in northern india.  
Head Entity: ganges river  
Tail Entity: yamuna river  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: on 3 july 1937 , the count married rocafort - atuzarrra in a lavish wedding in havana , cuba attended by the president of cuba , federico laredo brú .
Head Entity: federico laredo brú
Tail Entity: president of cuba
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: After serving as the mayor of New York City for eight years, Bill de Blasio announced his candidacy for governor of New York in 2022.  
Head Entity: Bill de Blasio  
Tail Entity: mayor of New York City  

Relation: position held  
Context: In 2015, Angela Merkel was recognized for her leadership as the Chancellor of Germany during a critical period in European politics.  
Head Entity: Angela Merkel  
Tail Entity: Chancellor of Germany  
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: " jedi outcast " was developed by raven software and powered by the i d tech 3 game engine .
Head Entity: jedi outcast
Tail Entity: raven software
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: "The Witcher 3: Wild Hunt" was developed by CD Projekt Red and has received numerous awards for its storytelling and gameplay.  
Head Entity: The Witcher 3: Wild Hunt  
Tail Entity: CD Projekt Red  

Relation: developer  
Context: "Minecraft" was developed by Mojang Studios and has become one of the best-selling video games of all time.  
Head Entity: Minecraft  
Tail Entity: Mojang Studios  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: morbid angel was formed in 1983 in tampa , florida by guitarist trey azagthoth and drummer / vocalist mike browning .
Head Entity: morbid angel
Tail Entity: tampa , florida
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the band coldplay was formed in 1996 at university college london, where the members met while studying.  
Head Entity: coldplay  
Tail Entity: university college london  

Relation: location of formation  
Context: the tech startup airbnb was founded in 2008 in san francisco, california by brian chesky, joe gebbia, and nate blecharczyk.  
Head Entity: airbnb  
Tail Entity: san francisco, california  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: she has appeared in some american television shows , beginning with her regular role as snow white 's stepmother , evil queen lillian " lily " white in the series " the charmings " .
Head Entity: the charmings
Tail Entity: american
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous dish sushi is traditionally made with vinegared rice and often includes seafood, vegetables, and sometimes tropical fruits, originating from Japan.  
Head Entity: sushi  
Tail Entity: Japan  

Relation: country of origin  
Context: The iconic painting "Mona Lisa," known for its captivating smile, was created by the renowned artist Leonardo da Vinci in Italy during the Renaissance period.  
Head Entity: Mona Lisa  
Tail Entity: Italy  
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.65%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.09%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 93.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 92.90%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.21%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 93.49%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.51%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.52%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 93.30%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.32%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.13%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.30%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.29%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.44%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.59%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 94.74%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.87%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.45%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.65%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 95.70%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.67%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.64%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.72%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.45%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.42%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.50%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.47%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.55%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 95.62%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.70%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.67%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.94%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.65%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.09%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 93.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 92.90%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.21%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 93.49%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.51%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.52%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 93.30%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.32%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.13%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.30%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.29%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.44%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.59%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 94.74%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.87%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.45%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.65%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 95.70%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.67%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.64%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.72%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.45%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.42%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.50%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.47%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.55%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 95.62%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.70%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.67%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.94%   
cur_acc:  ['0.9494']
his_acc:  ['0.9494']
CurrentTrain: epoch 15, batch     0 | loss: 19.6283427CurrentTrain: epoch 15, batch     1 | loss: 30.8891187CurrentTrain: epoch 15, batch     2 | loss: 17.6872579CurrentTrain: epoch  1, batch     3 | loss: 12.5664837CurrentTrain: epoch 15, batch     0 | loss: 18.5992616CurrentTrain: epoch 15, batch     1 | loss: 18.1101483CurrentTrain: epoch 15, batch     2 | loss: 17.1759680CurrentTrain: epoch  1, batch     3 | loss: 8.6860303CurrentTrain: epoch 15, batch     0 | loss: 15.7804267CurrentTrain: epoch 15, batch     1 | loss: 11.2574855CurrentTrain: epoch 15, batch     2 | loss: 13.3433781CurrentTrain: epoch  1, batch     3 | loss: 10.5569466CurrentTrain: epoch 15, batch     0 | loss: 12.5797075CurrentTrain: epoch 15, batch     1 | loss: 13.1396819CurrentTrain: epoch 15, batch     2 | loss: 12.5509102CurrentTrain: epoch  1, batch     3 | loss: 7.7164882CurrentTrain: epoch 15, batch     0 | loss: 13.9706771CurrentTrain: epoch 15, batch     1 | loss: 11.9508984CurrentTrain: epoch 15, batch     2 | loss: 15.2355944CurrentTrain: epoch  1, batch     3 | loss: 9.9826673CurrentTrain: epoch 15, batch     0 | loss: 12.7618261CurrentTrain: epoch 15, batch     1 | loss: 17.0243961CurrentTrain: epoch 15, batch     2 | loss: 15.4204892CurrentTrain: epoch  1, batch     3 | loss: 8.8900456CurrentTrain: epoch 15, batch     0 | loss: 9.4350932CurrentTrain: epoch 15, batch     1 | loss: 10.0948819CurrentTrain: epoch 15, batch     2 | loss: 10.9827984CurrentTrain: epoch  1, batch     3 | loss: 11.0467948CurrentTrain: epoch 15, batch     0 | loss: 10.0188786CurrentTrain: epoch 15, batch     1 | loss: 10.7355303CurrentTrain: epoch 15, batch     2 | loss: 8.2436882CurrentTrain: epoch  1, batch     3 | loss: 8.9804822CurrentTrain: epoch 15, batch     0 | loss: 15.8192666CurrentTrain: epoch 15, batch     1 | loss: 10.6788208CurrentTrain: epoch 15, batch     2 | loss: 7.1779548CurrentTrain: epoch  1, batch     3 | loss: 6.7876540CurrentTrain: epoch 15, batch     0 | loss: 21.0499676CurrentTrain: epoch 15, batch     1 | loss: 16.5458501CurrentTrain: epoch 15, batch     2 | loss: 8.4825005CurrentTrain: epoch  1, batch     3 | loss: 6.3353516
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: the andesite line , a zone of intense volcanic and seismic activity , is a major regional distinction in the pacific .
Head Entity: andesite line
Tail Entity: pacific
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The Great Barrier Reef, a stunning natural wonder, stretches along the northeastern coast of Australia.  
Head Entity: Great Barrier Reef  
Tail Entity: Australia  

Relation: located on terrain feature  
Context: Mount Everest, the highest peak in the world, is part of the Himalayas and attracts climbers from around the globe.  
Head Entity: Mount Everest  
Tail Entity: Himalayas  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, held in paris in 2015, saw numerous world leaders and organizations come together to discuss global warming and its impacts.  
Head Entity: united nations climate change conference  
Tail Entity: paris agreement  

Relation: participant of  
Context: the 2022 fifa world cup in qatar featured teams from around the globe competing for the prestigious trophy, showcasing the best of international football.  
Head Entity: 2022 fifa world cup  
Tail Entity: qatar
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: luna subsequently had a supporting role in the comedy film " gravy " , directed by james roday , which was released in october 2015 .
Head Entity: gravy
Tail Entity: james roday
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: the acclaimed director christopher nolan is known for his work on the blockbuster film "inception," which was released in 2010.  
Head Entity: inception  
Tail Entity: christopher nolan  

Relation: director  
Context: in the recent documentary "the last dance," directed by jason hehir, viewers get an inside look at michael jordan's career.  
Head Entity: the last dance  
Tail Entity: jason hehir  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: tata prima is a range of heavy trucks produced by tata daewoo , a wholly owned subsidiary of tata motors of india .
Head Entity: tata daewoo
Tail Entity: tata motors
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: The famous luxury brand Gucci is owned by the French conglomerate Kering, which also owns several other high-end fashion labels.  
Head Entity: Gucci  
Tail Entity: Kering  

Relation: owned by  
Context: The popular social media platform Instagram is owned by Meta Platforms, Inc., which also owns Facebook and WhatsApp.  
Head Entity: Instagram  
Tail Entity: Meta Platforms, Inc.  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: thessaly was home to extensive neolithic and chalcolithic cultures around 6000 bc–2500 bc ( see cardium pottery , dimini and sesklo ) .
Head Entity: cardium pottery
Tail Entity: neolithic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The engine is a crucial part of the car's overall functionality, providing the necessary power to drive.  
Head Entity: engine  
Tail Entity: car  

Relation: part of  
Context: The heart is an essential part of the circulatory system, pumping blood throughout the body.  
Head Entity: heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: the brewery closed in 1996 and is now the site of the halliwell jones stadium , home of warrington wolves , as well as a tesco store .
Head Entity: halliwell jones stadium
Tail Entity: warrington wolves
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was transformed into a cultural center, which now hosts the local art gallery and serves as the headquarters for the community theater group.  
Head Entity: cultural center  
Tail Entity: community theater group  

Relation: occupant  
Context: After the renovation, the old factory became a vibrant co-working space, attracting freelancers and startups from the tech industry.  
Head Entity: co-working space  
Tail Entity: tech industry
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: the art nouveau style was brought to italy by figures such as giuseppe sommaruga and ernesto basile ( who respectively designed the palazzo castiglioni and expanded the palazzo montecitorio in rome ) .
Head Entity: palazzo castiglioni
Tail Entity: giuseppe sommaruga
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: The iconic Sydney Opera House was designed by Danish architect Jørn Utzon, who won the design competition in 1957.  
Head Entity: Sydney Opera House  
Tail Entity: Jørn Utzon  

Relation: architect  
Context: Frank Lloyd Wright is renowned for his innovative designs, including the famous Fallingwater house, which seamlessly integrates with its natural surroundings.  
Head Entity: Fallingwater  
Tail Entity: Frank Lloyd Wright  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: cyrus alexander was born in pennsylvania , and his family soon moved to illinois .
Head Entity: cyrus alexander
Tail Entity: illinois
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: after living in new york for several years, emily decided to relocate to california for a fresh start.  
Head Entity: emily  
Tail Entity: california  

Relation: residence  
Context: during his childhood, michael spent most of his time in texas before moving to florida as a teenager.  
Head Entity: michael  
Tail Entity: florida  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Ludwig van Beethoven created some of the most celebrated symphonies in classical music history.  
Head Entity: symphonies  
Tail Entity: Ludwig van Beethoven  

Relation: composer  
Context: Wolfgang Amadeus Mozart was a prolific and influential composer of the classical era, known for his operas and symphonies.  
Head Entity: operas  
Tail Entity: Wolfgang Amadeus Mozart  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: one polish archer qualified for the women 's individual recurve by obtaining one of the eight olympic places available from the 2015 world archery championships in copenhagen , denmark .
Head Entity: 2015 world archery championships
Tail Entity: copenhagen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: The annual music festival will take place in the vibrant city of Austin, Texas, attracting thousands of fans from around the world.  
Head Entity: annual music festival  
Tail Entity: Austin, Texas  

Relation: location  
Context: The historic battle was fought near the banks of the river at Gettysburg, Pennsylvania, which is now a national park.  
Head Entity: historic battle  
Tail Entity: Gettysburg, Pennsylvania  
MemoryTrain:  epoch 15, batch     0 | loss: 8.9725871MemoryTrain:  epoch 15, batch     1 | loss: 12.3668261MemoryTrain:  epoch 15, batch     2 | loss: 6.2996800MemoryTrain:  epoch 11, batch     3 | loss: 6.8559038MemoryTrain:  epoch 15, batch     0 | loss: 13.9924565MemoryTrain:  epoch 15, batch     1 | loss: 9.7653499MemoryTrain:  epoch 15, batch     2 | loss: 6.0078555MemoryTrain:  epoch 11, batch     3 | loss: 3.7584823MemoryTrain:  epoch 15, batch     0 | loss: 4.9679555MemoryTrain:  epoch 15, batch     1 | loss: 4.7701590MemoryTrain:  epoch 15, batch     2 | loss: 9.4454880MemoryTrain:  epoch 11, batch     3 | loss: 4.7966451MemoryTrain:  epoch 15, batch     0 | loss: 3.9090124MemoryTrain:  epoch 15, batch     1 | loss: 6.6113037MemoryTrain:  epoch 15, batch     2 | loss: 5.7317719MemoryTrain:  epoch 11, batch     3 | loss: 3.8098486MemoryTrain:  epoch 15, batch     0 | loss: 4.1937074MemoryTrain:  epoch 15, batch     1 | loss: 4.4047607MemoryTrain:  epoch 15, batch     2 | loss: 2.8853293MemoryTrain:  epoch 11, batch     3 | loss: 4.4213215MemoryTrain:  epoch 15, batch     0 | loss: 7.3769821MemoryTrain:  epoch 15, batch     1 | loss: 6.1595970MemoryTrain:  epoch 15, batch     2 | loss: 3.7592169MemoryTrain:  epoch 11, batch     3 | loss: 2.6426118MemoryTrain:  epoch 15, batch     0 | loss: 4.8952141MemoryTrain:  epoch 15, batch     1 | loss: 3.5994108MemoryTrain:  epoch 15, batch     2 | loss: 4.0211408MemoryTrain:  epoch 11, batch     3 | loss: 2.6506587MemoryTrain:  epoch 15, batch     0 | loss: 2.7974049MemoryTrain:  epoch 15, batch     1 | loss: 2.5992966MemoryTrain:  epoch 15, batch     2 | loss: 3.2669659MemoryTrain:  epoch 11, batch     3 | loss: 2.3967018MemoryTrain:  epoch 15, batch     0 | loss: 3.8236725MemoryTrain:  epoch 15, batch     1 | loss: 4.4518848MemoryTrain:  epoch 15, batch     2 | loss: 6.3618889MemoryTrain:  epoch 11, batch     3 | loss: 4.1970368MemoryTrain:  epoch 15, batch     0 | loss: 2.4524646MemoryTrain:  epoch 15, batch     1 | loss: 3.4057899MemoryTrain:  epoch 15, batch     2 | loss: 4.4354699MemoryTrain:  epoch 11, batch     3 | loss: 2.4288834
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 29.17%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 29.69%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 30.00%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 37.50%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 43.75%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 50.00%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 54.37%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 57.95%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 60.94%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 63.46%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 65.18%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 66.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 70.22%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 71.53%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 71.71%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 71.25%   [EVAL] batch:   20 | acc: 50.00%,  total acc: 70.24%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 71.31%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 72.01%   [EVAL] batch:   23 | acc: 56.25%,  total acc: 71.35%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 71.25%   [EVAL] batch:   25 | acc: 18.75%,  total acc: 69.23%   [EVAL] batch:   26 | acc: 37.50%,  total acc: 68.06%   [EVAL] batch:   27 | acc: 25.00%,  total acc: 66.52%   [EVAL] batch:   28 | acc: 12.50%,  total acc: 64.66%   [EVAL] batch:   29 | acc: 31.25%,  total acc: 63.54%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 61.69%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 62.11%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 63.07%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 63.60%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 64.64%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 65.10%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 65.88%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 66.61%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 67.15%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 68.45%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 69.48%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 70.17%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 70.42%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 70.38%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 70.48%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 70.57%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 70.92%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 71.12%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 71.08%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 71.51%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 71.70%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 72.11%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 72.27%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 72.54%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 72.26%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 71.66%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 71.08%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 70.94%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 70.59%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 70.56%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 69.74%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 74.11%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 76.25%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 75.96%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 77.23%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 78.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 80.88%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.60%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.63%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 83.52%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 83.70%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 84.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.10%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.65%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.16%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 86.42%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.70%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 88.07%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 88.42%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 88.57%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 89.19%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 89.47%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.74%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.24%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.48%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 90.55%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 90.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 90.76%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 90.69%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 90.69%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 90.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 90.56%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 90.68%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 90.57%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 90.51%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 90.57%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 90.52%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 90.68%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 90.83%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 90.98%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 91.13%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 90.67%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 89.75%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 88.75%   [EVAL] batch:   65 | acc: 31.25%,  total acc: 87.88%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 87.03%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 86.12%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 85.78%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 85.80%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 85.92%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 86.02%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 86.22%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 86.32%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 86.33%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 86.51%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 86.44%   [EVAL] batch:   77 | acc: 93.75%,  total acc: 86.54%   [EVAL] batch:   78 | acc: 100.00%,  total acc: 86.71%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 86.80%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 86.88%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 86.43%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 86.14%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 86.01%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 86.10%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 85.90%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 85.56%   [EVAL] batch:   87 | acc: 43.75%,  total acc: 85.09%   [EVAL] batch:   88 | acc: 37.50%,  total acc: 84.55%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 83.96%   [EVAL] batch:   90 | acc: 6.25%,  total acc: 83.10%   [EVAL] batch:   91 | acc: 37.50%,  total acc: 82.61%   [EVAL] batch:   92 | acc: 12.50%,  total acc: 81.85%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 81.25%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 81.38%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 81.51%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 81.57%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 81.70%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 81.76%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 81.81%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 81.93%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 82.05%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 82.10%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 82.21%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 82.26%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 82.43%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 82.48%   [EVAL] batch:  107 | acc: 87.50%,  total acc: 82.52%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 82.28%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 82.33%   [EVAL] batch:  110 | acc: 75.00%,  total acc: 82.26%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 82.37%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 82.30%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 82.18%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 82.23%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 82.33%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 82.32%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 82.42%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 82.35%   [EVAL] batch:  119 | acc: 37.50%,  total acc: 81.98%   [EVAL] batch:  120 | acc: 37.50%,  total acc: 81.61%   [EVAL] batch:  121 | acc: 43.75%,  total acc: 81.30%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 81.15%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 81.00%   [EVAL] batch:  124 | acc: 50.00%,  total acc: 80.75%   
cur_acc:  ['0.9494', '0.6974']
his_acc:  ['0.9494', '0.8075']
CurrentTrain: epoch 15, batch     0 | loss: 15.4788131CurrentTrain: epoch 15, batch     1 | loss: 15.0350171CurrentTrain: epoch 15, batch     2 | loss: 20.7749315CurrentTrain: epoch  1, batch     3 | loss: 9.3408672CurrentTrain: epoch 15, batch     0 | loss: 13.4289128CurrentTrain: epoch 15, batch     1 | loss: 12.9490777CurrentTrain: epoch 15, batch     2 | loss: 9.7797093CurrentTrain: epoch  1, batch     3 | loss: 9.6800822CurrentTrain: epoch 15, batch     0 | loss: 12.5905707CurrentTrain: epoch 15, batch     1 | loss: 16.8872859CurrentTrain: epoch 15, batch     2 | loss: 16.2496637CurrentTrain: epoch  1, batch     3 | loss: 8.3225329CurrentTrain: epoch 15, batch     0 | loss: 8.1771036CurrentTrain: epoch 15, batch     1 | loss: 12.1520897CurrentTrain: epoch 15, batch     2 | loss: 10.9977425CurrentTrain: epoch  1, batch     3 | loss: 8.6494583CurrentTrain: epoch 15, batch     0 | loss: 9.1280576CurrentTrain: epoch 15, batch     1 | loss: 13.4524666CurrentTrain: epoch 15, batch     2 | loss: 14.6133872CurrentTrain: epoch  1, batch     3 | loss: 7.7197469CurrentTrain: epoch 15, batch     0 | loss: 8.8021876CurrentTrain: epoch 15, batch     1 | loss: 9.3313708CurrentTrain: epoch 15, batch     2 | loss: 15.0624286CurrentTrain: epoch  1, batch     3 | loss: 5.6827827CurrentTrain: epoch 15, batch     0 | loss: 11.0396272CurrentTrain: epoch 15, batch     1 | loss: 10.9017377CurrentTrain: epoch 15, batch     2 | loss: 7.1270087CurrentTrain: epoch  1, batch     3 | loss: 6.2605032CurrentTrain: epoch 15, batch     0 | loss: 6.5274251CurrentTrain: epoch 15, batch     1 | loss: 8.7992607CurrentTrain: epoch 15, batch     2 | loss: 8.8205235CurrentTrain: epoch  1, batch     3 | loss: 5.9402782CurrentTrain: epoch 15, batch     0 | loss: 8.9419885CurrentTrain: epoch 15, batch     1 | loss: 6.8550928CurrentTrain: epoch 15, batch     2 | loss: 9.1979874CurrentTrain: epoch  1, batch     3 | loss: 6.8327800CurrentTrain: epoch 15, batch     0 | loss: 9.2128680CurrentTrain: epoch 15, batch     1 | loss: 12.5258664CurrentTrain: epoch 15, batch     2 | loss: 5.8490963CurrentTrain: epoch  1, batch     3 | loss: 5.7445841
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: rohan is the son of narayana murthy , founder of infosys , and his wife sudha murthy , an engineer and author of note .
Head Entity: narayana murthy
Tail Entity: sudha murthy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: elena is a renowned artist who often collaborates with her husband, mark, a talented musician known for his soulful melodies.  
Head Entity: mark  
Tail Entity: elena  

Relation: spouse  
Context: during the award ceremony, it was revealed that the famous director, james cameron, has been married to suzy, a skilled producer, for over two decades.  
Head Entity: james cameron  
Tail Entity: suzy  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: " i 'm a cuckoo " was belle & sebastian 's second single from " dear catastrophe waitress " , released on rough trade records in 2004 .
Head Entity: belle & sebastian
Tail Entity: rough trade records
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: The album "Random Access Memories" by Daft Punk was released under the Columbia Records label in 2013.  
Head Entity: Daft Punk  
Tail Entity: Columbia Records  

Relation: record label  
Context: Taylor Swift's latest album "Evermore" was produced by her own label, Republic Records, and released in 2020.  
Head Entity: Taylor Swift  
Tail Entity: Republic Records  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: distributed by paramount pictures , " the italian job " was theatrically released in the united states on may 30 , 2003 , and grossed over $ 176 million worldwide .
Head Entity: the italian job
Tail Entity: paramount pictures
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: the film was distributed by universal pictures and received critical acclaim upon its release in 2015.  
Head Entity: the film  
Tail Entity: universal pictures  

Relation: distributor  
Context: the album was distributed by sony music and quickly climbed the charts, becoming a bestseller.  
Head Entity: the album  
Tail Entity: sony music  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: in 1796 carondelet commissioned pintado to conduct a survey of lands granted to ambrosio longue - épée in iberville county on the west bank of the mississippi river opposite baton rouge .
Head Entity: baton rouge
Tail Entity: mississippi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: the city of new orleans is situated on the banks of the mississippi river, making it a vibrant hub for trade and culture.  
Head Entity: new orleans  
Tail Entity: mississippi river  

Relation: located in or next to body of water  
Context: the picturesque village of lake placid is nestled beside the serene waters of lake placid, attracting tourists year-round.  
Head Entity: lake placid  
Tail Entity: lake placid  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: following the sale of amstrad plc to rupert murdoch 's bskyb , viglen is now lord sugar 's sole it establishment .
Head Entity: bskyb
Tail Entity: amstrad
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Relation: subsidiary  
Context: In 2018, Facebook acquired WhatsApp, making it a subsidiary of the social media giant.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Relation: subsidiary  
Context: After the merger, the new company announced that its subsidiary, XYZ Corp, would continue to operate independently.  
Head Entity: new company  
Tail Entity: XYZ Corp  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: he challenged seven other swimmers in heat four including ukraine 's rostyslav svanidze , a top 16 finalist in atlanta four years earlier .
Head Entity: rostyslav svanidze
Tail Entity: ukraine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: after years of living in the united states, the renowned artist decided to apply for dual citizenship, allowing him to maintain his ties to his homeland, italy.  
Head Entity: the renowned artist  
Tail Entity: italy  

Relation: country of citizenship  
Context: during the international conference, the delegate from canada proudly spoke about the rights and privileges of being a citizen of his country, emphasizing the importance of citizenship in shaping national identity.  
Head Entity: the delegate from canada  
Tail Entity: canada  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: he is also a part of the atlanta - based hip - hop / r&b;/soul musical collective the dungeon family .
Head Entity: the dungeon family
Tail Entity: hip - hop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the band is known for their unique blend of rock and electronic music, often categorized under the genre of synth-pop.  
Head Entity: the band  
Tail Entity: synth-pop  

Relation: genre  
Context: she has made significant contributions to the world of classical music, particularly in the genre of opera.  
Head Entity: she  
Tail Entity: opera  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: san lázaro is 660 km far from asunción and is located at the exact site of the confluence of the apa river with paraguay river , bordering brazil - chaco .
Head Entity: apa river
Tail Entity: paraguay river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: the mississippi river flows into the gulf of mexico, creating a rich delta ecosystem that supports diverse wildlife and plant species.  
Head Entity: mississippi river  
Tail Entity: gulf of mexico  

Relation: mouth of the watercourse  
Context: the rhine river, which runs through several countries in europe, ultimately empties into the north sea, providing a vital shipping route for international trade.  
Head Entity: rhine river  
Tail Entity: north sea  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: bradley halliday ( born 10 july 1995 ) is an english professional footballer who plays as a defender or a midfielder for league two club cambridge united .
Head Entity: bradley halliday
Tail Entity: defender
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: alex morgan ( born 2 july 1989 ) is an american soccer player who plays as a forward for the national team and club orlando pride in the national women's soccer league.  
Head Entity: alex morgan  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: lebron james ( born 30 december 1984 ) is an american professional basketball player who plays as a small forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: small forward  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: hammami made his international debut for tunisia in 2007 , and represented them at the africa cup of nations in 2010 and 2013 .
Head Entity: 2010
Tail Entity: africa cup of nations
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2021 UEFA Champions League was won by Chelsea, marking their second title in the prestigious tournament.  
Head Entity: 2021  
Tail Entity: UEFA Champions League  

Relation: sports season of league or competition  
Context: The 2019 Cricket World Cup saw England emerge victorious, claiming their first title in the history of the tournament.  
Head Entity: 2019  
Tail Entity: Cricket World Cup  
MemoryTrain:  epoch 15, batch     0 | loss: 5.3126558MemoryTrain:  epoch 15, batch     1 | loss: 5.1845958MemoryTrain:  epoch 15, batch     2 | loss: 5.0925952MemoryTrain:  epoch 15, batch     3 | loss: 5.7884127MemoryTrain:  epoch 15, batch     4 | loss: 8.2335415MemoryTrain:  epoch  9, batch     5 | loss: 5.8095956MemoryTrain:  epoch 15, batch     0 | loss: 4.8318457MemoryTrain:  epoch 15, batch     1 | loss: 3.8745827MemoryTrain:  epoch 15, batch     2 | loss: 5.1189498MemoryTrain:  epoch 15, batch     3 | loss: 6.2086616MemoryTrain:  epoch 15, batch     4 | loss: 7.1935801MemoryTrain:  epoch  9, batch     5 | loss: 5.5102000MemoryTrain:  epoch 15, batch     0 | loss: 4.1401974MemoryTrain:  epoch 15, batch     1 | loss: 3.3039315MemoryTrain:  epoch 15, batch     2 | loss: 4.1737830MemoryTrain:  epoch 15, batch     3 | loss: 3.5854278MemoryTrain:  epoch 15, batch     4 | loss: 3.5243952MemoryTrain:  epoch  9, batch     5 | loss: 6.4642354MemoryTrain:  epoch 15, batch     0 | loss: 3.8172519MemoryTrain:  epoch 15, batch     1 | loss: 2.9278619MemoryTrain:  epoch 15, batch     2 | loss: 3.2821941MemoryTrain:  epoch 15, batch     3 | loss: 3.5196438MemoryTrain:  epoch 15, batch     4 | loss: 2.9685402MemoryTrain:  epoch  9, batch     5 | loss: 3.5590563MemoryTrain:  epoch 15, batch     0 | loss: 2.7384090MemoryTrain:  epoch 15, batch     1 | loss: 2.7316136MemoryTrain:  epoch 15, batch     2 | loss: 3.4573844MemoryTrain:  epoch 15, batch     3 | loss: 2.6957507MemoryTrain:  epoch 15, batch     4 | loss: 2.9898842MemoryTrain:  epoch  9, batch     5 | loss: 2.5693618MemoryTrain:  epoch 15, batch     0 | loss: 5.8264586MemoryTrain:  epoch 15, batch     1 | loss: 3.2707677MemoryTrain:  epoch 15, batch     2 | loss: 4.6416657MemoryTrain:  epoch 15, batch     3 | loss: 2.9110580MemoryTrain:  epoch 15, batch     4 | loss: 3.9896069MemoryTrain:  epoch  9, batch     5 | loss: 3.9678125MemoryTrain:  epoch 15, batch     0 | loss: 5.5625067MemoryTrain:  epoch 15, batch     1 | loss: 2.7452961MemoryTrain:  epoch 15, batch     2 | loss: 7.6944136MemoryTrain:  epoch 15, batch     3 | loss: 5.3528329MemoryTrain:  epoch 15, batch     4 | loss: 4.9140020MemoryTrain:  epoch  9, batch     5 | loss: 2.2376433MemoryTrain:  epoch 15, batch     0 | loss: 3.8493499MemoryTrain:  epoch 15, batch     1 | loss: 2.2233619MemoryTrain:  epoch 15, batch     2 | loss: 4.8714855MemoryTrain:  epoch 15, batch     3 | loss: 3.0073410MemoryTrain:  epoch 15, batch     4 | loss: 2.6364846MemoryTrain:  epoch  9, batch     5 | loss: 1.6878417MemoryTrain:  epoch 15, batch     0 | loss: 1.8492584MemoryTrain:  epoch 15, batch     1 | loss: 2.6072225MemoryTrain:  epoch 15, batch     2 | loss: 2.2768084MemoryTrain:  epoch 15, batch     3 | loss: 1.8763031MemoryTrain:  epoch 15, batch     4 | loss: 2.2211050MemoryTrain:  epoch  9, batch     5 | loss: 5.0769905MemoryTrain:  epoch 15, batch     0 | loss: 1.9499378MemoryTrain:  epoch 15, batch     1 | loss: 2.0715608MemoryTrain:  epoch 15, batch     2 | loss: 1.8963287MemoryTrain:  epoch 15, batch     3 | loss: 4.2517697MemoryTrain:  epoch 15, batch     4 | loss: 2.0282060MemoryTrain:  epoch  9, batch     5 | loss: 2.5819559
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 96.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 96.43%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 96.09%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 95.14%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 93.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 93.18%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 93.23%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 93.27%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 89.71%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 88.89%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 88.82%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 87.19%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 86.01%   [EVAL] batch:   21 | acc: 56.25%,  total acc: 84.66%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 83.42%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 82.50%   [EVAL] batch:   25 | acc: 68.75%,  total acc: 81.97%   [EVAL] batch:   26 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   27 | acc: 50.00%,  total acc: 80.13%   [EVAL] batch:   28 | acc: 50.00%,  total acc: 79.09%   [EVAL] batch:   29 | acc: 56.25%,  total acc: 78.33%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 78.23%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 77.27%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 76.84%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 76.25%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 76.22%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 75.84%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 75.33%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 74.84%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 74.69%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 74.24%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 73.81%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 73.55%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 73.01%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 72.08%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 71.20%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 70.21%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 69.92%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 68.88%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 68.50%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 69.12%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 69.71%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 70.28%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 71.36%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 72.37%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 72.84%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 73.31%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 73.75%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 74.18%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 74.60%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 74.21%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 74.43%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 73.44%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 74.52%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 75.89%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 78.52%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 79.78%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 80.56%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 81.58%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.88%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 81.85%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 81.53%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 81.79%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 82.03%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 82.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.93%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.56%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.48%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.48%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 85.74%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.17%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 85.48%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 85.18%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 85.24%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 85.47%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 85.53%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.90%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 86.59%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 86.90%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.06%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 87.36%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 87.36%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 87.64%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 87.63%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 87.63%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 87.76%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 87.88%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 87.87%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 88.09%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 88.08%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 87.84%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 87.72%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 87.72%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 87.61%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 87.71%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 87.81%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 88.01%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 88.10%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 87.50%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 86.23%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 85.10%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 83.90%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 82.74%   [EVAL] batch:   67 | acc: 6.25%,  total acc: 81.62%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 80.98%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 81.07%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 81.42%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 81.51%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 81.67%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 81.58%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 81.83%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 81.82%   [EVAL] batch:   77 | acc: 93.75%,  total acc: 81.97%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 82.12%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 82.19%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 82.41%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 81.86%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 81.25%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 80.43%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 80.07%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 79.72%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 79.17%   [EVAL] batch:   87 | acc: 37.50%,  total acc: 78.69%   [EVAL] batch:   88 | acc: 25.00%,  total acc: 78.09%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 77.57%   [EVAL] batch:   90 | acc: 6.25%,  total acc: 76.79%   [EVAL] batch:   91 | acc: 37.50%,  total acc: 76.36%   [EVAL] batch:   92 | acc: 18.75%,  total acc: 75.74%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 75.13%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 75.20%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 75.33%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 75.52%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 75.70%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 75.82%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 75.81%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 75.93%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 76.10%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 76.27%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 76.44%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 76.61%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 76.83%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 76.93%   [EVAL] batch:  107 | acc: 87.50%,  total acc: 77.03%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 77.01%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 77.16%   [EVAL] batch:  110 | acc: 81.25%,  total acc: 77.20%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 77.40%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 77.38%   [EVAL] batch:  113 | acc: 75.00%,  total acc: 77.36%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 77.45%   [EVAL] batch:  115 | acc: 68.75%,  total acc: 77.37%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 77.51%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 77.60%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 77.57%   [EVAL] batch:  119 | acc: 43.75%,  total acc: 77.29%   [EVAL] batch:  120 | acc: 56.25%,  total acc: 77.12%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 76.90%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 76.78%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 76.71%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 76.60%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 76.79%   [EVAL] batch:  126 | acc: 93.75%,  total acc: 76.92%   [EVAL] batch:  127 | acc: 100.00%,  total acc: 77.10%   [EVAL] batch:  128 | acc: 93.75%,  total acc: 77.23%   [EVAL] batch:  129 | acc: 93.75%,  total acc: 77.36%   [EVAL] batch:  130 | acc: 93.75%,  total acc: 77.48%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 77.65%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 77.77%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 77.85%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 77.87%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 77.94%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 78.06%   [EVAL] batch:  137 | acc: 93.75%,  total acc: 78.17%   [EVAL] batch:  138 | acc: 81.25%,  total acc: 78.19%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 78.19%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 78.17%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 78.15%   [EVAL] batch:  143 | acc: 87.50%,  total acc: 78.21%   [EVAL] batch:  144 | acc: 56.25%,  total acc: 78.06%   [EVAL] batch:  145 | acc: 62.50%,  total acc: 77.95%   [EVAL] batch:  146 | acc: 56.25%,  total acc: 77.81%   [EVAL] batch:  147 | acc: 56.25%,  total acc: 77.66%   [EVAL] batch:  148 | acc: 81.25%,  total acc: 77.68%   [EVAL] batch:  149 | acc: 62.50%,  total acc: 77.58%   [EVAL] batch:  150 | acc: 68.75%,  total acc: 77.52%   [EVAL] batch:  151 | acc: 62.50%,  total acc: 77.43%   [EVAL] batch:  152 | acc: 50.00%,  total acc: 77.25%   [EVAL] batch:  153 | acc: 50.00%,  total acc: 77.07%   [EVAL] batch:  154 | acc: 56.25%,  total acc: 76.94%   [EVAL] batch:  155 | acc: 75.00%,  total acc: 76.92%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 76.91%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 76.74%   [EVAL] batch:  158 | acc: 62.50%,  total acc: 76.65%   [EVAL] batch:  159 | acc: 56.25%,  total acc: 76.52%   [EVAL] batch:  160 | acc: 75.00%,  total acc: 76.51%   [EVAL] batch:  161 | acc: 62.50%,  total acc: 76.43%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 76.30%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 76.18%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 76.14%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 76.02%   [EVAL] batch:  166 | acc: 56.25%,  total acc: 75.90%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 75.82%   [EVAL] batch:  168 | acc: 50.00%,  total acc: 75.67%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 75.40%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 75.15%   [EVAL] batch:  171 | acc: 25.00%,  total acc: 74.85%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 74.75%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 74.43%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 74.29%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 74.43%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 74.58%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 74.72%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 74.86%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 75.14%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 75.27%   [EVAL] batch:  182 | acc: 100.00%,  total acc: 75.41%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 75.54%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 75.68%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 75.81%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 75.94%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 75.80%   
cur_acc:  ['0.9494', '0.6974', '0.7421']
his_acc:  ['0.9494', '0.8075', '0.7580']
CurrentTrain: epoch 15, batch     0 | loss: 15.7142925CurrentTrain: epoch 15, batch     1 | loss: 12.6728161CurrentTrain: epoch 15, batch     2 | loss: 15.3627690CurrentTrain: epoch  1, batch     3 | loss: 9.0602586CurrentTrain: epoch 15, batch     0 | loss: 21.3453305CurrentTrain: epoch 15, batch     1 | loss: 8.9826376CurrentTrain: epoch 15, batch     2 | loss: 10.5203918CurrentTrain: epoch  1, batch     3 | loss: 6.2087618CurrentTrain: epoch 15, batch     0 | loss: 7.8397096CurrentTrain: epoch 15, batch     1 | loss: 7.7988222CurrentTrain: epoch 15, batch     2 | loss: 11.4452516CurrentTrain: epoch  1, batch     3 | loss: 25.3660954CurrentTrain: epoch 15, batch     0 | loss: 16.0023047CurrentTrain: epoch 15, batch     1 | loss: 9.6017999CurrentTrain: epoch 15, batch     2 | loss: 11.4478269CurrentTrain: epoch  1, batch     3 | loss: 9.2463350CurrentTrain: epoch 15, batch     0 | loss: 11.2390342CurrentTrain: epoch 15, batch     1 | loss: 14.6267016CurrentTrain: epoch 15, batch     2 | loss: 9.2240310CurrentTrain: epoch  1, batch     3 | loss: 11.5703081CurrentTrain: epoch 15, batch     0 | loss: 7.2076805CurrentTrain: epoch 15, batch     1 | loss: 16.3143976CurrentTrain: epoch 15, batch     2 | loss: 14.6943475CurrentTrain: epoch  1, batch     3 | loss: 11.5228828CurrentTrain: epoch 15, batch     0 | loss: 8.7009545CurrentTrain: epoch 15, batch     1 | loss: 15.7457234CurrentTrain: epoch 15, batch     2 | loss: 9.1541293CurrentTrain: epoch  1, batch     3 | loss: 7.9559612CurrentTrain: epoch 15, batch     0 | loss: 8.2539751CurrentTrain: epoch 15, batch     1 | loss: 13.6541959CurrentTrain: epoch 15, batch     2 | loss: 10.5806579CurrentTrain: epoch  1, batch     3 | loss: 6.8853599CurrentTrain: epoch 15, batch     0 | loss: 9.1341856CurrentTrain: epoch 15, batch     1 | loss: 6.9994611CurrentTrain: epoch 15, batch     2 | loss: 8.2974760CurrentTrain: epoch  1, batch     3 | loss: 9.4861960CurrentTrain: epoch 15, batch     0 | loss: 6.9187766CurrentTrain: epoch 15, batch     1 | loss: 8.1373133CurrentTrain: epoch 15, batch     2 | loss: 6.6343183CurrentTrain: epoch  1, batch     3 | loss: 10.0776470
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: she was elected to the northern territory legislative assembly representing the division of araluen for the country liberal party in a 2010 by - election following the resignation of jodeen carney .
Head Entity: 2010 by - election
Tail Entity: araluen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The new environmental regulations introduced by the federal government are applicable to all states, including California, which has its own set of stringent laws.  
Head Entity: federal government  
Tail Entity: California  

Relation: applies to jurisdiction  
Context: The recent amendments to the education policy will be enforced in all public schools across New York City, ensuring that every student receives equal opportunities.  
Head Entity: education policy  
Tail Entity: New York City  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: " tenth avenue freeze - out " is the second song on bruce springsteen 's breakthrough album " born to run " , released in 1975 .
Head Entity: tenth avenue freeze - out
Tail Entity: bruce springsteen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: "Shape of You" is a popular song by Ed Sheeran that topped charts worldwide after its release in 2017.  
Head Entity: Shape of You  
Tail Entity: Ed Sheeran  

Relation: performer  
Context: The Beatles are widely regarded as one of the most influential bands in the history of music, with hits like "Hey Jude" and "Let It Be."  
Head Entity: The Beatles  
Tail Entity: Hey Jude  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the glasflügel h-201 standard " libelle " ( german : " dragonfly " ) is an early composite standard class single - seat sailplane produced by glasflügel from 1967 .
Head Entity: dragonfly
Tail Entity: glasflügel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The iPhone 13, a smartphone developed by Apple, was released in September 2021 and features advanced camera technology and improved battery life.  
Head Entity: iPhone 13  
Tail Entity: Apple  

Relation: manufacturer  
Context: The Tesla Model S is an all-electric luxury sedan that was first introduced by Tesla, Inc. in 2012, showcasing innovative technology and performance.  
Head Entity: Tesla Model S  
Tail Entity: Tesla, Inc.  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: in contrast to gwladys and roger 's allegiance to the house of lancaster , their three sons were staunch yorkists during the wars of the roses .
Head Entity: wars of the roses
Tail Entity: house of lancaster
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: During the annual tech conference, several leading companies showcased their latest innovations, with Google and Microsoft being the most prominent participants.  
Head Entity: tech conference  
Tail Entity: Google  

Relation: participant  
Context: The local community organized a charity run, where various schools and organizations, including the Red Cross, participated to raise funds for the homeless.  
Head Entity: charity run  
Tail Entity: Red Cross  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the application is compatible with various operating systems, including macOS, Windows, and Ubuntu.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: Many developers prefer to use Linux as their primary operating system due to its flexibility and open-source nature.  
Head Entity: developers  
Tail Entity: Linux  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in july 2007 , griffin lost by a technical knockout to light heavyweight champion danny green of the world boxing association at challenge stadium in perth , australia .
Head Entity: danny green
Tail Entity: light heavyweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: during the 2020 olympics, the men's 100 meters final showcased athletes from various competition classes, with usain bolt representing the sprinting class.  
Head Entity: usain bolt  
Tail Entity: sprinting  

Relation: competition class  
Context: the formula one race in monaco is known for its high-speed competition class, where lewis hamilton competed fiercely against other top drivers.  
Head Entity: lewis hamilton  
Tail Entity: formula one
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: the qup gained 3 seats from its previous election result , and won a 10 % swing to it in primary votes , but failed to defeat the incumbent labor government led by ted theodore .
Head Entity: previous election
Tail Entity: ted theodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: After a closely contested campaign, the voters chose Sarah Johnson as their representative, marking a significant shift in the local political landscape.  
Head Entity: closely contested campaign  
Tail Entity: Sarah Johnson  

Relation: successful candidate  
Context: In the recent mayoral election, the community rallied behind Mark Thompson, who promised to bring new ideas and energy to the office.  
Head Entity: recent mayoral election  
Tail Entity: Mark Thompson  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: although the latter 's eldest son , lǫgmaðr , appears to have succeeded to the kingship , he was soon forced to contend with factions supporting his younger brothers : haraldr , and óláfr .
Head Entity: lǫgmaðr
Tail Entity: óláfr
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: In the royal family of the ancient kingdom, the two brothers, Eirik and Sigurd, were known for their fierce loyalty to each other, having grown up under the same roof and shared the same parents.  
Head Entity: Eirik  
Tail Entity: Sigurd  

Relation: sibling  
Context: During the family reunion, it was evident that both Clara and her brother, James, inherited their parents' artistic talents, showcasing their skills in painting and music.  
Head Entity: Clara  
Tail Entity: James  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: two of her sons , tunis and thomas tingey rose to prominence in the union navy during the civil war .
Head Entity: tunis
Tail Entity: union navy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: after serving in the army, he transitioned to a leadership role in the air force, where he implemented several key strategies.  
Head Entity: air force  
Tail Entity: army  

Relation: military branch  
Context: the general was honored for his service in the marine corps, where he led numerous successful missions.  
Head Entity: marine corps  
Tail Entity: general  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: ziad rahbani is the son of the lebanese famous composer assi rahbani and nouhad haddad , the famous lebanese female singer known as fairuz .
Head Entity: assi rahbani
Tail Entity: ziad rahbani
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: in the animated series, the character simon is the son of the adventurous couple, marie and tom, who often embark on thrilling quests together.  
Head Entity: marie  
Tail Entity: simon  

Relation: child  
Context: the renowned physicist albert einstein had a daughter named lieserl, who was born in 1902 and later became a subject of much speculation.  
Head Entity: albert einstein  
Tail Entity: lieserl  
MemoryTrain:  epoch 15, batch     0 | loss: 9.2808945MemoryTrain:  epoch 15, batch     1 | loss: 4.5574075MemoryTrain:  epoch 15, batch     2 | loss: 2.9141271MemoryTrain:  epoch 15, batch     3 | loss: 4.1269677MemoryTrain:  epoch 15, batch     4 | loss: 4.6391880MemoryTrain:  epoch 15, batch     5 | loss: 4.2278445MemoryTrain:  epoch 15, batch     6 | loss: 4.8560036MemoryTrain:  epoch  7, batch     7 | loss: 2.9796656MemoryTrain:  epoch 15, batch     0 | loss: 4.0410602MemoryTrain:  epoch 15, batch     1 | loss: 7.1969546MemoryTrain:  epoch 15, batch     2 | loss: 5.8630026MemoryTrain:  epoch 15, batch     3 | loss: 3.0014936MemoryTrain:  epoch 15, batch     4 | loss: 2.8197673MemoryTrain:  epoch 15, batch     5 | loss: 6.1912427MemoryTrain:  epoch 15, batch     6 | loss: 4.8936636MemoryTrain:  epoch  7, batch     7 | loss: 2.2676135MemoryTrain:  epoch 15, batch     0 | loss: 2.3883252MemoryTrain:  epoch 15, batch     1 | loss: 5.3480777MemoryTrain:  epoch 15, batch     2 | loss: 2.6734022MemoryTrain:  epoch 15, batch     3 | loss: 2.5438241MemoryTrain:  epoch 15, batch     4 | loss: 2.8055811MemoryTrain:  epoch 15, batch     5 | loss: 2.2714535MemoryTrain:  epoch 15, batch     6 | loss: 4.3598199MemoryTrain:  epoch  7, batch     7 | loss: 2.7087715MemoryTrain:  epoch 15, batch     0 | loss: 2.4786908MemoryTrain:  epoch 15, batch     1 | loss: 2.5035041MemoryTrain:  epoch 15, batch     2 | loss: 3.8370272MemoryTrain:  epoch 15, batch     3 | loss: 2.8522455MemoryTrain:  epoch 15, batch     4 | loss: 2.6089524MemoryTrain:  epoch 15, batch     5 | loss: 4.3457693MemoryTrain:  epoch 15, batch     6 | loss: 6.6659457MemoryTrain:  epoch  7, batch     7 | loss: 1.9731066MemoryTrain:  epoch 15, batch     0 | loss: 3.3296999MemoryTrain:  epoch 15, batch     1 | loss: 2.0980496MemoryTrain:  epoch 15, batch     2 | loss: 1.9200005MemoryTrain:  epoch 15, batch     3 | loss: 2.4139956MemoryTrain:  epoch 15, batch     4 | loss: 3.1539212MemoryTrain:  epoch 15, batch     5 | loss: 4.2605685MemoryTrain:  epoch 15, batch     6 | loss: 2.1603658MemoryTrain:  epoch  7, batch     7 | loss: 2.0152892MemoryTrain:  epoch 15, batch     0 | loss: 2.3236024MemoryTrain:  epoch 15, batch     1 | loss: 2.0733421MemoryTrain:  epoch 15, batch     2 | loss: 4.3473335MemoryTrain:  epoch 15, batch     3 | loss: 4.4178041MemoryTrain:  epoch 15, batch     4 | loss: 4.2957273MemoryTrain:  epoch 15, batch     5 | loss: 3.3545864MemoryTrain:  epoch 15, batch     6 | loss: 2.0663477MemoryTrain:  epoch  7, batch     7 | loss: 2.1086816MemoryTrain:  epoch 15, batch     0 | loss: 1.9582352MemoryTrain:  epoch 15, batch     1 | loss: 5.5494200MemoryTrain:  epoch 15, batch     2 | loss: 3.0544606MemoryTrain:  epoch 15, batch     3 | loss: 1.8117351MemoryTrain:  epoch 15, batch     4 | loss: 4.2909312MemoryTrain:  epoch 15, batch     5 | loss: 2.2158907MemoryTrain:  epoch 15, batch     6 | loss: 1.9168171MemoryTrain:  epoch  7, batch     7 | loss: 1.5985443MemoryTrain:  epoch 15, batch     0 | loss: 2.4547816MemoryTrain:  epoch 15, batch     1 | loss: 4.1035572MemoryTrain:  epoch 15, batch     2 | loss: 1.9208644MemoryTrain:  epoch 15, batch     3 | loss: 4.3540403MemoryTrain:  epoch 15, batch     4 | loss: 1.9327514MemoryTrain:  epoch 15, batch     5 | loss: 1.7038230MemoryTrain:  epoch 15, batch     6 | loss: 1.7842345MemoryTrain:  epoch  7, batch     7 | loss: 3.9390189MemoryTrain:  epoch 15, batch     0 | loss: 1.8387384MemoryTrain:  epoch 15, batch     1 | loss: 2.2762399MemoryTrain:  epoch 15, batch     2 | loss: 1.9427711MemoryTrain:  epoch 15, batch     3 | loss: 2.7050999MemoryTrain:  epoch 15, batch     4 | loss: 3.9364371MemoryTrain:  epoch 15, batch     5 | loss: 1.7221268MemoryTrain:  epoch 15, batch     6 | loss: 2.4504268MemoryTrain:  epoch  7, batch     7 | loss: 1.5104865MemoryTrain:  epoch 15, batch     0 | loss: 1.8260569MemoryTrain:  epoch 15, batch     1 | loss: 1.4454657MemoryTrain:  epoch 15, batch     2 | loss: 3.1495798MemoryTrain:  epoch 15, batch     3 | loss: 2.7729272MemoryTrain:  epoch 15, batch     4 | loss: 4.0370275MemoryTrain:  epoch 15, batch     5 | loss: 1.6354400MemoryTrain:  epoch 15, batch     6 | loss: 1.8469971MemoryTrain:  epoch  7, batch     7 | loss: 1.3553882
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 60.94%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 58.75%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 61.61%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 64.84%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 65.28%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 66.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 67.61%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 72.32%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 73.83%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 76.04%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 76.64%   [EVAL] batch:   19 | acc: 6.25%,  total acc: 73.12%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 71.43%   [EVAL] batch:   21 | acc: 50.00%,  total acc: 70.45%   [EVAL] batch:   22 | acc: 37.50%,  total acc: 69.02%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 67.45%   [EVAL] batch:   24 | acc: 18.75%,  total acc: 65.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 66.83%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 68.06%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 68.97%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 70.04%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 71.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 71.98%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 72.85%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 73.67%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 74.45%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 75.18%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 75.87%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 76.52%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 77.14%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 77.72%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 78.51%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 79.02%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 79.36%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 79.69%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 79.03%   [EVAL] batch:   45 | acc: 43.75%,  total acc: 78.26%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 77.26%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 76.82%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 76.15%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 75.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 75.74%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 76.08%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 76.42%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 76.62%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 76.93%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 77.23%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 76.86%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 76.62%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 76.06%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 75.62%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 74.90%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 74.40%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 73.51%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 70.31%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 69.89%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 69.71%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 71.43%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 72.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 75.37%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 76.39%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 77.63%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 78.57%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 78.69%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 78.80%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 79.43%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 79.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.53%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 81.70%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 82.33%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.47%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 83.79%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 84.28%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 83.64%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 83.39%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 83.45%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 83.55%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.97%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.76%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 84.82%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 85.03%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 85.09%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 84.86%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 84.78%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 84.18%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 83.85%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 83.93%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 83.62%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 83.70%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 83.89%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 84.08%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 84.03%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 83.86%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 83.82%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 83.66%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 83.51%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 83.58%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 83.65%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 83.71%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 83.77%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 83.33%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 82.23%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 81.06%   [EVAL] batch:   65 | acc: 25.00%,  total acc: 80.21%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 79.10%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 78.22%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 77.72%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 77.86%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 78.08%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 78.30%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 78.60%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 78.72%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 78.83%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 79.11%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 79.14%   [EVAL] batch:   77 | acc: 93.75%,  total acc: 79.33%   [EVAL] batch:   78 | acc: 100.00%,  total acc: 79.59%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 79.77%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 79.94%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 79.50%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 79.07%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 78.35%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 78.24%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 77.91%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 77.51%   [EVAL] batch:   87 | acc: 37.50%,  total acc: 77.06%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 76.19%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 75.69%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 74.86%   [EVAL] batch:   91 | acc: 31.25%,  total acc: 74.39%   [EVAL] batch:   92 | acc: 12.50%,  total acc: 73.72%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 73.20%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 73.29%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 73.44%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 73.65%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 73.85%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 74.05%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 74.00%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 74.20%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 74.39%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 74.51%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 74.64%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 74.82%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 75.06%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 75.18%   [EVAL] batch:  107 | acc: 93.75%,  total acc: 75.35%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 75.17%   [EVAL] batch:  109 | acc: 81.25%,  total acc: 75.23%   [EVAL] batch:  110 | acc: 75.00%,  total acc: 75.23%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 75.45%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 75.22%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:  114 | acc: 50.00%,  total acc: 74.78%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 74.78%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 74.63%   [EVAL] batch:  117 | acc: 68.75%,  total acc: 74.58%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 74.42%   [EVAL] batch:  119 | acc: 43.75%,  total acc: 74.17%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 73.92%   [EVAL] batch:  121 | acc: 56.25%,  total acc: 73.77%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 73.68%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 73.64%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 73.50%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 73.56%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 73.52%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 73.34%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 73.06%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 73.08%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 73.09%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 73.20%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 73.36%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 73.46%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 73.52%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 73.58%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 73.72%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 73.82%   [EVAL] batch:  138 | acc: 81.25%,  total acc: 73.88%   [EVAL] batch:  139 | acc: 81.25%,  total acc: 73.93%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 74.07%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 74.16%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 74.17%   [EVAL] batch:  143 | acc: 87.50%,  total acc: 74.26%   [EVAL] batch:  144 | acc: 50.00%,  total acc: 74.09%   [EVAL] batch:  145 | acc: 50.00%,  total acc: 73.93%   [EVAL] batch:  146 | acc: 43.75%,  total acc: 73.72%   [EVAL] batch:  147 | acc: 50.00%,  total acc: 73.56%   [EVAL] batch:  148 | acc: 68.75%,  total acc: 73.53%   [EVAL] batch:  149 | acc: 62.50%,  total acc: 73.46%   [EVAL] batch:  150 | acc: 56.25%,  total acc: 73.34%   [EVAL] batch:  151 | acc: 50.00%,  total acc: 73.19%   [EVAL] batch:  152 | acc: 50.00%,  total acc: 73.04%   [EVAL] batch:  153 | acc: 43.75%,  total acc: 72.85%   [EVAL] batch:  154 | acc: 56.25%,  total acc: 72.74%   [EVAL] batch:  155 | acc: 43.75%,  total acc: 72.56%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 72.49%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 72.39%   [EVAL] batch:  158 | acc: 68.75%,  total acc: 72.37%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 72.30%   [EVAL] batch:  160 | acc: 75.00%,  total acc: 72.32%   [EVAL] batch:  161 | acc: 68.75%,  total acc: 72.30%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 72.28%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 72.26%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 72.27%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 72.21%   [EVAL] batch:  166 | acc: 56.25%,  total acc: 72.12%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 72.14%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 72.12%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 71.88%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 71.67%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 71.44%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 71.32%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 71.05%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 70.93%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 71.26%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 71.42%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 71.58%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 71.89%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 72.05%   [EVAL] batch:  182 | acc: 100.00%,  total acc: 72.20%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 72.35%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 72.65%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 72.79%   [EVAL] batch:  187 | acc: 75.00%,  total acc: 72.81%   [EVAL] batch:  188 | acc: 68.75%,  total acc: 72.78%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 72.80%   [EVAL] batch:  190 | acc: 62.50%,  total acc: 72.74%   [EVAL] batch:  191 | acc: 31.25%,  total acc: 72.53%   [EVAL] batch:  192 | acc: 68.75%,  total acc: 72.51%   [EVAL] batch:  193 | acc: 62.50%,  total acc: 72.45%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 72.47%   [EVAL] batch:  195 | acc: 81.25%,  total acc: 72.51%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 72.46%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 72.51%   [EVAL] batch:  198 | acc: 87.50%,  total acc: 72.58%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 72.66%   [EVAL] batch:  200 | acc: 100.00%,  total acc: 72.79%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 72.83%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 72.84%   [EVAL] batch:  203 | acc: 93.75%,  total acc: 72.95%   [EVAL] batch:  204 | acc: 93.75%,  total acc: 73.05%   [EVAL] batch:  205 | acc: 100.00%,  total acc: 73.18%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 73.01%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 72.78%   [EVAL] batch:  208 | acc: 62.50%,  total acc: 72.73%   [EVAL] batch:  209 | acc: 12.50%,  total acc: 72.44%   [EVAL] batch:  210 | acc: 43.75%,  total acc: 72.30%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 72.11%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 72.04%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 72.17%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 72.30%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 72.40%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 72.52%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 72.65%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 72.77%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 72.90%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 73.02%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 73.14%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 73.26%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 73.38%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 73.50%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 73.62%   [EVAL] batch:  226 | acc: 93.75%,  total acc: 73.71%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 73.82%   [EVAL] batch:  228 | acc: 93.75%,  total acc: 73.91%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 74.02%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 74.11%   [EVAL] batch:  231 | acc: 62.50%,  total acc: 74.06%   [EVAL] batch:  232 | acc: 43.75%,  total acc: 73.93%   [EVAL] batch:  233 | acc: 56.25%,  total acc: 73.85%   [EVAL] batch:  234 | acc: 31.25%,  total acc: 73.67%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 73.60%   [EVAL] batch:  236 | acc: 37.50%,  total acc: 73.44%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 73.42%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 73.48%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 73.59%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 73.65%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 73.73%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 73.82%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 73.85%   [EVAL] batch:  244 | acc: 56.25%,  total acc: 73.78%   [EVAL] batch:  245 | acc: 56.25%,  total acc: 73.70%   [EVAL] batch:  246 | acc: 37.50%,  total acc: 73.56%   [EVAL] batch:  247 | acc: 43.75%,  total acc: 73.44%   [EVAL] batch:  248 | acc: 25.00%,  total acc: 73.24%   [EVAL] batch:  249 | acc: 50.00%,  total acc: 73.15%   
cur_acc:  ['0.9494', '0.6974', '0.7421', '0.7351']
his_acc:  ['0.9494', '0.8075', '0.7580', '0.7315']
CurrentTrain: epoch 15, batch     0 | loss: 14.7590772CurrentTrain: epoch 15, batch     1 | loss: 13.9509050CurrentTrain: epoch 15, batch     2 | loss: 11.8961665CurrentTrain: epoch  1, batch     3 | loss: 10.5570929CurrentTrain: epoch 15, batch     0 | loss: 16.1485246CurrentTrain: epoch 15, batch     1 | loss: 11.3991057CurrentTrain: epoch 15, batch     2 | loss: 12.6428047CurrentTrain: epoch  1, batch     3 | loss: 24.7195427CurrentTrain: epoch 15, batch     0 | loss: 9.1043565CurrentTrain: epoch 15, batch     1 | loss: 14.6899703CurrentTrain: epoch 15, batch     2 | loss: 9.8147744CurrentTrain: epoch  1, batch     3 | loss: 11.5126260CurrentTrain: epoch 15, batch     0 | loss: 8.3328497CurrentTrain: epoch 15, batch     1 | loss: 14.1059674CurrentTrain: epoch 15, batch     2 | loss: 7.5775658CurrentTrain: epoch  1, batch     3 | loss: 5.9363158CurrentTrain: epoch 15, batch     0 | loss: 10.3064945CurrentTrain: epoch 15, batch     1 | loss: 12.1176219CurrentTrain: epoch 15, batch     2 | loss: 7.9065941CurrentTrain: epoch  1, batch     3 | loss: 7.6031822CurrentTrain: epoch 15, batch     0 | loss: 11.3347828CurrentTrain: epoch 15, batch     1 | loss: 6.4139083CurrentTrain: epoch 15, batch     2 | loss: 8.9140082CurrentTrain: epoch  1, batch     3 | loss: 7.4087803CurrentTrain: epoch 15, batch     0 | loss: 7.3549835CurrentTrain: epoch 15, batch     1 | loss: 10.4459083CurrentTrain: epoch 15, batch     2 | loss: 8.8555288CurrentTrain: epoch  1, batch     3 | loss: 15.4229204CurrentTrain: epoch 15, batch     0 | loss: 5.8251902CurrentTrain: epoch 15, batch     1 | loss: 7.2812483CurrentTrain: epoch 15, batch     2 | loss: 9.5101498CurrentTrain: epoch  1, batch     3 | loss: 6.9891736CurrentTrain: epoch 15, batch     0 | loss: 9.5330271CurrentTrain: epoch 15, batch     1 | loss: 9.7434378CurrentTrain: epoch 15, batch     2 | loss: 10.4800084CurrentTrain: epoch  1, batch     3 | loss: 6.2871352CurrentTrain: epoch 15, batch     0 | loss: 6.5327344CurrentTrain: epoch 15, batch     1 | loss: 14.1679989CurrentTrain: epoch 15, batch     2 | loss: 5.1191002CurrentTrain: epoch  1, batch     3 | loss: 5.4701691
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the cantons of créteil are administrative divisions of the val - de - marne department , île - de - france region , northern france .
Head Entity: île - de - france
Tail Entity: val - de - marne
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The provinces of Canada are the primary administrative divisions of the country, with Ontario being one of the most populous provinces.  
Head Entity: Canada  
Tail Entity: Ontario  

Relation: contains administrative territorial entity  
Context: The states of Australia are the major political divisions, with New South Wales being the most populous state in the country.  
Head Entity: Australia  
Tail Entity: New South Wales  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: enzo is an italian given name derivative of heinz , a form of henry .
Head Entity: henry
Tail Entity: heinz
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as each other due to their close genetic relationship.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: In some circles, the terms "artificial intelligence" and "machine learning" are said to be the same as each other, although experts argue there are key differences.  
Head Entity: artificial intelligence  
Tail Entity: machine learning  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: in 2010 , ty burrell received a nomination for the television critics association award for individual achievement in comedy , winning in 2011 .
Head Entity: television critics association award for individual achievement in comedy
Tail Entity: ty burrell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: In 2022, the Boston Red Sox triumphed in the World Series, securing their place in baseball history.  
Head Entity: World Series  
Tail Entity: Boston Red Sox  

Relation: winner  
Context: At the annual film festival, the documentary "Voices of Change" was awarded the best film, highlighting its impactful storytelling.  
Head Entity: best film  
Tail Entity: Voices of Change  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: among notable relatives were his maternal grandfather charles ready , a u.s. representative from tennessee and his cousin confederate states army brigadier general john hunt morgan .
Head Entity: john hunt morgan
Tail Entity: brigadier general
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: During the ceremony, General Sarah Thompson was promoted to the rank of major general, making her one of the highest-ranking women in the army.  
Head Entity: Sarah Thompson  
Tail Entity: major general  

Relation: military rank  
Context: The historical records indicate that Colonel James Anderson served with distinction during the war, eventually rising to the rank of colonel.  
Head Entity: James Anderson  
Tail Entity: colonel  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: she went on to the film school at san francisco state university , when she was hired as an artist for an atari game called " electrocop " .
Head Entity: electrocop
Tail Entity: atari
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: The renowned author released her latest novel through a well-known publishing house that has been in the industry for decades.  
Head Entity: latest novel  
Tail Entity: publishing house  

Relation: publisher  
Context: After years of hard work, the independent game developer finally secured a deal with a major publisher to distribute their new game worldwide.  
Head Entity: new game  
Tail Entity: major publisher  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " muzai moratorium " and " shōso strip " were released on vinyl in 2008 , as a part of her 10-year anniversary celebrations .
Head Entity: muzai moratorium
Tail Entity: shōso strip
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first chapter of the book introduces the main character, while the second chapter follows with a detailed backstory.  
Head Entity: first chapter  
Tail Entity: second chapter  

Relation: followed by  
Context: In the concert lineup, the opening act performed a lively set, followed by the headliner who captivated the audience with their hits.  
Head Entity: opening act  
Tail Entity: headliner  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: there is a memoir of his life and work by gustav queck ( jena , 1852 ) .
Head Entity: gustav queck
Tail Entity: jena
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: The headquarters of the company is situated in San Francisco, where they have been operating since 2010.  
Head Entity: the company  
Tail Entity: San Francisco  

Relation: work location  
Context: During her time at the university, she conducted research in various labs located in Cambridge.  
Head Entity: she  
Tail Entity: Cambridge  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in cancer treatment.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a talented software engineer, has been working at tech innovations for over five years.  
Head Entity: john smith  
Tail Entity: software engineer  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: the margaret houston house is on the national register of historic places , as are the seward plantation and the asa hoxey house .
Head Entity: seward plantation
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: the ancient city of Petra is recognized as a UNESCO World Heritage Site, highlighting its cultural significance and historical value.  
Head Entity: Petra  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: the great wall of china has been designated as a world heritage site by UNESCO, attracting millions of visitors each year.  
Head Entity: great wall of china  
Tail Entity: world heritage site
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: beverly bower ( september 30 , 1925 – march 24 , 2002 ) was an american operatic soprano who had an active international opera career from the mid-1950s through the early 1970s .
Head Entity: beverly bower
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti (october 12, 1935 – september 6, 2007) was an italian operatic tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey (born march 27, 1969) is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: soprano  
MemoryTrain:  epoch 15, batch     0 | loss: 3.1080384MemoryTrain:  epoch 15, batch     1 | loss: 3.6377031MemoryTrain:  epoch 15, batch     2 | loss: 2.6560797MemoryTrain:  epoch 15, batch     3 | loss: 3.5103089MemoryTrain:  epoch 15, batch     4 | loss: 3.8911167MemoryTrain:  epoch 15, batch     5 | loss: 2.7555255MemoryTrain:  epoch 15, batch     6 | loss: 6.7568595MemoryTrain:  epoch 15, batch     7 | loss: 3.9253379MemoryTrain:  epoch 15, batch     8 | loss: 3.0467043MemoryTrain:  epoch  5, batch     9 | loss: 11.1019159MemoryTrain:  epoch 15, batch     0 | loss: 2.4436007MemoryTrain:  epoch 15, batch     1 | loss: 3.3063166MemoryTrain:  epoch 15, batch     2 | loss: 5.6927093MemoryTrain:  epoch 15, batch     3 | loss: 3.5165651MemoryTrain:  epoch 15, batch     4 | loss: 2.8401130MemoryTrain:  epoch 15, batch     5 | loss: 3.8000368MemoryTrain:  epoch 15, batch     6 | loss: 2.7613243MemoryTrain:  epoch 15, batch     7 | loss: 3.0911702MemoryTrain:  epoch 15, batch     8 | loss: 2.8257254MemoryTrain:  epoch  5, batch     9 | loss: 8.7119409MemoryTrain:  epoch 15, batch     0 | loss: 2.5395799MemoryTrain:  epoch 15, batch     1 | loss: 2.0736200MemoryTrain:  epoch 15, batch     2 | loss: 2.0344950MemoryTrain:  epoch 15, batch     3 | loss: 2.9891168MemoryTrain:  epoch 15, batch     4 | loss: 3.8249658MemoryTrain:  epoch 15, batch     5 | loss: 2.2130520MemoryTrain:  epoch 15, batch     6 | loss: 3.4134537MemoryTrain:  epoch 15, batch     7 | loss: 2.4782412MemoryTrain:  epoch 15, batch     8 | loss: 2.3139348MemoryTrain:  epoch  5, batch     9 | loss: 13.9198718MemoryTrain:  epoch 15, batch     0 | loss: 2.9034587MemoryTrain:  epoch 15, batch     1 | loss: 1.9474279MemoryTrain:  epoch 15, batch     2 | loss: 2.2700898MemoryTrain:  epoch 15, batch     3 | loss: 2.4703976MemoryTrain:  epoch 15, batch     4 | loss: 4.0242131MemoryTrain:  epoch 15, batch     5 | loss: 2.2772559MemoryTrain:  epoch 15, batch     6 | loss: 1.9322835MemoryTrain:  epoch 15, batch     7 | loss: 2.4923823MemoryTrain:  epoch 15, batch     8 | loss: 2.1275503MemoryTrain:  epoch  5, batch     9 | loss: 8.6987758MemoryTrain:  epoch 15, batch     0 | loss: 1.8658293MemoryTrain:  epoch 15, batch     1 | loss: 3.7741339MemoryTrain:  epoch 15, batch     2 | loss: 2.2050684MemoryTrain:  epoch 15, batch     3 | loss: 1.7535834MemoryTrain:  epoch 15, batch     4 | loss: 2.7065676MemoryTrain:  epoch 15, batch     5 | loss: 2.3246175MemoryTrain:  epoch 15, batch     6 | loss: 3.2623369MemoryTrain:  epoch 15, batch     7 | loss: 1.7250407MemoryTrain:  epoch 15, batch     8 | loss: 2.6419286MemoryTrain:  epoch  5, batch     9 | loss: 7.9609649MemoryTrain:  epoch 15, batch     0 | loss: 4.2276236MemoryTrain:  epoch 15, batch     1 | loss: 2.3260530MemoryTrain:  epoch 15, batch     2 | loss: 1.6490652MemoryTrain:  epoch 15, batch     3 | loss: 1.8614445MemoryTrain:  epoch 15, batch     4 | loss: 1.6565835MemoryTrain:  epoch 15, batch     5 | loss: 3.9036532MemoryTrain:  epoch 15, batch     6 | loss: 1.8491588MemoryTrain:  epoch 15, batch     7 | loss: 2.0938749MemoryTrain:  epoch 15, batch     8 | loss: 1.9757893MemoryTrain:  epoch  5, batch     9 | loss: 9.2798608MemoryTrain:  epoch 15, batch     0 | loss: 4.1276789MemoryTrain:  epoch 15, batch     1 | loss: 3.9316476MemoryTrain:  epoch 15, batch     2 | loss: 2.0846768MemoryTrain:  epoch 15, batch     3 | loss: 1.7962609MemoryTrain:  epoch 15, batch     4 | loss: 3.0227514MemoryTrain:  epoch 15, batch     5 | loss: 1.8628275MemoryTrain:  epoch 15, batch     6 | loss: 2.0359003MemoryTrain:  epoch 15, batch     7 | loss: 2.4442781MemoryTrain:  epoch 15, batch     8 | loss: 3.2041690MemoryTrain:  epoch  5, batch     9 | loss: 13.1743107MemoryTrain:  epoch 15, batch     0 | loss: 2.6482637MemoryTrain:  epoch 15, batch     1 | loss: 4.0232644MemoryTrain:  epoch 15, batch     2 | loss: 2.2125376MemoryTrain:  epoch 15, batch     3 | loss: 4.0035402MemoryTrain:  epoch 15, batch     4 | loss: 2.5349890MemoryTrain:  epoch 15, batch     5 | loss: 1.6164268MemoryTrain:  epoch 15, batch     6 | loss: 2.4004440MemoryTrain:  epoch 15, batch     7 | loss: 1.8625646MemoryTrain:  epoch 15, batch     8 | loss: 2.2723379MemoryTrain:  epoch  5, batch     9 | loss: 20.0839797MemoryTrain:  epoch 15, batch     0 | loss: 1.8936528MemoryTrain:  epoch 15, batch     1 | loss: 1.7884542MemoryTrain:  epoch 15, batch     2 | loss: 1.6212033MemoryTrain:  epoch 15, batch     3 | loss: 3.9791522MemoryTrain:  epoch 15, batch     4 | loss: 2.0365105MemoryTrain:  epoch 15, batch     5 | loss: 2.1407871MemoryTrain:  epoch 15, batch     6 | loss: 1.7880223MemoryTrain:  epoch 15, batch     7 | loss: 1.9628634MemoryTrain:  epoch 15, batch     8 | loss: 2.7979237MemoryTrain:  epoch  5, batch     9 | loss: 8.1339691MemoryTrain:  epoch 15, batch     0 | loss: 1.6883571MemoryTrain:  epoch 15, batch     1 | loss: 3.9313143MemoryTrain:  epoch 15, batch     2 | loss: 1.9623738MemoryTrain:  epoch 15, batch     3 | loss: 2.7303065MemoryTrain:  epoch 15, batch     4 | loss: 1.8013422MemoryTrain:  epoch 15, batch     5 | loss: 1.8596772MemoryTrain:  epoch 15, batch     6 | loss: 1.9325129MemoryTrain:  epoch 15, batch     7 | loss: 1.6890143MemoryTrain:  epoch 15, batch     8 | loss: 1.6711894MemoryTrain:  epoch  5, batch     9 | loss: 7.3178886
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 59.82%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 61.72%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 61.36%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 61.46%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 64.42%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 67.50%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 68.36%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 69.49%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 71.18%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 72.70%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 73.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 74.40%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 75.57%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 75.82%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 76.30%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 77.25%   [EVAL] batch:   25 | acc: 43.75%,  total acc: 75.96%   [EVAL] batch:   26 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:   27 | acc: 37.50%,  total acc: 73.66%   [EVAL] batch:   28 | acc: 31.25%,  total acc: 72.20%   [EVAL] batch:   29 | acc: 43.75%,  total acc: 71.25%   [EVAL] batch:   30 | acc: 62.50%,  total acc: 70.97%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 70.51%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 69.89%   [EVAL] batch:   33 | acc: 37.50%,  total acc: 68.93%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 68.21%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 67.36%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 66.39%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 66.61%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 66.83%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 67.34%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 67.53%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 68.01%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 68.31%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 68.47%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 68.62%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 68.62%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 68.88%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 69.12%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 69.73%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 70.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 71.41%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 71.93%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 72.43%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 73.28%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 73.73%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 74.17%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 74.59%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 74.60%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 65.18%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 61.11%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 58.75%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 57.95%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 56.25%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 56.73%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 59.38%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 61.25%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 63.28%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 65.07%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 68.09%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 69.64%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 70.45%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 70.38%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 71.09%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 71.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.60%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 73.61%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 74.55%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 75.43%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.02%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 77.54%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 78.22%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 77.76%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 77.68%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 77.78%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 78.04%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 78.69%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.22%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 79.73%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 79.91%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 80.23%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 80.40%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 79.86%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 79.35%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 78.72%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 78.12%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 77.68%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 76.88%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 76.84%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 76.80%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 76.65%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 76.16%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 75.68%   [EVAL] batch:   55 | acc: 56.25%,  total acc: 75.33%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 75.22%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 75.11%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 75.11%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 75.31%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 75.61%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 75.81%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 75.50%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 74.51%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 73.46%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 72.63%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 71.64%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 70.86%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 70.38%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 70.62%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 71.04%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 71.35%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 71.75%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 72.04%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 72.08%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 72.37%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 72.40%   [EVAL] batch:   77 | acc: 93.75%,  total acc: 72.68%   [EVAL] batch:   78 | acc: 100.00%,  total acc: 73.02%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 73.28%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 73.61%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 73.17%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 72.52%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 71.80%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 71.62%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 71.22%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 70.62%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 69.96%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 69.17%   [EVAL] batch:   89 | acc: 18.75%,  total acc: 68.61%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 67.86%   [EVAL] batch:   91 | acc: 25.00%,  total acc: 67.39%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 66.73%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 66.29%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 66.45%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 66.67%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 66.95%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 67.22%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 67.49%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 67.50%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 67.76%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 67.95%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 68.14%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 68.39%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 68.63%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 68.93%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 68.81%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 68.23%   [EVAL] batch:  108 | acc: 6.25%,  total acc: 67.66%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 67.10%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 66.67%   [EVAL] batch:  111 | acc: 6.25%,  total acc: 66.13%   [EVAL] batch:  112 | acc: 25.00%,  total acc: 65.76%   [EVAL] batch:  113 | acc: 43.75%,  total acc: 65.57%   [EVAL] batch:  114 | acc: 43.75%,  total acc: 65.38%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 65.46%   [EVAL] batch:  116 | acc: 50.00%,  total acc: 65.33%   [EVAL] batch:  117 | acc: 68.75%,  total acc: 65.36%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 65.28%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 65.16%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 64.98%   [EVAL] batch:  121 | acc: 62.50%,  total acc: 64.96%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 64.89%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 64.92%   [EVAL] batch:  124 | acc: 50.00%,  total acc: 64.80%   [EVAL] batch:  125 | acc: 37.50%,  total acc: 64.58%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 64.37%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 64.06%   [EVAL] batch:  128 | acc: 12.50%,  total acc: 63.66%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 63.61%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 63.45%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 63.59%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 63.86%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 64.04%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 64.17%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 64.34%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 64.60%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 64.72%   [EVAL] batch:  138 | acc: 81.25%,  total acc: 64.84%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 64.87%   [EVAL] batch:  140 | acc: 68.75%,  total acc: 64.89%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 64.96%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 64.99%   [EVAL] batch:  143 | acc: 87.50%,  total acc: 65.15%   [EVAL] batch:  144 | acc: 43.75%,  total acc: 65.00%   [EVAL] batch:  145 | acc: 37.50%,  total acc: 64.81%   [EVAL] batch:  146 | acc: 43.75%,  total acc: 64.67%   [EVAL] batch:  147 | acc: 43.75%,  total acc: 64.53%   [EVAL] batch:  148 | acc: 50.00%,  total acc: 64.43%   [EVAL] batch:  149 | acc: 56.25%,  total acc: 64.38%   [EVAL] batch:  150 | acc: 62.50%,  total acc: 64.36%   [EVAL] batch:  151 | acc: 62.50%,  total acc: 64.35%   [EVAL] batch:  152 | acc: 43.75%,  total acc: 64.22%   [EVAL] batch:  153 | acc: 50.00%,  total acc: 64.12%   [EVAL] batch:  154 | acc: 56.25%,  total acc: 64.07%   [EVAL] batch:  155 | acc: 43.75%,  total acc: 63.94%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 63.93%   [EVAL] batch:  157 | acc: 43.75%,  total acc: 63.81%   [EVAL] batch:  158 | acc: 62.50%,  total acc: 63.80%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 63.79%   [EVAL] batch:  160 | acc: 75.00%,  total acc: 63.86%   [EVAL] batch:  161 | acc: 68.75%,  total acc: 63.89%   [EVAL] batch:  162 | acc: 43.75%,  total acc: 63.77%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 63.76%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 63.83%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 63.78%   [EVAL] batch:  166 | acc: 50.00%,  total acc: 63.70%   [EVAL] batch:  167 | acc: 68.75%,  total acc: 63.73%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 63.72%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 63.57%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 63.41%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 63.23%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 63.15%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 62.97%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 62.89%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 63.10%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 63.31%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 63.52%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 63.72%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 63.92%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 64.12%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 64.32%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 64.48%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 64.64%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 64.83%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 65.02%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 65.21%   [EVAL] batch:  187 | acc: 75.00%,  total acc: 65.26%   [EVAL] batch:  188 | acc: 62.50%,  total acc: 65.24%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 65.30%   [EVAL] batch:  190 | acc: 50.00%,  total acc: 65.22%   [EVAL] batch:  191 | acc: 31.25%,  total acc: 65.04%   [EVAL] batch:  192 | acc: 62.50%,  total acc: 65.03%   [EVAL] batch:  193 | acc: 68.75%,  total acc: 65.05%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 65.10%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 65.15%   [EVAL] batch:  196 | acc: 56.25%,  total acc: 65.10%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 65.18%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 65.23%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 65.31%   [EVAL] batch:  200 | acc: 87.50%,  total acc: 65.42%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 65.50%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 65.55%   [EVAL] batch:  203 | acc: 93.75%,  total acc: 65.69%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 65.76%   [EVAL] batch:  205 | acc: 100.00%,  total acc: 65.93%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 65.76%   [EVAL] batch:  207 | acc: 6.25%,  total acc: 65.47%   [EVAL] batch:  208 | acc: 31.25%,  total acc: 65.31%   [EVAL] batch:  209 | acc: 18.75%,  total acc: 65.09%   [EVAL] batch:  210 | acc: 12.50%,  total acc: 64.84%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 64.62%   [EVAL] batch:  212 | acc: 43.75%,  total acc: 64.52%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 64.69%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 64.85%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 64.96%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 65.12%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 65.25%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 65.41%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 65.57%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 65.72%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 65.88%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 66.03%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 66.18%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 66.33%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 66.48%   [EVAL] batch:  226 | acc: 93.75%,  total acc: 66.60%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 66.75%   [EVAL] batch:  228 | acc: 93.75%,  total acc: 66.87%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 67.01%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 67.13%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 67.13%   [EVAL] batch:  232 | acc: 37.50%,  total acc: 67.01%   [EVAL] batch:  233 | acc: 56.25%,  total acc: 66.96%   [EVAL] batch:  234 | acc: 31.25%,  total acc: 66.81%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 66.79%   [EVAL] batch:  236 | acc: 43.75%,  total acc: 66.69%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 66.75%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 66.84%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 66.98%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 67.06%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 67.15%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 67.26%   [EVAL] batch:  243 | acc: 75.00%,  total acc: 67.29%   [EVAL] batch:  244 | acc: 56.25%,  total acc: 67.24%   [EVAL] batch:  245 | acc: 62.50%,  total acc: 67.23%   [EVAL] batch:  246 | acc: 50.00%,  total acc: 67.16%   [EVAL] batch:  247 | acc: 43.75%,  total acc: 67.06%   [EVAL] batch:  248 | acc: 56.25%,  total acc: 67.02%   [EVAL] batch:  249 | acc: 62.50%,  total acc: 67.00%   [EVAL] batch:  250 | acc: 50.00%,  total acc: 66.93%   [EVAL] batch:  251 | acc: 62.50%,  total acc: 66.91%   [EVAL] batch:  252 | acc: 56.25%,  total acc: 66.87%   [EVAL] batch:  253 | acc: 68.75%,  total acc: 66.88%   [EVAL] batch:  254 | acc: 75.00%,  total acc: 66.91%   [EVAL] batch:  255 | acc: 62.50%,  total acc: 66.89%   [EVAL] batch:  256 | acc: 43.75%,  total acc: 66.80%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 66.84%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 66.84%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 66.83%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 66.76%   [EVAL] batch:  261 | acc: 62.50%,  total acc: 66.75%   [EVAL] batch:  262 | acc: 100.00%,  total acc: 66.87%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 66.93%   [EVAL] batch:  264 | acc: 93.75%,  total acc: 67.03%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 67.08%   [EVAL] batch:  266 | acc: 87.50%,  total acc: 67.16%   [EVAL] batch:  267 | acc: 100.00%,  total acc: 67.28%   [EVAL] batch:  268 | acc: 100.00%,  total acc: 67.40%   [EVAL] batch:  269 | acc: 81.25%,  total acc: 67.45%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 67.57%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 67.69%   [EVAL] batch:  272 | acc: 81.25%,  total acc: 67.74%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 67.81%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:  275 | acc: 43.75%,  total acc: 67.84%   [EVAL] batch:  276 | acc: 50.00%,  total acc: 67.78%   [EVAL] batch:  277 | acc: 37.50%,  total acc: 67.67%   [EVAL] batch:  278 | acc: 31.25%,  total acc: 67.54%   [EVAL] batch:  279 | acc: 43.75%,  total acc: 67.46%   [EVAL] batch:  280 | acc: 62.50%,  total acc: 67.44%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 67.40%   [EVAL] batch:  282 | acc: 50.00%,  total acc: 67.34%   [EVAL] batch:  283 | acc: 37.50%,  total acc: 67.23%   [EVAL] batch:  284 | acc: 43.75%,  total acc: 67.15%   [EVAL] batch:  285 | acc: 37.50%,  total acc: 67.05%   [EVAL] batch:  286 | acc: 31.25%,  total acc: 66.92%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 66.95%   [EVAL] batch:  288 | acc: 75.00%,  total acc: 66.98%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 67.05%   [EVAL] batch:  290 | acc: 75.00%,  total acc: 67.07%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 67.14%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 67.19%   [EVAL] batch:  293 | acc: 75.00%,  total acc: 67.22%   [EVAL] batch:  294 | acc: 81.25%,  total acc: 67.27%   [EVAL] batch:  295 | acc: 68.75%,  total acc: 67.27%   [EVAL] batch:  296 | acc: 62.50%,  total acc: 67.26%   [EVAL] batch:  297 | acc: 68.75%,  total acc: 67.26%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 67.31%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 67.35%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 67.46%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 67.57%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 67.68%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 67.78%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 67.89%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 67.99%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 68.10%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 68.18%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 68.28%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 68.39%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 68.49%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 68.59%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 68.53%   
cur_acc:  ['0.9494', '0.6974', '0.7421', '0.7351', '0.7460']
his_acc:  ['0.9494', '0.8075', '0.7580', '0.7315', '0.6853']
CurrentTrain: epoch 15, batch     0 | loss: 14.2717108CurrentTrain: epoch 15, batch     1 | loss: 12.1263575CurrentTrain: epoch 15, batch     2 | loss: 35.1828152CurrentTrain: epoch  1, batch     3 | loss: 8.3522547CurrentTrain: epoch 15, batch     0 | loss: 12.2398197CurrentTrain: epoch 15, batch     1 | loss: 14.9522077CurrentTrain: epoch 15, batch     2 | loss: 13.0612090CurrentTrain: epoch  1, batch     3 | loss: 8.6614225CurrentTrain: epoch 15, batch     0 | loss: 9.6379892CurrentTrain: epoch 15, batch     1 | loss: 12.3808918CurrentTrain: epoch 15, batch     2 | loss: 13.5789163CurrentTrain: epoch  1, batch     3 | loss: 9.2172804CurrentTrain: epoch 15, batch     0 | loss: 12.9457915CurrentTrain: epoch 15, batch     1 | loss: 14.0122252CurrentTrain: epoch 15, batch     2 | loss: 8.1612888CurrentTrain: epoch  1, batch     3 | loss: 8.3723781CurrentTrain: epoch 15, batch     0 | loss: 8.8642669CurrentTrain: epoch 15, batch     1 | loss: 11.1290334CurrentTrain: epoch 15, batch     2 | loss: 9.5648303CurrentTrain: epoch  1, batch     3 | loss: 6.8437599CurrentTrain: epoch 15, batch     0 | loss: 9.4912464CurrentTrain: epoch 15, batch     1 | loss: 9.1798684CurrentTrain: epoch 15, batch     2 | loss: 9.9729276CurrentTrain: epoch  1, batch     3 | loss: 8.2974691CurrentTrain: epoch 15, batch     0 | loss: 7.9891718CurrentTrain: epoch 15, batch     1 | loss: 11.8644609CurrentTrain: epoch 15, batch     2 | loss: 11.2415724CurrentTrain: epoch  1, batch     3 | loss: 8.7155842CurrentTrain: epoch 15, batch     0 | loss: 8.1803780CurrentTrain: epoch 15, batch     1 | loss: 8.2689371CurrentTrain: epoch 15, batch     2 | loss: 11.5469140CurrentTrain: epoch  1, batch     3 | loss: 6.2110622CurrentTrain: epoch 15, batch     0 | loss: 5.9363258CurrentTrain: epoch 15, batch     1 | loss: 11.8726617CurrentTrain: epoch 15, batch     2 | loss: 6.3054826CurrentTrain: epoch  1, batch     3 | loss: 13.1978412CurrentTrain: epoch 15, batch     0 | loss: 14.5863802CurrentTrain: epoch 15, batch     1 | loss: 11.4019760CurrentTrain: epoch 15, batch     2 | loss: 9.3616142CurrentTrain: epoch  1, batch     3 | loss: 8.8556405
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: bryson 's best known work is his 1985 book " evil angels " which chronicles the story of lindy chamberlain 's trial for murder , following the death of her baby daughter , azaria .
Head Entity: evil angels
Tail Entity: death
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: in her groundbreaking 2010 book "the immortal life of henrietta lacks," rebecca skloot explores the life of henrietta lacks and the impact of her cells on medical research.  
Head Entity: the immortal life of henrietta lacks  
Tail Entity: henrietta lacks  

Relation: main subject  
Context: "sapiens: a brief history of humankind" by yuval noah harari examines the history and impact of homo sapiens from the emergence of our species to the present day.  
Head Entity: sapiens: a brief history of humankind  
Tail Entity: homo sapiens  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: he represented romania at euro 2000 , his first major tournament where they beat england 3–2 , and drew 1–1 against germany .
Head Entity: euro 2000
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: The Brazilian national team showcased their skills at the 2014 FIFA World Cup, where they faced off against Germany in the semi-finals.  
Head Entity: 2014 FIFA World Cup  
Tail Entity: Germany  

Relation: participating team  
Context: During the 2021 Tokyo Olympics, the United States women's soccer team competed fiercely, ultimately playing against Canada in the semi-finals.  
Head Entity: 2021 Tokyo Olympics  
Tail Entity: Canada  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: among the competing artists was previous eurovision song contest entrant robin juhkental who represented estonia in 2010 as part of the group malcolm lincoln .
Head Entity: malcolm lincoln
Tail Entity: robin juhkental
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The famous painting "The Last Supper" features several apostles, including Peter, who is depicted prominently on the left side of the artwork.  
Head Entity: The Last Supper  
Tail Entity: Peter  

Relation: has part  
Context: The smartphone model Galaxy S21 comes with a high-resolution camera that includes a wide-angle lens, allowing users to capture stunning landscapes.  
Head Entity: Galaxy S21  
Tail Entity: wide-angle lens  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: he was nominated for the academy award for best picture , along with steven spielberg , ian bryce , mark gordon for the film " saving private ryan " .
Head Entity: ian bryce
Tail Entity: academy award for best picture
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film "The Shape of Water" received multiple nominations, including a nod for best director for Guillermo del Toro at the Academy Awards.  
Head Entity: Guillermo del Toro  
Tail Entity: Academy Awards  

Relation: nominated for  
Context: The popular band was nominated for the Grammy Award for Best New Artist after their debut album topped the charts.  
Head Entity: the popular band  
Tail Entity: Grammy Award for Best New Artist  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: in 2007 , the county municipality created the transit authority called " skyss " that would market public transport while is would be operated by private companies based on public service obligation contracts .
Head Entity: skyss
Tail Entity: transit authority
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone known as the iPhone was first released by Apple Inc. in 2007, revolutionizing the mobile phone industry and setting new standards for design and functionality.  
Head Entity: iPhone  
Tail Entity: smartphone  

Relation: instance of  
Context: The Great Wall of China is a series of fortifications made of various materials, built to protect the Chinese states and empires from invasions and raids.  
Head Entity: Great Wall of China  
Tail Entity: fortification
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: abus valley ( ) is an ice - free valley southeast of turnstile ridge at the north end of britannia range .
Head Entity: turnstile ridge
Tail Entity: britannia range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the sierra nevada ( ) is a mountain range in the western united states, primarily in california, that includes the famous yosemite national park and is part of the larger sierra mountain system.  
Head Entity: sierra nevada  
Tail Entity: sierra mountain system  

Relation: mountain range  
Context: the appalachian mountains ( ) are a vast system of mountains in eastern north america, stretching from canada down to alabama, and are known for their rich biodiversity and scenic beauty.  
Head Entity: appalachian mountains  
Tail Entity: north america  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: later that year he had a minor hit film with " landru " , written by françoise sagan and starring charles denner , michèle morgan , danielle darrieux and hildegard knef .
Head Entity: " landru "
Tail Entity: françoise sagan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: The acclaimed film "Inception," directed by Christopher Nolan, features a complex narrative crafted by the talented screenwriter, Jonathan Nolan.  
Head Entity: "Inception"  
Tail Entity: Jonathan Nolan  

Relation: screenwriter  
Context: The beloved animated movie "Toy Story" was brought to life through the creative writing of the screenwriter, Joss Whedon, who infused it with humor and heart.  
Head Entity: "Toy Story"  
Tail Entity: Joss Whedon  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the publisher found success in 1993 with tarō gomi 's " everyone poops " , a japanese language import that sold over one million copies and is now their best - known title .
Head Entity: everyone poops
Tail Entity: japanese language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The animated series "Avatar: The Last Airbender" features a rich narrative and is primarily produced in English, appealing to a global audience.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  

Relation: language of work or name  
Context: The famous novel "Cien años de soledad" by Gabriel García Márquez is a cornerstone of Latin American literature and is originally written in Spanish.  
Head Entity: Cien años de soledad  
Tail Entity: Spanish  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: the aircraft was a boeing 707 - 321c that first entered service with pan american world airways ( pan am ) in 1963 .
Head Entity: boeing 707 - 321c
Tail Entity: pan american world airways
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: the new electric bus fleet is managed by the city transportation authority, which ensures efficient public transit services.  
Head Entity: electric bus fleet  
Tail Entity: city transportation authority  

Relation: operator  
Context: the state-of-the-art research facility is operated by the national science foundation, focusing on advanced materials.  
Head Entity: research facility  
Tail Entity: national science foundation  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: the most important lama of this series was the third changkya , rolpai dorje , who was preceptor to the qianlong emperor of china , and chief representative of tibetan buddhism at the qing court .
Head Entity: rolpai dorje
Tail Entity: tibetan buddhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the cathedral of notre-dame is a famous example of french gothic architecture and serves as a major center for the roman catholic faith in paris.  
Head Entity: notre-dame  
Tail Entity: roman catholic faith  

Relation: religion  
Context: the dalai lama is a prominent figure in the practice of tibetan buddhism, advocating for peace and compassion around the world.  
Head Entity: dalai lama  
Tail Entity: tibetan buddhism  
MemoryTrain:  epoch 15, batch     0 | loss: 3.3949062MemoryTrain:  epoch 15, batch     1 | loss: 4.8269534MemoryTrain:  epoch 15, batch     2 | loss: 3.1302869MemoryTrain:  epoch 15, batch     3 | loss: 3.2848551MemoryTrain:  epoch 15, batch     4 | loss: 2.5604441MemoryTrain:  epoch 15, batch     5 | loss: 3.5824410MemoryTrain:  epoch 15, batch     6 | loss: 3.6405101MemoryTrain:  epoch 15, batch     7 | loss: 2.4083144MemoryTrain:  epoch 15, batch     8 | loss: 2.6116470MemoryTrain:  epoch 15, batch     9 | loss: 3.3975955MemoryTrain:  epoch 15, batch    10 | loss: 3.1440348MemoryTrain:  epoch  3, batch    11 | loss: 12.8213051MemoryTrain:  epoch 15, batch     0 | loss: 2.7937353MemoryTrain:  epoch 15, batch     1 | loss: 2.7092264MemoryTrain:  epoch 15, batch     2 | loss: 5.1282605MemoryTrain:  epoch 15, batch     3 | loss: 2.0290829MemoryTrain:  epoch 15, batch     4 | loss: 2.9365496MemoryTrain:  epoch 15, batch     5 | loss: 3.9172064MemoryTrain:  epoch 15, batch     6 | loss: 3.6322474MemoryTrain:  epoch 15, batch     7 | loss: 2.2410901MemoryTrain:  epoch 15, batch     8 | loss: 2.4720813MemoryTrain:  epoch 15, batch     9 | loss: 4.0231639MemoryTrain:  epoch 15, batch    10 | loss: 2.5968069MemoryTrain:  epoch  3, batch    11 | loss: 10.7525588MemoryTrain:  epoch 15, batch     0 | loss: 2.1417944MemoryTrain:  epoch 15, batch     1 | loss: 2.5636837MemoryTrain:  epoch 15, batch     2 | loss: 2.0109199MemoryTrain:  epoch 15, batch     3 | loss: 2.1400577MemoryTrain:  epoch 15, batch     4 | loss: 4.9157175MemoryTrain:  epoch 15, batch     5 | loss: 4.0626150MemoryTrain:  epoch 15, batch     6 | loss: 3.0431754MemoryTrain:  epoch 15, batch     7 | loss: 2.8132539MemoryTrain:  epoch 15, batch     8 | loss: 3.0267510MemoryTrain:  epoch 15, batch     9 | loss: 2.9765492MemoryTrain:  epoch 15, batch    10 | loss: 4.4584181MemoryTrain:  epoch  3, batch    11 | loss: 10.9891303MemoryTrain:  epoch 15, batch     0 | loss: 5.0258241MemoryTrain:  epoch 15, batch     1 | loss: 1.8684406MemoryTrain:  epoch 15, batch     2 | loss: 2.0499417MemoryTrain:  epoch 15, batch     3 | loss: 1.5019039MemoryTrain:  epoch 15, batch     4 | loss: 2.4483734MemoryTrain:  epoch 15, batch     5 | loss: 2.8952113MemoryTrain:  epoch 15, batch     6 | loss: 2.4025832MemoryTrain:  epoch 15, batch     7 | loss: 2.1399400MemoryTrain:  epoch 15, batch     8 | loss: 2.3683929MemoryTrain:  epoch 15, batch     9 | loss: 2.2886934MemoryTrain:  epoch 15, batch    10 | loss: 4.8418373MemoryTrain:  epoch  3, batch    11 | loss: 10.6462795MemoryTrain:  epoch 15, batch     0 | loss: 2.5400061MemoryTrain:  epoch 15, batch     1 | loss: 3.7734264MemoryTrain:  epoch 15, batch     2 | loss: 2.5071259MemoryTrain:  epoch 15, batch     3 | loss: 2.3056499MemoryTrain:  epoch 15, batch     4 | loss: 6.4603147MemoryTrain:  epoch 15, batch     5 | loss: 2.1285264MemoryTrain:  epoch 15, batch     6 | loss: 1.9768374MemoryTrain:  epoch 15, batch     7 | loss: 2.9880203MemoryTrain:  epoch 15, batch     8 | loss: 2.9338283MemoryTrain:  epoch 15, batch     9 | loss: 2.2880229MemoryTrain:  epoch 15, batch    10 | loss: 2.7464084MemoryTrain:  epoch  3, batch    11 | loss: 23.5294836MemoryTrain:  epoch 15, batch     0 | loss: 2.0632470MemoryTrain:  epoch 15, batch     1 | loss: 1.9700607MemoryTrain:  epoch 15, batch     2 | loss: 1.5064930MemoryTrain:  epoch 15, batch     3 | loss: 1.7193381MemoryTrain:  epoch 15, batch     4 | loss: 4.2027127MemoryTrain:  epoch 15, batch     5 | loss: 2.5969761MemoryTrain:  epoch 15, batch     6 | loss: 1.8276785MemoryTrain:  epoch 15, batch     7 | loss: 2.0113667MemoryTrain:  epoch 15, batch     8 | loss: 1.6773440MemoryTrain:  epoch 15, batch     9 | loss: 2.0147783MemoryTrain:  epoch 15, batch    10 | loss: 4.0271756MemoryTrain:  epoch  3, batch    11 | loss: 10.7522783MemoryTrain:  epoch 15, batch     0 | loss: 1.8463385MemoryTrain:  epoch 15, batch     1 | loss: 1.7115558MemoryTrain:  epoch 15, batch     2 | loss: 4.7652754MemoryTrain:  epoch 15, batch     3 | loss: 3.7627027MemoryTrain:  epoch 15, batch     4 | loss: 1.7957257MemoryTrain:  epoch 15, batch     5 | loss: 1.6601109MemoryTrain:  epoch 15, batch     6 | loss: 1.5445457MemoryTrain:  epoch 15, batch     7 | loss: 1.9881296MemoryTrain:  epoch 15, batch     8 | loss: 1.8052044MemoryTrain:  epoch 15, batch     9 | loss: 1.8056839MemoryTrain:  epoch 15, batch    10 | loss: 1.9906417MemoryTrain:  epoch  3, batch    11 | loss: 9.8577699MemoryTrain:  epoch 15, batch     0 | loss: 1.6476821MemoryTrain:  epoch 15, batch     1 | loss: 1.5541583MemoryTrain:  epoch 15, batch     2 | loss: 2.3960931MemoryTrain:  epoch 15, batch     3 | loss: 1.9137971MemoryTrain:  epoch 15, batch     4 | loss: 1.5480638MemoryTrain:  epoch 15, batch     5 | loss: 2.0643928MemoryTrain:  epoch 15, batch     6 | loss: 1.9028633MemoryTrain:  epoch 15, batch     7 | loss: 3.4164509MemoryTrain:  epoch 15, batch     8 | loss: 4.6825719MemoryTrain:  epoch 15, batch     9 | loss: 1.8035258MemoryTrain:  epoch 15, batch    10 | loss: 1.8272282MemoryTrain:  epoch  3, batch    11 | loss: 10.7288247MemoryTrain:  epoch 15, batch     0 | loss: 3.8057422MemoryTrain:  epoch 15, batch     1 | loss: 1.5183547MemoryTrain:  epoch 15, batch     2 | loss: 2.6704671MemoryTrain:  epoch 15, batch     3 | loss: 3.7231625MemoryTrain:  epoch 15, batch     4 | loss: 1.7593404MemoryTrain:  epoch 15, batch     5 | loss: 3.7928274MemoryTrain:  epoch 15, batch     6 | loss: 1.9631562MemoryTrain:  epoch 15, batch     7 | loss: 2.8364834MemoryTrain:  epoch 15, batch     8 | loss: 1.5929812MemoryTrain:  epoch 15, batch     9 | loss: 1.7615498MemoryTrain:  epoch 15, batch    10 | loss: 1.2912871MemoryTrain:  epoch  3, batch    11 | loss: 10.1110528MemoryTrain:  epoch 15, batch     0 | loss: 1.7692323MemoryTrain:  epoch 15, batch     1 | loss: 1.5227726MemoryTrain:  epoch 15, batch     2 | loss: 1.8835099MemoryTrain:  epoch 15, batch     3 | loss: 1.9908773MemoryTrain:  epoch 15, batch     4 | loss: 3.6827848MemoryTrain:  epoch 15, batch     5 | loss: 1.6522274MemoryTrain:  epoch 15, batch     6 | loss: 4.1728295MemoryTrain:  epoch 15, batch     7 | loss: 1.6330881MemoryTrain:  epoch 15, batch     8 | loss: 1.2901650MemoryTrain:  epoch 15, batch     9 | loss: 1.7747036MemoryTrain:  epoch 15, batch    10 | loss: 3.6841187MemoryTrain:  epoch  3, batch    11 | loss: 9.8759493
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 71.02%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 70.31%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 65.62%   [EVAL] batch:   14 | acc: 18.75%,  total acc: 62.50%   [EVAL] batch:   15 | acc: 12.50%,  total acc: 59.38%   [EVAL] batch:   16 | acc: 31.25%,  total acc: 57.72%   [EVAL] batch:   17 | acc: 37.50%,  total acc: 56.60%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 55.59%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 57.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 59.52%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 61.36%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 63.04%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 65.75%   [EVAL] batch:   25 | acc: 0.00%,  total acc: 63.22%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 61.34%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 59.38%   [EVAL] batch:   28 | acc: 18.75%,  total acc: 57.97%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 56.46%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 54.64%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 55.08%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 57.54%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 58.57%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 59.38%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 60.47%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 61.18%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 60.26%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 60.31%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 59.76%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 59.52%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 59.45%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 59.52%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 59.72%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 60.19%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 60.64%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 60.68%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 60.84%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 61.38%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 60.91%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 60.70%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 60.73%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 60.76%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 60.91%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 60.94%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 61.18%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 61.64%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 62.18%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 63.01%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 63.31%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 63.10%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 69.64%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 67.19%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 64.58%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 61.88%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 61.36%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 59.38%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 59.62%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 62.05%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 63.75%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 67.28%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 70.07%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 70.31%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 71.59%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 71.74%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 72.40%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 72.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.56%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 74.54%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.45%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 76.29%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 77.62%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 77.93%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 78.60%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 78.04%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 78.38%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 78.45%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 79.01%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 80.03%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 80.52%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 80.68%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 80.42%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 80.03%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 79.26%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 78.52%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 78.06%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 77.38%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 77.21%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 77.04%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 76.77%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 76.27%   [EVAL] batch:   54 | acc: 37.50%,  total acc: 75.57%   [EVAL] batch:   55 | acc: 56.25%,  total acc: 75.22%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 74.89%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 74.68%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 74.58%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 74.79%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 74.90%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 74.60%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 73.44%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 72.31%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 71.21%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 70.24%   [EVAL] batch:   67 | acc: 6.25%,  total acc: 69.30%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 68.84%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 69.11%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 69.45%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 70.21%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 70.44%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 70.58%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 70.89%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 70.94%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 71.15%   [EVAL] batch:   78 | acc: 100.00%,  total acc: 71.52%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 71.72%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 71.99%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 71.57%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 70.86%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 70.16%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 69.85%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 69.48%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 68.97%   [EVAL] batch:   87 | acc: 6.25%,  total acc: 68.25%   [EVAL] batch:   88 | acc: 6.25%,  total acc: 67.56%   [EVAL] batch:   89 | acc: 12.50%,  total acc: 66.94%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 66.21%   [EVAL] batch:   91 | acc: 18.75%,  total acc: 65.69%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 65.05%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 64.56%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 64.74%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 64.97%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 65.27%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 65.85%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 65.88%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 66.15%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 66.36%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 66.57%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 66.77%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 67.02%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 67.33%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 67.23%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 66.84%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 66.46%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 65.97%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 65.71%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 65.46%   [EVAL] batch:  112 | acc: 25.00%,  total acc: 65.10%   [EVAL] batch:  113 | acc: 37.50%,  total acc: 64.86%   [EVAL] batch:  114 | acc: 43.75%,  total acc: 64.67%   [EVAL] batch:  115 | acc: 56.25%,  total acc: 64.60%   [EVAL] batch:  116 | acc: 37.50%,  total acc: 64.37%   [EVAL] batch:  117 | acc: 56.25%,  total acc: 64.30%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 64.23%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 64.11%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 63.95%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 63.83%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 63.77%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 63.86%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 63.80%   [EVAL] batch:  125 | acc: 43.75%,  total acc: 63.64%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 63.48%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 63.23%   [EVAL] batch:  128 | acc: 6.25%,  total acc: 62.79%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 62.79%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 62.64%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 62.78%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 63.06%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 63.25%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 63.38%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 63.56%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 63.82%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 63.95%   [EVAL] batch:  138 | acc: 81.25%,  total acc: 64.07%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 64.02%   [EVAL] batch:  140 | acc: 68.75%,  total acc: 64.05%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 64.08%   [EVAL] batch:  142 | acc: 62.50%,  total acc: 64.07%   [EVAL] batch:  143 | acc: 75.00%,  total acc: 64.15%   [EVAL] batch:  144 | acc: 50.00%,  total acc: 64.05%   [EVAL] batch:  145 | acc: 62.50%,  total acc: 64.04%   [EVAL] batch:  146 | acc: 43.75%,  total acc: 63.90%   [EVAL] batch:  147 | acc: 50.00%,  total acc: 63.81%   [EVAL] batch:  148 | acc: 62.50%,  total acc: 63.80%   [EVAL] batch:  149 | acc: 43.75%,  total acc: 63.67%   [EVAL] batch:  150 | acc: 50.00%,  total acc: 63.58%   [EVAL] batch:  151 | acc: 68.75%,  total acc: 63.61%   [EVAL] batch:  152 | acc: 56.25%,  total acc: 63.56%   [EVAL] batch:  153 | acc: 68.75%,  total acc: 63.60%   [EVAL] batch:  154 | acc: 56.25%,  total acc: 63.55%   [EVAL] batch:  155 | acc: 56.25%,  total acc: 63.50%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 63.50%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 63.41%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 63.36%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 63.36%   [EVAL] batch:  160 | acc: 75.00%,  total acc: 63.43%   [EVAL] batch:  161 | acc: 62.50%,  total acc: 63.43%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 63.46%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 63.45%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 63.48%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 63.44%   [EVAL] batch:  166 | acc: 56.25%,  total acc: 63.40%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 63.39%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 63.39%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 63.20%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 63.05%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 62.86%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 62.83%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 62.64%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 62.57%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 62.78%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 62.99%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 63.20%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 63.41%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 63.61%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 63.81%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 64.01%   [EVAL] batch:  182 | acc: 100.00%,  total acc: 64.21%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 64.37%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 64.56%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 64.75%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 64.94%   [EVAL] batch:  187 | acc: 75.00%,  total acc: 64.99%   [EVAL] batch:  188 | acc: 56.25%,  total acc: 64.95%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 64.97%   [EVAL] batch:  190 | acc: 62.50%,  total acc: 64.95%   [EVAL] batch:  191 | acc: 43.75%,  total acc: 64.84%   [EVAL] batch:  192 | acc: 75.00%,  total acc: 64.90%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 64.95%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 65.00%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 65.02%   [EVAL] batch:  196 | acc: 56.25%,  total acc: 64.97%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 65.09%   [EVAL] batch:  198 | acc: 81.25%,  total acc: 65.17%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 65.25%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 65.24%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 65.32%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 65.33%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 65.41%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 65.46%   [EVAL] batch:  205 | acc: 81.25%,  total acc: 65.53%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 65.37%   [EVAL] batch:  207 | acc: 12.50%,  total acc: 65.11%   [EVAL] batch:  208 | acc: 18.75%,  total acc: 64.89%   [EVAL] batch:  209 | acc: 12.50%,  total acc: 64.64%   [EVAL] batch:  210 | acc: 12.50%,  total acc: 64.40%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 64.18%   [EVAL] batch:  212 | acc: 43.75%,  total acc: 64.08%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 64.25%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 64.42%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 64.53%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 64.69%   [EVAL] batch:  217 | acc: 87.50%,  total acc: 64.79%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 65.11%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 65.27%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 65.43%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 65.58%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 65.74%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 65.89%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 66.04%   [EVAL] batch:  226 | acc: 93.75%,  total acc: 66.16%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 66.31%   [EVAL] batch:  228 | acc: 93.75%,  total acc: 66.43%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 66.58%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 66.69%   [EVAL] batch:  231 | acc: 75.00%,  total acc: 66.73%   [EVAL] batch:  232 | acc: 43.75%,  total acc: 66.63%   [EVAL] batch:  233 | acc: 68.75%,  total acc: 66.64%   [EVAL] batch:  234 | acc: 31.25%,  total acc: 66.49%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 66.47%   [EVAL] batch:  236 | acc: 43.75%,  total acc: 66.38%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 66.44%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 66.53%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 66.75%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 66.84%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 66.95%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 67.01%   [EVAL] batch:  244 | acc: 56.25%,  total acc: 66.96%   [EVAL] batch:  245 | acc: 56.25%,  total acc: 66.92%   [EVAL] batch:  246 | acc: 43.75%,  total acc: 66.83%   [EVAL] batch:  247 | acc: 37.50%,  total acc: 66.71%   [EVAL] batch:  248 | acc: 50.00%,  total acc: 66.64%   [EVAL] batch:  249 | acc: 62.50%,  total acc: 66.62%   [EVAL] batch:  250 | acc: 43.75%,  total acc: 66.53%   [EVAL] batch:  251 | acc: 68.75%,  total acc: 66.54%   [EVAL] batch:  252 | acc: 62.50%,  total acc: 66.53%   [EVAL] batch:  253 | acc: 68.75%,  total acc: 66.54%   [EVAL] batch:  254 | acc: 75.00%,  total acc: 66.57%   [EVAL] batch:  255 | acc: 75.00%,  total acc: 66.60%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 66.54%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 66.57%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 66.58%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 66.61%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 66.55%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 66.56%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 66.63%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 66.70%   [EVAL] batch:  265 | acc: 87.50%,  total acc: 66.78%   [EVAL] batch:  266 | acc: 87.50%,  total acc: 66.85%   [EVAL] batch:  267 | acc: 100.00%,  total acc: 66.98%   [EVAL] batch:  268 | acc: 100.00%,  total acc: 67.10%   [EVAL] batch:  269 | acc: 81.25%,  total acc: 67.15%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 67.27%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 67.39%   [EVAL] batch:  272 | acc: 75.00%,  total acc: 67.42%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 67.50%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 67.61%   [EVAL] batch:  275 | acc: 68.75%,  total acc: 67.62%   [EVAL] batch:  276 | acc: 68.75%,  total acc: 67.62%   [EVAL] batch:  277 | acc: 50.00%,  total acc: 67.56%   [EVAL] batch:  278 | acc: 56.25%,  total acc: 67.52%   [EVAL] batch:  279 | acc: 62.50%,  total acc: 67.50%   [EVAL] batch:  280 | acc: 75.00%,  total acc: 67.53%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 67.51%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 67.49%   [EVAL] batch:  283 | acc: 37.50%,  total acc: 67.39%   [EVAL] batch:  284 | acc: 62.50%,  total acc: 67.37%   [EVAL] batch:  285 | acc: 37.50%,  total acc: 67.26%   [EVAL] batch:  286 | acc: 43.75%,  total acc: 67.18%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 67.21%   [EVAL] batch:  288 | acc: 75.00%,  total acc: 67.24%   [EVAL] batch:  289 | acc: 81.25%,  total acc: 67.28%   [EVAL] batch:  290 | acc: 68.75%,  total acc: 67.29%   [EVAL] batch:  291 | acc: 75.00%,  total acc: 67.32%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 67.36%   [EVAL] batch:  293 | acc: 68.75%,  total acc: 67.37%   [EVAL] batch:  294 | acc: 87.50%,  total acc: 67.44%   [EVAL] batch:  295 | acc: 68.75%,  total acc: 67.44%   [EVAL] batch:  296 | acc: 75.00%,  total acc: 67.47%   [EVAL] batch:  297 | acc: 68.75%,  total acc: 67.47%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 67.52%   [EVAL] batch:  299 | acc: 87.50%,  total acc: 67.58%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 67.69%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 67.80%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 67.90%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 68.01%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 68.11%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 68.22%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 68.32%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 68.41%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 68.51%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 68.61%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 68.71%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 68.81%   [EVAL] batch:  312 | acc: 87.50%,  total acc: 68.87%   [EVAL] batch:  313 | acc: 62.50%,  total acc: 68.85%   [EVAL] batch:  314 | acc: 62.50%,  total acc: 68.83%   [EVAL] batch:  315 | acc: 81.25%,  total acc: 68.87%   [EVAL] batch:  316 | acc: 81.25%,  total acc: 68.91%   [EVAL] batch:  317 | acc: 75.00%,  total acc: 68.93%   [EVAL] batch:  318 | acc: 62.50%,  total acc: 68.91%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 68.93%   [EVAL] batch:  320 | acc: 62.50%,  total acc: 68.91%   [EVAL] batch:  321 | acc: 68.75%,  total acc: 68.91%   [EVAL] batch:  322 | acc: 81.25%,  total acc: 68.94%   [EVAL] batch:  323 | acc: 62.50%,  total acc: 68.92%   [EVAL] batch:  324 | acc: 56.25%,  total acc: 68.88%   [EVAL] batch:  325 | acc: 31.25%,  total acc: 68.77%   [EVAL] batch:  326 | acc: 25.00%,  total acc: 68.64%   [EVAL] batch:  327 | acc: 12.50%,  total acc: 68.46%   [EVAL] batch:  328 | acc: 31.25%,  total acc: 68.35%   [EVAL] batch:  329 | acc: 37.50%,  total acc: 68.26%   [EVAL] batch:  330 | acc: 18.75%,  total acc: 68.11%   [EVAL] batch:  331 | acc: 75.00%,  total acc: 68.13%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 68.22%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 68.32%   [EVAL] batch:  334 | acc: 100.00%,  total acc: 68.41%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 68.51%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 68.58%   [EVAL] batch:  337 | acc: 50.00%,  total acc: 68.53%   [EVAL] batch:  338 | acc: 6.25%,  total acc: 68.34%   [EVAL] batch:  339 | acc: 12.50%,  total acc: 68.18%   [EVAL] batch:  340 | acc: 6.25%,  total acc: 68.00%   [EVAL] batch:  341 | acc: 18.75%,  total acc: 67.85%   [EVAL] batch:  342 | acc: 6.25%,  total acc: 67.67%   [EVAL] batch:  343 | acc: 25.00%,  total acc: 67.55%   [EVAL] batch:  344 | acc: 93.75%,  total acc: 67.63%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 67.70%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 67.78%   [EVAL] batch:  347 | acc: 87.50%,  total acc: 67.83%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 68.02%   [EVAL] batch:  350 | acc: 56.25%,  total acc: 67.98%   [EVAL] batch:  351 | acc: 31.25%,  total acc: 67.88%   [EVAL] batch:  352 | acc: 56.25%,  total acc: 67.85%   [EVAL] batch:  353 | acc: 37.50%,  total acc: 67.76%   [EVAL] batch:  354 | acc: 62.50%,  total acc: 67.75%   [EVAL] batch:  355 | acc: 43.75%,  total acc: 67.68%   [EVAL] batch:  356 | acc: 75.00%,  total acc: 67.70%   [EVAL] batch:  357 | acc: 75.00%,  total acc: 67.72%   [EVAL] batch:  358 | acc: 81.25%,  total acc: 67.76%   [EVAL] batch:  359 | acc: 81.25%,  total acc: 67.80%   [EVAL] batch:  360 | acc: 68.75%,  total acc: 67.80%   [EVAL] batch:  361 | acc: 68.75%,  total acc: 67.80%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 67.79%   [EVAL] batch:  363 | acc: 37.50%,  total acc: 67.70%   [EVAL] batch:  364 | acc: 68.75%,  total acc: 67.71%   [EVAL] batch:  365 | acc: 68.75%,  total acc: 67.71%   [EVAL] batch:  366 | acc: 56.25%,  total acc: 67.68%   [EVAL] batch:  367 | acc: 50.00%,  total acc: 67.63%   [EVAL] batch:  368 | acc: 68.75%,  total acc: 67.63%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 67.70%   [EVAL] batch:  370 | acc: 93.75%,  total acc: 67.77%   [EVAL] batch:  371 | acc: 81.25%,  total acc: 67.81%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 67.86%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 67.91%   [EVAL] batch:  374 | acc: 93.75%,  total acc: 67.98%   
cur_acc:  ['0.9494', '0.6974', '0.7421', '0.7351', '0.7460', '0.6310']
his_acc:  ['0.9494', '0.8075', '0.7580', '0.7315', '0.6853', '0.6798']
CurrentTrain: epoch 15, batch     0 | loss: 16.6648672CurrentTrain: epoch 15, batch     1 | loss: 12.3870392CurrentTrain: epoch 15, batch     2 | loss: 12.8241519CurrentTrain: epoch  1, batch     3 | loss: 9.1682330CurrentTrain: epoch 15, batch     0 | loss: 9.3466918CurrentTrain: epoch 15, batch     1 | loss: 15.4433439CurrentTrain: epoch 15, batch     2 | loss: 16.1597635CurrentTrain: epoch  1, batch     3 | loss: 5.9002458CurrentTrain: epoch 15, batch     0 | loss: 7.0303325CurrentTrain: epoch 15, batch     1 | loss: 9.9763081CurrentTrain: epoch 15, batch     2 | loss: 17.6431320CurrentTrain: epoch  1, batch     3 | loss: 7.9051295CurrentTrain: epoch 15, batch     0 | loss: 10.1712685CurrentTrain: epoch 15, batch     1 | loss: 13.7224007CurrentTrain: epoch 15, batch     2 | loss: 8.4892528CurrentTrain: epoch  1, batch     3 | loss: 10.0056579CurrentTrain: epoch 15, batch     0 | loss: 8.9792087CurrentTrain: epoch 15, batch     1 | loss: 8.4732911CurrentTrain: epoch 15, batch     2 | loss: 12.7058784CurrentTrain: epoch  1, batch     3 | loss: 6.3404940CurrentTrain: epoch 15, batch     0 | loss: 8.4671284CurrentTrain: epoch 15, batch     1 | loss: 8.5420778CurrentTrain: epoch 15, batch     2 | loss: 8.3273855CurrentTrain: epoch  1, batch     3 | loss: 6.2225713CurrentTrain: epoch 15, batch     0 | loss: 4.5044669CurrentTrain: epoch 15, batch     1 | loss: 7.0581286CurrentTrain: epoch 15, batch     2 | loss: 11.4035257CurrentTrain: epoch  1, batch     3 | loss: 6.1611989CurrentTrain: epoch 15, batch     0 | loss: 7.9090644CurrentTrain: epoch 15, batch     1 | loss: 5.7626753CurrentTrain: epoch 15, batch     2 | loss: 5.7113263CurrentTrain: epoch  1, batch     3 | loss: 14.8701433CurrentTrain: epoch 15, batch     0 | loss: 7.8659319CurrentTrain: epoch 15, batch     1 | loss: 6.6445882CurrentTrain: epoch 15, batch     2 | loss: 11.4220799CurrentTrain: epoch  1, batch     3 | loss: 5.7480042CurrentTrain: epoch 15, batch     0 | loss: 7.0866228CurrentTrain: epoch 15, batch     1 | loss: 8.1122371CurrentTrain: epoch 15, batch     2 | loss: 4.4695779CurrentTrain: epoch  1, batch     3 | loss: 5.4887234
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: alongside evan durbin and hugh gaitskell , he brought the thinking of john maynard keynes to the labour party , especially in relation to price determination .
Head Entity: hugh gaitskell
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: After serving as the mayor of Chicago, he became a prominent figure in the Democratic Party, advocating for social justice and economic reform.  
Head Entity: mayor of Chicago  
Tail Entity: Democratic Party  

Relation: member of political party  
Context: Throughout her career, she has been a staunch advocate for environmental policies as a member of the Green Party, pushing for sustainable practices at both local and national levels.  
Head Entity: she  
Tail Entity: Green Party  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: the original story of real life escape of betty mahmoody is depicted in the movie " not without my daughter " which itself was based on betty mahmoody 's book of the same name .
Head Entity: not without my daughter
Tail Entity: betty mahmoody
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film "the great gatsby" draws heavily from f. scott fitzgerald's classic novel, capturing the essence of the roaring twenties and the complexities of love and ambition.  
Head Entity: the great gatsby  
Tail Entity: f. scott fitzgerald  

Relation: after a work by  
Context: the animated feature "the lion king" was inspired by shakespeare's play "hamlet," incorporating themes of betrayal, revenge, and the struggle for power.  
Head Entity: the lion king  
Tail Entity: shakespeare
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: several paintings are also mentioned , including rousseau 's " the dream " and van gogh 's " bedroom in arles " .
Head Entity: rousseau
Tail Entity: the dream
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: Among his many contributions to literature, one of his most acclaimed novels is "The Great Gatsby," which explores themes of wealth and social change in 1920s America.  
Head Entity: F. Scott Fitzgerald  
Tail Entity: The Great Gatsby  

Relation: notable work  
Context: The artist is best known for his groundbreaking sculpture "The Thinker," which has become a symbol of philosophy and contemplation.  
Head Entity: Auguste Rodin  
Tail Entity: The Thinker  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: in 1986 fram traveled to poland to play against katowice in a very even duel fram eventually lost . sparta prague came to reykjavík 1987 to play against fram reykjavik at laugardalsvöllur stadium .
Head Entity: fram reykjavik
Tail Entity: reykjavík
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: in 2001, the tech company apple inc. moved its headquarters to cupertino, california, where it has since developed numerous innovative products.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: headquarters location  
Context: the multinational corporation unilever established its headquarters in rotterdam, netherlands, in the early 20th century, becoming a leader in consumer goods.  
Head Entity: unilever  
Tail Entity: rotterdam  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: vallicula is a genus of ctenophora in family coeloplanidae , containing a single species , vallicula multiformis .
Head Entity: coeloplanidae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: Panthera leo is a species of the genus Panthera, which belongs to the family Felidae, known for its large cats.  
Head Entity: Panthera  
Tail Entity: genus  

Relation: taxon rank  
Context: The genus Quercus includes various species of oak trees, which are classified under the family Fagaceae.  
Head Entity: Quercus  
Tail Entity: genus  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: he later collaborated with vito volterra and explored laplace transforms and other parts of functional analysis .
Head Entity: vito volterra
Tail Entity: functional analysis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: Marie Curie was renowned for her groundbreaking research in radioactivity, which laid the foundation for future advancements in nuclear physics.  
Head Entity: Marie Curie  
Tail Entity: nuclear physics  

Relation: field of work  
Context: The company specializes in artificial intelligence and machine learning, providing innovative solutions for various industries.  
Head Entity: The company  
Tail Entity: artificial intelligence
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: since march 2006 , wkxp 's programming has been simulcasted on 97.3 wzad wurtsboro , new york .
Head Entity: wzad
Tail Entity: wurtsboro , new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: in 2010, the station wxyz began broadcasting to the community of springfield, illinois.  
Head Entity: wxyz  
Tail Entity: springfield, illinois  

Relation: licensed to broadcast to  
Context: the radio station kqrs has been granted a license to broadcast to the area of minneapolis, minnesota since 1995.  
Head Entity: kqrs  
Tail Entity: minneapolis, minnesota  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: tau² eridani ( τ² eridani , abbreviated tau² eri , τ² eri ) , also named angetenar , is a star in the constellation of eridanus .
Head Entity: angetenar
Tail Entity: eridanus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: betelgeuse ( α orionis ) is a red supergiant star in the constellation of orion, known for its brightness and distinctive reddish hue.  
Head Entity: betelgeuse  
Tail Entity: orion  

Relation: constellation  
Context: the star deneb is located in the constellation of cygnus and is one of the brightest stars in the northern sky.  
Head Entity: deneb  
Tail Entity: cygnus  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: services to the island run from san juan 's luis muñoz marín international airport , ceiba airport or isla grande airport ( 20- to 30-minute flight ) .
Head Entity: isla grande airport
Tail Entity: san juan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The city of Barcelona is well-connected to the surrounding regions through the El Prat Airport, which offers numerous flights to various destinations.  
Head Entity: El Prat Airport  
Tail Entity: Barcelona  

Relation: place served by transport hub  
Context: The central train station in Zurich provides access to multiple Swiss cities and international destinations, making it a key transport hub in the region.  
Head Entity: Zurich central train station  
Tail Entity: Zurich  
MemoryTrain:  epoch 15, batch     0 | loss: 2.8539033MemoryTrain:  epoch 15, batch     1 | loss: 2.6528690MemoryTrain:  epoch 15, batch     2 | loss: 2.1849102MemoryTrain:  epoch 15, batch     3 | loss: 2.6052048MemoryTrain:  epoch 15, batch     4 | loss: 1.9929779MemoryTrain:  epoch 15, batch     5 | loss: 3.8420352MemoryTrain:  epoch 15, batch     6 | loss: 2.1396165MemoryTrain:  epoch 15, batch     7 | loss: 1.9838837MemoryTrain:  epoch 15, batch     8 | loss: 2.1933159MemoryTrain:  epoch 15, batch     9 | loss: 4.5356128MemoryTrain:  epoch 15, batch    10 | loss: 2.5299403MemoryTrain:  epoch 15, batch    11 | loss: 3.5253540MemoryTrain:  epoch 15, batch    12 | loss: 3.0921845MemoryTrain:  epoch  1, batch    13 | loss: 11.7170641MemoryTrain:  epoch 15, batch     0 | loss: 2.8896654MemoryTrain:  epoch 15, batch     1 | loss: 2.0841731MemoryTrain:  epoch 15, batch     2 | loss: 4.2617116MemoryTrain:  epoch 15, batch     3 | loss: 1.8513949MemoryTrain:  epoch 15, batch     4 | loss: 2.6748240MemoryTrain:  epoch 15, batch     5 | loss: 2.2613699MemoryTrain:  epoch 15, batch     6 | loss: 1.7988372MemoryTrain:  epoch 15, batch     7 | loss: 1.7908689MemoryTrain:  epoch 15, batch     8 | loss: 2.1343875MemoryTrain:  epoch 15, batch     9 | loss: 1.9301002MemoryTrain:  epoch 15, batch    10 | loss: 2.4901456MemoryTrain:  epoch 15, batch    11 | loss: 4.8913908MemoryTrain:  epoch 15, batch    12 | loss: 2.7956974MemoryTrain:  epoch  1, batch    13 | loss: 6.0665647MemoryTrain:  epoch 15, batch     0 | loss: 1.6301011MemoryTrain:  epoch 15, batch     1 | loss: 2.1964421MemoryTrain:  epoch 15, batch     2 | loss: 2.8396503MemoryTrain:  epoch 15, batch     3 | loss: 2.0396245MemoryTrain:  epoch 15, batch     4 | loss: 1.7761524MemoryTrain:  epoch 15, batch     5 | loss: 2.1729618MemoryTrain:  epoch 15, batch     6 | loss: 2.0129511MemoryTrain:  epoch 15, batch     7 | loss: 2.0127185MemoryTrain:  epoch 15, batch     8 | loss: 2.1647973MemoryTrain:  epoch 15, batch     9 | loss: 1.5593792MemoryTrain:  epoch 15, batch    10 | loss: 4.8391464MemoryTrain:  epoch 15, batch    11 | loss: 3.4258760MemoryTrain:  epoch 15, batch    12 | loss: 2.0585024MemoryTrain:  epoch  1, batch    13 | loss: 6.1483197MemoryTrain:  epoch 15, batch     0 | loss: 1.8912672MemoryTrain:  epoch 15, batch     1 | loss: 2.1373904MemoryTrain:  epoch 15, batch     2 | loss: 1.4065849MemoryTrain:  epoch 15, batch     3 | loss: 4.3580854MemoryTrain:  epoch 15, batch     4 | loss: 2.7122439MemoryTrain:  epoch 15, batch     5 | loss: 1.6145807MemoryTrain:  epoch 15, batch     6 | loss: 1.5217061MemoryTrain:  epoch 15, batch     7 | loss: 2.6069505MemoryTrain:  epoch 15, batch     8 | loss: 1.3736807MemoryTrain:  epoch 15, batch     9 | loss: 4.1231676MemoryTrain:  epoch 15, batch    10 | loss: 1.8896603MemoryTrain:  epoch 15, batch    11 | loss: 1.8985259MemoryTrain:  epoch 15, batch    12 | loss: 2.4388090MemoryTrain:  epoch  1, batch    13 | loss: 6.1339759MemoryTrain:  epoch 15, batch     0 | loss: 1.8725208MemoryTrain:  epoch 15, batch     1 | loss: 1.4200999MemoryTrain:  epoch 15, batch     2 | loss: 1.7913825MemoryTrain:  epoch 15, batch     3 | loss: 4.2533032MemoryTrain:  epoch 15, batch     4 | loss: 1.8034809MemoryTrain:  epoch 15, batch     5 | loss: 2.0956971MemoryTrain:  epoch 15, batch     6 | loss: 1.6015113MemoryTrain:  epoch 15, batch     7 | loss: 1.6962017MemoryTrain:  epoch 15, batch     8 | loss: 1.6414826MemoryTrain:  epoch 15, batch     9 | loss: 1.5441689MemoryTrain:  epoch 15, batch    10 | loss: 1.6014335MemoryTrain:  epoch 15, batch    11 | loss: 1.4409038MemoryTrain:  epoch 15, batch    12 | loss: 2.0555179MemoryTrain:  epoch  1, batch    13 | loss: 5.6704791MemoryTrain:  epoch 15, batch     0 | loss: 2.9291155MemoryTrain:  epoch 15, batch     1 | loss: 1.8798517MemoryTrain:  epoch 15, batch     2 | loss: 1.7419543MemoryTrain:  epoch 15, batch     3 | loss: 2.0590358MemoryTrain:  epoch 15, batch     4 | loss: 1.4780033MemoryTrain:  epoch 15, batch     5 | loss: 1.5539066MemoryTrain:  epoch 15, batch     6 | loss: 1.5127617MemoryTrain:  epoch 15, batch     7 | loss: 1.7037445MemoryTrain:  epoch 15, batch     8 | loss: 1.3407945MemoryTrain:  epoch 15, batch     9 | loss: 1.7098094MemoryTrain:  epoch 15, batch    10 | loss: 1.7822277MemoryTrain:  epoch 15, batch    11 | loss: 1.4539214MemoryTrain:  epoch 15, batch    12 | loss: 1.5549919MemoryTrain:  epoch  1, batch    13 | loss: 6.6262519MemoryTrain:  epoch 15, batch     0 | loss: 1.5169392MemoryTrain:  epoch 15, batch     1 | loss: 3.9966368MemoryTrain:  epoch 15, batch     2 | loss: 1.6755898MemoryTrain:  epoch 15, batch     3 | loss: 1.4197835MemoryTrain:  epoch 15, batch     4 | loss: 2.4277274MemoryTrain:  epoch 15, batch     5 | loss: 1.5955808MemoryTrain:  epoch 15, batch     6 | loss: 3.9217177MemoryTrain:  epoch 15, batch     7 | loss: 1.5260039MemoryTrain:  epoch 15, batch     8 | loss: 1.4504932MemoryTrain:  epoch 15, batch     9 | loss: 1.5639976MemoryTrain:  epoch 15, batch    10 | loss: 1.8468187MemoryTrain:  epoch 15, batch    11 | loss: 1.6661540MemoryTrain:  epoch 15, batch    12 | loss: 2.2292957MemoryTrain:  epoch  1, batch    13 | loss: 6.9206819MemoryTrain:  epoch 15, batch     0 | loss: 1.6845080MemoryTrain:  epoch 15, batch     1 | loss: 1.6907412MemoryTrain:  epoch 15, batch     2 | loss: 1.4645841MemoryTrain:  epoch 15, batch     3 | loss: 1.4579060MemoryTrain:  epoch 15, batch     4 | loss: 1.6014433MemoryTrain:  epoch 15, batch     5 | loss: 2.4589729MemoryTrain:  epoch 15, batch     6 | loss: 1.3564028MemoryTrain:  epoch 15, batch     7 | loss: 2.5012375MemoryTrain:  epoch 15, batch     8 | loss: 1.5243077MemoryTrain:  epoch 15, batch     9 | loss: 3.7708292MemoryTrain:  epoch 15, batch    10 | loss: 1.3854147MemoryTrain:  epoch 15, batch    11 | loss: 1.6891330MemoryTrain:  epoch 15, batch    12 | loss: 1.5441138MemoryTrain:  epoch  1, batch    13 | loss: 7.0172917MemoryTrain:  epoch 15, batch     0 | loss: 2.5474389MemoryTrain:  epoch 15, batch     1 | loss: 1.7186551MemoryTrain:  epoch 15, batch     2 | loss: 1.4025292MemoryTrain:  epoch 15, batch     3 | loss: 1.4692421MemoryTrain:  epoch 15, batch     4 | loss: 1.9516053MemoryTrain:  epoch 15, batch     5 | loss: 3.4634182MemoryTrain:  epoch 15, batch     6 | loss: 1.5956763MemoryTrain:  epoch 15, batch     7 | loss: 1.3568831MemoryTrain:  epoch 15, batch     8 | loss: 2.3710978MemoryTrain:  epoch 15, batch     9 | loss: 1.4145879MemoryTrain:  epoch 15, batch    10 | loss: 2.8880996MemoryTrain:  epoch 15, batch    11 | loss: 9.3280817MemoryTrain:  epoch 15, batch    12 | loss: 2.6797671MemoryTrain:  epoch  1, batch    13 | loss: 6.6206391MemoryTrain:  epoch 15, batch     0 | loss: 1.7266609MemoryTrain:  epoch 15, batch     1 | loss: 1.7028080MemoryTrain:  epoch 15, batch     2 | loss: 1.5280668MemoryTrain:  epoch 15, batch     3 | loss: 1.3450689MemoryTrain:  epoch 15, batch     4 | loss: 1.3462945MemoryTrain:  epoch 15, batch     5 | loss: 4.4127910MemoryTrain:  epoch 15, batch     6 | loss: 2.0080834MemoryTrain:  epoch 15, batch     7 | loss: 1.5688810MemoryTrain:  epoch 15, batch     8 | loss: 1.7900884MemoryTrain:  epoch 15, batch     9 | loss: 3.8181078MemoryTrain:  epoch 15, batch    10 | loss: 1.3948820MemoryTrain:  epoch 15, batch    11 | loss: 1.7130239MemoryTrain:  epoch 15, batch    12 | loss: 1.7400419MemoryTrain:  epoch  1, batch    13 | loss: 5.6021394
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 87.50%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 85.27%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 83.75%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 81.64%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 79.41%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 78.12%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 76.64%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 75.94%   [EVAL] batch:   20 | acc: 50.00%,  total acc: 74.70%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 73.30%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 73.37%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 73.18%   [EVAL] batch:   24 | acc: 50.00%,  total acc: 72.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.32%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 74.31%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.22%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 76.08%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.62%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 77.73%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 77.65%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 77.21%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 77.32%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 77.26%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 76.69%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 76.64%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 77.24%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 77.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 78.35%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 78.87%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 79.36%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 79.69%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 80.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 80.57%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 80.98%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 81.38%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 81.76%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 82.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 82.35%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 82.57%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 82.78%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 82.99%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 83.07%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 83.37%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 83.55%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 83.51%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 83.58%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 83.85%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 84.12%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 84.17%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 83.63%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 66.07%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 62.50%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 61.11%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 60.00%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 58.52%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 56.77%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 57.21%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 59.82%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 61.67%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 63.67%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 65.44%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 67.01%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 68.42%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 68.44%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 69.03%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 69.53%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 70.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 72.22%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 73.21%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 74.14%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 75.60%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 76.17%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 76.70%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 76.29%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 76.25%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 76.22%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 76.52%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 76.64%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 77.24%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 77.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 78.35%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 78.57%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 78.92%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 79.12%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 78.61%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 77.99%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 77.13%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 76.43%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 75.89%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 75.00%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 74.88%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 74.52%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 74.29%   [EVAL] batch:   53 | acc: 37.50%,  total acc: 73.61%   [EVAL] batch:   54 | acc: 31.25%,  total acc: 72.84%   [EVAL] batch:   55 | acc: 31.25%,  total acc: 72.10%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 72.04%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 71.82%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 72.08%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 72.23%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 72.38%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 72.02%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 70.90%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 69.90%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 68.84%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 67.91%   [EVAL] batch:   67 | acc: 6.25%,  total acc: 67.00%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 66.58%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 66.88%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 67.25%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 67.62%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 68.07%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 68.50%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 68.67%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 68.51%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 68.67%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 68.83%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 68.67%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 68.67%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 68.06%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 67.39%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 66.82%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 66.54%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 66.28%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 65.66%   [EVAL] batch:   87 | acc: 6.25%,  total acc: 64.99%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 64.26%   [EVAL] batch:   89 | acc: 12.50%,  total acc: 63.68%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 62.98%   [EVAL] batch:   91 | acc: 18.75%,  total acc: 62.50%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 61.90%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 61.44%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 61.64%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 61.91%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 62.11%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 62.44%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 62.56%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 62.56%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 62.87%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 63.11%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 63.29%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 63.58%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 63.87%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 64.21%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 64.19%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 63.83%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 63.53%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 63.24%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 62.95%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 62.78%   [EVAL] batch:  112 | acc: 37.50%,  total acc: 62.56%   [EVAL] batch:  113 | acc: 37.50%,  total acc: 62.34%   [EVAL] batch:  114 | acc: 43.75%,  total acc: 62.17%   [EVAL] batch:  115 | acc: 43.75%,  total acc: 62.02%   [EVAL] batch:  116 | acc: 50.00%,  total acc: 61.91%   [EVAL] batch:  117 | acc: 68.75%,  total acc: 61.97%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 61.97%   [EVAL] batch:  119 | acc: 37.50%,  total acc: 61.77%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 61.67%   [EVAL] batch:  121 | acc: 43.75%,  total acc: 61.53%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 61.48%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 61.59%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 61.55%   [EVAL] batch:  125 | acc: 37.50%,  total acc: 61.36%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 61.27%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 61.13%   [EVAL] batch:  128 | acc: 6.25%,  total acc: 60.71%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 60.72%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 60.59%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 60.75%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 61.04%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 61.29%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 61.44%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 61.63%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 61.91%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 62.05%   [EVAL] batch:  138 | acc: 75.00%,  total acc: 62.14%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 62.10%   [EVAL] batch:  140 | acc: 75.00%,  total acc: 62.19%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 62.24%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 62.28%   [EVAL] batch:  143 | acc: 75.00%,  total acc: 62.37%   [EVAL] batch:  144 | acc: 43.75%,  total acc: 62.24%   [EVAL] batch:  145 | acc: 56.25%,  total acc: 62.20%   [EVAL] batch:  146 | acc: 43.75%,  total acc: 62.07%   [EVAL] batch:  147 | acc: 37.50%,  total acc: 61.91%   [EVAL] batch:  148 | acc: 43.75%,  total acc: 61.79%   [EVAL] batch:  149 | acc: 43.75%,  total acc: 61.67%   [EVAL] batch:  150 | acc: 37.50%,  total acc: 61.51%   [EVAL] batch:  151 | acc: 62.50%,  total acc: 61.51%   [EVAL] batch:  152 | acc: 43.75%,  total acc: 61.40%   [EVAL] batch:  153 | acc: 81.25%,  total acc: 61.53%   [EVAL] batch:  154 | acc: 37.50%,  total acc: 61.37%   [EVAL] batch:  155 | acc: 50.00%,  total acc: 61.30%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 61.39%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 61.31%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 61.28%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 61.29%   [EVAL] batch:  160 | acc: 75.00%,  total acc: 61.37%   [EVAL] batch:  161 | acc: 68.75%,  total acc: 61.42%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 61.43%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 61.43%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 61.44%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 61.41%   [EVAL] batch:  166 | acc: 56.25%,  total acc: 61.38%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 61.38%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 61.39%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 61.25%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 61.11%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 60.94%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 60.87%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 60.70%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 60.64%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 60.87%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 61.09%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 61.31%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 61.52%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 61.74%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 61.95%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 62.16%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 62.33%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 62.70%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 62.90%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 63.07%   [EVAL] batch:  187 | acc: 75.00%,  total acc: 63.13%   [EVAL] batch:  188 | acc: 62.50%,  total acc: 63.13%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 63.19%   [EVAL] batch:  190 | acc: 75.00%,  total acc: 63.25%   [EVAL] batch:  191 | acc: 43.75%,  total acc: 63.15%   [EVAL] batch:  192 | acc: 75.00%,  total acc: 63.21%   [EVAL] batch:  193 | acc: 68.75%,  total acc: 63.24%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 63.30%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 63.33%   [EVAL] batch:  196 | acc: 56.25%,  total acc: 63.29%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 63.42%   [EVAL] batch:  198 | acc: 87.50%,  total acc: 63.54%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 63.62%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 63.68%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 63.77%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 63.79%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 63.88%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 63.96%   [EVAL] batch:  205 | acc: 87.50%,  total acc: 64.08%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 63.92%   [EVAL] batch:  207 | acc: 6.25%,  total acc: 63.64%   [EVAL] batch:  208 | acc: 12.50%,  total acc: 63.40%   [EVAL] batch:  209 | acc: 6.25%,  total acc: 63.12%   [EVAL] batch:  210 | acc: 12.50%,  total acc: 62.89%   [EVAL] batch:  211 | acc: 6.25%,  total acc: 62.62%   [EVAL] batch:  212 | acc: 43.75%,  total acc: 62.53%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 62.68%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 62.85%   [EVAL] batch:  215 | acc: 75.00%,  total acc: 62.91%   [EVAL] batch:  216 | acc: 93.75%,  total acc: 63.05%   [EVAL] batch:  217 | acc: 75.00%,  total acc: 63.10%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 63.21%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 63.38%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 63.55%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 63.71%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 63.87%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 64.03%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 64.19%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 64.35%   [EVAL] batch:  226 | acc: 93.75%,  total acc: 64.48%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 64.64%   [EVAL] batch:  228 | acc: 93.75%,  total acc: 64.77%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 64.92%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 65.04%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 65.06%   [EVAL] batch:  232 | acc: 43.75%,  total acc: 64.97%   [EVAL] batch:  233 | acc: 68.75%,  total acc: 64.98%   [EVAL] batch:  234 | acc: 31.25%,  total acc: 64.84%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 64.83%   [EVAL] batch:  236 | acc: 43.75%,  total acc: 64.74%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 64.81%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 64.91%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 65.05%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 65.15%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 65.24%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 65.33%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 65.45%   [EVAL] batch:  244 | acc: 56.25%,  total acc: 65.41%   [EVAL] batch:  245 | acc: 62.50%,  total acc: 65.40%   [EVAL] batch:  246 | acc: 43.75%,  total acc: 65.31%   [EVAL] batch:  247 | acc: 50.00%,  total acc: 65.25%   [EVAL] batch:  248 | acc: 56.25%,  total acc: 65.21%   [EVAL] batch:  249 | acc: 68.75%,  total acc: 65.22%   [EVAL] batch:  250 | acc: 37.50%,  total acc: 65.11%   [EVAL] batch:  251 | acc: 50.00%,  total acc: 65.05%   [EVAL] batch:  252 | acc: 56.25%,  total acc: 65.02%   [EVAL] batch:  253 | acc: 37.50%,  total acc: 64.91%   [EVAL] batch:  254 | acc: 56.25%,  total acc: 64.88%   [EVAL] batch:  255 | acc: 56.25%,  total acc: 64.84%   [EVAL] batch:  256 | acc: 43.75%,  total acc: 64.76%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 64.78%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 64.79%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 64.81%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 64.75%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 64.77%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 64.88%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 64.91%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 64.95%   [EVAL] batch:  265 | acc: 87.50%,  total acc: 65.04%   [EVAL] batch:  266 | acc: 87.50%,  total acc: 65.12%   [EVAL] batch:  267 | acc: 100.00%,  total acc: 65.25%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 65.36%   [EVAL] batch:  269 | acc: 81.25%,  total acc: 65.42%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 65.54%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 65.67%   [EVAL] batch:  272 | acc: 81.25%,  total acc: 65.73%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 65.81%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 65.93%   [EVAL] batch:  275 | acc: 75.00%,  total acc: 65.96%   [EVAL] batch:  276 | acc: 68.75%,  total acc: 65.97%   [EVAL] batch:  277 | acc: 68.75%,  total acc: 65.98%   [EVAL] batch:  278 | acc: 56.25%,  total acc: 65.95%   [EVAL] batch:  279 | acc: 81.25%,  total acc: 66.00%   [EVAL] batch:  280 | acc: 81.25%,  total acc: 66.06%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 66.02%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 66.01%   [EVAL] batch:  283 | acc: 43.75%,  total acc: 65.93%   [EVAL] batch:  284 | acc: 50.00%,  total acc: 65.88%   [EVAL] batch:  285 | acc: 37.50%,  total acc: 65.78%   [EVAL] batch:  286 | acc: 37.50%,  total acc: 65.68%   [EVAL] batch:  287 | acc: 56.25%,  total acc: 65.65%   [EVAL] batch:  288 | acc: 31.25%,  total acc: 65.53%   [EVAL] batch:  289 | acc: 56.25%,  total acc: 65.50%   [EVAL] batch:  290 | acc: 25.00%,  total acc: 65.36%   [EVAL] batch:  291 | acc: 31.25%,  total acc: 65.24%   [EVAL] batch:  292 | acc: 56.25%,  total acc: 65.21%   [EVAL] batch:  293 | acc: 37.50%,  total acc: 65.11%   [EVAL] batch:  294 | acc: 81.25%,  total acc: 65.17%   [EVAL] batch:  295 | acc: 68.75%,  total acc: 65.18%   [EVAL] batch:  296 | acc: 62.50%,  total acc: 65.17%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 65.14%   [EVAL] batch:  298 | acc: 68.75%,  total acc: 65.15%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 65.21%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 65.32%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 65.44%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 65.55%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 65.67%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 65.78%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 65.89%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 66.00%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 66.09%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 66.20%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 66.31%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 66.42%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 66.53%   [EVAL] batch:  312 | acc: 81.25%,  total acc: 66.57%   [EVAL] batch:  313 | acc: 62.50%,  total acc: 66.56%   [EVAL] batch:  314 | acc: 56.25%,  total acc: 66.53%   [EVAL] batch:  315 | acc: 75.00%,  total acc: 66.55%   [EVAL] batch:  316 | acc: 68.75%,  total acc: 66.56%   [EVAL] batch:  317 | acc: 81.25%,  total acc: 66.61%   [EVAL] batch:  318 | acc: 75.00%,  total acc: 66.63%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 66.68%   [EVAL] batch:  320 | acc: 75.00%,  total acc: 66.71%   [EVAL] batch:  321 | acc: 68.75%,  total acc: 66.71%   [EVAL] batch:  322 | acc: 81.25%,  total acc: 66.76%   [EVAL] batch:  323 | acc: 62.50%,  total acc: 66.74%   [EVAL] batch:  324 | acc: 62.50%,  total acc: 66.73%   [EVAL] batch:  325 | acc: 37.50%,  total acc: 66.64%   [EVAL] batch:  326 | acc: 18.75%,  total acc: 66.49%   [EVAL] batch:  327 | acc: 12.50%,  total acc: 66.33%   [EVAL] batch:  328 | acc: 25.00%,  total acc: 66.20%   [EVAL] batch:  329 | acc: 50.00%,  total acc: 66.16%   [EVAL] batch:  330 | acc: 18.75%,  total acc: 66.01%   [EVAL] batch:  331 | acc: 75.00%,  total acc: 66.04%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 66.14%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 66.24%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 66.32%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 66.42%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 66.51%   [EVAL] batch:  337 | acc: 50.00%,  total acc: 66.46%   [EVAL] batch:  338 | acc: 0.00%,  total acc: 66.26%   [EVAL] batch:  339 | acc: 6.25%,  total acc: 66.08%   [EVAL] batch:  340 | acc: 0.00%,  total acc: 65.89%   [EVAL] batch:  341 | acc: 25.00%,  total acc: 65.77%   [EVAL] batch:  342 | acc: 6.25%,  total acc: 65.60%   [EVAL] batch:  343 | acc: 18.75%,  total acc: 65.46%   [EVAL] batch:  344 | acc: 87.50%,  total acc: 65.53%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 65.61%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 65.69%   [EVAL] batch:  347 | acc: 87.50%,  total acc: 65.75%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 65.85%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 65.95%   [EVAL] batch:  350 | acc: 18.75%,  total acc: 65.81%   [EVAL] batch:  351 | acc: 18.75%,  total acc: 65.68%   [EVAL] batch:  352 | acc: 37.50%,  total acc: 65.60%   [EVAL] batch:  353 | acc: 37.50%,  total acc: 65.52%   [EVAL] batch:  354 | acc: 50.00%,  total acc: 65.48%   [EVAL] batch:  355 | acc: 25.00%,  total acc: 65.36%   [EVAL] batch:  356 | acc: 75.00%,  total acc: 65.39%   [EVAL] batch:  357 | acc: 75.00%,  total acc: 65.42%   [EVAL] batch:  358 | acc: 81.25%,  total acc: 65.46%   [EVAL] batch:  359 | acc: 87.50%,  total acc: 65.52%   [EVAL] batch:  360 | acc: 68.75%,  total acc: 65.53%   [EVAL] batch:  361 | acc: 68.75%,  total acc: 65.54%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 65.53%   [EVAL] batch:  363 | acc: 31.25%,  total acc: 65.44%   [EVAL] batch:  364 | acc: 56.25%,  total acc: 65.41%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 65.39%   [EVAL] batch:  366 | acc: 43.75%,  total acc: 65.33%   [EVAL] batch:  367 | acc: 50.00%,  total acc: 65.29%   [EVAL] batch:  368 | acc: 62.50%,  total acc: 65.28%   [EVAL] batch:  369 | acc: 87.50%,  total acc: 65.34%   [EVAL] batch:  370 | acc: 93.75%,  total acc: 65.41%   [EVAL] batch:  371 | acc: 81.25%,  total acc: 65.46%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 65.50%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 65.54%   [EVAL] batch:  374 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:  375 | acc: 93.75%,  total acc: 65.69%   [EVAL] batch:  376 | acc: 87.50%,  total acc: 65.75%   [EVAL] batch:  377 | acc: 93.75%,  total acc: 65.82%   [EVAL] batch:  378 | acc: 81.25%,  total acc: 65.86%   [EVAL] batch:  379 | acc: 93.75%,  total acc: 65.94%   [EVAL] batch:  380 | acc: 100.00%,  total acc: 66.03%   [EVAL] batch:  381 | acc: 81.25%,  total acc: 66.07%   [EVAL] batch:  382 | acc: 93.75%,  total acc: 66.14%   [EVAL] batch:  383 | acc: 87.50%,  total acc: 66.19%   [EVAL] batch:  384 | acc: 93.75%,  total acc: 66.27%   [EVAL] batch:  385 | acc: 87.50%,  total acc: 66.32%   [EVAL] batch:  386 | acc: 87.50%,  total acc: 66.38%   [EVAL] batch:  387 | acc: 56.25%,  total acc: 66.35%   [EVAL] batch:  388 | acc: 56.25%,  total acc: 66.32%   [EVAL] batch:  389 | acc: 62.50%,  total acc: 66.31%   [EVAL] batch:  390 | acc: 50.00%,  total acc: 66.27%   [EVAL] batch:  391 | acc: 43.75%,  total acc: 66.21%   [EVAL] batch:  392 | acc: 56.25%,  total acc: 66.19%   [EVAL] batch:  393 | acc: 50.00%,  total acc: 66.15%   [EVAL] batch:  394 | acc: 62.50%,  total acc: 66.14%   [EVAL] batch:  395 | acc: 50.00%,  total acc: 66.10%   [EVAL] batch:  396 | acc: 43.75%,  total acc: 66.04%   [EVAL] batch:  397 | acc: 75.00%,  total acc: 66.06%   [EVAL] batch:  398 | acc: 68.75%,  total acc: 66.07%   [EVAL] batch:  399 | acc: 50.00%,  total acc: 66.03%   [EVAL] batch:  400 | acc: 100.00%,  total acc: 66.12%   [EVAL] batch:  401 | acc: 100.00%,  total acc: 66.20%   [EVAL] batch:  402 | acc: 100.00%,  total acc: 66.28%   [EVAL] batch:  403 | acc: 100.00%,  total acc: 66.37%   [EVAL] batch:  404 | acc: 100.00%,  total acc: 66.45%   [EVAL] batch:  405 | acc: 100.00%,  total acc: 66.53%   [EVAL] batch:  406 | acc: 81.25%,  total acc: 66.57%   [EVAL] batch:  407 | acc: 75.00%,  total acc: 66.59%   [EVAL] batch:  408 | acc: 62.50%,  total acc: 66.58%   [EVAL] batch:  409 | acc: 81.25%,  total acc: 66.62%   [EVAL] batch:  410 | acc: 75.00%,  total acc: 66.64%   [EVAL] batch:  411 | acc: 56.25%,  total acc: 66.61%   [EVAL] batch:  412 | acc: 75.00%,  total acc: 66.63%   [EVAL] batch:  413 | acc: 100.00%,  total acc: 66.71%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 66.79%   [EVAL] batch:  415 | acc: 100.00%,  total acc: 66.87%   [EVAL] batch:  416 | acc: 100.00%,  total acc: 66.95%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 67.03%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 67.09%   [EVAL] batch:  419 | acc: 100.00%,  total acc: 67.17%   [EVAL] batch:  420 | acc: 100.00%,  total acc: 67.25%   [EVAL] batch:  421 | acc: 100.00%,  total acc: 67.33%   [EVAL] batch:  422 | acc: 100.00%,  total acc: 67.41%   [EVAL] batch:  423 | acc: 100.00%,  total acc: 67.48%   [EVAL] batch:  424 | acc: 100.00%,  total acc: 67.56%   [EVAL] batch:  425 | acc: 93.75%,  total acc: 67.62%   [EVAL] batch:  426 | acc: 93.75%,  total acc: 67.68%   [EVAL] batch:  427 | acc: 93.75%,  total acc: 67.74%   [EVAL] batch:  428 | acc: 93.75%,  total acc: 67.80%   [EVAL] batch:  429 | acc: 87.50%,  total acc: 67.85%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 67.92%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 67.98%   [EVAL] batch:  432 | acc: 81.25%,  total acc: 68.01%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 68.06%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 68.13%   [EVAL] batch:  435 | acc: 100.00%,  total acc: 68.21%   [EVAL] batch:  436 | acc: 87.50%,  total acc: 68.25%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 68.21%   
cur_acc:  ['0.9494', '0.6974', '0.7421', '0.7351', '0.7460', '0.6310', '0.8363']
his_acc:  ['0.9494', '0.8075', '0.7580', '0.7315', '0.6853', '0.6798', '0.6821']
CurrentTrain: epoch 15, batch     0 | loss: 13.7068861CurrentTrain: epoch 15, batch     1 | loss: 8.2455081CurrentTrain: epoch 15, batch     2 | loss: 17.7540536CurrentTrain: epoch  1, batch     3 | loss: 11.6254143CurrentTrain: epoch 15, batch     0 | loss: 9.6169411CurrentTrain: epoch 15, batch     1 | loss: 9.6030489CurrentTrain: epoch 15, batch     2 | loss: 10.9272070CurrentTrain: epoch  1, batch     3 | loss: 8.5611864CurrentTrain: epoch 15, batch     0 | loss: 9.0539044CurrentTrain: epoch 15, batch     1 | loss: 9.6703379CurrentTrain: epoch 15, batch     2 | loss: 12.1302353CurrentTrain: epoch  1, batch     3 | loss: 9.9657560CurrentTrain: epoch 15, batch     0 | loss: 9.0256214CurrentTrain: epoch 15, batch     1 | loss: 8.0128519CurrentTrain: epoch 15, batch     2 | loss: 24.7889277CurrentTrain: epoch  1, batch     3 | loss: 7.1953381CurrentTrain: epoch 15, batch     0 | loss: 7.4278666CurrentTrain: epoch 15, batch     1 | loss: 5.5963434CurrentTrain: epoch 15, batch     2 | loss: 9.3870063CurrentTrain: epoch  1, batch     3 | loss: 17.1654516CurrentTrain: epoch 15, batch     0 | loss: 7.0471949CurrentTrain: epoch 15, batch     1 | loss: 11.8614234CurrentTrain: epoch 15, batch     2 | loss: 11.3277155CurrentTrain: epoch  1, batch     3 | loss: 7.3355492CurrentTrain: epoch 15, batch     0 | loss: 9.2308049CurrentTrain: epoch 15, batch     1 | loss: 13.3314300CurrentTrain: epoch 15, batch     2 | loss: 8.7951256CurrentTrain: epoch  1, batch     3 | loss: 7.0632281CurrentTrain: epoch 15, batch     0 | loss: 10.0753452CurrentTrain: epoch 15, batch     1 | loss: 10.3415051CurrentTrain: epoch 15, batch     2 | loss: 5.5728996CurrentTrain: epoch  1, batch     3 | loss: 6.0780774CurrentTrain: epoch 15, batch     0 | loss: 5.7011201CurrentTrain: epoch 15, batch     1 | loss: 13.1959339CurrentTrain: epoch 15, batch     2 | loss: 8.2175940CurrentTrain: epoch  1, batch     3 | loss: 5.6363152CurrentTrain: epoch 15, batch     0 | loss: 7.0489261CurrentTrain: epoch 15, batch     1 | loss: 8.4263068CurrentTrain: epoch 15, batch     2 | loss: 6.2576269CurrentTrain: epoch  1, batch     3 | loss: 5.7230678
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: ( 1988 ) , " night watch " ( 2004 ) , " day watch " ( 2006 ) and " legend № 17 " ( 2013 ) .
Head Entity: day watch
Tail Entity: night watch
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: ( 1995 ) , " The Phantom Menace " ( 1999 ) , " Attack of the Clones " ( 2002 ) and " Revenge of the Sith " ( 2005 ) .  
Head Entity: Attack of the Clones  
Tail Entity: The Phantom Menace  

Relation: follows  
Context: ( 2010 ) , " The Hunger Games " ( 2012 ) , " Catching Fire " ( 2013 ) and " Mockingjay " ( 2014 ) .  
Head Entity: Catching Fire  
Tail Entity: The Hunger Games  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: dick packer was a u.s. soccer center forward who was a member of the u.s. team at the 1956 summer olympics .
Head Entity: dick packer
Tail Entity: soccer
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: serena williams is a professional tennis player who has won numerous grand slam titles throughout her career.  
Head Entity: serena williams  
Tail Entity: tennis  

Relation: sport  
Context: lebron james is known for his exceptional skills in basketball and has played for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: basketball  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: alphonse louis pierre pyrame de candolle ( 28 october 18064 april 1893 ) was a french - swiss botanist , the son of the swiss botanist augustin pyramus de candolle .
Head Entity: alphonse louis pierre pyrame de candolle
Tail Entity: augustin pyramus de candolle
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: charles darwin ( 12 february 1809 – 19 april 1882 ) was an english naturalist and geologist, best known for his contributions to the science of evolution. he was the son of robert darwin, a wealthy society doctor.  
Head Entity: charles darwin  
Tail Entity: robert darwin  

Relation: father  
Context: barack obama ( born august 4, 1961 ) is an american politician and attorney who served as the 44th president of the united states from 2009 to 2017. he is the son of barack obama sr., a kenyan economist.  
Head Entity: barack obama  
Tail Entity: barack obama sr.  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: he also made a special appearance in kbs ' romantic comedy drama " fight for my way " .
Head Entity: fight for my way
Tail Entity: kbs
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: the popular series "breaking bad" was first aired on amc, gaining a massive following.  
Head Entity: breaking bad  
Tail Entity: amc  

Relation: original network  
Context: the animated show "the simpsons" has been a staple of fox's programming since its debut.  
Head Entity: the simpsons  
Tail Entity: fox  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: the 1954 film , directed by kadri venkata reddy , " peddamanushulu " was honoured with the first president 's silver medal for best feature film in telugu .
Head Entity: peddamanushulu
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: the 1994 animated film "The Lion King," produced by Walt Disney, was originally created in English before being dubbed into several other languages.  
Head Entity: The Lion King  
Tail Entity: English  

Relation: original language of film or TV show  
Context: the critically acclaimed 2001 film "Amélie," directed by Jean-Pierre Jeunet, was filmed in French and has since gained a global audience.  
Head Entity: Amélie  
Tail Entity: French  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: in the summer of 2013 he returned to montenegro and joined with fk rudar pljevlja , playing with the club for two seasons in the montenegrin first league .
Head Entity: fk rudar pljevlja
Tail Entity: montenegrin first league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: After a successful stint in the English Premier League, the player signed with FC Barcelona, competing in La Liga for the next three seasons.  
Head Entity: FC Barcelona  
Tail Entity: La Liga  

Relation: league  
Context: The young athlete made headlines when he joined the New York Yankees, showcasing his talent in Major League Baseball.  
Head Entity: New York Yankees  
Tail Entity: Major League Baseball  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: the first stewart king of scotland and son of marjorie bruce and walter stewart , robert ii , is believed to have been born in the abbey .
Head Entity: robert ii
Tail Entity: marjorie bruce
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: in the historical records, it is noted that queen elizabeth i was the daughter of anne boleyn, who served as her mother during her early years.  
Head Entity: queen elizabeth i  
Tail Entity: anne boleyn  

Relation: mother  
Context: the famous artist pablo picasso often credited his mother, maría ruiz, as a significant influence on his early artistic development.  
Head Entity: pablo picasso  
Tail Entity: maría ruiz  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres. he is particularly famous for his interpretations of classical cello concertos.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: a u.s. government - funded $ 36 million bridge over the panj river connects sher khan bandar in afghanistan with nizhniy pyanzh in tajikistan , which transport more than 150 trucks or 1,000 cars daily .
Head Entity: sher khan bandar
Tail Entity: afghanistan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the ancient city of petra, located in southern jordan, is famous for its rock-cut architecture and water conduit system, attracting thousands of tourists each year.  
Head Entity: petra  
Tail Entity: jordan  

Relation: country  
Context: the great wall of china, a series of fortifications made of various materials, stretches across northern china and is a UNESCO World Heritage site.  
Head Entity: great wall of china  
Tail Entity: china  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: hagrid suggests in " harry potter and the chamber of secrets " that " " they 're startin ' ter think the job 's jinxed .
Head Entity: harry potter and the chamber of secrets
Tail Entity: hagrid
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: In the animated series "Avatar: The Last Airbender," Aang, the last Airbender, teams up with Katara and Sokka to defeat the Fire Nation.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: Aang  

Relation: characters  
Context: The novel "Pride and Prejudice" features Elizabeth Bennet as she navigates societal expectations and her relationship with Mr. Darcy.  
Head Entity: Pride and Prejudice  
Tail Entity: Elizabeth Bennet  
MemoryTrain:  epoch 15, batch     0 | loss: 3.3289782MemoryTrain:  epoch 15, batch     1 | loss: 2.2579284MemoryTrain:  epoch 15, batch     2 | loss: 3.2283677MemoryTrain:  epoch 15, batch     3 | loss: 6.2935843MemoryTrain:  epoch 15, batch     4 | loss: 2.7601511MemoryTrain:  epoch 15, batch     5 | loss: 2.1536834MemoryTrain:  epoch 15, batch     6 | loss: 2.9053656MemoryTrain:  epoch 15, batch     7 | loss: 3.1912737MemoryTrain:  epoch 15, batch     8 | loss: 4.5178384MemoryTrain:  epoch 15, batch     9 | loss: 3.2804749MemoryTrain:  epoch 15, batch    10 | loss: 2.4034722MemoryTrain:  epoch 15, batch    11 | loss: 5.5506114MemoryTrain:  epoch 15, batch    12 | loss: 2.4251142MemoryTrain:  epoch 15, batch    13 | loss: 2.8915865MemoryTrain:  epoch 15, batch    14 | loss: 2.4161889MemoryTrain:  epoch 15, batch     0 | loss: 4.0926329MemoryTrain:  epoch 15, batch     1 | loss: 2.2090256MemoryTrain:  epoch 15, batch     2 | loss: 2.1539686MemoryTrain:  epoch 15, batch     3 | loss: 1.9177197MemoryTrain:  epoch 15, batch     4 | loss: 3.7081750MemoryTrain:  epoch 15, batch     5 | loss: 1.8736965MemoryTrain:  epoch 15, batch     6 | loss: 2.7203039MemoryTrain:  epoch 15, batch     7 | loss: 4.9917630MemoryTrain:  epoch 15, batch     8 | loss: 1.8308977MemoryTrain:  epoch 15, batch     9 | loss: 1.9674452MemoryTrain:  epoch 15, batch    10 | loss: 3.9284840MemoryTrain:  epoch 15, batch    11 | loss: 4.9220681MemoryTrain:  epoch 15, batch    12 | loss: 2.4093216MemoryTrain:  epoch 15, batch    13 | loss: 2.3224349MemoryTrain:  epoch 15, batch    14 | loss: 2.4841574MemoryTrain:  epoch 15, batch     0 | loss: 5.0408685MemoryTrain:  epoch 15, batch     1 | loss: 2.2519829MemoryTrain:  epoch 15, batch     2 | loss: 2.3429654MemoryTrain:  epoch 15, batch     3 | loss: 1.6106718MemoryTrain:  epoch 15, batch     4 | loss: 1.9909081MemoryTrain:  epoch 15, batch     5 | loss: 2.3323302MemoryTrain:  epoch 15, batch     6 | loss: 1.8309240MemoryTrain:  epoch 15, batch     7 | loss: 2.0701948MemoryTrain:  epoch 15, batch     8 | loss: 4.1444231MemoryTrain:  epoch 15, batch     9 | loss: 4.2171764MemoryTrain:  epoch 15, batch    10 | loss: 1.4083162MemoryTrain:  epoch 15, batch    11 | loss: 1.7855446MemoryTrain:  epoch 15, batch    12 | loss: 2.0374651MemoryTrain:  epoch 15, batch    13 | loss: 2.2562810MemoryTrain:  epoch 15, batch    14 | loss: 1.5667815MemoryTrain:  epoch 15, batch     0 | loss: 1.7835365MemoryTrain:  epoch 15, batch     1 | loss: 2.2259039MemoryTrain:  epoch 15, batch     2 | loss: 4.4234430MemoryTrain:  epoch 15, batch     3 | loss: 2.0986690MemoryTrain:  epoch 15, batch     4 | loss: 1.7624244MemoryTrain:  epoch 15, batch     5 | loss: 2.0892454MemoryTrain:  epoch 15, batch     6 | loss: 1.4067176MemoryTrain:  epoch 15, batch     7 | loss: 1.6279778MemoryTrain:  epoch 15, batch     8 | loss: 3.5615597MemoryTrain:  epoch 15, batch     9 | loss: 2.4851421MemoryTrain:  epoch 15, batch    10 | loss: 1.6604947MemoryTrain:  epoch 15, batch    11 | loss: 2.3192124MemoryTrain:  epoch 15, batch    12 | loss: 3.7809275MemoryTrain:  epoch 15, batch    13 | loss: 2.5413085MemoryTrain:  epoch 15, batch    14 | loss: 2.0663050MemoryTrain:  epoch 15, batch     0 | loss: 1.7774883MemoryTrain:  epoch 15, batch     1 | loss: 1.8749057MemoryTrain:  epoch 15, batch     2 | loss: 1.3414731MemoryTrain:  epoch 15, batch     3 | loss: 1.9983856MemoryTrain:  epoch 15, batch     4 | loss: 1.7051329MemoryTrain:  epoch 15, batch     5 | loss: 2.0854914MemoryTrain:  epoch 15, batch     6 | loss: 1.4367851MemoryTrain:  epoch 15, batch     7 | loss: 2.2714609MemoryTrain:  epoch 15, batch     8 | loss: 1.5097496MemoryTrain:  epoch 15, batch     9 | loss: 3.5981071MemoryTrain:  epoch 15, batch    10 | loss: 1.9287954MemoryTrain:  epoch 15, batch    11 | loss: 1.4967646MemoryTrain:  epoch 15, batch    12 | loss: 1.7732496MemoryTrain:  epoch 15, batch    13 | loss: 1.5955338MemoryTrain:  epoch 15, batch    14 | loss: 4.6866445MemoryTrain:  epoch 15, batch     0 | loss: 1.8399790MemoryTrain:  epoch 15, batch     1 | loss: 1.8290775MemoryTrain:  epoch 15, batch     2 | loss: 1.8241220MemoryTrain:  epoch 15, batch     3 | loss: 1.6097269MemoryTrain:  epoch 15, batch     4 | loss: 2.0529850MemoryTrain:  epoch 15, batch     5 | loss: 2.3751989MemoryTrain:  epoch 15, batch     6 | loss: 1.3799081MemoryTrain:  epoch 15, batch     7 | loss: 1.7865670MemoryTrain:  epoch 15, batch     8 | loss: 2.0105445MemoryTrain:  epoch 15, batch     9 | loss: 2.0579099MemoryTrain:  epoch 15, batch    10 | loss: 1.4549750MemoryTrain:  epoch 15, batch    11 | loss: 1.2996270MemoryTrain:  epoch 15, batch    12 | loss: 1.3408464MemoryTrain:  epoch 15, batch    13 | loss: 1.4587301MemoryTrain:  epoch 15, batch    14 | loss: 3.6116439MemoryTrain:  epoch 15, batch     0 | loss: 2.6001806MemoryTrain:  epoch 15, batch     1 | loss: 1.6491947MemoryTrain:  epoch 15, batch     2 | loss: 1.8588869MemoryTrain:  epoch 15, batch     3 | loss: 2.7964120MemoryTrain:  epoch 15, batch     4 | loss: 1.4196700MemoryTrain:  epoch 15, batch     5 | loss: 1.4429595MemoryTrain:  epoch 15, batch     6 | loss: 1.7107941MemoryTrain:  epoch 15, batch     7 | loss: 2.2956013MemoryTrain:  epoch 15, batch     8 | loss: 1.2745561MemoryTrain:  epoch 15, batch     9 | loss: 1.3924683MemoryTrain:  epoch 15, batch    10 | loss: 2.7933146MemoryTrain:  epoch 15, batch    11 | loss: 1.4514737MemoryTrain:  epoch 15, batch    12 | loss: 2.3636015MemoryTrain:  epoch 15, batch    13 | loss: 1.2952851MemoryTrain:  epoch 15, batch    14 | loss: 1.6453322MemoryTrain:  epoch 15, batch     0 | loss: 1.2712796MemoryTrain:  epoch 15, batch     1 | loss: 2.5410495MemoryTrain:  epoch 15, batch     2 | loss: 1.3047198MemoryTrain:  epoch 15, batch     3 | loss: 1.5710025MemoryTrain:  epoch 15, batch     4 | loss: 1.4760765MemoryTrain:  epoch 15, batch     5 | loss: 1.3444987MemoryTrain:  epoch 15, batch     6 | loss: 1.5367160MemoryTrain:  epoch 15, batch     7 | loss: 1.3557561MemoryTrain:  epoch 15, batch     8 | loss: 5.5226242MemoryTrain:  epoch 15, batch     9 | loss: 1.3132302MemoryTrain:  epoch 15, batch    10 | loss: 1.5881829MemoryTrain:  epoch 15, batch    11 | loss: 2.6424583MemoryTrain:  epoch 15, batch    12 | loss: 1.7831690MemoryTrain:  epoch 15, batch    13 | loss: 1.3345259MemoryTrain:  epoch 15, batch    14 | loss: 2.2049555MemoryTrain:  epoch 15, batch     0 | loss: 1.2954088MemoryTrain:  epoch 15, batch     1 | loss: 1.7066742MemoryTrain:  epoch 15, batch     2 | loss: 2.2965704MemoryTrain:  epoch 15, batch     3 | loss: 1.6426660MemoryTrain:  epoch 15, batch     4 | loss: 3.7756162MemoryTrain:  epoch 15, batch     5 | loss: 3.7794358MemoryTrain:  epoch 15, batch     6 | loss: 2.2708631MemoryTrain:  epoch 15, batch     7 | loss: 1.6290016MemoryTrain:  epoch 15, batch     8 | loss: 3.4539125MemoryTrain:  epoch 15, batch     9 | loss: 1.5851669MemoryTrain:  epoch 15, batch    10 | loss: 1.3013792MemoryTrain:  epoch 15, batch    11 | loss: 1.4338568MemoryTrain:  epoch 15, batch    12 | loss: 1.3136773MemoryTrain:  epoch 15, batch    13 | loss: 1.6210431MemoryTrain:  epoch 15, batch    14 | loss: 2.3674161MemoryTrain:  epoch 15, batch     0 | loss: 1.6474274MemoryTrain:  epoch 15, batch     1 | loss: 1.6280963MemoryTrain:  epoch 15, batch     2 | loss: 3.7391360MemoryTrain:  epoch 15, batch     3 | loss: 3.7794639MemoryTrain:  epoch 15, batch     4 | loss: 1.5425525MemoryTrain:  epoch 15, batch     5 | loss: 1.5222156MemoryTrain:  epoch 15, batch     6 | loss: 6.8783722MemoryTrain:  epoch 15, batch     7 | loss: 1.4724474MemoryTrain:  epoch 15, batch     8 | loss: 1.4170914MemoryTrain:  epoch 15, batch     9 | loss: 1.6133573MemoryTrain:  epoch 15, batch    10 | loss: 1.8752874MemoryTrain:  epoch 15, batch    11 | loss: 1.3762493MemoryTrain:  epoch 15, batch    12 | loss: 3.9679053MemoryTrain:  epoch 15, batch    13 | loss: 3.5474246MemoryTrain:  epoch 15, batch    14 | loss: 4.0041280
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 32.50%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 35.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 42.97%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 48.61%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 53.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 55.68%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 58.33%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 58.65%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 57.59%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 57.50%   [EVAL] batch:   15 | acc: 43.75%,  total acc: 56.64%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 56.62%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 58.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 60.42%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 61.93%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 63.59%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 64.84%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 65.75%   [EVAL] batch:   25 | acc: 81.25%,  total acc: 66.35%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 67.36%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 68.08%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 68.97%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 69.58%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 70.36%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 71.29%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 72.16%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 72.98%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 73.57%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 74.13%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 74.83%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 75.16%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 75.16%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 74.39%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 73.81%   [EVAL] batch:   42 | acc: 43.75%,  total acc: 73.11%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 73.01%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 73.61%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 73.91%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 74.34%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 74.87%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 75.26%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 75.50%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 75.25%   [EVAL] batch:   51 | acc: 37.50%,  total acc: 74.52%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 74.17%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 73.84%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 73.75%   [EVAL] batch:   55 | acc: 37.50%,  total acc: 73.10%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 72.74%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 72.56%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 72.29%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 72.44%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 72.58%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 72.02%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 69.64%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 65.62%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 61.81%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 60.00%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 57.95%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 56.77%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 57.21%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 59.82%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 61.67%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 63.28%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 65.07%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 68.09%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 67.81%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 68.15%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 68.18%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 67.93%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 67.97%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 68.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 69.71%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 72.63%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 73.54%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 74.19%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 74.61%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 75.38%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 74.82%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 74.64%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 74.65%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 75.33%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 75.96%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 77.13%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 77.38%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 77.76%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 77.84%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 76.81%   [EVAL] batch:   45 | acc: 43.75%,  total acc: 76.09%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 75.40%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 74.74%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 74.23%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 73.38%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 73.28%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 72.96%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 72.76%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 72.22%   [EVAL] batch:   54 | acc: 31.25%,  total acc: 71.48%   [EVAL] batch:   55 | acc: 31.25%,  total acc: 70.76%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 70.50%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 70.04%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 69.92%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 70.21%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 70.18%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 70.26%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 69.84%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 68.75%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 67.79%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 66.76%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 65.76%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 64.80%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 64.31%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 64.64%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 65.05%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 65.45%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 65.92%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 66.22%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 66.42%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 66.69%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 66.64%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 66.83%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 67.01%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 67.03%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 67.05%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 66.46%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 65.89%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 65.40%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 65.15%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 64.90%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 64.37%   [EVAL] batch:   87 | acc: 6.25%,  total acc: 63.71%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 62.99%   [EVAL] batch:   89 | acc: 6.25%,  total acc: 62.36%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 61.68%   [EVAL] batch:   91 | acc: 12.50%,  total acc: 61.14%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 60.55%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 60.11%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 60.20%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 60.48%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 60.70%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 61.03%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 61.24%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 61.25%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 61.57%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 61.83%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 62.01%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 62.32%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 62.62%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 62.97%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 62.91%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 62.50%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 62.27%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 61.82%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 61.49%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 61.22%   [EVAL] batch:  112 | acc: 37.50%,  total acc: 61.01%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 60.91%   [EVAL] batch:  114 | acc: 37.50%,  total acc: 60.71%   [EVAL] batch:  115 | acc: 56.25%,  total acc: 60.67%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 60.68%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 60.70%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 60.71%   [EVAL] batch:  119 | acc: 37.50%,  total acc: 60.52%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 60.43%   [EVAL] batch:  121 | acc: 43.75%,  total acc: 60.30%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 60.26%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 60.38%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 60.35%   [EVAL] batch:  125 | acc: 37.50%,  total acc: 60.17%   [EVAL] batch:  126 | acc: 18.75%,  total acc: 59.84%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 59.57%   [EVAL] batch:  128 | acc: 6.25%,  total acc: 59.16%   [EVAL] batch:  129 | acc: 37.50%,  total acc: 58.99%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 58.73%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 58.90%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 59.21%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 59.42%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 59.58%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 59.79%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 60.08%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 60.14%   [EVAL] batch:  138 | acc: 50.00%,  total acc: 60.07%   [EVAL] batch:  139 | acc: 50.00%,  total acc: 60.00%   [EVAL] batch:  140 | acc: 56.25%,  total acc: 59.97%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 60.04%   [EVAL] batch:  142 | acc: 56.25%,  total acc: 60.01%   [EVAL] batch:  143 | acc: 75.00%,  total acc: 60.11%   [EVAL] batch:  144 | acc: 37.50%,  total acc: 59.96%   [EVAL] batch:  145 | acc: 31.25%,  total acc: 59.76%   [EVAL] batch:  146 | acc: 25.00%,  total acc: 59.52%   [EVAL] batch:  147 | acc: 18.75%,  total acc: 59.25%   [EVAL] batch:  148 | acc: 31.25%,  total acc: 59.06%   [EVAL] batch:  149 | acc: 43.75%,  total acc: 58.96%   [EVAL] batch:  150 | acc: 37.50%,  total acc: 58.82%   [EVAL] batch:  151 | acc: 56.25%,  total acc: 58.80%   [EVAL] batch:  152 | acc: 56.25%,  total acc: 58.78%   [EVAL] batch:  153 | acc: 68.75%,  total acc: 58.85%   [EVAL] batch:  154 | acc: 31.25%,  total acc: 58.67%   [EVAL] batch:  155 | acc: 43.75%,  total acc: 58.57%   [EVAL] batch:  156 | acc: 50.00%,  total acc: 58.52%   [EVAL] batch:  157 | acc: 43.75%,  total acc: 58.43%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 58.37%   [EVAL] batch:  159 | acc: 43.75%,  total acc: 58.28%   [EVAL] batch:  160 | acc: 68.75%,  total acc: 58.35%   [EVAL] batch:  161 | acc: 56.25%,  total acc: 58.33%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 58.28%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 58.31%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 58.30%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 58.28%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 58.31%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 58.41%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 58.43%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 58.31%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 58.19%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 58.03%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 57.98%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 57.83%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 57.79%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 58.03%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 58.26%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 58.50%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 58.73%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 58.96%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 59.19%   [EVAL] batch:  181 | acc: 87.50%,  total acc: 59.34%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 59.43%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 59.58%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 59.73%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 59.88%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 59.99%   [EVAL] batch:  187 | acc: 75.00%,  total acc: 60.07%   [EVAL] batch:  188 | acc: 56.25%,  total acc: 60.05%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 60.13%   [EVAL] batch:  190 | acc: 75.00%,  total acc: 60.21%   [EVAL] batch:  191 | acc: 37.50%,  total acc: 60.09%   [EVAL] batch:  192 | acc: 62.50%,  total acc: 60.10%   [EVAL] batch:  193 | acc: 62.50%,  total acc: 60.12%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 60.16%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 60.20%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 60.22%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 60.32%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 60.40%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 60.53%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 60.60%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 60.71%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 60.75%   [EVAL] batch:  203 | acc: 75.00%,  total acc: 60.81%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 60.91%   [EVAL] batch:  205 | acc: 87.50%,  total acc: 61.04%   [EVAL] batch:  206 | acc: 25.00%,  total acc: 60.87%   [EVAL] batch:  207 | acc: 6.25%,  total acc: 60.61%   [EVAL] batch:  208 | acc: 6.25%,  total acc: 60.35%   [EVAL] batch:  209 | acc: 6.25%,  total acc: 60.09%   [EVAL] batch:  210 | acc: 12.50%,  total acc: 59.86%   [EVAL] batch:  211 | acc: 6.25%,  total acc: 59.61%   [EVAL] batch:  212 | acc: 37.50%,  total acc: 59.51%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 59.67%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 59.85%   [EVAL] batch:  215 | acc: 62.50%,  total acc: 59.87%   [EVAL] batch:  216 | acc: 93.75%,  total acc: 60.02%   [EVAL] batch:  217 | acc: 75.00%,  total acc: 60.09%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 60.25%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 60.43%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 60.61%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 60.78%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 60.96%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 61.13%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 61.31%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 61.48%   [EVAL] batch:  226 | acc: 93.75%,  total acc: 61.62%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 61.79%   [EVAL] batch:  228 | acc: 93.75%,  total acc: 61.93%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 62.09%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 62.23%   [EVAL] batch:  231 | acc: 56.25%,  total acc: 62.20%   [EVAL] batch:  232 | acc: 31.25%,  total acc: 62.07%   [EVAL] batch:  233 | acc: 31.25%,  total acc: 61.94%   [EVAL] batch:  234 | acc: 31.25%,  total acc: 61.81%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 61.81%   [EVAL] batch:  236 | acc: 43.75%,  total acc: 61.74%   [EVAL] batch:  237 | acc: 56.25%,  total acc: 61.71%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 61.82%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 61.98%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 62.06%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 62.16%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 62.27%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 62.35%   [EVAL] batch:  244 | acc: 12.50%,  total acc: 62.14%   [EVAL] batch:  245 | acc: 18.75%,  total acc: 61.97%   [EVAL] batch:  246 | acc: 18.75%,  total acc: 61.79%   [EVAL] batch:  247 | acc: 6.25%,  total acc: 61.57%   [EVAL] batch:  248 | acc: 0.00%,  total acc: 61.32%   [EVAL] batch:  249 | acc: 6.25%,  total acc: 61.10%   [EVAL] batch:  250 | acc: 31.25%,  total acc: 60.98%   [EVAL] batch:  251 | acc: 25.00%,  total acc: 60.84%   [EVAL] batch:  252 | acc: 56.25%,  total acc: 60.82%   [EVAL] batch:  253 | acc: 37.50%,  total acc: 60.73%   [EVAL] batch:  254 | acc: 50.00%,  total acc: 60.69%   [EVAL] batch:  255 | acc: 50.00%,  total acc: 60.64%   [EVAL] batch:  256 | acc: 43.75%,  total acc: 60.58%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 60.61%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 60.64%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 60.67%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 60.63%   [EVAL] batch:  261 | acc: 62.50%,  total acc: 60.64%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 60.77%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 60.82%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 60.90%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 60.97%   [EVAL] batch:  266 | acc: 87.50%,  total acc: 61.07%   [EVAL] batch:  267 | acc: 100.00%,  total acc: 61.22%   [EVAL] batch:  268 | acc: 100.00%,  total acc: 61.36%   [EVAL] batch:  269 | acc: 81.25%,  total acc: 61.44%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 61.58%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 61.72%   [EVAL] batch:  272 | acc: 87.50%,  total acc: 61.81%   [EVAL] batch:  273 | acc: 81.25%,  total acc: 61.88%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 62.02%   [EVAL] batch:  275 | acc: 75.00%,  total acc: 62.07%   [EVAL] batch:  276 | acc: 62.50%,  total acc: 62.07%   [EVAL] batch:  277 | acc: 75.00%,  total acc: 62.12%   [EVAL] batch:  278 | acc: 75.00%,  total acc: 62.16%   [EVAL] batch:  279 | acc: 93.75%,  total acc: 62.28%   [EVAL] batch:  280 | acc: 75.00%,  total acc: 62.32%   [EVAL] batch:  281 | acc: 37.50%,  total acc: 62.23%   [EVAL] batch:  282 | acc: 18.75%,  total acc: 62.08%   [EVAL] batch:  283 | acc: 0.00%,  total acc: 61.86%   [EVAL] batch:  284 | acc: 0.00%,  total acc: 61.64%   [EVAL] batch:  285 | acc: 6.25%,  total acc: 61.45%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 61.26%   [EVAL] batch:  287 | acc: 31.25%,  total acc: 61.15%   [EVAL] batch:  288 | acc: 31.25%,  total acc: 61.05%   [EVAL] batch:  289 | acc: 50.00%,  total acc: 61.01%   [EVAL] batch:  290 | acc: 31.25%,  total acc: 60.91%   [EVAL] batch:  291 | acc: 50.00%,  total acc: 60.87%   [EVAL] batch:  292 | acc: 56.25%,  total acc: 60.86%   [EVAL] batch:  293 | acc: 37.50%,  total acc: 60.78%   [EVAL] batch:  294 | acc: 75.00%,  total acc: 60.83%   [EVAL] batch:  295 | acc: 62.50%,  total acc: 60.83%   [EVAL] batch:  296 | acc: 50.00%,  total acc: 60.80%   [EVAL] batch:  297 | acc: 62.50%,  total acc: 60.80%   [EVAL] batch:  298 | acc: 62.50%,  total acc: 60.81%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 60.85%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 60.98%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 61.11%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 61.24%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 61.37%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 61.50%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 61.62%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 61.73%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 61.81%   [EVAL] batch:  308 | acc: 81.25%,  total acc: 61.87%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 62.00%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 62.08%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 62.20%   [EVAL] batch:  312 | acc: 81.25%,  total acc: 62.26%   [EVAL] batch:  313 | acc: 56.25%,  total acc: 62.24%   [EVAL] batch:  314 | acc: 56.25%,  total acc: 62.22%   [EVAL] batch:  315 | acc: 75.00%,  total acc: 62.26%   [EVAL] batch:  316 | acc: 68.75%,  total acc: 62.28%   [EVAL] batch:  317 | acc: 62.50%,  total acc: 62.28%   [EVAL] batch:  318 | acc: 81.25%,  total acc: 62.34%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 62.42%   [EVAL] batch:  320 | acc: 75.00%,  total acc: 62.46%   [EVAL] batch:  321 | acc: 68.75%,  total acc: 62.48%   [EVAL] batch:  322 | acc: 75.00%,  total acc: 62.52%   [EVAL] batch:  323 | acc: 68.75%,  total acc: 62.54%   [EVAL] batch:  324 | acc: 62.50%,  total acc: 62.54%   [EVAL] batch:  325 | acc: 18.75%,  total acc: 62.40%   [EVAL] batch:  326 | acc: 12.50%,  total acc: 62.25%   [EVAL] batch:  327 | acc: 25.00%,  total acc: 62.14%   [EVAL] batch:  328 | acc: 25.00%,  total acc: 62.03%   [EVAL] batch:  329 | acc: 25.00%,  total acc: 61.91%   [EVAL] batch:  330 | acc: 12.50%,  total acc: 61.76%   [EVAL] batch:  331 | acc: 75.00%,  total acc: 61.80%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 61.92%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 62.03%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 62.13%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 62.24%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 62.33%   [EVAL] batch:  337 | acc: 50.00%,  total acc: 62.30%   [EVAL] batch:  338 | acc: 0.00%,  total acc: 62.11%   [EVAL] batch:  339 | acc: 6.25%,  total acc: 61.95%   [EVAL] batch:  340 | acc: 0.00%,  total acc: 61.77%   [EVAL] batch:  341 | acc: 31.25%,  total acc: 61.68%   [EVAL] batch:  342 | acc: 6.25%,  total acc: 61.52%   [EVAL] batch:  343 | acc: 25.00%,  total acc: 61.41%   [EVAL] batch:  344 | acc: 93.75%,  total acc: 61.50%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 61.60%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 61.69%   [EVAL] batch:  347 | acc: 87.50%,  total acc: 61.76%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 61.87%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 61.98%   [EVAL] batch:  350 | acc: 6.25%,  total acc: 61.82%   [EVAL] batch:  351 | acc: 18.75%,  total acc: 61.70%   [EVAL] batch:  352 | acc: 25.00%,  total acc: 61.60%   [EVAL] batch:  353 | acc: 18.75%,  total acc: 61.48%   [EVAL] batch:  354 | acc: 43.75%,  total acc: 61.43%   [EVAL] batch:  355 | acc: 25.00%,  total acc: 61.32%   [EVAL] batch:  356 | acc: 43.75%,  total acc: 61.27%   [EVAL] batch:  357 | acc: 43.75%,  total acc: 61.23%   [EVAL] batch:  358 | acc: 50.00%,  total acc: 61.19%   [EVAL] batch:  359 | acc: 43.75%,  total acc: 61.15%   [EVAL] batch:  360 | acc: 50.00%,  total acc: 61.11%   [EVAL] batch:  361 | acc: 43.75%,  total acc: 61.07%   [EVAL] batch:  362 | acc: 37.50%,  total acc: 61.00%   [EVAL] batch:  363 | acc: 37.50%,  total acc: 60.94%   [EVAL] batch:  364 | acc: 68.75%,  total acc: 60.96%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 60.93%   [EVAL] batch:  366 | acc: 50.00%,  total acc: 60.90%   [EVAL] batch:  367 | acc: 37.50%,  total acc: 60.84%   [EVAL] batch:  368 | acc: 62.50%,  total acc: 60.84%   [EVAL] batch:  369 | acc: 100.00%,  total acc: 60.95%   [EVAL] batch:  370 | acc: 93.75%,  total acc: 61.03%   [EVAL] batch:  371 | acc: 87.50%,  total acc: 61.11%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 61.18%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 61.25%   [EVAL] batch:  374 | acc: 93.75%,  total acc: 61.33%   [EVAL] batch:  375 | acc: 93.75%,  total acc: 61.42%   [EVAL] batch:  376 | acc: 87.50%,  total acc: 61.49%   [EVAL] batch:  377 | acc: 100.00%,  total acc: 61.59%   [EVAL] batch:  378 | acc: 87.50%,  total acc: 61.66%   [EVAL] batch:  379 | acc: 93.75%,  total acc: 61.74%   [EVAL] batch:  380 | acc: 100.00%,  total acc: 61.84%   [EVAL] batch:  381 | acc: 62.50%,  total acc: 61.85%   [EVAL] batch:  382 | acc: 75.00%,  total acc: 61.88%   [EVAL] batch:  383 | acc: 56.25%,  total acc: 61.87%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 61.93%   [EVAL] batch:  385 | acc: 75.00%,  total acc: 61.97%   [EVAL] batch:  386 | acc: 68.75%,  total acc: 61.98%   [EVAL] batch:  387 | acc: 62.50%,  total acc: 61.98%   [EVAL] batch:  388 | acc: 62.50%,  total acc: 61.99%   [EVAL] batch:  389 | acc: 50.00%,  total acc: 61.96%   [EVAL] batch:  390 | acc: 43.75%,  total acc: 61.91%   [EVAL] batch:  391 | acc: 43.75%,  total acc: 61.86%   [EVAL] batch:  392 | acc: 50.00%,  total acc: 61.83%   [EVAL] batch:  393 | acc: 43.75%,  total acc: 61.79%   [EVAL] batch:  394 | acc: 56.25%,  total acc: 61.77%   [EVAL] batch:  395 | acc: 43.75%,  total acc: 61.73%   [EVAL] batch:  396 | acc: 43.75%,  total acc: 61.68%   [EVAL] batch:  397 | acc: 75.00%,  total acc: 61.71%   [EVAL] batch:  398 | acc: 75.00%,  total acc: 61.75%   [EVAL] batch:  399 | acc: 50.00%,  total acc: 61.72%   [EVAL] batch:  400 | acc: 100.00%,  total acc: 61.81%   [EVAL] batch:  401 | acc: 100.00%,  total acc: 61.91%   [EVAL] batch:  402 | acc: 100.00%,  total acc: 62.00%   [EVAL] batch:  403 | acc: 100.00%,  total acc: 62.10%   [EVAL] batch:  404 | acc: 100.00%,  total acc: 62.19%   [EVAL] batch:  405 | acc: 100.00%,  total acc: 62.28%   [EVAL] batch:  406 | acc: 75.00%,  total acc: 62.32%   [EVAL] batch:  407 | acc: 81.25%,  total acc: 62.36%   [EVAL] batch:  408 | acc: 75.00%,  total acc: 62.39%   [EVAL] batch:  409 | acc: 81.25%,  total acc: 62.44%   [EVAL] batch:  410 | acc: 75.00%,  total acc: 62.47%   [EVAL] batch:  411 | acc: 62.50%,  total acc: 62.47%   [EVAL] batch:  412 | acc: 68.75%,  total acc: 62.48%   [EVAL] batch:  413 | acc: 100.00%,  total acc: 62.58%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 62.67%   [EVAL] batch:  415 | acc: 100.00%,  total acc: 62.76%   [EVAL] batch:  416 | acc: 100.00%,  total acc: 62.84%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 62.93%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 63.01%   [EVAL] batch:  419 | acc: 100.00%,  total acc: 63.10%   [EVAL] batch:  420 | acc: 100.00%,  total acc: 63.18%   [EVAL] batch:  421 | acc: 100.00%,  total acc: 63.27%   [EVAL] batch:  422 | acc: 100.00%,  total acc: 63.36%   [EVAL] batch:  423 | acc: 100.00%,  total acc: 63.44%   [EVAL] batch:  424 | acc: 100.00%,  total acc: 63.53%   [EVAL] batch:  425 | acc: 93.75%,  total acc: 63.60%   [EVAL] batch:  426 | acc: 93.75%,  total acc: 63.67%   [EVAL] batch:  427 | acc: 93.75%,  total acc: 63.74%   [EVAL] batch:  428 | acc: 93.75%,  total acc: 63.81%   [EVAL] batch:  429 | acc: 87.50%,  total acc: 63.87%   [EVAL] batch:  430 | acc: 93.75%,  total acc: 63.94%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 64.00%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 64.06%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 64.13%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 64.21%   [EVAL] batch:  435 | acc: 100.00%,  total acc: 64.29%   [EVAL] batch:  436 | acc: 81.25%,  total acc: 64.33%   [EVAL] batch:  437 | acc: 81.25%,  total acc: 64.37%   [EVAL] batch:  438 | acc: 25.00%,  total acc: 64.28%   [EVAL] batch:  439 | acc: 25.00%,  total acc: 64.19%   [EVAL] batch:  440 | acc: 25.00%,  total acc: 64.10%   [EVAL] batch:  441 | acc: 31.25%,  total acc: 64.03%   [EVAL] batch:  442 | acc: 37.50%,  total acc: 63.97%   [EVAL] batch:  443 | acc: 31.25%,  total acc: 63.89%   [EVAL] batch:  444 | acc: 87.50%,  total acc: 63.95%   [EVAL] batch:  445 | acc: 100.00%,  total acc: 64.03%   [EVAL] batch:  446 | acc: 93.75%,  total acc: 64.09%   [EVAL] batch:  447 | acc: 81.25%,  total acc: 64.13%   [EVAL] batch:  448 | acc: 87.50%,  total acc: 64.18%   [EVAL] batch:  449 | acc: 93.75%,  total acc: 64.25%   [EVAL] batch:  450 | acc: 43.75%,  total acc: 64.20%   [EVAL] batch:  451 | acc: 43.75%,  total acc: 64.16%   [EVAL] batch:  452 | acc: 43.75%,  total acc: 64.11%   [EVAL] batch:  453 | acc: 56.25%,  total acc: 64.10%   [EVAL] batch:  454 | acc: 50.00%,  total acc: 64.07%   [EVAL] batch:  455 | acc: 50.00%,  total acc: 64.04%   [EVAL] batch:  456 | acc: 81.25%,  total acc: 64.07%   [EVAL] batch:  457 | acc: 100.00%,  total acc: 64.15%   [EVAL] batch:  458 | acc: 100.00%,  total acc: 64.23%   [EVAL] batch:  459 | acc: 93.75%,  total acc: 64.29%   [EVAL] batch:  460 | acc: 93.75%,  total acc: 64.36%   [EVAL] batch:  461 | acc: 100.00%,  total acc: 64.43%   [EVAL] batch:  462 | acc: 75.00%,  total acc: 64.46%   [EVAL] batch:  463 | acc: 87.50%,  total acc: 64.51%   [EVAL] batch:  464 | acc: 87.50%,  total acc: 64.56%   [EVAL] batch:  465 | acc: 100.00%,  total acc: 64.63%   [EVAL] batch:  466 | acc: 81.25%,  total acc: 64.67%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 64.74%   [EVAL] batch:  468 | acc: 93.75%,  total acc: 64.81%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 64.88%   [EVAL] batch:  470 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:  471 | acc: 93.75%,  total acc: 65.02%   [EVAL] batch:  472 | acc: 93.75%,  total acc: 65.08%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 65.15%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 65.22%   [EVAL] batch:  475 | acc: 68.75%,  total acc: 65.23%   [EVAL] batch:  476 | acc: 81.25%,  total acc: 65.26%   [EVAL] batch:  477 | acc: 62.50%,  total acc: 65.26%   [EVAL] batch:  478 | acc: 37.50%,  total acc: 65.20%   [EVAL] batch:  479 | acc: 50.00%,  total acc: 65.17%   [EVAL] batch:  480 | acc: 50.00%,  total acc: 65.14%   [EVAL] batch:  481 | acc: 93.75%,  total acc: 65.20%   [EVAL] batch:  482 | acc: 93.75%,  total acc: 65.26%   [EVAL] batch:  483 | acc: 87.50%,  total acc: 65.30%   [EVAL] batch:  484 | acc: 100.00%,  total acc: 65.37%   [EVAL] batch:  485 | acc: 100.00%,  total acc: 65.44%   [EVAL] batch:  486 | acc: 81.25%,  total acc: 65.48%   [EVAL] batch:  487 | acc: 87.50%,  total acc: 65.52%   [EVAL] batch:  488 | acc: 43.75%,  total acc: 65.48%   [EVAL] batch:  489 | acc: 43.75%,  total acc: 65.43%   [EVAL] batch:  490 | acc: 56.25%,  total acc: 65.41%   [EVAL] batch:  491 | acc: 68.75%,  total acc: 65.42%   [EVAL] batch:  492 | acc: 56.25%,  total acc: 65.40%   [EVAL] batch:  493 | acc: 43.75%,  total acc: 65.36%   [EVAL] batch:  494 | acc: 75.00%,  total acc: 65.38%   [EVAL] batch:  495 | acc: 62.50%,  total acc: 65.37%   [EVAL] batch:  496 | acc: 43.75%,  total acc: 65.33%   [EVAL] batch:  497 | acc: 75.00%,  total acc: 65.35%   [EVAL] batch:  498 | acc: 87.50%,  total acc: 65.39%   [EVAL] batch:  499 | acc: 68.75%,  total acc: 65.40%   
cur_acc:  ['0.9494', '0.6974', '0.7421', '0.7351', '0.7460', '0.6310', '0.8363', '0.7202']
his_acc:  ['0.9494', '0.8075', '0.7580', '0.7315', '0.6853', '0.6798', '0.6821', '0.6540']
--------Round  2
seed:  300
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
CurrentTrain: epoch 15, batch     0 | loss: 25.1209245CurrentTrain: epoch 15, batch     1 | loss: 32.9610560CurrentTrain: epoch 15, batch     2 | loss: 20.2114637CurrentTrain: epoch 15, batch     3 | loss: 27.5633731CurrentTrain: epoch 15, batch     4 | loss: 23.4069844CurrentTrain: epoch 15, batch     5 | loss: 34.0799436CurrentTrain: epoch 15, batch     6 | loss: 20.9593208CurrentTrain: epoch 15, batch     7 | loss: 24.1290380CurrentTrain: epoch 15, batch     8 | loss: 22.0939327CurrentTrain: epoch 15, batch     9 | loss: 25.3625048CurrentTrain: epoch 15, batch    10 | loss: 20.2065177CurrentTrain: epoch 15, batch    11 | loss: 16.9159798CurrentTrain: epoch 15, batch    12 | loss: 23.0945546CurrentTrain: epoch 15, batch    13 | loss: 17.7388300CurrentTrain: epoch 15, batch    14 | loss: 27.5850486CurrentTrain: epoch 15, batch    15 | loss: 32.2174272CurrentTrain: epoch 15, batch    16 | loss: 20.4368891CurrentTrain: epoch 15, batch    17 | loss: 24.8788012CurrentTrain: epoch 15, batch    18 | loss: 26.9261689CurrentTrain: epoch 15, batch    19 | loss: 16.7940356CurrentTrain: epoch 15, batch    20 | loss: 18.4498697CurrentTrain: epoch 15, batch    21 | loss: 18.7166120CurrentTrain: epoch 15, batch    22 | loss: 21.3482528CurrentTrain: epoch 15, batch    23 | loss: 17.2817389CurrentTrain: epoch 15, batch    24 | loss: 18.5628626CurrentTrain: epoch 15, batch    25 | loss: 13.8297598CurrentTrain: epoch 15, batch    26 | loss: 15.8323313CurrentTrain: epoch 15, batch    27 | loss: 22.3888536CurrentTrain: epoch 15, batch    28 | loss: 18.2925410CurrentTrain: epoch 15, batch    29 | loss: 25.9703537CurrentTrain: epoch 15, batch    30 | loss: 12.8617235CurrentTrain: epoch 15, batch    31 | loss: 15.0003932CurrentTrain: epoch 15, batch    32 | loss: 16.4489257CurrentTrain: epoch 15, batch    33 | loss: 14.4571512CurrentTrain: epoch 15, batch    34 | loss: 21.4303819CurrentTrain: epoch 15, batch    35 | loss: 17.9238336CurrentTrain: epoch 15, batch    36 | loss: 15.5832615CurrentTrain: epoch 15, batch    37 | loss: 14.0664195CurrentTrain: epoch 15, batch    38 | loss: 24.8924477CurrentTrain: epoch 15, batch    39 | loss: 16.7188071CurrentTrain: epoch 15, batch    40 | loss: 14.5530509CurrentTrain: epoch 15, batch    41 | loss: 11.4532901CurrentTrain: epoch 15, batch    42 | loss: 14.8087400CurrentTrain: epoch 15, batch    43 | loss: 16.9321577CurrentTrain: epoch 15, batch    44 | loss: 18.1639497CurrentTrain: epoch 15, batch    45 | loss: 14.4771063CurrentTrain: epoch 15, batch    46 | loss: 26.5629051CurrentTrain: epoch 15, batch    47 | loss: 20.8064987CurrentTrain: epoch 15, batch    48 | loss: 14.2743919CurrentTrain: epoch 15, batch    49 | loss: 15.8386839CurrentTrain: epoch 15, batch    50 | loss: 21.9688664CurrentTrain: epoch 15, batch    51 | loss: 18.4194317CurrentTrain: epoch 15, batch    52 | loss: 24.5344871CurrentTrain: epoch 15, batch    53 | loss: 12.4581040CurrentTrain: epoch 15, batch    54 | loss: 15.8043975CurrentTrain: epoch 15, batch    55 | loss: 15.9209851CurrentTrain: epoch 15, batch    56 | loss: 16.0871348CurrentTrain: epoch 15, batch    57 | loss: 32.5571542CurrentTrain: epoch 15, batch    58 | loss: 14.8962368CurrentTrain: epoch 15, batch    59 | loss: 18.5079411CurrentTrain: epoch 15, batch    60 | loss: 16.5979738CurrentTrain: epoch 15, batch    61 | loss: 20.4956986CurrentTrain: epoch  7, batch    62 | loss: 17.4775184CurrentTrain: epoch 15, batch     0 | loss: 14.3322882CurrentTrain: epoch 15, batch     1 | loss: 16.0019772CurrentTrain: epoch 15, batch     2 | loss: 13.6723302CurrentTrain: epoch 15, batch     3 | loss: 13.3806746CurrentTrain: epoch 15, batch     4 | loss: 19.1587403CurrentTrain: epoch 15, batch     5 | loss: 20.9385349CurrentTrain: epoch 15, batch     6 | loss: 13.8440390CurrentTrain: epoch 15, batch     7 | loss: 16.8296870CurrentTrain: epoch 15, batch     8 | loss: 13.2507038CurrentTrain: epoch 15, batch     9 | loss: 17.0561914CurrentTrain: epoch 15, batch    10 | loss: 19.7684159CurrentTrain: epoch 15, batch    11 | loss: 16.7677194CurrentTrain: epoch 15, batch    12 | loss: 12.4881864CurrentTrain: epoch 15, batch    13 | loss: 18.8256762CurrentTrain: epoch 15, batch    14 | loss: 14.3560086CurrentTrain: epoch 15, batch    15 | loss: 15.0354586CurrentTrain: epoch 15, batch    16 | loss: 12.8631974CurrentTrain: epoch 15, batch    17 | loss: 13.9780731CurrentTrain: epoch 15, batch    18 | loss: 19.4035467CurrentTrain: epoch 15, batch    19 | loss: 13.7328913CurrentTrain: epoch 15, batch    20 | loss: 27.9251027CurrentTrain: epoch 15, batch    21 | loss: 17.4179102CurrentTrain: epoch 15, batch    22 | loss: 12.6496185CurrentTrain: epoch 15, batch    23 | loss: 13.8337302CurrentTrain: epoch 15, batch    24 | loss: 19.9087760CurrentTrain: epoch 15, batch    25 | loss: 12.9594411CurrentTrain: epoch 15, batch    26 | loss: 26.4806288CurrentTrain: epoch 15, batch    27 | loss: 16.1673217CurrentTrain: epoch 15, batch    28 | loss: 14.1280624CurrentTrain: epoch 15, batch    29 | loss: 14.9769253CurrentTrain: epoch 15, batch    30 | loss: 10.6186512CurrentTrain: epoch 15, batch    31 | loss: 14.1354654CurrentTrain: epoch 15, batch    32 | loss: 10.5278207CurrentTrain: epoch 15, batch    33 | loss: 14.3481538CurrentTrain: epoch 15, batch    34 | loss: 20.4187770CurrentTrain: epoch 15, batch    35 | loss: 16.6861848CurrentTrain: epoch 15, batch    36 | loss: 10.4514194CurrentTrain: epoch 15, batch    37 | loss: 15.2837434CurrentTrain: epoch 15, batch    38 | loss: 11.8717658CurrentTrain: epoch 15, batch    39 | loss: 19.4251441CurrentTrain: epoch 15, batch    40 | loss: 10.4112635CurrentTrain: epoch 15, batch    41 | loss: 15.1857134CurrentTrain: epoch 15, batch    42 | loss: 12.8499774CurrentTrain: epoch 15, batch    43 | loss: 25.3474714CurrentTrain: epoch 15, batch    44 | loss: 16.2794044CurrentTrain: epoch 15, batch    45 | loss: 15.4830671CurrentTrain: epoch 15, batch    46 | loss: 15.6270693CurrentTrain: epoch 15, batch    47 | loss: 10.1904870CurrentTrain: epoch 15, batch    48 | loss: 17.4758729CurrentTrain: epoch 15, batch    49 | loss: 14.1067030CurrentTrain: epoch 15, batch    50 | loss: 19.7825829CurrentTrain: epoch 15, batch    51 | loss: 13.8399982CurrentTrain: epoch 15, batch    52 | loss: 25.4257145CurrentTrain: epoch 15, batch    53 | loss: 16.1611019CurrentTrain: epoch 15, batch    54 | loss: 10.2009346CurrentTrain: epoch 15, batch    55 | loss: 16.4587956CurrentTrain: epoch 15, batch    56 | loss: 13.1476505CurrentTrain: epoch 15, batch    57 | loss: 24.0741713CurrentTrain: epoch 15, batch    58 | loss: 14.6671068CurrentTrain: epoch 15, batch    59 | loss: 10.8170076CurrentTrain: epoch 15, batch    60 | loss: 13.0653377CurrentTrain: epoch 15, batch    61 | loss: 15.5668912CurrentTrain: epoch  7, batch    62 | loss: 17.9310810CurrentTrain: epoch 15, batch     0 | loss: 12.0261450CurrentTrain: epoch 15, batch     1 | loss: 13.2589926CurrentTrain: epoch 15, batch     2 | loss: 20.4593767CurrentTrain: epoch 15, batch     3 | loss: 14.7822132CurrentTrain: epoch 15, batch     4 | loss: 13.9176193CurrentTrain: epoch 15, batch     5 | loss: 16.3733035CurrentTrain: epoch 15, batch     6 | loss: 11.8977149CurrentTrain: epoch 15, batch     7 | loss: 11.5551629CurrentTrain: epoch 15, batch     8 | loss: 33.9700341CurrentTrain: epoch 15, batch     9 | loss: 14.7672268CurrentTrain: epoch 15, batch    10 | loss: 11.3334561CurrentTrain: epoch 15, batch    11 | loss: 17.9692135CurrentTrain: epoch 15, batch    12 | loss: 13.8184701CurrentTrain: epoch 15, batch    13 | loss: 15.2888530CurrentTrain: epoch 15, batch    14 | loss: 15.1674345CurrentTrain: epoch 15, batch    15 | loss: 11.0100269CurrentTrain: epoch 15, batch    16 | loss: 10.7105972CurrentTrain: epoch 15, batch    17 | loss: 11.0067431CurrentTrain: epoch 15, batch    18 | loss: 17.4275375CurrentTrain: epoch 15, batch    19 | loss: 14.5291785CurrentTrain: epoch 15, batch    20 | loss: 8.8974895CurrentTrain: epoch 15, batch    21 | loss: 20.0759470CurrentTrain: epoch 15, batch    22 | loss: 29.3204583CurrentTrain: epoch 15, batch    23 | loss: 15.3805721CurrentTrain: epoch 15, batch    24 | loss: 9.0102724CurrentTrain: epoch 15, batch    25 | loss: 11.6590821CurrentTrain: epoch 15, batch    26 | loss: 17.1031571CurrentTrain: epoch 15, batch    27 | loss: 12.6699144CurrentTrain: epoch 15, batch    28 | loss: 15.5870489CurrentTrain: epoch 15, batch    29 | loss: 17.2212785CurrentTrain: epoch 15, batch    30 | loss: 10.3631477CurrentTrain: epoch 15, batch    31 | loss: 19.6586668CurrentTrain: epoch 15, batch    32 | loss: 11.5655458CurrentTrain: epoch 15, batch    33 | loss: 14.1419293CurrentTrain: epoch 15, batch    34 | loss: 11.6909456CurrentTrain: epoch 15, batch    35 | loss: 11.4830361CurrentTrain: epoch 15, batch    36 | loss: 14.8777644CurrentTrain: epoch 15, batch    37 | loss: 11.4846564CurrentTrain: epoch 15, batch    38 | loss: 16.1992200CurrentTrain: epoch 15, batch    39 | loss: 9.7787577CurrentTrain: epoch 15, batch    40 | loss: 14.2457887CurrentTrain: epoch 15, batch    41 | loss: 18.5988793CurrentTrain: epoch 15, batch    42 | loss: 13.5209661CurrentTrain: epoch 15, batch    43 | loss: 21.8622465CurrentTrain: epoch 15, batch    44 | loss: 11.5278590CurrentTrain: epoch 15, batch    45 | loss: 20.1348831CurrentTrain: epoch 15, batch    46 | loss: 22.2902772CurrentTrain: epoch 15, batch    47 | loss: 16.0324871CurrentTrain: epoch 15, batch    48 | loss: 10.0461393CurrentTrain: epoch 15, batch    49 | loss: 12.5081338CurrentTrain: epoch 15, batch    50 | loss: 23.4535404CurrentTrain: epoch 15, batch    51 | loss: 11.0565562CurrentTrain: epoch 15, batch    52 | loss: 29.5475664CurrentTrain: epoch 15, batch    53 | loss: 17.5548464CurrentTrain: epoch 15, batch    54 | loss: 11.8454824CurrentTrain: epoch 15, batch    55 | loss: 11.3947098CurrentTrain: epoch 15, batch    56 | loss: 17.3984758CurrentTrain: epoch 15, batch    57 | loss: 11.1411788CurrentTrain: epoch 15, batch    58 | loss: 9.7233420CurrentTrain: epoch 15, batch    59 | loss: 10.8245960CurrentTrain: epoch 15, batch    60 | loss: 20.6438177CurrentTrain: epoch 15, batch    61 | loss: 15.5957612CurrentTrain: epoch  7, batch    62 | loss: 15.9806544CurrentTrain: epoch 15, batch     0 | loss: 9.3674443CurrentTrain: epoch 15, batch     1 | loss: 10.8215351CurrentTrain: epoch 15, batch     2 | loss: 13.8524005CurrentTrain: epoch 15, batch     3 | loss: 10.1786755CurrentTrain: epoch 15, batch     4 | loss: 20.3461484CurrentTrain: epoch 15, batch     5 | loss: 11.7787943CurrentTrain: epoch 15, batch     6 | loss: 8.5430631CurrentTrain: epoch 15, batch     7 | loss: 11.9005596CurrentTrain: epoch 15, batch     8 | loss: 8.5476039CurrentTrain: epoch 15, batch     9 | loss: 9.6371139CurrentTrain: epoch 15, batch    10 | loss: 11.2760934CurrentTrain: epoch 15, batch    11 | loss: 14.0064712CurrentTrain: epoch 15, batch    12 | loss: 17.3493663CurrentTrain: epoch 15, batch    13 | loss: 18.3605889CurrentTrain: epoch 15, batch    14 | loss: 10.5535279CurrentTrain: epoch 15, batch    15 | loss: 9.8890226CurrentTrain: epoch 15, batch    16 | loss: 16.4973075CurrentTrain: epoch 15, batch    17 | loss: 11.4794965CurrentTrain: epoch 15, batch    18 | loss: 13.7514379CurrentTrain: epoch 15, batch    19 | loss: 17.0654374CurrentTrain: epoch 15, batch    20 | loss: 12.8290523CurrentTrain: epoch 15, batch    21 | loss: 10.2758994CurrentTrain: epoch 15, batch    22 | loss: 23.5295472CurrentTrain: epoch 15, batch    23 | loss: 16.5287460CurrentTrain: epoch 15, batch    24 | loss: 17.8941392CurrentTrain: epoch 15, batch    25 | loss: 10.2162619CurrentTrain: epoch 15, batch    26 | loss: 21.1882903CurrentTrain: epoch 15, batch    27 | loss: 15.2256959CurrentTrain: epoch 15, batch    28 | loss: 16.8731296CurrentTrain: epoch 15, batch    29 | loss: 11.3055982CurrentTrain: epoch 15, batch    30 | loss: 11.5045511CurrentTrain: epoch 15, batch    31 | loss: 13.3116713CurrentTrain: epoch 15, batch    32 | loss: 15.0251041CurrentTrain: epoch 15, batch    33 | loss: 19.7179073CurrentTrain: epoch 15, batch    34 | loss: 9.4722749CurrentTrain: epoch 15, batch    35 | loss: 14.3650104CurrentTrain: epoch 15, batch    36 | loss: 13.9139245CurrentTrain: epoch 15, batch    37 | loss: 7.4275952CurrentTrain: epoch 15, batch    38 | loss: 12.5307179CurrentTrain: epoch 15, batch    39 | loss: 9.4729850CurrentTrain: epoch 15, batch    40 | loss: 9.6302405CurrentTrain: epoch 15, batch    41 | loss: 21.6159141CurrentTrain: epoch 15, batch    42 | loss: 8.1788964CurrentTrain: epoch 15, batch    43 | loss: 14.5002882CurrentTrain: epoch 15, batch    44 | loss: 13.0100355CurrentTrain: epoch 15, batch    45 | loss: 8.4322064CurrentTrain: epoch 15, batch    46 | loss: 10.8571601CurrentTrain: epoch 15, batch    47 | loss: 11.1793016CurrentTrain: epoch 15, batch    48 | loss: 12.0203007CurrentTrain: epoch 15, batch    49 | loss: 10.6922832CurrentTrain: epoch 15, batch    50 | loss: 9.4287663CurrentTrain: epoch 15, batch    51 | loss: 22.4604199CurrentTrain: epoch 15, batch    52 | loss: 9.3831373CurrentTrain: epoch 15, batch    53 | loss: 8.9540287CurrentTrain: epoch 15, batch    54 | loss: 15.9013337CurrentTrain: epoch 15, batch    55 | loss: 16.9847926CurrentTrain: epoch 15, batch    56 | loss: 16.2732747CurrentTrain: epoch 15, batch    57 | loss: 7.1691714CurrentTrain: epoch 15, batch    58 | loss: 10.1970775CurrentTrain: epoch 15, batch    59 | loss: 15.2025084CurrentTrain: epoch 15, batch    60 | loss: 11.9159226CurrentTrain: epoch 15, batch    61 | loss: 11.0292538CurrentTrain: epoch  7, batch    62 | loss: 10.2791169CurrentTrain: epoch 15, batch     0 | loss: 9.0900385CurrentTrain: epoch 15, batch     1 | loss: 14.7771964CurrentTrain: epoch 15, batch     2 | loss: 13.1570731CurrentTrain: epoch 15, batch     3 | loss: 15.5396746CurrentTrain: epoch 15, batch     4 | loss: 10.2734512CurrentTrain: epoch 15, batch     5 | loss: 9.3899511CurrentTrain: epoch 15, batch     6 | loss: 13.6177397CurrentTrain: epoch 15, batch     7 | loss: 13.8118278CurrentTrain: epoch 15, batch     8 | loss: 14.8702804CurrentTrain: epoch 15, batch     9 | loss: 10.1919278CurrentTrain: epoch 15, batch    10 | loss: 9.3105055CurrentTrain: epoch 15, batch    11 | loss: 8.8223908CurrentTrain: epoch 15, batch    12 | loss: 10.2734473CurrentTrain: epoch 15, batch    13 | loss: 14.0891415CurrentTrain: epoch 15, batch    14 | loss: 10.2050477CurrentTrain: epoch 15, batch    15 | loss: 15.7611616CurrentTrain: epoch 15, batch    16 | loss: 9.8466855CurrentTrain: epoch 15, batch    17 | loss: 8.9710473CurrentTrain: epoch 15, batch    18 | loss: 25.4636054CurrentTrain: epoch 15, batch    19 | loss: 15.7511299CurrentTrain: epoch 15, batch    20 | loss: 8.9278621CurrentTrain: epoch 15, batch    21 | loss: 9.3203674CurrentTrain: epoch 15, batch    22 | loss: 11.9869006CurrentTrain: epoch 15, batch    23 | loss: 9.6643427CurrentTrain: epoch 15, batch    24 | loss: 10.8776331CurrentTrain: epoch 15, batch    25 | loss: 11.8734439CurrentTrain: epoch 15, batch    26 | loss: 10.4373226CurrentTrain: epoch 15, batch    27 | loss: 15.8760341CurrentTrain: epoch 15, batch    28 | loss: 10.1940460CurrentTrain: epoch 15, batch    29 | loss: 10.5100540CurrentTrain: epoch 15, batch    30 | loss: 6.4488131CurrentTrain: epoch 15, batch    31 | loss: 12.0676924CurrentTrain: epoch 15, batch    32 | loss: 24.2550512CurrentTrain: epoch 15, batch    33 | loss: 22.3531038CurrentTrain: epoch 15, batch    34 | loss: 11.8418025CurrentTrain: epoch 15, batch    35 | loss: 9.2050487CurrentTrain: epoch 15, batch    36 | loss: 12.9519038CurrentTrain: epoch 15, batch    37 | loss: 11.8633931CurrentTrain: epoch 15, batch    38 | loss: 18.2300666CurrentTrain: epoch 15, batch    39 | loss: 17.2893888CurrentTrain: epoch 15, batch    40 | loss: 14.3756223CurrentTrain: epoch 15, batch    41 | loss: 8.7650525CurrentTrain: epoch 15, batch    42 | loss: 15.3486928CurrentTrain: epoch 15, batch    43 | loss: 10.2722656CurrentTrain: epoch 15, batch    44 | loss: 9.1873496CurrentTrain: epoch 15, batch    45 | loss: 10.9020877CurrentTrain: epoch 15, batch    46 | loss: 9.4933048CurrentTrain: epoch 15, batch    47 | loss: 24.5665531CurrentTrain: epoch 15, batch    48 | loss: 25.2608750CurrentTrain: epoch 15, batch    49 | loss: 11.2155139CurrentTrain: epoch 15, batch    50 | loss: 10.1007840CurrentTrain: epoch 15, batch    51 | loss: 8.9855793CurrentTrain: epoch 15, batch    52 | loss: 9.4160599CurrentTrain: epoch 15, batch    53 | loss: 10.5246292CurrentTrain: epoch 15, batch    54 | loss: 14.6842201CurrentTrain: epoch 15, batch    55 | loss: 10.6566003CurrentTrain: epoch 15, batch    56 | loss: 11.4302920CurrentTrain: epoch 15, batch    57 | loss: 17.7995981CurrentTrain: epoch 15, batch    58 | loss: 11.8040336CurrentTrain: epoch 15, batch    59 | loss: 13.4705757CurrentTrain: epoch 15, batch    60 | loss: 16.4569232CurrentTrain: epoch 15, batch    61 | loss: 11.7610354CurrentTrain: epoch  7, batch    62 | loss: 13.4934842CurrentTrain: epoch 15, batch     0 | loss: 10.0969925CurrentTrain: epoch 15, batch     1 | loss: 19.1279009CurrentTrain: epoch 15, batch     2 | loss: 10.4642611CurrentTrain: epoch 15, batch     3 | loss: 10.5095409CurrentTrain: epoch 15, batch     4 | loss: 12.4234695CurrentTrain: epoch 15, batch     5 | loss: 8.5996357CurrentTrain: epoch 15, batch     6 | loss: 11.1730781CurrentTrain: epoch 15, batch     7 | loss: 12.1026744CurrentTrain: epoch 15, batch     8 | loss: 8.3564962CurrentTrain: epoch 15, batch     9 | loss: 9.5670607CurrentTrain: epoch 15, batch    10 | loss: 10.5023191CurrentTrain: epoch 15, batch    11 | loss: 11.6858604CurrentTrain: epoch 15, batch    12 | loss: 15.4604290CurrentTrain: epoch 15, batch    13 | loss: 9.7924552CurrentTrain: epoch 15, batch    14 | loss: 8.6057560CurrentTrain: epoch 15, batch    15 | loss: 25.2935641CurrentTrain: epoch 15, batch    16 | loss: 11.9568216CurrentTrain: epoch 15, batch    17 | loss: 12.7791977CurrentTrain: epoch 15, batch    18 | loss: 11.5055673CurrentTrain: epoch 15, batch    19 | loss: 16.0684282CurrentTrain: epoch 15, batch    20 | loss: 11.3130161CurrentTrain: epoch 15, batch    21 | loss: 20.7118444CurrentTrain: epoch 15, batch    22 | loss: 10.1181158CurrentTrain: epoch 15, batch    23 | loss: 10.1429782CurrentTrain: epoch 15, batch    24 | loss: 8.7977369CurrentTrain: epoch 15, batch    25 | loss: 17.2619985CurrentTrain: epoch 15, batch    26 | loss: 8.8035195CurrentTrain: epoch 15, batch    27 | loss: 9.2663433CurrentTrain: epoch 15, batch    28 | loss: 9.0273521CurrentTrain: epoch 15, batch    29 | loss: 10.7095743CurrentTrain: epoch 15, batch    30 | loss: 21.7719900CurrentTrain: epoch 15, batch    31 | loss: 11.2693818CurrentTrain: epoch 15, batch    32 | loss: 11.9709449CurrentTrain: epoch 15, batch    33 | loss: 9.7060366CurrentTrain: epoch 15, batch    34 | loss: 10.0280994CurrentTrain: epoch 15, batch    35 | loss: 10.1885643CurrentTrain: epoch 15, batch    36 | loss: 13.8747594CurrentTrain: epoch 15, batch    37 | loss: 19.1956040CurrentTrain: epoch 15, batch    38 | loss: 14.5832750CurrentTrain: epoch 15, batch    39 | loss: 8.0156594CurrentTrain: epoch 15, batch    40 | loss: 10.7691770CurrentTrain: epoch 15, batch    41 | loss: 8.2191609CurrentTrain: epoch 15, batch    42 | loss: 13.9653357CurrentTrain: epoch 15, batch    43 | loss: 15.1621257CurrentTrain: epoch 15, batch    44 | loss: 15.1569097CurrentTrain: epoch 15, batch    45 | loss: 8.0370763CurrentTrain: epoch 15, batch    46 | loss: 16.3333245CurrentTrain: epoch 15, batch    47 | loss: 8.8334018CurrentTrain: epoch 15, batch    48 | loss: 6.4670893CurrentTrain: epoch 15, batch    49 | loss: 9.7967861CurrentTrain: epoch 15, batch    50 | loss: 10.4222994CurrentTrain: epoch 15, batch    51 | loss: 17.0669657CurrentTrain: epoch 15, batch    52 | loss: 12.1119291CurrentTrain: epoch 15, batch    53 | loss: 15.5052064CurrentTrain: epoch 15, batch    54 | loss: 10.3514892CurrentTrain: epoch 15, batch    55 | loss: 10.8448733CurrentTrain: epoch 15, batch    56 | loss: 10.7284328CurrentTrain: epoch 15, batch    57 | loss: 14.2238367CurrentTrain: epoch 15, batch    58 | loss: 10.5754428CurrentTrain: epoch 15, batch    59 | loss: 9.7220498CurrentTrain: epoch 15, batch    60 | loss: 14.3419097CurrentTrain: epoch 15, batch    61 | loss: 14.2454140CurrentTrain: epoch  7, batch    62 | loss: 12.3408763CurrentTrain: epoch 15, batch     0 | loss: 9.0378616CurrentTrain: epoch 15, batch     1 | loss: 12.2358462CurrentTrain: epoch 15, batch     2 | loss: 10.7222165CurrentTrain: epoch 15, batch     3 | loss: 13.4285048CurrentTrain: epoch 15, batch     4 | loss: 15.6371890CurrentTrain: epoch 15, batch     5 | loss: 6.8418942CurrentTrain: epoch 15, batch     6 | loss: 17.7391382CurrentTrain: epoch 15, batch     7 | loss: 13.6802945CurrentTrain: epoch 15, batch     8 | loss: 8.9776624CurrentTrain: epoch 15, batch     9 | loss: 15.3889231CurrentTrain: epoch 15, batch    10 | loss: 19.5277409CurrentTrain: epoch 15, batch    11 | loss: 12.8077251CurrentTrain: epoch 15, batch    12 | loss: 7.7110307CurrentTrain: epoch 15, batch    13 | loss: 16.1942907CurrentTrain: epoch 15, batch    14 | loss: 11.7502124CurrentTrain: epoch 15, batch    15 | loss: 10.7302111CurrentTrain: epoch 15, batch    16 | loss: 15.9288823CurrentTrain: epoch 15, batch    17 | loss: 8.6170473CurrentTrain: epoch 15, batch    18 | loss: 9.0105105CurrentTrain: epoch 15, batch    19 | loss: 10.1071285CurrentTrain: epoch 15, batch    20 | loss: 10.2187104CurrentTrain: epoch 15, batch    21 | loss: 20.2355804CurrentTrain: epoch 15, batch    22 | loss: 10.2586596CurrentTrain: epoch 15, batch    23 | loss: 14.8405159CurrentTrain: epoch 15, batch    24 | loss: 14.0336865CurrentTrain: epoch 15, batch    25 | loss: 11.4605459CurrentTrain: epoch 15, batch    26 | loss: 10.7606789CurrentTrain: epoch 15, batch    27 | loss: 12.5016028CurrentTrain: epoch 15, batch    28 | loss: 7.8764666CurrentTrain: epoch 15, batch    29 | loss: 9.9666784CurrentTrain: epoch 15, batch    30 | loss: 9.4787640CurrentTrain: epoch 15, batch    31 | loss: 10.2789480CurrentTrain: epoch 15, batch    32 | loss: 21.0144081CurrentTrain: epoch 15, batch    33 | loss: 9.2298314CurrentTrain: epoch 15, batch    34 | loss: 9.9484341CurrentTrain: epoch 15, batch    35 | loss: 10.2126397CurrentTrain: epoch 15, batch    36 | loss: 10.0423574CurrentTrain: epoch 15, batch    37 | loss: 8.7513018CurrentTrain: epoch 15, batch    38 | loss: 7.1451245CurrentTrain: epoch 15, batch    39 | loss: 17.8753890CurrentTrain: epoch 15, batch    40 | loss: 8.1783808CurrentTrain: epoch 15, batch    41 | loss: 12.7084661CurrentTrain: epoch 15, batch    42 | loss: 7.6434536CurrentTrain: epoch 15, batch    43 | loss: 20.9094868CurrentTrain: epoch 15, batch    44 | loss: 11.6291168CurrentTrain: epoch 15, batch    45 | loss: 7.9898432CurrentTrain: epoch 15, batch    46 | loss: 20.3458303CurrentTrain: epoch 15, batch    47 | loss: 11.0236718CurrentTrain: epoch 15, batch    48 | loss: 8.6180826CurrentTrain: epoch 15, batch    49 | loss: 13.8765064CurrentTrain: epoch 15, batch    50 | loss: 15.5082427CurrentTrain: epoch 15, batch    51 | loss: 10.0912202CurrentTrain: epoch 15, batch    52 | loss: 9.3295415CurrentTrain: epoch 15, batch    53 | loss: 12.8671006CurrentTrain: epoch 15, batch    54 | loss: 9.5540505CurrentTrain: epoch 15, batch    55 | loss: 14.2025510CurrentTrain: epoch 15, batch    56 | loss: 22.3335748CurrentTrain: epoch 15, batch    57 | loss: 7.0241975CurrentTrain: epoch 15, batch    58 | loss: 9.0812363CurrentTrain: epoch 15, batch    59 | loss: 12.5582688CurrentTrain: epoch 15, batch    60 | loss: 10.2936582CurrentTrain: epoch 15, batch    61 | loss: 8.4214455CurrentTrain: epoch  7, batch    62 | loss: 15.1504388CurrentTrain: epoch 15, batch     0 | loss: 12.9490640CurrentTrain: epoch 15, batch     1 | loss: 9.1506685CurrentTrain: epoch 15, batch     2 | loss: 15.8796856CurrentTrain: epoch 15, batch     3 | loss: 13.5661928CurrentTrain: epoch 15, batch     4 | loss: 10.3670526CurrentTrain: epoch 15, batch     5 | loss: 11.6828154CurrentTrain: epoch 15, batch     6 | loss: 9.0954578CurrentTrain: epoch 15, batch     7 | loss: 9.9745857CurrentTrain: epoch 15, batch     8 | loss: 10.9410695CurrentTrain: epoch 15, batch     9 | loss: 8.3852465CurrentTrain: epoch 15, batch    10 | loss: 9.5981294CurrentTrain: epoch 15, batch    11 | loss: 12.9114826CurrentTrain: epoch 15, batch    12 | loss: 12.9813667CurrentTrain: epoch 15, batch    13 | loss: 11.3948669CurrentTrain: epoch 15, batch    14 | loss: 12.9989178CurrentTrain: epoch 15, batch    15 | loss: 8.4418625CurrentTrain: epoch 15, batch    16 | loss: 10.9457674CurrentTrain: epoch 15, batch    17 | loss: 9.8407302CurrentTrain: epoch 15, batch    18 | loss: 9.2505700CurrentTrain: epoch 15, batch    19 | loss: 21.1565976CurrentTrain: epoch 15, batch    20 | loss: 24.7740563CurrentTrain: epoch 15, batch    21 | loss: 10.9627120CurrentTrain: epoch 15, batch    22 | loss: 8.3649315CurrentTrain: epoch 15, batch    23 | loss: 15.3159330CurrentTrain: epoch 15, batch    24 | loss: 8.7406508CurrentTrain: epoch 15, batch    25 | loss: 10.0003610CurrentTrain: epoch 15, batch    26 | loss: 13.1765700CurrentTrain: epoch 15, batch    27 | loss: 12.5056472CurrentTrain: epoch 15, batch    28 | loss: 8.5604068CurrentTrain: epoch 15, batch    29 | loss: 9.1764074CurrentTrain: epoch 15, batch    30 | loss: 10.5509545CurrentTrain: epoch 15, batch    31 | loss: 8.9498684CurrentTrain: epoch 15, batch    32 | loss: 12.5105263CurrentTrain: epoch 15, batch    33 | loss: 12.9778776CurrentTrain: epoch 15, batch    34 | loss: 12.5579136CurrentTrain: epoch 15, batch    35 | loss: 10.2121739CurrentTrain: epoch 15, batch    36 | loss: 15.0574132CurrentTrain: epoch 15, batch    37 | loss: 12.5255143CurrentTrain: epoch 15, batch    38 | loss: 11.7348105CurrentTrain: epoch 15, batch    39 | loss: 8.2808329CurrentTrain: epoch 15, batch    40 | loss: 8.5364021CurrentTrain: epoch 15, batch    41 | loss: 10.7785482CurrentTrain: epoch 15, batch    42 | loss: 10.5927033CurrentTrain: epoch 15, batch    43 | loss: 10.5950031CurrentTrain: epoch 15, batch    44 | loss: 13.9269901CurrentTrain: epoch 15, batch    45 | loss: 8.0819862CurrentTrain: epoch 15, batch    46 | loss: 7.7669601CurrentTrain: epoch 15, batch    47 | loss: 9.7559660CurrentTrain: epoch 15, batch    48 | loss: 9.8675762CurrentTrain: epoch 15, batch    49 | loss: 14.1866610CurrentTrain: epoch 15, batch    50 | loss: 6.7432310CurrentTrain: epoch 15, batch    51 | loss: 8.9851571CurrentTrain: epoch 15, batch    52 | loss: 6.6953483CurrentTrain: epoch 15, batch    53 | loss: 18.0541512CurrentTrain: epoch 15, batch    54 | loss: 12.6414163CurrentTrain: epoch 15, batch    55 | loss: 15.4701965CurrentTrain: epoch 15, batch    56 | loss: 10.0598508CurrentTrain: epoch 15, batch    57 | loss: 14.0989235CurrentTrain: epoch 15, batch    58 | loss: 10.2275186CurrentTrain: epoch 15, batch    59 | loss: 10.5307931CurrentTrain: epoch 15, batch    60 | loss: 8.1300731CurrentTrain: epoch 15, batch    61 | loss: 9.2628243CurrentTrain: epoch  7, batch    62 | loss: 7.5911306CurrentTrain: epoch 15, batch     0 | loss: 17.2536947CurrentTrain: epoch 15, batch     1 | loss: 11.4706193CurrentTrain: epoch 15, batch     2 | loss: 14.3581784CurrentTrain: epoch 15, batch     3 | loss: 11.6274854CurrentTrain: epoch 15, batch     4 | loss: 9.4669407CurrentTrain: epoch 15, batch     5 | loss: 8.0938795CurrentTrain: epoch 15, batch     6 | loss: 11.4120670CurrentTrain: epoch 15, batch     7 | loss: 11.9571265CurrentTrain: epoch 15, batch     8 | loss: 9.2791752CurrentTrain: epoch 15, batch     9 | loss: 13.0910299CurrentTrain: epoch 15, batch    10 | loss: 6.8254451CurrentTrain: epoch 15, batch    11 | loss: 7.5450002CurrentTrain: epoch 15, batch    12 | loss: 8.5248436CurrentTrain: epoch 15, batch    13 | loss: 8.5374880CurrentTrain: epoch 15, batch    14 | loss: 11.7067602CurrentTrain: epoch 15, batch    15 | loss: 13.4907550CurrentTrain: epoch 15, batch    16 | loss: 8.4948155CurrentTrain: epoch 15, batch    17 | loss: 12.7343667CurrentTrain: epoch 15, batch    18 | loss: 10.1131555CurrentTrain: epoch 15, batch    19 | loss: 7.3955269CurrentTrain: epoch 15, batch    20 | loss: 7.7304428CurrentTrain: epoch 15, batch    21 | loss: 7.2994924CurrentTrain: epoch 15, batch    22 | loss: 8.5297940CurrentTrain: epoch 15, batch    23 | loss: 9.3463813CurrentTrain: epoch 15, batch    24 | loss: 19.2634542CurrentTrain: epoch 15, batch    25 | loss: 7.4349826CurrentTrain: epoch 15, batch    26 | loss: 11.8088910CurrentTrain: epoch 15, batch    27 | loss: 8.9724381CurrentTrain: epoch 15, batch    28 | loss: 8.8324382CurrentTrain: epoch 15, batch    29 | loss: 13.5623425CurrentTrain: epoch 15, batch    30 | loss: 30.7311830CurrentTrain: epoch 15, batch    31 | loss: 15.7375422CurrentTrain: epoch 15, batch    32 | loss: 15.1001346CurrentTrain: epoch 15, batch    33 | loss: 11.8772315CurrentTrain: epoch 15, batch    34 | loss: 12.4582700CurrentTrain: epoch 15, batch    35 | loss: 7.9065321CurrentTrain: epoch 15, batch    36 | loss: 8.5588696CurrentTrain: epoch 15, batch    37 | loss: 13.5091591CurrentTrain: epoch 15, batch    38 | loss: 12.4538626CurrentTrain: epoch 15, batch    39 | loss: 10.9128265CurrentTrain: epoch 15, batch    40 | loss: 11.4786709CurrentTrain: epoch 15, batch    41 | loss: 11.6458621CurrentTrain: epoch 15, batch    42 | loss: 11.1956534CurrentTrain: epoch 15, batch    43 | loss: 8.9055876CurrentTrain: epoch 15, batch    44 | loss: 14.6279815CurrentTrain: epoch 15, batch    45 | loss: 6.9536414CurrentTrain: epoch 15, batch    46 | loss: 9.8815187CurrentTrain: epoch 15, batch    47 | loss: 9.3278815CurrentTrain: epoch 15, batch    48 | loss: 9.9432696CurrentTrain: epoch 15, batch    49 | loss: 8.4965563CurrentTrain: epoch 15, batch    50 | loss: 9.3788203CurrentTrain: epoch 15, batch    51 | loss: 9.0905188CurrentTrain: epoch 15, batch    52 | loss: 9.5310502CurrentTrain: epoch 15, batch    53 | loss: 8.9010849CurrentTrain: epoch 15, batch    54 | loss: 7.9712449CurrentTrain: epoch 15, batch    55 | loss: 14.2294261CurrentTrain: epoch 15, batch    56 | loss: 6.3166355CurrentTrain: epoch 15, batch    57 | loss: 24.1370923CurrentTrain: epoch 15, batch    58 | loss: 10.0214057CurrentTrain: epoch 15, batch    59 | loss: 9.7991553CurrentTrain: epoch 15, batch    60 | loss: 13.6314654CurrentTrain: epoch 15, batch    61 | loss: 7.8450173CurrentTrain: epoch  7, batch    62 | loss: 20.4692888CurrentTrain: epoch 15, batch     0 | loss: 10.7048985CurrentTrain: epoch 15, batch     1 | loss: 12.4539940CurrentTrain: epoch 15, batch     2 | loss: 7.2080344CurrentTrain: epoch 15, batch     3 | loss: 8.8462615CurrentTrain: epoch 15, batch     4 | loss: 13.9516924CurrentTrain: epoch 15, batch     5 | loss: 10.3622250CurrentTrain: epoch 15, batch     6 | loss: 16.2324521CurrentTrain: epoch 15, batch     7 | loss: 9.3722700CurrentTrain: epoch 15, batch     8 | loss: 20.0302250CurrentTrain: epoch 15, batch     9 | loss: 8.5632379CurrentTrain: epoch 15, batch    10 | loss: 10.8563425CurrentTrain: epoch 15, batch    11 | loss: 8.4348471CurrentTrain: epoch 15, batch    12 | loss: 20.1007020CurrentTrain: epoch 15, batch    13 | loss: 8.2177877CurrentTrain: epoch 15, batch    14 | loss: 8.7797455CurrentTrain: epoch 15, batch    15 | loss: 8.2945198CurrentTrain: epoch 15, batch    16 | loss: 7.9609827CurrentTrain: epoch 15, batch    17 | loss: 7.6912470CurrentTrain: epoch 15, batch    18 | loss: 13.8069959CurrentTrain: epoch 15, batch    19 | loss: 11.0264438CurrentTrain: epoch 15, batch    20 | loss: 7.3355064CurrentTrain: epoch 15, batch    21 | loss: 9.8994655CurrentTrain: epoch 15, batch    22 | loss: 9.9645962CurrentTrain: epoch 15, batch    23 | loss: 8.6850317CurrentTrain: epoch 15, batch    24 | loss: 19.6781211CurrentTrain: epoch 15, batch    25 | loss: 11.7834621CurrentTrain: epoch 15, batch    26 | loss: 13.7419321CurrentTrain: epoch 15, batch    27 | loss: 12.8331337CurrentTrain: epoch 15, batch    28 | loss: 9.8219636CurrentTrain: epoch 15, batch    29 | loss: 12.9656647CurrentTrain: epoch 15, batch    30 | loss: 7.3588811CurrentTrain: epoch 15, batch    31 | loss: 17.6513684CurrentTrain: epoch 15, batch    32 | loss: 11.2102887CurrentTrain: epoch 15, batch    33 | loss: 12.6103511CurrentTrain: epoch 15, batch    34 | loss: 8.2529581CurrentTrain: epoch 15, batch    35 | loss: 11.0797101CurrentTrain: epoch 15, batch    36 | loss: 10.6516154CurrentTrain: epoch 15, batch    37 | loss: 7.0348082CurrentTrain: epoch 15, batch    38 | loss: 9.4559560CurrentTrain: epoch 15, batch    39 | loss: 7.6647358CurrentTrain: epoch 15, batch    40 | loss: 8.3782913CurrentTrain: epoch 15, batch    41 | loss: 12.9352989CurrentTrain: epoch 15, batch    42 | loss: 9.9789687CurrentTrain: epoch 15, batch    43 | loss: 5.5537974CurrentTrain: epoch 15, batch    44 | loss: 11.1135870CurrentTrain: epoch 15, batch    45 | loss: 10.4791336CurrentTrain: epoch 15, batch    46 | loss: 14.4821052CurrentTrain: epoch 15, batch    47 | loss: 15.7828863CurrentTrain: epoch 15, batch    48 | loss: 11.6694903CurrentTrain: epoch 15, batch    49 | loss: 19.3080711CurrentTrain: epoch 15, batch    50 | loss: 23.5994524CurrentTrain: epoch 15, batch    51 | loss: 7.7641457CurrentTrain: epoch 15, batch    52 | loss: 9.5868440CurrentTrain: epoch 15, batch    53 | loss: 8.0257333CurrentTrain: epoch 15, batch    54 | loss: 8.6211222CurrentTrain: epoch 15, batch    55 | loss: 10.3842168CurrentTrain: epoch 15, batch    56 | loss: 14.4364285CurrentTrain: epoch 15, batch    57 | loss: 12.1876553CurrentTrain: epoch 15, batch    58 | loss: 15.3092474CurrentTrain: epoch 15, batch    59 | loss: 14.6913846CurrentTrain: epoch 15, batch    60 | loss: 12.7101316CurrentTrain: epoch 15, batch    61 | loss: 23.8891435CurrentTrain: epoch  7, batch    62 | loss: 8.3519880
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: the theme of the loss of identity in crowds in this painting can be seen as a precursor to expressionist painting , particularly the work of james ensor .
Head Entity: james ensor
Tail Entity: expressionist
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: The surrealist movement, which sought to challenge the conventional perception of reality, was significantly influenced by the works of Salvador Dalí.  
Head Entity: Salvador Dalí  
Tail Entity: surrealist  

Relation: movement  
Context: The emergence of the romantic movement in literature was marked by the profound impact of poets like William Wordsworth and Samuel Taylor Coleridge.  
Head Entity: William Wordsworth  
Tail Entity: romantic  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: lyons hill is a restored village , and former parish with church , now part of the community of ardclough in north county kildare .
Head Entity: ardclough
Tail Entity: county kildare
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: The city of San Francisco is known for its iconic Golden Gate Bridge and is part of the larger San Francisco County in California.  
Head Entity: San Francisco  
Tail Entity: San Francisco County  

Relation: located in the administrative territorial entity  
Context: The historic town of Bath is famous for its Roman baths and is situated within the unitary authority of Bath and North East Somerset.  
Head Entity: Bath  
Tail Entity: Bath and North East Somerset  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the nation as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: he also went on a road trip from san antonio to oakland for the magnum project “ postcards from america ” with jim goldberg , alec soth , mikhael subotzky and susan meiselas .
Head Entity: jim goldberg
Tail Entity: magnum
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: After years of dedication and hard work, she was finally recognized as a key member of the prestigious art collective known as the "Creative Minds."  
Head Entity: she  
Tail Entity: Creative Minds  

Relation: member of  
Context: The renowned guitarist was proud to be a member of the legendary rock band that shaped the music scene in the 70s.  
Head Entity: guitarist  
Tail Entity: rock band  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: the green island bridge crosses the hudson river in new york , connecting green island with troy .
Head Entity: green island bridge
Tail Entity: hudson river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: the golden gate bridge crosses the san francisco bay, linking san francisco to marin county.  
Head Entity: golden gate bridge  
Tail Entity: san francisco bay  

Relation: crosses  
Context: the london underground crosses the thames river, providing a vital transport link between the north and south of the city.  
Head Entity: london underground  
Tail Entity: thames river  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: it is on the panj river just downstream from the mouth of the bartang river at the point where the panj briefly turns west before resuming its northerly course .
Head Entity: panj river
Tail Entity: bartang river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The Missouri River, which is a major tributary of the Mississippi River, flows through several states before joining the larger watercourse.  
Head Entity: Missouri River  
Tail Entity: Mississippi River  

Relation: tributary  
Context: The Rio Grande serves as a natural border between the United States and Mexico, and it has several tributaries, including the Pecos River.  
Head Entity: Rio Grande  
Tail Entity: Pecos River  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: in september 2001 , mimica became minister of european integration under prime minister ivica račan , position he held until december 2003 .
Head Entity: ivica račan
Tail Entity: prime minister
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: in 2010, barack obama was elected as the 44th president of the united states, a position he held until 2017.  
Head Entity: barack obama  
Tail Entity: president of the united states  

Relation: position held  
Context: after serving as the mayor of new york city from 2002 to 2013, michael bloomberg transitioned to a role in philanthropy, a position he held until 2020.  
Head Entity: michael bloomberg  
Tail Entity: mayor of new york city  
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: it was developed by syn sophia , who also developed the other games in the " style savvy " series .
Head Entity: style savvy
Tail Entity: syn sophia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: the groundbreaking software was created by tech innovations inc., a leader in the field of artificial intelligence.  
Head Entity: artificial intelligence  
Tail Entity: tech innovations inc.  

Relation: developer  
Context: the popular mobile app was designed by creative minds ltd., which specializes in user-friendly applications.  
Head Entity: mobile app  
Tail Entity: creative minds ltd.  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: holy molar are a san diego based band formed in 2001 .
Head Entity: holy molar
Tail Entity: san diego
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the tech startup was established in silicon valley in 2015.  
Head Entity: the tech startup  
Tail Entity: silicon valley  

Relation: location of formation  
Context: the famous rock band was created in new york city during the summer of 1999.  
Head Entity: the famous rock band  
Tail Entity: new york city  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: he has also played on german chart - topper albums of die fantastischen vier and fettes brot and has collaborated with uk artists lemn sissay , ( dr. ) robert howard and martin glover .
Head Entity: fettes brot
Tail Entity: german
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous dish sushi is traditionally associated with Japan, where it has been a staple for centuries, showcasing the country's rich culinary heritage.  
Head Entity: sushi  
Tail Entity: Japan  

Relation: country of origin  
Context: The renowned brand of chocolate, Lindt, is known for its high-quality products that originate from Switzerland, a country famous for its chocolate-making expertise.  
Head Entity: Lindt  
Tail Entity: Switzerland  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 92.36%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 94.03%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.29%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.47%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.44%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.42%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.40%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.92%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.08%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.22%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.39%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.51%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.62%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.73%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 95.78%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.01%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.96%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.05%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.08%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.91%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.95%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 95.80%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.76%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.72%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.69%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.76%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.90%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.14%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 92.36%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 94.03%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.29%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.47%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.44%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.42%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.40%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.92%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.08%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.22%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.39%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.51%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.62%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.73%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 95.78%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.01%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.96%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.05%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.08%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.91%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.95%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 95.80%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.76%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.72%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.69%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.76%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.90%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.14%   
cur_acc:  ['0.9514']
his_acc:  ['0.9514']
CurrentTrain: epoch 15, batch     0 | loss: 21.7062986CurrentTrain: epoch 15, batch     1 | loss: 28.7641555CurrentTrain: epoch 15, batch     2 | loss: 12.4675277CurrentTrain: epoch  1, batch     3 | loss: 10.8036246CurrentTrain: epoch 15, batch     0 | loss: 16.7453660CurrentTrain: epoch 15, batch     1 | loss: 15.6055502CurrentTrain: epoch 15, batch     2 | loss: 19.2614543CurrentTrain: epoch  1, batch     3 | loss: 10.1998342CurrentTrain: epoch 15, batch     0 | loss: 14.3741149CurrentTrain: epoch 15, batch     1 | loss: 15.0469396CurrentTrain: epoch 15, batch     2 | loss: 17.1606490CurrentTrain: epoch  1, batch     3 | loss: 13.3099976CurrentTrain: epoch 15, batch     0 | loss: 21.7363382CurrentTrain: epoch 15, batch     1 | loss: 11.4891555CurrentTrain: epoch 15, batch     2 | loss: 16.9869147CurrentTrain: epoch  1, batch     3 | loss: 7.3739137CurrentTrain: epoch 15, batch     0 | loss: 17.6720610CurrentTrain: epoch 15, batch     1 | loss: 10.5728452CurrentTrain: epoch 15, batch     2 | loss: 26.5476359CurrentTrain: epoch  1, batch     3 | loss: 8.2270593CurrentTrain: epoch 15, batch     0 | loss: 18.8188704CurrentTrain: epoch 15, batch     1 | loss: 8.7731210CurrentTrain: epoch 15, batch     2 | loss: 9.3051330CurrentTrain: epoch  1, batch     3 | loss: 6.0435432CurrentTrain: epoch 15, batch     0 | loss: 9.2606258CurrentTrain: epoch 15, batch     1 | loss: 8.8505861CurrentTrain: epoch 15, batch     2 | loss: 10.1109507CurrentTrain: epoch  1, batch     3 | loss: 7.2117076CurrentTrain: epoch 15, batch     0 | loss: 16.1611843CurrentTrain: epoch 15, batch     1 | loss: 10.3351878CurrentTrain: epoch 15, batch     2 | loss: 12.1316258CurrentTrain: epoch  1, batch     3 | loss: 7.2769069CurrentTrain: epoch 15, batch     0 | loss: 15.2882427CurrentTrain: epoch 15, batch     1 | loss: 8.1595569CurrentTrain: epoch 15, batch     2 | loss: 7.7663485CurrentTrain: epoch  1, batch     3 | loss: 19.7768452CurrentTrain: epoch 15, batch     0 | loss: 11.8180027CurrentTrain: epoch 15, batch     1 | loss: 6.8197136CurrentTrain: epoch 15, batch     2 | loss: 9.3880363CurrentTrain: epoch  1, batch     3 | loss: 6.3567506
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: there was concern in the gun lobby over restrictions on second amendment rights in the united states .. ( march 7 , 2000 ) cnn .
Head Entity: second amendment
Tail Entity: united states
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The new environmental regulations introduced by the European Union aim to reduce carbon emissions across member states.  
Head Entity: environmental regulations  
Tail Entity: European Union  

Relation: applies to jurisdiction  
Context: The recent changes in tax laws have raised questions about their applicability in various states across the country.  
Head Entity: tax laws  
Tail Entity: states
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: it is prominently featured on jean michel jarre 's albums " oxygène " ( 1977 ) and " équinoxe " ( 1978 ) .
Head Entity: équinoxe
Tail Entity: jean michel jarre
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: The legendary band Queen is known for their iconic song "Bohemian Rhapsody," which showcases Freddie Mercury's extraordinary vocal talent.  
Head Entity: Bohemian Rhapsody  
Tail Entity: Freddie Mercury  

Relation: performer  
Context: Taylor Swift performed her hit single "Shake It Off" at the Grammy Awards, captivating the audience with her energetic stage presence.  
Head Entity: Shake It Off  
Tail Entity: Taylor Swift  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the glasflügel h-201 standard " libelle " ( german : " dragonfly " ) is an early composite standard class single - seat sailplane produced by glasflügel from 1967 .
Head Entity: dragonfly
Tail Entity: glasflügel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The iPhone 13, a smartphone developed by Apple, was released in September 2021 and features advanced camera technology and improved battery life.  
Head Entity: iPhone 13  
Tail Entity: Apple  

Relation: manufacturer  
Context: The Tesla Model S is an all-electric luxury sedan that has been produced by Tesla, Inc. since 2012, showcasing cutting-edge technology and performance.  
Head Entity: Tesla Model S  
Tail Entity: Tesla, Inc.  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: in contrast to gwladys and roger 's allegiance to the house of lancaster , their three sons were staunch yorkists during the wars of the roses .
Head Entity: wars of the roses
Tail Entity: house of lancaster
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: During the annual tech conference, several leading companies showcased their latest innovations, with Google and Microsoft being the most prominent participants.  
Head Entity: tech conference  
Tail Entity: Google  

Relation: participant  
Context: The local community organized a charity run, where various schools and organizations, including the Red Cross, were active participants in raising funds for the homeless.  
Head Entity: charity run  
Tail Entity: Red Cross  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the application is compatible with various operating systems, including macOS, Windows, and Ubuntu.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: Many developers prefer to use Linux as their primary operating system due to its flexibility and open-source nature.  
Head Entity: developers  
Tail Entity: Linux  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in july 2007 , griffin lost by a technical knockout to light heavyweight champion danny green of the world boxing association at challenge stadium in perth , australia .
Head Entity: danny green
Tail Entity: light heavyweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: during the 2020 olympics, the men's 100 meters final showcased athletes from various competition classes, with usain bolt representing the sprinting class.  
Head Entity: usain bolt  
Tail Entity: sprinting  

Relation: competition class  
Context: the formula one race in monaco is known for its high-speed competition class, where lewis hamilton competed fiercely against other top drivers.  
Head Entity: lewis hamilton  
Tail Entity: formula one
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: the qup gained 3 seats from its previous election result , and won a 10 % swing to it in primary votes , but failed to defeat the incumbent labor government led by ted theodore .
Head Entity: previous election
Tail Entity: ted theodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: After a closely contested campaign, the voters chose Sarah Johnson as their representative, marking a significant shift in the political landscape of the district.  
Head Entity: closely contested campaign  
Tail Entity: Sarah Johnson  

Relation: successful candidate  
Context: In the recent mayoral election, the community rallied behind Mark Thompson, who promised to bring change and transparency to the local government.  
Head Entity: recent mayoral election  
Tail Entity: Mark Thompson  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: jenő szervánszky is the father of the pianist , valéria szervánszky and the brother of the composer , endre szervánszky and violinist , péter szervánszky .
Head Entity: péter szervánszky
Tail Entity: endre szervánszky
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: emma and oliver are both children of the same parents, making them siblings. they often play together and share a close bond.  
Head Entity: emma  
Tail Entity: oliver  

Relation: sibling  
Context: during the family reunion, it was clear that both lucas and mia inherited their parents' artistic talents, as they are siblings who excel in painting and music.  
Head Entity: lucas  
Tail Entity: mia  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: iain harrison is a competitive shooter and former british army captain .
Head Entity: iain harrison
Tail Entity: british army
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: sergeant johnson served in the united states marine corps during his military career.  
Head Entity: sergeant johnson  
Tail Entity: united states marine corps  

Relation: military branch  
Context: general smith was a prominent figure in the royal air force, leading several key missions.  
Head Entity: general smith  
Tail Entity: royal air force  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: developed on the east bank of the river near the former native village of otstonwakin , the borough is named for andrew montour , the french / native american and son of madame montour .
Head Entity: madame montour
Tail Entity: andrew montour
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: In the small town of Maplewood, Sarah Johnson is known for her dedication to her family, especially her son, Michael, who is a talented musician.  
Head Entity: Sarah Johnson  
Tail Entity: Michael  

Relation: child  
Context: The famous author, J.K. Rowling, often shares stories about her daughter, Jessica, who inspired many of the characters in her books.  
Head Entity: J.K. Rowling  
Tail Entity: Jessica  
MemoryTrain:  epoch 15, batch     0 | loss: 7.7553300MemoryTrain:  epoch 15, batch     1 | loss: 9.8303655MemoryTrain:  epoch 15, batch     2 | loss: 7.9624777MemoryTrain:  epoch 11, batch     3 | loss: 13.5002695MemoryTrain:  epoch 15, batch     0 | loss: 7.2169872MemoryTrain:  epoch 15, batch     1 | loss: 7.7108236MemoryTrain:  epoch 15, batch     2 | loss: 7.3786883MemoryTrain:  epoch 11, batch     3 | loss: 4.4810727MemoryTrain:  epoch 15, batch     0 | loss: 7.5944241MemoryTrain:  epoch 15, batch     1 | loss: 5.0229368MemoryTrain:  epoch 15, batch     2 | loss: 4.3752917MemoryTrain:  epoch 11, batch     3 | loss: 8.6955137MemoryTrain:  epoch 15, batch     0 | loss: 6.0307651MemoryTrain:  epoch 15, batch     1 | loss: 3.5495742MemoryTrain:  epoch 15, batch     2 | loss: 7.3310262MemoryTrain:  epoch 11, batch     3 | loss: 3.3732929MemoryTrain:  epoch 15, batch     0 | loss: 5.8940109MemoryTrain:  epoch 15, batch     1 | loss: 4.6618968MemoryTrain:  epoch 15, batch     2 | loss: 4.0293473MemoryTrain:  epoch 11, batch     3 | loss: 3.3045651MemoryTrain:  epoch 15, batch     0 | loss: 4.6844753MemoryTrain:  epoch 15, batch     1 | loss: 4.8870870MemoryTrain:  epoch 15, batch     2 | loss: 5.4657477MemoryTrain:  epoch 11, batch     3 | loss: 2.5163449MemoryTrain:  epoch 15, batch     0 | loss: 2.5671792MemoryTrain:  epoch 15, batch     1 | loss: 2.3242614MemoryTrain:  epoch 15, batch     2 | loss: 3.2111385MemoryTrain:  epoch 11, batch     3 | loss: 2.5860279MemoryTrain:  epoch 15, batch     0 | loss: 5.4418441MemoryTrain:  epoch 15, batch     1 | loss: 5.0786662MemoryTrain:  epoch 15, batch     2 | loss: 3.8421379MemoryTrain:  epoch 11, batch     3 | loss: 2.8659119MemoryTrain:  epoch 15, batch     0 | loss: 2.6416056MemoryTrain:  epoch 15, batch     1 | loss: 3.4487693MemoryTrain:  epoch 15, batch     2 | loss: 2.3968061MemoryTrain:  epoch 11, batch     3 | loss: 4.0112042MemoryTrain:  epoch 15, batch     0 | loss: 2.1224080MemoryTrain:  epoch 15, batch     1 | loss: 5.3442568MemoryTrain:  epoch 15, batch     2 | loss: 7.0890260MemoryTrain:  epoch 11, batch     3 | loss: 1.9598023
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 16.67%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 15.62%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 15.00%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 17.71%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 24.11%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 32.03%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 37.50%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 41.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 45.45%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 48.44%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 51.92%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 54.91%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 57.08%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 59.38%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 61.03%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 63.19%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 65.13%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 66.56%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 67.56%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 69.57%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 70.57%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 71.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.36%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 73.38%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 74.11%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 75.83%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 76.61%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 78.03%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 78.68%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 79.29%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 79.86%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 80.41%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 80.92%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 81.41%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 81.70%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 81.98%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 81.96%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 81.25%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 80.30%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 79.79%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 79.56%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 78.83%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 78.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 78.92%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 79.21%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 79.36%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 79.51%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 79.77%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 80.02%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 80.37%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 80.50%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 80.72%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 80.31%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 80.43%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 80.54%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 80.06%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.80%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.28%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 92.43%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 92.33%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 91.85%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 91.93%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 91.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.07%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 92.36%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 92.63%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 92.89%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.93%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.93%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.10%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.26%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 94.41%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.55%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.82%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 94.79%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 94.77%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 94.46%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.58%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 94.57%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 94.02%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 93.75%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.88%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 93.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.50%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 93.39%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.52%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 93.41%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.42%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 93.42%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 93.32%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 93.43%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 93.44%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 93.55%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 92.96%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 91.60%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 90.58%   [EVAL] batch:   65 | acc: 25.00%,  total acc: 89.58%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 88.34%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 87.41%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 86.59%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 86.61%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 86.71%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 86.46%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 86.30%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 86.40%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 86.42%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 86.51%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 86.53%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 86.54%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 86.63%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 86.88%   [EVAL] batch:   81 | acc: 100.00%,  total acc: 87.04%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 87.05%   [EVAL] batch:   83 | acc: 93.75%,  total acc: 87.13%   [EVAL] batch:   84 | acc: 87.50%,  total acc: 87.13%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 87.14%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 87.21%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 87.29%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 87.43%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 87.57%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 87.64%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 87.77%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 87.90%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 88.03%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 88.16%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 88.40%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 88.52%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 88.80%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 88.85%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 88.77%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 88.76%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 88.87%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 88.92%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 88.55%   [EVAL] batch:  107 | acc: 43.75%,  total acc: 88.14%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 87.84%   [EVAL] batch:  109 | acc: 50.00%,  total acc: 87.50%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 87.27%   [EVAL] batch:  111 | acc: 56.25%,  total acc: 87.00%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 87.00%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 87.01%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 87.12%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 87.07%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 87.13%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 87.18%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 87.24%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 87.29%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 87.35%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 87.30%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 87.09%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 87.10%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 87.15%   
cur_acc:  ['0.9514', '0.8006']
his_acc:  ['0.9514', '0.8715']
CurrentTrain: epoch 15, batch     0 | loss: 22.7210251CurrentTrain: epoch 15, batch     1 | loss: 15.1898926CurrentTrain: epoch 15, batch     2 | loss: 18.9060186CurrentTrain: epoch  1, batch     3 | loss: 8.9065143CurrentTrain: epoch 15, batch     0 | loss: 11.9066059CurrentTrain: epoch 15, batch     1 | loss: 12.1099636CurrentTrain: epoch 15, batch     2 | loss: 14.0085907CurrentTrain: epoch  1, batch     3 | loss: 14.1937927CurrentTrain: epoch 15, batch     0 | loss: 12.5205721CurrentTrain: epoch 15, batch     1 | loss: 10.2891184CurrentTrain: epoch 15, batch     2 | loss: 9.7614988CurrentTrain: epoch  1, batch     3 | loss: 7.1779964CurrentTrain: epoch 15, batch     0 | loss: 13.4800823CurrentTrain: epoch 15, batch     1 | loss: 8.3889618CurrentTrain: epoch 15, batch     2 | loss: 11.3779983CurrentTrain: epoch  1, batch     3 | loss: 8.1095273CurrentTrain: epoch 15, batch     0 | loss: 14.1294696CurrentTrain: epoch 15, batch     1 | loss: 9.9021971CurrentTrain: epoch 15, batch     2 | loss: 20.6787327CurrentTrain: epoch  1, batch     3 | loss: 5.8563921CurrentTrain: epoch 15, batch     0 | loss: 9.3912426CurrentTrain: epoch 15, batch     1 | loss: 7.4537604CurrentTrain: epoch 15, batch     2 | loss: 7.7295701CurrentTrain: epoch  1, batch     3 | loss: 6.2328126CurrentTrain: epoch 15, batch     0 | loss: 8.4830041CurrentTrain: epoch 15, batch     1 | loss: 8.9800901CurrentTrain: epoch 15, batch     2 | loss: 9.6421218CurrentTrain: epoch  1, batch     3 | loss: 6.9357772CurrentTrain: epoch 15, batch     0 | loss: 8.3374685CurrentTrain: epoch 15, batch     1 | loss: 7.5839537CurrentTrain: epoch 15, batch     2 | loss: 9.0525123CurrentTrain: epoch  1, batch     3 | loss: 6.8222622CurrentTrain: epoch 15, batch     0 | loss: 8.3179305CurrentTrain: epoch 15, batch     1 | loss: 6.7330313CurrentTrain: epoch 15, batch     2 | loss: 7.9637000CurrentTrain: epoch  1, batch     3 | loss: 13.9861845CurrentTrain: epoch 15, batch     0 | loss: 14.0702287CurrentTrain: epoch 15, batch     1 | loss: 8.0895901CurrentTrain: epoch 15, batch     2 | loss: 8.7296430CurrentTrain: epoch  1, batch     3 | loss: 6.9126905
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: rohan is the son of narayana murthy , founder of infosys , and his wife sudha murthy , an engineer and author of note .
Head Entity: narayana murthy
Tail Entity: sudha murthy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: elena is a renowned scientist who often collaborates with her husband, dr. mark thompson, on various research projects.  
Head Entity: dr. mark thompson  
Tail Entity: elena  

Relation: spouse  
Context: during the award ceremony, it was revealed that the famous actor, john doe, had been married to his long-time partner, jane smith, for over a decade.  
Head Entity: john doe  
Tail Entity: jane smith  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: it was first released on a single in the uk by parlophone in september 1967 , and on the group 's self - titled album " tomorrow " in february 1968 .
Head Entity: tomorrow
Tail Entity: parlophone
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: The band's debut album was launched under the iconic label Atlantic Records, which has been home to many legendary artists.  
Head Entity: debut album  
Tail Entity: Atlantic Records  

Relation: record label  
Context: After signing with Universal Music, the artist released their latest single, which quickly climbed the charts.  
Head Entity: latest single  
Tail Entity: Universal Music  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: distributed by paramount pictures , " the italian job " was theatrically released in the united states on may 30 , 2003 , and grossed over $ 176 million worldwide .
Head Entity: the italian job
Tail Entity: paramount pictures
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: the film was distributed by universal pictures and received critical acclaim upon its release in 2015.  
Head Entity: the film  
Tail Entity: universal pictures  

Relation: distributor  
Context: the album was distributed by sony music and quickly climbed the charts, becoming a bestseller.  
Head Entity: the album  
Tail Entity: sony music  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: in 1796 carondelet commissioned pintado to conduct a survey of lands granted to ambrosio longue - épée in iberville county on the west bank of the mississippi river opposite baton rouge .
Head Entity: baton rouge
Tail Entity: mississippi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: the city of new orleans is situated on the banks of the mississippi river, making it a vibrant hub for trade and culture.  
Head Entity: new orleans  
Tail Entity: mississippi river  

Relation: located in or next to body of water  
Context: the picturesque village of lake placid is nestled beside the serene waters of lake placid, attracting visitors year-round.  
Head Entity: lake placid  
Tail Entity: lake placid  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: following the sale of amstrad plc to rupert murdoch 's bskyb , viglen is now lord sugar 's sole it establishment .
Head Entity: bskyb
Tail Entity: amstrad
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Relation: subsidiary  
Context: In 2018, Facebook acquired WhatsApp, making it a subsidiary of the social media giant.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Relation: subsidiary  
Context: After the merger, the new company announced that its subsidiary, XYZ Corp, would continue to operate independently.  
Head Entity: new company  
Tail Entity: XYZ Corp  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: he challenged seven other swimmers in heat four including ukraine 's rostyslav svanidze , a top 16 finalist in atlanta four years earlier .
Head Entity: rostyslav svanidze
Tail Entity: ukraine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: after years of living in the united states, the renowned artist decided to apply for dual citizenship, allowing him to maintain his ties to his homeland, italy.  
Head Entity: the renowned artist  
Tail Entity: italy  

Relation: country of citizenship  
Context: during the international conference, the delegate from canada proudly spoke about the rights and privileges of being a citizen of his country, emphasizing the importance of citizenship in shaping national identity.  
Head Entity: the delegate from canada  
Tail Entity: canada  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: nina cites " duffy , muse , and other pop and alternative artists " as her major influences .
Head Entity: duffy
Tail Entity: pop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the film "inception" is often categorized as a science fiction thriller, blending elements of action and psychological drama.  
Head Entity: inception  
Tail Entity: science fiction  

Relation: genre  
Context: beethoven's symphonies are considered masterpieces of classical music, showcasing his innovative approach to composition.  
Head Entity: beethoven  
Tail Entity: classical music  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: san lázaro is 660 km far from asunción and is located at the exact site of the confluence of the apa river with paraguay river , bordering brazil - chaco .
Head Entity: apa river
Tail Entity: paraguay river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: the mississippi river flows into the gulf of mexico, creating a rich delta ecosystem that supports diverse wildlife and plant species.  
Head Entity: mississippi river  
Tail Entity: gulf of mexico  

Relation: mouth of the watercourse  
Context: the rhine river, which runs through several countries in europe, ultimately empties into the north sea, providing a vital shipping route for international trade.  
Head Entity: rhine river  
Tail Entity: north sea  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: bradley halliday ( born 10 july 1995 ) is an english professional footballer who plays as a defender or a midfielder for league two club cambridge united .
Head Entity: bradley halliday
Tail Entity: defender
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: alex morgan ( born 2 july 1989 ) is an american soccer player who plays as a forward for the national team and club orlando pride in the national women's soccer league.  
Head Entity: alex morgan  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: lebron james ( born 30 december 1984 ) is an american professional basketball player who plays as a small forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: small forward  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: hammami made his international debut for tunisia in 2007 , and represented them at the africa cup of nations in 2010 and 2013 .
Head Entity: 2010
Tail Entity: africa cup of nations
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2021 UEFA Champions League was won by Chelsea, who defeated Manchester City in the final held in Porto.  
Head Entity: 2021  
Tail Entity: UEFA Champions League  

Relation: sports season of league or competition  
Context: The 2019 Cricket World Cup took place in England and Wales, featuring ten teams competing for the title.  
Head Entity: 2019  
Tail Entity: Cricket World Cup  
MemoryTrain:  epoch 15, batch     0 | loss: 4.0371402MemoryTrain:  epoch 15, batch     1 | loss: 4.9553728MemoryTrain:  epoch 15, batch     2 | loss: 7.0319723MemoryTrain:  epoch 15, batch     3 | loss: 4.8895979MemoryTrain:  epoch 15, batch     4 | loss: 6.3234939MemoryTrain:  epoch  9, batch     5 | loss: 5.7191557MemoryTrain:  epoch 15, batch     0 | loss: 5.1450251MemoryTrain:  epoch 15, batch     1 | loss: 6.7036364MemoryTrain:  epoch 15, batch     2 | loss: 5.5941600MemoryTrain:  epoch 15, batch     3 | loss: 5.6934455MemoryTrain:  epoch 15, batch     4 | loss: 6.0522221MemoryTrain:  epoch  9, batch     5 | loss: 8.3819136MemoryTrain:  epoch 15, batch     0 | loss: 3.9222670MemoryTrain:  epoch 15, batch     1 | loss: 5.9939268MemoryTrain:  epoch 15, batch     2 | loss: 4.3875053MemoryTrain:  epoch 15, batch     3 | loss: 7.9053742MemoryTrain:  epoch 15, batch     4 | loss: 3.1470574MemoryTrain:  epoch  9, batch     5 | loss: 3.8373457MemoryTrain:  epoch 15, batch     0 | loss: 4.3288969MemoryTrain:  epoch 15, batch     1 | loss: 6.5059367MemoryTrain:  epoch 15, batch     2 | loss: 3.0100823MemoryTrain:  epoch 15, batch     3 | loss: 3.1491879MemoryTrain:  epoch 15, batch     4 | loss: 2.6825756MemoryTrain:  epoch  9, batch     5 | loss: 4.9427065MemoryTrain:  epoch 15, batch     0 | loss: 6.6517534MemoryTrain:  epoch 15, batch     1 | loss: 4.0446813MemoryTrain:  epoch 15, batch     2 | loss: 3.8888144MemoryTrain:  epoch 15, batch     3 | loss: 3.0663080MemoryTrain:  epoch 15, batch     4 | loss: 2.7497075MemoryTrain:  epoch  9, batch     5 | loss: 1.8302255MemoryTrain:  epoch 15, batch     0 | loss: 3.6616333MemoryTrain:  epoch 15, batch     1 | loss: 2.1496437MemoryTrain:  epoch 15, batch     2 | loss: 4.8653208MemoryTrain:  epoch 15, batch     3 | loss: 3.0727907MemoryTrain:  epoch 15, batch     4 | loss: 4.9051867MemoryTrain:  epoch  9, batch     5 | loss: 5.3016510MemoryTrain:  epoch 15, batch     0 | loss: 3.1865078MemoryTrain:  epoch 15, batch     1 | loss: 4.1655785MemoryTrain:  epoch 15, batch     2 | loss: 2.8246411MemoryTrain:  epoch 15, batch     3 | loss: 6.6441884MemoryTrain:  epoch 15, batch     4 | loss: 4.3779182MemoryTrain:  epoch  9, batch     5 | loss: 6.6163935MemoryTrain:  epoch 15, batch     0 | loss: 2.1700551MemoryTrain:  epoch 15, batch     1 | loss: 2.4511556MemoryTrain:  epoch 15, batch     2 | loss: 3.0908551MemoryTrain:  epoch 15, batch     3 | loss: 2.6115035MemoryTrain:  epoch 15, batch     4 | loss: 3.9169076MemoryTrain:  epoch  9, batch     5 | loss: 5.4725342MemoryTrain:  epoch 15, batch     0 | loss: 3.1233371MemoryTrain:  epoch 15, batch     1 | loss: 3.9683872MemoryTrain:  epoch 15, batch     2 | loss: 2.3149282MemoryTrain:  epoch 15, batch     3 | loss: 1.7911296MemoryTrain:  epoch 15, batch     4 | loss: 5.1750898MemoryTrain:  epoch  9, batch     5 | loss: 10.2926341MemoryTrain:  epoch 15, batch     0 | loss: 4.9452270MemoryTrain:  epoch 15, batch     1 | loss: 3.8806846MemoryTrain:  epoch 15, batch     2 | loss: 3.8688152MemoryTrain:  epoch 15, batch     3 | loss: 2.3275296MemoryTrain:  epoch 15, batch     4 | loss: 2.5504814MemoryTrain:  epoch  9, batch     5 | loss: 1.6472108
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 69.64%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 74.31%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 73.75%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 74.43%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 77.40%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 76.79%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 75.39%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 75.37%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 74.65%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 75.33%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 75.94%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 76.49%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 76.70%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 76.90%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 77.60%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 78.25%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 78.61%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 78.24%   [EVAL] batch:   27 | acc: 81.25%,  total acc: 78.35%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 78.66%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 77.71%   [EVAL] batch:   30 | acc: 62.50%,  total acc: 77.22%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 77.15%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 76.89%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 76.65%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 76.25%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 75.87%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 75.84%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 75.82%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 75.80%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 75.94%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 76.22%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 76.04%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 76.31%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 76.14%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 75.14%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 74.18%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 73.27%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 72.92%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 71.81%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 71.38%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 71.94%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 72.48%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 73.00%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 73.50%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 73.98%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 74.44%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 74.89%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 75.32%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 75.74%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 76.15%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 76.54%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 76.81%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 76.29%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 83.17%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 84.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 86.03%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 86.84%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 87.19%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 87.22%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 87.23%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.98%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 88.43%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.84%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 89.22%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 89.92%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 90.04%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 90.34%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 89.71%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 89.29%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 89.19%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 89.14%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.42%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.94%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 90.03%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 90.12%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 90.06%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 90.14%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 89.95%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 89.63%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 89.32%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.54%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 89.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.22%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 89.18%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 89.27%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 89.47%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 89.43%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.51%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 89.33%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 89.30%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 89.38%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 89.55%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 89.52%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 88.99%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 87.79%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 87.02%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 85.98%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 84.89%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 84.10%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 83.33%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 83.21%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 82.92%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 82.55%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 82.11%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 81.84%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 81.92%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 82.15%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 82.22%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 82.21%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 82.36%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 82.34%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 82.56%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 82.24%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 82.15%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 82.14%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 81.99%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 81.76%   [EVAL] batch:   86 | acc: 75.00%,  total acc: 81.68%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 81.53%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 81.74%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 81.94%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 82.34%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 82.53%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 82.71%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 82.89%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 83.07%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 83.25%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 83.42%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 83.85%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 83.95%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 83.92%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 84.01%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 84.17%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 84.26%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 83.88%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 83.45%   [EVAL] batch:  108 | acc: 68.75%,  total acc: 83.31%   [EVAL] batch:  109 | acc: 50.00%,  total acc: 83.01%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 82.77%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 82.42%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 82.47%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 82.51%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 82.66%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 82.70%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 82.80%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 82.89%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 82.93%   [EVAL] batch:  119 | acc: 75.00%,  total acc: 82.86%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 82.54%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 82.27%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 82.06%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 81.91%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 81.85%   [EVAL] batch:  125 | acc: 87.50%,  total acc: 81.89%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 81.79%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 81.64%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 81.35%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 81.30%   [EVAL] batch:  130 | acc: 56.25%,  total acc: 81.11%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 81.20%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 81.30%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 81.34%   [EVAL] batch:  134 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 81.39%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 81.43%   [EVAL] batch:  138 | acc: 68.75%,  total acc: 81.34%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:  140 | acc: 62.50%,  total acc: 81.12%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 81.07%   [EVAL] batch:  142 | acc: 62.50%,  total acc: 80.94%   [EVAL] batch:  143 | acc: 87.50%,  total acc: 80.99%   [EVAL] batch:  144 | acc: 87.50%,  total acc: 81.03%   [EVAL] batch:  145 | acc: 87.50%,  total acc: 81.08%   [EVAL] batch:  146 | acc: 81.25%,  total acc: 81.08%   [EVAL] batch:  147 | acc: 81.25%,  total acc: 81.08%   [EVAL] batch:  148 | acc: 93.75%,  total acc: 81.17%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:  150 | acc: 87.50%,  total acc: 81.29%   [EVAL] batch:  151 | acc: 68.75%,  total acc: 81.21%   [EVAL] batch:  152 | acc: 81.25%,  total acc: 81.21%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:  154 | acc: 50.00%,  total acc: 81.05%   [EVAL] batch:  155 | acc: 62.50%,  total acc: 80.93%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 80.89%   [EVAL] batch:  157 | acc: 68.75%,  total acc: 80.81%   [EVAL] batch:  158 | acc: 68.75%,  total acc: 80.74%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 80.62%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 80.51%   [EVAL] batch:  161 | acc: 75.00%,  total acc: 80.48%   [EVAL] batch:  162 | acc: 75.00%,  total acc: 80.44%   [EVAL] batch:  163 | acc: 75.00%,  total acc: 80.41%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 80.42%   [EVAL] batch:  165 | acc: 87.50%,  total acc: 80.46%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 80.39%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 80.43%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 80.36%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 80.07%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 79.79%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 79.51%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 79.37%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 79.02%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 78.86%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 78.98%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 79.10%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 79.21%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 79.33%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 79.44%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 79.56%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 79.67%   [EVAL] batch:  182 | acc: 100.00%,  total acc: 79.78%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 79.89%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 80.11%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 80.18%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 79.99%   
cur_acc:  ['0.9514', '0.8006', '0.7629']
his_acc:  ['0.9514', '0.8715', '0.7999']
CurrentTrain: epoch 15, batch     0 | loss: 15.4240071CurrentTrain: epoch 15, batch     1 | loss: 12.6793450CurrentTrain: epoch 15, batch     2 | loss: 11.7931762CurrentTrain: epoch  1, batch     3 | loss: 10.7422684CurrentTrain: epoch 15, batch     0 | loss: 10.1981866CurrentTrain: epoch 15, batch     1 | loss: 12.9656996CurrentTrain: epoch 15, batch     2 | loss: 12.6789686CurrentTrain: epoch  1, batch     3 | loss: 16.7334695CurrentTrain: epoch 15, batch     0 | loss: 9.4353956CurrentTrain: epoch 15, batch     1 | loss: 9.6825184CurrentTrain: epoch 15, batch     2 | loss: 10.5908489CurrentTrain: epoch  1, batch     3 | loss: 7.3819591CurrentTrain: epoch 15, batch     0 | loss: 17.0247581CurrentTrain: epoch 15, batch     1 | loss: 9.9117921CurrentTrain: epoch 15, batch     2 | loss: 12.8239780CurrentTrain: epoch  1, batch     3 | loss: 9.1217343CurrentTrain: epoch 15, batch     0 | loss: 12.8210557CurrentTrain: epoch 15, batch     1 | loss: 9.7692327CurrentTrain: epoch 15, batch     2 | loss: 10.7147469CurrentTrain: epoch  1, batch     3 | loss: 20.8946440CurrentTrain: epoch 15, batch     0 | loss: 11.1191814CurrentTrain: epoch 15, batch     1 | loss: 15.6308629CurrentTrain: epoch 15, batch     2 | loss: 21.7629722CurrentTrain: epoch  1, batch     3 | loss: 6.3854788CurrentTrain: epoch 15, batch     0 | loss: 16.3437974CurrentTrain: epoch 15, batch     1 | loss: 14.6917970CurrentTrain: epoch 15, batch     2 | loss: 9.0573845CurrentTrain: epoch  1, batch     3 | loss: 6.9349889CurrentTrain: epoch 15, batch     0 | loss: 8.4579312CurrentTrain: epoch 15, batch     1 | loss: 7.4748821CurrentTrain: epoch 15, batch     2 | loss: 6.9296251CurrentTrain: epoch  1, batch     3 | loss: 7.0916823CurrentTrain: epoch 15, batch     0 | loss: 9.1412568CurrentTrain: epoch 15, batch     1 | loss: 16.3843429CurrentTrain: epoch 15, batch     2 | loss: 8.4199993CurrentTrain: epoch  1, batch     3 | loss: 7.7441302CurrentTrain: epoch 15, batch     0 | loss: 8.6549155CurrentTrain: epoch 15, batch     1 | loss: 12.5179176CurrentTrain: epoch 15, batch     2 | loss: 8.1018220CurrentTrain: epoch  1, batch     3 | loss: 7.2666300
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: ( 1988 ) , " night watch " ( 2004 ) , " day watch " ( 2006 ) and " legend № 17 " ( 2013 ) .
Head Entity: day watch
Tail Entity: night watch
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: ( 1995 ) , " The Phantom Menace " ( 1999 ) , " Attack of the Clones " ( 2002 ) and " Revenge of the Sith " ( 2005 ) .  
Head Entity: Attack of the Clones  
Tail Entity: The Phantom Menace  

Relation: follows  
Context: ( 2010 ) , " The Hunger Games " ( 2012 ) , " Catching Fire " ( 2013 ) and " Mockingjay " ( 2014 ) .  
Head Entity: Catching Fire  
Tail Entity: The Hunger Games  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: dick packer was a u.s. soccer center forward who was a member of the u.s. team at the 1956 summer olympics .
Head Entity: dick packer
Tail Entity: soccer
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: serena williams is a professional tennis player who has won numerous grand slam titles throughout her career.  
Head Entity: serena williams  
Tail Entity: tennis  

Relation: sport  
Context: lebron james is known for his exceptional skills in basketball and has played for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: basketball  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael, the youngest son, had always looked up to his father, robert, for guidance and support.  
Head Entity: michael  
Tail Entity: robert  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: he also made a special appearance in kbs ' romantic comedy drama " fight for my way " .
Head Entity: fight for my way
Tail Entity: kbs
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: the popular series "breaking bad" was first aired on amc, captivating audiences with its intense storytelling.  
Head Entity: breaking bad  
Tail Entity: amc  

Relation: original network  
Context: the groundbreaking documentary "planet earth" was produced by bbc, showcasing the beauty of our planet.  
Head Entity: planet earth  
Tail Entity: bbc  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: the 1954 film , directed by kadri venkata reddy , " peddamanushulu " was honoured with the first president 's silver medal for best feature film in telugu .
Head Entity: peddamanushulu
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: the 1994 animated film "The Lion King," produced by Walt Disney, was originally created in English before being dubbed into several other languages.  
Head Entity: The Lion King  
Tail Entity: English  

Relation: original language of film or TV show  
Context: the critically acclaimed 2001 film "Amélie," directed by Jean-Pierre Jeunet, was filmed in French and has since gained a global audience.  
Head Entity: Amélie  
Tail Entity: French  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: in the summer of 2013 he returned to montenegro and joined with fk rudar pljevlja , playing with the club for two seasons in the montenegrin first league .
Head Entity: fk rudar pljevlja
Tail Entity: montenegrin first league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: After a successful stint in the English Premier League, the player signed with FC Barcelona, competing in La Liga for the next three seasons.  
Head Entity: FC Barcelona  
Tail Entity: La Liga  

Relation: league  
Context: The young athlete made headlines when he joined the New York Yankees, showcasing his talent in Major League Baseball.  
Head Entity: New York Yankees  
Tail Entity: Major League Baseball  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: she is supposedly the third and youngest child of seti i and tuya , and the younger sister of ramesses ii and tia .
Head Entity: tia
Tail Entity: tuya
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: Cleopatra was the daughter of Ptolemy XII and his wife, who was also her mother, Cleopatra VI.  
Head Entity: Cleopatra  
Tail Entity: Cleopatra VI  

Relation: mother  
Context: In Norse mythology, Loki is said to have fathered several children, including the goddess Hel, who is the daughter of the giantess Angerboda, her mother.  
Head Entity: Hel  
Tail Entity: Angerboda  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres. he is particularly famous for his interpretations of classical cello concertos.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: a u.s. government - funded $ 36 million bridge over the panj river connects sher khan bandar in afghanistan with nizhniy pyanzh in tajikistan , which transport more than 150 trucks or 1,000 cars daily .
Head Entity: sher khan bandar
Tail Entity: afghanistan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the ancient city of petra, located in southern jordan, is famous for its rock-cut architecture and water conduit system, attracting thousands of tourists each year.  
Head Entity: petra  
Tail Entity: jordan  

Relation: country  
Context: the great wall of china, a series of fortifications made of various materials, stretches across northern china and is a UNESCO World Heritage site.  
Head Entity: great wall of china  
Tail Entity: china  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: hagrid suggests in " harry potter and the chamber of secrets " that " " they 're startin ' ter think the job 's jinxed .
Head Entity: harry potter and the chamber of secrets
Tail Entity: hagrid
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: In the animated series "Avatar: The Last Airbender," Aang, the last Airbender, teams up with Katara and Sokka to defeat the Fire Nation.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: Aang  

Relation: characters  
Context: The novel "Pride and Prejudice" features Elizabeth Bennet as she navigates societal expectations and her relationship with Mr. Darcy.  
Head Entity: Pride and Prejudice  
Tail Entity: Elizabeth Bennet  
MemoryTrain:  epoch 15, batch     0 | loss: 4.8210165MemoryTrain:  epoch 15, batch     1 | loss: 7.7357986MemoryTrain:  epoch 15, batch     2 | loss: 5.5442555MemoryTrain:  epoch 15, batch     3 | loss: 3.6722221MemoryTrain:  epoch 15, batch     4 | loss: 3.8746090MemoryTrain:  epoch 15, batch     5 | loss: 2.8419763MemoryTrain:  epoch 15, batch     6 | loss: 5.1871009MemoryTrain:  epoch  7, batch     7 | loss: 2.4848693MemoryTrain:  epoch 15, batch     0 | loss: 3.1637523MemoryTrain:  epoch 15, batch     1 | loss: 5.2213894MemoryTrain:  epoch 15, batch     2 | loss: 2.4245345MemoryTrain:  epoch 15, batch     3 | loss: 2.9211798MemoryTrain:  epoch 15, batch     4 | loss: 2.8323104MemoryTrain:  epoch 15, batch     5 | loss: 2.7470929MemoryTrain:  epoch 15, batch     6 | loss: 4.4761956MemoryTrain:  epoch  7, batch     7 | loss: 2.4715124MemoryTrain:  epoch 15, batch     0 | loss: 2.8770742MemoryTrain:  epoch 15, batch     1 | loss: 2.5273828MemoryTrain:  epoch 15, batch     2 | loss: 5.3341716MemoryTrain:  epoch 15, batch     3 | loss: 2.6289448MemoryTrain:  epoch 15, batch     4 | loss: 3.3407243MemoryTrain:  epoch 15, batch     5 | loss: 5.0006071MemoryTrain:  epoch 15, batch     6 | loss: 2.3225137MemoryTrain:  epoch  7, batch     7 | loss: 3.0895311MemoryTrain:  epoch 15, batch     0 | loss: 4.7000440MemoryTrain:  epoch 15, batch     1 | loss: 4.5910005MemoryTrain:  epoch 15, batch     2 | loss: 4.3923851MemoryTrain:  epoch 15, batch     3 | loss: 3.1087419MemoryTrain:  epoch 15, batch     4 | loss: 2.5725572MemoryTrain:  epoch 15, batch     5 | loss: 3.2744523MemoryTrain:  epoch 15, batch     6 | loss: 4.5302508MemoryTrain:  epoch  7, batch     7 | loss: 1.8537133MemoryTrain:  epoch 15, batch     0 | loss: 2.5202786MemoryTrain:  epoch 15, batch     1 | loss: 4.4795766MemoryTrain:  epoch 15, batch     2 | loss: 2.9543941MemoryTrain:  epoch 15, batch     3 | loss: 2.6040834MemoryTrain:  epoch 15, batch     4 | loss: 4.1260557MemoryTrain:  epoch 15, batch     5 | loss: 1.6979093MemoryTrain:  epoch 15, batch     6 | loss: 2.0070956MemoryTrain:  epoch  7, batch     7 | loss: 2.9275393MemoryTrain:  epoch 15, batch     0 | loss: 2.1773587MemoryTrain:  epoch 15, batch     1 | loss: 1.9639660MemoryTrain:  epoch 15, batch     2 | loss: 2.1229744MemoryTrain:  epoch 15, batch     3 | loss: 2.2723488MemoryTrain:  epoch 15, batch     4 | loss: 3.7702254MemoryTrain:  epoch 15, batch     5 | loss: 2.7101253MemoryTrain:  epoch 15, batch     6 | loss: 4.6365070MemoryTrain:  epoch  7, batch     7 | loss: 4.2777272MemoryTrain:  epoch 15, batch     0 | loss: 1.7350860MemoryTrain:  epoch 15, batch     1 | loss: 4.1446182MemoryTrain:  epoch 15, batch     2 | loss: 2.6233756MemoryTrain:  epoch 15, batch     3 | loss: 3.9440890MemoryTrain:  epoch 15, batch     4 | loss: 2.9377316MemoryTrain:  epoch 15, batch     5 | loss: 2.6464384MemoryTrain:  epoch 15, batch     6 | loss: 1.9375719MemoryTrain:  epoch  7, batch     7 | loss: 4.1253325MemoryTrain:  epoch 15, batch     0 | loss: 2.0417202MemoryTrain:  epoch 15, batch     1 | loss: 1.8076218MemoryTrain:  epoch 15, batch     2 | loss: 1.4988894MemoryTrain:  epoch 15, batch     3 | loss: 1.6681203MemoryTrain:  epoch 15, batch     4 | loss: 1.9015755MemoryTrain:  epoch 15, batch     5 | loss: 1.7943799MemoryTrain:  epoch 15, batch     6 | loss: 7.0557521MemoryTrain:  epoch  7, batch     7 | loss: 4.9054918MemoryTrain:  epoch 15, batch     0 | loss: 1.9487564MemoryTrain:  epoch 15, batch     1 | loss: 3.7184669MemoryTrain:  epoch 15, batch     2 | loss: 3.9195089MemoryTrain:  epoch 15, batch     3 | loss: 1.8509752MemoryTrain:  epoch 15, batch     4 | loss: 4.3182384MemoryTrain:  epoch 15, batch     5 | loss: 1.7533900MemoryTrain:  epoch 15, batch     6 | loss: 1.6849749MemoryTrain:  epoch  7, batch     7 | loss: 3.7072629MemoryTrain:  epoch 15, batch     0 | loss: 1.8100345MemoryTrain:  epoch 15, batch     1 | loss: 1.9305019MemoryTrain:  epoch 15, batch     2 | loss: 1.6215535MemoryTrain:  epoch 15, batch     3 | loss: 3.9520354MemoryTrain:  epoch 15, batch     4 | loss: 1.5703226MemoryTrain:  epoch 15, batch     5 | loss: 1.9043727MemoryTrain:  epoch 15, batch     6 | loss: 1.8182543MemoryTrain:  epoch  7, batch     7 | loss: 1.4654676
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 25.00%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 26.56%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 22.50%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 22.92%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 27.68%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 34.38%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 38.89%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 43.75%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 47.16%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 50.00%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 50.48%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 50.45%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 50.83%   [EVAL] batch:   15 | acc: 31.25%,  total acc: 49.61%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 50.74%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 51.74%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 51.97%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 54.37%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 56.55%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 58.24%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 60.05%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 61.72%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 63.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 64.18%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 65.51%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 66.29%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 67.46%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 68.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 69.56%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 70.51%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 71.40%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 72.06%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 72.50%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 73.09%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 73.65%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 74.18%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 74.20%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 74.06%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 74.24%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 73.96%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 73.40%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 73.58%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 74.03%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 74.32%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 74.73%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 75.13%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 75.51%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 75.88%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 75.86%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 75.84%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 75.59%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 75.46%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 75.57%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 74.78%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 75.11%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 75.11%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 75.21%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 75.61%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 76.01%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 75.60%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 77.78%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 77.50%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 76.70%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 76.04%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 76.92%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 79.69%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 80.88%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.60%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 82.19%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 82.74%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 82.39%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 82.34%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 82.55%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 82.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.41%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.03%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.60%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.13%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.09%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.33%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.74%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 86.03%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 85.71%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 85.59%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 85.81%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 85.86%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.22%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 86.56%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 86.89%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 87.05%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.21%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 87.07%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 87.22%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 87.23%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 86.97%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 86.72%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 86.99%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 86.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 86.89%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 87.02%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 87.15%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 87.38%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 87.39%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 87.39%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 86.64%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 86.23%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 86.15%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 86.17%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 85.89%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 85.42%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 84.28%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 83.37%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 82.29%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 81.34%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 80.70%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 79.98%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 79.91%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 79.75%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 79.51%   [EVAL] batch:   72 | acc: 62.50%,  total acc: 79.28%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 79.22%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 79.33%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 79.52%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 79.63%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 79.65%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 79.83%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 79.92%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 80.17%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 79.80%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 79.22%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 78.87%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 78.60%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 78.49%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 78.38%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 78.27%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 78.51%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 78.98%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 79.21%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 79.44%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 79.65%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 79.87%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 80.08%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 80.28%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 80.48%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 80.68%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 80.88%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 81.00%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 81.13%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 81.19%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 81.31%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 81.49%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 81.60%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 81.25%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 80.67%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 80.22%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 79.72%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 79.39%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 79.02%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 78.98%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 79.06%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 79.24%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 79.31%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 79.43%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 79.56%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 79.46%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 78.80%   [EVAL] batch:  120 | acc: 0.00%,  total acc: 78.15%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 77.51%   [EVAL] batch:  122 | acc: 0.00%,  total acc: 76.88%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 76.26%   [EVAL] batch:  124 | acc: 0.00%,  total acc: 75.65%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 75.50%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 75.15%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 74.85%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 74.47%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 74.38%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 74.14%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 74.24%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 74.44%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 74.53%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 74.54%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 74.59%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 74.77%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 74.86%   [EVAL] batch:  138 | acc: 50.00%,  total acc: 74.69%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 74.55%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 74.60%   [EVAL] batch:  141 | acc: 81.25%,  total acc: 74.65%   [EVAL] batch:  142 | acc: 62.50%,  total acc: 74.56%   [EVAL] batch:  143 | acc: 81.25%,  total acc: 74.61%   [EVAL] batch:  144 | acc: 68.75%,  total acc: 74.57%   [EVAL] batch:  145 | acc: 68.75%,  total acc: 74.53%   [EVAL] batch:  146 | acc: 62.50%,  total acc: 74.45%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 74.41%   [EVAL] batch:  148 | acc: 75.00%,  total acc: 74.41%   [EVAL] batch:  149 | acc: 68.75%,  total acc: 74.38%   [EVAL] batch:  150 | acc: 87.50%,  total acc: 74.46%   [EVAL] batch:  151 | acc: 81.25%,  total acc: 74.51%   [EVAL] batch:  152 | acc: 87.50%,  total acc: 74.59%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 74.72%   [EVAL] batch:  154 | acc: 62.50%,  total acc: 74.64%   [EVAL] batch:  155 | acc: 93.75%,  total acc: 74.76%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 74.72%   [EVAL] batch:  157 | acc: 68.75%,  total acc: 74.68%   [EVAL] batch:  158 | acc: 68.75%,  total acc: 74.65%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 74.57%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 74.50%   [EVAL] batch:  161 | acc: 62.50%,  total acc: 74.42%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 74.39%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 74.35%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 74.36%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 74.32%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 74.33%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 74.40%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 74.41%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 74.15%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 73.90%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 73.66%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 73.55%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 73.28%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 73.14%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 73.30%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 73.45%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 73.60%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 73.74%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 73.89%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 74.03%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 74.04%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 73.91%   [EVAL] batch:  183 | acc: 62.50%,  total acc: 73.85%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 73.82%   [EVAL] batch:  185 | acc: 62.50%,  total acc: 73.76%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 73.73%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 73.60%   [EVAL] batch:  188 | acc: 25.00%,  total acc: 73.35%   [EVAL] batch:  189 | acc: 31.25%,  total acc: 73.12%   [EVAL] batch:  190 | acc: 18.75%,  total acc: 72.84%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 72.56%   [EVAL] batch:  192 | acc: 25.00%,  total acc: 72.31%   [EVAL] batch:  193 | acc: 25.00%,  total acc: 72.07%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 72.12%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 72.10%   [EVAL] batch:  196 | acc: 87.50%,  total acc: 72.18%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 72.22%   [EVAL] batch:  198 | acc: 81.25%,  total acc: 72.27%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 72.28%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 72.23%   [EVAL] batch:  201 | acc: 31.25%,  total acc: 72.03%   [EVAL] batch:  202 | acc: 43.75%,  total acc: 71.89%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 71.86%   [EVAL] batch:  205 | acc: 37.50%,  total acc: 71.69%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 71.80%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 71.94%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 72.07%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 72.17%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 72.30%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 72.44%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 72.51%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 72.63%   [EVAL] batch:  214 | acc: 87.50%,  total acc: 72.70%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 72.83%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 72.96%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 73.08%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 73.20%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 73.32%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 73.51%   [EVAL] batch:  222 | acc: 87.50%,  total acc: 73.57%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 73.66%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 73.78%   [EVAL] batch:  225 | acc: 81.25%,  total acc: 73.81%   [EVAL] batch:  226 | acc: 68.75%,  total acc: 73.79%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 73.79%   [EVAL] batch:  228 | acc: 75.00%,  total acc: 73.80%   [EVAL] batch:  229 | acc: 50.00%,  total acc: 73.70%   [EVAL] batch:  230 | acc: 68.75%,  total acc: 73.67%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 73.73%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 73.82%   [EVAL] batch:  233 | acc: 87.50%,  total acc: 73.88%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 73.99%   [EVAL] batch:  235 | acc: 87.50%,  total acc: 74.05%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 74.13%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 74.16%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 74.22%   [EVAL] batch:  239 | acc: 56.25%,  total acc: 74.14%   [EVAL] batch:  240 | acc: 68.75%,  total acc: 74.12%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 74.17%   [EVAL] batch:  242 | acc: 56.25%,  total acc: 74.10%   [EVAL] batch:  243 | acc: 37.50%,  total acc: 73.95%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 74.03%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 74.09%   [EVAL] batch:  246 | acc: 68.75%,  total acc: 74.06%   [EVAL] batch:  247 | acc: 93.75%,  total acc: 74.14%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 74.25%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 74.35%   
cur_acc:  ['0.9514', '0.8006', '0.7629', '0.7560']
his_acc:  ['0.9514', '0.8715', '0.7999', '0.7435']
CurrentTrain: epoch 15, batch     0 | loss: 14.6199726CurrentTrain: epoch 15, batch     1 | loss: 13.8717560CurrentTrain: epoch 15, batch     2 | loss: 15.1881572CurrentTrain: epoch  1, batch     3 | loss: 15.9121945CurrentTrain: epoch 15, batch     0 | loss: 9.6675504CurrentTrain: epoch 15, batch     1 | loss: 14.4118030CurrentTrain: epoch 15, batch     2 | loss: 7.5887045CurrentTrain: epoch  1, batch     3 | loss: 10.5735573CurrentTrain: epoch 15, batch     0 | loss: 8.5281553CurrentTrain: epoch 15, batch     1 | loss: 11.8034652CurrentTrain: epoch 15, batch     2 | loss: 19.6930870CurrentTrain: epoch  1, batch     3 | loss: 6.1903850CurrentTrain: epoch 15, batch     0 | loss: 9.0803026CurrentTrain: epoch 15, batch     1 | loss: 15.1046396CurrentTrain: epoch 15, batch     2 | loss: 25.7113562CurrentTrain: epoch  1, batch     3 | loss: 8.4612549CurrentTrain: epoch 15, batch     0 | loss: 9.9860000CurrentTrain: epoch 15, batch     1 | loss: 14.5402060CurrentTrain: epoch 15, batch     2 | loss: 14.9381620CurrentTrain: epoch  1, batch     3 | loss: 7.7142202CurrentTrain: epoch 15, batch     0 | loss: 9.1387252CurrentTrain: epoch 15, batch     1 | loss: 6.3231744CurrentTrain: epoch 15, batch     2 | loss: 4.6876357CurrentTrain: epoch  1, batch     3 | loss: 5.6873277CurrentTrain: epoch 15, batch     0 | loss: 7.3820011CurrentTrain: epoch 15, batch     1 | loss: 7.7099831CurrentTrain: epoch 15, batch     2 | loss: 7.9251715CurrentTrain: epoch  1, batch     3 | loss: 7.3779963CurrentTrain: epoch 15, batch     0 | loss: 9.1414777CurrentTrain: epoch 15, batch     1 | loss: 7.9106467CurrentTrain: epoch 15, batch     2 | loss: 10.9433539CurrentTrain: epoch  1, batch     3 | loss: 6.6329664CurrentTrain: epoch 15, batch     0 | loss: 15.2387970CurrentTrain: epoch 15, batch     1 | loss: 10.8980316CurrentTrain: epoch 15, batch     2 | loss: 14.9831799CurrentTrain: epoch  1, batch     3 | loss: 6.2948413CurrentTrain: epoch 15, batch     0 | loss: 7.2539996CurrentTrain: epoch 15, batch     1 | loss: 7.1038167CurrentTrain: epoch 15, batch     2 | loss: 6.5435730CurrentTrain: epoch  1, batch     3 | loss: 5.4605989
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: alongside evan durbin and hugh gaitskell , he brought the thinking of john maynard keynes to the labour party , especially in relation to price determination .
Head Entity: hugh gaitskell
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: After serving as the mayor of Chicago, he became a prominent figure in the Democratic Party, advocating for social justice and economic reform.  
Head Entity: mayor of Chicago  
Tail Entity: Democratic Party  

Relation: member of political party  
Context: Throughout her career, she has been a staunch advocate for environmental policies as a member of the Green Party, pushing for sustainable practices at both local and national levels.  
Head Entity: she  
Tail Entity: Green Party  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: the original story of real life escape of betty mahmoody is depicted in the movie " not without my daughter " which itself was based on betty mahmoody 's book of the same name .
Head Entity: not without my daughter
Tail Entity: betty mahmoody
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film "the great gatsby" draws heavily from f. scott fitzgerald's classic novel, capturing the essence of the roaring twenties and the complexities of love and ambition.  
Head Entity: the great gatsby  
Tail Entity: f. scott fitzgerald  

Relation: after a work by  
Context: the animated feature "the lion king" was inspired by shakespeare's play "hamlet," incorporating themes of betrayal, revenge, and the struggle for power.  
Head Entity: the lion king  
Tail Entity: shakespeare
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: the influential victorian art critic john ruskin expounded theories about social justice in " unto this last " ( 1860 ) .
Head Entity: john ruskin
Tail Entity: unto this last
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: the renowned physicist albert einstein published his groundbreaking paper on the theory of relativity in "on the electrodynamics of moving bodies" (1905).  
Head Entity: albert einstein  
Tail Entity: on the electrodynamics of moving bodies  

Relation: notable work  
Context: the celebrated author j.k. rowling captivated readers with her first book in the harry potter series, "harry potter and the philosopher's stone" (1997).  
Head Entity: j.k. rowling  
Tail Entity: harry potter and the philosopher's stone  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: later he started a textile shop in thrissur city which flourished into kalyan silks , kalyan sarees and kalyan collections .
Head Entity: kalyan sarees
Tail Entity: thrissur
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: The tech giant Google has its headquarters in Mountain View, California, where it has developed numerous innovative products.  
Head Entity: Google  
Tail Entity: Mountain View  

Relation: headquarters location  
Context: The famous car manufacturer Toyota is headquartered in Toyota City, Japan, where it was originally founded.  
Head Entity: Toyota  
Tail Entity: Toyota City  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most recognized as the wolf, which is classified under the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of flowering plants, the family Rosaceae encompasses a variety of species, including roses and strawberries, which are categorized under the order Rosales.  
Head Entity: Rosaceae  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: he later collaborated with vito volterra and explored laplace transforms and other parts of functional analysis .
Head Entity: vito volterra
Tail Entity: functional analysis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: Marie Curie conducted pioneering research on radioactivity, which laid the groundwork for advancements in nuclear physics.  
Head Entity: Marie Curie  
Tail Entity: nuclear physics  

Relation: field of work  
Context: The company specializes in artificial intelligence and machine learning, providing innovative solutions for various industries.  
Head Entity: The company  
Tail Entity: artificial intelligence
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: this early involvement in radio led him to buy many small , under performing radio stations beginning with wcos in columbia , south carolina , and turn them around with new formats and improved management .
Head Entity: wcos
Tail Entity: columbia , south carolina
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: The new television station, KXYZ, has been granted permission to broadcast to the entire metropolitan area of San Francisco, ensuring that residents can access a variety of programming.  
Head Entity: KXYZ  
Tail Entity: San Francisco  

Relation: licensed to broadcast to  
Context: After acquiring the rights, the radio station WABC was officially licensed to broadcast to the listeners in the greater New York City area, expanding its reach significantly.  
Head Entity: WABC  
Tail Entity: greater New York City area  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: ngc 192 is a type sba spiral galaxy located in the constellation cetus .
Head Entity: ngc 192
Tail Entity: cetus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: the orion nebula is located in the constellation orion and is one of the brightest nebulae visible to the naked eye.  
Head Entity: orion nebula  
Tail Entity: orion  

Relation: constellation  
Context: the star sirius is part of the constellation canis major, which is known for its prominence in the night sky.  
Head Entity: sirius  
Tail Entity: canis major  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: services to the island run from san juan 's luis muñoz marín international airport , ceiba airport or isla grande airport ( 20- to 30-minute flight ) .
Head Entity: isla grande airport
Tail Entity: san juan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The city of Barcelona is well-connected to various destinations through the El Prat Airport, which offers flights to numerous European cities.  
Head Entity: El Prat Airport  
Tail Entity: Barcelona  

Relation: place served by transport hub  
Context: The central train station in Zurich provides access to multiple Swiss cities and international destinations, making it a key transport hub in the region.  
Head Entity: Zurich central train station  
Tail Entity: Zurich  
MemoryTrain:  epoch 15, batch     0 | loss: 5.3292231MemoryTrain:  epoch 15, batch     1 | loss: 4.3005820MemoryTrain:  epoch 15, batch     2 | loss: 5.8647999MemoryTrain:  epoch 15, batch     3 | loss: 2.4137440MemoryTrain:  epoch 15, batch     4 | loss: 2.6242140MemoryTrain:  epoch 15, batch     5 | loss: 2.5173590MemoryTrain:  epoch 15, batch     6 | loss: 2.7374745MemoryTrain:  epoch 15, batch     7 | loss: 4.0361410MemoryTrain:  epoch 15, batch     8 | loss: 3.8310186MemoryTrain:  epoch  5, batch     9 | loss: 8.8088252MemoryTrain:  epoch 15, batch     0 | loss: 2.3008281MemoryTrain:  epoch 15, batch     1 | loss: 2.9711495MemoryTrain:  epoch 15, batch     2 | loss: 2.1803941MemoryTrain:  epoch 15, batch     3 | loss: 2.7084347MemoryTrain:  epoch 15, batch     4 | loss: 2.1694408MemoryTrain:  epoch 15, batch     5 | loss: 2.7363957MemoryTrain:  epoch 15, batch     6 | loss: 1.9313277MemoryTrain:  epoch 15, batch     7 | loss: 3.8600624MemoryTrain:  epoch 15, batch     8 | loss: 2.6805501MemoryTrain:  epoch  5, batch     9 | loss: 8.0281606MemoryTrain:  epoch 15, batch     0 | loss: 2.1038382MemoryTrain:  epoch 15, batch     1 | loss: 4.2757600MemoryTrain:  epoch 15, batch     2 | loss: 2.1806534MemoryTrain:  epoch 15, batch     3 | loss: 5.2651036MemoryTrain:  epoch 15, batch     4 | loss: 2.8482524MemoryTrain:  epoch 15, batch     5 | loss: 1.9415504MemoryTrain:  epoch 15, batch     6 | loss: 1.9767067MemoryTrain:  epoch 15, batch     7 | loss: 1.9092087MemoryTrain:  epoch 15, batch     8 | loss: 4.0744890MemoryTrain:  epoch  5, batch     9 | loss: 8.7859083MemoryTrain:  epoch 15, batch     0 | loss: 3.7930453MemoryTrain:  epoch 15, batch     1 | loss: 2.0223331MemoryTrain:  epoch 15, batch     2 | loss: 4.3485708MemoryTrain:  epoch 15, batch     3 | loss: 1.7869199MemoryTrain:  epoch 15, batch     4 | loss: 2.6922748MemoryTrain:  epoch 15, batch     5 | loss: 1.4141076MemoryTrain:  epoch 15, batch     6 | loss: 1.4526292MemoryTrain:  epoch 15, batch     7 | loss: 1.7757743MemoryTrain:  epoch 15, batch     8 | loss: 1.8001001MemoryTrain:  epoch  5, batch     9 | loss: 8.1909482MemoryTrain:  epoch 15, batch     0 | loss: 2.8067113MemoryTrain:  epoch 15, batch     1 | loss: 2.5792535MemoryTrain:  epoch 15, batch     2 | loss: 1.7629025MemoryTrain:  epoch 15, batch     3 | loss: 1.5289345MemoryTrain:  epoch 15, batch     4 | loss: 3.1309766MemoryTrain:  epoch 15, batch     5 | loss: 1.6078891MemoryTrain:  epoch 15, batch     6 | loss: 2.5842316MemoryTrain:  epoch 15, batch     7 | loss: 1.8315118MemoryTrain:  epoch 15, batch     8 | loss: 1.5198500MemoryTrain:  epoch  5, batch     9 | loss: 8.2705939MemoryTrain:  epoch 15, batch     0 | loss: 3.7641587MemoryTrain:  epoch 15, batch     1 | loss: 1.6915846MemoryTrain:  epoch 15, batch     2 | loss: 3.7622723MemoryTrain:  epoch 15, batch     3 | loss: 1.5954320MemoryTrain:  epoch 15, batch     4 | loss: 6.0465151MemoryTrain:  epoch 15, batch     5 | loss: 6.9814915MemoryTrain:  epoch 15, batch     6 | loss: 1.8733155MemoryTrain:  epoch 15, batch     7 | loss: 1.6643776MemoryTrain:  epoch 15, batch     8 | loss: 1.6544020MemoryTrain:  epoch  5, batch     9 | loss: 8.1370807MemoryTrain:  epoch 15, batch     0 | loss: 1.7342246MemoryTrain:  epoch 15, batch     1 | loss: 3.9227845MemoryTrain:  epoch 15, batch     2 | loss: 1.7124566MemoryTrain:  epoch 15, batch     3 | loss: 1.7009739MemoryTrain:  epoch 15, batch     4 | loss: 1.5325484MemoryTrain:  epoch 15, batch     5 | loss: 1.7603996MemoryTrain:  epoch 15, batch     6 | loss: 1.7588936MemoryTrain:  epoch 15, batch     7 | loss: 1.9852333MemoryTrain:  epoch 15, batch     8 | loss: 1.8326787MemoryTrain:  epoch  5, batch     9 | loss: 11.1806186MemoryTrain:  epoch 15, batch     0 | loss: 4.1522333MemoryTrain:  epoch 15, batch     1 | loss: 1.9678988MemoryTrain:  epoch 15, batch     2 | loss: 1.5338903MemoryTrain:  epoch 15, batch     3 | loss: 3.7249475MemoryTrain:  epoch 15, batch     4 | loss: 1.6351199MemoryTrain:  epoch 15, batch     5 | loss: 1.5907016MemoryTrain:  epoch 15, batch     6 | loss: 8.9651104MemoryTrain:  epoch 15, batch     7 | loss: 1.6393912MemoryTrain:  epoch 15, batch     8 | loss: 3.8653444MemoryTrain:  epoch  5, batch     9 | loss: 7.8585292MemoryTrain:  epoch 15, batch     0 | loss: 1.4460971MemoryTrain:  epoch 15, batch     1 | loss: 3.5173905MemoryTrain:  epoch 15, batch     2 | loss: 2.5737554MemoryTrain:  epoch 15, batch     3 | loss: 3.5628995MemoryTrain:  epoch 15, batch     4 | loss: 3.5563760MemoryTrain:  epoch 15, batch     5 | loss: 1.7926505MemoryTrain:  epoch 15, batch     6 | loss: 2.6568758MemoryTrain:  epoch 15, batch     7 | loss: 2.5432979MemoryTrain:  epoch 15, batch     8 | loss: 1.4590772MemoryTrain:  epoch  5, batch     9 | loss: 7.9851111MemoryTrain:  epoch 15, batch     0 | loss: 3.9624510MemoryTrain:  epoch 15, batch     1 | loss: 2.3386292MemoryTrain:  epoch 15, batch     2 | loss: 1.7864828MemoryTrain:  epoch 15, batch     3 | loss: 1.4774621MemoryTrain:  epoch 15, batch     4 | loss: 1.5928169MemoryTrain:  epoch 15, batch     5 | loss: 2.2295938MemoryTrain:  epoch 15, batch     6 | loss: 2.3666484MemoryTrain:  epoch 15, batch     7 | loss: 1.6206936MemoryTrain:  epoch 15, batch     8 | loss: 1.3700301MemoryTrain:  epoch  5, batch     9 | loss: 7.7419491
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 96.43%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 97.22%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 97.16%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 95.83%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 95.19%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 93.75%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 92.92%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 91.41%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 89.71%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 88.54%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 87.17%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 85.31%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 84.66%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 83.42%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:   24 | acc: 43.75%,  total acc: 81.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.45%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.10%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.71%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.27%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 84.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.28%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 85.35%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 85.23%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 84.93%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 85.18%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 84.90%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 84.80%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 84.70%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.10%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.82%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 86.16%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 86.48%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 86.65%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 86.94%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 87.23%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 87.76%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.01%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 88.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 88.36%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 88.46%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 88.56%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 88.77%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 88.95%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 89.04%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 89.12%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 89.09%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 89.17%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 89.34%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 89.21%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 88.59%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 73.75%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 71.02%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 69.71%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 71.43%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 72.50%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 73.44%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 74.63%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 75.69%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 76.32%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 76.56%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 76.79%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 76.42%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 76.36%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 76.30%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 76.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.64%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.47%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.24%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 79.74%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.42%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.05%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 81.45%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.01%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 81.62%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 81.43%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 81.60%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 81.93%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 82.07%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 82.53%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.97%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 83.38%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 83.63%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 83.87%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 83.66%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 83.89%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 83.56%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 83.11%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 83.07%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 83.16%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 82.75%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 82.72%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 83.02%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 82.87%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 82.50%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 82.70%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 82.57%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 82.00%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 81.57%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 81.46%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 81.45%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 81.15%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 80.56%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 79.49%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 78.65%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 77.65%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 76.77%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 76.01%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 75.36%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 75.36%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 75.26%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:   72 | acc: 62.50%,  total acc: 74.83%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 74.83%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 75.33%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 75.49%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 75.56%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 75.79%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 75.94%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 76.23%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 76.07%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 75.83%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 75.67%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 75.66%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 75.65%   [EVAL] batch:   86 | acc: 75.00%,  total acc: 75.65%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 75.78%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 75.98%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 76.25%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 76.37%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 76.56%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 76.75%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 76.99%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 77.24%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 77.47%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 77.71%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 77.93%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 78.16%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 78.38%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 78.59%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 78.74%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 78.88%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 79.03%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 79.23%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 79.36%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 79.03%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 78.47%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 77.98%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 77.39%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 76.97%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 76.56%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 76.55%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 76.59%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 76.79%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 76.89%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 77.03%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 77.17%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 77.10%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 76.46%   [EVAL] batch:  120 | acc: 0.00%,  total acc: 75.83%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 75.20%   [EVAL] batch:  122 | acc: 0.00%,  total acc: 74.59%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 73.99%   [EVAL] batch:  124 | acc: 0.00%,  total acc: 73.40%   [EVAL] batch:  125 | acc: 43.75%,  total acc: 73.16%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 72.83%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 72.56%   [EVAL] batch:  128 | acc: 18.75%,  total acc: 72.14%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 72.02%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 71.80%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 71.92%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 72.13%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 72.20%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 72.22%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 72.29%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 72.49%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 72.55%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 72.44%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 72.32%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 72.43%   [EVAL] batch:  141 | acc: 81.25%,  total acc: 72.49%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 72.47%   [EVAL] batch:  143 | acc: 81.25%,  total acc: 72.53%   [EVAL] batch:  144 | acc: 43.75%,  total acc: 72.33%   [EVAL] batch:  145 | acc: 50.00%,  total acc: 72.17%   [EVAL] batch:  146 | acc: 50.00%,  total acc: 72.02%   [EVAL] batch:  147 | acc: 37.50%,  total acc: 71.79%   [EVAL] batch:  148 | acc: 62.50%,  total acc: 71.73%   [EVAL] batch:  149 | acc: 50.00%,  total acc: 71.58%   [EVAL] batch:  150 | acc: 87.50%,  total acc: 71.69%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 71.83%   [EVAL] batch:  152 | acc: 87.50%,  total acc: 71.94%   [EVAL] batch:  153 | acc: 100.00%,  total acc: 72.12%   [EVAL] batch:  154 | acc: 75.00%,  total acc: 72.14%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 72.24%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 72.21%   [EVAL] batch:  157 | acc: 62.50%,  total acc: 72.15%   [EVAL] batch:  158 | acc: 62.50%,  total acc: 72.09%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 72.03%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 71.97%   [EVAL] batch:  161 | acc: 68.75%,  total acc: 71.95%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 71.93%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 71.89%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 71.89%   [EVAL] batch:  167 | acc: 81.25%,  total acc: 71.95%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 71.93%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 71.69%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 71.45%   [EVAL] batch:  171 | acc: 25.00%,  total acc: 71.18%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 71.03%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 70.76%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 70.64%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 70.81%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 70.97%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 71.14%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 71.30%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 71.46%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 71.62%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 71.63%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 71.55%   [EVAL] batch:  183 | acc: 56.25%,  total acc: 71.47%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 71.45%   [EVAL] batch:  185 | acc: 62.50%,  total acc: 71.40%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 71.39%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 71.24%   [EVAL] batch:  188 | acc: 12.50%,  total acc: 70.93%   [EVAL] batch:  189 | acc: 31.25%,  total acc: 70.72%   [EVAL] batch:  190 | acc: 18.75%,  total acc: 70.45%   [EVAL] batch:  191 | acc: 12.50%,  total acc: 70.15%   [EVAL] batch:  192 | acc: 18.75%,  total acc: 69.88%   [EVAL] batch:  193 | acc: 25.00%,  total acc: 69.65%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 69.68%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 69.64%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 69.70%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 69.76%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 69.79%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 69.81%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 69.78%   [EVAL] batch:  201 | acc: 31.25%,  total acc: 69.59%   [EVAL] batch:  202 | acc: 37.50%,  total acc: 69.43%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 69.42%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 69.42%   [EVAL] batch:  205 | acc: 37.50%,  total acc: 69.27%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 69.38%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 69.68%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 69.93%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 70.08%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 70.19%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 70.30%   [EVAL] batch:  214 | acc: 93.75%,  total acc: 70.41%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 70.68%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 70.81%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 70.95%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 71.08%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 71.21%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 71.31%   [EVAL] batch:  222 | acc: 87.50%,  total acc: 71.38%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 71.48%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 71.61%   [EVAL] batch:  225 | acc: 87.50%,  total acc: 71.68%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 71.75%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 71.77%   [EVAL] batch:  228 | acc: 62.50%,  total acc: 71.72%   [EVAL] batch:  229 | acc: 50.00%,  total acc: 71.63%   [EVAL] batch:  230 | acc: 68.75%,  total acc: 71.62%   [EVAL] batch:  231 | acc: 81.25%,  total acc: 71.66%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 71.75%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 71.85%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 71.97%   [EVAL] batch:  235 | acc: 87.50%,  total acc: 72.03%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 72.07%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 72.11%   [EVAL] batch:  238 | acc: 68.75%,  total acc: 72.10%   [EVAL] batch:  239 | acc: 62.50%,  total acc: 72.06%   [EVAL] batch:  240 | acc: 62.50%,  total acc: 72.02%   [EVAL] batch:  241 | acc: 68.75%,  total acc: 72.00%   [EVAL] batch:  242 | acc: 56.25%,  total acc: 71.94%   [EVAL] batch:  243 | acc: 37.50%,  total acc: 71.80%   [EVAL] batch:  244 | acc: 75.00%,  total acc: 71.81%   [EVAL] batch:  245 | acc: 62.50%,  total acc: 71.77%   [EVAL] batch:  246 | acc: 56.25%,  total acc: 71.71%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 71.75%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 71.84%   [EVAL] batch:  249 | acc: 62.50%,  total acc: 71.80%   [EVAL] batch:  250 | acc: 100.00%,  total acc: 71.91%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 72.00%   [EVAL] batch:  252 | acc: 100.00%,  total acc: 72.11%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 72.17%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 72.25%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 72.36%   [EVAL] batch:  256 | acc: 100.00%,  total acc: 72.47%   [EVAL] batch:  257 | acc: 100.00%,  total acc: 72.58%   [EVAL] batch:  258 | acc: 100.00%,  total acc: 72.68%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 72.76%   [EVAL] batch:  260 | acc: 100.00%,  total acc: 72.87%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 72.90%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 72.96%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 72.96%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 73.00%   [EVAL] batch:  265 | acc: 68.75%,  total acc: 72.98%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 72.94%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:  268 | acc: 62.50%,  total acc: 72.89%   [EVAL] batch:  269 | acc: 50.00%,  total acc: 72.80%   [EVAL] batch:  270 | acc: 93.75%,  total acc: 72.88%   [EVAL] batch:  271 | acc: 62.50%,  total acc: 72.84%   [EVAL] batch:  272 | acc: 56.25%,  total acc: 72.78%   [EVAL] batch:  273 | acc: 81.25%,  total acc: 72.81%   [EVAL] batch:  274 | acc: 43.75%,  total acc: 72.70%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 72.80%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 72.90%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 73.00%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 73.10%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 73.19%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 73.29%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 73.34%   [EVAL] batch:  282 | acc: 81.25%,  total acc: 73.37%   [EVAL] batch:  283 | acc: 75.00%,  total acc: 73.37%   [EVAL] batch:  284 | acc: 93.75%,  total acc: 73.44%   [EVAL] batch:  285 | acc: 75.00%,  total acc: 73.45%   [EVAL] batch:  286 | acc: 81.25%,  total acc: 73.48%   [EVAL] batch:  287 | acc: 81.25%,  total acc: 73.50%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 73.59%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 73.69%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 73.78%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 73.87%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 73.95%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 74.02%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 74.20%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 74.28%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 74.37%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 74.46%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 74.54%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 74.61%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 74.67%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 74.73%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 74.81%   [EVAL] batch:  304 | acc: 87.50%,  total acc: 74.86%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 74.94%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 75.06%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 75.10%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 75.16%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 75.24%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 75.26%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 75.18%   
cur_acc:  ['0.9514', '0.8006', '0.7629', '0.7560', '0.8859']
his_acc:  ['0.9514', '0.8715', '0.7999', '0.7435', '0.7518']
CurrentTrain: epoch 15, batch     0 | loss: 19.1292547CurrentTrain: epoch 15, batch     1 | loss: 26.8865532CurrentTrain: epoch 15, batch     2 | loss: 12.1019110CurrentTrain: epoch  1, batch     3 | loss: 27.0553275CurrentTrain: epoch 15, batch     0 | loss: 16.1002423CurrentTrain: epoch 15, batch     1 | loss: 10.7801676CurrentTrain: epoch 15, batch     2 | loss: 12.3007112CurrentTrain: epoch  1, batch     3 | loss: 6.2272702CurrentTrain: epoch 15, batch     0 | loss: 12.4736437CurrentTrain: epoch 15, batch     1 | loss: 15.0616601CurrentTrain: epoch 15, batch     2 | loss: 11.5949682CurrentTrain: epoch  1, batch     3 | loss: 6.8431733CurrentTrain: epoch 15, batch     0 | loss: 17.9099788CurrentTrain: epoch 15, batch     1 | loss: 7.8794754CurrentTrain: epoch 15, batch     2 | loss: 9.7152844CurrentTrain: epoch  1, batch     3 | loss: 10.7812835CurrentTrain: epoch 15, batch     0 | loss: 9.0412451CurrentTrain: epoch 15, batch     1 | loss: 12.6710944CurrentTrain: epoch 15, batch     2 | loss: 9.9517120CurrentTrain: epoch  1, batch     3 | loss: 7.5210909CurrentTrain: epoch 15, batch     0 | loss: 13.2390235CurrentTrain: epoch 15, batch     1 | loss: 8.0823287CurrentTrain: epoch 15, batch     2 | loss: 9.2888340CurrentTrain: epoch  1, batch     3 | loss: 7.2344238CurrentTrain: epoch 15, batch     0 | loss: 8.6854907CurrentTrain: epoch 15, batch     1 | loss: 6.7211953CurrentTrain: epoch 15, batch     2 | loss: 14.3872951CurrentTrain: epoch  1, batch     3 | loss: 8.3683510CurrentTrain: epoch 15, batch     0 | loss: 9.8371373CurrentTrain: epoch 15, batch     1 | loss: 9.3476130CurrentTrain: epoch 15, batch     2 | loss: 6.4187001CurrentTrain: epoch  1, batch     3 | loss: 7.0635797CurrentTrain: epoch 15, batch     0 | loss: 9.7561376CurrentTrain: epoch 15, batch     1 | loss: 11.5292601CurrentTrain: epoch 15, batch     2 | loss: 7.8465880CurrentTrain: epoch  1, batch     3 | loss: 6.8649418CurrentTrain: epoch 15, batch     0 | loss: 7.2426229CurrentTrain: epoch 15, batch     1 | loss: 6.5461277CurrentTrain: epoch 15, batch     2 | loss: 7.3572948CurrentTrain: epoch  1, batch     3 | loss: 7.6785218
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: it is situated in the historic lower lusatia region , about south of beeskow , and north of cottbus .
Head Entity: cottbus
Tail Entity: lower lusatia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The ancient ruins are found near the banks of the river Nile, surrounded by vast deserts and fertile lands.  
Head Entity: Nile  
Tail Entity: deserts  

Relation: located on terrain feature  
Context: The village is nestled in the foothills of the Rocky Mountains, offering breathtaking views of the surrounding landscape.  
Head Entity: village  
Tail Entity: Rocky Mountains  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, also known as cop26, took place in glasgow, scotland, where numerous world leaders gathered to discuss climate action.  
Head Entity: united nations climate change conference  
Tail Entity: cop26  

Relation: participant of  
Context: the world health organization held a global summit on health equity, bringing together experts and leaders from various countries to address health disparities.  
Head Entity: world health organization  
Tail Entity: global summit on health equity  
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: luna subsequently had a supporting role in the comedy film " gravy " , directed by james roday , which was released in october 2015 .
Head Entity: gravy
Tail Entity: james roday
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: the acclaimed drama series "the crown" was brought to life by the visionary director peter morgan, who skillfully navigated the complexities of the royal family.  
Head Entity: the crown  
Tail Entity: peter morgan  

Relation: director  
Context: in the thrilling action movie "inception," the brilliant director christopher nolan crafted a mind-bending narrative that captivated audiences worldwide.  
Head Entity: inception  
Tail Entity: christopher nolan  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: the station is owned by the hearst television subsidiary of the hearst corporation .
Head Entity: hearst television
Tail Entity: hearst corporation
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: the famous painting is owned by the metropolitan museum of art in new york.  
Head Entity: metropolitan museum of art  
Tail Entity: famous painting  

Relation: owned by  
Context: the luxury car brand is owned by a major automotive conglomerate based in germany.  
Head Entity: major automotive conglomerate  
Tail Entity: luxury car brand  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: remains of taxa that can be confidently assigned to istiodactylidae have been found in the uk and china , in rocks dating from the early cretaceous period ( barremian to aptian stage ) .
Head Entity: barremian
Tail Entity: early cretaceous
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The Great Wall of China is a series of fortifications made of various materials, and it is part of the UNESCO World Heritage Sites.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Sites  

Relation: part of  
Context: The human heart consists of four chambers, and the left atrium is part of the heart's structure.  
Head Entity: left atrium  
Tail Entity: heart
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: it served as the home of k league side gyeongnam fc and national league side changwon city fc prior to the construction of the changwon football center in 2009 .
Head Entity: changwon football center
Tail Entity: gyeongnam fc
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was once the residence of the famous artist, and it now serves as a gallery showcasing his works, attracting visitors from all over the country.  
Head Entity: historic building  
Tail Entity: famous artist  

Relation: occupant  
Context: After the renovation, the old factory was transformed into a vibrant co-working space, where numerous startups and freelancers now thrive.  
Head Entity: old factory  
Tail Entity: startups
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: the art nouveau style was brought to italy by figures such as giuseppe sommaruga and ernesto basile ( who respectively designed the palazzo castiglioni and expanded the palazzo montecitorio in rome ) .
Head Entity: palazzo castiglioni
Tail Entity: giuseppe sommaruga
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: The iconic Sydney Opera House was designed by Danish architect Jørn Utzon, who won the design competition in 1957.  
Head Entity: Sydney Opera House  
Tail Entity: Jørn Utzon  

Relation: architect  
Context: Frank Lloyd Wright, an influential American architect, is best known for designing the Fallingwater house, which harmonizes beautifully with its natural surroundings.  
Head Entity: Fallingwater  
Tail Entity: Frank Lloyd Wright  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: in 2001 brian haw set up the parliament square peace campaign outside the houses of parliament in london .
Head Entity: brian haw
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: After moving to New York City in 2010, Sarah found her dream apartment in Brooklyn.  
Head Entity: Sarah  
Tail Entity: Brooklyn  

Relation: residence  
Context: The famous author lived in a quaint cottage in the countryside for many years before relocating to the city.  
Head Entity: The famous author  
Tail Entity: the city  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Ludwig van Beethoven created some of the most celebrated symphonies in classical music history.  
Head Entity: symphonies  
Tail Entity: Ludwig van Beethoven  

Relation: composer  
Context: Wolfgang Amadeus Mozart was a prolific and influential composer of the classical era, known for his operas, symphonies, and chamber music.  
Head Entity: operas  
Tail Entity: Wolfgang Amadeus Mozart  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: one polish archer qualified for the women 's individual recurve by obtaining one of the eight olympic places available from the 2015 world archery championships in copenhagen , denmark .
Head Entity: 2015 world archery championships
Tail Entity: copenhagen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: The annual music festival will take place in the vibrant city of Austin, Texas, attracting thousands of fans from around the world.  
Head Entity: annual music festival  
Tail Entity: Austin, Texas  

Relation: location  
Context: The historic battle was fought near the banks of the river in Gettysburg, Pennsylvania, which is now a national park.  
Head Entity: historic battle  
Tail Entity: Gettysburg, Pennsylvania  
MemoryTrain:  epoch 15, batch     0 | loss: 2.8073060MemoryTrain:  epoch 15, batch     1 | loss: 3.8228524MemoryTrain:  epoch 15, batch     2 | loss: 3.4562773MemoryTrain:  epoch 15, batch     3 | loss: 3.7165081MemoryTrain:  epoch 15, batch     4 | loss: 4.6697032MemoryTrain:  epoch 15, batch     5 | loss: 3.6207568MemoryTrain:  epoch 15, batch     6 | loss: 3.3526675MemoryTrain:  epoch 15, batch     7 | loss: 2.8549359MemoryTrain:  epoch 15, batch     8 | loss: 2.9775010MemoryTrain:  epoch 15, batch     9 | loss: 4.9172660MemoryTrain:  epoch 15, batch    10 | loss: 2.3678984MemoryTrain:  epoch  3, batch    11 | loss: 9.9603062MemoryTrain:  epoch 15, batch     0 | loss: 3.5918967MemoryTrain:  epoch 15, batch     1 | loss: 2.3653013MemoryTrain:  epoch 15, batch     2 | loss: 2.8822264MemoryTrain:  epoch 15, batch     3 | loss: 2.3619130MemoryTrain:  epoch 15, batch     4 | loss: 2.9754666MemoryTrain:  epoch 15, batch     5 | loss: 3.0800596MemoryTrain:  epoch 15, batch     6 | loss: 3.4893126MemoryTrain:  epoch 15, batch     7 | loss: 2.3581042MemoryTrain:  epoch 15, batch     8 | loss: 2.7128468MemoryTrain:  epoch 15, batch     9 | loss: 2.8412045MemoryTrain:  epoch 15, batch    10 | loss: 2.2774225MemoryTrain:  epoch  3, batch    11 | loss: 11.4699350MemoryTrain:  epoch 15, batch     0 | loss: 1.6364879MemoryTrain:  epoch 15, batch     1 | loss: 2.2591343MemoryTrain:  epoch 15, batch     2 | loss: 1.7407578MemoryTrain:  epoch 15, batch     3 | loss: 2.1921251MemoryTrain:  epoch 15, batch     4 | loss: 3.3460205MemoryTrain:  epoch 15, batch     5 | loss: 4.4756397MemoryTrain:  epoch 15, batch     6 | loss: 3.2079282MemoryTrain:  epoch 15, batch     7 | loss: 2.7706464MemoryTrain:  epoch 15, batch     8 | loss: 2.2777129MemoryTrain:  epoch 15, batch     9 | loss: 1.9707688MemoryTrain:  epoch 15, batch    10 | loss: 4.0706420MemoryTrain:  epoch  3, batch    11 | loss: 9.5744615MemoryTrain:  epoch 15, batch     0 | loss: 2.3669028MemoryTrain:  epoch 15, batch     1 | loss: 2.1355611MemoryTrain:  epoch 15, batch     2 | loss: 2.0961638MemoryTrain:  epoch 15, batch     3 | loss: 4.1934358MemoryTrain:  epoch 15, batch     4 | loss: 4.1088416MemoryTrain:  epoch 15, batch     5 | loss: 4.4238585MemoryTrain:  epoch 15, batch     6 | loss: 2.0257919MemoryTrain:  epoch 15, batch     7 | loss: 1.9871629MemoryTrain:  epoch 15, batch     8 | loss: 2.6022720MemoryTrain:  epoch 15, batch     9 | loss: 1.7934073MemoryTrain:  epoch 15, batch    10 | loss: 4.4644325MemoryTrain:  epoch  3, batch    11 | loss: 10.0880818MemoryTrain:  epoch 15, batch     0 | loss: 4.7559345MemoryTrain:  epoch 15, batch     1 | loss: 1.5874796MemoryTrain:  epoch 15, batch     2 | loss: 1.7080743MemoryTrain:  epoch 15, batch     3 | loss: 1.7755714MemoryTrain:  epoch 15, batch     4 | loss: 2.1749583MemoryTrain:  epoch 15, batch     5 | loss: 2.0048640MemoryTrain:  epoch 15, batch     6 | loss: 1.8699696MemoryTrain:  epoch 15, batch     7 | loss: 4.1534995MemoryTrain:  epoch 15, batch     8 | loss: 3.9526153MemoryTrain:  epoch 15, batch     9 | loss: 1.7269177MemoryTrain:  epoch 15, batch    10 | loss: 4.0067430MemoryTrain:  epoch  3, batch    11 | loss: 10.1406592MemoryTrain:  epoch 15, batch     0 | loss: 1.7198822MemoryTrain:  epoch 15, batch     1 | loss: 1.5334844MemoryTrain:  epoch 15, batch     2 | loss: 2.7871664MemoryTrain:  epoch 15, batch     3 | loss: 1.8644087MemoryTrain:  epoch 15, batch     4 | loss: 4.4072065MemoryTrain:  epoch 15, batch     5 | loss: 4.0095690MemoryTrain:  epoch 15, batch     6 | loss: 1.7577789MemoryTrain:  epoch 15, batch     7 | loss: 2.2518898MemoryTrain:  epoch 15, batch     8 | loss: 1.6269766MemoryTrain:  epoch 15, batch     9 | loss: 1.8680695MemoryTrain:  epoch 15, batch    10 | loss: 1.9168192MemoryTrain:  epoch  3, batch    11 | loss: 9.8042326MemoryTrain:  epoch 15, batch     0 | loss: 1.7890453MemoryTrain:  epoch 15, batch     1 | loss: 2.5103304MemoryTrain:  epoch 15, batch     2 | loss: 1.5835279MemoryTrain:  epoch 15, batch     3 | loss: 2.4476386MemoryTrain:  epoch 15, batch     4 | loss: 1.7405140MemoryTrain:  epoch 15, batch     5 | loss: 4.0511091MemoryTrain:  epoch 15, batch     6 | loss: 2.7386447MemoryTrain:  epoch 15, batch     7 | loss: 1.6237360MemoryTrain:  epoch 15, batch     8 | loss: 3.0555392MemoryTrain:  epoch 15, batch     9 | loss: 1.6184745MemoryTrain:  epoch 15, batch    10 | loss: 4.4143180MemoryTrain:  epoch  3, batch    11 | loss: 9.6965201MemoryTrain:  epoch 15, batch     0 | loss: 1.6346388MemoryTrain:  epoch 15, batch     1 | loss: 3.8399195MemoryTrain:  epoch 15, batch     2 | loss: 1.6103492MemoryTrain:  epoch 15, batch     3 | loss: 2.3040534MemoryTrain:  epoch 15, batch     4 | loss: 1.5707590MemoryTrain:  epoch 15, batch     5 | loss: 1.7387031MemoryTrain:  epoch 15, batch     6 | loss: 2.7776101MemoryTrain:  epoch 15, batch     7 | loss: 1.5314585MemoryTrain:  epoch 15, batch     8 | loss: 3.6599779MemoryTrain:  epoch 15, batch     9 | loss: 1.4428297MemoryTrain:  epoch 15, batch    10 | loss: 2.3514020MemoryTrain:  epoch  3, batch    11 | loss: 10.8558341MemoryTrain:  epoch 15, batch     0 | loss: 3.8328717MemoryTrain:  epoch 15, batch     1 | loss: 3.8231019MemoryTrain:  epoch 15, batch     2 | loss: 1.9458738MemoryTrain:  epoch 15, batch     3 | loss: 3.4735832MemoryTrain:  epoch 15, batch     4 | loss: 1.5803606MemoryTrain:  epoch 15, batch     5 | loss: 1.4330923MemoryTrain:  epoch 15, batch     6 | loss: 1.6914458MemoryTrain:  epoch 15, batch     7 | loss: 1.7351619MemoryTrain:  epoch 15, batch     8 | loss: 1.5058239MemoryTrain:  epoch 15, batch     9 | loss: 1.9410034MemoryTrain:  epoch 15, batch    10 | loss: 1.3946660MemoryTrain:  epoch  3, batch    11 | loss: 10.6198446MemoryTrain:  epoch 15, batch     0 | loss: 1.4809712MemoryTrain:  epoch 15, batch     1 | loss: 1.6417477MemoryTrain:  epoch 15, batch     2 | loss: 1.9333072MemoryTrain:  epoch 15, batch     3 | loss: 3.8588737MemoryTrain:  epoch 15, batch     4 | loss: 1.4979196MemoryTrain:  epoch 15, batch     5 | loss: 1.6038151MemoryTrain:  epoch 15, batch     6 | loss: 1.4354157MemoryTrain:  epoch 15, batch     7 | loss: 1.3519377MemoryTrain:  epoch 15, batch     8 | loss: 2.0676552MemoryTrain:  epoch 15, batch     9 | loss: 1.7934905MemoryTrain:  epoch 15, batch    10 | loss: 1.8357409MemoryTrain:  epoch  3, batch    11 | loss: 10.1639476
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 32.81%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 36.25%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 38.54%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 41.96%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 46.88%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 50.69%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 53.75%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 55.11%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 58.17%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 60.27%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 64.45%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 65.81%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 67.01%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 66.78%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 64.69%   [EVAL] batch:   20 | acc: 6.25%,  total acc: 61.90%   [EVAL] batch:   21 | acc: 12.50%,  total acc: 59.66%   [EVAL] batch:   22 | acc: 37.50%,  total acc: 58.70%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 57.55%   [EVAL] batch:   24 | acc: 31.25%,  total acc: 56.50%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 54.57%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 52.78%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 50.89%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 49.35%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 47.92%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 46.57%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 47.27%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 48.86%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 49.82%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 51.25%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 52.43%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 53.38%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 54.61%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 55.45%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 56.86%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 57.44%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 57.99%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 58.95%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 59.44%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 59.92%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 60.24%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 60.55%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 60.97%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 61.38%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 61.03%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 61.42%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 61.08%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 61.34%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 61.25%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 61.27%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 60.86%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 60.67%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 60.49%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 60.83%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 60.66%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 60.89%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 60.32%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 31.25%,  total acc: 66.41%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 65.28%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 65.00%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 61.93%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 60.42%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 60.58%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 62.95%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 64.58%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 66.41%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 68.38%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 71.05%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 71.56%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 72.02%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 71.59%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 71.74%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 72.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.56%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 74.54%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.45%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 76.08%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.88%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 77.42%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 77.73%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 78.41%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 78.04%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 78.38%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 78.62%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 80.18%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 80.51%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 80.81%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 80.97%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 81.11%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 80.85%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 80.86%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 80.99%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 80.62%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 80.64%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 80.77%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 81.01%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 80.90%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 80.57%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 80.80%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 80.70%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 80.17%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 79.66%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 79.61%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 79.33%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 78.67%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 77.73%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 76.83%   [EVAL] batch:   65 | acc: 25.00%,  total acc: 76.04%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 75.19%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 74.45%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 73.82%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 73.84%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 73.77%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 73.61%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 73.72%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 73.73%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 73.92%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 74.10%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 74.27%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 74.36%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 74.60%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 74.69%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 74.55%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 74.48%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 74.49%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 74.56%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 74.43%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 74.43%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 74.65%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 74.86%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 74.93%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 75.14%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 75.13%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 75.40%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 75.66%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 75.91%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 76.16%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 76.40%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 76.64%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 76.88%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 77.04%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 77.21%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 77.37%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 77.40%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 77.62%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 77.77%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 77.39%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 76.79%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 76.26%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 75.68%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 75.28%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 74.89%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 74.89%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 74.95%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 75.16%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 75.27%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 75.43%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 75.58%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 75.47%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 74.84%   [EVAL] batch:  120 | acc: 0.00%,  total acc: 74.23%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 73.62%   [EVAL] batch:  122 | acc: 0.00%,  total acc: 73.02%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 72.43%   [EVAL] batch:  124 | acc: 0.00%,  total acc: 71.85%   [EVAL] batch:  125 | acc: 43.75%,  total acc: 71.63%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 71.36%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 71.04%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 70.69%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 70.62%   [EVAL] batch:  130 | acc: 56.25%,  total acc: 70.52%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 70.64%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 70.86%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 70.94%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 70.97%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 71.05%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 71.26%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 71.33%   [EVAL] batch:  138 | acc: 50.00%,  total acc: 71.18%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 71.07%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 71.14%   [EVAL] batch:  141 | acc: 81.25%,  total acc: 71.21%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 71.20%   [EVAL] batch:  143 | acc: 68.75%,  total acc: 71.18%   [EVAL] batch:  144 | acc: 12.50%,  total acc: 70.78%   [EVAL] batch:  145 | acc: 31.25%,  total acc: 70.51%   [EVAL] batch:  146 | acc: 18.75%,  total acc: 70.15%   [EVAL] batch:  147 | acc: 6.25%,  total acc: 69.72%   [EVAL] batch:  148 | acc: 18.75%,  total acc: 69.38%   [EVAL] batch:  149 | acc: 18.75%,  total acc: 69.04%   [EVAL] batch:  150 | acc: 68.75%,  total acc: 69.04%   [EVAL] batch:  151 | acc: 81.25%,  total acc: 69.12%   [EVAL] batch:  152 | acc: 81.25%,  total acc: 69.20%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 69.36%   [EVAL] batch:  154 | acc: 56.25%,  total acc: 69.27%   [EVAL] batch:  155 | acc: 50.00%,  total acc: 69.15%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 69.11%   [EVAL] batch:  157 | acc: 62.50%,  total acc: 69.07%   [EVAL] batch:  158 | acc: 68.75%,  total acc: 69.06%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 69.02%   [EVAL] batch:  160 | acc: 75.00%,  total acc: 69.06%   [EVAL] batch:  161 | acc: 62.50%,  total acc: 69.02%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 69.02%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 68.98%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 69.02%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 69.01%   [EVAL] batch:  166 | acc: 81.25%,  total acc: 69.09%   [EVAL] batch:  167 | acc: 81.25%,  total acc: 69.16%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 69.16%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 68.90%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 68.68%   [EVAL] batch:  171 | acc: 25.00%,  total acc: 68.42%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 68.28%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 68.03%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 67.93%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 68.11%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 68.29%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 68.47%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 68.65%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 68.82%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 68.99%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 69.02%   [EVAL] batch:  182 | acc: 43.75%,  total acc: 68.89%   [EVAL] batch:  183 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:  185 | acc: 62.50%,  total acc: 68.72%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 68.62%   [EVAL] batch:  188 | acc: 18.75%,  total acc: 68.35%   [EVAL] batch:  189 | acc: 25.00%,  total acc: 68.12%   [EVAL] batch:  190 | acc: 18.75%,  total acc: 67.87%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 67.64%   [EVAL] batch:  192 | acc: 12.50%,  total acc: 67.36%   [EVAL] batch:  193 | acc: 25.00%,  total acc: 67.14%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 67.21%   [EVAL] batch:  195 | acc: 100.00%,  total acc: 67.38%   [EVAL] batch:  196 | acc: 93.75%,  total acc: 67.51%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 67.58%   [EVAL] batch:  198 | acc: 87.50%,  total acc: 67.68%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 67.78%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 67.75%   [EVAL] batch:  201 | acc: 31.25%,  total acc: 67.57%   [EVAL] batch:  202 | acc: 37.50%,  total acc: 67.43%   [EVAL] batch:  203 | acc: 75.00%,  total acc: 67.46%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 67.47%   [EVAL] batch:  205 | acc: 37.50%,  total acc: 67.32%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 67.45%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 67.61%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 67.76%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 67.89%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 68.01%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 68.16%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 68.25%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 68.40%   [EVAL] batch:  214 | acc: 93.75%,  total acc: 68.52%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 68.66%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 68.81%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 68.95%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 69.06%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 69.20%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 69.34%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 69.45%   [EVAL] batch:  222 | acc: 87.50%,  total acc: 69.53%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 69.64%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 69.78%   [EVAL] batch:  225 | acc: 87.50%,  total acc: 69.86%   [EVAL] batch:  226 | acc: 93.75%,  total acc: 69.96%   [EVAL] batch:  227 | acc: 81.25%,  total acc: 70.01%   [EVAL] batch:  228 | acc: 62.50%,  total acc: 69.98%   [EVAL] batch:  229 | acc: 50.00%,  total acc: 69.89%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 69.86%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 69.94%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 70.04%   [EVAL] batch:  233 | acc: 87.50%,  total acc: 70.11%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 70.24%   [EVAL] batch:  235 | acc: 87.50%,  total acc: 70.31%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 70.41%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 70.48%   [EVAL] batch:  238 | acc: 68.75%,  total acc: 70.48%   [EVAL] batch:  239 | acc: 56.25%,  total acc: 70.42%   [EVAL] batch:  240 | acc: 62.50%,  total acc: 70.38%   [EVAL] batch:  241 | acc: 62.50%,  total acc: 70.35%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 70.32%   [EVAL] batch:  243 | acc: 31.25%,  total acc: 70.16%   [EVAL] batch:  244 | acc: 68.75%,  total acc: 70.15%   [EVAL] batch:  245 | acc: 50.00%,  total acc: 70.07%   [EVAL] batch:  246 | acc: 43.75%,  total acc: 69.96%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 70.01%   [EVAL] batch:  248 | acc: 81.25%,  total acc: 70.06%   [EVAL] batch:  249 | acc: 50.00%,  total acc: 69.97%   [EVAL] batch:  250 | acc: 100.00%,  total acc: 70.09%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 70.19%   [EVAL] batch:  252 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 70.37%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 70.47%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 70.58%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 70.57%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 70.62%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 70.63%   [EVAL] batch:  259 | acc: 81.25%,  total acc: 70.67%   [EVAL] batch:  260 | acc: 56.25%,  total acc: 70.62%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 70.61%   [EVAL] batch:  262 | acc: 68.75%,  total acc: 70.60%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 70.62%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 70.61%   [EVAL] batch:  265 | acc: 50.00%,  total acc: 70.54%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 70.48%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 70.45%   [EVAL] batch:  268 | acc: 50.00%,  total acc: 70.38%   [EVAL] batch:  269 | acc: 43.75%,  total acc: 70.28%   [EVAL] batch:  270 | acc: 62.50%,  total acc: 70.25%   [EVAL] batch:  271 | acc: 56.25%,  total acc: 70.20%   [EVAL] batch:  272 | acc: 43.75%,  total acc: 70.10%   [EVAL] batch:  273 | acc: 75.00%,  total acc: 70.12%   [EVAL] batch:  274 | acc: 12.50%,  total acc: 69.91%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 70.02%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 70.13%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 70.23%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 70.34%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 70.45%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 70.55%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 70.57%   [EVAL] batch:  282 | acc: 75.00%,  total acc: 70.58%   [EVAL] batch:  283 | acc: 68.75%,  total acc: 70.58%   [EVAL] batch:  284 | acc: 87.50%,  total acc: 70.64%   [EVAL] batch:  285 | acc: 75.00%,  total acc: 70.65%   [EVAL] batch:  286 | acc: 75.00%,  total acc: 70.67%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 70.68%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 70.78%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 70.88%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 70.98%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 71.08%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 71.18%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 71.26%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 71.36%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 71.45%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 71.55%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 71.64%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 71.83%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 71.91%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 71.98%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 72.05%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 72.14%   [EVAL] batch:  304 | acc: 87.50%,  total acc: 72.19%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 72.28%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 72.35%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 72.40%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 72.47%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 72.56%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 72.65%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 72.68%   [EVAL] batch:  312 | acc: 68.75%,  total acc: 72.66%   [EVAL] batch:  313 | acc: 31.25%,  total acc: 72.53%   [EVAL] batch:  314 | acc: 25.00%,  total acc: 72.38%   [EVAL] batch:  315 | acc: 37.50%,  total acc: 72.27%   [EVAL] batch:  316 | acc: 43.75%,  total acc: 72.18%   [EVAL] batch:  317 | acc: 31.25%,  total acc: 72.05%   [EVAL] batch:  318 | acc: 68.75%,  total acc: 72.04%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 72.05%   [EVAL] batch:  320 | acc: 75.00%,  total acc: 72.06%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 72.09%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 72.16%   [EVAL] batch:  323 | acc: 62.50%,  total acc: 72.13%   [EVAL] batch:  324 | acc: 68.75%,  total acc: 72.12%   [EVAL] batch:  325 | acc: 93.75%,  total acc: 72.18%   [EVAL] batch:  326 | acc: 81.25%,  total acc: 72.21%   [EVAL] batch:  327 | acc: 93.75%,  total acc: 72.28%   [EVAL] batch:  328 | acc: 93.75%,  total acc: 72.34%   [EVAL] batch:  329 | acc: 87.50%,  total acc: 72.39%   [EVAL] batch:  330 | acc: 87.50%,  total acc: 72.43%   [EVAL] batch:  331 | acc: 31.25%,  total acc: 72.31%   [EVAL] batch:  332 | acc: 12.50%,  total acc: 72.13%   [EVAL] batch:  333 | acc: 12.50%,  total acc: 71.95%   [EVAL] batch:  334 | acc: 25.00%,  total acc: 71.81%   [EVAL] batch:  335 | acc: 31.25%,  total acc: 71.69%   [EVAL] batch:  336 | acc: 25.00%,  total acc: 71.55%   [EVAL] batch:  337 | acc: 31.25%,  total acc: 71.43%   [EVAL] batch:  338 | acc: 0.00%,  total acc: 71.22%   [EVAL] batch:  339 | acc: 6.25%,  total acc: 71.03%   [EVAL] batch:  340 | acc: 0.00%,  total acc: 70.82%   [EVAL] batch:  341 | acc: 6.25%,  total acc: 70.63%   [EVAL] batch:  342 | acc: 12.50%,  total acc: 70.46%   [EVAL] batch:  343 | acc: 25.00%,  total acc: 70.33%   [EVAL] batch:  344 | acc: 93.75%,  total acc: 70.40%   [EVAL] batch:  345 | acc: 87.50%,  total acc: 70.45%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 70.52%   [EVAL] batch:  347 | acc: 100.00%,  total acc: 70.60%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 70.67%   [EVAL] batch:  349 | acc: 87.50%,  total acc: 70.71%   [EVAL] batch:  350 | acc: 93.75%,  total acc: 70.78%   [EVAL] batch:  351 | acc: 87.50%,  total acc: 70.83%   [EVAL] batch:  352 | acc: 81.25%,  total acc: 70.86%   [EVAL] batch:  353 | acc: 81.25%,  total acc: 70.89%   [EVAL] batch:  354 | acc: 87.50%,  total acc: 70.93%   [EVAL] batch:  355 | acc: 87.50%,  total acc: 70.98%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 71.03%   [EVAL] batch:  357 | acc: 87.50%,  total acc: 71.07%   [EVAL] batch:  358 | acc: 68.75%,  total acc: 71.07%   [EVAL] batch:  359 | acc: 81.25%,  total acc: 71.09%   [EVAL] batch:  360 | acc: 81.25%,  total acc: 71.12%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 71.17%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 71.14%   [EVAL] batch:  363 | acc: 56.25%,  total acc: 71.10%   [EVAL] batch:  364 | acc: 56.25%,  total acc: 71.06%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 71.02%   [EVAL] batch:  366 | acc: 68.75%,  total acc: 71.01%   [EVAL] batch:  367 | acc: 62.50%,  total acc: 70.99%   [EVAL] batch:  368 | acc: 50.00%,  total acc: 70.93%   [EVAL] batch:  369 | acc: 43.75%,  total acc: 70.86%   [EVAL] batch:  370 | acc: 50.00%,  total acc: 70.81%   [EVAL] batch:  371 | acc: 62.50%,  total acc: 70.78%   [EVAL] batch:  372 | acc: 62.50%,  total acc: 70.76%   [EVAL] batch:  373 | acc: 68.75%,  total acc: 70.76%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 70.73%   
cur_acc:  ['0.9514', '0.8006', '0.7629', '0.7560', '0.8859', '0.6032']
his_acc:  ['0.9514', '0.8715', '0.7999', '0.7435', '0.7518', '0.7073']
CurrentTrain: epoch 15, batch     0 | loss: 11.7862828CurrentTrain: epoch 15, batch     1 | loss: 10.0287976CurrentTrain: epoch 15, batch     2 | loss: 18.2695847CurrentTrain: epoch  1, batch     3 | loss: 8.6850738CurrentTrain: epoch 15, batch     0 | loss: 14.7567281CurrentTrain: epoch 15, batch     1 | loss: 13.7555118CurrentTrain: epoch 15, batch     2 | loss: 13.9290800CurrentTrain: epoch  1, batch     3 | loss: 7.7563156CurrentTrain: epoch 15, batch     0 | loss: 13.3913970CurrentTrain: epoch 15, batch     1 | loss: 12.9732911CurrentTrain: epoch 15, batch     2 | loss: 11.8913652CurrentTrain: epoch  1, batch     3 | loss: 7.2092003CurrentTrain: epoch 15, batch     0 | loss: 8.9372354CurrentTrain: epoch 15, batch     1 | loss: 10.2010847CurrentTrain: epoch 15, batch     2 | loss: 18.4431336CurrentTrain: epoch  1, batch     3 | loss: 7.1142705CurrentTrain: epoch 15, batch     0 | loss: 14.2010700CurrentTrain: epoch 15, batch     1 | loss: 9.0470055CurrentTrain: epoch 15, batch     2 | loss: 6.4802011CurrentTrain: epoch  1, batch     3 | loss: 7.8700987CurrentTrain: epoch 15, batch     0 | loss: 8.7825604CurrentTrain: epoch 15, batch     1 | loss: 11.9246455CurrentTrain: epoch 15, batch     2 | loss: 13.9408733CurrentTrain: epoch  1, batch     3 | loss: 6.9286506CurrentTrain: epoch 15, batch     0 | loss: 10.1126435CurrentTrain: epoch 15, batch     1 | loss: 8.0958841CurrentTrain: epoch 15, batch     2 | loss: 14.4175431CurrentTrain: epoch  1, batch     3 | loss: 5.8146783CurrentTrain: epoch 15, batch     0 | loss: 9.1281803CurrentTrain: epoch 15, batch     1 | loss: 13.4921253CurrentTrain: epoch 15, batch     2 | loss: 10.3305440CurrentTrain: epoch  1, batch     3 | loss: 5.6584717CurrentTrain: epoch 15, batch     0 | loss: 8.1253724CurrentTrain: epoch 15, batch     1 | loss: 7.9456212CurrentTrain: epoch 15, batch     2 | loss: 7.1053880CurrentTrain: epoch  1, batch     3 | loss: 12.2171679CurrentTrain: epoch 15, batch     0 | loss: 17.6388694CurrentTrain: epoch 15, batch     1 | loss: 8.8690147CurrentTrain: epoch 15, batch     2 | loss: 10.7015039CurrentTrain: epoch  1, batch     3 | loss: 7.4724380
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: auerbach is prominently featured in the documentary film , " the first basket " , about jewish basketball history .
Head Entity: the first basket
Tail Entity: basketball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: the novel "Pride and Prejudice" explores the themes of love and social class in early 19th century England.  
Head Entity: Pride and Prejudice  
Tail Entity: love  

Relation: main subject  
Context: the documentary "Our Planet" highlights the impact of climate change on various ecosystems around the world.  
Head Entity: Our Planet  
Tail Entity: climate change  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: group b of uefa euro 2016 contained england , russia , wales and slovakia .
Head Entity: uefa euro 2016
Tail Entity: slovakia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: the 2020 summer olympics featured countries like the united states, china, and japan competing in various sports.  
Head Entity: 2020 summer olympics  
Tail Entity: japan  

Relation: participating team  
Context: the fifa world cup 2018 saw teams such as france, croatia, and belgium battling for the championship title.  
Head Entity: fifa world cup 2018  
Tail Entity: croatia  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: among the competing artists was previous eurovision song contest entrant robin juhkental who represented estonia in 2010 as part of the group malcolm lincoln .
Head Entity: malcolm lincoln
Tail Entity: robin juhkental
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The famous painting "The Last Supper" features several apostles, including Peter, who is depicted prominently on the left side of the artwork.  
Head Entity: The Last Supper  
Tail Entity: Peter  

Relation: has part  
Context: The smartphone model Galaxy S21 comes with a high-resolution camera that includes a wide-angle lens, allowing users to capture stunning landscapes.  
Head Entity: Galaxy S21  
Tail Entity: wide-angle lens  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: for her performance in the musical - drama " crazy heart " ( 2009 ) , she was nominated for the academy award for best supporting actress .
Head Entity: crazy heart
Tail Entity: academy award for best supporting actress
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film "The Shape of Water" received critical acclaim and was nominated for several prestigious awards, including the Academy Award for Best Picture.  
Head Entity: The Shape of Water  
Tail Entity: Academy Award for Best Picture  

Relation: nominated for  
Context: In 2021, the song "Fight for You" from the film "Judas and the Black Messiah" was nominated for the Grammy Award for Best Song Written for Visual Media.  
Head Entity: Fight for You  
Tail Entity: Grammy Award for Best Song Written for Visual Media  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: in 2007 , the county municipality created the transit authority called " skyss " that would market public transport while is would be operated by private companies based on public service obligation contracts .
Head Entity: skyss
Tail Entity: transit authority
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone known as the iPhone was first released by Apple Inc. in 2007, revolutionizing the mobile phone industry and setting new standards for design and functionality.  
Head Entity: iPhone  
Tail Entity: smartphone  

Relation: instance of  
Context: The Great Wall of China is a series of fortifications made of various materials, built to protect the Chinese states and empires from invasions and raids.  
Head Entity: Great Wall of China  
Tail Entity: fortification
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: the mount kapaz or kepez ( ) is a mountain in lesser caucasus near ganja city in central azerbaijan .
Head Entity: mount kapaz
Tail Entity: lesser caucasus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the sierra nevada is a mountain range in the western united states, primarily located in the state of california.  
Head Entity: sierra nevada  
Tail Entity: california  

Relation: mountain range  
Context: the alps are a major mountain range in europe, stretching across eight countries including france, germany, and italy.  
Head Entity: alps  
Tail Entity: europe  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: anders matthesen has also released several cds with his radio material , in addition to the animated movie " terkel in trouble " , based on one of these .
Head Entity: terkel in trouble
Tail Entity: anders matthesen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: the acclaimed film "inception" features a complex narrative that was intricately crafted by its talented screenwriter, who is known for his unique storytelling style.  
Head Entity: inception  
Tail Entity: christopher nolan  

Relation: screenwriter  
Context: the beloved animated feature "finding nemo" was brought to life through the creative vision of its screenwriter, whose work has captivated audiences of all ages.  
Head Entity: finding nemo  
Tail Entity: andrew stanton  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: " aamulehti " was founded in 1881 to “ improve the position of the finnish people and the finnish language ” during russia ’s rule over finland .
Head Entity: aamulehti
Tail Entity: finnish language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The novel "Cien años de soledad" is written in Spanish and is considered one of the most significant works of literature in the Spanish language.  
Head Entity: Cien años de soledad  
Tail Entity: Spanish  

Relation: language of work or name  
Context: The popular animated series "Naruto" is originally produced in Japanese and has gained a massive following among fans of the Japanese language and culture.  
Head Entity: Naruto  
Tail Entity: Japanese  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: the aircraft was a boeing 707 - 321c that first entered service with pan american world airways ( pan am ) in 1963 .
Head Entity: boeing 707 - 321c
Tail Entity: pan american world airways
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: the new electric bus fleet is managed by the city transportation authority, which ensures efficient public transit services.  
Head Entity: electric bus fleet  
Tail Entity: city transportation authority  

Relation: operator  
Context: the state-of-the-art research facility is operated by the national science foundation, providing resources for groundbreaking studies.  
Head Entity: research facility  
Tail Entity: national science foundation  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: udasi has been an early sect based on the teachings of sri chand ( 1494–1643 ) , the son of guru nanak , the founder and the first guru of sikhism .
Head Entity: guru nanak
Tail Entity: sikhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the baha'i faith is a religion founded by baha'u'llah in the 19th century, emphasizing the spiritual unity of all humankind.  
Head Entity: baha'u'llah  
Tail Entity: baha'i faith  

Relation: religion  
Context: the ancient greeks practiced a polytheistic religion that included a pantheon of gods such as zeus, athena, and apollo.  
Head Entity: ancient greeks  
Tail Entity: polytheistic religion  
MemoryTrain:  epoch 15, batch     0 | loss: 3.4189388MemoryTrain:  epoch 15, batch     1 | loss: 4.1396102MemoryTrain:  epoch 15, batch     2 | loss: 2.2931183MemoryTrain:  epoch 15, batch     3 | loss: 1.8419651MemoryTrain:  epoch 15, batch     4 | loss: 2.6944129MemoryTrain:  epoch 15, batch     5 | loss: 4.1916200MemoryTrain:  epoch 15, batch     6 | loss: 2.3924035MemoryTrain:  epoch 15, batch     7 | loss: 2.7023675MemoryTrain:  epoch 15, batch     8 | loss: 2.1521668MemoryTrain:  epoch 15, batch     9 | loss: 2.3142539MemoryTrain:  epoch 15, batch    10 | loss: 4.8613096MemoryTrain:  epoch 15, batch    11 | loss: 3.0398900MemoryTrain:  epoch 15, batch    12 | loss: 3.0221889MemoryTrain:  epoch  1, batch    13 | loss: 8.9938902MemoryTrain:  epoch 15, batch     0 | loss: 2.7600535MemoryTrain:  epoch 15, batch     1 | loss: 3.1867786MemoryTrain:  epoch 15, batch     2 | loss: 2.8282546MemoryTrain:  epoch 15, batch     3 | loss: 2.8960794MemoryTrain:  epoch 15, batch     4 | loss: 2.7390839MemoryTrain:  epoch 15, batch     5 | loss: 2.4147213MemoryTrain:  epoch 15, batch     6 | loss: 1.7844246MemoryTrain:  epoch 15, batch     7 | loss: 2.4729929MemoryTrain:  epoch 15, batch     8 | loss: 2.6073153MemoryTrain:  epoch 15, batch     9 | loss: 4.3043343MemoryTrain:  epoch 15, batch    10 | loss: 2.7280068MemoryTrain:  epoch 15, batch    11 | loss: 2.8346311MemoryTrain:  epoch 15, batch    12 | loss: 2.8909624MemoryTrain:  epoch  1, batch    13 | loss: 5.0066206MemoryTrain:  epoch 15, batch     0 | loss: 3.2187450MemoryTrain:  epoch 15, batch     1 | loss: 2.6261035MemoryTrain:  epoch 15, batch     2 | loss: 4.5553242MemoryTrain:  epoch 15, batch     3 | loss: 6.1521700MemoryTrain:  epoch 15, batch     4 | loss: 4.8654499MemoryTrain:  epoch 15, batch     5 | loss: 2.4089417MemoryTrain:  epoch 15, batch     6 | loss: 8.5509279MemoryTrain:  epoch 15, batch     7 | loss: 2.3313494MemoryTrain:  epoch 15, batch     8 | loss: 2.4093362MemoryTrain:  epoch 15, batch     9 | loss: 1.9199772MemoryTrain:  epoch 15, batch    10 | loss: 2.5107489MemoryTrain:  epoch 15, batch    11 | loss: 1.5782979MemoryTrain:  epoch 15, batch    12 | loss: 1.6306959MemoryTrain:  epoch  1, batch    13 | loss: 6.6461036MemoryTrain:  epoch 15, batch     0 | loss: 2.6768463MemoryTrain:  epoch 15, batch     1 | loss: 2.2532229MemoryTrain:  epoch 15, batch     2 | loss: 1.8211670MemoryTrain:  epoch 15, batch     3 | loss: 2.2843880MemoryTrain:  epoch 15, batch     4 | loss: 1.8099552MemoryTrain:  epoch 15, batch     5 | loss: 2.4055711MemoryTrain:  epoch 15, batch     6 | loss: 2.5265016MemoryTrain:  epoch 15, batch     7 | loss: 4.6738994MemoryTrain:  epoch 15, batch     8 | loss: 1.8424792MemoryTrain:  epoch 15, batch     9 | loss: 2.1455491MemoryTrain:  epoch 15, batch    10 | loss: 1.6188497MemoryTrain:  epoch 15, batch    11 | loss: 2.1369522MemoryTrain:  epoch 15, batch    12 | loss: 2.0835747MemoryTrain:  epoch  1, batch    13 | loss: 8.8521943MemoryTrain:  epoch 15, batch     0 | loss: 1.8598504MemoryTrain:  epoch 15, batch     1 | loss: 2.4170243MemoryTrain:  epoch 15, batch     2 | loss: 2.0730288MemoryTrain:  epoch 15, batch     3 | loss: 4.2931979MemoryTrain:  epoch 15, batch     4 | loss: 2.0318271MemoryTrain:  epoch 15, batch     5 | loss: 1.5764118MemoryTrain:  epoch 15, batch     6 | loss: 1.9320413MemoryTrain:  epoch 15, batch     7 | loss: 2.9959642MemoryTrain:  epoch 15, batch     8 | loss: 3.9163039MemoryTrain:  epoch 15, batch     9 | loss: 1.9529528MemoryTrain:  epoch 15, batch    10 | loss: 2.0058103MemoryTrain:  epoch 15, batch    11 | loss: 2.1059112MemoryTrain:  epoch 15, batch    12 | loss: 4.2655319MemoryTrain:  epoch  1, batch    13 | loss: 5.8406694MemoryTrain:  epoch 15, batch     0 | loss: 1.5350043MemoryTrain:  epoch 15, batch     1 | loss: 1.8711003MemoryTrain:  epoch 15, batch     2 | loss: 1.7636722MemoryTrain:  epoch 15, batch     3 | loss: 4.3063140MemoryTrain:  epoch 15, batch     4 | loss: 2.3495902MemoryTrain:  epoch 15, batch     5 | loss: 1.7274495MemoryTrain:  epoch 15, batch     6 | loss: 3.8249630MemoryTrain:  epoch 15, batch     7 | loss: 1.4451592MemoryTrain:  epoch 15, batch     8 | loss: 1.6346042MemoryTrain:  epoch 15, batch     9 | loss: 1.9504361MemoryTrain:  epoch 15, batch    10 | loss: 2.2403896MemoryTrain:  epoch 15, batch    11 | loss: 1.9947709MemoryTrain:  epoch 15, batch    12 | loss: 1.7556678MemoryTrain:  epoch  1, batch    13 | loss: 5.9143669MemoryTrain:  epoch 15, batch     0 | loss: 1.8418622MemoryTrain:  epoch 15, batch     1 | loss: 1.9709961MemoryTrain:  epoch 15, batch     2 | loss: 1.8379373MemoryTrain:  epoch 15, batch     3 | loss: 4.0546089MemoryTrain:  epoch 15, batch     4 | loss: 2.2489206MemoryTrain:  epoch 15, batch     5 | loss: 1.8657195MemoryTrain:  epoch 15, batch     6 | loss: 1.4876553MemoryTrain:  epoch 15, batch     7 | loss: 1.6271935MemoryTrain:  epoch 15, batch     8 | loss: 1.8236922MemoryTrain:  epoch 15, batch     9 | loss: 1.7209995MemoryTrain:  epoch 15, batch    10 | loss: 3.5257156MemoryTrain:  epoch 15, batch    11 | loss: 2.3416204MemoryTrain:  epoch 15, batch    12 | loss: 1.7744613MemoryTrain:  epoch  1, batch    13 | loss: 5.9234157MemoryTrain:  epoch 15, batch     0 | loss: 1.9070782MemoryTrain:  epoch 15, batch     1 | loss: 1.6480716MemoryTrain:  epoch 15, batch     2 | loss: 2.5251328MemoryTrain:  epoch 15, batch     3 | loss: 1.9039942MemoryTrain:  epoch 15, batch     4 | loss: 1.8010160MemoryTrain:  epoch 15, batch     5 | loss: 1.8666739MemoryTrain:  epoch 15, batch     6 | loss: 1.7222838MemoryTrain:  epoch 15, batch     7 | loss: 1.3579201MemoryTrain:  epoch 15, batch     8 | loss: 1.8760552MemoryTrain:  epoch 15, batch     9 | loss: 1.8117934MemoryTrain:  epoch 15, batch    10 | loss: 2.7407037MemoryTrain:  epoch 15, batch    11 | loss: 1.8628472MemoryTrain:  epoch 15, batch    12 | loss: 1.5218205MemoryTrain:  epoch  1, batch    13 | loss: 5.3970677MemoryTrain:  epoch 15, batch     0 | loss: 1.5940757MemoryTrain:  epoch 15, batch     1 | loss: 1.7577216MemoryTrain:  epoch 15, batch     2 | loss: 1.5579293MemoryTrain:  epoch 15, batch     3 | loss: 1.8681465MemoryTrain:  epoch 15, batch     4 | loss: 1.3951406MemoryTrain:  epoch 15, batch     5 | loss: 1.6429750MemoryTrain:  epoch 15, batch     6 | loss: 3.5260968MemoryTrain:  epoch 15, batch     7 | loss: 3.5670846MemoryTrain:  epoch 15, batch     8 | loss: 1.3483888MemoryTrain:  epoch 15, batch     9 | loss: 2.0336954MemoryTrain:  epoch 15, batch    10 | loss: 1.4724961MemoryTrain:  epoch 15, batch    11 | loss: 1.6001880MemoryTrain:  epoch 15, batch    12 | loss: 1.6552175MemoryTrain:  epoch  1, batch    13 | loss: 5.6927525MemoryTrain:  epoch 15, batch     0 | loss: 1.4988695MemoryTrain:  epoch 15, batch     1 | loss: 1.6143435MemoryTrain:  epoch 15, batch     2 | loss: 1.5005882MemoryTrain:  epoch 15, batch     3 | loss: 1.6696062MemoryTrain:  epoch 15, batch     4 | loss: 1.3874883MemoryTrain:  epoch 15, batch     5 | loss: 1.3262768MemoryTrain:  epoch 15, batch     6 | loss: 3.8549020MemoryTrain:  epoch 15, batch     7 | loss: 1.7167390MemoryTrain:  epoch 15, batch     8 | loss: 1.2821813MemoryTrain:  epoch 15, batch     9 | loss: 4.1066568MemoryTrain:  epoch 15, batch    10 | loss: 1.7576476MemoryTrain:  epoch 15, batch    11 | loss: 1.9414995MemoryTrain:  epoch 15, batch    12 | loss: 1.5954048MemoryTrain:  epoch  1, batch    13 | loss: 4.9818185
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 54.69%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 58.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 61.46%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 64.29%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 67.19%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 70.45%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 71.35%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 71.15%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 68.30%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 65.83%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 62.89%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 62.13%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 61.11%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 59.87%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 61.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 63.39%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 64.77%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 67.71%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   25 | acc: 0.00%,  total acc: 66.11%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 63.89%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 61.83%   [EVAL] batch:   28 | acc: 25.00%,  total acc: 60.56%   [EVAL] batch:   29 | acc: 18.75%,  total acc: 59.17%   [EVAL] batch:   30 | acc: 12.50%,  total acc: 57.66%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 58.01%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 58.90%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 59.38%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 60.36%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 60.94%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 61.66%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 62.01%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 61.86%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 62.19%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 61.59%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 61.31%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 61.34%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 61.36%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 61.81%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 62.23%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 62.90%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 63.02%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 63.65%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 64.25%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 64.46%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 64.54%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 64.62%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 64.58%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 64.89%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 64.84%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 65.02%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 65.41%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 65.78%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 66.15%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 66.60%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 66.83%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 66.47%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 69.64%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 65.62%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 64.58%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 63.75%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 61.36%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 59.38%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 59.62%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 62.05%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 63.33%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 64.84%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 66.91%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 68.40%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 69.41%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 70.00%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 70.24%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 69.89%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 69.84%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 70.57%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 71.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.12%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 73.15%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 74.78%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 75.62%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 76.21%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 76.65%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 76.61%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 76.74%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 77.03%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 77.30%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 77.88%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 78.44%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 78.96%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 79.32%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 79.65%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 79.97%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 80.28%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 80.30%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 80.05%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 80.08%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 80.25%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 80.27%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 80.41%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 80.66%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 80.56%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 80.23%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 80.36%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 80.04%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 79.74%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 79.45%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 79.69%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 79.71%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 79.54%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 79.07%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 78.03%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 77.21%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 76.33%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 75.47%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 74.82%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 74.28%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 74.29%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 74.12%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 73.87%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 73.89%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 73.90%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 73.92%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 74.10%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 74.19%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 74.12%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 74.21%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 74.38%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 74.54%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 74.24%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 73.80%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 73.51%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 73.01%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 72.53%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 72.27%   [EVAL] batch:   87 | acc: 43.75%,  total acc: 71.95%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 72.19%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 72.43%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 72.39%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 72.62%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 72.65%   [EVAL] batch:   93 | acc: 81.25%,  total acc: 72.74%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 73.03%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 73.31%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 73.58%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 73.85%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 74.12%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 74.38%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 74.57%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 74.75%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 74.94%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 75.12%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 75.36%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 75.53%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 75.18%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 74.54%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 74.03%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 73.47%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 73.03%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 72.60%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 72.62%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 72.70%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 72.93%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 73.01%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 73.18%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 73.31%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 73.27%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 72.66%   [EVAL] batch:  120 | acc: 0.00%,  total acc: 72.06%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 71.47%   [EVAL] batch:  122 | acc: 0.00%,  total acc: 70.88%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 70.31%   [EVAL] batch:  124 | acc: 0.00%,  total acc: 69.75%   [EVAL] batch:  125 | acc: 43.75%,  total acc: 69.54%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 69.24%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 69.04%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 68.70%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 68.65%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 68.61%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 68.98%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 69.12%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 69.21%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 69.30%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 69.66%   [EVAL] batch:  138 | acc: 50.00%,  total acc: 69.51%   [EVAL] batch:  139 | acc: 62.50%,  total acc: 69.46%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 69.59%   [EVAL] batch:  141 | acc: 81.25%,  total acc: 69.67%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 69.71%   [EVAL] batch:  143 | acc: 81.25%,  total acc: 69.79%   [EVAL] batch:  144 | acc: 25.00%,  total acc: 69.48%   [EVAL] batch:  145 | acc: 31.25%,  total acc: 69.22%   [EVAL] batch:  146 | acc: 25.00%,  total acc: 68.92%   [EVAL] batch:  147 | acc: 0.00%,  total acc: 68.45%   [EVAL] batch:  148 | acc: 18.75%,  total acc: 68.12%   [EVAL] batch:  149 | acc: 18.75%,  total acc: 67.79%   [EVAL] batch:  150 | acc: 68.75%,  total acc: 67.80%   [EVAL] batch:  151 | acc: 75.00%,  total acc: 67.85%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 67.89%   [EVAL] batch:  153 | acc: 100.00%,  total acc: 68.10%   [EVAL] batch:  154 | acc: 62.50%,  total acc: 68.06%   [EVAL] batch:  155 | acc: 43.75%,  total acc: 67.91%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 67.91%   [EVAL] batch:  157 | acc: 62.50%,  total acc: 67.88%   [EVAL] batch:  158 | acc: 68.75%,  total acc: 67.89%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 67.85%   [EVAL] batch:  160 | acc: 75.00%,  total acc: 67.90%   [EVAL] batch:  161 | acc: 62.50%,  total acc: 67.86%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 67.87%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 67.80%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 67.77%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 67.73%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 67.74%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 67.78%   [EVAL] batch:  168 | acc: 50.00%,  total acc: 67.68%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 67.46%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 67.25%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 67.04%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 66.91%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 66.70%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 66.61%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 66.80%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 66.98%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 67.17%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 67.35%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 67.53%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 67.71%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 67.75%   [EVAL] batch:  182 | acc: 43.75%,  total acc: 67.62%   [EVAL] batch:  183 | acc: 62.50%,  total acc: 67.60%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 67.64%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 67.64%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 67.68%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 67.59%   [EVAL] batch:  188 | acc: 18.75%,  total acc: 67.33%   [EVAL] batch:  189 | acc: 31.25%,  total acc: 67.14%   [EVAL] batch:  190 | acc: 18.75%,  total acc: 66.88%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 66.67%   [EVAL] batch:  192 | acc: 25.00%,  total acc: 66.45%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 66.20%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 66.28%   [EVAL] batch:  195 | acc: 93.75%,  total acc: 66.42%   [EVAL] batch:  196 | acc: 93.75%,  total acc: 66.56%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 66.60%   [EVAL] batch:  198 | acc: 81.25%,  total acc: 66.68%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 66.78%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 66.73%   [EVAL] batch:  201 | acc: 31.25%,  total acc: 66.55%   [EVAL] batch:  202 | acc: 31.25%,  total acc: 66.38%   [EVAL] batch:  203 | acc: 75.00%,  total acc: 66.42%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 66.43%   [EVAL] batch:  205 | acc: 37.50%,  total acc: 66.29%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 66.43%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 66.59%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 66.75%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 66.88%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 67.00%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 67.16%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 67.25%   [EVAL] batch:  213 | acc: 75.00%,  total acc: 67.29%   [EVAL] batch:  214 | acc: 68.75%,  total acc: 67.30%   [EVAL] batch:  215 | acc: 81.25%,  total acc: 67.36%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 67.40%   [EVAL] batch:  217 | acc: 62.50%,  total acc: 67.37%   [EVAL] batch:  218 | acc: 81.25%,  total acc: 67.44%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 67.59%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 67.73%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 67.82%   [EVAL] batch:  222 | acc: 87.50%,  total acc: 67.91%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 68.02%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 68.17%   [EVAL] batch:  225 | acc: 87.50%,  total acc: 68.25%   [EVAL] batch:  226 | acc: 93.75%,  total acc: 68.36%   [EVAL] batch:  227 | acc: 81.25%,  total acc: 68.42%   [EVAL] batch:  228 | acc: 68.75%,  total acc: 68.42%   [EVAL] batch:  229 | acc: 43.75%,  total acc: 68.32%   [EVAL] batch:  230 | acc: 68.75%,  total acc: 68.32%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 68.40%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 68.51%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 68.62%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  235 | acc: 87.50%,  total acc: 68.83%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 68.88%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 68.93%   [EVAL] batch:  238 | acc: 62.50%,  total acc: 68.91%   [EVAL] batch:  239 | acc: 56.25%,  total acc: 68.85%   [EVAL] batch:  240 | acc: 56.25%,  total acc: 68.80%   [EVAL] batch:  241 | acc: 50.00%,  total acc: 68.72%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 68.70%   [EVAL] batch:  243 | acc: 31.25%,  total acc: 68.55%   [EVAL] batch:  244 | acc: 56.25%,  total acc: 68.49%   [EVAL] batch:  245 | acc: 31.25%,  total acc: 68.34%   [EVAL] batch:  246 | acc: 37.50%,  total acc: 68.22%   [EVAL] batch:  247 | acc: 56.25%,  total acc: 68.17%   [EVAL] batch:  248 | acc: 62.50%,  total acc: 68.15%   [EVAL] batch:  249 | acc: 50.00%,  total acc: 68.08%   [EVAL] batch:  250 | acc: 100.00%,  total acc: 68.20%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 68.30%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 68.40%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 68.48%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 68.58%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 68.70%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 68.65%   [EVAL] batch:  257 | acc: 43.75%,  total acc: 68.56%   [EVAL] batch:  258 | acc: 43.75%,  total acc: 68.46%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 68.41%   [EVAL] batch:  260 | acc: 18.75%,  total acc: 68.22%   [EVAL] batch:  261 | acc: 37.50%,  total acc: 68.11%   [EVAL] batch:  262 | acc: 56.25%,  total acc: 68.06%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 68.09%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 68.07%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 68.02%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 68.00%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 67.98%   [EVAL] batch:  268 | acc: 50.00%,  total acc: 67.91%   [EVAL] batch:  269 | acc: 56.25%,  total acc: 67.87%   [EVAL] batch:  270 | acc: 62.50%,  total acc: 67.85%   [EVAL] batch:  271 | acc: 56.25%,  total acc: 67.81%   [EVAL] batch:  272 | acc: 56.25%,  total acc: 67.77%   [EVAL] batch:  273 | acc: 68.75%,  total acc: 67.77%   [EVAL] batch:  274 | acc: 31.25%,  total acc: 67.64%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 67.75%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 67.87%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 67.99%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 68.10%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 68.21%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 68.33%   [EVAL] batch:  281 | acc: 81.25%,  total acc: 68.37%   [EVAL] batch:  282 | acc: 81.25%,  total acc: 68.42%   [EVAL] batch:  283 | acc: 68.75%,  total acc: 68.42%   [EVAL] batch:  284 | acc: 87.50%,  total acc: 68.49%   [EVAL] batch:  285 | acc: 68.75%,  total acc: 68.49%   [EVAL] batch:  286 | acc: 75.00%,  total acc: 68.51%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 68.53%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 68.64%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 68.86%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 68.96%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 69.07%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 69.15%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 69.26%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 69.36%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 69.47%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 69.67%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 69.77%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 69.85%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 69.93%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 70.01%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 70.11%   [EVAL] batch:  304 | acc: 87.50%,  total acc: 70.16%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 70.26%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 70.34%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 70.41%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 70.49%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 70.58%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 70.68%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 70.71%   [EVAL] batch:  312 | acc: 56.25%,  total acc: 70.67%   [EVAL] batch:  313 | acc: 18.75%,  total acc: 70.50%   [EVAL] batch:  314 | acc: 6.25%,  total acc: 70.30%   [EVAL] batch:  315 | acc: 6.25%,  total acc: 70.09%   [EVAL] batch:  316 | acc: 25.00%,  total acc: 69.95%   [EVAL] batch:  317 | acc: 12.50%,  total acc: 69.77%   [EVAL] batch:  318 | acc: 50.00%,  total acc: 69.71%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 69.73%   [EVAL] batch:  320 | acc: 81.25%,  total acc: 69.76%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 69.80%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 69.87%   [EVAL] batch:  323 | acc: 68.75%,  total acc: 69.87%   [EVAL] batch:  324 | acc: 68.75%,  total acc: 69.87%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 69.86%   [EVAL] batch:  326 | acc: 50.00%,  total acc: 69.80%   [EVAL] batch:  327 | acc: 68.75%,  total acc: 69.80%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 69.81%   [EVAL] batch:  329 | acc: 50.00%,  total acc: 69.75%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 69.75%   [EVAL] batch:  331 | acc: 12.50%,  total acc: 69.58%   [EVAL] batch:  332 | acc: 12.50%,  total acc: 69.41%   [EVAL] batch:  333 | acc: 6.25%,  total acc: 69.22%   [EVAL] batch:  334 | acc: 25.00%,  total acc: 69.09%   [EVAL] batch:  335 | acc: 18.75%,  total acc: 68.94%   [EVAL] batch:  336 | acc: 6.25%,  total acc: 68.75%   [EVAL] batch:  337 | acc: 12.50%,  total acc: 68.58%   [EVAL] batch:  338 | acc: 12.50%,  total acc: 68.42%   [EVAL] batch:  339 | acc: 31.25%,  total acc: 68.31%   [EVAL] batch:  340 | acc: 0.00%,  total acc: 68.11%   [EVAL] batch:  341 | acc: 6.25%,  total acc: 67.93%   [EVAL] batch:  342 | acc: 12.50%,  total acc: 67.77%   [EVAL] batch:  343 | acc: 25.00%,  total acc: 67.64%   [EVAL] batch:  344 | acc: 81.25%,  total acc: 67.68%   [EVAL] batch:  345 | acc: 87.50%,  total acc: 67.74%   [EVAL] batch:  346 | acc: 100.00%,  total acc: 67.83%   [EVAL] batch:  347 | acc: 100.00%,  total acc: 67.92%   [EVAL] batch:  348 | acc: 87.50%,  total acc: 67.98%   [EVAL] batch:  349 | acc: 75.00%,  total acc: 68.00%   [EVAL] batch:  350 | acc: 87.50%,  total acc: 68.06%   [EVAL] batch:  351 | acc: 87.50%,  total acc: 68.11%   [EVAL] batch:  352 | acc: 87.50%,  total acc: 68.17%   [EVAL] batch:  353 | acc: 87.50%,  total acc: 68.22%   [EVAL] batch:  354 | acc: 87.50%,  total acc: 68.27%   [EVAL] batch:  355 | acc: 93.75%,  total acc: 68.35%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 68.40%   [EVAL] batch:  357 | acc: 81.25%,  total acc: 68.44%   [EVAL] batch:  358 | acc: 68.75%,  total acc: 68.44%   [EVAL] batch:  359 | acc: 81.25%,  total acc: 68.47%   [EVAL] batch:  360 | acc: 81.25%,  total acc: 68.51%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 68.56%   [EVAL] batch:  362 | acc: 50.00%,  total acc: 68.51%   [EVAL] batch:  363 | acc: 56.25%,  total acc: 68.48%   [EVAL] batch:  364 | acc: 56.25%,  total acc: 68.44%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 68.41%   [EVAL] batch:  366 | acc: 68.75%,  total acc: 68.41%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 68.38%   [EVAL] batch:  368 | acc: 68.75%,  total acc: 68.38%   [EVAL] batch:  369 | acc: 43.75%,  total acc: 68.31%   [EVAL] batch:  370 | acc: 50.00%,  total acc: 68.26%   [EVAL] batch:  371 | acc: 62.50%,  total acc: 68.25%   [EVAL] batch:  372 | acc: 62.50%,  total acc: 68.23%   [EVAL] batch:  373 | acc: 75.00%,  total acc: 68.25%   [EVAL] batch:  374 | acc: 68.75%,  total acc: 68.25%   [EVAL] batch:  375 | acc: 75.00%,  total acc: 68.27%   [EVAL] batch:  376 | acc: 43.75%,  total acc: 68.20%   [EVAL] batch:  377 | acc: 37.50%,  total acc: 68.12%   [EVAL] batch:  378 | acc: 62.50%,  total acc: 68.11%   [EVAL] batch:  379 | acc: 75.00%,  total acc: 68.12%   [EVAL] batch:  380 | acc: 75.00%,  total acc: 68.14%   [EVAL] batch:  381 | acc: 81.25%,  total acc: 68.18%   [EVAL] batch:  382 | acc: 87.50%,  total acc: 68.23%   [EVAL] batch:  383 | acc: 62.50%,  total acc: 68.21%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 68.26%   [EVAL] batch:  385 | acc: 87.50%,  total acc: 68.31%   [EVAL] batch:  386 | acc: 81.25%,  total acc: 68.35%   [EVAL] batch:  387 | acc: 68.75%,  total acc: 68.35%   [EVAL] batch:  388 | acc: 31.25%,  total acc: 68.25%   [EVAL] batch:  389 | acc: 31.25%,  total acc: 68.16%   [EVAL] batch:  390 | acc: 18.75%,  total acc: 68.03%   [EVAL] batch:  391 | acc: 50.00%,  total acc: 67.98%   [EVAL] batch:  392 | acc: 43.75%,  total acc: 67.92%   [EVAL] batch:  393 | acc: 37.50%,  total acc: 67.85%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 67.91%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 67.99%   [EVAL] batch:  396 | acc: 93.75%,  total acc: 68.06%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 68.14%   [EVAL] batch:  398 | acc: 100.00%,  total acc: 68.22%   [EVAL] batch:  399 | acc: 93.75%,  total acc: 68.28%   [EVAL] batch:  400 | acc: 0.00%,  total acc: 68.11%   [EVAL] batch:  401 | acc: 6.25%,  total acc: 67.96%   [EVAL] batch:  402 | acc: 6.25%,  total acc: 67.80%   [EVAL] batch:  403 | acc: 25.00%,  total acc: 67.70%   [EVAL] batch:  404 | acc: 18.75%,  total acc: 67.58%   [EVAL] batch:  405 | acc: 12.50%,  total acc: 67.44%   [EVAL] batch:  406 | acc: 68.75%,  total acc: 67.44%   [EVAL] batch:  407 | acc: 87.50%,  total acc: 67.49%   [EVAL] batch:  408 | acc: 75.00%,  total acc: 67.51%   [EVAL] batch:  409 | acc: 93.75%,  total acc: 67.58%   [EVAL] batch:  410 | acc: 81.25%,  total acc: 67.61%   [EVAL] batch:  411 | acc: 87.50%,  total acc: 67.66%   [EVAL] batch:  412 | acc: 75.00%,  total acc: 67.68%   [EVAL] batch:  413 | acc: 56.25%,  total acc: 67.65%   [EVAL] batch:  414 | acc: 75.00%,  total acc: 67.67%   [EVAL] batch:  415 | acc: 37.50%,  total acc: 67.59%   [EVAL] batch:  416 | acc: 50.00%,  total acc: 67.55%   [EVAL] batch:  417 | acc: 62.50%,  total acc: 67.54%   [EVAL] batch:  418 | acc: 62.50%,  total acc: 67.53%   [EVAL] batch:  419 | acc: 81.25%,  total acc: 67.56%   [EVAL] batch:  420 | acc: 81.25%,  total acc: 67.59%   [EVAL] batch:  421 | acc: 93.75%,  total acc: 67.65%   [EVAL] batch:  422 | acc: 68.75%,  total acc: 67.66%   [EVAL] batch:  423 | acc: 93.75%,  total acc: 67.72%   [EVAL] batch:  424 | acc: 93.75%,  total acc: 67.78%   [EVAL] batch:  425 | acc: 75.00%,  total acc: 67.80%   [EVAL] batch:  426 | acc: 68.75%,  total acc: 67.80%   [EVAL] batch:  427 | acc: 68.75%,  total acc: 67.80%   [EVAL] batch:  428 | acc: 62.50%,  total acc: 67.79%   [EVAL] batch:  429 | acc: 81.25%,  total acc: 67.82%   [EVAL] batch:  430 | acc: 62.50%,  total acc: 67.81%   [EVAL] batch:  431 | acc: 75.00%,  total acc: 67.82%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 67.87%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 67.91%   [EVAL] batch:  434 | acc: 87.50%,  total acc: 67.96%   [EVAL] batch:  435 | acc: 93.75%,  total acc: 68.02%   [EVAL] batch:  436 | acc: 81.25%,  total acc: 68.05%   [EVAL] batch:  437 | acc: 43.75%,  total acc: 67.99%   
cur_acc:  ['0.9514', '0.8006', '0.7629', '0.7560', '0.8859', '0.6032', '0.6647']
his_acc:  ['0.9514', '0.8715', '0.7999', '0.7435', '0.7518', '0.7073', '0.6799']
CurrentTrain: epoch 15, batch     0 | loss: 12.0005819CurrentTrain: epoch 15, batch     1 | loss: 13.2928401CurrentTrain: epoch 15, batch     2 | loss: 10.8479545CurrentTrain: epoch  1, batch     3 | loss: 10.2185237CurrentTrain: epoch 15, batch     0 | loss: 10.9372750CurrentTrain: epoch 15, batch     1 | loss: 7.6818323CurrentTrain: epoch 15, batch     2 | loss: 15.4325236CurrentTrain: epoch  1, batch     3 | loss: 6.8277327CurrentTrain: epoch 15, batch     0 | loss: 9.1200549CurrentTrain: epoch 15, batch     1 | loss: 9.0013416CurrentTrain: epoch 15, batch     2 | loss: 7.0598234CurrentTrain: epoch  1, batch     3 | loss: 13.8386329CurrentTrain: epoch 15, batch     0 | loss: 9.1833398CurrentTrain: epoch 15, batch     1 | loss: 7.6089129CurrentTrain: epoch 15, batch     2 | loss: 14.2121233CurrentTrain: epoch  1, batch     3 | loss: 6.1347794CurrentTrain: epoch 15, batch     0 | loss: 11.5573357CurrentTrain: epoch 15, batch     1 | loss: 14.5222448CurrentTrain: epoch 15, batch     2 | loss: 12.8487016CurrentTrain: epoch  1, batch     3 | loss: 5.6064847CurrentTrain: epoch 15, batch     0 | loss: 8.8757599CurrentTrain: epoch 15, batch     1 | loss: 6.7698222CurrentTrain: epoch 15, batch     2 | loss: 7.3396829CurrentTrain: epoch  1, batch     3 | loss: 5.4395875CurrentTrain: epoch 15, batch     0 | loss: 6.8971063CurrentTrain: epoch 15, batch     1 | loss: 11.8905380CurrentTrain: epoch 15, batch     2 | loss: 5.6818664CurrentTrain: epoch  1, batch     3 | loss: 7.0371210CurrentTrain: epoch 15, batch     0 | loss: 8.5617999CurrentTrain: epoch 15, batch     1 | loss: 11.5282041CurrentTrain: epoch 15, batch     2 | loss: 5.6692215CurrentTrain: epoch  1, batch     3 | loss: 8.2027930CurrentTrain: epoch 15, batch     0 | loss: 8.9779470CurrentTrain: epoch 15, batch     1 | loss: 6.7031019CurrentTrain: epoch 15, batch     2 | loss: 7.3175953CurrentTrain: epoch  1, batch     3 | loss: 5.7346270CurrentTrain: epoch 15, batch     0 | loss: 12.8330011CurrentTrain: epoch 15, batch     1 | loss: 4.9613005CurrentTrain: epoch 15, batch     2 | loss: 9.6023723CurrentTrain: epoch  1, batch     3 | loss: 5.1150658
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the sutagao are the chibcha - speaking indios sutagaos indigenous people from the region of fusagasugá , bogotá savanna , cundinamarca , colombia .
Head Entity: colombia
Tail Entity: cundinamarca
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The state of California is known for its diverse geography and is divided into several counties, including Los Angeles County, which is the most populous county in the United States.  
Head Entity: California  
Tail Entity: Los Angeles County  

Relation: contains administrative territorial entity  
Context: The country of Japan is made up of several prefectures, with Tokyo being the most prominent and the capital of the nation.  
Head Entity: Japan  
Tail Entity: Tokyo  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: enzo is an italian given name derivative of heinz , a form of henry .
Head Entity: henry
Tail Entity: heinz
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as each other due to their close genetic relationship.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: In some circles, the terms "artificial intelligence" and "machine learning" are said to be the same as each other, although experts argue there are key differences.  
Head Entity: artificial intelligence  
Tail Entity: machine learning  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: he considers his father to be biggest influence on his career as his brother javier castellano recipient of four eclipse award for outstanding jockey in the row ( 2013 , 2014,2015 and 2016 ) .
Head Entity: eclipse award for outstanding jockey
Tail Entity: javier castellano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: In the thrilling finale of the championship, Sarah Thompson emerged victorious, claiming the title of best player in the tournament, while her teammate, Mark Johnson, was awarded the runner-up position.  
Head Entity: best player in the tournament  
Tail Entity: Sarah Thompson  

Relation: winner  
Context: The annual science fair concluded with Emily Chen taking home the grand prize for her innovative project on renewable energy, while her classmate, David Lee, received an honorable mention for his work on robotics.  
Head Entity: grand prize  
Tail Entity: Emily Chen  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: the calais region was defended by the 15th army under " generaloberst " ( colonel general ) hans von salmuth , and normandy by the 7th army commanded by " generaloberst " friedrich dollmann .
Head Entity: hans von salmuth
Tail Entity: generaloberst
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: during the battle of stalingrad, the soviet forces were led by general vasily chuikov, who held the rank of major general, while the german troops were commanded by general friedrich paulus.  
Head Entity: vasily chuikov  
Tail Entity: major general  

Relation: military rank  
Context: admiral horatio nelson was a prominent figure in the royal navy, known for his rank of vice admiral during the napoleonic wars, particularly at the battle of trafalgar.  
Head Entity: horatio nelson  
Tail Entity: vice admiral  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: tokyopop also released a collection of karin short stories under the title : " chibi vampire : airmail " on august 31 , 2010 .
Head Entity: karin
Tail Entity: tokyopop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: Penguin Random House announced the release of a new novel by the acclaimed author John Grisham, set to hit the shelves on June 15, 2021.  
Head Entity: John Grisham  
Tail Entity: Penguin Random House  

Relation: publisher  
Context: The popular science magazine, Scientific American, published an article discussing the latest advancements in renewable energy technologies.  
Head Entity: Scientific American  
Tail Entity: renewable energy technologies  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " muzai moratorium " and " shōso strip " were released on vinyl in 2008 , as a part of her 10-year anniversary celebrations .
Head Entity: muzai moratorium
Tail Entity: shōso strip
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first chapter of the book introduces the main character, while the second chapter follows with a detailed backstory.  
Head Entity: first chapter  
Tail Entity: second chapter  

Relation: followed by  
Context: In the concert lineup, the opening act was a local band, followed by a well-known pop artist who headlined the event.  
Head Entity: local band  
Tail Entity: pop artist  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: there is a memoir of his life and work by gustav queck ( jena , 1852 ) .
Head Entity: gustav queck
Tail Entity: jena
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: the headquarters of the company is situated in san francisco, where they have been operating since 2010.  
Head Entity: the company  
Tail Entity: san francisco  

Relation: work location  
Context: during her time at the university, she conducted research in boston, which greatly influenced her career.  
Head Entity: she  
Tail Entity: boston  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in enzyme catalysis.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a former professional athlete, now works as a sports commentator for major networks.  
Head Entity: john smith  
Tail Entity: sports commentator  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a World Heritage Site, attracting tourists from around the globe.  
Head Entity: Petra  
Tail Entity: World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: beverly bower ( september 30 , 1925 – march 24 , 2002 ) was an american operatic soprano who had an active international opera career from the mid-1950s through the early 1970s .
Head Entity: beverly bower
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti ( october 12 , 1935 – september 6 , 2007 ) was an italian tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey ( born march 27 , 1969 ) is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: singer  
MemoryTrain:  epoch 15, batch     0 | loss: 3.1337177MemoryTrain:  epoch 15, batch     1 | loss: 4.2212273MemoryTrain:  epoch 15, batch     2 | loss: 2.3787739MemoryTrain:  epoch 15, batch     3 | loss: 2.4788978MemoryTrain:  epoch 15, batch     4 | loss: 3.2929449MemoryTrain:  epoch 15, batch     5 | loss: 2.1962377MemoryTrain:  epoch 15, batch     6 | loss: 2.1818256MemoryTrain:  epoch 15, batch     7 | loss: 2.4521680MemoryTrain:  epoch 15, batch     8 | loss: 2.2492916MemoryTrain:  epoch 15, batch     9 | loss: 3.2277895MemoryTrain:  epoch 15, batch    10 | loss: 5.1023664MemoryTrain:  epoch 15, batch    11 | loss: 5.2498431MemoryTrain:  epoch 15, batch    12 | loss: 3.1829431MemoryTrain:  epoch 15, batch    13 | loss: 3.1204067MemoryTrain:  epoch 15, batch    14 | loss: 6.2051803MemoryTrain:  epoch 15, batch     0 | loss: 2.8333137MemoryTrain:  epoch 15, batch     1 | loss: 2.5992625MemoryTrain:  epoch 15, batch     2 | loss: 3.9141127MemoryTrain:  epoch 15, batch     3 | loss: 4.1469446MemoryTrain:  epoch 15, batch     4 | loss: 2.5466518MemoryTrain:  epoch 15, batch     5 | loss: 3.7337860MemoryTrain:  epoch 15, batch     6 | loss: 4.3088729MemoryTrain:  epoch 15, batch     7 | loss: 1.9503764MemoryTrain:  epoch 15, batch     8 | loss: 4.4297009MemoryTrain:  epoch 15, batch     9 | loss: 2.5829132MemoryTrain:  epoch 15, batch    10 | loss: 3.2241152MemoryTrain:  epoch 15, batch    11 | loss: 2.7558892MemoryTrain:  epoch 15, batch    12 | loss: 2.3015408MemoryTrain:  epoch 15, batch    13 | loss: 5.2494531MemoryTrain:  epoch 15, batch    14 | loss: 1.9492047MemoryTrain:  epoch 15, batch     0 | loss: 1.7244867MemoryTrain:  epoch 15, batch     1 | loss: 2.7916433MemoryTrain:  epoch 15, batch     2 | loss: 2.1542993MemoryTrain:  epoch 15, batch     3 | loss: 2.9601592MemoryTrain:  epoch 15, batch     4 | loss: 1.4554160MemoryTrain:  epoch 15, batch     5 | loss: 2.1554206MemoryTrain:  epoch 15, batch     6 | loss: 4.0240434MemoryTrain:  epoch 15, batch     7 | loss: 1.6604452MemoryTrain:  epoch 15, batch     8 | loss: 2.5288153MemoryTrain:  epoch 15, batch     9 | loss: 1.9147253MemoryTrain:  epoch 15, batch    10 | loss: 1.8707986MemoryTrain:  epoch 15, batch    11 | loss: 1.4952236MemoryTrain:  epoch 15, batch    12 | loss: 1.7572795MemoryTrain:  epoch 15, batch    13 | loss: 1.7682972MemoryTrain:  epoch 15, batch    14 | loss: 2.6290225MemoryTrain:  epoch 15, batch     0 | loss: 1.7151425MemoryTrain:  epoch 15, batch     1 | loss: 1.9570869MemoryTrain:  epoch 15, batch     2 | loss: 2.6270437MemoryTrain:  epoch 15, batch     3 | loss: 2.1054168MemoryTrain:  epoch 15, batch     4 | loss: 1.6173517MemoryTrain:  epoch 15, batch     5 | loss: 3.8522342MemoryTrain:  epoch 15, batch     6 | loss: 2.8708355MemoryTrain:  epoch 15, batch     7 | loss: 2.5239952MemoryTrain:  epoch 15, batch     8 | loss: 1.5145812MemoryTrain:  epoch 15, batch     9 | loss: 1.4009591MemoryTrain:  epoch 15, batch    10 | loss: 2.4413283MemoryTrain:  epoch 15, batch    11 | loss: 1.5762012MemoryTrain:  epoch 15, batch    12 | loss: 2.5776081MemoryTrain:  epoch 15, batch    13 | loss: 2.9223225MemoryTrain:  epoch 15, batch    14 | loss: 2.6929289MemoryTrain:  epoch 15, batch     0 | loss: 1.5768650MemoryTrain:  epoch 15, batch     1 | loss: 1.4715185MemoryTrain:  epoch 15, batch     2 | loss: 2.1367862MemoryTrain:  epoch 15, batch     3 | loss: 1.7115276MemoryTrain:  epoch 15, batch     4 | loss: 1.6826088MemoryTrain:  epoch 15, batch     5 | loss: 1.9679419MemoryTrain:  epoch 15, batch     6 | loss: 1.9351082MemoryTrain:  epoch 15, batch     7 | loss: 1.7692720MemoryTrain:  epoch 15, batch     8 | loss: 1.8505367MemoryTrain:  epoch 15, batch     9 | loss: 1.7938404MemoryTrain:  epoch 15, batch    10 | loss: 2.4011456MemoryTrain:  epoch 15, batch    11 | loss: 2.1643698MemoryTrain:  epoch 15, batch    12 | loss: 1.6023436MemoryTrain:  epoch 15, batch    13 | loss: 4.5089163MemoryTrain:  epoch 15, batch    14 | loss: 1.9452417MemoryTrain:  epoch 15, batch     0 | loss: 4.5201914MemoryTrain:  epoch 15, batch     1 | loss: 1.8273215MemoryTrain:  epoch 15, batch     2 | loss: 1.7564783MemoryTrain:  epoch 15, batch     3 | loss: 1.3337793MemoryTrain:  epoch 15, batch     4 | loss: 1.5229291MemoryTrain:  epoch 15, batch     5 | loss: 1.3581936MemoryTrain:  epoch 15, batch     6 | loss: 2.9348316MemoryTrain:  epoch 15, batch     7 | loss: 1.4845414MemoryTrain:  epoch 15, batch     8 | loss: 1.7490166MemoryTrain:  epoch 15, batch     9 | loss: 1.5370700MemoryTrain:  epoch 15, batch    10 | loss: 2.0680381MemoryTrain:  epoch 15, batch    11 | loss: 4.1639503MemoryTrain:  epoch 15, batch    12 | loss: 1.7497432MemoryTrain:  epoch 15, batch    13 | loss: 1.5801575MemoryTrain:  epoch 15, batch    14 | loss: 1.4464676MemoryTrain:  epoch 15, batch     0 | loss: 1.8905627MemoryTrain:  epoch 15, batch     1 | loss: 1.7607885MemoryTrain:  epoch 15, batch     2 | loss: 2.4788230MemoryTrain:  epoch 15, batch     3 | loss: 3.7319835MemoryTrain:  epoch 15, batch     4 | loss: 1.7729038MemoryTrain:  epoch 15, batch     5 | loss: 3.9154429MemoryTrain:  epoch 15, batch     6 | loss: 2.2174064MemoryTrain:  epoch 15, batch     7 | loss: 2.3208210MemoryTrain:  epoch 15, batch     8 | loss: 1.7932954MemoryTrain:  epoch 15, batch     9 | loss: 2.3745833MemoryTrain:  epoch 15, batch    10 | loss: 1.8090441MemoryTrain:  epoch 15, batch    11 | loss: 1.4517995MemoryTrain:  epoch 15, batch    12 | loss: 2.2320317MemoryTrain:  epoch 15, batch    13 | loss: 2.2845079MemoryTrain:  epoch 15, batch    14 | loss: 1.4726350MemoryTrain:  epoch 15, batch     0 | loss: 1.8312329MemoryTrain:  epoch 15, batch     1 | loss: 1.8138666MemoryTrain:  epoch 15, batch     2 | loss: 2.2779320MemoryTrain:  epoch 15, batch     3 | loss: 1.4705238MemoryTrain:  epoch 15, batch     4 | loss: 1.2153952MemoryTrain:  epoch 15, batch     5 | loss: 2.3122800MemoryTrain:  epoch 15, batch     6 | loss: 1.8527017MemoryTrain:  epoch 15, batch     7 | loss: 2.0064696MemoryTrain:  epoch 15, batch     8 | loss: 1.4888516MemoryTrain:  epoch 15, batch     9 | loss: 1.6537513MemoryTrain:  epoch 15, batch    10 | loss: 4.2447192MemoryTrain:  epoch 15, batch    11 | loss: 1.3061751MemoryTrain:  epoch 15, batch    12 | loss: 1.2952417MemoryTrain:  epoch 15, batch    13 | loss: 1.4364816MemoryTrain:  epoch 15, batch    14 | loss: 1.4302294MemoryTrain:  epoch 15, batch     0 | loss: 1.5473415MemoryTrain:  epoch 15, batch     1 | loss: 1.5154654MemoryTrain:  epoch 15, batch     2 | loss: 1.4588707MemoryTrain:  epoch 15, batch     3 | loss: 1.5024240MemoryTrain:  epoch 15, batch     4 | loss: 1.8279508MemoryTrain:  epoch 15, batch     5 | loss: 1.2916852MemoryTrain:  epoch 15, batch     6 | loss: 1.7782025MemoryTrain:  epoch 15, batch     7 | loss: 1.3077807MemoryTrain:  epoch 15, batch     8 | loss: 1.4315567MemoryTrain:  epoch 15, batch     9 | loss: 1.6526739MemoryTrain:  epoch 15, batch    10 | loss: 1.6379496MemoryTrain:  epoch 15, batch    11 | loss: 4.0039897MemoryTrain:  epoch 15, batch    12 | loss: 1.4360305MemoryTrain:  epoch 15, batch    13 | loss: 2.2791062MemoryTrain:  epoch 15, batch    14 | loss: 1.8274135MemoryTrain:  epoch 15, batch     0 | loss: 4.0955808MemoryTrain:  epoch 15, batch     1 | loss: 1.3607838MemoryTrain:  epoch 15, batch     2 | loss: 1.5550267MemoryTrain:  epoch 15, batch     3 | loss: 2.1984967MemoryTrain:  epoch 15, batch     4 | loss: 1.2584634MemoryTrain:  epoch 15, batch     5 | loss: 1.2821532MemoryTrain:  epoch 15, batch     6 | loss: 1.2877626MemoryTrain:  epoch 15, batch     7 | loss: 1.4503557MemoryTrain:  epoch 15, batch     8 | loss: 3.9340470MemoryTrain:  epoch 15, batch     9 | loss: 2.0179108MemoryTrain:  epoch 15, batch    10 | loss: 1.6332504MemoryTrain:  epoch 15, batch    11 | loss: 4.0920991MemoryTrain:  epoch 15, batch    12 | loss: 1.4729497MemoryTrain:  epoch 15, batch    13 | loss: 1.3447342MemoryTrain:  epoch 15, batch    14 | loss: 2.1723218
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 35.94%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 37.50%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 39.58%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 40.18%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 43.75%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 46.53%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 49.38%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 48.86%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 50.52%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 53.85%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 54.02%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 54.58%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 55.47%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 55.88%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 56.94%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 58.22%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 60.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 61.90%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 63.64%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 64.95%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 66.15%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 67.50%   [EVAL] batch:   25 | acc: 68.75%,  total acc: 67.55%   [EVAL] batch:   26 | acc: 56.25%,  total acc: 67.13%   [EVAL] batch:   27 | acc: 68.75%,  total acc: 67.19%   [EVAL] batch:   28 | acc: 56.25%,  total acc: 66.81%   [EVAL] batch:   29 | acc: 43.75%,  total acc: 66.04%   [EVAL] batch:   30 | acc: 62.50%,  total acc: 65.93%   [EVAL] batch:   31 | acc: 50.00%,  total acc: 65.43%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 64.96%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 63.24%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 62.14%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 60.76%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 59.63%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 59.62%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 59.60%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 60.12%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 60.17%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 60.51%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 60.97%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 61.14%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 61.30%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 61.33%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 61.61%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 62.12%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 62.87%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 63.58%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 64.27%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 64.93%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 65.57%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 66.18%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 67.03%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 67.37%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 67.81%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 68.14%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 68.65%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 68.35%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 66.96%   [EVAL] batch:    7 | acc: 25.00%,  total acc: 61.72%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 60.42%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 56.82%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 55.21%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 54.81%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 57.59%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 59.17%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 60.94%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 62.87%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 64.93%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 66.12%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 66.56%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 67.86%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 67.61%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 67.39%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 68.23%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 69.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 70.19%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 71.30%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 72.10%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 72.84%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 73.54%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 74.19%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 74.61%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 75.38%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 75.17%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 75.51%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 75.66%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 76.28%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 76.88%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 77.44%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 77.68%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 78.05%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 78.33%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 77.93%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 77.73%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 77.93%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 77.88%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 77.94%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 78.18%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 77.61%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 77.79%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 77.52%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 77.26%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 76.91%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 77.15%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 76.92%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 76.39%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 75.39%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 74.33%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 73.39%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 72.57%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 71.88%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 71.29%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 71.34%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 71.13%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 70.92%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 70.63%   [EVAL] batch:   73 | acc: 37.50%,  total acc: 70.19%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 70.25%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 70.39%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 70.54%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 70.59%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 70.65%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 70.78%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 70.99%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 70.50%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 69.88%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 69.42%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 68.90%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 68.46%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 68.10%   [EVAL] batch:   87 | acc: 37.50%,  total acc: 67.76%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 68.05%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 68.33%   [EVAL] batch:   90 | acc: 43.75%,  total acc: 68.06%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 68.34%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 68.41%   [EVAL] batch:   93 | acc: 81.25%,  total acc: 68.55%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 68.88%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 69.21%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 69.52%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 69.83%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 70.44%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 70.67%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 70.89%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 71.18%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 71.39%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 71.67%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 71.55%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 70.95%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 70.47%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 69.94%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 69.59%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 69.20%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 69.25%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 69.35%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 69.57%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 69.72%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 69.87%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 69.97%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 69.96%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 69.38%   [EVAL] batch:  120 | acc: 0.00%,  total acc: 68.80%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 68.24%   [EVAL] batch:  122 | acc: 0.00%,  total acc: 67.68%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 67.14%   [EVAL] batch:  124 | acc: 0.00%,  total acc: 66.60%   [EVAL] batch:  125 | acc: 37.50%,  total acc: 66.37%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 66.09%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 65.87%   [EVAL] batch:  128 | acc: 18.75%,  total acc: 65.50%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 65.43%   [EVAL] batch:  130 | acc: 56.25%,  total acc: 65.36%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 65.48%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 65.74%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 65.90%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 65.97%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 66.13%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 66.38%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 66.49%   [EVAL] batch:  138 | acc: 50.00%,  total acc: 66.37%   [EVAL] batch:  139 | acc: 62.50%,  total acc: 66.34%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 66.45%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 66.51%   [EVAL] batch:  142 | acc: 62.50%,  total acc: 66.48%   [EVAL] batch:  143 | acc: 75.00%,  total acc: 66.54%   [EVAL] batch:  144 | acc: 25.00%,  total acc: 66.25%   [EVAL] batch:  145 | acc: 31.25%,  total acc: 66.01%   [EVAL] batch:  146 | acc: 18.75%,  total acc: 65.69%   [EVAL] batch:  147 | acc: 6.25%,  total acc: 65.29%   [EVAL] batch:  148 | acc: 25.00%,  total acc: 65.02%   [EVAL] batch:  149 | acc: 18.75%,  total acc: 64.71%   [EVAL] batch:  150 | acc: 56.25%,  total acc: 64.65%   [EVAL] batch:  151 | acc: 68.75%,  total acc: 64.68%   [EVAL] batch:  152 | acc: 56.25%,  total acc: 64.62%   [EVAL] batch:  153 | acc: 81.25%,  total acc: 64.73%   [EVAL] batch:  154 | acc: 37.50%,  total acc: 64.56%   [EVAL] batch:  155 | acc: 37.50%,  total acc: 64.38%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 64.33%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 64.28%   [EVAL] batch:  158 | acc: 62.50%,  total acc: 64.27%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 64.26%   [EVAL] batch:  160 | acc: 68.75%,  total acc: 64.29%   [EVAL] batch:  161 | acc: 56.25%,  total acc: 64.24%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 64.26%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 64.29%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 64.24%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 64.19%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 64.22%   [EVAL] batch:  167 | acc: 81.25%,  total acc: 64.32%   [EVAL] batch:  168 | acc: 50.00%,  total acc: 64.24%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 64.04%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 63.85%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 63.66%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 63.55%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 63.36%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 63.29%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 63.49%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 63.70%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 63.90%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 64.11%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 64.31%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 64.50%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 64.56%   [EVAL] batch:  182 | acc: 43.75%,  total acc: 64.45%   [EVAL] batch:  183 | acc: 62.50%,  total acc: 64.44%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 64.46%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 64.52%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 64.54%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 64.39%   [EVAL] batch:  188 | acc: 12.50%,  total acc: 64.12%   [EVAL] batch:  189 | acc: 18.75%,  total acc: 63.88%   [EVAL] batch:  190 | acc: 0.00%,  total acc: 63.55%   [EVAL] batch:  191 | acc: 6.25%,  total acc: 63.25%   [EVAL] batch:  192 | acc: 6.25%,  total acc: 62.95%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 62.73%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 62.82%   [EVAL] batch:  195 | acc: 93.75%,  total acc: 62.98%   [EVAL] batch:  196 | acc: 87.50%,  total acc: 63.10%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 63.16%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 63.22%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 63.34%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 63.31%   [EVAL] batch:  201 | acc: 31.25%,  total acc: 63.15%   [EVAL] batch:  202 | acc: 31.25%,  total acc: 62.99%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 63.02%   [EVAL] batch:  204 | acc: 56.25%,  total acc: 62.99%   [EVAL] batch:  205 | acc: 37.50%,  total acc: 62.86%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 63.01%   [EVAL] batch:  207 | acc: 93.75%,  total acc: 63.16%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 63.34%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 63.48%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 63.63%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 63.80%   [EVAL] batch:  212 | acc: 81.25%,  total acc: 63.88%   [EVAL] batch:  213 | acc: 68.75%,  total acc: 63.90%   [EVAL] batch:  214 | acc: 68.75%,  total acc: 63.92%   [EVAL] batch:  215 | acc: 68.75%,  total acc: 63.95%   [EVAL] batch:  216 | acc: 68.75%,  total acc: 63.97%   [EVAL] batch:  217 | acc: 62.50%,  total acc: 63.96%   [EVAL] batch:  218 | acc: 81.25%,  total acc: 64.04%   [EVAL] batch:  219 | acc: 93.75%,  total acc: 64.18%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 64.34%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 64.44%   [EVAL] batch:  222 | acc: 87.50%,  total acc: 64.55%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 64.70%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 64.86%   [EVAL] batch:  225 | acc: 87.50%,  total acc: 64.96%   [EVAL] batch:  226 | acc: 93.75%,  total acc: 65.09%   [EVAL] batch:  227 | acc: 81.25%,  total acc: 65.16%   [EVAL] batch:  228 | acc: 68.75%,  total acc: 65.17%   [EVAL] batch:  229 | acc: 50.00%,  total acc: 65.11%   [EVAL] batch:  230 | acc: 56.25%,  total acc: 65.07%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 65.17%   [EVAL] batch:  232 | acc: 87.50%,  total acc: 65.26%   [EVAL] batch:  233 | acc: 81.25%,  total acc: 65.33%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 65.48%   [EVAL] batch:  235 | acc: 68.75%,  total acc: 65.49%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 65.56%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:  238 | acc: 62.50%,  total acc: 65.61%   [EVAL] batch:  239 | acc: 43.75%,  total acc: 65.52%   [EVAL] batch:  240 | acc: 43.75%,  total acc: 65.43%   [EVAL] batch:  241 | acc: 43.75%,  total acc: 65.34%   [EVAL] batch:  242 | acc: 56.25%,  total acc: 65.30%   [EVAL] batch:  243 | acc: 18.75%,  total acc: 65.11%   [EVAL] batch:  244 | acc: 50.00%,  total acc: 65.05%   [EVAL] batch:  245 | acc: 25.00%,  total acc: 64.89%   [EVAL] batch:  246 | acc: 31.25%,  total acc: 64.75%   [EVAL] batch:  247 | acc: 43.75%,  total acc: 64.67%   [EVAL] batch:  248 | acc: 62.50%,  total acc: 64.66%   [EVAL] batch:  249 | acc: 37.50%,  total acc: 64.55%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 64.67%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 64.78%   [EVAL] batch:  252 | acc: 100.00%,  total acc: 64.92%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 65.01%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 65.12%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 65.26%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 65.22%   [EVAL] batch:  257 | acc: 56.25%,  total acc: 65.19%   [EVAL] batch:  258 | acc: 43.75%,  total acc: 65.11%   [EVAL] batch:  259 | acc: 50.00%,  total acc: 65.05%   [EVAL] batch:  260 | acc: 18.75%,  total acc: 64.87%   [EVAL] batch:  261 | acc: 31.25%,  total acc: 64.74%   [EVAL] batch:  262 | acc: 56.25%,  total acc: 64.71%   [EVAL] batch:  263 | acc: 68.75%,  total acc: 64.73%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 64.67%   [EVAL] batch:  265 | acc: 50.00%,  total acc: 64.61%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 64.58%   [EVAL] batch:  267 | acc: 50.00%,  total acc: 64.53%   [EVAL] batch:  268 | acc: 37.50%,  total acc: 64.43%   [EVAL] batch:  269 | acc: 25.00%,  total acc: 64.28%   [EVAL] batch:  270 | acc: 50.00%,  total acc: 64.23%   [EVAL] batch:  271 | acc: 37.50%,  total acc: 64.13%   [EVAL] batch:  272 | acc: 25.00%,  total acc: 63.99%   [EVAL] batch:  273 | acc: 56.25%,  total acc: 63.96%   [EVAL] batch:  274 | acc: 12.50%,  total acc: 63.77%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 63.90%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 64.03%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 64.16%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 64.29%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 64.42%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 64.55%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 64.54%   [EVAL] batch:  282 | acc: 68.75%,  total acc: 64.55%   [EVAL] batch:  283 | acc: 62.50%,  total acc: 64.55%   [EVAL] batch:  284 | acc: 68.75%,  total acc: 64.56%   [EVAL] batch:  285 | acc: 62.50%,  total acc: 64.55%   [EVAL] batch:  286 | acc: 56.25%,  total acc: 64.53%   [EVAL] batch:  287 | acc: 62.50%,  total acc: 64.52%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 64.64%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 64.76%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 64.88%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 65.00%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 65.12%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 65.22%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 65.34%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 65.46%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 65.57%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 65.69%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 65.80%   [EVAL] batch:  299 | acc: 93.75%,  total acc: 65.90%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 65.99%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 66.08%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 66.17%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 66.26%   [EVAL] batch:  304 | acc: 87.50%,  total acc: 66.33%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 66.44%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 66.53%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 66.62%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 66.71%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 66.81%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 66.92%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 66.97%   [EVAL] batch:  312 | acc: 56.25%,  total acc: 66.93%   [EVAL] batch:  313 | acc: 25.00%,  total acc: 66.80%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 66.65%   [EVAL] batch:  315 | acc: 6.25%,  total acc: 66.46%   [EVAL] batch:  316 | acc: 25.00%,  total acc: 66.32%   [EVAL] batch:  317 | acc: 12.50%,  total acc: 66.16%   [EVAL] batch:  318 | acc: 50.00%,  total acc: 66.11%   [EVAL] batch:  319 | acc: 68.75%,  total acc: 66.11%   [EVAL] batch:  320 | acc: 75.00%,  total acc: 66.14%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 66.19%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 66.25%   [EVAL] batch:  323 | acc: 62.50%,  total acc: 66.24%   [EVAL] batch:  324 | acc: 62.50%,  total acc: 66.23%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 66.24%   [EVAL] batch:  326 | acc: 50.00%,  total acc: 66.19%   [EVAL] batch:  327 | acc: 68.75%,  total acc: 66.20%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 66.22%   [EVAL] batch:  329 | acc: 56.25%,  total acc: 66.19%   [EVAL] batch:  330 | acc: 75.00%,  total acc: 66.22%   [EVAL] batch:  331 | acc: 12.50%,  total acc: 66.06%   [EVAL] batch:  332 | acc: 6.25%,  total acc: 65.88%   [EVAL] batch:  333 | acc: 6.25%,  total acc: 65.70%   [EVAL] batch:  334 | acc: 31.25%,  total acc: 65.60%   [EVAL] batch:  335 | acc: 18.75%,  total acc: 65.46%   [EVAL] batch:  336 | acc: 0.00%,  total acc: 65.26%   [EVAL] batch:  337 | acc: 6.25%,  total acc: 65.09%   [EVAL] batch:  338 | acc: 6.25%,  total acc: 64.92%   [EVAL] batch:  339 | acc: 6.25%,  total acc: 64.74%   [EVAL] batch:  340 | acc: 0.00%,  total acc: 64.55%   [EVAL] batch:  341 | acc: 6.25%,  total acc: 64.38%   [EVAL] batch:  342 | acc: 12.50%,  total acc: 64.23%   [EVAL] batch:  343 | acc: 25.00%,  total acc: 64.12%   [EVAL] batch:  344 | acc: 81.25%,  total acc: 64.17%   [EVAL] batch:  345 | acc: 87.50%,  total acc: 64.23%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 64.32%   [EVAL] batch:  347 | acc: 100.00%,  total acc: 64.42%   [EVAL] batch:  348 | acc: 87.50%,  total acc: 64.49%   [EVAL] batch:  349 | acc: 75.00%,  total acc: 64.52%   [EVAL] batch:  350 | acc: 87.50%,  total acc: 64.58%   [EVAL] batch:  351 | acc: 87.50%,  total acc: 64.65%   [EVAL] batch:  352 | acc: 87.50%,  total acc: 64.71%   [EVAL] batch:  353 | acc: 81.25%,  total acc: 64.76%   [EVAL] batch:  354 | acc: 87.50%,  total acc: 64.82%   [EVAL] batch:  355 | acc: 87.50%,  total acc: 64.89%   [EVAL] batch:  356 | acc: 43.75%,  total acc: 64.83%   [EVAL] batch:  357 | acc: 37.50%,  total acc: 64.75%   [EVAL] batch:  358 | acc: 31.25%,  total acc: 64.66%   [EVAL] batch:  359 | acc: 18.75%,  total acc: 64.53%   [EVAL] batch:  360 | acc: 37.50%,  total acc: 64.46%   [EVAL] batch:  361 | acc: 25.00%,  total acc: 64.35%   [EVAL] batch:  362 | acc: 25.00%,  total acc: 64.24%   [EVAL] batch:  363 | acc: 68.75%,  total acc: 64.25%   [EVAL] batch:  364 | acc: 37.50%,  total acc: 64.18%   [EVAL] batch:  365 | acc: 62.50%,  total acc: 64.17%   [EVAL] batch:  366 | acc: 62.50%,  total acc: 64.17%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 64.15%   [EVAL] batch:  368 | acc: 68.75%,  total acc: 64.16%   [EVAL] batch:  369 | acc: 43.75%,  total acc: 64.10%   [EVAL] batch:  370 | acc: 62.50%,  total acc: 64.10%   [EVAL] batch:  371 | acc: 68.75%,  total acc: 64.11%   [EVAL] batch:  372 | acc: 62.50%,  total acc: 64.11%   [EVAL] batch:  373 | acc: 75.00%,  total acc: 64.14%   [EVAL] batch:  374 | acc: 68.75%,  total acc: 64.15%   [EVAL] batch:  375 | acc: 62.50%,  total acc: 64.15%   [EVAL] batch:  376 | acc: 56.25%,  total acc: 64.12%   [EVAL] batch:  377 | acc: 37.50%,  total acc: 64.05%   [EVAL] batch:  378 | acc: 56.25%,  total acc: 64.03%   [EVAL] batch:  379 | acc: 56.25%,  total acc: 64.01%   [EVAL] batch:  380 | acc: 62.50%,  total acc: 64.01%   [EVAL] batch:  381 | acc: 87.50%,  total acc: 64.07%   [EVAL] batch:  382 | acc: 93.75%,  total acc: 64.15%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 64.16%   [EVAL] batch:  384 | acc: 81.25%,  total acc: 64.20%   [EVAL] batch:  385 | acc: 87.50%,  total acc: 64.26%   [EVAL] batch:  386 | acc: 93.75%,  total acc: 64.34%   [EVAL] batch:  387 | acc: 62.50%,  total acc: 64.34%   [EVAL] batch:  388 | acc: 18.75%,  total acc: 64.22%   [EVAL] batch:  389 | acc: 31.25%,  total acc: 64.13%   [EVAL] batch:  390 | acc: 12.50%,  total acc: 64.00%   [EVAL] batch:  391 | acc: 37.50%,  total acc: 63.93%   [EVAL] batch:  392 | acc: 43.75%,  total acc: 63.88%   [EVAL] batch:  393 | acc: 37.50%,  total acc: 63.82%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 63.89%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 63.98%   [EVAL] batch:  396 | acc: 93.75%,  total acc: 64.06%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 64.15%   [EVAL] batch:  398 | acc: 100.00%,  total acc: 64.24%   [EVAL] batch:  399 | acc: 93.75%,  total acc: 64.31%   [EVAL] batch:  400 | acc: 0.00%,  total acc: 64.15%   [EVAL] batch:  401 | acc: 6.25%,  total acc: 64.01%   [EVAL] batch:  402 | acc: 6.25%,  total acc: 63.86%   [EVAL] batch:  403 | acc: 12.50%,  total acc: 63.74%   [EVAL] batch:  404 | acc: 12.50%,  total acc: 63.61%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 63.45%   [EVAL] batch:  406 | acc: 68.75%,  total acc: 63.47%   [EVAL] batch:  407 | acc: 87.50%,  total acc: 63.53%   [EVAL] batch:  408 | acc: 75.00%,  total acc: 63.55%   [EVAL] batch:  409 | acc: 93.75%,  total acc: 63.63%   [EVAL] batch:  410 | acc: 87.50%,  total acc: 63.69%   [EVAL] batch:  411 | acc: 87.50%,  total acc: 63.74%   [EVAL] batch:  412 | acc: 75.00%,  total acc: 63.77%   [EVAL] batch:  413 | acc: 50.00%,  total acc: 63.74%   [EVAL] batch:  414 | acc: 68.75%,  total acc: 63.75%   [EVAL] batch:  415 | acc: 31.25%,  total acc: 63.67%   [EVAL] batch:  416 | acc: 43.75%,  total acc: 63.62%   [EVAL] batch:  417 | acc: 62.50%,  total acc: 63.62%   [EVAL] batch:  418 | acc: 62.50%,  total acc: 63.62%   [EVAL] batch:  419 | acc: 81.25%,  total acc: 63.66%   [EVAL] batch:  420 | acc: 81.25%,  total acc: 63.70%   [EVAL] batch:  421 | acc: 87.50%,  total acc: 63.76%   [EVAL] batch:  422 | acc: 75.00%,  total acc: 63.79%   [EVAL] batch:  423 | acc: 87.50%,  total acc: 63.84%   [EVAL] batch:  424 | acc: 81.25%,  total acc: 63.88%   [EVAL] batch:  425 | acc: 68.75%,  total acc: 63.89%   [EVAL] batch:  426 | acc: 62.50%,  total acc: 63.89%   [EVAL] batch:  427 | acc: 68.75%,  total acc: 63.90%   [EVAL] batch:  428 | acc: 56.25%,  total acc: 63.88%   [EVAL] batch:  429 | acc: 75.00%,  total acc: 63.91%   [EVAL] batch:  430 | acc: 56.25%,  total acc: 63.89%   [EVAL] batch:  431 | acc: 62.50%,  total acc: 63.89%   [EVAL] batch:  432 | acc: 75.00%,  total acc: 63.91%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 63.97%   [EVAL] batch:  434 | acc: 68.75%,  total acc: 63.98%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 64.03%   [EVAL] batch:  436 | acc: 75.00%,  total acc: 64.06%   [EVAL] batch:  437 | acc: 43.75%,  total acc: 64.01%   [EVAL] batch:  438 | acc: 43.75%,  total acc: 63.97%   [EVAL] batch:  439 | acc: 25.00%,  total acc: 63.88%   [EVAL] batch:  440 | acc: 50.00%,  total acc: 63.85%   [EVAL] batch:  441 | acc: 50.00%,  total acc: 63.82%   [EVAL] batch:  442 | acc: 43.75%,  total acc: 63.77%   [EVAL] batch:  443 | acc: 31.25%,  total acc: 63.70%   [EVAL] batch:  444 | acc: 62.50%,  total acc: 63.69%   [EVAL] batch:  445 | acc: 68.75%,  total acc: 63.71%   [EVAL] batch:  446 | acc: 68.75%,  total acc: 63.72%   [EVAL] batch:  447 | acc: 75.00%,  total acc: 63.74%   [EVAL] batch:  448 | acc: 31.25%,  total acc: 63.67%   [EVAL] batch:  449 | acc: 100.00%,  total acc: 63.75%   [EVAL] batch:  450 | acc: 75.00%,  total acc: 63.77%   [EVAL] batch:  451 | acc: 56.25%,  total acc: 63.76%   [EVAL] batch:  452 | acc: 75.00%,  total acc: 63.78%   [EVAL] batch:  453 | acc: 56.25%,  total acc: 63.77%   [EVAL] batch:  454 | acc: 68.75%,  total acc: 63.78%   [EVAL] batch:  455 | acc: 81.25%,  total acc: 63.82%   [EVAL] batch:  456 | acc: 81.25%,  total acc: 63.85%   [EVAL] batch:  457 | acc: 100.00%,  total acc: 63.93%   [EVAL] batch:  458 | acc: 100.00%,  total acc: 64.01%   [EVAL] batch:  459 | acc: 93.75%,  total acc: 64.08%   [EVAL] batch:  460 | acc: 93.75%,  total acc: 64.14%   [EVAL] batch:  461 | acc: 100.00%,  total acc: 64.22%   [EVAL] batch:  462 | acc: 81.25%,  total acc: 64.25%   [EVAL] batch:  463 | acc: 68.75%,  total acc: 64.26%   [EVAL] batch:  464 | acc: 50.00%,  total acc: 64.23%   [EVAL] batch:  465 | acc: 68.75%,  total acc: 64.24%   [EVAL] batch:  466 | acc: 56.25%,  total acc: 64.23%   [EVAL] batch:  467 | acc: 56.25%,  total acc: 64.21%   [EVAL] batch:  468 | acc: 50.00%,  total acc: 64.18%   [EVAL] batch:  469 | acc: 50.00%,  total acc: 64.15%   [EVAL] batch:  470 | acc: 25.00%,  total acc: 64.07%   [EVAL] batch:  471 | acc: 12.50%,  total acc: 63.96%   [EVAL] batch:  472 | acc: 25.00%,  total acc: 63.87%   [EVAL] batch:  473 | acc: 12.50%,  total acc: 63.77%   [EVAL] batch:  474 | acc: 37.50%,  total acc: 63.71%   [EVAL] batch:  475 | acc: 56.25%,  total acc: 63.69%   [EVAL] batch:  476 | acc: 62.50%,  total acc: 63.69%   [EVAL] batch:  477 | acc: 50.00%,  total acc: 63.66%   [EVAL] batch:  478 | acc: 81.25%,  total acc: 63.70%   [EVAL] batch:  479 | acc: 75.00%,  total acc: 63.72%   [EVAL] batch:  480 | acc: 68.75%,  total acc: 63.73%   [EVAL] batch:  481 | acc: 81.25%,  total acc: 63.77%   [EVAL] batch:  482 | acc: 68.75%,  total acc: 63.78%   [EVAL] batch:  483 | acc: 75.00%,  total acc: 63.80%   [EVAL] batch:  484 | acc: 62.50%,  total acc: 63.80%   [EVAL] batch:  485 | acc: 62.50%,  total acc: 63.80%   [EVAL] batch:  486 | acc: 87.50%,  total acc: 63.85%   [EVAL] batch:  487 | acc: 93.75%,  total acc: 63.91%   [EVAL] batch:  488 | acc: 100.00%,  total acc: 63.98%   [EVAL] batch:  489 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:  490 | acc: 100.00%,  total acc: 64.13%   [EVAL] batch:  491 | acc: 100.00%,  total acc: 64.20%   [EVAL] batch:  492 | acc: 100.00%,  total acc: 64.27%   [EVAL] batch:  493 | acc: 100.00%,  total acc: 64.35%   [EVAL] batch:  494 | acc: 93.75%,  total acc: 64.41%   [EVAL] batch:  495 | acc: 81.25%,  total acc: 64.44%   [EVAL] batch:  496 | acc: 93.75%,  total acc: 64.50%   [EVAL] batch:  497 | acc: 81.25%,  total acc: 64.53%   [EVAL] batch:  498 | acc: 100.00%,  total acc: 64.60%   [EVAL] batch:  499 | acc: 100.00%,  total acc: 64.68%   
cur_acc:  ['0.9514', '0.8006', '0.7629', '0.7560', '0.8859', '0.6032', '0.6647', '0.6835']
his_acc:  ['0.9514', '0.8715', '0.7999', '0.7435', '0.7518', '0.7073', '0.6799', '0.6468']
--------Round  3
seed:  400
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 0 1 2 5 3 4 6]
prepared data!
CurrentTrain: epoch 15, batch     0 | loss: 22.4878506CurrentTrain: epoch 15, batch     1 | loss: 21.9296248CurrentTrain: epoch 15, batch     2 | loss: 23.6595851CurrentTrain: epoch 15, batch     3 | loss: 31.3667758CurrentTrain: epoch 15, batch     4 | loss: 21.9311011CurrentTrain: epoch 15, batch     5 | loss: 24.9904355CurrentTrain: epoch 15, batch     6 | loss: 21.0380397CurrentTrain: epoch 15, batch     7 | loss: 20.1843764CurrentTrain: epoch 15, batch     8 | loss: 19.5439836CurrentTrain: epoch 15, batch     9 | loss: 24.9898828CurrentTrain: epoch 15, batch    10 | loss: 21.9645808CurrentTrain: epoch 15, batch    11 | loss: 19.5505546CurrentTrain: epoch 15, batch    12 | loss: 20.7411905CurrentTrain: epoch 15, batch    13 | loss: 25.5480521CurrentTrain: epoch 15, batch    14 | loss: 17.5663297CurrentTrain: epoch 15, batch    15 | loss: 23.6006346CurrentTrain: epoch 15, batch    16 | loss: 21.8638703CurrentTrain: epoch 15, batch    17 | loss: 17.4998386CurrentTrain: epoch 15, batch    18 | loss: 16.1320594CurrentTrain: epoch 15, batch    19 | loss: 16.0278725CurrentTrain: epoch 15, batch    20 | loss: 17.1093702CurrentTrain: epoch 15, batch    21 | loss: 12.8116377CurrentTrain: epoch 15, batch    22 | loss: 21.1318607CurrentTrain: epoch 15, batch    23 | loss: 15.3643784CurrentTrain: epoch 15, batch    24 | loss: 13.8579313CurrentTrain: epoch 15, batch    25 | loss: 15.6563823CurrentTrain: epoch 15, batch    26 | loss: 20.5169766CurrentTrain: epoch 15, batch    27 | loss: 15.9607040CurrentTrain: epoch 15, batch    28 | loss: 18.9212570CurrentTrain: epoch 15, batch    29 | loss: 19.1882482CurrentTrain: epoch 15, batch    30 | loss: 22.0630479CurrentTrain: epoch 15, batch    31 | loss: 29.2019326CurrentTrain: epoch 15, batch    32 | loss: 14.1679923CurrentTrain: epoch 15, batch    33 | loss: 16.8414898CurrentTrain: epoch 15, batch    34 | loss: 14.5314342CurrentTrain: epoch 15, batch    35 | loss: 12.7467896CurrentTrain: epoch 15, batch    36 | loss: 14.6267017CurrentTrain: epoch 15, batch    37 | loss: 15.0634209CurrentTrain: epoch 15, batch    38 | loss: 14.3197848CurrentTrain: epoch 15, batch    39 | loss: 10.6940645CurrentTrain: epoch 15, batch    40 | loss: 23.9816559CurrentTrain: epoch 15, batch    41 | loss: 18.0019683CurrentTrain: epoch 15, batch    42 | loss: 32.0525127CurrentTrain: epoch 15, batch    43 | loss: 22.7818676CurrentTrain: epoch 15, batch    44 | loss: 19.8203214CurrentTrain: epoch 15, batch    45 | loss: 24.9253379CurrentTrain: epoch 15, batch    46 | loss: 14.7652344CurrentTrain: epoch 15, batch    47 | loss: 20.1736975CurrentTrain: epoch 15, batch    48 | loss: 13.4178131CurrentTrain: epoch 15, batch    49 | loss: 15.6022156CurrentTrain: epoch 15, batch    50 | loss: 18.1621800CurrentTrain: epoch 15, batch    51 | loss: 19.3777102CurrentTrain: epoch 15, batch    52 | loss: 18.6120572CurrentTrain: epoch 15, batch    53 | loss: 14.7082556CurrentTrain: epoch 15, batch    54 | loss: 17.6165816CurrentTrain: epoch 15, batch    55 | loss: 12.8915436CurrentTrain: epoch 15, batch    56 | loss: 17.0224457CurrentTrain: epoch 15, batch    57 | loss: 17.8830443CurrentTrain: epoch 15, batch    58 | loss: 18.7588901CurrentTrain: epoch 15, batch    59 | loss: 11.5620490CurrentTrain: epoch 15, batch    60 | loss: 19.2728722CurrentTrain: epoch 15, batch    61 | loss: 30.5373785CurrentTrain: epoch  7, batch    62 | loss: 17.8458928CurrentTrain: epoch 15, batch     0 | loss: 19.7506168CurrentTrain: epoch 15, batch     1 | loss: 21.9057880CurrentTrain: epoch 15, batch     2 | loss: 19.8816572CurrentTrain: epoch 15, batch     3 | loss: 13.7225902CurrentTrain: epoch 15, batch     4 | loss: 13.0240220CurrentTrain: epoch 15, batch     5 | loss: 13.9344174CurrentTrain: epoch 15, batch     6 | loss: 24.7345906CurrentTrain: epoch 15, batch     7 | loss: 18.2660881CurrentTrain: epoch 15, batch     8 | loss: 16.0968679CurrentTrain: epoch 15, batch     9 | loss: 11.4468135CurrentTrain: epoch 15, batch    10 | loss: 19.0079828CurrentTrain: epoch 15, batch    11 | loss: 14.2545517CurrentTrain: epoch 15, batch    12 | loss: 11.6964641CurrentTrain: epoch 15, batch    13 | loss: 14.4156322CurrentTrain: epoch 15, batch    14 | loss: 13.7310982CurrentTrain: epoch 15, batch    15 | loss: 12.8883423CurrentTrain: epoch 15, batch    16 | loss: 12.2806629CurrentTrain: epoch 15, batch    17 | loss: 17.9253633CurrentTrain: epoch 15, batch    18 | loss: 29.2266066CurrentTrain: epoch 15, batch    19 | loss: 14.1060297CurrentTrain: epoch 15, batch    20 | loss: 20.6252739CurrentTrain: epoch 15, batch    21 | loss: 17.8885705CurrentTrain: epoch 15, batch    22 | loss: 12.3268754CurrentTrain: epoch 15, batch    23 | loss: 17.9744758CurrentTrain: epoch 15, batch    24 | loss: 9.8081268CurrentTrain: epoch 15, batch    25 | loss: 14.4422586CurrentTrain: epoch 15, batch    26 | loss: 16.6165064CurrentTrain: epoch 15, batch    27 | loss: 11.8366333CurrentTrain: epoch 15, batch    28 | loss: 13.8819406CurrentTrain: epoch 15, batch    29 | loss: 14.3437395CurrentTrain: epoch 15, batch    30 | loss: 13.4028710CurrentTrain: epoch 15, batch    31 | loss: 11.6104760CurrentTrain: epoch 15, batch    32 | loss: 14.2093472CurrentTrain: epoch 15, batch    33 | loss: 14.2982760CurrentTrain: epoch 15, batch    34 | loss: 11.6158815CurrentTrain: epoch 15, batch    35 | loss: 14.7216505CurrentTrain: epoch 15, batch    36 | loss: 16.0891400CurrentTrain: epoch 15, batch    37 | loss: 13.2472454CurrentTrain: epoch 15, batch    38 | loss: 17.1049596CurrentTrain: epoch 15, batch    39 | loss: 13.4860976CurrentTrain: epoch 15, batch    40 | loss: 13.2845470CurrentTrain: epoch 15, batch    41 | loss: 17.7272779CurrentTrain: epoch 15, batch    42 | loss: 13.2338790CurrentTrain: epoch 15, batch    43 | loss: 13.4171578CurrentTrain: epoch 15, batch    44 | loss: 13.2132018CurrentTrain: epoch 15, batch    45 | loss: 11.7335628CurrentTrain: epoch 15, batch    46 | loss: 27.5515758CurrentTrain: epoch 15, batch    47 | loss: 13.2758470CurrentTrain: epoch 15, batch    48 | loss: 19.9952953CurrentTrain: epoch 15, batch    49 | loss: 11.3596423CurrentTrain: epoch 15, batch    50 | loss: 10.9310750CurrentTrain: epoch 15, batch    51 | loss: 10.9370047CurrentTrain: epoch 15, batch    52 | loss: 15.7643100CurrentTrain: epoch 15, batch    53 | loss: 21.3652669CurrentTrain: epoch 15, batch    54 | loss: 26.7860402CurrentTrain: epoch 15, batch    55 | loss: 12.0402165CurrentTrain: epoch 15, batch    56 | loss: 11.7127484CurrentTrain: epoch 15, batch    57 | loss: 13.5728014CurrentTrain: epoch 15, batch    58 | loss: 15.2008886CurrentTrain: epoch 15, batch    59 | loss: 13.2604193CurrentTrain: epoch 15, batch    60 | loss: 21.2906714CurrentTrain: epoch 15, batch    61 | loss: 11.0276709CurrentTrain: epoch  7, batch    62 | loss: 12.6510564CurrentTrain: epoch 15, batch     0 | loss: 11.0635804CurrentTrain: epoch 15, batch     1 | loss: 16.8296839CurrentTrain: epoch 15, batch     2 | loss: 12.8993508CurrentTrain: epoch 15, batch     3 | loss: 9.8915838CurrentTrain: epoch 15, batch     4 | loss: 12.1385022CurrentTrain: epoch 15, batch     5 | loss: 11.0782209CurrentTrain: epoch 15, batch     6 | loss: 14.8828321CurrentTrain: epoch 15, batch     7 | loss: 20.7181946CurrentTrain: epoch 15, batch     8 | loss: 11.1163819CurrentTrain: epoch 15, batch     9 | loss: 15.3981026CurrentTrain: epoch 15, batch    10 | loss: 12.3888055CurrentTrain: epoch 15, batch    11 | loss: 12.2385740CurrentTrain: epoch 15, batch    12 | loss: 16.2946606CurrentTrain: epoch 15, batch    13 | loss: 8.7230123CurrentTrain: epoch 15, batch    14 | loss: 14.2201449CurrentTrain: epoch 15, batch    15 | loss: 12.0900378CurrentTrain: epoch 15, batch    16 | loss: 10.0793289CurrentTrain: epoch 15, batch    17 | loss: 12.5260111CurrentTrain: epoch 15, batch    18 | loss: 17.3366338CurrentTrain: epoch 15, batch    19 | loss: 14.7910226CurrentTrain: epoch 15, batch    20 | loss: 12.1648731CurrentTrain: epoch 15, batch    21 | loss: 10.1447942CurrentTrain: epoch 15, batch    22 | loss: 16.4727869CurrentTrain: epoch 15, batch    23 | loss: 38.5218418CurrentTrain: epoch 15, batch    24 | loss: 12.8466684CurrentTrain: epoch 15, batch    25 | loss: 16.8629332CurrentTrain: epoch 15, batch    26 | loss: 12.3811632CurrentTrain: epoch 15, batch    27 | loss: 19.9958587CurrentTrain: epoch 15, batch    28 | loss: 16.4314936CurrentTrain: epoch 15, batch    29 | loss: 17.6859047CurrentTrain: epoch 15, batch    30 | loss: 13.0810083CurrentTrain: epoch 15, batch    31 | loss: 12.7234967CurrentTrain: epoch 15, batch    32 | loss: 10.7332148CurrentTrain: epoch 15, batch    33 | loss: 22.9798811CurrentTrain: epoch 15, batch    34 | loss: 25.0974337CurrentTrain: epoch 15, batch    35 | loss: 12.3882194CurrentTrain: epoch 15, batch    36 | loss: 11.5592954CurrentTrain: epoch 15, batch    37 | loss: 12.9267081CurrentTrain: epoch 15, batch    38 | loss: 10.3986267CurrentTrain: epoch 15, batch    39 | loss: 16.4092186CurrentTrain: epoch 15, batch    40 | loss: 13.1151029CurrentTrain: epoch 15, batch    41 | loss: 14.8926056CurrentTrain: epoch 15, batch    42 | loss: 12.5379810CurrentTrain: epoch 15, batch    43 | loss: 11.5615913CurrentTrain: epoch 15, batch    44 | loss: 10.5982510CurrentTrain: epoch 15, batch    45 | loss: 11.8600538CurrentTrain: epoch 15, batch    46 | loss: 14.6925788CurrentTrain: epoch 15, batch    47 | loss: 9.7193669CurrentTrain: epoch 15, batch    48 | loss: 15.4461859CurrentTrain: epoch 15, batch    49 | loss: 12.4492306CurrentTrain: epoch 15, batch    50 | loss: 11.2176107CurrentTrain: epoch 15, batch    51 | loss: 25.7019443CurrentTrain: epoch 15, batch    52 | loss: 10.9823407CurrentTrain: epoch 15, batch    53 | loss: 10.2240094CurrentTrain: epoch 15, batch    54 | loss: 16.5937559CurrentTrain: epoch 15, batch    55 | loss: 9.5177623CurrentTrain: epoch 15, batch    56 | loss: 16.7742232CurrentTrain: epoch 15, batch    57 | loss: 11.7490653CurrentTrain: epoch 15, batch    58 | loss: 11.5920160CurrentTrain: epoch 15, batch    59 | loss: 9.3579533CurrentTrain: epoch 15, batch    60 | loss: 12.7076040CurrentTrain: epoch 15, batch    61 | loss: 11.1334692CurrentTrain: epoch  7, batch    62 | loss: 8.0971205CurrentTrain: epoch 15, batch     0 | loss: 11.7838705CurrentTrain: epoch 15, batch     1 | loss: 11.1717817CurrentTrain: epoch 15, batch     2 | loss: 10.2944036CurrentTrain: epoch 15, batch     3 | loss: 17.2193838CurrentTrain: epoch 15, batch     4 | loss: 17.0841084CurrentTrain: epoch 15, batch     5 | loss: 22.7762587CurrentTrain: epoch 15, batch     6 | loss: 21.4243065CurrentTrain: epoch 15, batch     7 | loss: 15.6889952CurrentTrain: epoch 15, batch     8 | loss: 12.0709002CurrentTrain: epoch 15, batch     9 | loss: 11.9215269CurrentTrain: epoch 15, batch    10 | loss: 8.9719100CurrentTrain: epoch 15, batch    11 | loss: 22.1379832CurrentTrain: epoch 15, batch    12 | loss: 23.0339028CurrentTrain: epoch 15, batch    13 | loss: 9.6939835CurrentTrain: epoch 15, batch    14 | loss: 12.6926417CurrentTrain: epoch 15, batch    15 | loss: 10.5959732CurrentTrain: epoch 15, batch    16 | loss: 19.7043470CurrentTrain: epoch 15, batch    17 | loss: 16.2189707CurrentTrain: epoch 15, batch    18 | loss: 24.9284648CurrentTrain: epoch 15, batch    19 | loss: 11.0975246CurrentTrain: epoch 15, batch    20 | loss: 8.9965606CurrentTrain: epoch 15, batch    21 | loss: 16.0634168CurrentTrain: epoch 15, batch    22 | loss: 12.8377747CurrentTrain: epoch 15, batch    23 | loss: 10.1736083CurrentTrain: epoch 15, batch    24 | loss: 13.2128154CurrentTrain: epoch 15, batch    25 | loss: 9.3853620CurrentTrain: epoch 15, batch    26 | loss: 17.7518988CurrentTrain: epoch 15, batch    27 | loss: 12.2603193CurrentTrain: epoch 15, batch    28 | loss: 12.4025263CurrentTrain: epoch 15, batch    29 | loss: 18.8821901CurrentTrain: epoch 15, batch    30 | loss: 16.8281054CurrentTrain: epoch 15, batch    31 | loss: 15.8830720CurrentTrain: epoch 15, batch    32 | loss: 27.9520033CurrentTrain: epoch 15, batch    33 | loss: 11.4831710CurrentTrain: epoch 15, batch    34 | loss: 14.3166013CurrentTrain: epoch 15, batch    35 | loss: 13.7475594CurrentTrain: epoch 15, batch    36 | loss: 12.9398762CurrentTrain: epoch 15, batch    37 | loss: 10.2539949CurrentTrain: epoch 15, batch    38 | loss: 9.7937927CurrentTrain: epoch 15, batch    39 | loss: 14.3480463CurrentTrain: epoch 15, batch    40 | loss: 12.4680201CurrentTrain: epoch 15, batch    41 | loss: 8.9738935CurrentTrain: epoch 15, batch    42 | loss: 23.3242229CurrentTrain: epoch 15, batch    43 | loss: 7.8850909CurrentTrain: epoch 15, batch    44 | loss: 11.7079286CurrentTrain: epoch 15, batch    45 | loss: 15.9884669CurrentTrain: epoch 15, batch    46 | loss: 9.6183697CurrentTrain: epoch 15, batch    47 | loss: 11.5268493CurrentTrain: epoch 15, batch    48 | loss: 12.3057346CurrentTrain: epoch 15, batch    49 | loss: 9.2161386CurrentTrain: epoch 15, batch    50 | loss: 37.4647081CurrentTrain: epoch 15, batch    51 | loss: 15.7145393CurrentTrain: epoch 15, batch    52 | loss: 9.5531022CurrentTrain: epoch 15, batch    53 | loss: 9.8802712CurrentTrain: epoch 15, batch    54 | loss: 8.6345350CurrentTrain: epoch 15, batch    55 | loss: 13.3408976CurrentTrain: epoch 15, batch    56 | loss: 12.3225779CurrentTrain: epoch 15, batch    57 | loss: 7.6287329CurrentTrain: epoch 15, batch    58 | loss: 24.7766444CurrentTrain: epoch 15, batch    59 | loss: 15.2400227CurrentTrain: epoch 15, batch    60 | loss: 13.6353083CurrentTrain: epoch 15, batch    61 | loss: 14.2412492CurrentTrain: epoch  7, batch    62 | loss: 8.2819778CurrentTrain: epoch 15, batch     0 | loss: 8.4649168CurrentTrain: epoch 15, batch     1 | loss: 11.9227593CurrentTrain: epoch 15, batch     2 | loss: 13.4781085CurrentTrain: epoch 15, batch     3 | loss: 25.8762014CurrentTrain: epoch 15, batch     4 | loss: 8.7378910CurrentTrain: epoch 15, batch     5 | loss: 8.3664283CurrentTrain: epoch 15, batch     6 | loss: 26.4772150CurrentTrain: epoch 15, batch     7 | loss: 13.9621203CurrentTrain: epoch 15, batch     8 | loss: 9.2332645CurrentTrain: epoch 15, batch     9 | loss: 10.0574065CurrentTrain: epoch 15, batch    10 | loss: 16.9954563CurrentTrain: epoch 15, batch    11 | loss: 9.4046611CurrentTrain: epoch 15, batch    12 | loss: 17.5597652CurrentTrain: epoch 15, batch    13 | loss: 13.3226557CurrentTrain: epoch 15, batch    14 | loss: 9.0796999CurrentTrain: epoch 15, batch    15 | loss: 15.5075247CurrentTrain: epoch 15, batch    16 | loss: 22.8667717CurrentTrain: epoch 15, batch    17 | loss: 9.7685738CurrentTrain: epoch 15, batch    18 | loss: 12.3223939CurrentTrain: epoch 15, batch    19 | loss: 9.1944559CurrentTrain: epoch 15, batch    20 | loss: 9.4136570CurrentTrain: epoch 15, batch    21 | loss: 19.0128346CurrentTrain: epoch 15, batch    22 | loss: 16.4737877CurrentTrain: epoch 15, batch    23 | loss: 6.9026708CurrentTrain: epoch 15, batch    24 | loss: 37.2949291CurrentTrain: epoch 15, batch    25 | loss: 26.8052994CurrentTrain: epoch 15, batch    26 | loss: 10.9651188CurrentTrain: epoch 15, batch    27 | loss: 13.9628629CurrentTrain: epoch 15, batch    28 | loss: 7.4445498CurrentTrain: epoch 15, batch    29 | loss: 17.8076907CurrentTrain: epoch 15, batch    30 | loss: 9.6103408CurrentTrain: epoch 15, batch    31 | loss: 7.8542925CurrentTrain: epoch 15, batch    32 | loss: 11.7826792CurrentTrain: epoch 15, batch    33 | loss: 8.8143258CurrentTrain: epoch 15, batch    34 | loss: 9.2206107CurrentTrain: epoch 15, batch    35 | loss: 11.5001413CurrentTrain: epoch 15, batch    36 | loss: 10.0765325CurrentTrain: epoch 15, batch    37 | loss: 10.2889013CurrentTrain: epoch 15, batch    38 | loss: 10.7017097CurrentTrain: epoch 15, batch    39 | loss: 11.4791317CurrentTrain: epoch 15, batch    40 | loss: 13.6300606CurrentTrain: epoch 15, batch    41 | loss: 7.8126437CurrentTrain: epoch 15, batch    42 | loss: 10.9997680CurrentTrain: epoch 15, batch    43 | loss: 10.9274339CurrentTrain: epoch 15, batch    44 | loss: 17.0476296CurrentTrain: epoch 15, batch    45 | loss: 14.3674224CurrentTrain: epoch 15, batch    46 | loss: 12.1732062CurrentTrain: epoch 15, batch    47 | loss: 13.4543689CurrentTrain: epoch 15, batch    48 | loss: 23.8021792CurrentTrain: epoch 15, batch    49 | loss: 13.3860410CurrentTrain: epoch 15, batch    50 | loss: 7.9326897CurrentTrain: epoch 15, batch    51 | loss: 12.0855313CurrentTrain: epoch 15, batch    52 | loss: 11.4112763CurrentTrain: epoch 15, batch    53 | loss: 12.2576222CurrentTrain: epoch 15, batch    54 | loss: 16.2744044CurrentTrain: epoch 15, batch    55 | loss: 17.7354780CurrentTrain: epoch 15, batch    56 | loss: 9.3128536CurrentTrain: epoch 15, batch    57 | loss: 10.2173376CurrentTrain: epoch 15, batch    58 | loss: 10.6449902CurrentTrain: epoch 15, batch    59 | loss: 15.4055781CurrentTrain: epoch 15, batch    60 | loss: 36.6102671CurrentTrain: epoch 15, batch    61 | loss: 7.6588613CurrentTrain: epoch  7, batch    62 | loss: 11.7187474CurrentTrain: epoch 15, batch     0 | loss: 14.5890783CurrentTrain: epoch 15, batch     1 | loss: 14.4203028CurrentTrain: epoch 15, batch     2 | loss: 6.6855050CurrentTrain: epoch 15, batch     3 | loss: 8.7478995CurrentTrain: epoch 15, batch     4 | loss: 9.6274762CurrentTrain: epoch 15, batch     5 | loss: 10.1166936CurrentTrain: epoch 15, batch     6 | loss: 12.3369337CurrentTrain: epoch 15, batch     7 | loss: 10.6735192CurrentTrain: epoch 15, batch     8 | loss: 15.0918662CurrentTrain: epoch 15, batch     9 | loss: 9.9677516CurrentTrain: epoch 15, batch    10 | loss: 12.6578603CurrentTrain: epoch 15, batch    11 | loss: 9.3577207CurrentTrain: epoch 15, batch    12 | loss: 9.5936630CurrentTrain: epoch 15, batch    13 | loss: 8.8308177CurrentTrain: epoch 15, batch    14 | loss: 14.7382565CurrentTrain: epoch 15, batch    15 | loss: 11.2334802CurrentTrain: epoch 15, batch    16 | loss: 14.9061267CurrentTrain: epoch 15, batch    17 | loss: 12.9165070CurrentTrain: epoch 15, batch    18 | loss: 7.9439074CurrentTrain: epoch 15, batch    19 | loss: 8.5385068CurrentTrain: epoch 15, batch    20 | loss: 8.9855094CurrentTrain: epoch 15, batch    21 | loss: 10.4892354CurrentTrain: epoch 15, batch    22 | loss: 11.1484509CurrentTrain: epoch 15, batch    23 | loss: 8.3171478CurrentTrain: epoch 15, batch    24 | loss: 9.4668313CurrentTrain: epoch 15, batch    25 | loss: 9.6808410CurrentTrain: epoch 15, batch    26 | loss: 12.9408628CurrentTrain: epoch 15, batch    27 | loss: 10.6778851CurrentTrain: epoch 15, batch    28 | loss: 9.0249014CurrentTrain: epoch 15, batch    29 | loss: 8.4709821CurrentTrain: epoch 15, batch    30 | loss: 14.9412254CurrentTrain: epoch 15, batch    31 | loss: 9.8005986CurrentTrain: epoch 15, batch    32 | loss: 8.9852840CurrentTrain: epoch 15, batch    33 | loss: 8.0927652CurrentTrain: epoch 15, batch    34 | loss: 10.5553325CurrentTrain: epoch 15, batch    35 | loss: 9.6182208CurrentTrain: epoch 15, batch    36 | loss: 10.6984040CurrentTrain: epoch 15, batch    37 | loss: 15.9244194CurrentTrain: epoch 15, batch    38 | loss: 29.3486420CurrentTrain: epoch 15, batch    39 | loss: 16.6835335CurrentTrain: epoch 15, batch    40 | loss: 6.3323832CurrentTrain: epoch 15, batch    41 | loss: 8.8314313CurrentTrain: epoch 15, batch    42 | loss: 13.7498460CurrentTrain: epoch 15, batch    43 | loss: 14.8471893CurrentTrain: epoch 15, batch    44 | loss: 13.5903809CurrentTrain: epoch 15, batch    45 | loss: 12.1708547CurrentTrain: epoch 15, batch    46 | loss: 8.8116134CurrentTrain: epoch 15, batch    47 | loss: 17.4317128CurrentTrain: epoch 15, batch    48 | loss: 22.9155496CurrentTrain: epoch 15, batch    49 | loss: 18.8119527CurrentTrain: epoch 15, batch    50 | loss: 8.3051967CurrentTrain: epoch 15, batch    51 | loss: 14.6574545CurrentTrain: epoch 15, batch    52 | loss: 14.0937489CurrentTrain: epoch 15, batch    53 | loss: 21.7991861CurrentTrain: epoch 15, batch    54 | loss: 17.2180803CurrentTrain: epoch 15, batch    55 | loss: 15.2365708CurrentTrain: epoch 15, batch    56 | loss: 14.7386738CurrentTrain: epoch 15, batch    57 | loss: 14.5951863CurrentTrain: epoch 15, batch    58 | loss: 7.9665971CurrentTrain: epoch 15, batch    59 | loss: 15.5912784CurrentTrain: epoch 15, batch    60 | loss: 6.7259305CurrentTrain: epoch 15, batch    61 | loss: 18.7914159CurrentTrain: epoch  7, batch    62 | loss: 5.3708059CurrentTrain: epoch 15, batch     0 | loss: 14.0215087CurrentTrain: epoch 15, batch     1 | loss: 16.1262714CurrentTrain: epoch 15, batch     2 | loss: 9.2321173CurrentTrain: epoch 15, batch     3 | loss: 9.8036850CurrentTrain: epoch 15, batch     4 | loss: 14.6281981CurrentTrain: epoch 15, batch     5 | loss: 10.4668166CurrentTrain: epoch 15, batch     6 | loss: 7.5095098CurrentTrain: epoch 15, batch     7 | loss: 7.6311639CurrentTrain: epoch 15, batch     8 | loss: 18.8161160CurrentTrain: epoch 15, batch     9 | loss: 8.7755845CurrentTrain: epoch 15, batch    10 | loss: 21.2957709CurrentTrain: epoch 15, batch    11 | loss: 16.3907550CurrentTrain: epoch 15, batch    12 | loss: 16.0387514CurrentTrain: epoch 15, batch    13 | loss: 12.9081320CurrentTrain: epoch 15, batch    14 | loss: 15.7930261CurrentTrain: epoch 15, batch    15 | loss: 13.7229699CurrentTrain: epoch 15, batch    16 | loss: 12.2640384CurrentTrain: epoch 15, batch    17 | loss: 11.3161214CurrentTrain: epoch 15, batch    18 | loss: 6.3090878CurrentTrain: epoch 15, batch    19 | loss: 17.0037019CurrentTrain: epoch 15, batch    20 | loss: 11.8683813CurrentTrain: epoch 15, batch    21 | loss: 9.0314864CurrentTrain: epoch 15, batch    22 | loss: 11.2407392CurrentTrain: epoch 15, batch    23 | loss: 15.1382183CurrentTrain: epoch 15, batch    24 | loss: 13.3276820CurrentTrain: epoch 15, batch    25 | loss: 11.5199481CurrentTrain: epoch 15, batch    26 | loss: 9.5179673CurrentTrain: epoch 15, batch    27 | loss: 10.1581875CurrentTrain: epoch 15, batch    28 | loss: 12.7611961CurrentTrain: epoch 15, batch    29 | loss: 16.6330584CurrentTrain: epoch 15, batch    30 | loss: 9.6129582CurrentTrain: epoch 15, batch    31 | loss: 13.1503323CurrentTrain: epoch 15, batch    32 | loss: 10.9648708CurrentTrain: epoch 15, batch    33 | loss: 9.5788927CurrentTrain: epoch 15, batch    34 | loss: 15.7419584CurrentTrain: epoch 15, batch    35 | loss: 9.1504468CurrentTrain: epoch 15, batch    36 | loss: 13.5486383CurrentTrain: epoch 15, batch    37 | loss: 11.1573061CurrentTrain: epoch 15, batch    38 | loss: 8.1241304CurrentTrain: epoch 15, batch    39 | loss: 7.8205654CurrentTrain: epoch 15, batch    40 | loss: 10.2536103CurrentTrain: epoch 15, batch    41 | loss: 10.3511144CurrentTrain: epoch 15, batch    42 | loss: 11.6944736CurrentTrain: epoch 15, batch    43 | loss: 11.0083839CurrentTrain: epoch 15, batch    44 | loss: 10.2377154CurrentTrain: epoch 15, batch    45 | loss: 13.3057211CurrentTrain: epoch 15, batch    46 | loss: 7.1332567CurrentTrain: epoch 15, batch    47 | loss: 10.7622289CurrentTrain: epoch 15, batch    48 | loss: 7.5430033CurrentTrain: epoch 15, batch    49 | loss: 8.7458235CurrentTrain: epoch 15, batch    50 | loss: 8.8682814CurrentTrain: epoch 15, batch    51 | loss: 20.7111421CurrentTrain: epoch 15, batch    52 | loss: 14.9213066CurrentTrain: epoch 15, batch    53 | loss: 7.1440207CurrentTrain: epoch 15, batch    54 | loss: 10.8123363CurrentTrain: epoch 15, batch    55 | loss: 7.7175008CurrentTrain: epoch 15, batch    56 | loss: 12.9091108CurrentTrain: epoch 15, batch    57 | loss: 9.9292126CurrentTrain: epoch 15, batch    58 | loss: 10.1530980CurrentTrain: epoch 15, batch    59 | loss: 15.4047515CurrentTrain: epoch 15, batch    60 | loss: 9.4581468CurrentTrain: epoch 15, batch    61 | loss: 9.8828748CurrentTrain: epoch  7, batch    62 | loss: 15.0782700CurrentTrain: epoch 15, batch     0 | loss: 13.4592401CurrentTrain: epoch 15, batch     1 | loss: 11.1823423CurrentTrain: epoch 15, batch     2 | loss: 8.4949149CurrentTrain: epoch 15, batch     3 | loss: 14.3771114CurrentTrain: epoch 15, batch     4 | loss: 7.8668613CurrentTrain: epoch 15, batch     5 | loss: 15.4283896CurrentTrain: epoch 15, batch     6 | loss: 10.9850192CurrentTrain: epoch 15, batch     7 | loss: 10.2244247CurrentTrain: epoch 15, batch     8 | loss: 8.1474755CurrentTrain: epoch 15, batch     9 | loss: 7.4908448CurrentTrain: epoch 15, batch    10 | loss: 18.1476036CurrentTrain: epoch 15, batch    11 | loss: 9.1349537CurrentTrain: epoch 15, batch    12 | loss: 10.4644544CurrentTrain: epoch 15, batch    13 | loss: 14.3387278CurrentTrain: epoch 15, batch    14 | loss: 7.4879040CurrentTrain: epoch 15, batch    15 | loss: 8.9956043CurrentTrain: epoch 15, batch    16 | loss: 9.4900118CurrentTrain: epoch 15, batch    17 | loss: 12.7016405CurrentTrain: epoch 15, batch    18 | loss: 8.1612639CurrentTrain: epoch 15, batch    19 | loss: 9.8437595CurrentTrain: epoch 15, batch    20 | loss: 12.0153178CurrentTrain: epoch 15, batch    21 | loss: 12.2353749CurrentTrain: epoch 15, batch    22 | loss: 10.2285727CurrentTrain: epoch 15, batch    23 | loss: 13.0422848CurrentTrain: epoch 15, batch    24 | loss: 10.3970691CurrentTrain: epoch 15, batch    25 | loss: 15.5172918CurrentTrain: epoch 15, batch    26 | loss: 8.9061284CurrentTrain: epoch 15, batch    27 | loss: 9.8094000CurrentTrain: epoch 15, batch    28 | loss: 10.2579393CurrentTrain: epoch 15, batch    29 | loss: 21.0513769CurrentTrain: epoch 15, batch    30 | loss: 10.9542284CurrentTrain: epoch 15, batch    31 | loss: 9.6513346CurrentTrain: epoch 15, batch    32 | loss: 8.6389311CurrentTrain: epoch 15, batch    33 | loss: 11.0305998CurrentTrain: epoch 15, batch    34 | loss: 9.8924500CurrentTrain: epoch 15, batch    35 | loss: 14.0178624CurrentTrain: epoch 15, batch    36 | loss: 15.6541531CurrentTrain: epoch 15, batch    37 | loss: 6.1589957CurrentTrain: epoch 15, batch    38 | loss: 32.4725886CurrentTrain: epoch 15, batch    39 | loss: 10.2017416CurrentTrain: epoch 15, batch    40 | loss: 10.6780734CurrentTrain: epoch 15, batch    41 | loss: 8.7069314CurrentTrain: epoch 15, batch    42 | loss: 20.7552588CurrentTrain: epoch 15, batch    43 | loss: 9.1844623CurrentTrain: epoch 15, batch    44 | loss: 14.9243887CurrentTrain: epoch 15, batch    45 | loss: 11.3048094CurrentTrain: epoch 15, batch    46 | loss: 13.4711476CurrentTrain: epoch 15, batch    47 | loss: 14.2252160CurrentTrain: epoch 15, batch    48 | loss: 9.3540860CurrentTrain: epoch 15, batch    49 | loss: 19.9459033CurrentTrain: epoch 15, batch    50 | loss: 8.0691921CurrentTrain: epoch 15, batch    51 | loss: 12.2290406CurrentTrain: epoch 15, batch    52 | loss: 10.6002630CurrentTrain: epoch 15, batch    53 | loss: 21.1445478CurrentTrain: epoch 15, batch    54 | loss: 5.9324117CurrentTrain: epoch 15, batch    55 | loss: 8.2990239CurrentTrain: epoch 15, batch    56 | loss: 11.2145824CurrentTrain: epoch 15, batch    57 | loss: 9.8586210CurrentTrain: epoch 15, batch    58 | loss: 13.1709991CurrentTrain: epoch 15, batch    59 | loss: 11.8458781CurrentTrain: epoch 15, batch    60 | loss: 22.8678812CurrentTrain: epoch 15, batch    61 | loss: 12.1377179CurrentTrain: epoch  7, batch    62 | loss: 20.2143754CurrentTrain: epoch 15, batch     0 | loss: 10.6115294CurrentTrain: epoch 15, batch     1 | loss: 12.4352751CurrentTrain: epoch 15, batch     2 | loss: 11.1373546CurrentTrain: epoch 15, batch     3 | loss: 9.1310268CurrentTrain: epoch 15, batch     4 | loss: 13.2242638CurrentTrain: epoch 15, batch     5 | loss: 9.2956292CurrentTrain: epoch 15, batch     6 | loss: 6.6763958CurrentTrain: epoch 15, batch     7 | loss: 8.6366409CurrentTrain: epoch 15, batch     8 | loss: 14.3215045CurrentTrain: epoch 15, batch     9 | loss: 17.6059450CurrentTrain: epoch 15, batch    10 | loss: 8.6191150CurrentTrain: epoch 15, batch    11 | loss: 11.9130275CurrentTrain: epoch 15, batch    12 | loss: 11.2523239CurrentTrain: epoch 15, batch    13 | loss: 16.0737675CurrentTrain: epoch 15, batch    14 | loss: 9.7202127CurrentTrain: epoch 15, batch    15 | loss: 6.8722634CurrentTrain: epoch 15, batch    16 | loss: 11.3328612CurrentTrain: epoch 15, batch    17 | loss: 9.7229963CurrentTrain: epoch 15, batch    18 | loss: 14.1630849CurrentTrain: epoch 15, batch    19 | loss: 10.8663677CurrentTrain: epoch 15, batch    20 | loss: 11.5334912CurrentTrain: epoch 15, batch    21 | loss: 10.2248369CurrentTrain: epoch 15, batch    22 | loss: 13.2947450CurrentTrain: epoch 15, batch    23 | loss: 10.0847502CurrentTrain: epoch 15, batch    24 | loss: 15.4735070CurrentTrain: epoch 15, batch    25 | loss: 8.8365663CurrentTrain: epoch 15, batch    26 | loss: 7.6515148CurrentTrain: epoch 15, batch    27 | loss: 9.3653402CurrentTrain: epoch 15, batch    28 | loss: 8.9313608CurrentTrain: epoch 15, batch    29 | loss: 11.2360961CurrentTrain: epoch 15, batch    30 | loss: 12.8499787CurrentTrain: epoch 15, batch    31 | loss: 8.2522533CurrentTrain: epoch 15, batch    32 | loss: 15.3098670CurrentTrain: epoch 15, batch    33 | loss: 9.0633589CurrentTrain: epoch 15, batch    34 | loss: 9.4127156CurrentTrain: epoch 15, batch    35 | loss: 15.2408871CurrentTrain: epoch 15, batch    36 | loss: 10.5807192CurrentTrain: epoch 15, batch    37 | loss: 11.8548837CurrentTrain: epoch 15, batch    38 | loss: 18.2369312CurrentTrain: epoch 15, batch    39 | loss: 11.6557938CurrentTrain: epoch 15, batch    40 | loss: 12.3444978CurrentTrain: epoch 15, batch    41 | loss: 9.6692236CurrentTrain: epoch 15, batch    42 | loss: 9.8606312CurrentTrain: epoch 15, batch    43 | loss: 6.9039952CurrentTrain: epoch 15, batch    44 | loss: 15.8389488CurrentTrain: epoch 15, batch    45 | loss: 10.7268339CurrentTrain: epoch 15, batch    46 | loss: 12.8801829CurrentTrain: epoch 15, batch    47 | loss: 16.8810558CurrentTrain: epoch 15, batch    48 | loss: 6.3876119CurrentTrain: epoch 15, batch    49 | loss: 12.6405351CurrentTrain: epoch 15, batch    50 | loss: 6.6405047CurrentTrain: epoch 15, batch    51 | loss: 19.3577356CurrentTrain: epoch 15, batch    52 | loss: 11.4923423CurrentTrain: epoch 15, batch    53 | loss: 10.7082766CurrentTrain: epoch 15, batch    54 | loss: 5.5377229CurrentTrain: epoch 15, batch    55 | loss: 14.6294806CurrentTrain: epoch 15, batch    56 | loss: 9.6595851CurrentTrain: epoch 15, batch    57 | loss: 7.9912728CurrentTrain: epoch 15, batch    58 | loss: 8.7482403CurrentTrain: epoch 15, batch    59 | loss: 12.3958207CurrentTrain: epoch 15, batch    60 | loss: 14.2381649CurrentTrain: epoch 15, batch    61 | loss: 9.6012772CurrentTrain: epoch  7, batch    62 | loss: 5.3061364CurrentTrain: epoch 15, batch     0 | loss: 37.5212635CurrentTrain: epoch 15, batch     1 | loss: 8.8741821CurrentTrain: epoch 15, batch     2 | loss: 8.2219753CurrentTrain: epoch 15, batch     3 | loss: 9.3280844CurrentTrain: epoch 15, batch     4 | loss: 10.6854953CurrentTrain: epoch 15, batch     5 | loss: 9.8859126CurrentTrain: epoch 15, batch     6 | loss: 15.4851773CurrentTrain: epoch 15, batch     7 | loss: 10.3041380CurrentTrain: epoch 15, batch     8 | loss: 8.7968359CurrentTrain: epoch 15, batch     9 | loss: 10.2776830CurrentTrain: epoch 15, batch    10 | loss: 13.6279205CurrentTrain: epoch 15, batch    11 | loss: 15.3145007CurrentTrain: epoch 15, batch    12 | loss: 10.7900059CurrentTrain: epoch 15, batch    13 | loss: 11.1927686CurrentTrain: epoch 15, batch    14 | loss: 14.3105181CurrentTrain: epoch 15, batch    15 | loss: 8.1099589CurrentTrain: epoch 15, batch    16 | loss: 21.6581475CurrentTrain: epoch 15, batch    17 | loss: 9.4970990CurrentTrain: epoch 15, batch    18 | loss: 11.3487659CurrentTrain: epoch 15, batch    19 | loss: 6.5182849CurrentTrain: epoch 15, batch    20 | loss: 14.5975240CurrentTrain: epoch 15, batch    21 | loss: 10.2266488CurrentTrain: epoch 15, batch    22 | loss: 7.5147922CurrentTrain: epoch 15, batch    23 | loss: 8.3302852CurrentTrain: epoch 15, batch    24 | loss: 14.4349628CurrentTrain: epoch 15, batch    25 | loss: 9.6594950CurrentTrain: epoch 15, batch    26 | loss: 8.6115222CurrentTrain: epoch 15, batch    27 | loss: 9.8957436CurrentTrain: epoch 15, batch    28 | loss: 8.4358574CurrentTrain: epoch 15, batch    29 | loss: 7.9225293CurrentTrain: epoch 15, batch    30 | loss: 10.0724410CurrentTrain: epoch 15, batch    31 | loss: 15.4033354CurrentTrain: epoch 15, batch    32 | loss: 8.1049367CurrentTrain: epoch 15, batch    33 | loss: 8.4804903CurrentTrain: epoch 15, batch    34 | loss: 13.9269376CurrentTrain: epoch 15, batch    35 | loss: 7.7821965CurrentTrain: epoch 15, batch    36 | loss: 10.4277352CurrentTrain: epoch 15, batch    37 | loss: 16.8565914CurrentTrain: epoch 15, batch    38 | loss: 12.0650368CurrentTrain: epoch 15, batch    39 | loss: 9.9865183CurrentTrain: epoch 15, batch    40 | loss: 8.3676492CurrentTrain: epoch 15, batch    41 | loss: 6.5596811CurrentTrain: epoch 15, batch    42 | loss: 9.7897222CurrentTrain: epoch 15, batch    43 | loss: 15.1363623CurrentTrain: epoch 15, batch    44 | loss: 23.3333465CurrentTrain: epoch 15, batch    45 | loss: 9.8242136CurrentTrain: epoch 15, batch    46 | loss: 16.2695426CurrentTrain: epoch 15, batch    47 | loss: 15.1519522CurrentTrain: epoch 15, batch    48 | loss: 14.7593040CurrentTrain: epoch 15, batch    49 | loss: 9.4193222CurrentTrain: epoch 15, batch    50 | loss: 21.9194953CurrentTrain: epoch 15, batch    51 | loss: 15.0731526CurrentTrain: epoch 15, batch    52 | loss: 14.3599087CurrentTrain: epoch 15, batch    53 | loss: 8.8689282CurrentTrain: epoch 15, batch    54 | loss: 14.6106058CurrentTrain: epoch 15, batch    55 | loss: 9.4087966CurrentTrain: epoch 15, batch    56 | loss: 9.5179164CurrentTrain: epoch 15, batch    57 | loss: 8.5626113CurrentTrain: epoch 15, batch    58 | loss: 13.6225865CurrentTrain: epoch 15, batch    59 | loss: 11.3491545CurrentTrain: epoch 15, batch    60 | loss: 11.3211493CurrentTrain: epoch 15, batch    61 | loss: 6.6054148CurrentTrain: epoch  7, batch    62 | loss: 10.9933605
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: while the milanese painting scene was dominated by neoclassic painters andrea appiani and luigi sabatelli , giovannni migliara stayed with the historical themes and medieval subtlety of romanticism .
Head Entity: andrea appiani
Tail Entity: neoclassic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: during the early 20th century, the surrealist movement gained prominence, with artists like salvador dalí and max ernst pushing the boundaries of creativity and imagination.  
Head Entity: salvador dalí  
Tail Entity: surrealist  

Relation: movement  
Context: the beat generation was a literary movement that emerged in the 1950s, characterized by its rejection of conventional society and exploration of spirituality, with figures like jack kerouac and allen ginsberg at the forefront.  
Head Entity: jack kerouac  
Tail Entity: beat generation  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: port alberta is a joint venture between the edmonton economic development corporation ( eedc ) and edmonton international airport ( eia ) based in the edmonton capital region in alberta , canada .
Head Entity: edmonton capital region
Tail Entity: alberta
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: the city of san francisco is known for its iconic golden gate bridge and is part of the larger san francisco bay area in california, united states.  
Head Entity: san francisco bay area  
Tail Entity: california  

Relation: located in the administrative territorial entity  
Context: the university of toronto is one of the leading educational institutions in canada and is situated in the province of ontario.  
Head Entity: university of toronto  
Tail Entity: ontario  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the nation as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: in early june 2006 , it was announced through the german " rock hard " magazine that abbath and horgh would reunite as immortal .
Head Entity: abbath
Tail Entity: immortal
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: After years of performing solo, the talented musician decided to join the renowned jazz ensemble, bringing his unique style to the group.  
Head Entity: the talented musician  
Tail Entity: the renowned jazz ensemble  

Relation: member of  
Context: The young athlete was thrilled to be accepted into the prestigious soccer club, where he would train with some of the best players in the country.  
Head Entity: the young athlete  
Tail Entity: the prestigious soccer club  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: on july 2 , 1935 , the rip van winkle bridge over the hudson river between catskill and greenport was opened to traffic .
Head Entity: rip van winkle bridge
Tail Entity: hudson river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: The Golden Gate Bridge spans the Golden Gate Strait, connecting San Francisco to Marin County and allowing vehicles to cross over the waterway.  
Head Entity: Golden Gate Bridge  
Tail Entity: Golden Gate Strait  

Relation: crosses  
Context: The Channel Tunnel, also known as the Chunnel, runs beneath the English Channel, providing a passage for trains to cross under the sea between England and France.  
Head Entity: Channel Tunnel  
Tail Entity: English Channel  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: the eldon river , part of the king river catchment , is a perennial river located in the west coast region of tasmania , australia .
Head Entity: king river
Tail Entity: eldon river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: the missouri river is the longest river in north america and it flows into the mississippi river, which is one of the largest river systems in the world.  
Head Entity: mississippi river  
Tail Entity: missouri river  

Relation: tributary  
Context: the thames river, which runs through london, is fed by several smaller rivers, including the river colne that contributes to its flow.  
Head Entity: thames river  
Tail Entity: river colne  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: she faced republican phil scott , the lieutenant governor of vermont , in the november general election .
Head Entity: phil scott
Tail Entity: governor of vermont
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: after serving as the mayor of newark, cory booker was elected to the united states senate in 2013.  
Head Entity: cory booker  
Tail Entity: mayor of newark  

Relation: position held  
Context: during his tenure, barack obama held the office of president of the united states from 2009 to 2017.  
Head Entity: barack obama  
Tail Entity: president of the united states  
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: a week before the game 's publishing , krisalis software changed its name to " airport tycoon " in some markets , a more catchy title .
Head Entity: airport tycoon
Tail Entity: krisalis software
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: In 2015, the popular mobile game "Clash of Clans" was developed by Supercell, which quickly gained a massive player base worldwide.  
Head Entity: Clash of Clans  
Tail Entity: Supercell  

Relation: developer  
Context: The innovative software "Photoshop" was created by Adobe Systems, revolutionizing the way digital images are edited and manipulated.  
Head Entity: Photoshop  
Tail Entity: Adobe Systems  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: dean reis , meanwhile , performs with the heartaches , while rubalcaba plays in earthless and is part - owner of the independent record store thirsty moon records in the hillcrest area of san diego .
Head Entity: earthless
Tail Entity: san diego
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the tech startup was founded in a small garage in palo alto, where a group of innovative engineers came together to create groundbreaking software.  
Head Entity: tech startup  
Tail Entity: palo alto  

Relation: location of formation  
Context: the famous rock band originated in a vibrant music scene in nashville, where they played their first gigs and built a loyal fanbase.  
Head Entity: rock band  
Tail Entity: nashville  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: he has also played on german chart - topper albums of die fantastischen vier and fettes brot and has collaborated with uk artists lemn sissay , ( dr. ) robert howard and martin glover .
Head Entity: fettes brot
Tail Entity: german
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous dish sushi is traditionally associated with Japan, where it originated and has become a staple of Japanese cuisine.  
Head Entity: sushi  
Tail Entity: Japan  

Relation: country of origin  
Context: The renowned painting "The Starry Night" was created by Vincent van Gogh, a Dutch post-impressionist painter, who was born in the Netherlands.  
Head Entity: The Starry Night  
Tail Entity: Netherlands  
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.65%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.09%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 93.47%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.01%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.99%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.98%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.34%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.51%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.67%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.93%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.90%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.60%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.69%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.74%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.96%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.79%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.75%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 95.68%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.65%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.69%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.76%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.90%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.14%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.65%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.09%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 93.47%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.01%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.99%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.98%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.34%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.51%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.67%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.93%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.90%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.60%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.69%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.74%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.96%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.79%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.75%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 95.68%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.65%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.69%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.76%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.90%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.14%   
cur_acc:  ['0.9514']
his_acc:  ['0.9514']
CurrentTrain: epoch 15, batch     0 | loss: 25.2267866CurrentTrain: epoch 15, batch     1 | loss: 16.3252098CurrentTrain: epoch 15, batch     2 | loss: 16.2649501CurrentTrain: epoch  1, batch     3 | loss: 13.0201394CurrentTrain: epoch 15, batch     0 | loss: 17.9215189CurrentTrain: epoch 15, batch     1 | loss: 15.9211357CurrentTrain: epoch 15, batch     2 | loss: 17.5628369CurrentTrain: epoch  1, batch     3 | loss: 11.1627941CurrentTrain: epoch 15, batch     0 | loss: 15.8578574CurrentTrain: epoch 15, batch     1 | loss: 18.8317303CurrentTrain: epoch 15, batch     2 | loss: 11.7812780CurrentTrain: epoch  1, batch     3 | loss: 9.6518680CurrentTrain: epoch 15, batch     0 | loss: 16.5092417CurrentTrain: epoch 15, batch     1 | loss: 16.1570331CurrentTrain: epoch 15, batch     2 | loss: 16.6548179CurrentTrain: epoch  1, batch     3 | loss: 16.8567214CurrentTrain: epoch 15, batch     0 | loss: 14.4164797CurrentTrain: epoch 15, batch     1 | loss: 16.2374336CurrentTrain: epoch 15, batch     2 | loss: 10.9270945CurrentTrain: epoch  1, batch     3 | loss: 7.9929417CurrentTrain: epoch 15, batch     0 | loss: 12.6819180CurrentTrain: epoch 15, batch     1 | loss: 10.5041563CurrentTrain: epoch 15, batch     2 | loss: 11.7367420CurrentTrain: epoch  1, batch     3 | loss: 9.9930797CurrentTrain: epoch 15, batch     0 | loss: 12.5328132CurrentTrain: epoch 15, batch     1 | loss: 11.2048056CurrentTrain: epoch 15, batch     2 | loss: 13.3777066CurrentTrain: epoch  1, batch     3 | loss: 6.6824569CurrentTrain: epoch 15, batch     0 | loss: 14.7119027CurrentTrain: epoch 15, batch     1 | loss: 11.1312627CurrentTrain: epoch 15, batch     2 | loss: 10.9183609CurrentTrain: epoch  1, batch     3 | loss: 23.4682623CurrentTrain: epoch 15, batch     0 | loss: 9.8149005CurrentTrain: epoch 15, batch     1 | loss: 10.9125278CurrentTrain: epoch 15, batch     2 | loss: 9.7228312CurrentTrain: epoch  1, batch     3 | loss: 7.8853348CurrentTrain: epoch 15, batch     0 | loss: 9.4459783CurrentTrain: epoch 15, batch     1 | loss: 20.8839218CurrentTrain: epoch 15, batch     2 | loss: 14.6874837CurrentTrain: epoch  1, batch     3 | loss: 8.0702667
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: auerbach is prominently featured in the documentary film , " the first basket " , about jewish basketball history .
Head Entity: the first basket
Tail Entity: basketball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: the novel "Pride and Prejudice" explores the themes of love and social class in early 19th century England.  
Head Entity: Pride and Prejudice  
Tail Entity: love  

Relation: main subject  
Context: the documentary "Our Planet" highlights the impact of climate change on various ecosystems around the world.  
Head Entity: Our Planet  
Tail Entity: climate change  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: he represented romania at euro 2000 , his first major tournament where they beat england 3–2 , and drew 1–1 against germany .
Head Entity: euro 2000
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: The Brazilian national team showcased their skills at the 2014 FIFA World Cup, where they faced off against Germany in the semi-finals.  
Head Entity: 2014 FIFA World Cup  
Tail Entity: Germany  

Relation: participating team  
Context: During the 2021 Tokyo Olympics, the United States women's soccer team competed fiercely, ultimately playing against Canada in the semi-finals.  
Head Entity: 2021 Tokyo Olympics  
Tail Entity: Canada  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: among the competing artists was previous eurovision song contest entrant robin juhkental who represented estonia in 2010 as part of the group malcolm lincoln .
Head Entity: malcolm lincoln
Tail Entity: robin juhkental
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The famous painting "The Last Supper" features several apostles, including Judas Iscariot, who is depicted as part of the group gathered around Jesus.  
Head Entity: The Last Supper  
Tail Entity: Judas Iscariot  

Relation: has part  
Context: The smartphone model Galaxy S21 includes various components, such as the Exynos 2100 processor, which is an integral part of its performance.  
Head Entity: Galaxy S21  
Tail Entity: Exynos 2100 processor  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: he was nominated for the academy award for best picture , along with steven spielberg , ian bryce , mark gordon for the film " saving private ryan " .
Head Entity: ian bryce
Tail Entity: academy award for best picture
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film "The Shape of Water" received multiple nominations, including a nod for best director, which was awarded to Guillermo del Toro.  
Head Entity: Guillermo del Toro  
Tail Entity: best director  

Relation: nominated for  
Context: In 2020, the popular series "Succession" was nominated for several Emmy Awards, showcasing its critical acclaim and audience popularity.  
Head Entity: Succession  
Tail Entity: Emmy Awards  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: in 2007 , the county municipality created the transit authority called " skyss " that would market public transport while is would be operated by private companies based on public service obligation contracts .
Head Entity: skyss
Tail Entity: transit authority
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone known as the iPhone was first released by Apple Inc. in 2007, revolutionizing the mobile phone industry and setting new standards for design and functionality.  
Head Entity: iPhone  
Tail Entity: smartphone  

Relation: instance of  
Context: The Great Wall of China is a series of fortifications made of various materials, built to protect the Chinese states and empires from invasions and raids.  
Head Entity: Great Wall of China  
Tail Entity: fortification
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: brand peak is a sharp snow - covered peak located east - southeast of the eternity range and northwest of mount duemler , in palmer land .
Head Entity: mount duemler
Tail Entity: eternity range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the sierra nevada is a major mountain range in the western united states, known for its stunning landscapes and diverse ecosystems, including the famous lake tahoe, which is located near the eastern edge of the range.  
Head Entity: lake tahoe  
Tail Entity: sierra nevada  

Relation: mountain range  
Context: the appalachian mountains stretch from the southern united states to eastern canada, with the blue ridge mountains being a prominent subrange that attracts hikers and nature enthusiasts.  
Head Entity: blue ridge mountains  
Tail Entity: appalachian mountains  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: future films include " mcqueen " , ian bonhote ’s documentary about the fashion designer alexander mcqueen , brad anderson ’s thriller " beirut " , and mark pellington ’s drama " nostalgia " .
Head Entity: nostalgia
Tail Entity: mark pellington
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: the upcoming movie "the great adventure" is penned by renowned screenwriter jessica taylor, who has previously won multiple awards for her work in the industry.  
Head Entity: the great adventure  
Tail Entity: jessica taylor  

Relation: screenwriter  
Context: in a recent interview, director samuel l. jackson revealed that the screenplay for "the last stand" was crafted by the talented screenwriter robert m. clark, known for his gripping narratives.  
Head Entity: the last stand  
Tail Entity: robert m. clark  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the publisher found success in 1993 with tarō gomi 's " everyone poops " , a japanese language import that sold over one million copies and is now their best - known title .
Head Entity: everyone poops
Tail Entity: japanese language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The animated series "Avatar: The Last Airbender" features a rich narrative and is primarily produced in English, appealing to a global audience.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  

Relation: language of work or name  
Context: The famous novel "Cien años de soledad" by Gabriel García Márquez is celebrated for its magical realism and is originally written in Spanish, reflecting the culture of Latin America.  
Head Entity: Cien años de soledad  
Tail Entity: Spanish  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: saratov airlines ( saratov airlines joint stock company , , " saratovskije avialinii " ) is a russian airline headquartered in saratov and based at saratov tsentralny airport .
Head Entity: saratov tsentralny airport
Tail Entity: saratov airlines
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: The New York City Transit Authority operates the subway system, providing essential transportation services to millions of commuters daily.  
Head Entity: New York City Transit Authority  
Tail Entity: subway system  

Relation: operator  
Context: Tesla, Inc. is known for its electric vehicles and operates several Gigafactories around the world to produce batteries and vehicles.  
Head Entity: Tesla, Inc.  
Tail Entity: Gigafactories
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: udasi has been an early sect based on the teachings of sri chand ( 1494–1643 ) , the son of guru nanak , the founder and the first guru of sikhism .
Head Entity: guru nanak
Tail Entity: sikhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the baha'i faith is a religion founded by baha'u'llah in the 19th century, emphasizing the spiritual unity of all humankind.  
Head Entity: baha'u'llah  
Tail Entity: baha'i faith  

Relation: religion  
Context: the ancient greeks practiced a polytheistic religion that included a pantheon of gods such as zeus, athena, and apollo.  
Head Entity: ancient greeks  
Tail Entity: polytheistic religion  
MemoryTrain:  epoch 15, batch     0 | loss: 6.2473946MemoryTrain:  epoch 15, batch     1 | loss: 6.3853892MemoryTrain:  epoch 15, batch     2 | loss: 5.3811053MemoryTrain:  epoch 11, batch     3 | loss: 7.2208479MemoryTrain:  epoch 15, batch     0 | loss: 5.3141305MemoryTrain:  epoch 15, batch     1 | loss: 5.0265411MemoryTrain:  epoch 15, batch     2 | loss: 5.4570352MemoryTrain:  epoch 11, batch     3 | loss: 9.5990650MemoryTrain:  epoch 15, batch     0 | loss: 4.1715189MemoryTrain:  epoch 15, batch     1 | loss: 8.9805758MemoryTrain:  epoch 15, batch     2 | loss: 4.3507871MemoryTrain:  epoch 11, batch     3 | loss: 5.2556933MemoryTrain:  epoch 15, batch     0 | loss: 4.2872998MemoryTrain:  epoch 15, batch     1 | loss: 4.9443384MemoryTrain:  epoch 15, batch     2 | loss: 6.3307563MemoryTrain:  epoch 11, batch     3 | loss: 6.4080866MemoryTrain:  epoch 15, batch     0 | loss: 3.6764453MemoryTrain:  epoch 15, batch     1 | loss: 4.1310420MemoryTrain:  epoch 15, batch     2 | loss: 7.2571198MemoryTrain:  epoch 11, batch     3 | loss: 2.6606798MemoryTrain:  epoch 15, batch     0 | loss: 4.2719473MemoryTrain:  epoch 15, batch     1 | loss: 5.2351919MemoryTrain:  epoch 15, batch     2 | loss: 4.7930460MemoryTrain:  epoch 11, batch     3 | loss: 2.8561514MemoryTrain:  epoch 15, batch     0 | loss: 4.7040902MemoryTrain:  epoch 15, batch     1 | loss: 4.1951798MemoryTrain:  epoch 15, batch     2 | loss: 2.8171002MemoryTrain:  epoch 11, batch     3 | loss: 2.5939452MemoryTrain:  epoch 15, batch     0 | loss: 7.3459437MemoryTrain:  epoch 15, batch     1 | loss: 8.7983492MemoryTrain:  epoch 15, batch     2 | loss: 5.6037901MemoryTrain:  epoch 11, batch     3 | loss: 2.4855979MemoryTrain:  epoch 15, batch     0 | loss: 2.7094183MemoryTrain:  epoch 15, batch     1 | loss: 3.4443370MemoryTrain:  epoch 15, batch     2 | loss: 2.9784071MemoryTrain:  epoch 11, batch     3 | loss: 2.3076729MemoryTrain:  epoch 15, batch     0 | loss: 5.1582488MemoryTrain:  epoch 15, batch     1 | loss: 2.6215551MemoryTrain:  epoch 15, batch     2 | loss: 2.7868219MemoryTrain:  epoch 11, batch     3 | loss: 5.3185755
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 84.62%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 81.25%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 77.92%   [EVAL] batch:   15 | acc: 31.25%,  total acc: 75.00%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 73.53%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 72.22%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 70.39%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 73.21%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 74.15%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 75.27%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 76.30%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 77.25%   [EVAL] batch:   25 | acc: 25.00%,  total acc: 75.24%   [EVAL] batch:   26 | acc: 18.75%,  total acc: 73.15%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 70.76%   [EVAL] batch:   28 | acc: 50.00%,  total acc: 70.04%   [EVAL] batch:   29 | acc: 18.75%,  total acc: 68.33%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 66.33%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 66.60%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 67.42%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 68.38%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 69.29%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 69.97%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 70.78%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 71.55%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 72.28%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 72.97%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 73.63%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 74.26%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 74.85%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 75.43%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 75.42%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 75.41%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 75.80%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 75.91%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 76.28%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 76.62%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 76.23%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 76.20%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 76.18%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 76.16%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 76.48%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 76.67%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 76.75%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 76.83%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 77.12%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 77.19%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 77.46%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 77.62%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 77.08%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 82.95%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 83.65%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 86.76%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 87.15%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 87.83%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 87.81%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 88.10%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 88.07%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 88.32%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 88.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 88.94%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 89.35%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 89.51%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 89.66%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 90.32%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 90.91%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 91.18%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 91.49%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 91.72%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 91.94%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.15%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.34%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.53%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 92.88%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.04%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.19%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.34%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 93.22%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 93.23%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.37%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 93.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.38%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.39%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.51%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.63%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 93.41%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.42%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 93.31%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 92.67%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 92.48%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 92.40%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 92.21%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 92.04%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 92.06%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 91.70%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 91.35%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 91.29%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 91.04%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 90.99%   [EVAL] batch:   68 | acc: 87.50%,  total acc: 90.94%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 90.98%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 90.85%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 90.80%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 90.84%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 90.96%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 91.08%   [EVAL] batch:   75 | acc: 50.00%,  total acc: 90.54%   [EVAL] batch:   76 | acc: 31.25%,  total acc: 89.77%   [EVAL] batch:   77 | acc: 31.25%,  total acc: 89.02%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 88.37%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 87.89%   [EVAL] batch:   80 | acc: 31.25%,  total acc: 87.19%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 87.12%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 87.27%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 87.43%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 87.65%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 87.79%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 87.50%   [EVAL] batch:   88 | acc: 18.75%,  total acc: 86.73%   [EVAL] batch:   89 | acc: 18.75%,  total acc: 85.97%   [EVAL] batch:   90 | acc: 18.75%,  total acc: 85.23%   [EVAL] batch:   91 | acc: 43.75%,  total acc: 84.78%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 83.94%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 83.44%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 83.55%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 83.66%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 83.83%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 84.25%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 84.41%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 84.56%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 84.71%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 84.86%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 85.14%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 85.16%   [EVAL] batch:  107 | acc: 75.00%,  total acc: 85.07%   [EVAL] batch:  108 | acc: 87.50%,  total acc: 85.09%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 85.17%   [EVAL] batch:  110 | acc: 81.25%,  total acc: 85.14%   [EVAL] batch:  111 | acc: 87.50%,  total acc: 85.16%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 85.07%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 84.87%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 84.89%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 84.86%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 84.78%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 84.75%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 84.72%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 84.79%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 84.81%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 84.78%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 84.81%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 84.88%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 84.90%   
cur_acc:  ['0.9514', '0.7708']
his_acc:  ['0.9514', '0.8490']
CurrentTrain: epoch 15, batch     0 | loss: 13.7551875CurrentTrain: epoch 15, batch     1 | loss: 15.3529113CurrentTrain: epoch 15, batch     2 | loss: 30.2962328CurrentTrain: epoch  1, batch     3 | loss: 10.4828711CurrentTrain: epoch 15, batch     0 | loss: 10.0498192CurrentTrain: epoch 15, batch     1 | loss: 12.6343780CurrentTrain: epoch 15, batch     2 | loss: 11.9355851CurrentTrain: epoch  1, batch     3 | loss: 7.5247082CurrentTrain: epoch 15, batch     0 | loss: 10.4504646CurrentTrain: epoch 15, batch     1 | loss: 13.6755080CurrentTrain: epoch 15, batch     2 | loss: 16.9351360CurrentTrain: epoch  1, batch     3 | loss: 11.3034124CurrentTrain: epoch 15, batch     0 | loss: 13.7494285CurrentTrain: epoch 15, batch     1 | loss: 11.5281975CurrentTrain: epoch 15, batch     2 | loss: 8.4004813CurrentTrain: epoch  1, batch     3 | loss: 8.4267837CurrentTrain: epoch 15, batch     0 | loss: 15.6847232CurrentTrain: epoch 15, batch     1 | loss: 8.4225782CurrentTrain: epoch 15, batch     2 | loss: 8.9416340CurrentTrain: epoch  1, batch     3 | loss: 7.3780703CurrentTrain: epoch 15, batch     0 | loss: 10.4696837CurrentTrain: epoch 15, batch     1 | loss: 9.5591290CurrentTrain: epoch 15, batch     2 | loss: 10.5201008CurrentTrain: epoch  1, batch     3 | loss: 6.5473993CurrentTrain: epoch 15, batch     0 | loss: 8.0184373CurrentTrain: epoch 15, batch     1 | loss: 16.3453179CurrentTrain: epoch 15, batch     2 | loss: 10.6986463CurrentTrain: epoch  1, batch     3 | loss: 5.9065444CurrentTrain: epoch 15, batch     0 | loss: 8.9205273CurrentTrain: epoch 15, batch     1 | loss: 5.2742969CurrentTrain: epoch 15, batch     2 | loss: 7.6685397CurrentTrain: epoch  1, batch     3 | loss: 7.0770216CurrentTrain: epoch 15, batch     0 | loss: 10.2030954CurrentTrain: epoch 15, batch     1 | loss: 8.2324639CurrentTrain: epoch 15, batch     2 | loss: 5.7169031CurrentTrain: epoch  1, batch     3 | loss: 12.5165178CurrentTrain: epoch 15, batch     0 | loss: 15.0306371CurrentTrain: epoch 15, batch     1 | loss: 9.2435341CurrentTrain: epoch 15, batch     2 | loss: 7.9400288CurrentTrain: epoch  1, batch     3 | loss: 6.5512793
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: ( 1988 ) , " night watch " ( 2004 ) , " day watch " ( 2006 ) and " legend № 17 " ( 2013 ) .
Head Entity: day watch
Tail Entity: night watch
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: ( 1995 ) , " The Phantom Menace " ( 1999 ) , " Attack of the Clones " ( 2002 ) and " Revenge of the Sith " ( 2005 ) .  
Head Entity: Attack of the Clones  
Tail Entity: The Phantom Menace  

Relation: follows  
Context: ( 2010 ) , " The Hunger Games " ( 2012 ) , " Catching Fire " ( 2013 ) and " Mockingjay " ( 2014 ) .  
Head Entity: Catching Fire  
Tail Entity: The Hunger Games  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: it is primarily used for baseball and serves as the home stadium for yaquis de obregón .
Head Entity: yaquis de obregón
Tail Entity: baseball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: She has been a professional player in the world of tennis, winning several championships throughout her career.  
Head Entity: She  
Tail Entity: tennis  

Relation: sport  
Context: The city is known for its vibrant soccer culture, with numerous teams competing in the national league.  
Head Entity: The city  
Tail Entity: soccer  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael was the father of two daughters, emily and sarah, who both excelled in their studies.  
Head Entity: michael  
Tail Entity: emily  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: since 2009 , culshaw has starred in the bbc one comedy sketch show " the impressions show " alongside debra stephenson .
Head Entity: the impressions show
Tail Entity: bbc one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: The popular series "Breaking Bad" first aired on AMC, captivating audiences with its intense storytelling and character development.  
Head Entity: Breaking Bad  
Tail Entity: AMC  

Relation: original network  
Context: "Friends" became a cultural phenomenon after it premiered on NBC, showcasing the lives of six friends living in New York City.  
Head Entity: Friends  
Tail Entity: NBC  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: he was also nominated twice for the filmfare best telugu actor award , for the films bhale bhale magadivoy and " gentleman " .
Head Entity: gentleman
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: The movie "Parasite" received critical acclaim and was originally produced in Korean, showcasing the talents of its director Bong Joon-ho.  
Head Entity: Parasite  
Tail Entity: Korean  

Relation: original language of film or TV show  
Context: The animated series "Doraemon" has been dubbed in multiple languages, but it was originally created in Japanese, captivating audiences worldwide.  
Head Entity: Doraemon  
Tail Entity: Japanese  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: the league champions for the fifth time in their history ( and the second season running ) were taunton town , but runners - up mangotsfield united took promotion to the southern league .
Head Entity: mangotsfield united
Tail Entity: southern league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: After an impressive season, the team secured a spot in the prestigious league, where they will compete against top clubs, including their rivals, the city wanderers.  
Head Entity: city wanderers  
Tail Entity: prestigious league  

Relation: league  
Context: The young athlete showcased his skills in the national league, where he played alongside some of the best players in the country, including his teammate, the star striker.  
Head Entity: star striker  
Tail Entity: national league  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: she is supposedly the third and youngest child of seti i and tuya , and the younger sister of ramesses ii and tia .
Head Entity: tia
Tail Entity: tuya
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: Cleopatra was the daughter of Ptolemy XII and his wife, who was also her mother, Cleopatra VI.  
Head Entity: Cleopatra  
Tail Entity: Cleopatra VI  

Relation: mother  
Context: In Norse mythology, Loki is said to have fathered several children, including the goddess Hel, who is the daughter of the giantess Angerboda, her mother.  
Head Entity: Hel  
Tail Entity: Angerboda  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres. he is particularly famous for his interpretations of classical cello concertos.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: a u.s. government - funded $ 36 million bridge over the panj river connects sher khan bandar in afghanistan with nizhniy pyanzh in tajikistan , which transport more than 150 trucks or 1,000 cars daily .
Head Entity: sher khan bandar
Tail Entity: afghanistan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the ancient city of petra, located in southern jordan, is famous for its rock-cut architecture and water conduit system, attracting thousands of tourists each year.  
Head Entity: petra  
Tail Entity: jordan  

Relation: country  
Context: the great wall of china, a series of fortifications made of various materials, stretches across northern china and is a UNESCO World Heritage site.  
Head Entity: great wall of china  
Tail Entity: china  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: klaus kinski , who would play dracula himself nine years later in " nosferatu the vampyre " , is also featured in the film as renfield .
Head Entity: nosferatu the vampyre
Tail Entity: renfield
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: in the animated series "avatar: the last airbender," zuko is a complex character who struggles with his identity and family legacy.  
Head Entity: avatar: the last airbender  
Tail Entity: zuko  

Relation: characters  
Context: the novel "pride and prejudice" features elizabeth bennet, a strong-willed protagonist who navigates societal expectations and personal relationships.  
Head Entity: pride and prejudice  
Tail Entity: elizabeth bennet  
MemoryTrain:  epoch 15, batch     0 | loss: 6.5746054MemoryTrain:  epoch 15, batch     1 | loss: 5.9419865MemoryTrain:  epoch 15, batch     2 | loss: 4.8648543MemoryTrain:  epoch 15, batch     3 | loss: 4.5816278MemoryTrain:  epoch 15, batch     4 | loss: 5.2930567MemoryTrain:  epoch  9, batch     5 | loss: 8.1723868MemoryTrain:  epoch 15, batch     0 | loss: 7.1820592MemoryTrain:  epoch 15, batch     1 | loss: 5.9629491MemoryTrain:  epoch 15, batch     2 | loss: 4.7309380MemoryTrain:  epoch 15, batch     3 | loss: 3.5384302MemoryTrain:  epoch 15, batch     4 | loss: 3.9980721MemoryTrain:  epoch  9, batch     5 | loss: 4.0025235MemoryTrain:  epoch 15, batch     0 | loss: 2.6911569MemoryTrain:  epoch 15, batch     1 | loss: 3.8645124MemoryTrain:  epoch 15, batch     2 | loss: 4.9718784MemoryTrain:  epoch 15, batch     3 | loss: 5.5970908MemoryTrain:  epoch 15, batch     4 | loss: 3.6018825MemoryTrain:  epoch  9, batch     5 | loss: 3.6543437MemoryTrain:  epoch 15, batch     0 | loss: 2.9537392MemoryTrain:  epoch 15, batch     1 | loss: 2.6570140MemoryTrain:  epoch 15, batch     2 | loss: 6.0128744MemoryTrain:  epoch 15, batch     3 | loss: 3.3344994MemoryTrain:  epoch 15, batch     4 | loss: 3.2654469MemoryTrain:  epoch  9, batch     5 | loss: 5.5694201MemoryTrain:  epoch 15, batch     0 | loss: 2.9389390MemoryTrain:  epoch 15, batch     1 | loss: 2.3802798MemoryTrain:  epoch 15, batch     2 | loss: 3.8813141MemoryTrain:  epoch 15, batch     3 | loss: 3.3814140MemoryTrain:  epoch 15, batch     4 | loss: 2.1093326MemoryTrain:  epoch  9, batch     5 | loss: 2.5100773MemoryTrain:  epoch 15, batch     0 | loss: 1.9744006MemoryTrain:  epoch 15, batch     1 | loss: 5.6457552MemoryTrain:  epoch 15, batch     2 | loss: 4.0091001MemoryTrain:  epoch 15, batch     3 | loss: 3.1886971MemoryTrain:  epoch 15, batch     4 | loss: 6.1109882MemoryTrain:  epoch  9, batch     5 | loss: 2.1751430MemoryTrain:  epoch 15, batch     0 | loss: 2.5705147MemoryTrain:  epoch 15, batch     1 | loss: 2.0676736MemoryTrain:  epoch 15, batch     2 | loss: 2.4042008MemoryTrain:  epoch 15, batch     3 | loss: 2.2200537MemoryTrain:  epoch 15, batch     4 | loss: 2.1920287MemoryTrain:  epoch  9, batch     5 | loss: 2.0353437MemoryTrain:  epoch 15, batch     0 | loss: 4.2347808MemoryTrain:  epoch 15, batch     1 | loss: 3.1108067MemoryTrain:  epoch 15, batch     2 | loss: 6.9859338MemoryTrain:  epoch 15, batch     3 | loss: 3.5011956MemoryTrain:  epoch 15, batch     4 | loss: 5.5079230MemoryTrain:  epoch  9, batch     5 | loss: 4.7339667MemoryTrain:  epoch 15, batch     0 | loss: 4.3167698MemoryTrain:  epoch 15, batch     1 | loss: 7.1272167MemoryTrain:  epoch 15, batch     2 | loss: 2.1602611MemoryTrain:  epoch 15, batch     3 | loss: 2.7669132MemoryTrain:  epoch 15, batch     4 | loss: 3.0783714MemoryTrain:  epoch  9, batch     5 | loss: 2.0106978MemoryTrain:  epoch 15, batch     0 | loss: 4.4215738MemoryTrain:  epoch 15, batch     1 | loss: 1.8643058MemoryTrain:  epoch 15, batch     2 | loss: 2.9021999MemoryTrain:  epoch 15, batch     3 | loss: 4.1824275MemoryTrain:  epoch 15, batch     4 | loss: 6.8426718MemoryTrain:  epoch  9, batch     5 | loss: 3.6181739
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 45.83%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 51.56%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 46.25%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 46.88%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 51.79%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 57.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 71.35%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 73.08%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 74.11%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 74.17%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 73.44%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 73.53%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 74.31%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 75.33%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 78.41%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 79.35%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 80.21%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 80.50%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 80.77%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   27 | acc: 68.75%,  total acc: 80.80%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 81.03%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 80.83%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 81.05%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.82%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 82.17%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 83.28%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 83.06%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 82.37%   [EVAL] batch:   39 | acc: 37.50%,  total acc: 81.25%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 80.03%   [EVAL] batch:   41 | acc: 37.50%,  total acc: 79.02%   [EVAL] batch:   42 | acc: 18.75%,  total acc: 77.62%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 77.27%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 77.78%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 78.59%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 78.91%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 79.21%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 79.50%   [EVAL] batch:   50 | acc: 50.00%,  total acc: 78.92%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 78.49%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 77.95%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 77.55%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 77.27%   [EVAL] batch:   55 | acc: 31.25%,  total acc: 76.45%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 76.10%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 76.08%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 75.64%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 75.83%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 76.02%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 76.21%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 75.69%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 81.77%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 82.21%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 85.66%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 86.84%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 86.88%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 87.20%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 87.22%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.98%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 88.43%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 88.62%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 88.79%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 89.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 89.52%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 90.15%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 90.44%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 90.54%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 90.80%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 91.05%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 91.28%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.51%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.72%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.92%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 92.15%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 92.36%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 92.26%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 92.15%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.35%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 92.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.40%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 92.43%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 92.45%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.59%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 92.39%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 92.32%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 91.59%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 91.31%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 91.15%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 90.98%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 90.73%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 90.77%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 90.23%   [EVAL] batch:   64 | acc: 56.25%,  total acc: 89.71%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 89.49%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 89.09%   [EVAL] batch:   67 | acc: 81.25%,  total acc: 88.97%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 88.86%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 88.93%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 88.91%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 88.80%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 88.78%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 88.94%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 89.00%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 88.24%   [EVAL] batch:   76 | acc: 31.25%,  total acc: 87.50%   [EVAL] batch:   77 | acc: 31.25%,  total acc: 86.78%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 86.16%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 85.70%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 84.95%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 84.91%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 85.09%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 85.27%   [EVAL] batch:   84 | acc: 100.00%,  total acc: 85.44%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 85.61%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 85.78%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 85.58%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 84.62%   [EVAL] batch:   89 | acc: 12.50%,  total acc: 83.82%   [EVAL] batch:   90 | acc: 18.75%,  total acc: 83.10%   [EVAL] batch:   91 | acc: 31.25%,  total acc: 82.54%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 81.72%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 81.25%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 81.45%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 81.58%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 81.77%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 81.95%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 82.13%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 82.31%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 82.49%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 82.60%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 82.58%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 82.75%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 82.92%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 83.08%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 82.94%   [EVAL] batch:  107 | acc: 68.75%,  total acc: 82.81%   [EVAL] batch:  108 | acc: 68.75%,  total acc: 82.68%   [EVAL] batch:  109 | acc: 50.00%,  total acc: 82.39%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 82.21%   [EVAL] batch:  111 | acc: 56.25%,  total acc: 81.98%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 81.91%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 81.74%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 81.79%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 81.79%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 81.84%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 81.83%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 81.93%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 82.03%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 82.13%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 82.12%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 82.22%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 82.21%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 82.30%   [EVAL] batch:  125 | acc: 50.00%,  total acc: 82.04%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 81.84%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 81.45%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 81.35%   [EVAL] batch:  129 | acc: 25.00%,  total acc: 80.91%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 80.68%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 80.68%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 80.83%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 80.97%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 81.06%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 81.20%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 81.34%   [EVAL] batch:  137 | acc: 93.75%,  total acc: 81.43%   [EVAL] batch:  138 | acc: 87.50%,  total acc: 81.47%   [EVAL] batch:  139 | acc: 75.00%,  total acc: 81.43%   [EVAL] batch:  140 | acc: 62.50%,  total acc: 81.29%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:  142 | acc: 87.50%,  total acc: 81.29%   [EVAL] batch:  143 | acc: 93.75%,  total acc: 81.38%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 81.51%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 81.64%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 81.72%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 81.84%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 81.96%   [EVAL] batch:  149 | acc: 87.50%,  total acc: 82.00%   [EVAL] batch:  150 | acc: 87.50%,  total acc: 82.04%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 82.11%   [EVAL] batch:  152 | acc: 68.75%,  total acc: 82.03%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 82.06%   [EVAL] batch:  154 | acc: 75.00%,  total acc: 82.02%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 82.05%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 82.09%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 82.20%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 82.27%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 82.34%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 82.41%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 82.52%   [EVAL] batch:  162 | acc: 75.00%,  total acc: 82.48%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 82.32%   [EVAL] batch:  164 | acc: 37.50%,  total acc: 82.05%   [EVAL] batch:  165 | acc: 31.25%,  total acc: 81.74%   [EVAL] batch:  166 | acc: 37.50%,  total acc: 81.47%   [EVAL] batch:  167 | acc: 18.75%,  total acc: 81.10%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 80.99%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 81.10%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 81.18%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 81.29%   [EVAL] batch:  172 | acc: 93.75%,  total acc: 81.36%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 81.43%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 81.50%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 81.32%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 81.18%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 81.00%   [EVAL] batch:  178 | acc: 56.25%,  total acc: 80.87%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 80.76%   [EVAL] batch:  180 | acc: 31.25%,  total acc: 80.49%   [EVAL] batch:  181 | acc: 56.25%,  total acc: 80.36%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 80.33%   [EVAL] batch:  183 | acc: 50.00%,  total acc: 80.16%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 80.20%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 80.24%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 80.28%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 80.09%   
cur_acc:  ['0.9514', '0.7708', '0.7569']
his_acc:  ['0.9514', '0.8490', '0.8009']
CurrentTrain: epoch 15, batch     0 | loss: 12.7743180CurrentTrain: epoch 15, batch     1 | loss: 21.4865407CurrentTrain: epoch 15, batch     2 | loss: 14.3593317CurrentTrain: epoch  1, batch     3 | loss: 6.7403420CurrentTrain: epoch 15, batch     0 | loss: 11.4135800CurrentTrain: epoch 15, batch     1 | loss: 13.8535100CurrentTrain: epoch 15, batch     2 | loss: 10.9672244CurrentTrain: epoch  1, batch     3 | loss: 7.9349215CurrentTrain: epoch 15, batch     0 | loss: 12.1020798CurrentTrain: epoch 15, batch     1 | loss: 14.4931693CurrentTrain: epoch 15, batch     2 | loss: 13.6789271CurrentTrain: epoch  1, batch     3 | loss: 10.6862858CurrentTrain: epoch 15, batch     0 | loss: 17.5464396CurrentTrain: epoch 15, batch     1 | loss: 7.5010956CurrentTrain: epoch 15, batch     2 | loss: 13.0680433CurrentTrain: epoch  1, batch     3 | loss: 6.9398355CurrentTrain: epoch 15, batch     0 | loss: 9.5150394CurrentTrain: epoch 15, batch     1 | loss: 8.9660876CurrentTrain: epoch 15, batch     2 | loss: 11.0913987CurrentTrain: epoch  1, batch     3 | loss: 6.4578393CurrentTrain: epoch 15, batch     0 | loss: 11.2982596CurrentTrain: epoch 15, batch     1 | loss: 10.0948835CurrentTrain: epoch 15, batch     2 | loss: 8.0055887CurrentTrain: epoch  1, batch     3 | loss: 6.3782340CurrentTrain: epoch 15, batch     0 | loss: 10.1148298CurrentTrain: epoch 15, batch     1 | loss: 14.0615572CurrentTrain: epoch 15, batch     2 | loss: 7.0173274CurrentTrain: epoch  1, batch     3 | loss: 5.8665584CurrentTrain: epoch 15, batch     0 | loss: 9.7921918CurrentTrain: epoch 15, batch     1 | loss: 10.1955284CurrentTrain: epoch 15, batch     2 | loss: 8.5066345CurrentTrain: epoch  1, batch     3 | loss: 8.5494511CurrentTrain: epoch 15, batch     0 | loss: 10.5933116CurrentTrain: epoch 15, batch     1 | loss: 9.1224626CurrentTrain: epoch 15, batch     2 | loss: 10.5468780CurrentTrain: epoch  1, batch     3 | loss: 9.8355076CurrentTrain: epoch 15, batch     0 | loss: 10.2004831CurrentTrain: epoch 15, batch     1 | loss: 6.1514643CurrentTrain: epoch 15, batch     2 | loss: 13.0661804CurrentTrain: epoch  1, batch     3 | loss: 6.2901859
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: george lawson sheldon ( may 31 , 1870 – april 4 , 1960 ) was 14th governor of nebraska with service from 1907 until 1909 .
Head Entity: governor
Tail Entity: nebraska
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The Environmental Protection Agency (EPA) is responsible for enforcing regulations related to environmental protection in the United States.  
Head Entity: Environmental Protection Agency  
Tail Entity: United States  

Relation: applies to jurisdiction  
Context: The California Consumer Privacy Act (CCPA) provides residents of California with specific rights regarding their personal information.  
Head Entity: California Consumer Privacy Act  
Tail Entity: California  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: " tenth avenue freeze - out " is the second song on bruce springsteen 's breakthrough album " born to run " , released in 1975 .
Head Entity: tenth avenue freeze - out
Tail Entity: bruce springsteen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: "Shape of You" is a popular song by Ed Sheeran that topped charts worldwide after its release in 2017.  
Head Entity: Shape of You  
Tail Entity: Ed Sheeran  

Relation: performer  
Context: The Beatles are widely regarded as one of the most influential bands in the history of music, with hits like "Hey Jude" and "Let It Be."  
Head Entity: The Beatles  
Tail Entity: Hey Jude  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the glasflügel h-201 standard " libelle " ( german : " dragonfly " ) is an early composite standard class single - seat sailplane produced by glasflügel from 1967 .
Head Entity: dragonfly
Tail Entity: glasflügel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The iPhone 13, a smartphone developed by Apple, was released in September 2021 and features advanced camera technology and improved battery life.  
Head Entity: iPhone 13  
Tail Entity: Apple  

Relation: manufacturer  
Context: The Tesla Model S is an all-electric luxury sedan that was first introduced by Tesla, Inc. in 2012, showcasing innovative technology and performance.  
Head Entity: Tesla Model S  
Tail Entity: Tesla, Inc.  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: at the 2016 summer olympics , heath and schofield won the silver k-2 200 event , finishing behind spain 's saúl craviotto and cristian toro .
Head Entity: 2016 summer olympics
Tail Entity: spain
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: during the 2020 presidential election, the democratic party nominated joe biden as their candidate, while the republican party supported donald trump.  
Head Entity: 2020 presidential election  
Tail Entity: democratic party  

Relation: participant  
Context: in the 2022 world cup, france emerged victorious, defeating argentina in a thrilling final match that captivated millions of fans worldwide.  
Head Entity: 2022 world cup  
Tail Entity: france  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the application is compatible with various operating systems, including macOS, Windows, and Ubuntu.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: Many developers prefer to use Linux as their primary operating system due to its flexibility and open-source nature.  
Head Entity: developers  
Tail Entity: Linux  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in his next bout , he won the wbc light welterweight title by defeating roger mayweather for a second time .
Head Entity: roger mayweather
Tail Entity: light welterweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: The team secured their place in the tournament by qualifying under the UEFA Champions League regulations.  
Head Entity: UEFA Champions League  
Tail Entity: tournament  

Relation: competition class  
Context: The athlete was classified under the Olympic weightlifting category, allowing him to compete in the upcoming games.  
Head Entity: Olympic weightlifting  
Tail Entity: games  
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: the qup gained 3 seats from its previous election result , and won a 10 % swing to it in primary votes , but failed to defeat the incumbent labor government led by ted theodore .
Head Entity: previous election
Tail Entity: ted theodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: After a closely contested campaign, the voters chose Sarah Johnson as their representative, marking a significant shift in the political landscape of the district.  
Head Entity: closely contested campaign  
Tail Entity: Sarah Johnson  

Relation: successful candidate  
Context: In the recent mayoral election, the community rallied behind Mark Thompson, who promised to bring new initiatives to the city, ultimately leading to his victory.  
Head Entity: recent mayoral election  
Tail Entity: Mark Thompson  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: the deaths of his brothers wenceslaus ii ( 1487 ) , casimir ii ( 1490 ) and władysław ( 1494 ) allowed jan v to reunificated the whole duchy of zator .
Head Entity: casimir ii
Tail Entity: wenceslaus ii
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: both elizabeth and her brother, charles, were known for their contributions to the arts, showcasing their family's rich cultural heritage.  
Head Entity: elizabeth  
Tail Entity: charles  

Relation: sibling  
Context: during the family reunion, it was heartwarming to see how much john and his sister, sarah, resembled each other, a clear sign of their shared lineage.  
Head Entity: john  
Tail Entity: sarah  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: two of her sons , tunis and thomas tingey rose to prominence in the union navy during the civil war .
Head Entity: tunis
Tail Entity: union navy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: after serving in the army, he transitioned to a leadership role in the air force, where he implemented several key strategies.  
Head Entity: air force  
Tail Entity: army  

Relation: military branch  
Context: the general was honored for his service in the marine corps, which played a crucial role in several key operations.  
Head Entity: marine corps  
Tail Entity: general  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: ziad rahbani is the son of the lebanese famous composer assi rahbani and nouhad haddad , the famous lebanese female singer known as fairuz .
Head Entity: assi rahbani
Tail Entity: ziad rahbani
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: in the animated series, the character of simon is the son of the adventurous couple, marie and tom.  
Head Entity: marie  
Tail Entity: simon  

Relation: child  
Context: the renowned physicist albert einstein had a daughter named lieserl, who was born in 1902.  
Head Entity: albert einstein  
Tail Entity: lieserl  
MemoryTrain:  epoch 15, batch     0 | loss: 8.8066804MemoryTrain:  epoch 15, batch     1 | loss: 4.1040941MemoryTrain:  epoch 15, batch     2 | loss: 3.9234311MemoryTrain:  epoch 15, batch     3 | loss: 4.0718365MemoryTrain:  epoch 15, batch     4 | loss: 7.6506949MemoryTrain:  epoch 15, batch     5 | loss: 5.7390799MemoryTrain:  epoch 15, batch     6 | loss: 3.5140598MemoryTrain:  epoch  7, batch     7 | loss: 4.0569041MemoryTrain:  epoch 15, batch     0 | loss: 5.8450575MemoryTrain:  epoch 15, batch     1 | loss: 4.7903498MemoryTrain:  epoch 15, batch     2 | loss: 3.6111926MemoryTrain:  epoch 15, batch     3 | loss: 3.7452071MemoryTrain:  epoch 15, batch     4 | loss: 5.2412385MemoryTrain:  epoch 15, batch     5 | loss: 8.7186417MemoryTrain:  epoch 15, batch     6 | loss: 3.2945695MemoryTrain:  epoch  7, batch     7 | loss: 3.8068057MemoryTrain:  epoch 15, batch     0 | loss: 3.5211818MemoryTrain:  epoch 15, batch     1 | loss: 6.8710113MemoryTrain:  epoch 15, batch     2 | loss: 4.5238833MemoryTrain:  epoch 15, batch     3 | loss: 2.6149479MemoryTrain:  epoch 15, batch     4 | loss: 3.7731778MemoryTrain:  epoch 15, batch     5 | loss: 5.5422132MemoryTrain:  epoch 15, batch     6 | loss: 2.9832833MemoryTrain:  epoch  7, batch     7 | loss: 4.6543354MemoryTrain:  epoch 15, batch     0 | loss: 5.5207813MemoryTrain:  epoch 15, batch     1 | loss: 4.9832800MemoryTrain:  epoch 15, batch     2 | loss: 6.8950593MemoryTrain:  epoch 15, batch     3 | loss: 2.8911270MemoryTrain:  epoch 15, batch     4 | loss: 3.1482274MemoryTrain:  epoch 15, batch     5 | loss: 4.2553877MemoryTrain:  epoch 15, batch     6 | loss: 6.0842459MemoryTrain:  epoch  7, batch     7 | loss: 6.8729037MemoryTrain:  epoch 15, batch     0 | loss: 2.8187300MemoryTrain:  epoch 15, batch     1 | loss: 2.1423557MemoryTrain:  epoch 15, batch     2 | loss: 3.0562056MemoryTrain:  epoch 15, batch     3 | loss: 2.4207764MemoryTrain:  epoch 15, batch     4 | loss: 2.7728856MemoryTrain:  epoch 15, batch     5 | loss: 3.4562233MemoryTrain:  epoch 15, batch     6 | loss: 4.3678193MemoryTrain:  epoch  7, batch     7 | loss: 1.8524138MemoryTrain:  epoch 15, batch     0 | loss: 2.8389629MemoryTrain:  epoch 15, batch     1 | loss: 2.5719588MemoryTrain:  epoch 15, batch     2 | loss: 2.6090358MemoryTrain:  epoch 15, batch     3 | loss: 3.1040784MemoryTrain:  epoch 15, batch     4 | loss: 2.4655737MemoryTrain:  epoch 15, batch     5 | loss: 2.0980915MemoryTrain:  epoch 15, batch     6 | loss: 2.1379979MemoryTrain:  epoch  7, batch     7 | loss: 2.2694012MemoryTrain:  epoch 15, batch     0 | loss: 4.3104357MemoryTrain:  epoch 15, batch     1 | loss: 4.2166748MemoryTrain:  epoch 15, batch     2 | loss: 1.9530648MemoryTrain:  epoch 15, batch     3 | loss: 2.5899640MemoryTrain:  epoch 15, batch     4 | loss: 3.4100842MemoryTrain:  epoch 15, batch     5 | loss: 2.6451686MemoryTrain:  epoch 15, batch     6 | loss: 2.4312815MemoryTrain:  epoch  7, batch     7 | loss: 5.1805258MemoryTrain:  epoch 15, batch     0 | loss: 4.8243453MemoryTrain:  epoch 15, batch     1 | loss: 1.7734666MemoryTrain:  epoch 15, batch     2 | loss: 2.4361137MemoryTrain:  epoch 15, batch     3 | loss: 1.5562214MemoryTrain:  epoch 15, batch     4 | loss: 7.3856913MemoryTrain:  epoch 15, batch     5 | loss: 5.9371727MemoryTrain:  epoch 15, batch     6 | loss: 4.3614493MemoryTrain:  epoch  7, batch     7 | loss: 1.7246298MemoryTrain:  epoch 15, batch     0 | loss: 6.8384992MemoryTrain:  epoch 15, batch     1 | loss: 2.0914339MemoryTrain:  epoch 15, batch     2 | loss: 4.2156410MemoryTrain:  epoch 15, batch     3 | loss: 4.7329921MemoryTrain:  epoch 15, batch     4 | loss: 3.6420817MemoryTrain:  epoch 15, batch     5 | loss: 2.3660484MemoryTrain:  epoch 15, batch     6 | loss: 2.4419162MemoryTrain:  epoch  7, batch     7 | loss: 4.6593111MemoryTrain:  epoch 15, batch     0 | loss: 2.1746711MemoryTrain:  epoch 15, batch     1 | loss: 2.5138408MemoryTrain:  epoch 15, batch     2 | loss: 3.5702413MemoryTrain:  epoch 15, batch     3 | loss: 6.5771125MemoryTrain:  epoch 15, batch     4 | loss: 1.6208556MemoryTrain:  epoch 15, batch     5 | loss: 2.0120051MemoryTrain:  epoch 15, batch     6 | loss: 2.0883935MemoryTrain:  epoch  7, batch     7 | loss: 1.3357766
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 75.57%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 75.52%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 77.40%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 79.58%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 80.08%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 80.88%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.60%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 81.58%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 80.62%   [EVAL] batch:   20 | acc: 25.00%,  total acc: 77.98%   [EVAL] batch:   21 | acc: 50.00%,  total acc: 76.70%   [EVAL] batch:   22 | acc: 18.75%,  total acc: 74.18%   [EVAL] batch:   23 | acc: 43.75%,  total acc: 72.92%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 72.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.32%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 74.31%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.22%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 76.08%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.62%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 79.41%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 80.56%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 81.08%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 81.58%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 81.89%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 81.56%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 81.55%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 81.85%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 82.12%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 82.24%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 81.81%   [EVAL] batch:   45 | acc: 43.75%,  total acc: 80.98%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 80.59%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 80.34%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 79.72%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 79.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 79.41%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 79.69%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 79.95%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 80.09%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 80.34%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 80.47%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 79.93%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 79.09%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 78.39%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 77.92%   [EVAL] batch:   60 | acc: 25.00%,  total acc: 77.05%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 76.11%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 75.30%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 80.56%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 78.41%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 76.56%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 77.40%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 79.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 80.86%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 81.62%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 82.74%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 82.67%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 81.52%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 82.03%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 82.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.69%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 83.71%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 84.05%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 84.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.08%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.55%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.98%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 86.40%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 86.61%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 86.98%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 87.33%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.66%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.98%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.57%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.66%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 88.64%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 88.45%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 88.16%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 87.89%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 88.01%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 87.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 87.87%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 88.09%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 88.31%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 88.30%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 88.27%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 87.61%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 87.08%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 86.77%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 86.58%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 86.29%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 86.31%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 85.84%   [EVAL] batch:   64 | acc: 56.25%,  total acc: 85.38%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 85.23%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 84.98%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 84.83%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 84.78%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 84.64%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 84.33%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 84.03%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 83.82%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 83.70%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 83.50%   [EVAL] batch:   75 | acc: 25.00%,  total acc: 82.73%   [EVAL] batch:   76 | acc: 12.50%,  total acc: 81.82%   [EVAL] batch:   77 | acc: 31.25%,  total acc: 81.17%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 80.54%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 80.08%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 79.32%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 79.34%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 79.59%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 79.84%   [EVAL] batch:   84 | acc: 100.00%,  total acc: 80.07%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 80.31%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 80.53%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 80.26%   [EVAL] batch:   88 | acc: 12.50%,  total acc: 79.49%   [EVAL] batch:   89 | acc: 12.50%,  total acc: 78.75%   [EVAL] batch:   90 | acc: 6.25%,  total acc: 77.95%   [EVAL] batch:   91 | acc: 31.25%,  total acc: 77.45%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 76.68%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 76.13%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 76.38%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 76.56%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 76.80%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 77.04%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 77.27%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 77.50%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 77.66%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 77.88%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 77.85%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 77.94%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 78.15%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 78.30%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 78.21%   [EVAL] batch:  107 | acc: 75.00%,  total acc: 78.18%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 78.15%   [EVAL] batch:  109 | acc: 50.00%,  total acc: 77.90%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 77.76%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 77.62%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 77.54%   [EVAL] batch:  113 | acc: 37.50%,  total acc: 77.19%   [EVAL] batch:  114 | acc: 56.25%,  total acc: 77.01%   [EVAL] batch:  115 | acc: 50.00%,  total acc: 76.78%   [EVAL] batch:  116 | acc: 31.25%,  total acc: 76.39%   [EVAL] batch:  117 | acc: 43.75%,  total acc: 76.11%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 76.10%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 76.25%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 76.39%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 76.49%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 76.63%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 76.66%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 76.85%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 76.44%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 76.18%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 75.78%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 75.48%   [EVAL] batch:  129 | acc: 12.50%,  total acc: 75.00%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 74.71%   [EVAL] batch:  131 | acc: 56.25%,  total acc: 74.57%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 74.67%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 74.72%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 74.77%   [EVAL] batch:  135 | acc: 75.00%,  total acc: 74.77%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 74.86%   [EVAL] batch:  137 | acc: 56.25%,  total acc: 74.73%   [EVAL] batch:  138 | acc: 31.25%,  total acc: 74.42%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 74.15%   [EVAL] batch:  140 | acc: 37.50%,  total acc: 73.89%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 73.64%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 73.43%   [EVAL] batch:  143 | acc: 56.25%,  total acc: 73.31%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 73.49%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 73.67%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 73.81%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 73.99%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 74.16%   [EVAL] batch:  149 | acc: 81.25%,  total acc: 74.21%   [EVAL] batch:  150 | acc: 87.50%,  total acc: 74.30%   [EVAL] batch:  151 | acc: 87.50%,  total acc: 74.38%   [EVAL] batch:  152 | acc: 68.75%,  total acc: 74.35%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 74.43%   [EVAL] batch:  154 | acc: 75.00%,  total acc: 74.44%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 74.52%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 74.60%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 74.76%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 74.88%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 75.12%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 75.27%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 75.19%   [EVAL] batch:  163 | acc: 12.50%,  total acc: 74.81%   [EVAL] batch:  164 | acc: 31.25%,  total acc: 74.55%   [EVAL] batch:  165 | acc: 18.75%,  total acc: 74.21%   [EVAL] batch:  166 | acc: 12.50%,  total acc: 73.84%   [EVAL] batch:  167 | acc: 6.25%,  total acc: 73.44%   [EVAL] batch:  168 | acc: 31.25%,  total acc: 73.19%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 73.35%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 73.46%   [EVAL] batch:  171 | acc: 93.75%,  total acc: 73.58%   [EVAL] batch:  172 | acc: 100.00%,  total acc: 73.74%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 73.85%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 73.96%   [EVAL] batch:  175 | acc: 37.50%,  total acc: 73.76%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 73.62%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 73.49%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 73.36%   [EVAL] batch:  179 | acc: 37.50%,  total acc: 73.16%   [EVAL] batch:  180 | acc: 25.00%,  total acc: 72.89%   [EVAL] batch:  181 | acc: 43.75%,  total acc: 72.73%   [EVAL] batch:  182 | acc: 62.50%,  total acc: 72.68%   [EVAL] batch:  183 | acc: 50.00%,  total acc: 72.55%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 72.60%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 72.58%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 72.66%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 72.71%   [EVAL] batch:  188 | acc: 75.00%,  total acc: 72.72%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 72.70%   [EVAL] batch:  190 | acc: 81.25%,  total acc: 72.74%   [EVAL] batch:  191 | acc: 50.00%,  total acc: 72.62%   [EVAL] batch:  192 | acc: 75.00%,  total acc: 72.64%   [EVAL] batch:  193 | acc: 68.75%,  total acc: 72.62%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 72.69%   [EVAL] batch:  195 | acc: 81.25%,  total acc: 72.74%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 72.68%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 72.79%   [EVAL] batch:  198 | acc: 81.25%,  total acc: 72.83%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 72.91%   [EVAL] batch:  200 | acc: 100.00%,  total acc: 73.04%   [EVAL] batch:  201 | acc: 93.75%,  total acc: 73.14%   [EVAL] batch:  202 | acc: 87.50%,  total acc: 73.21%   [EVAL] batch:  203 | acc: 87.50%,  total acc: 73.28%   [EVAL] batch:  204 | acc: 93.75%,  total acc: 73.38%   [EVAL] batch:  205 | acc: 100.00%,  total acc: 73.51%   [EVAL] batch:  206 | acc: 68.75%,  total acc: 73.49%   [EVAL] batch:  207 | acc: 31.25%,  total acc: 73.29%   [EVAL] batch:  208 | acc: 37.50%,  total acc: 73.12%   [EVAL] batch:  209 | acc: 43.75%,  total acc: 72.98%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 72.75%   [EVAL] batch:  211 | acc: 50.00%,  total acc: 72.64%   [EVAL] batch:  212 | acc: 81.25%,  total acc: 72.68%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 72.81%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 72.94%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 73.06%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 73.19%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 73.31%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 73.40%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 73.52%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 73.64%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 73.76%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 73.88%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 74.00%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:  225 | acc: 93.75%,  total acc: 74.20%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 74.23%   [EVAL] batch:  227 | acc: 81.25%,  total acc: 74.26%   [EVAL] batch:  228 | acc: 81.25%,  total acc: 74.29%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 74.40%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 74.49%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 74.46%   [EVAL] batch:  232 | acc: 50.00%,  total acc: 74.36%   [EVAL] batch:  233 | acc: 62.50%,  total acc: 74.31%   [EVAL] batch:  234 | acc: 56.25%,  total acc: 74.23%   [EVAL] batch:  235 | acc: 68.75%,  total acc: 74.21%   [EVAL] batch:  236 | acc: 37.50%,  total acc: 74.05%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 74.08%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 74.14%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 74.24%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 74.30%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 74.38%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 74.46%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 74.49%   [EVAL] batch:  244 | acc: 18.75%,  total acc: 74.26%   [EVAL] batch:  245 | acc: 37.50%,  total acc: 74.11%   [EVAL] batch:  246 | acc: 43.75%,  total acc: 73.99%   [EVAL] batch:  247 | acc: 31.25%,  total acc: 73.82%   [EVAL] batch:  248 | acc: 18.75%,  total acc: 73.59%   [EVAL] batch:  249 | acc: 43.75%,  total acc: 73.47%   
cur_acc:  ['0.9514', '0.7708', '0.7569', '0.7530']
his_acc:  ['0.9514', '0.8490', '0.8009', '0.7348']
CurrentTrain: epoch 15, batch     0 | loss: 12.0360192CurrentTrain: epoch 15, batch     1 | loss: 10.6825875CurrentTrain: epoch 15, batch     2 | loss: 9.9937192CurrentTrain: epoch  1, batch     3 | loss: 6.6744178CurrentTrain: epoch 15, batch     0 | loss: 7.5137501CurrentTrain: epoch 15, batch     1 | loss: 9.4881351CurrentTrain: epoch 15, batch     2 | loss: 12.7446916CurrentTrain: epoch  1, batch     3 | loss: 21.2283165CurrentTrain: epoch 15, batch     0 | loss: 22.3371962CurrentTrain: epoch 15, batch     1 | loss: 10.1635068CurrentTrain: epoch 15, batch     2 | loss: 9.9981126CurrentTrain: epoch  1, batch     3 | loss: 6.4415239CurrentTrain: epoch 15, batch     0 | loss: 5.3462310CurrentTrain: epoch 15, batch     1 | loss: 7.7236092CurrentTrain: epoch 15, batch     2 | loss: 9.5374497CurrentTrain: epoch  1, batch     3 | loss: 8.4445245CurrentTrain: epoch 15, batch     0 | loss: 8.5768285CurrentTrain: epoch 15, batch     1 | loss: 12.6084967CurrentTrain: epoch 15, batch     2 | loss: 7.7519035CurrentTrain: epoch  1, batch     3 | loss: 7.3668907CurrentTrain: epoch 15, batch     0 | loss: 24.8875145CurrentTrain: epoch 15, batch     1 | loss: 14.0079698CurrentTrain: epoch 15, batch     2 | loss: 8.7160110CurrentTrain: epoch  1, batch     3 | loss: 6.5723693CurrentTrain: epoch 15, batch     0 | loss: 9.4855135CurrentTrain: epoch 15, batch     1 | loss: 4.9434517CurrentTrain: epoch 15, batch     2 | loss: 9.0524313CurrentTrain: epoch  1, batch     3 | loss: 7.2579748CurrentTrain: epoch 15, batch     0 | loss: 8.5616524CurrentTrain: epoch 15, batch     1 | loss: 5.3364800CurrentTrain: epoch 15, batch     2 | loss: 7.6764978CurrentTrain: epoch  1, batch     3 | loss: 7.4241552CurrentTrain: epoch 15, batch     0 | loss: 8.2329880CurrentTrain: epoch 15, batch     1 | loss: 8.6835788CurrentTrain: epoch 15, batch     2 | loss: 7.0155072CurrentTrain: epoch  1, batch     3 | loss: 6.5629036CurrentTrain: epoch 15, batch     0 | loss: 7.7189490CurrentTrain: epoch 15, batch     1 | loss: 8.8224030CurrentTrain: epoch 15, batch     2 | loss: 7.9945400CurrentTrain: epoch  1, batch     3 | loss: 12.1557160
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: alongside evan durbin and hugh gaitskell , he brought the thinking of john maynard keynes to the labour party , especially in relation to price determination .
Head Entity: hugh gaitskell
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: After serving as the mayor of Chicago, he became a prominent figure in the Democratic Party, advocating for social justice and economic reform.  
Head Entity: mayor of Chicago  
Tail Entity: Democratic Party  

Relation: member of political party  
Context: Throughout her career, she has been a staunch advocate for environmental policies as a member of the Green Party, pushing for sustainable practices at both local and national levels.  
Head Entity: she  
Tail Entity: Green Party  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: the original story of real life escape of betty mahmoody is depicted in the movie " not without my daughter " which itself was based on betty mahmoody 's book of the same name .
Head Entity: not without my daughter
Tail Entity: betty mahmoody
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film "the great gatsby" draws heavily from f. scott fitzgerald's classic novel, capturing the essence of the roaring twenties and the tragic love story within.  
Head Entity: the great gatsby  
Tail Entity: f. scott fitzgerald  

Relation: after a work by  
Context: the animated feature "the lion king" was inspired by shakespeare's play "hamlet," incorporating themes of betrayal and redemption in a unique setting.  
Head Entity: the lion king  
Tail Entity: shakespeare
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: several paintings are also mentioned , including rousseau 's " the dream " and van gogh 's " bedroom in arles " .
Head Entity: rousseau
Tail Entity: the dream
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: Among his many contributions to literature, one of his most acclaimed novels is "The Great Gatsby," which explores themes of wealth and social change in 1920s America.  
Head Entity: F. Scott Fitzgerald  
Tail Entity: The Great Gatsby  

Relation: notable work  
Context: The artist is best known for his groundbreaking sculpture "The Thinker," which has become a symbol of philosophy and contemplation.  
Head Entity: Auguste Rodin  
Tail Entity: The Thinker  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: in 1986 fram traveled to poland to play against katowice in a very even duel fram eventually lost . sparta prague came to reykjavík 1987 to play against fram reykjavik at laugardalsvöllur stadium .
Head Entity: fram reykjavik
Tail Entity: reykjavík
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: in 2001, the tech company apple inc. moved its headquarters to cupertino, california, where it has since developed numerous innovative products.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: headquarters location  
Context: the multinational corporation unilever has its headquarters located in london, where it oversees its global operations and product development.  
Head Entity: unilever  
Tail Entity: london  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most recognized as the wolf, which is classified under the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of flowering plants, the family Rosaceae encompasses a variety of species, including roses and strawberries, which are categorized under the order Rosales.  
Head Entity: Rosaceae  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: he later collaborated with vito volterra and explored laplace transforms and other parts of functional analysis .
Head Entity: vito volterra
Tail Entity: functional analysis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: Marie Curie conducted pioneering research on radioactivity, which laid the groundwork for advancements in nuclear physics.  
Head Entity: Marie Curie  
Tail Entity: nuclear physics  

Relation: field of work  
Context: The company specializes in artificial intelligence and machine learning, providing innovative solutions for various industries.  
Head Entity: The company  
Tail Entity: artificial intelligence  
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: this early involvement in radio led him to buy many small , under performing radio stations beginning with wcos in columbia , south carolina , and turn them around with new formats and improved management .
Head Entity: wcos
Tail Entity: columbia , south carolina
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: The new television station, KXYZ, has been granted permission to broadcast to the entire metropolitan area of San Francisco, ensuring that local residents have access to a variety of programming.  
Head Entity: KXYZ  
Tail Entity: San Francisco  

Relation: licensed to broadcast to  
Context: After acquiring the rights, the radio station WABC can now officially broadcast to listeners in the greater New York City area, reaching millions of potential audience members.  
Head Entity: WABC  
Tail Entity: greater New York City area  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: ngc 192 is a type sba spiral galaxy located in the constellation cetus .
Head Entity: ngc 192
Tail Entity: cetus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: the orion nebula is located in the constellation orion, which is one of the most prominent constellations in the night sky.  
Head Entity: orion nebula  
Tail Entity: orion  

Relation: constellation  
Context: the star sirius is part of the constellation canis major, known for being the brightest star in the night sky.  
Head Entity: sirius  
Tail Entity: canis major  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: services to the island run from san juan 's luis muñoz marín international airport , ceiba airport or isla grande airport ( 20- to 30-minute flight ) .
Head Entity: isla grande airport
Tail Entity: san juan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The city of Barcelona is well-connected to various destinations through the El Prat Airport, which offers flights to numerous European cities.  
Head Entity: El Prat Airport  
Tail Entity: Barcelona  

Relation: place served by transport hub  
Context: The central train station in Zurich provides access to multiple Swiss cities and international destinations, making it a key transport hub in the region.  
Head Entity: Zurich central train station  
Tail Entity: Zurich  
MemoryTrain:  epoch 15, batch     0 | loss: 4.9976594MemoryTrain:  epoch 15, batch     1 | loss: 3.7238259MemoryTrain:  epoch 15, batch     2 | loss: 6.0907535MemoryTrain:  epoch 15, batch     3 | loss: 3.2900621MemoryTrain:  epoch 15, batch     4 | loss: 4.6888434MemoryTrain:  epoch 15, batch     5 | loss: 3.2181337MemoryTrain:  epoch 15, batch     6 | loss: 5.2960365MemoryTrain:  epoch 15, batch     7 | loss: 3.1444507MemoryTrain:  epoch 15, batch     8 | loss: 5.9625744MemoryTrain:  epoch  5, batch     9 | loss: 8.8118555MemoryTrain:  epoch 15, batch     0 | loss: 3.1299007MemoryTrain:  epoch 15, batch     1 | loss: 3.7529181MemoryTrain:  epoch 15, batch     2 | loss: 2.2363470MemoryTrain:  epoch 15, batch     3 | loss: 3.0941243MemoryTrain:  epoch 15, batch     4 | loss: 4.5020201MemoryTrain:  epoch 15, batch     5 | loss: 2.9112906MemoryTrain:  epoch 15, batch     6 | loss: 3.0343309MemoryTrain:  epoch 15, batch     7 | loss: 2.8615748MemoryTrain:  epoch 15, batch     8 | loss: 4.7405013MemoryTrain:  epoch  5, batch     9 | loss: 8.4297206MemoryTrain:  epoch 15, batch     0 | loss: 4.8286414MemoryTrain:  epoch 15, batch     1 | loss: 2.3847204MemoryTrain:  epoch 15, batch     2 | loss: 2.0110706MemoryTrain:  epoch 15, batch     3 | loss: 2.1016842MemoryTrain:  epoch 15, batch     4 | loss: 2.0967784MemoryTrain:  epoch 15, batch     5 | loss: 2.3191791MemoryTrain:  epoch 15, batch     6 | loss: 1.9119662MemoryTrain:  epoch 15, batch     7 | loss: 4.9237749MemoryTrain:  epoch 15, batch     8 | loss: 2.5783128MemoryTrain:  epoch  5, batch     9 | loss: 9.1554154MemoryTrain:  epoch 15, batch     0 | loss: 3.7556831MemoryTrain:  epoch 15, batch     1 | loss: 2.0154221MemoryTrain:  epoch 15, batch     2 | loss: 1.9076744MemoryTrain:  epoch 15, batch     3 | loss: 2.7112185MemoryTrain:  epoch 15, batch     4 | loss: 1.7586254MemoryTrain:  epoch 15, batch     5 | loss: 2.3715948MemoryTrain:  epoch 15, batch     6 | loss: 1.9760422MemoryTrain:  epoch 15, batch     7 | loss: 1.6918944MemoryTrain:  epoch 15, batch     8 | loss: 2.6513170MemoryTrain:  epoch  5, batch     9 | loss: 8.0919506MemoryTrain:  epoch 15, batch     0 | loss: 2.8606099MemoryTrain:  epoch 15, batch     1 | loss: 2.7280409MemoryTrain:  epoch 15, batch     2 | loss: 1.8619680MemoryTrain:  epoch 15, batch     3 | loss: 1.8158497MemoryTrain:  epoch 15, batch     4 | loss: 2.3121584MemoryTrain:  epoch 15, batch     5 | loss: 1.9258487MemoryTrain:  epoch 15, batch     6 | loss: 1.8622439MemoryTrain:  epoch 15, batch     7 | loss: 2.2546128MemoryTrain:  epoch 15, batch     8 | loss: 2.8876856MemoryTrain:  epoch  5, batch     9 | loss: 8.1315656MemoryTrain:  epoch 15, batch     0 | loss: 1.9062408MemoryTrain:  epoch 15, batch     1 | loss: 3.9100297MemoryTrain:  epoch 15, batch     2 | loss: 4.3319621MemoryTrain:  epoch 15, batch     3 | loss: 3.6435329MemoryTrain:  epoch 15, batch     4 | loss: 4.1789570MemoryTrain:  epoch 15, batch     5 | loss: 1.8361577MemoryTrain:  epoch 15, batch     6 | loss: 1.6833024MemoryTrain:  epoch 15, batch     7 | loss: 1.5343510MemoryTrain:  epoch 15, batch     8 | loss: 3.8119269MemoryTrain:  epoch  5, batch     9 | loss: 8.7444628MemoryTrain:  epoch 15, batch     0 | loss: 1.6460046MemoryTrain:  epoch 15, batch     1 | loss: 1.6554027MemoryTrain:  epoch 15, batch     2 | loss: 2.3411238MemoryTrain:  epoch 15, batch     3 | loss: 2.1778715MemoryTrain:  epoch 15, batch     4 | loss: 3.7666895MemoryTrain:  epoch 15, batch     5 | loss: 1.5849330MemoryTrain:  epoch 15, batch     6 | loss: 2.6876436MemoryTrain:  epoch 15, batch     7 | loss: 2.9750629MemoryTrain:  epoch 15, batch     8 | loss: 1.4983002MemoryTrain:  epoch  5, batch     9 | loss: 8.1414701MemoryTrain:  epoch 15, batch     0 | loss: 1.7416827MemoryTrain:  epoch 15, batch     1 | loss: 3.0459780MemoryTrain:  epoch 15, batch     2 | loss: 4.4724113MemoryTrain:  epoch 15, batch     3 | loss: 1.6210363MemoryTrain:  epoch 15, batch     4 | loss: 1.9000165MemoryTrain:  epoch 15, batch     5 | loss: 1.7980855MemoryTrain:  epoch 15, batch     6 | loss: 1.7569202MemoryTrain:  epoch 15, batch     7 | loss: 1.8465307MemoryTrain:  epoch 15, batch     8 | loss: 2.1501735MemoryTrain:  epoch  5, batch     9 | loss: 8.0564471MemoryTrain:  epoch 15, batch     0 | loss: 1.5440803MemoryTrain:  epoch 15, batch     1 | loss: 1.7287795MemoryTrain:  epoch 15, batch     2 | loss: 1.4689915MemoryTrain:  epoch 15, batch     3 | loss: 1.5899099MemoryTrain:  epoch 15, batch     4 | loss: 1.4624948MemoryTrain:  epoch 15, batch     5 | loss: 3.6340870MemoryTrain:  epoch 15, batch     6 | loss: 2.1941730MemoryTrain:  epoch 15, batch     7 | loss: 1.8691260MemoryTrain:  epoch 15, batch     8 | loss: 1.6280293MemoryTrain:  epoch  5, batch     9 | loss: 8.1595626MemoryTrain:  epoch 15, batch     0 | loss: 1.4713865MemoryTrain:  epoch 15, batch     1 | loss: 2.2066396MemoryTrain:  epoch 15, batch     2 | loss: 1.6241156MemoryTrain:  epoch 15, batch     3 | loss: 2.4396991MemoryTrain:  epoch 15, batch     4 | loss: 1.7620084MemoryTrain:  epoch 15, batch     5 | loss: 1.7655133MemoryTrain:  epoch 15, batch     6 | loss: 1.4473362MemoryTrain:  epoch 15, batch     7 | loss: 2.0070870MemoryTrain:  epoch 15, batch     8 | loss: 1.9166350MemoryTrain:  epoch  5, batch     9 | loss: 8.0865837
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 94.64%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 82.35%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 81.60%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 80.92%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 80.62%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 79.76%   [EVAL] batch:   21 | acc: 50.00%,  total acc: 78.41%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 78.53%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 78.39%   [EVAL] batch:   24 | acc: 31.25%,  total acc: 76.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.40%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.24%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.02%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 79.74%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.42%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.05%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 81.63%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 81.43%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 81.96%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 81.60%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 81.59%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 81.58%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 82.05%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.50%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 82.93%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 83.72%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 83.95%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 84.31%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 84.65%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 84.97%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 85.29%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 85.59%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 85.88%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 85.91%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 86.06%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 86.20%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 86.34%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 86.36%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 86.50%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 86.62%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 86.75%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 86.55%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 86.46%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 86.58%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 86.49%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 85.81%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 73.61%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 71.88%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 70.45%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 69.23%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 70.98%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 72.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 75.37%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 76.39%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 77.30%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 76.79%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 76.14%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 75.52%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 75.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.44%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 77.31%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.66%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 79.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.04%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.66%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 81.80%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 82.64%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 83.11%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 83.55%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.97%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.76%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 84.82%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 85.03%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 85.09%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 85.28%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 84.92%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 84.57%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 84.44%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 84.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 84.31%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 84.55%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 84.09%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 84.26%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 84.10%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 83.51%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 82.94%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 82.58%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 82.46%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 82.34%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 81.84%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 81.15%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 80.97%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 80.32%   [EVAL] batch:   67 | acc: 62.50%,  total acc: 80.06%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 79.89%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 79.73%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 79.40%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 79.17%   [EVAL] batch:   72 | acc: 62.50%,  total acc: 78.94%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 78.80%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 78.58%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 78.04%   [EVAL] batch:   76 | acc: 18.75%,  total acc: 77.27%   [EVAL] batch:   77 | acc: 31.25%,  total acc: 76.68%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 76.19%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 75.86%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 75.23%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 75.30%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 75.60%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 75.89%   [EVAL] batch:   84 | acc: 100.00%,  total acc: 76.18%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 76.45%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 76.72%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 76.49%   [EVAL] batch:   88 | acc: 6.25%,  total acc: 75.70%   [EVAL] batch:   89 | acc: 6.25%,  total acc: 74.93%   [EVAL] batch:   90 | acc: 12.50%,  total acc: 74.24%   [EVAL] batch:   91 | acc: 25.00%,  total acc: 73.71%   [EVAL] batch:   92 | acc: 0.00%,  total acc: 72.92%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 72.41%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 72.70%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 73.20%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 73.47%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 73.74%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 74.00%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 73.76%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 73.71%   [EVAL] batch:  102 | acc: 31.25%,  total acc: 73.30%   [EVAL] batch:  103 | acc: 37.50%,  total acc: 72.96%   [EVAL] batch:  104 | acc: 43.75%,  total acc: 72.68%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 72.64%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 72.49%   [EVAL] batch:  107 | acc: 75.00%,  total acc: 72.51%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 72.53%   [EVAL] batch:  109 | acc: 62.50%,  total acc: 72.44%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 72.35%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 72.27%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 72.23%   [EVAL] batch:  113 | acc: 25.00%,  total acc: 71.82%   [EVAL] batch:  114 | acc: 56.25%,  total acc: 71.68%   [EVAL] batch:  115 | acc: 37.50%,  total acc: 71.39%   [EVAL] batch:  116 | acc: 25.00%,  total acc: 70.99%   [EVAL] batch:  117 | acc: 43.75%,  total acc: 70.76%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 70.75%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 70.99%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 71.18%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 71.36%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 71.49%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 71.62%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 71.80%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 71.43%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 71.06%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 70.70%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 70.45%   [EVAL] batch:  129 | acc: 12.50%,  total acc: 70.00%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 69.75%   [EVAL] batch:  131 | acc: 56.25%,  total acc: 69.65%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 69.78%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 69.87%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 70.05%   [EVAL] batch:  135 | acc: 75.00%,  total acc: 70.08%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 70.21%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 70.24%   [EVAL] batch:  138 | acc: 62.50%,  total acc: 70.19%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 70.18%   [EVAL] batch:  140 | acc: 56.25%,  total acc: 70.08%   [EVAL] batch:  141 | acc: 56.25%,  total acc: 69.98%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 69.97%   [EVAL] batch:  143 | acc: 62.50%,  total acc: 69.92%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 70.13%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 70.33%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 70.49%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 70.69%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 70.89%   [EVAL] batch:  149 | acc: 87.50%,  total acc: 71.00%   [EVAL] batch:  150 | acc: 87.50%,  total acc: 71.11%   [EVAL] batch:  151 | acc: 87.50%,  total acc: 71.22%   [EVAL] batch:  152 | acc: 68.75%,  total acc: 71.20%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 71.31%   [EVAL] batch:  154 | acc: 75.00%,  total acc: 71.33%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 71.43%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 71.54%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 71.72%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 71.86%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 71.99%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 72.13%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 72.26%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 72.16%   [EVAL] batch:  163 | acc: 6.25%,  total acc: 71.76%   [EVAL] batch:  164 | acc: 18.75%,  total acc: 71.44%   [EVAL] batch:  165 | acc: 12.50%,  total acc: 71.08%   [EVAL] batch:  166 | acc: 6.25%,  total acc: 70.70%   [EVAL] batch:  167 | acc: 6.25%,  total acc: 70.31%   [EVAL] batch:  168 | acc: 25.00%,  total acc: 70.04%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 70.22%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 70.36%   [EVAL] batch:  171 | acc: 93.75%,  total acc: 70.49%   [EVAL] batch:  172 | acc: 100.00%,  total acc: 70.66%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 70.80%   [EVAL] batch:  174 | acc: 100.00%,  total acc: 70.96%   [EVAL] batch:  175 | acc: 43.75%,  total acc: 70.81%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 70.66%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 70.54%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 70.43%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 70.35%   [EVAL] batch:  180 | acc: 18.75%,  total acc: 70.06%   [EVAL] batch:  181 | acc: 37.50%,  total acc: 69.88%   [EVAL] batch:  182 | acc: 37.50%,  total acc: 69.71%   [EVAL] batch:  183 | acc: 43.75%,  total acc: 69.57%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 69.53%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 69.42%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 69.42%   [EVAL] batch:  187 | acc: 68.75%,  total acc: 69.41%   [EVAL] batch:  188 | acc: 75.00%,  total acc: 69.44%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 69.44%   [EVAL] batch:  190 | acc: 93.75%,  total acc: 69.57%   [EVAL] batch:  191 | acc: 75.00%,  total acc: 69.60%   [EVAL] batch:  192 | acc: 87.50%,  total acc: 69.69%   [EVAL] batch:  193 | acc: 68.75%,  total acc: 69.68%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 69.71%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 69.71%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 69.67%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 69.76%   [EVAL] batch:  198 | acc: 81.25%,  total acc: 69.82%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 69.91%   [EVAL] batch:  200 | acc: 93.75%,  total acc: 70.02%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 70.11%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 70.14%   [EVAL] batch:  203 | acc: 93.75%,  total acc: 70.25%   [EVAL] batch:  204 | acc: 87.50%,  total acc: 70.34%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 70.45%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 70.53%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 70.31%   [EVAL] batch:  208 | acc: 43.75%,  total acc: 70.19%   [EVAL] batch:  209 | acc: 43.75%,  total acc: 70.06%   [EVAL] batch:  210 | acc: 56.25%,  total acc: 69.99%   [EVAL] batch:  211 | acc: 43.75%,  total acc: 69.87%   [EVAL] batch:  212 | acc: 81.25%,  total acc: 69.92%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 70.04%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 70.17%   [EVAL] batch:  215 | acc: 81.25%,  total acc: 70.23%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 70.36%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 70.47%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 70.58%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 70.71%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 70.84%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 70.97%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 71.10%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 71.23%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 71.36%   [EVAL] batch:  225 | acc: 93.75%,  total acc: 71.46%   [EVAL] batch:  226 | acc: 75.00%,  total acc: 71.48%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 71.49%   [EVAL] batch:  228 | acc: 81.25%,  total acc: 71.53%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 71.63%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 71.73%   [EVAL] batch:  231 | acc: 62.50%,  total acc: 71.69%   [EVAL] batch:  232 | acc: 50.00%,  total acc: 71.59%   [EVAL] batch:  233 | acc: 75.00%,  total acc: 71.61%   [EVAL] batch:  234 | acc: 56.25%,  total acc: 71.54%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 71.50%   [EVAL] batch:  236 | acc: 50.00%,  total acc: 71.41%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 71.48%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 71.55%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 71.67%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 71.73%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 71.82%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 71.91%   [EVAL] batch:  243 | acc: 68.75%,  total acc: 71.90%   [EVAL] batch:  244 | acc: 18.75%,  total acc: 71.68%   [EVAL] batch:  245 | acc: 25.00%,  total acc: 71.49%   [EVAL] batch:  246 | acc: 18.75%,  total acc: 71.28%   [EVAL] batch:  247 | acc: 25.00%,  total acc: 71.09%   [EVAL] batch:  248 | acc: 6.25%,  total acc: 70.83%   [EVAL] batch:  249 | acc: 25.00%,  total acc: 70.65%   [EVAL] batch:  250 | acc: 100.00%,  total acc: 70.77%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 70.86%   [EVAL] batch:  252 | acc: 100.00%,  total acc: 70.97%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 71.04%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 71.13%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 71.24%   [EVAL] batch:  256 | acc: 87.50%,  total acc: 71.30%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 71.37%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 71.36%   [EVAL] batch:  259 | acc: 81.25%,  total acc: 71.39%   [EVAL] batch:  260 | acc: 75.00%,  total acc: 71.41%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 71.42%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 71.51%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 71.52%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 71.53%   [EVAL] batch:  265 | acc: 50.00%,  total acc: 71.45%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 71.40%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 71.39%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 71.38%   [EVAL] batch:  269 | acc: 75.00%,  total acc: 71.39%   [EVAL] batch:  270 | acc: 62.50%,  total acc: 71.36%   [EVAL] batch:  271 | acc: 50.00%,  total acc: 71.28%   [EVAL] batch:  272 | acc: 81.25%,  total acc: 71.31%   [EVAL] batch:  273 | acc: 75.00%,  total acc: 71.33%   [EVAL] batch:  274 | acc: 31.25%,  total acc: 71.18%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 71.29%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 71.39%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 71.49%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 71.59%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 71.70%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 71.80%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 71.85%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 71.93%   [EVAL] batch:  283 | acc: 75.00%,  total acc: 71.94%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 72.04%   [EVAL] batch:  285 | acc: 68.75%,  total acc: 72.03%   [EVAL] batch:  286 | acc: 81.25%,  total acc: 72.06%   [EVAL] batch:  287 | acc: 81.25%,  total acc: 72.09%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 72.19%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 72.28%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 72.38%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 72.47%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 72.57%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 72.64%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 72.73%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 72.83%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 73.01%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 73.10%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 73.19%   [EVAL] batch:  300 | acc: 87.50%,  total acc: 73.24%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 73.30%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 73.37%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 73.44%   [EVAL] batch:  304 | acc: 87.50%,  total acc: 73.48%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 73.55%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 73.62%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 73.68%   [EVAL] batch:  308 | acc: 75.00%,  total acc: 73.69%   [EVAL] batch:  309 | acc: 81.25%,  total acc: 73.71%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 73.77%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 73.80%   [EVAL] batch:  312 | acc: 43.75%,  total acc: 73.70%   
cur_acc:  ['0.9514', '0.7708', '0.7569', '0.7530', '0.8581']
his_acc:  ['0.9514', '0.8490', '0.8009', '0.7348', '0.7370']
CurrentTrain: epoch 15, batch     0 | loss: 13.1519730CurrentTrain: epoch 15, batch     1 | loss: 19.5777929CurrentTrain: epoch 15, batch     2 | loss: 10.6770770CurrentTrain: epoch  1, batch     3 | loss: 7.8862927CurrentTrain: epoch 15, batch     0 | loss: 15.4525048CurrentTrain: epoch 15, batch     1 | loss: 8.3541220CurrentTrain: epoch 15, batch     2 | loss: 15.7968638CurrentTrain: epoch  1, batch     3 | loss: 7.6933909CurrentTrain: epoch 15, batch     0 | loss: 14.0845982CurrentTrain: epoch 15, batch     1 | loss: 10.3454531CurrentTrain: epoch 15, batch     2 | loss: 10.1111740CurrentTrain: epoch  1, batch     3 | loss: 9.2716393CurrentTrain: epoch 15, batch     0 | loss: 6.9229633CurrentTrain: epoch 15, batch     1 | loss: 8.3927300CurrentTrain: epoch 15, batch     2 | loss: 9.9255065CurrentTrain: epoch  1, batch     3 | loss: 6.4831246CurrentTrain: epoch 15, batch     0 | loss: 7.0249318CurrentTrain: epoch 15, batch     1 | loss: 10.4695819CurrentTrain: epoch 15, batch     2 | loss: 7.2053143CurrentTrain: epoch  1, batch     3 | loss: 7.0051806CurrentTrain: epoch 15, batch     0 | loss: 12.0701175CurrentTrain: epoch 15, batch     1 | loss: 7.2034050CurrentTrain: epoch 15, batch     2 | loss: 12.2025073CurrentTrain: epoch  1, batch     3 | loss: 5.8991566CurrentTrain: epoch 15, batch     0 | loss: 7.9729953CurrentTrain: epoch 15, batch     1 | loss: 7.5139444CurrentTrain: epoch 15, batch     2 | loss: 10.6325591CurrentTrain: epoch  1, batch     3 | loss: 6.9986032CurrentTrain: epoch 15, batch     0 | loss: 5.9698509CurrentTrain: epoch 15, batch     1 | loss: 4.4718794CurrentTrain: epoch 15, batch     2 | loss: 8.0857478CurrentTrain: epoch  1, batch     3 | loss: 6.0353361CurrentTrain: epoch 15, batch     0 | loss: 11.1861638CurrentTrain: epoch 15, batch     1 | loss: 10.6753715CurrentTrain: epoch 15, batch     2 | loss: 10.3242192CurrentTrain: epoch  1, batch     3 | loss: 5.9914953CurrentTrain: epoch 15, batch     0 | loss: 6.8409015CurrentTrain: epoch 15, batch     1 | loss: 8.6313313CurrentTrain: epoch 15, batch     2 | loss: 8.4939709CurrentTrain: epoch  1, batch     3 | loss: 5.8453968
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: rohan is the son of narayana murthy , founder of infosys , and his wife sudha murthy , an engineer and author of note .
Head Entity: narayana murthy
Tail Entity: sudha murthy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: elena is a renowned artist who often collaborates with her husband, mark, a famous photographer known for his stunning landscapes.  
Head Entity: mark  
Tail Entity: elena  

Relation: spouse  
Context: during the award ceremony, it was revealed that the director, james, and his wife, lucy, have been working together on several successful films for over a decade.  
Head Entity: james  
Tail Entity: lucy  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: it was first released on a single in the uk by parlophone in september 1967 , and on the group 's self - titled album " tomorrow " in february 1968 .
Head Entity: tomorrow
Tail Entity: parlophone
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: The band's debut album was launched under the iconic label Atlantic Records, which has been home to many legendary artists.  
Head Entity: debut album  
Tail Entity: Atlantic Records  

Relation: record label  
Context: After signing with Universal Music, the artist released their latest single, which quickly climbed the charts.  
Head Entity: latest single  
Tail Entity: Universal Music  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: " the many adventures of winnie the pooh " is a 1977 american animated buddy musical comedy film produced by walt disney productions and distributed by buena vista distribution .
Head Entity: the many adventures of winnie the pooh
Tail Entity: buena vista distribution
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: "Inception" is a 2010 science fiction film directed by Christopher Nolan and distributed by Warner Bros. Pictures.  
Head Entity: Inception  
Tail Entity: Warner Bros. Pictures  

Relation: distributor  
Context: "The Dark Knight" is a 2008 superhero film directed by Christopher Nolan and distributed by Warner Bros. Pictures.  
Head Entity: The Dark Knight  
Tail Entity: Warner Bros. Pictures  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: in 1796 carondelet commissioned pintado to conduct a survey of lands granted to ambrosio longue - épée in iberville county on the west bank of the mississippi river opposite baton rouge .
Head Entity: baton rouge
Tail Entity: mississippi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: The city of Venice is famous for its canals and is built on a group of 118 small islands separated by canals and linked by bridges, situated in the Adriatic Sea.  
Head Entity: Venice  
Tail Entity: Adriatic Sea  

Relation: located in or next to body of water  
Context: The town of Key West is known for its beautiful sunsets and is located at the southernmost point of the continental United States, right next to the Gulf of Mexico.  
Head Entity: Key West  
Tail Entity: Gulf of Mexico  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: following the sale of amstrad plc to rupert murdoch 's bskyb , viglen is now lord sugar 's sole it establishment .
Head Entity: bskyb
Tail Entity: amstrad
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Relation: subsidiary  
Context: In 2018, Facebook acquired WhatsApp, making it a subsidiary of the social media giant.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Relation: subsidiary  
Context: After the merger, the new company announced that its subsidiary, XYZ Corp, would continue to operate independently.  
Head Entity: new company  
Tail Entity: XYZ Corp  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: he challenged seven other swimmers in heat four including ukraine 's rostyslav svanidze , a top 16 finalist in atlanta four years earlier .
Head Entity: rostyslav svanidze
Tail Entity: ukraine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: after years of living in the united states, the renowned artist decided to apply for dual citizenship, allowing him to maintain his ties to his homeland, italy.  
Head Entity: the renowned artist  
Tail Entity: italy  

Relation: country of citizenship  
Context: during the international conference, the delegate from canada proudly spoke about the rights and privileges of being a citizen of his country, emphasizing the importance of citizenship in shaping national identity.  
Head Entity: the delegate from canada  
Tail Entity: canada  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: nina cites " duffy , muse , and other pop and alternative artists " as her major influences .
Head Entity: duffy
Tail Entity: pop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the film "inception" is often categorized as a science fiction thriller that explores complex themes of dreams and reality.  
Head Entity: inception  
Tail Entity: science fiction  

Relation: genre  
Context: the band "coldplay" is known for their melodic rock sound, blending elements of pop and alternative music in their albums.  
Head Entity: coldplay  
Tail Entity: melodic rock  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: san lázaro is 660 km far from asunción and is located at the exact site of the confluence of the apa river with paraguay river , bordering brazil - chaco .
Head Entity: apa river
Tail Entity: paraguay river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: the mississippi river flows into the gulf of mexico, creating a rich delta ecosystem that supports diverse wildlife and plant species.  
Head Entity: mississippi river  
Tail Entity: gulf of mexico  

Relation: mouth of the watercourse  
Context: the rhine river, which runs through several countries in europe, ultimately empties into the north sea, providing a vital shipping route for international trade.  
Head Entity: rhine river  
Tail Entity: north sea  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: bradley halliday ( born 10 july 1995 ) is an english professional footballer who plays as a defender or a midfielder for league two club cambridge united .
Head Entity: bradley halliday
Tail Entity: defender
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: alex morgan ( born 2 july 1989 ) is an american soccer player who plays as a forward for the national team and club orlando pride in the national women's soccer league.  
Head Entity: alex morgan  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: lebron james ( born 30 december 1984 ) is an american professional basketball player who plays as a small forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: small forward  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: the stade louis ii in monaco was the venue for the uefa super cup every year since 1998 .
Head Entity: 1998
Tail Entity: uefa super cup
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2020 NBA season was significantly impacted by the COVID-19 pandemic, leading to a unique playoff format.  
Head Entity: 2020  
Tail Entity: NBA  

Relation: sports season of league or competition  
Context: The 2019 Rugby World Cup took place in Japan, showcasing teams from around the globe.  
Head Entity: 2019  
Tail Entity: Rugby World Cup  
MemoryTrain:  epoch 15, batch     0 | loss: 4.5184952MemoryTrain:  epoch 15, batch     1 | loss: 2.7277883MemoryTrain:  epoch 15, batch     2 | loss: 2.8347742MemoryTrain:  epoch 15, batch     3 | loss: 5.1760105MemoryTrain:  epoch 15, batch     4 | loss: 4.7061382MemoryTrain:  epoch 15, batch     5 | loss: 4.7646727MemoryTrain:  epoch 15, batch     6 | loss: 2.1639757MemoryTrain:  epoch 15, batch     7 | loss: 2.7905141MemoryTrain:  epoch 15, batch     8 | loss: 5.9809204MemoryTrain:  epoch 15, batch     9 | loss: 2.4360920MemoryTrain:  epoch 15, batch    10 | loss: 4.1399055MemoryTrain:  epoch  3, batch    11 | loss: 11.7617899MemoryTrain:  epoch 15, batch     0 | loss: 2.7528163MemoryTrain:  epoch 15, batch     1 | loss: 1.9905567MemoryTrain:  epoch 15, batch     2 | loss: 2.9963929MemoryTrain:  epoch 15, batch     3 | loss: 3.1654977MemoryTrain:  epoch 15, batch     4 | loss: 2.4353668MemoryTrain:  epoch 15, batch     5 | loss: 3.7972879MemoryTrain:  epoch 15, batch     6 | loss: 2.6295283MemoryTrain:  epoch 15, batch     7 | loss: 6.7456528MemoryTrain:  epoch 15, batch     8 | loss: 2.1912912MemoryTrain:  epoch 15, batch     9 | loss: 4.9204680MemoryTrain:  epoch 15, batch    10 | loss: 2.6675255MemoryTrain:  epoch  3, batch    11 | loss: 10.7975862MemoryTrain:  epoch 15, batch     0 | loss: 5.9535611MemoryTrain:  epoch 15, batch     1 | loss: 2.4777223MemoryTrain:  epoch 15, batch     2 | loss: 2.2563640MemoryTrain:  epoch 15, batch     3 | loss: 1.7544843MemoryTrain:  epoch 15, batch     4 | loss: 2.3778754MemoryTrain:  epoch 15, batch     5 | loss: 3.6572748MemoryTrain:  epoch 15, batch     6 | loss: 2.1335803MemoryTrain:  epoch 15, batch     7 | loss: 2.0647886MemoryTrain:  epoch 15, batch     8 | loss: 2.1285615MemoryTrain:  epoch 15, batch     9 | loss: 2.4354447MemoryTrain:  epoch 15, batch    10 | loss: 4.4900665MemoryTrain:  epoch  3, batch    11 | loss: 11.0925275MemoryTrain:  epoch 15, batch     0 | loss: 4.3069131MemoryTrain:  epoch 15, batch     1 | loss: 2.2872741MemoryTrain:  epoch 15, batch     2 | loss: 1.7645138MemoryTrain:  epoch 15, batch     3 | loss: 1.7674418MemoryTrain:  epoch 15, batch     4 | loss: 7.1742785MemoryTrain:  epoch 15, batch     5 | loss: 3.0408618MemoryTrain:  epoch 15, batch     6 | loss: 2.4805170MemoryTrain:  epoch 15, batch     7 | loss: 4.4169932MemoryTrain:  epoch 15, batch     8 | loss: 2.5950280MemoryTrain:  epoch 15, batch     9 | loss: 1.4085854MemoryTrain:  epoch 15, batch    10 | loss: 4.2432606MemoryTrain:  epoch  3, batch    11 | loss: 10.5598972MemoryTrain:  epoch 15, batch     0 | loss: 1.7930809MemoryTrain:  epoch 15, batch     1 | loss: 1.9351534MemoryTrain:  epoch 15, batch     2 | loss: 1.8524767MemoryTrain:  epoch 15, batch     3 | loss: 3.7096022MemoryTrain:  epoch 15, batch     4 | loss: 1.8235420MemoryTrain:  epoch 15, batch     5 | loss: 1.9263350MemoryTrain:  epoch 15, batch     6 | loss: 2.6491110MemoryTrain:  epoch 15, batch     7 | loss: 1.6412495MemoryTrain:  epoch 15, batch     8 | loss: 1.8284575MemoryTrain:  epoch 15, batch     9 | loss: 1.6960457MemoryTrain:  epoch 15, batch    10 | loss: 1.8719981MemoryTrain:  epoch  3, batch    11 | loss: 10.9626437MemoryTrain:  epoch 15, batch     0 | loss: 1.9682996MemoryTrain:  epoch 15, batch     1 | loss: 1.7039295MemoryTrain:  epoch 15, batch     2 | loss: 1.7620923MemoryTrain:  epoch 15, batch     3 | loss: 2.5311818MemoryTrain:  epoch 15, batch     4 | loss: 1.5530958MemoryTrain:  epoch 15, batch     5 | loss: 4.6020554MemoryTrain:  epoch 15, batch     6 | loss: 3.8874433MemoryTrain:  epoch 15, batch     7 | loss: 2.5349237MemoryTrain:  epoch 15, batch     8 | loss: 1.4856465MemoryTrain:  epoch 15, batch     9 | loss: 1.6776007MemoryTrain:  epoch 15, batch    10 | loss: 1.4193006MemoryTrain:  epoch  3, batch    11 | loss: 9.5328200MemoryTrain:  epoch 15, batch     0 | loss: 1.7971917MemoryTrain:  epoch 15, batch     1 | loss: 3.9393942MemoryTrain:  epoch 15, batch     2 | loss: 1.8513302MemoryTrain:  epoch 15, batch     3 | loss: 1.9477036MemoryTrain:  epoch 15, batch     4 | loss: 3.7078402MemoryTrain:  epoch 15, batch     5 | loss: 1.6333377MemoryTrain:  epoch 15, batch     6 | loss: 1.4020822MemoryTrain:  epoch 15, batch     7 | loss: 3.4432139MemoryTrain:  epoch 15, batch     8 | loss: 1.4196022MemoryTrain:  epoch 15, batch     9 | loss: 3.8346260MemoryTrain:  epoch 15, batch    10 | loss: 1.4351608MemoryTrain:  epoch  3, batch    11 | loss: 9.8520708MemoryTrain:  epoch 15, batch     0 | loss: 5.0829940MemoryTrain:  epoch 15, batch     1 | loss: 1.6663117MemoryTrain:  epoch 15, batch     2 | loss: 2.5959407MemoryTrain:  epoch 15, batch     3 | loss: 4.7182009MemoryTrain:  epoch 15, batch     4 | loss: 1.7328503MemoryTrain:  epoch 15, batch     5 | loss: 2.3901785MemoryTrain:  epoch 15, batch     6 | loss: 3.5480030MemoryTrain:  epoch 15, batch     7 | loss: 4.2360159MemoryTrain:  epoch 15, batch     8 | loss: 7.4971906MemoryTrain:  epoch 15, batch     9 | loss: 2.5042380MemoryTrain:  epoch 15, batch    10 | loss: 2.6190981MemoryTrain:  epoch  3, batch    11 | loss: 9.8721098MemoryTrain:  epoch 15, batch     0 | loss: 2.1464018MemoryTrain:  epoch 15, batch     1 | loss: 1.6703361MemoryTrain:  epoch 15, batch     2 | loss: 1.6556002MemoryTrain:  epoch 15, batch     3 | loss: 2.4218516MemoryTrain:  epoch 15, batch     4 | loss: 1.4164143MemoryTrain:  epoch 15, batch     5 | loss: 1.4668475MemoryTrain:  epoch 15, batch     6 | loss: 2.6792653MemoryTrain:  epoch 15, batch     7 | loss: 1.7590011MemoryTrain:  epoch 15, batch     8 | loss: 1.6479243MemoryTrain:  epoch 15, batch     9 | loss: 2.4630137MemoryTrain:  epoch 15, batch    10 | loss: 3.6969447MemoryTrain:  epoch  3, batch    11 | loss: 10.0947786MemoryTrain:  epoch 15, batch     0 | loss: 1.6293001MemoryTrain:  epoch 15, batch     1 | loss: 1.4073487MemoryTrain:  epoch 15, batch     2 | loss: 1.6162620MemoryTrain:  epoch 15, batch     3 | loss: 1.5903443MemoryTrain:  epoch 15, batch     4 | loss: 2.5498919MemoryTrain:  epoch 15, batch     5 | loss: 3.5338056MemoryTrain:  epoch 15, batch     6 | loss: 2.4835809MemoryTrain:  epoch 15, batch     7 | loss: 1.7745180MemoryTrain:  epoch 15, batch     8 | loss: 2.4435475MemoryTrain:  epoch 15, batch     9 | loss: 2.4567159MemoryTrain:  epoch 15, batch    10 | loss: 2.8549955MemoryTrain:  epoch  3, batch    11 | loss: 10.2968131
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 29.17%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 28.12%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 32.50%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 33.33%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 41.07%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 47.66%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 52.08%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 54.37%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 56.82%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 60.42%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 62.05%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 64.06%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 65.07%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 65.28%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 66.12%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 65.94%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 66.67%   [EVAL] batch:   21 | acc: 56.25%,  total acc: 66.19%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 66.03%   [EVAL] batch:   23 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 65.75%   [EVAL] batch:   25 | acc: 43.75%,  total acc: 64.90%   [EVAL] batch:   26 | acc: 50.00%,  total acc: 64.35%   [EVAL] batch:   27 | acc: 75.00%,  total acc: 64.73%   [EVAL] batch:   28 | acc: 37.50%,  total acc: 63.79%   [EVAL] batch:   29 | acc: 37.50%,  total acc: 62.92%   [EVAL] batch:   30 | acc: 31.25%,  total acc: 61.90%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 61.72%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 61.36%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 61.21%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 60.71%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 60.94%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 60.98%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 61.18%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 61.38%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 61.88%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 62.04%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 62.35%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 62.94%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 63.21%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 62.78%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 62.23%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 61.84%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 61.98%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 61.35%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 61.38%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 62.13%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 62.86%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 63.56%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 64.24%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 64.89%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 65.51%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 65.57%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 65.30%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 65.25%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 65.21%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 65.06%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 65.02%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 64.48%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 73.61%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 72.50%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 70.45%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 69.23%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 70.54%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 72.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 73.83%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 74.63%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 75.69%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 76.64%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 75.94%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 76.19%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 75.85%   [EVAL] batch:   22 | acc: 43.75%,  total acc: 74.46%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 74.74%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.96%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 76.85%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.23%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.64%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 79.88%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 80.11%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 79.41%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 79.11%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 78.82%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 78.55%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 78.78%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 79.33%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.84%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 80.34%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 80.65%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 80.96%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 81.11%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 80.84%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 80.72%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 80.60%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 80.87%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 80.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 80.88%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 81.13%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 81.37%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 81.37%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 81.47%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 81.36%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 80.71%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 80.19%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 80.10%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 79.82%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 79.33%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 79.07%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 78.61%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 77.98%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 77.84%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 77.33%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 77.02%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 76.81%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 76.70%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 76.41%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 76.30%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 76.20%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 76.27%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 76.17%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 75.66%   [EVAL] batch:   76 | acc: 31.25%,  total acc: 75.08%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 74.60%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 74.21%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 73.91%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 73.30%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 73.40%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 73.72%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 74.03%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 74.26%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 74.56%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 74.78%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 74.57%   [EVAL] batch:   88 | acc: 6.25%,  total acc: 73.81%   [EVAL] batch:   89 | acc: 6.25%,  total acc: 73.06%   [EVAL] batch:   90 | acc: 6.25%,  total acc: 72.32%   [EVAL] batch:   91 | acc: 31.25%,  total acc: 71.88%   [EVAL] batch:   92 | acc: 0.00%,  total acc: 71.10%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 70.61%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 70.86%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 71.09%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 71.33%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 71.49%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 71.72%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 72.00%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 71.78%   [EVAL] batch:  101 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 71.60%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 71.39%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 71.31%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 71.29%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 71.20%   [EVAL] batch:  107 | acc: 75.00%,  total acc: 71.24%   [EVAL] batch:  108 | acc: 68.75%,  total acc: 71.22%   [EVAL] batch:  109 | acc: 62.50%,  total acc: 71.14%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 71.06%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 70.98%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 70.96%   [EVAL] batch:  113 | acc: 37.50%,  total acc: 70.67%   [EVAL] batch:  114 | acc: 43.75%,  total acc: 70.43%   [EVAL] batch:  115 | acc: 37.50%,  total acc: 70.15%   [EVAL] batch:  116 | acc: 25.00%,  total acc: 69.76%   [EVAL] batch:  117 | acc: 37.50%,  total acc: 69.49%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 69.38%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 69.64%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 69.83%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 70.03%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 70.22%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 70.31%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 70.50%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 70.19%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 69.83%   [EVAL] batch:  127 | acc: 18.75%,  total acc: 69.43%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 69.14%   [EVAL] batch:  129 | acc: 18.75%,  total acc: 68.75%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 68.42%   [EVAL] batch:  131 | acc: 56.25%,  total acc: 68.32%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 68.56%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 68.98%   [EVAL] batch:  135 | acc: 75.00%,  total acc: 69.03%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 69.21%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 69.25%   [EVAL] batch:  138 | acc: 31.25%,  total acc: 68.97%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 68.75%   [EVAL] batch:  140 | acc: 56.25%,  total acc: 68.66%   [EVAL] batch:  141 | acc: 50.00%,  total acc: 68.53%   [EVAL] batch:  142 | acc: 56.25%,  total acc: 68.44%   [EVAL] batch:  143 | acc: 62.50%,  total acc: 68.40%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 68.62%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 68.84%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 69.01%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 69.21%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 69.42%   [EVAL] batch:  149 | acc: 87.50%,  total acc: 69.54%   [EVAL] batch:  150 | acc: 87.50%,  total acc: 69.66%   [EVAL] batch:  151 | acc: 87.50%,  total acc: 69.78%   [EVAL] batch:  152 | acc: 68.75%,  total acc: 69.77%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 69.89%   [EVAL] batch:  154 | acc: 75.00%,  total acc: 69.92%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 70.03%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 70.18%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 70.37%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 70.52%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 70.66%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 70.81%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 70.99%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 70.90%   [EVAL] batch:  163 | acc: 25.00%,  total acc: 70.62%   [EVAL] batch:  164 | acc: 25.00%,  total acc: 70.34%   [EVAL] batch:  165 | acc: 25.00%,  total acc: 70.07%   [EVAL] batch:  166 | acc: 12.50%,  total acc: 69.72%   [EVAL] batch:  167 | acc: 25.00%,  total acc: 69.46%   [EVAL] batch:  168 | acc: 31.25%,  total acc: 69.23%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 69.41%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 69.55%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 69.73%   [EVAL] batch:  172 | acc: 93.75%,  total acc: 69.87%   [EVAL] batch:  173 | acc: 100.00%,  total acc: 70.04%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 70.18%   [EVAL] batch:  175 | acc: 43.75%,  total acc: 70.03%   [EVAL] batch:  176 | acc: 37.50%,  total acc: 69.84%   [EVAL] batch:  177 | acc: 31.25%,  total acc: 69.63%   [EVAL] batch:  178 | acc: 37.50%,  total acc: 69.45%   [EVAL] batch:  179 | acc: 31.25%,  total acc: 69.24%   [EVAL] batch:  180 | acc: 18.75%,  total acc: 68.96%   [EVAL] batch:  181 | acc: 31.25%,  total acc: 68.75%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 68.65%   [EVAL] batch:  183 | acc: 43.75%,  total acc: 68.51%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 68.51%   [EVAL] batch:  185 | acc: 62.50%,  total acc: 68.48%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 68.48%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 68.45%   [EVAL] batch:  188 | acc: 75.00%,  total acc: 68.49%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 68.49%   [EVAL] batch:  190 | acc: 93.75%,  total acc: 68.62%   [EVAL] batch:  191 | acc: 68.75%,  total acc: 68.62%   [EVAL] batch:  192 | acc: 81.25%,  total acc: 68.69%   [EVAL] batch:  193 | acc: 62.50%,  total acc: 68.65%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 68.65%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 68.62%   [EVAL] batch:  196 | acc: 50.00%,  total acc: 68.53%   [EVAL] batch:  197 | acc: 68.75%,  total acc: 68.53%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 68.53%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 68.56%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 68.59%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 68.69%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:  203 | acc: 100.00%,  total acc: 68.90%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 68.96%   [EVAL] batch:  205 | acc: 100.00%,  total acc: 69.11%   [EVAL] batch:  206 | acc: 81.25%,  total acc: 69.17%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 68.96%   [EVAL] batch:  208 | acc: 56.25%,  total acc: 68.90%   [EVAL] batch:  209 | acc: 43.75%,  total acc: 68.78%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 68.63%   [EVAL] batch:  211 | acc: 43.75%,  total acc: 68.51%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 68.52%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 68.63%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 68.78%   [EVAL] batch:  215 | acc: 81.25%,  total acc: 68.84%   [EVAL] batch:  216 | acc: 93.75%,  total acc: 68.95%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 69.07%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 69.18%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 69.32%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 69.46%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 69.59%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 69.73%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 69.87%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 70.13%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 70.18%   [EVAL] batch:  227 | acc: 87.50%,  total acc: 70.26%   [EVAL] batch:  228 | acc: 87.50%,  total acc: 70.33%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 70.46%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 70.56%   [EVAL] batch:  231 | acc: 50.00%,  total acc: 70.47%   [EVAL] batch:  232 | acc: 25.00%,  total acc: 70.28%   [EVAL] batch:  233 | acc: 56.25%,  total acc: 70.22%   [EVAL] batch:  234 | acc: 18.75%,  total acc: 70.00%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 69.94%   [EVAL] batch:  236 | acc: 18.75%,  total acc: 69.73%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 69.72%   [EVAL] batch:  238 | acc: 93.75%,  total acc: 69.82%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 69.95%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 70.02%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 70.24%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 70.29%   [EVAL] batch:  244 | acc: 12.50%,  total acc: 70.05%   [EVAL] batch:  245 | acc: 25.00%,  total acc: 69.87%   [EVAL] batch:  246 | acc: 18.75%,  total acc: 69.66%   [EVAL] batch:  247 | acc: 18.75%,  total acc: 69.46%   [EVAL] batch:  248 | acc: 6.25%,  total acc: 69.20%   [EVAL] batch:  249 | acc: 18.75%,  total acc: 69.00%   [EVAL] batch:  250 | acc: 100.00%,  total acc: 69.12%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 69.22%   [EVAL] batch:  252 | acc: 100.00%,  total acc: 69.34%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 69.41%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 69.51%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 69.63%   [EVAL] batch:  256 | acc: 93.75%,  total acc: 69.72%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 69.79%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 69.81%   [EVAL] batch:  259 | acc: 87.50%,  total acc: 69.88%   [EVAL] batch:  260 | acc: 75.00%,  total acc: 69.90%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 69.92%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 70.01%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 70.03%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 70.05%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 70.00%   [EVAL] batch:  266 | acc: 50.00%,  total acc: 69.92%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 69.92%   [EVAL] batch:  268 | acc: 56.25%,  total acc: 69.87%   [EVAL] batch:  269 | acc: 56.25%,  total acc: 69.81%   [EVAL] batch:  270 | acc: 56.25%,  total acc: 69.76%   [EVAL] batch:  271 | acc: 50.00%,  total acc: 69.69%   [EVAL] batch:  272 | acc: 68.75%,  total acc: 69.69%   [EVAL] batch:  273 | acc: 68.75%,  total acc: 69.69%   [EVAL] batch:  274 | acc: 31.25%,  total acc: 69.55%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 69.66%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 69.77%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 69.87%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 69.98%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 70.09%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 70.20%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 70.26%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 70.34%   [EVAL] batch:  283 | acc: 75.00%,  total acc: 70.36%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 70.46%   [EVAL] batch:  285 | acc: 75.00%,  total acc: 70.48%   [EVAL] batch:  286 | acc: 81.25%,  total acc: 70.51%   [EVAL] batch:  287 | acc: 81.25%,  total acc: 70.55%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 70.65%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 70.75%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 70.85%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 70.95%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 71.05%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 71.13%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 71.23%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 71.33%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 71.42%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 71.52%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 71.61%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 71.71%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 71.78%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 71.85%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 71.93%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 72.00%   [EVAL] batch:  304 | acc: 87.50%,  total acc: 72.05%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 72.14%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 72.19%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 72.24%   [EVAL] batch:  308 | acc: 68.75%,  total acc: 72.23%   [EVAL] batch:  309 | acc: 81.25%,  total acc: 72.26%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 72.31%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 72.34%   [EVAL] batch:  312 | acc: 56.25%,  total acc: 72.28%   [EVAL] batch:  313 | acc: 37.50%,  total acc: 72.17%   [EVAL] batch:  314 | acc: 25.00%,  total acc: 72.02%   [EVAL] batch:  315 | acc: 31.25%,  total acc: 71.89%   [EVAL] batch:  316 | acc: 37.50%,  total acc: 71.79%   [EVAL] batch:  317 | acc: 43.75%,  total acc: 71.70%   [EVAL] batch:  318 | acc: 56.25%,  total acc: 71.65%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 71.81%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 71.82%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 71.87%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 71.89%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 71.96%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 71.97%   [EVAL] batch:  326 | acc: 62.50%,  total acc: 71.94%   [EVAL] batch:  327 | acc: 81.25%,  total acc: 71.97%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 71.98%   [EVAL] batch:  329 | acc: 81.25%,  total acc: 72.01%   [EVAL] batch:  330 | acc: 75.00%,  total acc: 72.02%   [EVAL] batch:  331 | acc: 75.00%,  total acc: 72.03%   [EVAL] batch:  332 | acc: 62.50%,  total acc: 72.00%   [EVAL] batch:  333 | acc: 75.00%,  total acc: 72.01%   [EVAL] batch:  334 | acc: 56.25%,  total acc: 71.96%   [EVAL] batch:  335 | acc: 62.50%,  total acc: 71.93%   [EVAL] batch:  336 | acc: 56.25%,  total acc: 71.88%   [EVAL] batch:  337 | acc: 50.00%,  total acc: 71.82%   [EVAL] batch:  338 | acc: 50.00%,  total acc: 71.76%   [EVAL] batch:  339 | acc: 62.50%,  total acc: 71.73%   [EVAL] batch:  340 | acc: 68.75%,  total acc: 71.72%   [EVAL] batch:  341 | acc: 31.25%,  total acc: 71.60%   [EVAL] batch:  342 | acc: 31.25%,  total acc: 71.48%   [EVAL] batch:  343 | acc: 37.50%,  total acc: 71.38%   [EVAL] batch:  344 | acc: 62.50%,  total acc: 71.36%   [EVAL] batch:  345 | acc: 43.75%,  total acc: 71.28%   [EVAL] batch:  346 | acc: 62.50%,  total acc: 71.25%   [EVAL] batch:  347 | acc: 43.75%,  total acc: 71.17%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 71.19%   [EVAL] batch:  349 | acc: 68.75%,  total acc: 71.18%   [EVAL] batch:  350 | acc: 62.50%,  total acc: 71.15%   [EVAL] batch:  351 | acc: 75.00%,  total acc: 71.16%   [EVAL] batch:  352 | acc: 75.00%,  total acc: 71.18%   [EVAL] batch:  353 | acc: 75.00%,  total acc: 71.19%   [EVAL] batch:  354 | acc: 75.00%,  total acc: 71.20%   [EVAL] batch:  355 | acc: 93.75%,  total acc: 71.26%   [EVAL] batch:  356 | acc: 62.50%,  total acc: 71.24%   [EVAL] batch:  357 | acc: 31.25%,  total acc: 71.12%   [EVAL] batch:  358 | acc: 43.75%,  total acc: 71.05%   [EVAL] batch:  359 | acc: 56.25%,  total acc: 71.01%   [EVAL] batch:  360 | acc: 43.75%,  total acc: 70.93%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 70.87%   [EVAL] batch:  362 | acc: 81.25%,  total acc: 70.90%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 70.98%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 71.06%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 71.14%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 71.22%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 71.30%   [EVAL] batch:  368 | acc: 87.50%,  total acc: 71.34%   [EVAL] batch:  369 | acc: 62.50%,  total acc: 71.32%   [EVAL] batch:  370 | acc: 50.00%,  total acc: 71.26%   [EVAL] batch:  371 | acc: 62.50%,  total acc: 71.24%   [EVAL] batch:  372 | acc: 56.25%,  total acc: 71.20%   [EVAL] batch:  373 | acc: 62.50%,  total acc: 71.17%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 71.15%   
cur_acc:  ['0.9514', '0.7708', '0.7569', '0.7530', '0.8581', '0.6448']
his_acc:  ['0.9514', '0.8490', '0.8009', '0.7348', '0.7370', '0.7115']
CurrentTrain: epoch 15, batch     0 | loss: 13.6075431CurrentTrain: epoch 15, batch     1 | loss: 14.1941893CurrentTrain: epoch 15, batch     2 | loss: 16.8941147CurrentTrain: epoch  1, batch     3 | loss: 11.2981935CurrentTrain: epoch 15, batch     0 | loss: 15.1302484CurrentTrain: epoch 15, batch     1 | loss: 15.1408755CurrentTrain: epoch 15, batch     2 | loss: 9.6976744CurrentTrain: epoch  1, batch     3 | loss: 7.2674829CurrentTrain: epoch 15, batch     0 | loss: 12.6393784CurrentTrain: epoch 15, batch     1 | loss: 14.1397575CurrentTrain: epoch 15, batch     2 | loss: 12.2226645CurrentTrain: epoch  1, batch     3 | loss: 7.2485293CurrentTrain: epoch 15, batch     0 | loss: 9.1788783CurrentTrain: epoch 15, batch     1 | loss: 7.8002792CurrentTrain: epoch 15, batch     2 | loss: 9.9219329CurrentTrain: epoch  1, batch     3 | loss: 6.4154790CurrentTrain: epoch 15, batch     0 | loss: 9.0889894CurrentTrain: epoch 15, batch     1 | loss: 8.6870266CurrentTrain: epoch 15, batch     2 | loss: 6.5932289CurrentTrain: epoch  1, batch     3 | loss: 6.2982524CurrentTrain: epoch 15, batch     0 | loss: 10.8994742CurrentTrain: epoch 15, batch     1 | loss: 8.3581104CurrentTrain: epoch 15, batch     2 | loss: 8.7568368CurrentTrain: epoch  1, batch     3 | loss: 6.2564258CurrentTrain: epoch 15, batch     0 | loss: 7.5672276CurrentTrain: epoch 15, batch     1 | loss: 6.9141539CurrentTrain: epoch 15, batch     2 | loss: 6.1476229CurrentTrain: epoch  1, batch     3 | loss: 5.5661000CurrentTrain: epoch 15, batch     0 | loss: 11.7455381CurrentTrain: epoch 15, batch     1 | loss: 8.9798923CurrentTrain: epoch 15, batch     2 | loss: 8.2916046CurrentTrain: epoch  1, batch     3 | loss: 5.9158972CurrentTrain: epoch 15, batch     0 | loss: 8.1392170CurrentTrain: epoch 15, batch     1 | loss: 9.0402807CurrentTrain: epoch 15, batch     2 | loss: 7.7632411CurrentTrain: epoch  1, batch     3 | loss: 5.2779980CurrentTrain: epoch 15, batch     0 | loss: 8.1789932CurrentTrain: epoch 15, batch     1 | loss: 8.2604018CurrentTrain: epoch 15, batch     2 | loss: 4.5988594CurrentTrain: epoch  1, batch     3 | loss: 12.5128200
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the cantons of créteil are administrative divisions of the val - de - marne department , île - de - france region , northern france .
Head Entity: île - de - france
Tail Entity: val - de - marne
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The provinces of Canada are the primary administrative divisions of the country, with Ontario being one of the most populous provinces.  
Head Entity: Canada  
Tail Entity: Ontario  

Relation: contains administrative territorial entity  
Context: The states of Australia are the major political divisions, with New South Wales being the most populous state in the country.  
Head Entity: Australia  
Tail Entity: New South Wales  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: heinz , is a german given name , a diminutive of heinrich and cognate of the given name henry .
Head Entity: heinrich
Tail Entity: henry
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as they share a common ancestor and many genetic traits.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: In some cultures, the terms "soda" and "pop" are said to be the same as they refer to the same type of carbonated beverage, though preferences vary by region.  
Head Entity: soda  
Tail Entity: pop  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: in 2010 , ty burrell received a nomination for the television critics association award for individual achievement in comedy , winning in 2011 .
Head Entity: television critics association award for individual achievement in comedy
Tail Entity: ty burrell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: In 2022, the Boston Red Sox triumphed in the World Series, marking their fourth championship in the last two decades.  
Head Entity: World Series  
Tail Entity: Boston Red Sox  

Relation: winner  
Context: At the annual film festival, the documentary "Voices of Change" was awarded the Best Documentary prize, highlighting its impactful storytelling.  
Head Entity: Best Documentary prize  
Tail Entity: "Voices of Change"  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: the railroad car they were deported in was attached to the end of the last train out of drancy which also carried drancy commandant ss hauptsturmführer alois brunner and other german military personnel .
Head Entity: alois brunner
Tail Entity: hauptsturmführer
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: General John Smith was awarded the title of Major General for his exceptional leadership during the conflict, making him one of the highest-ranking officers in the army.  
Head Entity: John Smith  
Tail Entity: Major General  

Relation: military rank  
Context: During the ceremony, Colonel Jane Doe was recognized for her bravery and promoted to the rank of Brigadier General, a significant achievement in her military career.  
Head Entity: Jane Doe  
Tail Entity: Brigadier General  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: tokyopop also released a collection of karin short stories under the title : " chibi vampire : airmail " on august 31 , 2010 .
Head Entity: karin
Tail Entity: tokyopop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: Penguin Random House announced the release of a new novel by the acclaimed author John Grisham, set to hit the shelves on June 15, 2021.  
Head Entity: John Grisham  
Tail Entity: Penguin Random House  

Relation: publisher  
Context: The popular science magazine, Scientific American, published an article discussing the latest advancements in renewable energy technologies.  
Head Entity: Scientific American  
Tail Entity: renewable energy technologies  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: following this song were the # 21 " she wants to be wanted again " , the # 2 " loved too much " and the # 17 " i have to surrender " .
Head Entity: she wants to be wanted again
Tail Entity: loved too much
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The top three movies of the year were "Inception," followed by "The Social Network" and "Toy Story 3."  
Head Entity: Inception  
Tail Entity: The Social Network  

Relation: followed by  
Context: The conference schedule included a keynote speech, followed by a panel discussion and then a networking session.  
Head Entity: keynote speech  
Tail Entity: panel discussion  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: he was also associated with robert wilkinson in producing " londina illustrata " , an illustrated account of ancient buildings in london and westminster in two volumes ( 1819–25 ) .
Head Entity: robert wilkinson
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: The headquarters of the company is situated in the bustling city of San Francisco, where it has been operating since its inception in 2010.  
Head Entity: the company  
Tail Entity: San Francisco  

Relation: work location  
Context: During her tenure at the university, she conducted groundbreaking research in neuroscience, primarily based in the labs at Cambridge.  
Head Entity: she  
Tail Entity: Cambridge  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in enzyme catalysis.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a former professional athlete, now works as a sports commentator for major networks.  
Head Entity: john smith  
Tail Entity: sports commentator  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan was designated as a UNESCO World Heritage Site for its unique rock-cut architecture and water conduit system.  
Head Entity: ancient city of Petra  
Tail Entity: UNESCO World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: ada adini ( 1855 – february 1924 ) was an american operatic soprano who had an active international career from 1876 up into the first decade of the 20th century .
Head Entity: ada adini
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti ( october 12, 1935 – september 6, 2007 ) was an italian operatic tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey ( born march 27, 1969 ) is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: singer  
MemoryTrain:  epoch 15, batch     0 | loss: 3.4934256MemoryTrain:  epoch 15, batch     1 | loss: 4.2156810MemoryTrain:  epoch 15, batch     2 | loss: 2.3435280MemoryTrain:  epoch 15, batch     3 | loss: 2.7692891MemoryTrain:  epoch 15, batch     4 | loss: 3.6370338MemoryTrain:  epoch 15, batch     5 | loss: 6.6870639MemoryTrain:  epoch 15, batch     6 | loss: 3.2826442MemoryTrain:  epoch 15, batch     7 | loss: 4.4752921MemoryTrain:  epoch 15, batch     8 | loss: 3.1393347MemoryTrain:  epoch 15, batch     9 | loss: 3.2681895MemoryTrain:  epoch 15, batch    10 | loss: 2.1583845MemoryTrain:  epoch 15, batch    11 | loss: 3.7201379MemoryTrain:  epoch 15, batch    12 | loss: 2.6550841MemoryTrain:  epoch  1, batch    13 | loss: 9.1152577MemoryTrain:  epoch 15, batch     0 | loss: 1.9822461MemoryTrain:  epoch 15, batch     1 | loss: 4.1796269MemoryTrain:  epoch 15, batch     2 | loss: 4.5511043MemoryTrain:  epoch 15, batch     3 | loss: 2.2945883MemoryTrain:  epoch 15, batch     4 | loss: 2.7230862MemoryTrain:  epoch 15, batch     5 | loss: 2.5670998MemoryTrain:  epoch 15, batch     6 | loss: 3.3740635MemoryTrain:  epoch 15, batch     7 | loss: 2.8747905MemoryTrain:  epoch 15, batch     8 | loss: 2.5964387MemoryTrain:  epoch 15, batch     9 | loss: 2.2494449MemoryTrain:  epoch 15, batch    10 | loss: 3.3269154MemoryTrain:  epoch 15, batch    11 | loss: 2.0677225MemoryTrain:  epoch 15, batch    12 | loss: 2.2805396MemoryTrain:  epoch  1, batch    13 | loss: 10.2217080MemoryTrain:  epoch 15, batch     0 | loss: 2.3260874MemoryTrain:  epoch 15, batch     1 | loss: 2.1570115MemoryTrain:  epoch 15, batch     2 | loss: 1.8359189MemoryTrain:  epoch 15, batch     3 | loss: 1.7924850MemoryTrain:  epoch 15, batch     4 | loss: 1.8821229MemoryTrain:  epoch 15, batch     5 | loss: 4.7177824MemoryTrain:  epoch 15, batch     6 | loss: 1.9469266MemoryTrain:  epoch 15, batch     7 | loss: 2.2683913MemoryTrain:  epoch 15, batch     8 | loss: 1.9870466MemoryTrain:  epoch 15, batch     9 | loss: 1.9330010MemoryTrain:  epoch 15, batch    10 | loss: 2.8237611MemoryTrain:  epoch 15, batch    11 | loss: 5.0666874MemoryTrain:  epoch 15, batch    12 | loss: 4.1983046MemoryTrain:  epoch  1, batch    13 | loss: 5.6648114MemoryTrain:  epoch 15, batch     0 | loss: 2.1246810MemoryTrain:  epoch 15, batch     1 | loss: 1.8950828MemoryTrain:  epoch 15, batch     2 | loss: 1.8806314MemoryTrain:  epoch 15, batch     3 | loss: 3.8164913MemoryTrain:  epoch 15, batch     4 | loss: 1.6591503MemoryTrain:  epoch 15, batch     5 | loss: 2.7703042MemoryTrain:  epoch 15, batch     6 | loss: 1.7388650MemoryTrain:  epoch 15, batch     7 | loss: 1.9773337MemoryTrain:  epoch 15, batch     8 | loss: 2.5554684MemoryTrain:  epoch 15, batch     9 | loss: 1.8908960MemoryTrain:  epoch 15, batch    10 | loss: 2.1374943MemoryTrain:  epoch 15, batch    11 | loss: 3.8751113MemoryTrain:  epoch 15, batch    12 | loss: 1.9578913MemoryTrain:  epoch  1, batch    13 | loss: 8.0628001MemoryTrain:  epoch 15, batch     0 | loss: 1.9559398MemoryTrain:  epoch 15, batch     1 | loss: 1.5508777MemoryTrain:  epoch 15, batch     2 | loss: 1.5836767MemoryTrain:  epoch 15, batch     3 | loss: 1.9663612MemoryTrain:  epoch 15, batch     4 | loss: 1.7330744MemoryTrain:  epoch 15, batch     5 | loss: 4.7146498MemoryTrain:  epoch 15, batch     6 | loss: 1.4703151MemoryTrain:  epoch 15, batch     7 | loss: 1.6032699MemoryTrain:  epoch 15, batch     8 | loss: 2.0960759MemoryTrain:  epoch 15, batch     9 | loss: 2.1453011MemoryTrain:  epoch 15, batch    10 | loss: 2.4860773MemoryTrain:  epoch 15, batch    11 | loss: 2.2451151MemoryTrain:  epoch 15, batch    12 | loss: 2.0139255MemoryTrain:  epoch  1, batch    13 | loss: 6.4826739MemoryTrain:  epoch 15, batch     0 | loss: 1.9207829MemoryTrain:  epoch 15, batch     1 | loss: 1.8454180MemoryTrain:  epoch 15, batch     2 | loss: 3.9910117MemoryTrain:  epoch 15, batch     3 | loss: 1.7321805MemoryTrain:  epoch 15, batch     4 | loss: 1.9718071MemoryTrain:  epoch 15, batch     5 | loss: 1.4851562MemoryTrain:  epoch 15, batch     6 | loss: 6.6586137MemoryTrain:  epoch 15, batch     7 | loss: 2.5605953MemoryTrain:  epoch 15, batch     8 | loss: 3.9449931MemoryTrain:  epoch 15, batch     9 | loss: 1.3109269MemoryTrain:  epoch 15, batch    10 | loss: 1.6295652MemoryTrain:  epoch 15, batch    11 | loss: 1.4903555MemoryTrain:  epoch 15, batch    12 | loss: 1.8170439MemoryTrain:  epoch  1, batch    13 | loss: 5.6432497MemoryTrain:  epoch 15, batch     0 | loss: 1.6184456MemoryTrain:  epoch 15, batch     1 | loss: 1.7052677MemoryTrain:  epoch 15, batch     2 | loss: 1.4147243MemoryTrain:  epoch 15, batch     3 | loss: 1.7511138MemoryTrain:  epoch 15, batch     4 | loss: 1.7171370MemoryTrain:  epoch 15, batch     5 | loss: 1.3904655MemoryTrain:  epoch 15, batch     6 | loss: 1.7981875MemoryTrain:  epoch 15, batch     7 | loss: 3.9617624MemoryTrain:  epoch 15, batch     8 | loss: 2.2037419MemoryTrain:  epoch 15, batch     9 | loss: 3.6086354MemoryTrain:  epoch 15, batch    10 | loss: 1.7092729MemoryTrain:  epoch 15, batch    11 | loss: 4.6259541MemoryTrain:  epoch 15, batch    12 | loss: 1.9373720MemoryTrain:  epoch  1, batch    13 | loss: 6.4348608MemoryTrain:  epoch 15, batch     0 | loss: 6.5167043MemoryTrain:  epoch 15, batch     1 | loss: 1.7320929MemoryTrain:  epoch 15, batch     2 | loss: 1.9577303MemoryTrain:  epoch 15, batch     3 | loss: 1.6426755MemoryTrain:  epoch 15, batch     4 | loss: 1.6430134MemoryTrain:  epoch 15, batch     5 | loss: 1.5613097MemoryTrain:  epoch 15, batch     6 | loss: 1.5353113MemoryTrain:  epoch 15, batch     7 | loss: 1.7578020MemoryTrain:  epoch 15, batch     8 | loss: 1.4956758MemoryTrain:  epoch 15, batch     9 | loss: 1.4957128MemoryTrain:  epoch 15, batch    10 | loss: 2.0185281MemoryTrain:  epoch 15, batch    11 | loss: 2.0854906MemoryTrain:  epoch 15, batch    12 | loss: 1.5721334MemoryTrain:  epoch  1, batch    13 | loss: 5.1563571MemoryTrain:  epoch 15, batch     0 | loss: 1.7678652MemoryTrain:  epoch 15, batch     1 | loss: 1.8883925MemoryTrain:  epoch 15, batch     2 | loss: 2.0514721MemoryTrain:  epoch 15, batch     3 | loss: 2.0764071MemoryTrain:  epoch 15, batch     4 | loss: 2.2004616MemoryTrain:  epoch 15, batch     5 | loss: 3.7386399MemoryTrain:  epoch 15, batch     6 | loss: 1.5971538MemoryTrain:  epoch 15, batch     7 | loss: 1.3888136MemoryTrain:  epoch 15, batch     8 | loss: 1.6317999MemoryTrain:  epoch 15, batch     9 | loss: 3.7706859MemoryTrain:  epoch 15, batch    10 | loss: 1.5744785MemoryTrain:  epoch 15, batch    11 | loss: 1.8429839MemoryTrain:  epoch 15, batch    12 | loss: 4.0012520MemoryTrain:  epoch  1, batch    13 | loss: 5.2637779MemoryTrain:  epoch 15, batch     0 | loss: 1.3945005MemoryTrain:  epoch 15, batch     1 | loss: 1.5953175MemoryTrain:  epoch 15, batch     2 | loss: 3.8894842MemoryTrain:  epoch 15, batch     3 | loss: 2.7023144MemoryTrain:  epoch 15, batch     4 | loss: 1.6562968MemoryTrain:  epoch 15, batch     5 | loss: 1.3766877MemoryTrain:  epoch 15, batch     6 | loss: 1.3791997MemoryTrain:  epoch 15, batch     7 | loss: 2.2927533MemoryTrain:  epoch 15, batch     8 | loss: 1.9318693MemoryTrain:  epoch 15, batch     9 | loss: 1.7011922MemoryTrain:  epoch 15, batch    10 | loss: 1.6294729MemoryTrain:  epoch 15, batch    11 | loss: 1.4041756MemoryTrain:  epoch 15, batch    12 | loss: 1.4778755MemoryTrain:  epoch  1, batch    13 | loss: 5.6697541
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 53.12%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 55.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 57.29%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 57.14%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 59.38%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 60.42%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 61.88%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 60.80%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 60.94%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 61.54%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 63.33%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 63.24%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 63.89%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 65.13%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 66.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 67.86%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 69.32%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 70.38%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 71.35%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:   25 | acc: 50.00%,  total acc: 71.63%   [EVAL] batch:   26 | acc: 31.25%,  total acc: 70.14%   [EVAL] batch:   27 | acc: 75.00%,  total acc: 70.31%   [EVAL] batch:   28 | acc: 68.75%,  total acc: 70.26%   [EVAL] batch:   29 | acc: 37.50%,  total acc: 69.17%   [EVAL] batch:   30 | acc: 43.75%,  total acc: 68.35%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 68.16%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 67.61%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 66.36%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 65.18%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 63.72%   [EVAL] batch:   36 | acc: 25.00%,  total acc: 62.67%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 62.98%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 63.44%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 63.57%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 64.29%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 64.53%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 64.91%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 65.28%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 65.49%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 65.56%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 66.07%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 66.50%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 67.16%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 67.79%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 68.40%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 68.98%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 69.55%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 70.09%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 70.39%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 70.69%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 70.97%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 71.35%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 71.52%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 71.53%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 68.75%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 68.18%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 65.10%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 65.38%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 66.52%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 67.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 69.92%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 71.32%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 72.57%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 73.68%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 72.81%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 72.62%   [EVAL] batch:   21 | acc: 56.25%,  total acc: 71.88%   [EVAL] batch:   22 | acc: 43.75%,  total acc: 70.65%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 71.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.36%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 73.38%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 74.33%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 75.83%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 76.61%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 76.95%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 77.46%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 77.02%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 76.79%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 76.52%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 76.64%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 77.66%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 78.20%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 78.42%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 78.78%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 78.98%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 79.31%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 78.94%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 78.59%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 78.52%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 78.57%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 78.25%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 78.43%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 78.61%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 78.89%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 78.82%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 78.52%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 78.57%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 78.40%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 77.80%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 77.22%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 77.19%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 76.95%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 76.51%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 76.19%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 75.68%   [EVAL] batch:   64 | acc: 43.75%,  total acc: 75.19%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 74.63%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 74.36%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 74.37%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 74.38%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 74.12%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 73.87%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 73.89%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 73.73%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 73.58%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 73.11%   [EVAL] batch:   76 | acc: 31.25%,  total acc: 72.56%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 72.12%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 71.68%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 71.41%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 70.76%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 70.81%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 71.16%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 71.50%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 71.76%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 72.09%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 72.34%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 72.16%   [EVAL] batch:   88 | acc: 6.25%,  total acc: 71.42%   [EVAL] batch:   89 | acc: 6.25%,  total acc: 70.69%   [EVAL] batch:   90 | acc: 6.25%,  total acc: 69.99%   [EVAL] batch:   91 | acc: 18.75%,  total acc: 69.43%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 68.75%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 68.42%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 68.68%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 68.95%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 69.20%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 69.39%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 69.70%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 69.80%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 69.85%   [EVAL] batch:  102 | acc: 37.50%,  total acc: 69.54%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 69.41%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 69.46%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 69.46%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 69.45%   [EVAL] batch:  107 | acc: 75.00%,  total acc: 69.50%   [EVAL] batch:  108 | acc: 68.75%,  total acc: 69.50%   [EVAL] batch:  109 | acc: 62.50%,  total acc: 69.43%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 69.31%   [EVAL] batch:  111 | acc: 56.25%,  total acc: 69.20%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 69.14%   [EVAL] batch:  113 | acc: 25.00%,  total acc: 68.75%   [EVAL] batch:  114 | acc: 31.25%,  total acc: 68.42%   [EVAL] batch:  115 | acc: 31.25%,  total acc: 68.10%   [EVAL] batch:  116 | acc: 18.75%,  total acc: 67.68%   [EVAL] batch:  117 | acc: 31.25%,  total acc: 67.37%   [EVAL] batch:  118 | acc: 43.75%,  total acc: 67.17%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 67.29%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 67.51%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 67.73%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 67.89%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 67.99%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 68.20%   [EVAL] batch:  125 | acc: 18.75%,  total acc: 67.81%   [EVAL] batch:  126 | acc: 18.75%,  total acc: 67.42%   [EVAL] batch:  127 | acc: 12.50%,  total acc: 66.99%   [EVAL] batch:  128 | acc: 6.25%,  total acc: 66.52%   [EVAL] batch:  129 | acc: 18.75%,  total acc: 66.15%   [EVAL] batch:  130 | acc: 12.50%,  total acc: 65.74%   [EVAL] batch:  131 | acc: 43.75%,  total acc: 65.58%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 65.70%   [EVAL] batch:  133 | acc: 68.75%,  total acc: 65.72%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 65.88%   [EVAL] batch:  135 | acc: 75.00%,  total acc: 65.95%   [EVAL] batch:  136 | acc: 75.00%,  total acc: 66.01%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 65.99%   [EVAL] batch:  138 | acc: 37.50%,  total acc: 65.78%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 65.58%   [EVAL] batch:  140 | acc: 50.00%,  total acc: 65.47%   [EVAL] batch:  141 | acc: 31.25%,  total acc: 65.23%   [EVAL] batch:  142 | acc: 50.00%,  total acc: 65.12%   [EVAL] batch:  143 | acc: 56.25%,  total acc: 65.06%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 65.30%   [EVAL] batch:  145 | acc: 93.75%,  total acc: 65.50%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 65.69%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 65.92%   [EVAL] batch:  148 | acc: 93.75%,  total acc: 66.11%   [EVAL] batch:  149 | acc: 81.25%,  total acc: 66.21%   [EVAL] batch:  150 | acc: 87.50%,  total acc: 66.35%   [EVAL] batch:  151 | acc: 87.50%,  total acc: 66.49%   [EVAL] batch:  152 | acc: 68.75%,  total acc: 66.50%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 66.64%   [EVAL] batch:  154 | acc: 75.00%,  total acc: 66.69%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 66.83%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 67.00%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 67.21%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 67.37%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 67.54%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 67.70%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 67.90%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 67.87%   [EVAL] batch:  163 | acc: 31.25%,  total acc: 67.64%   [EVAL] batch:  164 | acc: 37.50%,  total acc: 67.46%   [EVAL] batch:  165 | acc: 31.25%,  total acc: 67.24%   [EVAL] batch:  166 | acc: 31.25%,  total acc: 67.03%   [EVAL] batch:  167 | acc: 25.00%,  total acc: 66.78%   [EVAL] batch:  168 | acc: 31.25%,  total acc: 66.57%   [EVAL] batch:  169 | acc: 87.50%,  total acc: 66.69%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 66.81%   [EVAL] batch:  171 | acc: 81.25%,  total acc: 66.90%   [EVAL] batch:  172 | acc: 93.75%,  total acc: 67.05%   [EVAL] batch:  173 | acc: 87.50%,  total acc: 67.17%   [EVAL] batch:  174 | acc: 75.00%,  total acc: 67.21%   [EVAL] batch:  175 | acc: 43.75%,  total acc: 67.08%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 66.95%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 66.85%   [EVAL] batch:  178 | acc: 43.75%,  total acc: 66.72%   [EVAL] batch:  179 | acc: 43.75%,  total acc: 66.60%   [EVAL] batch:  180 | acc: 25.00%,  total acc: 66.37%   [EVAL] batch:  181 | acc: 50.00%,  total acc: 66.28%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 66.19%   [EVAL] batch:  183 | acc: 43.75%,  total acc: 66.07%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 66.08%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 66.10%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 66.14%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 66.22%   [EVAL] batch:  188 | acc: 56.25%,  total acc: 66.17%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 66.22%   [EVAL] batch:  190 | acc: 75.00%,  total acc: 66.26%   [EVAL] batch:  191 | acc: 62.50%,  total acc: 66.24%   [EVAL] batch:  192 | acc: 75.00%,  total acc: 66.29%   [EVAL] batch:  193 | acc: 68.75%,  total acc: 66.30%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 66.35%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 66.36%   [EVAL] batch:  196 | acc: 50.00%,  total acc: 66.28%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 66.32%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 66.36%   [EVAL] batch:  199 | acc: 68.75%,  total acc: 66.38%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 66.39%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 66.49%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 66.53%   [EVAL] batch:  203 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:  204 | acc: 87.50%,  total acc: 66.77%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 66.90%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 66.76%   [EVAL] batch:  207 | acc: 12.50%,  total acc: 66.50%   [EVAL] batch:  208 | acc: 18.75%,  total acc: 66.27%   [EVAL] batch:  209 | acc: 18.75%,  total acc: 66.04%   [EVAL] batch:  210 | acc: 18.75%,  total acc: 65.82%   [EVAL] batch:  211 | acc: 25.00%,  total acc: 65.62%   [EVAL] batch:  212 | acc: 43.75%,  total acc: 65.52%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 65.65%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 65.81%   [EVAL] batch:  215 | acc: 68.75%,  total acc: 65.83%   [EVAL] batch:  216 | acc: 93.75%,  total acc: 65.96%   [EVAL] batch:  217 | acc: 87.50%,  total acc: 66.06%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 66.21%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 66.36%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 66.52%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 66.82%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 67.11%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 67.26%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 67.32%   [EVAL] batch:  227 | acc: 87.50%,  total acc: 67.41%   [EVAL] batch:  228 | acc: 93.75%,  total acc: 67.52%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 67.66%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 67.78%   [EVAL] batch:  231 | acc: 50.00%,  total acc: 67.70%   [EVAL] batch:  232 | acc: 25.00%,  total acc: 67.52%   [EVAL] batch:  233 | acc: 43.75%,  total acc: 67.41%   [EVAL] batch:  234 | acc: 31.25%,  total acc: 67.26%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 67.21%   [EVAL] batch:  236 | acc: 25.00%,  total acc: 67.04%   [EVAL] batch:  237 | acc: 56.25%,  total acc: 66.99%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 67.05%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 67.27%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 67.36%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 67.44%   [EVAL] batch:  243 | acc: 75.00%,  total acc: 67.47%   [EVAL] batch:  244 | acc: 6.25%,  total acc: 67.22%   [EVAL] batch:  245 | acc: 25.00%,  total acc: 67.05%   [EVAL] batch:  246 | acc: 18.75%,  total acc: 66.85%   [EVAL] batch:  247 | acc: 18.75%,  total acc: 66.66%   [EVAL] batch:  248 | acc: 6.25%,  total acc: 66.42%   [EVAL] batch:  249 | acc: 18.75%,  total acc: 66.22%   [EVAL] batch:  250 | acc: 100.00%,  total acc: 66.36%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 66.47%   [EVAL] batch:  252 | acc: 100.00%,  total acc: 66.60%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 66.68%   [EVAL] batch:  254 | acc: 100.00%,  total acc: 66.81%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 66.94%   [EVAL] batch:  256 | acc: 100.00%,  total acc: 67.07%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 67.15%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 67.16%   [EVAL] batch:  259 | acc: 87.50%,  total acc: 67.24%   [EVAL] batch:  260 | acc: 75.00%,  total acc: 67.27%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 67.32%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 67.42%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 67.45%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 67.48%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 67.43%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 67.42%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 67.40%   [EVAL] batch:  268 | acc: 56.25%,  total acc: 67.36%   [EVAL] batch:  269 | acc: 25.00%,  total acc: 67.20%   [EVAL] batch:  270 | acc: 31.25%,  total acc: 67.07%   [EVAL] batch:  271 | acc: 31.25%,  total acc: 66.93%   [EVAL] batch:  272 | acc: 31.25%,  total acc: 66.80%   [EVAL] batch:  273 | acc: 31.25%,  total acc: 66.67%   [EVAL] batch:  274 | acc: 25.00%,  total acc: 66.52%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 66.64%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 66.76%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 66.88%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 67.00%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 67.12%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 67.24%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 67.27%   [EVAL] batch:  282 | acc: 81.25%,  total acc: 67.31%   [EVAL] batch:  283 | acc: 68.75%,  total acc: 67.32%   [EVAL] batch:  284 | acc: 75.00%,  total acc: 67.35%   [EVAL] batch:  285 | acc: 62.50%,  total acc: 67.33%   [EVAL] batch:  286 | acc: 56.25%,  total acc: 67.29%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 67.30%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 67.41%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 67.52%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 67.63%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 67.74%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 67.85%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 67.94%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 68.05%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 68.16%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 68.27%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 68.37%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 68.48%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 68.58%   [EVAL] batch:  300 | acc: 87.50%,  total acc: 68.65%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 68.73%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 68.81%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 68.89%   [EVAL] batch:  304 | acc: 87.50%,  total acc: 68.95%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 69.04%   [EVAL] batch:  306 | acc: 75.00%,  total acc: 69.06%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 69.12%   [EVAL] batch:  308 | acc: 62.50%,  total acc: 69.09%   [EVAL] batch:  309 | acc: 75.00%,  total acc: 69.11%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 69.17%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 69.21%   [EVAL] batch:  312 | acc: 43.75%,  total acc: 69.13%   [EVAL] batch:  313 | acc: 37.50%,  total acc: 69.03%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 68.87%   [EVAL] batch:  315 | acc: 25.00%,  total acc: 68.73%   [EVAL] batch:  316 | acc: 37.50%,  total acc: 68.63%   [EVAL] batch:  317 | acc: 25.00%,  total acc: 68.49%   [EVAL] batch:  318 | acc: 50.00%,  total acc: 68.44%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 68.54%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 68.61%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 68.63%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 68.69%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 68.83%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 68.83%   [EVAL] batch:  326 | acc: 62.50%,  total acc: 68.81%   [EVAL] batch:  327 | acc: 75.00%,  total acc: 68.83%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 68.84%   [EVAL] batch:  329 | acc: 81.25%,  total acc: 68.88%   [EVAL] batch:  330 | acc: 62.50%,  total acc: 68.86%   [EVAL] batch:  331 | acc: 75.00%,  total acc: 68.88%   [EVAL] batch:  332 | acc: 56.25%,  total acc: 68.84%   [EVAL] batch:  333 | acc: 68.75%,  total acc: 68.84%   [EVAL] batch:  334 | acc: 50.00%,  total acc: 68.79%   [EVAL] batch:  335 | acc: 68.75%,  total acc: 68.79%   [EVAL] batch:  336 | acc: 50.00%,  total acc: 68.73%   [EVAL] batch:  337 | acc: 56.25%,  total acc: 68.69%   [EVAL] batch:  338 | acc: 37.50%,  total acc: 68.60%   [EVAL] batch:  339 | acc: 62.50%,  total acc: 68.58%   [EVAL] batch:  340 | acc: 62.50%,  total acc: 68.57%   [EVAL] batch:  341 | acc: 37.50%,  total acc: 68.48%   [EVAL] batch:  342 | acc: 25.00%,  total acc: 68.35%   [EVAL] batch:  343 | acc: 31.25%,  total acc: 68.24%   [EVAL] batch:  344 | acc: 68.75%,  total acc: 68.24%   [EVAL] batch:  345 | acc: 50.00%,  total acc: 68.19%   [EVAL] batch:  346 | acc: 56.25%,  total acc: 68.16%   [EVAL] batch:  347 | acc: 56.25%,  total acc: 68.12%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 68.14%   [EVAL] batch:  349 | acc: 81.25%,  total acc: 68.18%   [EVAL] batch:  350 | acc: 68.75%,  total acc: 68.18%   [EVAL] batch:  351 | acc: 68.75%,  total acc: 68.18%   [EVAL] batch:  352 | acc: 68.75%,  total acc: 68.18%   [EVAL] batch:  353 | acc: 75.00%,  total acc: 68.20%   [EVAL] batch:  354 | acc: 75.00%,  total acc: 68.22%   [EVAL] batch:  355 | acc: 87.50%,  total acc: 68.28%   [EVAL] batch:  356 | acc: 62.50%,  total acc: 68.26%   [EVAL] batch:  357 | acc: 31.25%,  total acc: 68.16%   [EVAL] batch:  358 | acc: 43.75%,  total acc: 68.09%   [EVAL] batch:  359 | acc: 50.00%,  total acc: 68.04%   [EVAL] batch:  360 | acc: 43.75%,  total acc: 67.97%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 67.92%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 67.94%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 68.03%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 68.12%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 68.20%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 68.29%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 68.38%   [EVAL] batch:  368 | acc: 87.50%,  total acc: 68.43%   [EVAL] batch:  369 | acc: 62.50%,  total acc: 68.41%   [EVAL] batch:  370 | acc: 56.25%,  total acc: 68.38%   [EVAL] batch:  371 | acc: 75.00%,  total acc: 68.40%   [EVAL] batch:  372 | acc: 68.75%,  total acc: 68.40%   [EVAL] batch:  373 | acc: 75.00%,  total acc: 68.42%   [EVAL] batch:  374 | acc: 68.75%,  total acc: 68.42%   [EVAL] batch:  375 | acc: 43.75%,  total acc: 68.35%   [EVAL] batch:  376 | acc: 50.00%,  total acc: 68.30%   [EVAL] batch:  377 | acc: 56.25%,  total acc: 68.27%   [EVAL] batch:  378 | acc: 62.50%,  total acc: 68.26%   [EVAL] batch:  379 | acc: 62.50%,  total acc: 68.24%   [EVAL] batch:  380 | acc: 68.75%,  total acc: 68.24%   [EVAL] batch:  381 | acc: 56.25%,  total acc: 68.21%   [EVAL] batch:  382 | acc: 75.00%,  total acc: 68.23%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 68.23%   [EVAL] batch:  384 | acc: 75.00%,  total acc: 68.25%   [EVAL] batch:  385 | acc: 50.00%,  total acc: 68.20%   [EVAL] batch:  386 | acc: 62.50%,  total acc: 68.18%   [EVAL] batch:  387 | acc: 68.75%,  total acc: 68.19%   [EVAL] batch:  388 | acc: 75.00%,  total acc: 68.20%   [EVAL] batch:  389 | acc: 75.00%,  total acc: 68.22%   [EVAL] batch:  390 | acc: 50.00%,  total acc: 68.17%   [EVAL] batch:  391 | acc: 75.00%,  total acc: 68.19%   [EVAL] batch:  392 | acc: 75.00%,  total acc: 68.21%   [EVAL] batch:  393 | acc: 87.50%,  total acc: 68.26%   [EVAL] batch:  394 | acc: 87.50%,  total acc: 68.31%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 68.39%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 68.47%   [EVAL] batch:  397 | acc: 93.75%,  total acc: 68.53%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 68.59%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 68.67%   [EVAL] batch:  400 | acc: 50.00%,  total acc: 68.63%   [EVAL] batch:  401 | acc: 31.25%,  total acc: 68.53%   [EVAL] batch:  402 | acc: 75.00%,  total acc: 68.55%   [EVAL] batch:  403 | acc: 68.75%,  total acc: 68.55%   [EVAL] batch:  404 | acc: 37.50%,  total acc: 68.47%   [EVAL] batch:  405 | acc: 43.75%,  total acc: 68.41%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 68.40%   [EVAL] batch:  407 | acc: 50.00%,  total acc: 68.35%   [EVAL] batch:  408 | acc: 25.00%,  total acc: 68.25%   [EVAL] batch:  409 | acc: 25.00%,  total acc: 68.14%   [EVAL] batch:  410 | acc: 12.50%,  total acc: 68.00%   [EVAL] batch:  411 | acc: 25.00%,  total acc: 67.90%   [EVAL] batch:  412 | acc: 56.25%,  total acc: 67.87%   [EVAL] batch:  413 | acc: 81.25%,  total acc: 67.90%   [EVAL] batch:  414 | acc: 81.25%,  total acc: 67.94%   [EVAL] batch:  415 | acc: 68.75%,  total acc: 67.94%   [EVAL] batch:  416 | acc: 93.75%,  total acc: 68.00%   [EVAL] batch:  417 | acc: 75.00%,  total acc: 68.02%   [EVAL] batch:  418 | acc: 81.25%,  total acc: 68.05%   [EVAL] batch:  419 | acc: 81.25%,  total acc: 68.08%   [EVAL] batch:  420 | acc: 75.00%,  total acc: 68.10%   [EVAL] batch:  421 | acc: 68.75%,  total acc: 68.10%   [EVAL] batch:  422 | acc: 68.75%,  total acc: 68.10%   [EVAL] batch:  423 | acc: 87.50%,  total acc: 68.15%   [EVAL] batch:  424 | acc: 87.50%,  total acc: 68.19%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 68.27%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 68.34%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 68.41%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 68.49%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 68.56%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 68.63%   [EVAL] batch:  431 | acc: 87.50%,  total acc: 68.68%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 68.72%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 68.76%   [EVAL] batch:  434 | acc: 93.75%,  total acc: 68.82%   [EVAL] batch:  435 | acc: 81.25%,  total acc: 68.85%   [EVAL] batch:  436 | acc: 93.75%,  total acc: 68.91%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 68.86%   
cur_acc:  ['0.9514', '0.7708', '0.7569', '0.7530', '0.8581', '0.6448', '0.7153']
his_acc:  ['0.9514', '0.8490', '0.8009', '0.7348', '0.7370', '0.7115', '0.6886']
CurrentTrain: epoch 15, batch     0 | loss: 16.2125574CurrentTrain: epoch 15, batch     1 | loss: 22.5422797CurrentTrain: epoch 15, batch     2 | loss: 19.9563144CurrentTrain: epoch  1, batch     3 | loss: 9.8565757CurrentTrain: epoch 15, batch     0 | loss: 22.4478703CurrentTrain: epoch 15, batch     1 | loss: 12.4691226CurrentTrain: epoch 15, batch     2 | loss: 20.4619434CurrentTrain: epoch  1, batch     3 | loss: 13.2010765CurrentTrain: epoch 15, batch     0 | loss: 11.3721984CurrentTrain: epoch 15, batch     1 | loss: 7.5502191CurrentTrain: epoch 15, batch     2 | loss: 14.3956570CurrentTrain: epoch  1, batch     3 | loss: 8.8412866CurrentTrain: epoch 15, batch     0 | loss: 14.4839431CurrentTrain: epoch 15, batch     1 | loss: 13.1828331CurrentTrain: epoch 15, batch     2 | loss: 16.3202028CurrentTrain: epoch  1, batch     3 | loss: 15.7903711CurrentTrain: epoch 15, batch     0 | loss: 13.0470986CurrentTrain: epoch 15, batch     1 | loss: 11.2562549CurrentTrain: epoch 15, batch     2 | loss: 11.3290132CurrentTrain: epoch  1, batch     3 | loss: 13.1655156CurrentTrain: epoch 15, batch     0 | loss: 11.3747020CurrentTrain: epoch 15, batch     1 | loss: 14.9648250CurrentTrain: epoch 15, batch     2 | loss: 14.3626441CurrentTrain: epoch  1, batch     3 | loss: 6.3422452CurrentTrain: epoch 15, batch     0 | loss: 10.5753201CurrentTrain: epoch 15, batch     1 | loss: 7.9357352CurrentTrain: epoch 15, batch     2 | loss: 5.3619685CurrentTrain: epoch  1, batch     3 | loss: 5.6299480CurrentTrain: epoch 15, batch     0 | loss: 11.1296014CurrentTrain: epoch 15, batch     1 | loss: 9.5963179CurrentTrain: epoch 15, batch     2 | loss: 10.6761050CurrentTrain: epoch  1, batch     3 | loss: 7.6336646CurrentTrain: epoch 15, batch     0 | loss: 8.0414659CurrentTrain: epoch 15, batch     1 | loss: 13.3073007CurrentTrain: epoch 15, batch     2 | loss: 8.1507452CurrentTrain: epoch  1, batch     3 | loss: 7.5721235CurrentTrain: epoch 15, batch     0 | loss: 4.5923423CurrentTrain: epoch 15, batch     1 | loss: 13.0379595CurrentTrain: epoch 15, batch     2 | loss: 6.9427359CurrentTrain: epoch  1, batch     3 | loss: 6.3104906
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: the andesite line , a zone of intense volcanic and seismic activity , is a major regional distinction in the pacific .
Head Entity: andesite line
Tail Entity: pacific
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The Great Barrier Reef, a stunning natural wonder, stretches along the northeastern coast of Australia.  
Head Entity: Great Barrier Reef  
Tail Entity: Australia  

Relation: located on terrain feature  
Context: Mount Everest, the highest peak in the world, is part of the Himalayas and attracts climbers from around the globe.  
Head Entity: Mount Everest  
Tail Entity: Himalayas  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, held in paris in 2015, saw numerous world leaders and activists come together to discuss global warming and its impacts.  
Head Entity: united nations climate change conference  
Tail Entity: paris agreement  

Relation: participant of  
Context: the 2022 fifa world cup in qatar featured teams from around the globe competing for the prestigious trophy, showcasing the best of international football.  
Head Entity: 2022 fifa world cup  
Tail Entity: qatar
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: luna subsequently had a supporting role in the comedy film " gravy " , directed by james roday , which was released in october 2015 .
Head Entity: gravy
Tail Entity: james roday
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: the acclaimed drama series "the crown" was brought to life by the visionary director peter morgan, who skillfully navigated the complexities of the royal family.  
Head Entity: the crown  
Tail Entity: peter morgan  

Relation: director  
Context: in the thrilling action movie "inception," the brilliant director christopher nolan crafted a mind-bending narrative that captivated audiences worldwide.  
Head Entity: inception  
Tail Entity: christopher nolan  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: tata prima is a range of heavy trucks produced by tata daewoo , a wholly owned subsidiary of tata motors of india .
Head Entity: tata daewoo
Tail Entity: tata motors
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: The iconic brand of luxury watches, Rolex, is owned by the Rolex Group, which is based in Geneva, Switzerland.  
Head Entity: Rolex Group  
Tail Entity: Rolex  

Relation: owned by  
Context: The popular fast-food chain, McDonald's, is owned by the McDonald's Corporation, which operates globally with thousands of franchises.  
Head Entity: McDonald's Corporation  
Tail Entity: McDonald's  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: he competed in the slopestyle event at the 2014 winter olympics in sochi , where he won a bronze medal , canada 's first medal in those games .
Head Entity: slopestyle event
Tail Entity: 2014 winter olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The engine is a crucial component of the car, providing the necessary power to drive the vehicle forward.  
Head Entity: engine  
Tail Entity: car  

Relation: part of  
Context: The heart is an essential organ in the human body, responsible for pumping blood and supplying oxygen to tissues.  
Head Entity: heart  
Tail Entity: human body  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: it served as the home of k league side gyeongnam fc and national league side changwon city fc prior to the construction of the changwon football center in 2009 .
Head Entity: changwon football center
Tail Entity: gyeongnam fc
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic mansion was once the residence of the famous artist, and it now serves as a gallery showcasing his works.  
Head Entity: historic mansion  
Tail Entity: famous artist  

Relation: occupant  
Context: After the renovation, the building became the headquarters for the local non-profit organization dedicated to community service.  
Head Entity: building  
Tail Entity: local non-profit organization  
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: the art nouveau style was brought to italy by figures such as giuseppe sommaruga and ernesto basile ( who respectively designed the palazzo castiglioni and expanded the palazzo montecitorio in rome ) .
Head Entity: palazzo castiglioni
Tail Entity: giuseppe sommaruga
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: The iconic Sydney Opera House was designed by Danish architect Jørn Utzon, who won the design competition in 1957.  
Head Entity: Sydney Opera House  
Tail Entity: Jørn Utzon  

Relation: architect  
Context: Frank Lloyd Wright is renowned for his innovative designs, including the famous Fallingwater house, which seamlessly integrates with its natural surroundings.  
Head Entity: Fallingwater  
Tail Entity: Frank Lloyd Wright  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: in 2001 brian haw set up the parliament square peace campaign outside the houses of parliament in london .
Head Entity: brian haw
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: After moving to New York City in 2010, Sarah found her dream apartment in Brooklyn.  
Head Entity: Sarah  
Tail Entity: Brooklyn  

Relation: residence  
Context: The famous author lived in a quaint cottage in the countryside for many years before relocating to the city.  
Head Entity: The famous author  
Tail Entity: the city  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Ludwig van Beethoven created some of the most celebrated symphonies in classical music history.  
Head Entity: symphonies  
Tail Entity: Ludwig van Beethoven  

Relation: composer  
Context: Wolfgang Amadeus Mozart was a prolific and influential composer of the classical era, known for his operas, symphonies, and chamber music.  
Head Entity: operas  
Tail Entity: Wolfgang Amadeus Mozart  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: one polish archer qualified for the women 's individual recurve by obtaining one of the eight olympic places available from the 2015 world archery championships in copenhagen , denmark .
Head Entity: 2015 world archery championships
Tail Entity: copenhagen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: The annual music festival will take place in the vibrant city of Austin, Texas, attracting thousands of fans from around the world.  
Head Entity: annual music festival  
Tail Entity: Austin, Texas  

Relation: location  
Context: The historic battle was fought near the banks of the Potomac River, a significant site in American history.  
Head Entity: historic battle  
Tail Entity: Potomac River  
MemoryTrain:  epoch 15, batch     0 | loss: 2.4558647MemoryTrain:  epoch 15, batch     1 | loss: 4.2270954MemoryTrain:  epoch 15, batch     2 | loss: 3.6251641MemoryTrain:  epoch 15, batch     3 | loss: 3.4252554MemoryTrain:  epoch 15, batch     4 | loss: 2.8824693MemoryTrain:  epoch 15, batch     5 | loss: 2.6187530MemoryTrain:  epoch 15, batch     6 | loss: 2.6839685MemoryTrain:  epoch 15, batch     7 | loss: 3.1514910MemoryTrain:  epoch 15, batch     8 | loss: 2.4691947MemoryTrain:  epoch 15, batch     9 | loss: 2.4991948MemoryTrain:  epoch 15, batch    10 | loss: 3.5691723MemoryTrain:  epoch 15, batch    11 | loss: 2.2925526MemoryTrain:  epoch 15, batch    12 | loss: 4.5237862MemoryTrain:  epoch 15, batch    13 | loss: 2.2815304MemoryTrain:  epoch 15, batch    14 | loss: 2.7819012MemoryTrain:  epoch 15, batch     0 | loss: 2.3298145MemoryTrain:  epoch 15, batch     1 | loss: 3.3387647MemoryTrain:  epoch 15, batch     2 | loss: 3.1336642MemoryTrain:  epoch 15, batch     3 | loss: 3.3170472MemoryTrain:  epoch 15, batch     4 | loss: 2.6474515MemoryTrain:  epoch 15, batch     5 | loss: 2.1151703MemoryTrain:  epoch 15, batch     6 | loss: 1.9993634MemoryTrain:  epoch 15, batch     7 | loss: 3.2188984MemoryTrain:  epoch 15, batch     8 | loss: 2.6843694MemoryTrain:  epoch 15, batch     9 | loss: 2.7000622MemoryTrain:  epoch 15, batch    10 | loss: 3.3226757MemoryTrain:  epoch 15, batch    11 | loss: 2.4164407MemoryTrain:  epoch 15, batch    12 | loss: 3.5021002MemoryTrain:  epoch 15, batch    13 | loss: 2.0914495MemoryTrain:  epoch 15, batch    14 | loss: 1.9456480MemoryTrain:  epoch 15, batch     0 | loss: 3.6024426MemoryTrain:  epoch 15, batch     1 | loss: 3.5778423MemoryTrain:  epoch 15, batch     2 | loss: 1.8067403MemoryTrain:  epoch 15, batch     3 | loss: 5.8316025MemoryTrain:  epoch 15, batch     4 | loss: 3.0877423MemoryTrain:  epoch 15, batch     5 | loss: 2.2718612MemoryTrain:  epoch 15, batch     6 | loss: 2.1248482MemoryTrain:  epoch 15, batch     7 | loss: 2.2077903MemoryTrain:  epoch 15, batch     8 | loss: 1.9421143MemoryTrain:  epoch 15, batch     9 | loss: 2.3925706MemoryTrain:  epoch 15, batch    10 | loss: 1.7824390MemoryTrain:  epoch 15, batch    11 | loss: 2.0480406MemoryTrain:  epoch 15, batch    12 | loss: 3.7374730MemoryTrain:  epoch 15, batch    13 | loss: 2.4072623MemoryTrain:  epoch 15, batch    14 | loss: 2.4523773MemoryTrain:  epoch 15, batch     0 | loss: 4.6463807MemoryTrain:  epoch 15, batch     1 | loss: 2.0533389MemoryTrain:  epoch 15, batch     2 | loss: 4.2566557MemoryTrain:  epoch 15, batch     3 | loss: 1.7465683MemoryTrain:  epoch 15, batch     4 | loss: 1.7555941MemoryTrain:  epoch 15, batch     5 | loss: 1.9777546MemoryTrain:  epoch 15, batch     6 | loss: 4.6765352MemoryTrain:  epoch 15, batch     7 | loss: 1.7517497MemoryTrain:  epoch 15, batch     8 | loss: 4.5563271MemoryTrain:  epoch 15, batch     9 | loss: 4.6662584MemoryTrain:  epoch 15, batch    10 | loss: 4.5499935MemoryTrain:  epoch 15, batch    11 | loss: 2.0252799MemoryTrain:  epoch 15, batch    12 | loss: 1.9287001MemoryTrain:  epoch 15, batch    13 | loss: 2.1176703MemoryTrain:  epoch 15, batch    14 | loss: 4.5624795MemoryTrain:  epoch 15, batch     0 | loss: 4.0364161MemoryTrain:  epoch 15, batch     1 | loss: 4.4264752MemoryTrain:  epoch 15, batch     2 | loss: 1.5873487MemoryTrain:  epoch 15, batch     3 | loss: 4.9735618MemoryTrain:  epoch 15, batch     4 | loss: 2.2632926MemoryTrain:  epoch 15, batch     5 | loss: 4.0017722MemoryTrain:  epoch 15, batch     6 | loss: 1.6902041MemoryTrain:  epoch 15, batch     7 | loss: 1.4255612MemoryTrain:  epoch 15, batch     8 | loss: 1.6773578MemoryTrain:  epoch 15, batch     9 | loss: 2.2880874MemoryTrain:  epoch 15, batch    10 | loss: 1.7973257MemoryTrain:  epoch 15, batch    11 | loss: 1.7751343MemoryTrain:  epoch 15, batch    12 | loss: 1.7962712MemoryTrain:  epoch 15, batch    13 | loss: 2.5532427MemoryTrain:  epoch 15, batch    14 | loss: 1.5405993MemoryTrain:  epoch 15, batch     0 | loss: 6.0516139MemoryTrain:  epoch 15, batch     1 | loss: 2.2740159MemoryTrain:  epoch 15, batch     2 | loss: 1.9745516MemoryTrain:  epoch 15, batch     3 | loss: 3.9878845MemoryTrain:  epoch 15, batch     4 | loss: 2.6175793MemoryTrain:  epoch 15, batch     5 | loss: 1.4920964MemoryTrain:  epoch 15, batch     6 | loss: 2.0500566MemoryTrain:  epoch 15, batch     7 | loss: 2.8703228MemoryTrain:  epoch 15, batch     8 | loss: 1.6092771MemoryTrain:  epoch 15, batch     9 | loss: 1.8413689MemoryTrain:  epoch 15, batch    10 | loss: 3.7307883MemoryTrain:  epoch 15, batch    11 | loss: 1.8161688MemoryTrain:  epoch 15, batch    12 | loss: 2.8038218MemoryTrain:  epoch 15, batch    13 | loss: 2.4369376MemoryTrain:  epoch 15, batch    14 | loss: 3.5998827MemoryTrain:  epoch 15, batch     0 | loss: 1.6721021MemoryTrain:  epoch 15, batch     1 | loss: 1.7145060MemoryTrain:  epoch 15, batch     2 | loss: 1.8823714MemoryTrain:  epoch 15, batch     3 | loss: 1.5722479MemoryTrain:  epoch 15, batch     4 | loss: 1.7070744MemoryTrain:  epoch 15, batch     5 | loss: 1.4814606MemoryTrain:  epoch 15, batch     6 | loss: 1.8981844MemoryTrain:  epoch 15, batch     7 | loss: 1.5274880MemoryTrain:  epoch 15, batch     8 | loss: 1.3294454MemoryTrain:  epoch 15, batch     9 | loss: 2.9572211MemoryTrain:  epoch 15, batch    10 | loss: 1.7842346MemoryTrain:  epoch 15, batch    11 | loss: 1.5458523MemoryTrain:  epoch 15, batch    12 | loss: 1.4659562MemoryTrain:  epoch 15, batch    13 | loss: 1.9632668MemoryTrain:  epoch 15, batch    14 | loss: 2.6161592MemoryTrain:  epoch 15, batch     0 | loss: 2.2883435MemoryTrain:  epoch 15, batch     1 | loss: 1.6922569MemoryTrain:  epoch 15, batch     2 | loss: 2.4664896MemoryTrain:  epoch 15, batch     3 | loss: 1.6020302MemoryTrain:  epoch 15, batch     4 | loss: 1.4267173MemoryTrain:  epoch 15, batch     5 | loss: 1.5023368MemoryTrain:  epoch 15, batch     6 | loss: 1.4372609MemoryTrain:  epoch 15, batch     7 | loss: 4.1781560MemoryTrain:  epoch 15, batch     8 | loss: 1.5370722MemoryTrain:  epoch 15, batch     9 | loss: 1.8715887MemoryTrain:  epoch 15, batch    10 | loss: 1.4837171MemoryTrain:  epoch 15, batch    11 | loss: 4.0224656MemoryTrain:  epoch 15, batch    12 | loss: 1.5456257MemoryTrain:  epoch 15, batch    13 | loss: 1.3880323MemoryTrain:  epoch 15, batch    14 | loss: 1.2792970MemoryTrain:  epoch 15, batch     0 | loss: 4.0892001MemoryTrain:  epoch 15, batch     1 | loss: 1.7664083MemoryTrain:  epoch 15, batch     2 | loss: 2.2100356MemoryTrain:  epoch 15, batch     3 | loss: 1.6886322MemoryTrain:  epoch 15, batch     4 | loss: 2.5339753MemoryTrain:  epoch 15, batch     5 | loss: 1.3590907MemoryTrain:  epoch 15, batch     6 | loss: 1.4325340MemoryTrain:  epoch 15, batch     7 | loss: 1.6320908MemoryTrain:  epoch 15, batch     8 | loss: 1.3645531MemoryTrain:  epoch 15, batch     9 | loss: 1.4266187MemoryTrain:  epoch 15, batch    10 | loss: 1.9037767MemoryTrain:  epoch 15, batch    11 | loss: 1.7247897MemoryTrain:  epoch 15, batch    12 | loss: 1.5392545MemoryTrain:  epoch 15, batch    13 | loss: 4.2740100MemoryTrain:  epoch 15, batch    14 | loss: 1.3171913MemoryTrain:  epoch 15, batch     0 | loss: 1.5457086MemoryTrain:  epoch 15, batch     1 | loss: 1.6359479MemoryTrain:  epoch 15, batch     2 | loss: 1.5271690MemoryTrain:  epoch 15, batch     3 | loss: 1.5465910MemoryTrain:  epoch 15, batch     4 | loss: 4.3263900MemoryTrain:  epoch 15, batch     5 | loss: 1.6759761MemoryTrain:  epoch 15, batch     6 | loss: 1.6380388MemoryTrain:  epoch 15, batch     7 | loss: 1.4770194MemoryTrain:  epoch 15, batch     8 | loss: 3.9958727MemoryTrain:  epoch 15, batch     9 | loss: 1.5467664MemoryTrain:  epoch 15, batch    10 | loss: 1.7326343MemoryTrain:  epoch 15, batch    11 | loss: 3.5723122MemoryTrain:  epoch 15, batch    12 | loss: 1.3867274MemoryTrain:  epoch 15, batch    13 | loss: 1.3655841MemoryTrain:  epoch 15, batch    14 | loss: 3.7748275
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 12.50%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 15.62%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 13.75%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 13.54%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 21.43%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 29.69%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 36.81%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 41.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 46.59%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 49.48%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 51.92%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 54.02%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 56.67%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 58.98%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 60.66%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 62.85%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 62.83%   [EVAL] batch:   19 | acc: 12.50%,  total acc: 60.31%   [EVAL] batch:   20 | acc: 0.00%,  total acc: 57.44%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 56.82%   [EVAL] batch:   22 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:   23 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 55.00%   [EVAL] batch:   25 | acc: 0.00%,  total acc: 52.88%   [EVAL] batch:   26 | acc: 18.75%,  total acc: 51.62%   [EVAL] batch:   27 | acc: 12.50%,  total acc: 50.22%   [EVAL] batch:   28 | acc: 12.50%,  total acc: 48.92%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 47.29%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 45.97%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 46.29%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 47.54%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 48.53%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 49.82%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 50.87%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 51.35%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 52.14%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 53.21%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 54.22%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 55.03%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 55.65%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 56.69%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 57.39%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 57.78%   [EVAL] batch:   45 | acc: 43.75%,  total acc: 57.47%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 57.31%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 57.03%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 57.27%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 57.12%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 56.74%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 57.09%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 56.84%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 57.06%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 57.27%   [EVAL] batch:   55 | acc: 56.25%,  total acc: 57.25%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 57.24%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 57.11%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 57.20%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 57.40%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 57.27%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 57.66%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 57.14%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 73.61%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 65.62%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 65.87%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 67.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 69.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 72.43%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 73.61%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 74.67%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 74.06%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 73.81%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 73.30%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 72.83%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 73.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 74.04%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.89%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 76.51%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 77.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 78.02%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 78.71%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 78.68%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 78.57%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 78.30%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 78.72%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 78.95%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 79.49%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 80.49%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 80.65%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 80.96%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 81.11%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 81.39%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 81.11%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 80.98%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 80.86%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 81.12%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 81.13%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 81.13%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 81.37%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 81.13%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 80.57%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 80.58%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 80.37%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 79.74%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 79.13%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 79.06%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 78.79%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 78.43%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 78.27%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 77.73%   [EVAL] batch:   64 | acc: 43.75%,  total acc: 77.21%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 76.99%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 76.68%   [EVAL] batch:   67 | acc: 62.50%,  total acc: 76.47%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 76.18%   [EVAL] batch:   69 | acc: 62.50%,  total acc: 75.98%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 75.62%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 75.35%   [EVAL] batch:   72 | acc: 62.50%,  total acc: 75.17%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 74.83%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 74.67%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 74.10%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 73.46%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 72.84%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 72.39%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 72.03%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 71.37%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 71.42%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 71.76%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 72.10%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 72.35%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 72.67%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 72.66%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 71.84%   [EVAL] batch:   89 | acc: 6.25%,  total acc: 71.11%   [EVAL] batch:   90 | acc: 12.50%,  total acc: 70.47%   [EVAL] batch:   91 | acc: 18.75%,  total acc: 69.90%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 69.22%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 68.88%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 69.14%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 69.40%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 69.59%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 69.77%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 70.01%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:  100 | acc: 6.25%,  total acc: 69.68%   [EVAL] batch:  101 | acc: 0.00%,  total acc: 69.00%   [EVAL] batch:  102 | acc: 6.25%,  total acc: 68.39%   [EVAL] batch:  103 | acc: 6.25%,  total acc: 67.79%   [EVAL] batch:  104 | acc: 0.00%,  total acc: 67.14%   [EVAL] batch:  105 | acc: 12.50%,  total acc: 66.63%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 66.53%   [EVAL] batch:  107 | acc: 75.00%,  total acc: 66.61%   [EVAL] batch:  108 | acc: 68.75%,  total acc: 66.63%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 66.53%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 66.44%   [EVAL] batch:  111 | acc: 56.25%,  total acc: 66.35%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 66.37%   [EVAL] batch:  113 | acc: 31.25%,  total acc: 66.06%   [EVAL] batch:  114 | acc: 43.75%,  total acc: 65.87%   [EVAL] batch:  115 | acc: 37.50%,  total acc: 65.62%   [EVAL] batch:  116 | acc: 25.00%,  total acc: 65.28%   [EVAL] batch:  117 | acc: 31.25%,  total acc: 64.99%   [EVAL] batch:  118 | acc: 43.75%,  total acc: 64.81%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 65.00%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 65.24%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 65.47%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 65.60%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 65.73%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 65.95%   [EVAL] batch:  125 | acc: 6.25%,  total acc: 65.48%   [EVAL] batch:  126 | acc: 6.25%,  total acc: 65.01%   [EVAL] batch:  127 | acc: 12.50%,  total acc: 64.60%   [EVAL] batch:  128 | acc: 6.25%,  total acc: 64.15%   [EVAL] batch:  129 | acc: 12.50%,  total acc: 63.75%   [EVAL] batch:  130 | acc: 12.50%,  total acc: 63.36%   [EVAL] batch:  131 | acc: 37.50%,  total acc: 63.16%   [EVAL] batch:  132 | acc: 75.00%,  total acc: 63.25%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 63.39%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 63.56%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 63.69%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 63.82%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 63.90%   [EVAL] batch:  138 | acc: 43.75%,  total acc: 63.76%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 63.71%   [EVAL] batch:  140 | acc: 62.50%,  total acc: 63.70%   [EVAL] batch:  141 | acc: 50.00%,  total acc: 63.60%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 63.64%   [EVAL] batch:  143 | acc: 56.25%,  total acc: 63.59%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 63.79%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 64.04%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 64.24%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 64.48%   [EVAL] batch:  148 | acc: 93.75%,  total acc: 64.68%   [EVAL] batch:  149 | acc: 81.25%,  total acc: 64.79%   [EVAL] batch:  150 | acc: 87.50%,  total acc: 64.94%   [EVAL] batch:  151 | acc: 87.50%,  total acc: 65.09%   [EVAL] batch:  152 | acc: 68.75%,  total acc: 65.11%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 65.26%   [EVAL] batch:  154 | acc: 75.00%,  total acc: 65.32%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 65.46%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 65.64%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 65.86%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 66.08%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 66.25%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 66.42%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 66.63%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 66.56%   [EVAL] batch:  163 | acc: 25.00%,  total acc: 66.31%   [EVAL] batch:  164 | acc: 31.25%,  total acc: 66.10%   [EVAL] batch:  165 | acc: 25.00%,  total acc: 65.85%   [EVAL] batch:  166 | acc: 25.00%,  total acc: 65.61%   [EVAL] batch:  167 | acc: 25.00%,  total acc: 65.36%   [EVAL] batch:  168 | acc: 31.25%,  total acc: 65.16%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 65.37%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 65.46%   [EVAL] batch:  171 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:  172 | acc: 93.75%,  total acc: 65.79%   [EVAL] batch:  173 | acc: 81.25%,  total acc: 65.88%   [EVAL] batch:  174 | acc: 75.00%,  total acc: 65.93%   [EVAL] batch:  175 | acc: 43.75%,  total acc: 65.80%   [EVAL] batch:  176 | acc: 31.25%,  total acc: 65.61%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 65.45%   [EVAL] batch:  178 | acc: 18.75%,  total acc: 65.19%   [EVAL] batch:  179 | acc: 43.75%,  total acc: 65.07%   [EVAL] batch:  180 | acc: 12.50%,  total acc: 64.78%   [EVAL] batch:  181 | acc: 43.75%,  total acc: 64.66%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 64.62%   [EVAL] batch:  183 | acc: 50.00%,  total acc: 64.54%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 64.53%   [EVAL] batch:  185 | acc: 56.25%,  total acc: 64.48%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 64.54%   [EVAL] batch:  187 | acc: 75.00%,  total acc: 64.59%   [EVAL] batch:  188 | acc: 56.25%,  total acc: 64.55%   [EVAL] batch:  189 | acc: 81.25%,  total acc: 64.64%   [EVAL] batch:  190 | acc: 81.25%,  total acc: 64.73%   [EVAL] batch:  191 | acc: 62.50%,  total acc: 64.71%   [EVAL] batch:  192 | acc: 75.00%,  total acc: 64.77%   [EVAL] batch:  193 | acc: 68.75%,  total acc: 64.79%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 64.81%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 64.80%   [EVAL] batch:  196 | acc: 50.00%,  total acc: 64.72%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 64.80%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 64.82%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 64.91%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 64.96%   [EVAL] batch:  201 | acc: 93.75%,  total acc: 65.10%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 65.09%   [EVAL] batch:  203 | acc: 93.75%,  total acc: 65.23%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 65.30%   [EVAL] batch:  205 | acc: 100.00%,  total acc: 65.47%   [EVAL] batch:  206 | acc: 50.00%,  total acc: 65.40%   [EVAL] batch:  207 | acc: 18.75%,  total acc: 65.17%   [EVAL] batch:  208 | acc: 37.50%,  total acc: 65.04%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 64.85%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 64.69%   [EVAL] batch:  211 | acc: 25.00%,  total acc: 64.50%   [EVAL] batch:  212 | acc: 43.75%,  total acc: 64.41%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 64.54%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 64.71%   [EVAL] batch:  215 | acc: 81.25%,  total acc: 64.79%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 65.08%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 65.24%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 65.40%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 65.55%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 65.71%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 65.86%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 66.02%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 66.17%   [EVAL] batch:  225 | acc: 93.75%,  total acc: 66.29%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 66.35%   [EVAL] batch:  227 | acc: 87.50%,  total acc: 66.45%   [EVAL] batch:  228 | acc: 81.25%,  total acc: 66.51%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 66.66%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 66.77%   [EVAL] batch:  231 | acc: 56.25%,  total acc: 66.73%   [EVAL] batch:  232 | acc: 31.25%,  total acc: 66.58%   [EVAL] batch:  233 | acc: 37.50%,  total acc: 66.45%   [EVAL] batch:  234 | acc: 31.25%,  total acc: 66.30%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 66.26%   [EVAL] batch:  236 | acc: 18.75%,  total acc: 66.06%   [EVAL] batch:  237 | acc: 56.25%,  total acc: 66.02%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 66.11%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 66.34%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 66.43%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 66.51%   [EVAL] batch:  243 | acc: 68.75%,  total acc: 66.52%   [EVAL] batch:  244 | acc: 6.25%,  total acc: 66.28%   [EVAL] batch:  245 | acc: 25.00%,  total acc: 66.11%   [EVAL] batch:  246 | acc: 25.00%,  total acc: 65.94%   [EVAL] batch:  247 | acc: 25.00%,  total acc: 65.78%   [EVAL] batch:  248 | acc: 6.25%,  total acc: 65.54%   [EVAL] batch:  249 | acc: 18.75%,  total acc: 65.35%   [EVAL] batch:  250 | acc: 100.00%,  total acc: 65.49%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 65.60%   [EVAL] batch:  252 | acc: 100.00%,  total acc: 65.74%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 65.82%   [EVAL] batch:  254 | acc: 100.00%,  total acc: 65.96%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 66.09%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 66.10%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 66.11%   [EVAL] batch:  258 | acc: 62.50%,  total acc: 66.10%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 66.08%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 66.02%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 66.03%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 66.02%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 66.05%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 66.06%   [EVAL] batch:  265 | acc: 50.00%,  total acc: 66.00%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 65.96%   [EVAL] batch:  267 | acc: 56.25%,  total acc: 65.93%   [EVAL] batch:  268 | acc: 50.00%,  total acc: 65.87%   [EVAL] batch:  269 | acc: 18.75%,  total acc: 65.69%   [EVAL] batch:  270 | acc: 18.75%,  total acc: 65.52%   [EVAL] batch:  271 | acc: 25.00%,  total acc: 65.37%   [EVAL] batch:  272 | acc: 6.25%,  total acc: 65.16%   [EVAL] batch:  273 | acc: 25.00%,  total acc: 65.01%   [EVAL] batch:  274 | acc: 25.00%,  total acc: 64.86%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 64.99%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 65.12%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 65.24%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 65.37%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 65.49%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 65.61%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 65.65%   [EVAL] batch:  282 | acc: 81.25%,  total acc: 65.70%   [EVAL] batch:  283 | acc: 68.75%,  total acc: 65.71%   [EVAL] batch:  284 | acc: 68.75%,  total acc: 65.72%   [EVAL] batch:  285 | acc: 68.75%,  total acc: 65.73%   [EVAL] batch:  286 | acc: 56.25%,  total acc: 65.70%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 65.71%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 65.83%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 65.95%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 66.07%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 66.18%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 66.39%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 66.50%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 66.62%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 66.73%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 66.84%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 66.95%   [EVAL] batch:  299 | acc: 93.75%,  total acc: 67.04%   [EVAL] batch:  300 | acc: 87.50%,  total acc: 67.11%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 67.20%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 67.31%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 67.39%   [EVAL] batch:  304 | acc: 87.50%,  total acc: 67.46%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 67.54%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 67.61%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 67.67%   [EVAL] batch:  308 | acc: 81.25%,  total acc: 67.72%   [EVAL] batch:  309 | acc: 81.25%,  total acc: 67.76%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 67.83%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 67.87%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 67.81%   [EVAL] batch:  313 | acc: 31.25%,  total acc: 67.70%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 67.54%   [EVAL] batch:  315 | acc: 25.00%,  total acc: 67.41%   [EVAL] batch:  316 | acc: 31.25%,  total acc: 67.29%   [EVAL] batch:  317 | acc: 25.00%,  total acc: 67.16%   [EVAL] batch:  318 | acc: 56.25%,  total acc: 67.12%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 67.23%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 67.31%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 67.33%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 67.40%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 67.46%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 67.54%   [EVAL] batch:  325 | acc: 62.50%,  total acc: 67.52%   [EVAL] batch:  326 | acc: 62.50%,  total acc: 67.51%   [EVAL] batch:  327 | acc: 75.00%,  total acc: 67.53%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 67.55%   [EVAL] batch:  329 | acc: 75.00%,  total acc: 67.58%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 67.58%   [EVAL] batch:  331 | acc: 50.00%,  total acc: 67.53%   [EVAL] batch:  332 | acc: 43.75%,  total acc: 67.45%   [EVAL] batch:  333 | acc: 56.25%,  total acc: 67.42%   [EVAL] batch:  334 | acc: 31.25%,  total acc: 67.31%   [EVAL] batch:  335 | acc: 43.75%,  total acc: 67.24%   [EVAL] batch:  336 | acc: 31.25%,  total acc: 67.14%   [EVAL] batch:  337 | acc: 37.50%,  total acc: 67.05%   [EVAL] batch:  338 | acc: 12.50%,  total acc: 66.89%   [EVAL] batch:  339 | acc: 31.25%,  total acc: 66.78%   [EVAL] batch:  340 | acc: 0.00%,  total acc: 66.59%   [EVAL] batch:  341 | acc: 6.25%,  total acc: 66.41%   [EVAL] batch:  342 | acc: 18.75%,  total acc: 66.27%   [EVAL] batch:  343 | acc: 12.50%,  total acc: 66.12%   [EVAL] batch:  344 | acc: 56.25%,  total acc: 66.09%   [EVAL] batch:  345 | acc: 43.75%,  total acc: 66.02%   [EVAL] batch:  346 | acc: 62.50%,  total acc: 66.01%   [EVAL] batch:  347 | acc: 56.25%,  total acc: 65.98%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 66.01%   [EVAL] batch:  349 | acc: 81.25%,  total acc: 66.05%   [EVAL] batch:  350 | acc: 68.75%,  total acc: 66.06%   [EVAL] batch:  351 | acc: 68.75%,  total acc: 66.07%   [EVAL] batch:  352 | acc: 75.00%,  total acc: 66.09%   [EVAL] batch:  353 | acc: 75.00%,  total acc: 66.12%   [EVAL] batch:  354 | acc: 75.00%,  total acc: 66.14%   [EVAL] batch:  355 | acc: 87.50%,  total acc: 66.20%   [EVAL] batch:  356 | acc: 62.50%,  total acc: 66.19%   [EVAL] batch:  357 | acc: 31.25%,  total acc: 66.10%   [EVAL] batch:  358 | acc: 37.50%,  total acc: 66.02%   [EVAL] batch:  359 | acc: 37.50%,  total acc: 65.94%   [EVAL] batch:  360 | acc: 37.50%,  total acc: 65.86%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 65.81%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 65.81%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 65.90%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 65.99%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 66.09%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 66.18%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 66.27%   [EVAL] batch:  368 | acc: 87.50%,  total acc: 66.33%   [EVAL] batch:  369 | acc: 81.25%,  total acc: 66.37%   [EVAL] batch:  370 | acc: 75.00%,  total acc: 66.39%   [EVAL] batch:  371 | acc: 81.25%,  total acc: 66.43%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 66.47%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 66.53%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 66.55%   [EVAL] batch:  375 | acc: 43.75%,  total acc: 66.49%   [EVAL] batch:  376 | acc: 43.75%,  total acc: 66.43%   [EVAL] batch:  377 | acc: 31.25%,  total acc: 66.34%   [EVAL] batch:  378 | acc: 56.25%,  total acc: 66.31%   [EVAL] batch:  379 | acc: 62.50%,  total acc: 66.30%   [EVAL] batch:  380 | acc: 68.75%,  total acc: 66.31%   [EVAL] batch:  381 | acc: 56.25%,  total acc: 66.28%   [EVAL] batch:  382 | acc: 75.00%,  total acc: 66.30%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 66.31%   [EVAL] batch:  384 | acc: 68.75%,  total acc: 66.31%   [EVAL] batch:  385 | acc: 50.00%,  total acc: 66.27%   [EVAL] batch:  386 | acc: 68.75%,  total acc: 66.28%   [EVAL] batch:  387 | acc: 81.25%,  total acc: 66.32%   [EVAL] batch:  388 | acc: 68.75%,  total acc: 66.32%   [EVAL] batch:  389 | acc: 75.00%,  total acc: 66.35%   [EVAL] batch:  390 | acc: 50.00%,  total acc: 66.30%   [EVAL] batch:  391 | acc: 81.25%,  total acc: 66.34%   [EVAL] batch:  392 | acc: 87.50%,  total acc: 66.40%   [EVAL] batch:  393 | acc: 93.75%,  total acc: 66.47%   [EVAL] batch:  394 | acc: 87.50%,  total acc: 66.52%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 66.60%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 66.69%   [EVAL] batch:  397 | acc: 93.75%,  total acc: 66.76%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 66.82%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 66.91%   [EVAL] batch:  400 | acc: 37.50%,  total acc: 66.83%   [EVAL] batch:  401 | acc: 31.25%,  total acc: 66.74%   [EVAL] batch:  402 | acc: 62.50%,  total acc: 66.73%   [EVAL] batch:  403 | acc: 68.75%,  total acc: 66.74%   [EVAL] batch:  404 | acc: 31.25%,  total acc: 66.65%   [EVAL] batch:  405 | acc: 43.75%,  total acc: 66.59%   [EVAL] batch:  406 | acc: 68.75%,  total acc: 66.60%   [EVAL] batch:  407 | acc: 50.00%,  total acc: 66.56%   [EVAL] batch:  408 | acc: 31.25%,  total acc: 66.47%   [EVAL] batch:  409 | acc: 31.25%,  total acc: 66.39%   [EVAL] batch:  410 | acc: 18.75%,  total acc: 66.27%   [EVAL] batch:  411 | acc: 25.00%,  total acc: 66.17%   [EVAL] batch:  412 | acc: 37.50%,  total acc: 66.10%   [EVAL] batch:  413 | acc: 56.25%,  total acc: 66.08%   [EVAL] batch:  414 | acc: 31.25%,  total acc: 65.99%   [EVAL] batch:  415 | acc: 31.25%,  total acc: 65.91%   [EVAL] batch:  416 | acc: 50.00%,  total acc: 65.87%   [EVAL] batch:  417 | acc: 50.00%,  total acc: 65.83%   [EVAL] batch:  418 | acc: 50.00%,  total acc: 65.80%   [EVAL] batch:  419 | acc: 87.50%,  total acc: 65.85%   [EVAL] batch:  420 | acc: 68.75%,  total acc: 65.86%   [EVAL] batch:  421 | acc: 75.00%,  total acc: 65.88%   [EVAL] batch:  422 | acc: 68.75%,  total acc: 65.88%   [EVAL] batch:  423 | acc: 87.50%,  total acc: 65.93%   [EVAL] batch:  424 | acc: 87.50%,  total acc: 65.99%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 66.07%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 66.14%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 66.22%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 66.38%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 66.46%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 66.52%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 66.57%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 66.62%   [EVAL] batch:  434 | acc: 93.75%,  total acc: 66.68%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 66.73%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 66.80%   [EVAL] batch:  437 | acc: 56.25%,  total acc: 66.78%   [EVAL] batch:  438 | acc: 18.75%,  total acc: 66.67%   [EVAL] batch:  439 | acc: 12.50%,  total acc: 66.55%   [EVAL] batch:  440 | acc: 18.75%,  total acc: 66.44%   [EVAL] batch:  441 | acc: 6.25%,  total acc: 66.30%   [EVAL] batch:  442 | acc: 12.50%,  total acc: 66.18%   [EVAL] batch:  443 | acc: 37.50%,  total acc: 66.12%   [EVAL] batch:  444 | acc: 81.25%,  total acc: 66.15%   [EVAL] batch:  445 | acc: 87.50%,  total acc: 66.20%   [EVAL] batch:  446 | acc: 87.50%,  total acc: 66.25%   [EVAL] batch:  447 | acc: 100.00%,  total acc: 66.32%   [EVAL] batch:  448 | acc: 81.25%,  total acc: 66.36%   [EVAL] batch:  449 | acc: 81.25%,  total acc: 66.39%   [EVAL] batch:  450 | acc: 93.75%,  total acc: 66.45%   [EVAL] batch:  451 | acc: 75.00%,  total acc: 66.47%   [EVAL] batch:  452 | acc: 93.75%,  total acc: 66.53%   [EVAL] batch:  453 | acc: 93.75%,  total acc: 66.59%   [EVAL] batch:  454 | acc: 93.75%,  total acc: 66.65%   [EVAL] batch:  455 | acc: 100.00%,  total acc: 66.72%   [EVAL] batch:  456 | acc: 25.00%,  total acc: 66.63%   [EVAL] batch:  457 | acc: 0.00%,  total acc: 66.48%   [EVAL] batch:  458 | acc: 18.75%,  total acc: 66.38%   [EVAL] batch:  459 | acc: 56.25%,  total acc: 66.36%   [EVAL] batch:  460 | acc: 43.75%,  total acc: 66.31%   [EVAL] batch:  461 | acc: 37.50%,  total acc: 66.25%   [EVAL] batch:  462 | acc: 12.50%,  total acc: 66.13%   [EVAL] batch:  463 | acc: 12.50%,  total acc: 66.02%   [EVAL] batch:  464 | acc: 12.50%,  total acc: 65.90%   [EVAL] batch:  465 | acc: 6.25%,  total acc: 65.77%   [EVAL] batch:  466 | acc: 12.50%,  total acc: 65.66%   [EVAL] batch:  467 | acc: 0.00%,  total acc: 65.52%   [EVAL] batch:  468 | acc: 25.00%,  total acc: 65.43%   [EVAL] batch:  469 | acc: 75.00%,  total acc: 65.45%   [EVAL] batch:  470 | acc: 81.25%,  total acc: 65.49%   [EVAL] batch:  471 | acc: 93.75%,  total acc: 65.55%   [EVAL] batch:  472 | acc: 93.75%,  total acc: 65.61%   [EVAL] batch:  473 | acc: 87.50%,  total acc: 65.65%   [EVAL] batch:  474 | acc: 56.25%,  total acc: 65.63%   [EVAL] batch:  475 | acc: 100.00%,  total acc: 65.70%   [EVAL] batch:  476 | acc: 87.50%,  total acc: 65.75%   [EVAL] batch:  477 | acc: 87.50%,  total acc: 65.79%   [EVAL] batch:  478 | acc: 87.50%,  total acc: 65.84%   [EVAL] batch:  479 | acc: 93.75%,  total acc: 65.90%   [EVAL] batch:  480 | acc: 100.00%,  total acc: 65.97%   [EVAL] batch:  481 | acc: 75.00%,  total acc: 65.99%   [EVAL] batch:  482 | acc: 56.25%,  total acc: 65.97%   [EVAL] batch:  483 | acc: 43.75%,  total acc: 65.92%   [EVAL] batch:  484 | acc: 62.50%,  total acc: 65.91%   [EVAL] batch:  485 | acc: 50.00%,  total acc: 65.88%   [EVAL] batch:  486 | acc: 68.75%,  total acc: 65.89%   [EVAL] batch:  487 | acc: 37.50%,  total acc: 65.83%   [EVAL] batch:  488 | acc: 50.00%,  total acc: 65.80%   [EVAL] batch:  489 | acc: 56.25%,  total acc: 65.78%   [EVAL] batch:  490 | acc: 56.25%,  total acc: 65.76%   [EVAL] batch:  491 | acc: 68.75%,  total acc: 65.76%   [EVAL] batch:  492 | acc: 68.75%,  total acc: 65.77%   [EVAL] batch:  493 | acc: 62.50%,  total acc: 65.76%   [EVAL] batch:  494 | acc: 43.75%,  total acc: 65.72%   [EVAL] batch:  495 | acc: 68.75%,  total acc: 65.73%   [EVAL] batch:  496 | acc: 50.00%,  total acc: 65.69%   [EVAL] batch:  497 | acc: 56.25%,  total acc: 65.68%   [EVAL] batch:  498 | acc: 75.00%,  total acc: 65.69%   [EVAL] batch:  499 | acc: 62.50%,  total acc: 65.69%   
cur_acc:  ['0.9514', '0.7708', '0.7569', '0.7530', '0.8581', '0.6448', '0.7153', '0.5714']
his_acc:  ['0.9514', '0.8490', '0.8009', '0.7348', '0.7370', '0.7115', '0.6886', '0.6569']
--------Round  4
seed:  500
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 5 6 4 2 1 3 0]
prepared data!
CurrentTrain: epoch 15, batch     0 | loss: 30.0739006CurrentTrain: epoch 15, batch     1 | loss: 19.6084027CurrentTrain: epoch 15, batch     2 | loss: 21.1123169CurrentTrain: epoch 15, batch     3 | loss: 27.5357657CurrentTrain: epoch 15, batch     4 | loss: 23.9224120CurrentTrain: epoch 15, batch     5 | loss: 24.3220492CurrentTrain: epoch 15, batch     6 | loss: 23.2754927CurrentTrain: epoch 15, batch     7 | loss: 24.6365789CurrentTrain: epoch 15, batch     8 | loss: 24.2058894CurrentTrain: epoch 15, batch     9 | loss: 21.7294322CurrentTrain: epoch 15, batch    10 | loss: 30.9419687CurrentTrain: epoch 15, batch    11 | loss: 30.1369453CurrentTrain: epoch 15, batch    12 | loss: 26.7727104CurrentTrain: epoch 15, batch    13 | loss: 16.6549392CurrentTrain: epoch 15, batch    14 | loss: 22.8909960CurrentTrain: epoch 15, batch    15 | loss: 15.8514071CurrentTrain: epoch 15, batch    16 | loss: 24.7519114CurrentTrain: epoch 15, batch    17 | loss: 25.2366426CurrentTrain: epoch 15, batch    18 | loss: 16.3518595CurrentTrain: epoch 15, batch    19 | loss: 18.1969291CurrentTrain: epoch 15, batch    20 | loss: 17.0754172CurrentTrain: epoch 15, batch    21 | loss: 14.4014916CurrentTrain: epoch 15, batch    22 | loss: 17.6227096CurrentTrain: epoch 15, batch    23 | loss: 20.2714717CurrentTrain: epoch 15, batch    24 | loss: 17.3565664CurrentTrain: epoch 15, batch    25 | loss: 19.0336986CurrentTrain: epoch 15, batch    26 | loss: 17.3889109CurrentTrain: epoch 15, batch    27 | loss: 39.8995976CurrentTrain: epoch 15, batch    28 | loss: 16.2716639CurrentTrain: epoch 15, batch    29 | loss: 12.5422485CurrentTrain: epoch 15, batch    30 | loss: 15.2316870CurrentTrain: epoch 15, batch    31 | loss: 22.9018603CurrentTrain: epoch 15, batch    32 | loss: 24.9988857CurrentTrain: epoch 15, batch    33 | loss: 19.8736394CurrentTrain: epoch 15, batch    34 | loss: 23.7241804CurrentTrain: epoch 15, batch    35 | loss: 11.9616961CurrentTrain: epoch 15, batch    36 | loss: 14.5590470CurrentTrain: epoch 15, batch    37 | loss: 24.8183823CurrentTrain: epoch 15, batch    38 | loss: 14.2383202CurrentTrain: epoch 15, batch    39 | loss: 14.8296258CurrentTrain: epoch 15, batch    40 | loss: 20.0440954CurrentTrain: epoch 15, batch    41 | loss: 19.4251641CurrentTrain: epoch 15, batch    42 | loss: 10.3768256CurrentTrain: epoch 15, batch    43 | loss: 24.4221637CurrentTrain: epoch 15, batch    44 | loss: 13.3632709CurrentTrain: epoch 15, batch    45 | loss: 14.4889853CurrentTrain: epoch 15, batch    46 | loss: 15.8093414CurrentTrain: epoch 15, batch    47 | loss: 29.4262456CurrentTrain: epoch 15, batch    48 | loss: 12.6546064CurrentTrain: epoch 15, batch    49 | loss: 19.5620025CurrentTrain: epoch 15, batch    50 | loss: 24.0433065CurrentTrain: epoch 15, batch    51 | loss: 13.5945464CurrentTrain: epoch 15, batch    52 | loss: 13.1356604CurrentTrain: epoch 15, batch    53 | loss: 11.2714636CurrentTrain: epoch 15, batch    54 | loss: 17.7528195CurrentTrain: epoch 15, batch    55 | loss: 15.0517868CurrentTrain: epoch 15, batch    56 | loss: 30.4235618CurrentTrain: epoch 15, batch    57 | loss: 20.6766620CurrentTrain: epoch 15, batch    58 | loss: 14.0334394CurrentTrain: epoch 15, batch    59 | loss: 11.3733173CurrentTrain: epoch 15, batch    60 | loss: 12.8306653CurrentTrain: epoch 15, batch    61 | loss: 13.6471814CurrentTrain: epoch  7, batch    62 | loss: 13.6104672CurrentTrain: epoch 15, batch     0 | loss: 20.0801935CurrentTrain: epoch 15, batch     1 | loss: 13.8137632CurrentTrain: epoch 15, batch     2 | loss: 13.4111262CurrentTrain: epoch 15, batch     3 | loss: 14.6461289CurrentTrain: epoch 15, batch     4 | loss: 24.9428390CurrentTrain: epoch 15, batch     5 | loss: 15.2935370CurrentTrain: epoch 15, batch     6 | loss: 19.0550211CurrentTrain: epoch 15, batch     7 | loss: 22.0684497CurrentTrain: epoch 15, batch     8 | loss: 20.6303769CurrentTrain: epoch 15, batch     9 | loss: 20.2515993CurrentTrain: epoch 15, batch    10 | loss: 12.3045592CurrentTrain: epoch 15, batch    11 | loss: 21.6340350CurrentTrain: epoch 15, batch    12 | loss: 20.3142127CurrentTrain: epoch 15, batch    13 | loss: 16.5615453CurrentTrain: epoch 15, batch    14 | loss: 16.4434742CurrentTrain: epoch 15, batch    15 | loss: 17.2922929CurrentTrain: epoch 15, batch    16 | loss: 10.4084128CurrentTrain: epoch 15, batch    17 | loss: 15.3738409CurrentTrain: epoch 15, batch    18 | loss: 18.2289301CurrentTrain: epoch 15, batch    19 | loss: 20.9372524CurrentTrain: epoch 15, batch    20 | loss: 25.3586312CurrentTrain: epoch 15, batch    21 | loss: 12.9167706CurrentTrain: epoch 15, batch    22 | loss: 16.5378647CurrentTrain: epoch 15, batch    23 | loss: 19.9352931CurrentTrain: epoch 15, batch    24 | loss: 19.7482296CurrentTrain: epoch 15, batch    25 | loss: 17.4045357CurrentTrain: epoch 15, batch    26 | loss: 16.6209762CurrentTrain: epoch 15, batch    27 | loss: 13.0721440CurrentTrain: epoch 15, batch    28 | loss: 17.0447523CurrentTrain: epoch 15, batch    29 | loss: 11.4191066CurrentTrain: epoch 15, batch    30 | loss: 12.0701457CurrentTrain: epoch 15, batch    31 | loss: 10.0026123CurrentTrain: epoch 15, batch    32 | loss: 19.0381363CurrentTrain: epoch 15, batch    33 | loss: 29.9131305CurrentTrain: epoch 15, batch    34 | loss: 17.0806401CurrentTrain: epoch 15, batch    35 | loss: 11.5891108CurrentTrain: epoch 15, batch    36 | loss: 15.6299406CurrentTrain: epoch 15, batch    37 | loss: 12.3377058CurrentTrain: epoch 15, batch    38 | loss: 17.2585061CurrentTrain: epoch 15, batch    39 | loss: 13.6250603CurrentTrain: epoch 15, batch    40 | loss: 9.5327048CurrentTrain: epoch 15, batch    41 | loss: 15.1763919CurrentTrain: epoch 15, batch    42 | loss: 22.9910433CurrentTrain: epoch 15, batch    43 | loss: 15.3436557CurrentTrain: epoch 15, batch    44 | loss: 12.2920373CurrentTrain: epoch 15, batch    45 | loss: 27.0831028CurrentTrain: epoch 15, batch    46 | loss: 11.2196857CurrentTrain: epoch 15, batch    47 | loss: 22.4883357CurrentTrain: epoch 15, batch    48 | loss: 12.6978181CurrentTrain: epoch 15, batch    49 | loss: 17.8559320CurrentTrain: epoch 15, batch    50 | loss: 18.3949616CurrentTrain: epoch 15, batch    51 | loss: 15.8336605CurrentTrain: epoch 15, batch    52 | loss: 17.5080097CurrentTrain: epoch 15, batch    53 | loss: 12.0043533CurrentTrain: epoch 15, batch    54 | loss: 15.1098948CurrentTrain: epoch 15, batch    55 | loss: 11.6998195CurrentTrain: epoch 15, batch    56 | loss: 17.3064562CurrentTrain: epoch 15, batch    57 | loss: 14.8180256CurrentTrain: epoch 15, batch    58 | loss: 14.0854967CurrentTrain: epoch 15, batch    59 | loss: 11.4526155CurrentTrain: epoch 15, batch    60 | loss: 16.5105698CurrentTrain: epoch 15, batch    61 | loss: 20.8581259CurrentTrain: epoch  7, batch    62 | loss: 10.5515179CurrentTrain: epoch 15, batch     0 | loss: 10.4488223CurrentTrain: epoch 15, batch     1 | loss: 11.7292946CurrentTrain: epoch 15, batch     2 | loss: 27.1819312CurrentTrain: epoch 15, batch     3 | loss: 12.9813137CurrentTrain: epoch 15, batch     4 | loss: 21.5049689CurrentTrain: epoch 15, batch     5 | loss: 15.7148574CurrentTrain: epoch 15, batch     6 | loss: 14.0439493CurrentTrain: epoch 15, batch     7 | loss: 14.9280412CurrentTrain: epoch 15, batch     8 | loss: 16.1341758CurrentTrain: epoch 15, batch     9 | loss: 18.6449255CurrentTrain: epoch 15, batch    10 | loss: 26.2532772CurrentTrain: epoch 15, batch    11 | loss: 16.1011200CurrentTrain: epoch 15, batch    12 | loss: 14.4498521CurrentTrain: epoch 15, batch    13 | loss: 17.5386656CurrentTrain: epoch 15, batch    14 | loss: 18.7119601CurrentTrain: epoch 15, batch    15 | loss: 8.5754813CurrentTrain: epoch 15, batch    16 | loss: 13.3719486CurrentTrain: epoch 15, batch    17 | loss: 15.8498913CurrentTrain: epoch 15, batch    18 | loss: 15.7100333CurrentTrain: epoch 15, batch    19 | loss: 12.3947896CurrentTrain: epoch 15, batch    20 | loss: 15.8427036CurrentTrain: epoch 15, batch    21 | loss: 14.5446627CurrentTrain: epoch 15, batch    22 | loss: 16.3936919CurrentTrain: epoch 15, batch    23 | loss: 13.2921991CurrentTrain: epoch 15, batch    24 | loss: 13.8014232CurrentTrain: epoch 15, batch    25 | loss: 12.8170385CurrentTrain: epoch 15, batch    26 | loss: 31.0925790CurrentTrain: epoch 15, batch    27 | loss: 12.6148539CurrentTrain: epoch 15, batch    28 | loss: 11.6170313CurrentTrain: epoch 15, batch    29 | loss: 13.6367768CurrentTrain: epoch 15, batch    30 | loss: 8.9952975CurrentTrain: epoch 15, batch    31 | loss: 17.3644568CurrentTrain: epoch 15, batch    32 | loss: 15.1692635CurrentTrain: epoch 15, batch    33 | loss: 10.2243705CurrentTrain: epoch 15, batch    34 | loss: 10.3501260CurrentTrain: epoch 15, batch    35 | loss: 9.4748488CurrentTrain: epoch 15, batch    36 | loss: 17.8711408CurrentTrain: epoch 15, batch    37 | loss: 20.7793760CurrentTrain: epoch 15, batch    38 | loss: 13.8942412CurrentTrain: epoch 15, batch    39 | loss: 12.0146946CurrentTrain: epoch 15, batch    40 | loss: 11.1439893CurrentTrain: epoch 15, batch    41 | loss: 10.7680638CurrentTrain: epoch 15, batch    42 | loss: 10.7946192CurrentTrain: epoch 15, batch    43 | loss: 8.0481417CurrentTrain: epoch 15, batch    44 | loss: 12.3125099CurrentTrain: epoch 15, batch    45 | loss: 9.1936329CurrentTrain: epoch 15, batch    46 | loss: 17.3291055CurrentTrain: epoch 15, batch    47 | loss: 9.1192089CurrentTrain: epoch 15, batch    48 | loss: 24.6134398CurrentTrain: epoch 15, batch    49 | loss: 11.1008160CurrentTrain: epoch 15, batch    50 | loss: 12.6506327CurrentTrain: epoch 15, batch    51 | loss: 14.0986224CurrentTrain: epoch 15, batch    52 | loss: 10.4990541CurrentTrain: epoch 15, batch    53 | loss: 17.6165991CurrentTrain: epoch 15, batch    54 | loss: 11.5450672CurrentTrain: epoch 15, batch    55 | loss: 24.7844833CurrentTrain: epoch 15, batch    56 | loss: 15.3152134CurrentTrain: epoch 15, batch    57 | loss: 11.3167610CurrentTrain: epoch 15, batch    58 | loss: 8.5532570CurrentTrain: epoch 15, batch    59 | loss: 11.9822615CurrentTrain: epoch 15, batch    60 | loss: 15.3559780CurrentTrain: epoch 15, batch    61 | loss: 12.6973289CurrentTrain: epoch  7, batch    62 | loss: 9.0769721CurrentTrain: epoch 15, batch     0 | loss: 10.8841457CurrentTrain: epoch 15, batch     1 | loss: 12.2772728CurrentTrain: epoch 15, batch     2 | loss: 9.0056933CurrentTrain: epoch 15, batch     3 | loss: 12.4009555CurrentTrain: epoch 15, batch     4 | loss: 12.3718938CurrentTrain: epoch 15, batch     5 | loss: 18.7051291CurrentTrain: epoch 15, batch     6 | loss: 19.8341271CurrentTrain: epoch 15, batch     7 | loss: 26.3690705CurrentTrain: epoch 15, batch     8 | loss: 14.4344005CurrentTrain: epoch 15, batch     9 | loss: 10.5698738CurrentTrain: epoch 15, batch    10 | loss: 10.7719796CurrentTrain: epoch 15, batch    11 | loss: 8.5965198CurrentTrain: epoch 15, batch    12 | loss: 11.0032460CurrentTrain: epoch 15, batch    13 | loss: 11.9513485CurrentTrain: epoch 15, batch    14 | loss: 15.3513341CurrentTrain: epoch 15, batch    15 | loss: 20.5744634CurrentTrain: epoch 15, batch    16 | loss: 10.9642360CurrentTrain: epoch 15, batch    17 | loss: 16.4428674CurrentTrain: epoch 15, batch    18 | loss: 12.8075886CurrentTrain: epoch 15, batch    19 | loss: 14.4260536CurrentTrain: epoch 15, batch    20 | loss: 25.3293645CurrentTrain: epoch 15, batch    21 | loss: 15.0677298CurrentTrain: epoch 15, batch    22 | loss: 11.7001483CurrentTrain: epoch 15, batch    23 | loss: 12.9212733CurrentTrain: epoch 15, batch    24 | loss: 10.8367204CurrentTrain: epoch 15, batch    25 | loss: 9.4529399CurrentTrain: epoch 15, batch    26 | loss: 11.5662411CurrentTrain: epoch 15, batch    27 | loss: 22.9402548CurrentTrain: epoch 15, batch    28 | loss: 9.9758823CurrentTrain: epoch 15, batch    29 | loss: 10.0829956CurrentTrain: epoch 15, batch    30 | loss: 16.2016119CurrentTrain: epoch 15, batch    31 | loss: 9.6715510CurrentTrain: epoch 15, batch    32 | loss: 16.1070010CurrentTrain: epoch 15, batch    33 | loss: 10.4372522CurrentTrain: epoch 15, batch    34 | loss: 13.1785017CurrentTrain: epoch 15, batch    35 | loss: 11.4432688CurrentTrain: epoch 15, batch    36 | loss: 7.5432088CurrentTrain: epoch 15, batch    37 | loss: 18.7810464CurrentTrain: epoch 15, batch    38 | loss: 12.8874884CurrentTrain: epoch 15, batch    39 | loss: 15.9346269CurrentTrain: epoch 15, batch    40 | loss: 11.0877480CurrentTrain: epoch 15, batch    41 | loss: 16.0577769CurrentTrain: epoch 15, batch    42 | loss: 15.9573445CurrentTrain: epoch 15, batch    43 | loss: 16.6765682CurrentTrain: epoch 15, batch    44 | loss: 9.7755196CurrentTrain: epoch 15, batch    45 | loss: 12.4668257CurrentTrain: epoch 15, batch    46 | loss: 14.0348273CurrentTrain: epoch 15, batch    47 | loss: 15.5809953CurrentTrain: epoch 15, batch    48 | loss: 13.6896531CurrentTrain: epoch 15, batch    49 | loss: 13.6779155CurrentTrain: epoch 15, batch    50 | loss: 15.7815971CurrentTrain: epoch 15, batch    51 | loss: 11.3655738CurrentTrain: epoch 15, batch    52 | loss: 16.5486315CurrentTrain: epoch 15, batch    53 | loss: 16.0511091CurrentTrain: epoch 15, batch    54 | loss: 12.5337708CurrentTrain: epoch 15, batch    55 | loss: 11.2672266CurrentTrain: epoch 15, batch    56 | loss: 11.0013575CurrentTrain: epoch 15, batch    57 | loss: 14.1592105CurrentTrain: epoch 15, batch    58 | loss: 12.0390716CurrentTrain: epoch 15, batch    59 | loss: 8.2950750CurrentTrain: epoch 15, batch    60 | loss: 12.1320613CurrentTrain: epoch 15, batch    61 | loss: 8.4693632CurrentTrain: epoch  7, batch    62 | loss: 8.7097232CurrentTrain: epoch 15, batch     0 | loss: 11.1788097CurrentTrain: epoch 15, batch     1 | loss: 10.6736328CurrentTrain: epoch 15, batch     2 | loss: 16.4642196CurrentTrain: epoch 15, batch     3 | loss: 9.4593567CurrentTrain: epoch 15, batch     4 | loss: 13.0559792CurrentTrain: epoch 15, batch     5 | loss: 14.3534914CurrentTrain: epoch 15, batch     6 | loss: 14.4304696CurrentTrain: epoch 15, batch     7 | loss: 11.3236743CurrentTrain: epoch 15, batch     8 | loss: 11.5097170CurrentTrain: epoch 15, batch     9 | loss: 15.2228574CurrentTrain: epoch 15, batch    10 | loss: 19.4753232CurrentTrain: epoch 15, batch    11 | loss: 11.6799050CurrentTrain: epoch 15, batch    12 | loss: 8.9696188CurrentTrain: epoch 15, batch    13 | loss: 11.8673003CurrentTrain: epoch 15, batch    14 | loss: 14.1384734CurrentTrain: epoch 15, batch    15 | loss: 18.1692863CurrentTrain: epoch 15, batch    16 | loss: 14.0694764CurrentTrain: epoch 15, batch    17 | loss: 16.0510550CurrentTrain: epoch 15, batch    18 | loss: 20.3492678CurrentTrain: epoch 15, batch    19 | loss: 17.0601549CurrentTrain: epoch 15, batch    20 | loss: 8.5905646CurrentTrain: epoch 15, batch    21 | loss: 9.4169670CurrentTrain: epoch 15, batch    22 | loss: 14.6158257CurrentTrain: epoch 15, batch    23 | loss: 12.3247291CurrentTrain: epoch 15, batch    24 | loss: 11.0347077CurrentTrain: epoch 15, batch    25 | loss: 9.4999291CurrentTrain: epoch 15, batch    26 | loss: 10.8294548CurrentTrain: epoch 15, batch    27 | loss: 16.2595792CurrentTrain: epoch 15, batch    28 | loss: 12.6542172CurrentTrain: epoch 15, batch    29 | loss: 12.3113218CurrentTrain: epoch 15, batch    30 | loss: 10.5058879CurrentTrain: epoch 15, batch    31 | loss: 11.2967622CurrentTrain: epoch 15, batch    32 | loss: 10.1082664CurrentTrain: epoch 15, batch    33 | loss: 9.6160326CurrentTrain: epoch 15, batch    34 | loss: 8.8622389CurrentTrain: epoch 15, batch    35 | loss: 10.3922671CurrentTrain: epoch 15, batch    36 | loss: 10.1738494CurrentTrain: epoch 15, batch    37 | loss: 11.7150130CurrentTrain: epoch 15, batch    38 | loss: 9.5441670CurrentTrain: epoch 15, batch    39 | loss: 13.4627749CurrentTrain: epoch 15, batch    40 | loss: 14.4769237CurrentTrain: epoch 15, batch    41 | loss: 11.3301926CurrentTrain: epoch 15, batch    42 | loss: 9.3865103CurrentTrain: epoch 15, batch    43 | loss: 37.3544116CurrentTrain: epoch 15, batch    44 | loss: 15.1983304CurrentTrain: epoch 15, batch    45 | loss: 15.3385045CurrentTrain: epoch 15, batch    46 | loss: 18.5377362CurrentTrain: epoch 15, batch    47 | loss: 9.9502892CurrentTrain: epoch 15, batch    48 | loss: 26.6160884CurrentTrain: epoch 15, batch    49 | loss: 25.3025496CurrentTrain: epoch 15, batch    50 | loss: 9.7396947CurrentTrain: epoch 15, batch    51 | loss: 8.9146782CurrentTrain: epoch 15, batch    52 | loss: 7.4033972CurrentTrain: epoch 15, batch    53 | loss: 14.0683249CurrentTrain: epoch 15, batch    54 | loss: 11.5582984CurrentTrain: epoch 15, batch    55 | loss: 9.9313992CurrentTrain: epoch 15, batch    56 | loss: 8.7202891CurrentTrain: epoch 15, batch    57 | loss: 21.1021865CurrentTrain: epoch 15, batch    58 | loss: 11.7868570CurrentTrain: epoch 15, batch    59 | loss: 15.6156975CurrentTrain: epoch 15, batch    60 | loss: 11.6284544CurrentTrain: epoch 15, batch    61 | loss: 10.0962884CurrentTrain: epoch  7, batch    62 | loss: 10.7845777CurrentTrain: epoch 15, batch     0 | loss: 15.7420375CurrentTrain: epoch 15, batch     1 | loss: 11.0892198CurrentTrain: epoch 15, batch     2 | loss: 9.3895952CurrentTrain: epoch 15, batch     3 | loss: 9.0316453CurrentTrain: epoch 15, batch     4 | loss: 9.8518310CurrentTrain: epoch 15, batch     5 | loss: 11.5957055CurrentTrain: epoch 15, batch     6 | loss: 10.0560937CurrentTrain: epoch 15, batch     7 | loss: 9.8017351CurrentTrain: epoch 15, batch     8 | loss: 17.7245177CurrentTrain: epoch 15, batch     9 | loss: 13.8365632CurrentTrain: epoch 15, batch    10 | loss: 11.8034568CurrentTrain: epoch 15, batch    11 | loss: 20.3297404CurrentTrain: epoch 15, batch    12 | loss: 8.9203921CurrentTrain: epoch 15, batch    13 | loss: 10.5919406CurrentTrain: epoch 15, batch    14 | loss: 9.1228482CurrentTrain: epoch 15, batch    15 | loss: 16.2273908CurrentTrain: epoch 15, batch    16 | loss: 7.9631851CurrentTrain: epoch 15, batch    17 | loss: 12.9464361CurrentTrain: epoch 15, batch    18 | loss: 16.9867732CurrentTrain: epoch 15, batch    19 | loss: 9.7282938CurrentTrain: epoch 15, batch    20 | loss: 8.2553775CurrentTrain: epoch 15, batch    21 | loss: 19.8229953CurrentTrain: epoch 15, batch    22 | loss: 23.9911318CurrentTrain: epoch 15, batch    23 | loss: 16.7841142CurrentTrain: epoch 15, batch    24 | loss: 16.4343192CurrentTrain: epoch 15, batch    25 | loss: 6.4167959CurrentTrain: epoch 15, batch    26 | loss: 11.8870229CurrentTrain: epoch 15, batch    27 | loss: 10.9697807CurrentTrain: epoch 15, batch    28 | loss: 15.3265086CurrentTrain: epoch 15, batch    29 | loss: 11.1277167CurrentTrain: epoch 15, batch    30 | loss: 9.7018613CurrentTrain: epoch 15, batch    31 | loss: 13.7484703CurrentTrain: epoch 15, batch    32 | loss: 9.2135083CurrentTrain: epoch 15, batch    33 | loss: 25.0821373CurrentTrain: epoch 15, batch    34 | loss: 12.3420001CurrentTrain: epoch 15, batch    35 | loss: 16.4044532CurrentTrain: epoch 15, batch    36 | loss: 11.7490511CurrentTrain: epoch 15, batch    37 | loss: 6.9112781CurrentTrain: epoch 15, batch    38 | loss: 9.3281170CurrentTrain: epoch 15, batch    39 | loss: 8.2500921CurrentTrain: epoch 15, batch    40 | loss: 16.4265058CurrentTrain: epoch 15, batch    41 | loss: 15.7012662CurrentTrain: epoch 15, batch    42 | loss: 9.6801494CurrentTrain: epoch 15, batch    43 | loss: 11.3369460CurrentTrain: epoch 15, batch    44 | loss: 16.0418226CurrentTrain: epoch 15, batch    45 | loss: 8.9969587CurrentTrain: epoch 15, batch    46 | loss: 11.9244816CurrentTrain: epoch 15, batch    47 | loss: 8.5454598CurrentTrain: epoch 15, batch    48 | loss: 10.0779446CurrentTrain: epoch 15, batch    49 | loss: 10.6124860CurrentTrain: epoch 15, batch    50 | loss: 15.7749374CurrentTrain: epoch 15, batch    51 | loss: 8.1566962CurrentTrain: epoch 15, batch    52 | loss: 11.3201370CurrentTrain: epoch 15, batch    53 | loss: 16.4399479CurrentTrain: epoch 15, batch    54 | loss: 10.5261383CurrentTrain: epoch 15, batch    55 | loss: 13.6591701CurrentTrain: epoch 15, batch    56 | loss: 9.1609233CurrentTrain: epoch 15, batch    57 | loss: 10.5685899CurrentTrain: epoch 15, batch    58 | loss: 9.2958092CurrentTrain: epoch 15, batch    59 | loss: 9.3382487CurrentTrain: epoch 15, batch    60 | loss: 10.1945228CurrentTrain: epoch 15, batch    61 | loss: 11.7490941CurrentTrain: epoch  7, batch    62 | loss: 10.3900440CurrentTrain: epoch 15, batch     0 | loss: 13.4561060CurrentTrain: epoch 15, batch     1 | loss: 9.0184314CurrentTrain: epoch 15, batch     2 | loss: 7.7592890CurrentTrain: epoch 15, batch     3 | loss: 9.8820543CurrentTrain: epoch 15, batch     4 | loss: 9.1891640CurrentTrain: epoch 15, batch     5 | loss: 12.7123324CurrentTrain: epoch 15, batch     6 | loss: 10.0655987CurrentTrain: epoch 15, batch     7 | loss: 20.0705895CurrentTrain: epoch 15, batch     8 | loss: 10.1911225CurrentTrain: epoch 15, batch     9 | loss: 9.1234368CurrentTrain: epoch 15, batch    10 | loss: 9.2273628CurrentTrain: epoch 15, batch    11 | loss: 14.0010125CurrentTrain: epoch 15, batch    12 | loss: 15.4942147CurrentTrain: epoch 15, batch    13 | loss: 11.5912467CurrentTrain: epoch 15, batch    14 | loss: 15.9297454CurrentTrain: epoch 15, batch    15 | loss: 12.1256108CurrentTrain: epoch 15, batch    16 | loss: 12.9120568CurrentTrain: epoch 15, batch    17 | loss: 9.5887626CurrentTrain: epoch 15, batch    18 | loss: 11.1196061CurrentTrain: epoch 15, batch    19 | loss: 11.5872943CurrentTrain: epoch 15, batch    20 | loss: 9.8366792CurrentTrain: epoch 15, batch    21 | loss: 10.2670348CurrentTrain: epoch 15, batch    22 | loss: 10.0873729CurrentTrain: epoch 15, batch    23 | loss: 11.9862795CurrentTrain: epoch 15, batch    24 | loss: 10.3580055CurrentTrain: epoch 15, batch    25 | loss: 14.8341441CurrentTrain: epoch 15, batch    26 | loss: 6.9790936CurrentTrain: epoch 15, batch    27 | loss: 8.9255619CurrentTrain: epoch 15, batch    28 | loss: 11.9974746CurrentTrain: epoch 15, batch    29 | loss: 13.3004473CurrentTrain: epoch 15, batch    30 | loss: 15.3419803CurrentTrain: epoch 15, batch    31 | loss: 12.7230223CurrentTrain: epoch 15, batch    32 | loss: 13.7627225CurrentTrain: epoch 15, batch    33 | loss: 14.8748325CurrentTrain: epoch 15, batch    34 | loss: 14.3064348CurrentTrain: epoch 15, batch    35 | loss: 14.8927937CurrentTrain: epoch 15, batch    36 | loss: 6.2418350CurrentTrain: epoch 15, batch    37 | loss: 13.3858865CurrentTrain: epoch 15, batch    38 | loss: 8.6928136CurrentTrain: epoch 15, batch    39 | loss: 19.7427019CurrentTrain: epoch 15, batch    40 | loss: 7.6842379CurrentTrain: epoch 15, batch    41 | loss: 14.7218588CurrentTrain: epoch 15, batch    42 | loss: 6.6501876CurrentTrain: epoch 15, batch    43 | loss: 8.7913707CurrentTrain: epoch 15, batch    44 | loss: 14.4229708CurrentTrain: epoch 15, batch    45 | loss: 9.3178965CurrentTrain: epoch 15, batch    46 | loss: 12.3956866CurrentTrain: epoch 15, batch    47 | loss: 10.7988363CurrentTrain: epoch 15, batch    48 | loss: 8.3588274CurrentTrain: epoch 15, batch    49 | loss: 9.6992218CurrentTrain: epoch 15, batch    50 | loss: 7.4114945CurrentTrain: epoch 15, batch    51 | loss: 29.4633935CurrentTrain: epoch 15, batch    52 | loss: 7.9624319CurrentTrain: epoch 15, batch    53 | loss: 15.8500347CurrentTrain: epoch 15, batch    54 | loss: 12.0896793CurrentTrain: epoch 15, batch    55 | loss: 9.1663101CurrentTrain: epoch 15, batch    56 | loss: 14.2333609CurrentTrain: epoch 15, batch    57 | loss: 7.6897491CurrentTrain: epoch 15, batch    58 | loss: 15.6509353CurrentTrain: epoch 15, batch    59 | loss: 8.5541154CurrentTrain: epoch 15, batch    60 | loss: 14.5277694CurrentTrain: epoch 15, batch    61 | loss: 15.6696308CurrentTrain: epoch  7, batch    62 | loss: 7.9354726CurrentTrain: epoch 15, batch     0 | loss: 14.9277451CurrentTrain: epoch 15, batch     1 | loss: 16.9504021CurrentTrain: epoch 15, batch     2 | loss: 9.6632254CurrentTrain: epoch 15, batch     3 | loss: 6.1637106CurrentTrain: epoch 15, batch     4 | loss: 9.4935197CurrentTrain: epoch 15, batch     5 | loss: 14.0023261CurrentTrain: epoch 15, batch     6 | loss: 8.6291030CurrentTrain: epoch 15, batch     7 | loss: 14.7469015CurrentTrain: epoch 15, batch     8 | loss: 9.0099338CurrentTrain: epoch 15, batch     9 | loss: 7.4070095CurrentTrain: epoch 15, batch    10 | loss: 14.1169816CurrentTrain: epoch 15, batch    11 | loss: 10.0504770CurrentTrain: epoch 15, batch    12 | loss: 11.0947075CurrentTrain: epoch 15, batch    13 | loss: 6.8755454CurrentTrain: epoch 15, batch    14 | loss: 16.2784324CurrentTrain: epoch 15, batch    15 | loss: 17.0908435CurrentTrain: epoch 15, batch    16 | loss: 19.2821754CurrentTrain: epoch 15, batch    17 | loss: 9.6257767CurrentTrain: epoch 15, batch    18 | loss: 7.8214302CurrentTrain: epoch 15, batch    19 | loss: 8.6133657CurrentTrain: epoch 15, batch    20 | loss: 12.5424143CurrentTrain: epoch 15, batch    21 | loss: 10.9132142CurrentTrain: epoch 15, batch    22 | loss: 15.0688710CurrentTrain: epoch 15, batch    23 | loss: 9.6286718CurrentTrain: epoch 15, batch    24 | loss: 15.8457503CurrentTrain: epoch 15, batch    25 | loss: 7.0939373CurrentTrain: epoch 15, batch    26 | loss: 13.8924278CurrentTrain: epoch 15, batch    27 | loss: 9.0952251CurrentTrain: epoch 15, batch    28 | loss: 8.8249121CurrentTrain: epoch 15, batch    29 | loss: 10.5335852CurrentTrain: epoch 15, batch    30 | loss: 9.5515179CurrentTrain: epoch 15, batch    31 | loss: 8.9637544CurrentTrain: epoch 15, batch    32 | loss: 13.8991134CurrentTrain: epoch 15, batch    33 | loss: 9.3709690CurrentTrain: epoch 15, batch    34 | loss: 9.4535636CurrentTrain: epoch 15, batch    35 | loss: 18.1167909CurrentTrain: epoch 15, batch    36 | loss: 6.8577061CurrentTrain: epoch 15, batch    37 | loss: 10.8987085CurrentTrain: epoch 15, batch    38 | loss: 6.5515558CurrentTrain: epoch 15, batch    39 | loss: 8.4808516CurrentTrain: epoch 15, batch    40 | loss: 15.3728352CurrentTrain: epoch 15, batch    41 | loss: 11.4861367CurrentTrain: epoch 15, batch    42 | loss: 18.5275909CurrentTrain: epoch 15, batch    43 | loss: 5.6522634CurrentTrain: epoch 15, batch    44 | loss: 14.2622510CurrentTrain: epoch 15, batch    45 | loss: 8.8686043CurrentTrain: epoch 15, batch    46 | loss: 14.8534626CurrentTrain: epoch 15, batch    47 | loss: 13.5786221CurrentTrain: epoch 15, batch    48 | loss: 10.6891388CurrentTrain: epoch 15, batch    49 | loss: 11.7038338CurrentTrain: epoch 15, batch    50 | loss: 13.6134821CurrentTrain: epoch 15, batch    51 | loss: 9.2121843CurrentTrain: epoch 15, batch    52 | loss: 15.0003862CurrentTrain: epoch 15, batch    53 | loss: 11.9099468CurrentTrain: epoch 15, batch    54 | loss: 14.6051102CurrentTrain: epoch 15, batch    55 | loss: 8.5226253CurrentTrain: epoch 15, batch    56 | loss: 8.8864555CurrentTrain: epoch 15, batch    57 | loss: 7.7297462CurrentTrain: epoch 15, batch    58 | loss: 9.9572688CurrentTrain: epoch 15, batch    59 | loss: 8.4960730CurrentTrain: epoch 15, batch    60 | loss: 8.5633191CurrentTrain: epoch 15, batch    61 | loss: 13.6334061CurrentTrain: epoch  7, batch    62 | loss: 7.5325567CurrentTrain: epoch 15, batch     0 | loss: 9.5378581CurrentTrain: epoch 15, batch     1 | loss: 7.7111919CurrentTrain: epoch 15, batch     2 | loss: 32.6194408CurrentTrain: epoch 15, batch     3 | loss: 8.2467625CurrentTrain: epoch 15, batch     4 | loss: 14.7378997CurrentTrain: epoch 15, batch     5 | loss: 10.2076579CurrentTrain: epoch 15, batch     6 | loss: 7.2870010CurrentTrain: epoch 15, batch     7 | loss: 12.2954902CurrentTrain: epoch 15, batch     8 | loss: 12.7856885CurrentTrain: epoch 15, batch     9 | loss: 25.2813115CurrentTrain: epoch 15, batch    10 | loss: 8.6556693CurrentTrain: epoch 15, batch    11 | loss: 16.0826066CurrentTrain: epoch 15, batch    12 | loss: 17.2704425CurrentTrain: epoch 15, batch    13 | loss: 7.8059200CurrentTrain: epoch 15, batch    14 | loss: 9.5492505CurrentTrain: epoch 15, batch    15 | loss: 11.4209549CurrentTrain: epoch 15, batch    16 | loss: 12.8124176CurrentTrain: epoch 15, batch    17 | loss: 7.6365086CurrentTrain: epoch 15, batch    18 | loss: 6.6525871CurrentTrain: epoch 15, batch    19 | loss: 10.6002388CurrentTrain: epoch 15, batch    20 | loss: 13.8401758CurrentTrain: epoch 15, batch    21 | loss: 13.9417925CurrentTrain: epoch 15, batch    22 | loss: 8.3179946CurrentTrain: epoch 15, batch    23 | loss: 16.2913603CurrentTrain: epoch 15, batch    24 | loss: 8.4293966CurrentTrain: epoch 15, batch    25 | loss: 7.6359761CurrentTrain: epoch 15, batch    26 | loss: 10.1016152CurrentTrain: epoch 15, batch    27 | loss: 15.6563739CurrentTrain: epoch 15, batch    28 | loss: 7.5114464CurrentTrain: epoch 15, batch    29 | loss: 9.8985280CurrentTrain: epoch 15, batch    30 | loss: 6.5202825CurrentTrain: epoch 15, batch    31 | loss: 8.7094120CurrentTrain: epoch 15, batch    32 | loss: 12.5009765CurrentTrain: epoch 15, batch    33 | loss: 9.3313500CurrentTrain: epoch 15, batch    34 | loss: 10.3116018CurrentTrain: epoch 15, batch    35 | loss: 8.9385101CurrentTrain: epoch 15, batch    36 | loss: 10.2419751CurrentTrain: epoch 15, batch    37 | loss: 8.1574239CurrentTrain: epoch 15, batch    38 | loss: 6.8494606CurrentTrain: epoch 15, batch    39 | loss: 7.5513054CurrentTrain: epoch 15, batch    40 | loss: 9.2046972CurrentTrain: epoch 15, batch    41 | loss: 23.2141883CurrentTrain: epoch 15, batch    42 | loss: 13.5979455CurrentTrain: epoch 15, batch    43 | loss: 7.9658671CurrentTrain: epoch 15, batch    44 | loss: 21.8115181CurrentTrain: epoch 15, batch    45 | loss: 8.2703646CurrentTrain: epoch 15, batch    46 | loss: 9.4679674CurrentTrain: epoch 15, batch    47 | loss: 8.3492351CurrentTrain: epoch 15, batch    48 | loss: 12.3427571CurrentTrain: epoch 15, batch    49 | loss: 11.7176293CurrentTrain: epoch 15, batch    50 | loss: 5.8837971CurrentTrain: epoch 15, batch    51 | loss: 10.2766807CurrentTrain: epoch 15, batch    52 | loss: 10.7291124CurrentTrain: epoch 15, batch    53 | loss: 10.9499094CurrentTrain: epoch 15, batch    54 | loss: 8.2721034CurrentTrain: epoch 15, batch    55 | loss: 10.4903045CurrentTrain: epoch 15, batch    56 | loss: 6.4831126CurrentTrain: epoch 15, batch    57 | loss: 14.6579490CurrentTrain: epoch 15, batch    58 | loss: 12.3205042CurrentTrain: epoch 15, batch    59 | loss: 8.9631468CurrentTrain: epoch 15, batch    60 | loss: 6.4348466CurrentTrain: epoch 15, batch    61 | loss: 14.4188206CurrentTrain: epoch  7, batch    62 | loss: 4.2764772CurrentTrain: epoch 15, batch     0 | loss: 7.3114355CurrentTrain: epoch 15, batch     1 | loss: 10.6790801CurrentTrain: epoch 15, batch     2 | loss: 11.1559083CurrentTrain: epoch 15, batch     3 | loss: 13.3387367CurrentTrain: epoch 15, batch     4 | loss: 13.4927451CurrentTrain: epoch 15, batch     5 | loss: 9.1360994CurrentTrain: epoch 15, batch     6 | loss: 13.9160284CurrentTrain: epoch 15, batch     7 | loss: 8.5994084CurrentTrain: epoch 15, batch     8 | loss: 8.1701007CurrentTrain: epoch 15, batch     9 | loss: 9.5643038CurrentTrain: epoch 15, batch    10 | loss: 17.2680405CurrentTrain: epoch 15, batch    11 | loss: 9.2341050CurrentTrain: epoch 15, batch    12 | loss: 16.0234530CurrentTrain: epoch 15, batch    13 | loss: 10.0455860CurrentTrain: epoch 15, batch    14 | loss: 8.6736162CurrentTrain: epoch 15, batch    15 | loss: 15.8605300CurrentTrain: epoch 15, batch    16 | loss: 5.8877170CurrentTrain: epoch 15, batch    17 | loss: 15.5200749CurrentTrain: epoch 15, batch    18 | loss: 10.2157485CurrentTrain: epoch 15, batch    19 | loss: 14.1287761CurrentTrain: epoch 15, batch    20 | loss: 7.8716705CurrentTrain: epoch 15, batch    21 | loss: 10.0126167CurrentTrain: epoch 15, batch    22 | loss: 11.9310154CurrentTrain: epoch 15, batch    23 | loss: 7.0151881CurrentTrain: epoch 15, batch    24 | loss: 8.0241668CurrentTrain: epoch 15, batch    25 | loss: 12.2698342CurrentTrain: epoch 15, batch    26 | loss: 10.9800670CurrentTrain: epoch 15, batch    27 | loss: 12.1206356CurrentTrain: epoch 15, batch    28 | loss: 7.7255532CurrentTrain: epoch 15, batch    29 | loss: 7.4871625CurrentTrain: epoch 15, batch    30 | loss: 9.5314083CurrentTrain: epoch 15, batch    31 | loss: 7.3435729CurrentTrain: epoch 15, batch    32 | loss: 7.4741599CurrentTrain: epoch 15, batch    33 | loss: 12.3299042CurrentTrain: epoch 15, batch    34 | loss: 8.6614030CurrentTrain: epoch 15, batch    35 | loss: 8.3529936CurrentTrain: epoch 15, batch    36 | loss: 11.6131698CurrentTrain: epoch 15, batch    37 | loss: 12.8922636CurrentTrain: epoch 15, batch    38 | loss: 8.3532339CurrentTrain: epoch 15, batch    39 | loss: 16.0110450CurrentTrain: epoch 15, batch    40 | loss: 14.0871348CurrentTrain: epoch 15, batch    41 | loss: 10.6776668CurrentTrain: epoch 15, batch    42 | loss: 6.7717591CurrentTrain: epoch 15, batch    43 | loss: 9.7404785CurrentTrain: epoch 15, batch    44 | loss: 14.0895334CurrentTrain: epoch 15, batch    45 | loss: 14.3431016CurrentTrain: epoch 15, batch    46 | loss: 17.3814523CurrentTrain: epoch 15, batch    47 | loss: 15.4203185CurrentTrain: epoch 15, batch    48 | loss: 6.4981806CurrentTrain: epoch 15, batch    49 | loss: 8.5209778CurrentTrain: epoch 15, batch    50 | loss: 25.1263463CurrentTrain: epoch 15, batch    51 | loss: 22.0701996CurrentTrain: epoch 15, batch    52 | loss: 16.2477350CurrentTrain: epoch 15, batch    53 | loss: 10.1959011CurrentTrain: epoch 15, batch    54 | loss: 14.0919711CurrentTrain: epoch 15, batch    55 | loss: 10.3533719CurrentTrain: epoch 15, batch    56 | loss: 16.5646684CurrentTrain: epoch 15, batch    57 | loss: 6.9169889CurrentTrain: epoch 15, batch    58 | loss: 13.3395920CurrentTrain: epoch 15, batch    59 | loss: 7.1688673CurrentTrain: epoch 15, batch    60 | loss: 7.6092425CurrentTrain: epoch 15, batch    61 | loss: 7.1458552CurrentTrain: epoch  7, batch    62 | loss: 4.2530799
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: the large workshops of peter paul rubens and jacob jordaens , and the influence of anthony van dyck , made antwerp the center of the flemish baroque .
Head Entity: jacob jordaens
Tail Entity: flemish baroque
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: The emergence of surrealism in the early 20th century was significantly influenced by the works of Salvador Dalí and André Breton.  
Head Entity: Salvador Dalí  
Tail Entity: surrealism  

Relation: movement  
Context: The impact of the Harlem Renaissance on American literature and culture can be seen through the works of Langston Hughes and Zora Neale Hurston.  
Head Entity: Langston Hughes  
Tail Entity: Harlem Renaissance  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: lyons hill is a restored village , and former parish with church , now part of the community of ardclough in north county kildare .
Head Entity: ardclough
Tail Entity: county kildare
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: The city of San Francisco is known for its iconic Golden Gate Bridge and is part of the larger San Francisco County in California.  
Head Entity: San Francisco  
Tail Entity: San Francisco County  

Relation: located in the administrative territorial entity  
Context: The village of Haverfordwest serves as the county town of Pembrokeshire, situated in the picturesque region of Wales.  
Head Entity: Haverfordwest  
Tail Entity: Pembrokeshire  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the nation as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: in october 1913 , jessie fauset wrote to joel spingarn , who was deeply involved with the naacp , explaining lee ’s situation .
Head Entity: joel spingarn
Tail Entity: naacp
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: during her time at the university, sarah joined the debate team, where she honed her skills and made lasting friendships with fellow members.  
Head Entity: sarah  
Tail Entity: debate team  

Relation: member of  
Context: after years of dedication to the sport, michael was proud to be recognized as a member of the national soccer league, representing his hometown.  
Head Entity: michael  
Tail Entity: national soccer league  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: the bosideng bridge ( ) is a highway bridge over the yangtze river in hejiang county , sichuan , china .
Head Entity: bosideng bridge
Tail Entity: yangtze river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: the golden gate bridge ( ) spans the golden gate strait, connecting san francisco to marin county in california.  
Head Entity: golden gate bridge  
Tail Entity: golden gate strait  

Relation: crosses  
Context: the channel tunnel ( ) runs beneath the english channel, linking the united kingdom with mainland europe.  
Head Entity: channel tunnel  
Tail Entity: english channel  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: it is on the panj river just downstream from the mouth of the bartang river at the point where the panj briefly turns west before resuming its northerly course .
Head Entity: panj river
Tail Entity: bartang river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The Missouri River, which is a major tributary of the Mississippi River, flows through several states before joining the larger watercourse.  
Head Entity: Missouri River  
Tail Entity: Mississippi River  

Relation: tributary  
Context: The Rio Grande serves as a natural border between the United States and Mexico, and it has numerous tributaries that contribute to its flow, including the Pecos River.  
Head Entity: Rio Grande  
Tail Entity: Pecos River  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: in september 2001 , mimica became minister of european integration under prime minister ivica račan , position he held until december 2003 .
Head Entity: ivica račan
Tail Entity: prime minister
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: after serving as the mayor of new york city for eight years, michael bloomberg announced his candidacy for a third term, a position he held from 2002 to 2013.  
Head Entity: michael bloomberg  
Tail Entity: mayor of new york city  

Relation: position held  
Context: in 2015, angela merkel was re-elected as chancellor of germany, a position she held since 2005, making her one of the longest-serving leaders in the country’s history.  
Head Entity: angela merkel  
Tail Entity: chancellor of germany  
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: " jedi outcast " was developed by raven software and powered by the i d tech 3 game engine .
Head Entity: jedi outcast
Tail Entity: raven software
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: "The Witcher 3: Wild Hunt" was developed by CD Projekt Red and has received numerous awards for its storytelling and gameplay.  
Head Entity: The Witcher 3: Wild Hunt  
Tail Entity: CD Projekt Red  

Relation: developer  
Context: "Minecraft" was developed by Mojang Studios and has become one of the best-selling video games of all time.  
Head Entity: Minecraft  
Tail Entity: Mojang Studios  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: holy molar are a san diego based band formed in 2001 .
Head Entity: holy molar
Tail Entity: san diego
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the tech startup was established in silicon valley in 2015.  
Head Entity: the tech startup  
Tail Entity: silicon valley  

Relation: location of formation  
Context: the famous rock band was formed in new york city during the summer of 1999.  
Head Entity: the famous rock band  
Tail Entity: new york city  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: she has appeared in some american television shows , beginning with her regular role as snow white 's stepmother , evil queen lillian " lily " white in the series " the charmings " .
Head Entity: the charmings
Tail Entity: american
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous dish sushi is traditionally made with vinegared rice and often includes seafood, vegetables, and occasionally tropical fruits, originating from Japan.  
Head Entity: sushi  
Tail Entity: Japan  

Relation: country of origin  
Context: The iconic painting "Mona Lisa," known for its captivating smile, was created by the renowned artist Leonardo da Vinci in Italy during the Renaissance period.  
Head Entity: Mona Lisa  
Tail Entity: Italy  
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.91%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.01%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.43%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.90%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.21%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 93.49%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.51%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.52%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.14%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.32%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.46%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.62%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 94.90%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 95.35%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.45%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.65%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 95.70%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.67%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.64%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.72%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.45%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.42%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.39%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.37%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 95.52%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.59%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.84%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.91%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.01%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.43%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.90%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.21%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 93.49%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.51%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.52%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.14%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.32%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.46%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.62%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 94.90%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 95.35%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.45%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.65%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 95.70%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.67%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.64%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.72%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.45%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.42%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.39%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.37%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 95.52%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.59%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.84%   
cur_acc:  ['0.9484']
his_acc:  ['0.9484']
CurrentTrain: epoch 15, batch     0 | loss: 13.9556657CurrentTrain: epoch 15, batch     1 | loss: 10.5951721CurrentTrain: epoch 15, batch     2 | loss: 12.4419268CurrentTrain: epoch  1, batch     3 | loss: 10.2232834CurrentTrain: epoch 15, batch     0 | loss: 18.3173562CurrentTrain: epoch 15, batch     1 | loss: 9.8995475CurrentTrain: epoch 15, batch     2 | loss: 13.7447494CurrentTrain: epoch  1, batch     3 | loss: 6.4956661CurrentTrain: epoch 15, batch     0 | loss: 8.1025758CurrentTrain: epoch 15, batch     1 | loss: 9.8186403CurrentTrain: epoch 15, batch     2 | loss: 13.1817013CurrentTrain: epoch  1, batch     3 | loss: 11.8707783CurrentTrain: epoch 15, batch     0 | loss: 9.6270371CurrentTrain: epoch 15, batch     1 | loss: 11.9217054CurrentTrain: epoch 15, batch     2 | loss: 10.2370382CurrentTrain: epoch  1, batch     3 | loss: 6.6060705CurrentTrain: epoch 15, batch     0 | loss: 10.6769759CurrentTrain: epoch 15, batch     1 | loss: 10.0613972CurrentTrain: epoch 15, batch     2 | loss: 5.6224149CurrentTrain: epoch  1, batch     3 | loss: 5.4607751CurrentTrain: epoch 15, batch     0 | loss: 7.9152895CurrentTrain: epoch 15, batch     1 | loss: 6.2038043CurrentTrain: epoch 15, batch     2 | loss: 11.3728030CurrentTrain: epoch  1, batch     3 | loss: 5.6032832CurrentTrain: epoch 15, batch     0 | loss: 12.2554103CurrentTrain: epoch 15, batch     1 | loss: 7.2504616CurrentTrain: epoch 15, batch     2 | loss: 8.9680651CurrentTrain: epoch  1, batch     3 | loss: 6.1252125CurrentTrain: epoch 15, batch     0 | loss: 11.8796495CurrentTrain: epoch 15, batch     1 | loss: 10.3282580CurrentTrain: epoch 15, batch     2 | loss: 14.1855296CurrentTrain: epoch  1, batch     3 | loss: 5.9778832CurrentTrain: epoch 15, batch     0 | loss: 6.4781354CurrentTrain: epoch 15, batch     1 | loss: 7.1925352CurrentTrain: epoch 15, batch     2 | loss: 6.0408789CurrentTrain: epoch  1, batch     3 | loss: 5.9393781CurrentTrain: epoch 15, batch     0 | loss: 6.3528615CurrentTrain: epoch 15, batch     1 | loss: 8.2018211CurrentTrain: epoch 15, batch     2 | loss: 4.6187036CurrentTrain: epoch  1, batch     3 | loss: 6.7705206
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: alongside evan durbin and hugh gaitskell , he brought the thinking of john maynard keynes to the labour party , especially in relation to price determination .
Head Entity: hugh gaitskell
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: After serving as the mayor of Chicago, he became a prominent figure in the Democratic Party, advocating for social justice and economic reform.  
Head Entity: mayor of Chicago  
Tail Entity: Democratic Party  

Relation: member of political party  
Context: Throughout her career, she has been a staunch advocate for environmental policies as a member of the Green Party, pushing for sustainable practices at both local and national levels.  
Head Entity: she  
Tail Entity: Green Party  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: the original story of real life escape of betty mahmoody is depicted in the movie " not without my daughter " which itself was based on betty mahmoody 's book of the same name .
Head Entity: not without my daughter
Tail Entity: betty mahmoody
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film "the great gatsby" draws heavily from f. scott fitzgerald's classic novel, capturing the essence of the roaring twenties and the complexities of love and ambition.  
Head Entity: the great gatsby  
Tail Entity: f. scott fitzgerald  

Relation: after a work by  
Context: the animated feature "the lion king" was inspired by shakespeare's play "hamlet," incorporating themes of betrayal, revenge, and the struggle for power.  
Head Entity: the lion king  
Tail Entity: shakespeare
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: the influential victorian art critic john ruskin expounded theories about social justice in " unto this last " ( 1860 ) .
Head Entity: john ruskin
Tail Entity: unto this last
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: the renowned physicist albert einstein published his groundbreaking paper on the theory of relativity in "on the electrodynamics of moving bodies" (1905).  
Head Entity: albert einstein  
Tail Entity: on the electrodynamics of moving bodies  

Relation: notable work  
Context: the celebrated author j.k. rowling captivated readers worldwide with her first book in the harry potter series, "harry potter and the philosopher's stone" (1997).  
Head Entity: j.k. rowling  
Tail Entity: harry potter and the philosopher's stone  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: at the request of the stockholm institute for bible translation , mirza khazar translated the bible – the new testament and old testament – into the azerbaijani language .
Head Entity: institute for bible translation
Tail Entity: stockholm
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: the global technology company, headquartered in san francisco, has been a leader in software development for over two decades.  
Head Entity: global technology company  
Tail Entity: san francisco  

Relation: headquarters location  
Context: after relocating its main office to new york city, the startup saw a significant increase in investment and growth opportunities.  
Head Entity: startup  
Tail Entity: new york city  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most recognized as the wolf, which belongs to the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of plants, the family Rosaceae encompasses a variety of species, including roses and strawberries, which are categorized under the order Rosales.  
Head Entity: Rosaceae  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: he later collaborated with vito volterra and explored laplace transforms and other parts of functional analysis .
Head Entity: vito volterra
Tail Entity: functional analysis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: Marie Curie conducted pioneering research on radioactivity, which laid the groundwork for advancements in nuclear physics.  
Head Entity: Marie Curie  
Tail Entity: nuclear physics  

Relation: field of work  
Context: The company specializes in artificial intelligence and machine learning, providing innovative solutions for various industries.  
Head Entity: The company  
Tail Entity: artificial intelligence
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: this early involvement in radio led him to buy many small , under performing radio stations beginning with wcos in columbia , south carolina , and turn them around with new formats and improved management .
Head Entity: wcos
Tail Entity: columbia , south carolina
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: The new television station, KXYZ, has been granted permission to broadcast to the entire metropolitan area of San Francisco, ensuring that residents can access a variety of programming.  
Head Entity: KXYZ  
Tail Entity: San Francisco  

Relation: licensed to broadcast to  
Context: After acquiring the rights, the radio station WABC was officially licensed to broadcast to the listeners in the greater New York City area, expanding its reach significantly.  
Head Entity: WABC  
Tail Entity: greater New York City area  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: ngc 192 is a type sba spiral galaxy located in the constellation cetus .
Head Entity: ngc 192
Tail Entity: cetus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: the orion nebula is located in the constellation orion and is one of the brightest nebulae visible to the naked eye.  
Head Entity: orion nebula  
Tail Entity: orion  

Relation: constellation  
Context: the star betelgeuse is a prominent feature in the constellation orion, known for its reddish hue.  
Head Entity: betelgeuse  
Tail Entity: orion  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: the dallas - fort worth metroplex 's major international airport , dallas - fort worth international airport , is located a short distance to the northwest .
Head Entity: dallas - fort worth international airport
Tail Entity: dallas - fort worth metroplex
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: the bustling city of los angeles is served by several major airports, including los angeles international airport, which connects travelers to destinations worldwide.  
Head Entity: los angeles international airport  
Tail Entity: los angeles  

Relation: place served by transport hub  
Context: the central train station in berlin, berlin hauptbahnhof, serves as a key transport hub for the city and connects various regional and international train services.  
Head Entity: berlin hauptbahnhof  
Tail Entity: berlin  
MemoryTrain:  epoch 15, batch     0 | loss: 7.3685150MemoryTrain:  epoch 15, batch     1 | loss: 7.3889217MemoryTrain:  epoch 15, batch     2 | loss: 4.3066649MemoryTrain:  epoch 11, batch     3 | loss: 7.5480476MemoryTrain:  epoch 15, batch     0 | loss: 7.2530575MemoryTrain:  epoch 15, batch     1 | loss: 7.2784157MemoryTrain:  epoch 15, batch     2 | loss: 7.6201697MemoryTrain:  epoch 11, batch     3 | loss: 5.5304095MemoryTrain:  epoch 15, batch     0 | loss: 4.2331425MemoryTrain:  epoch 15, batch     1 | loss: 4.8337928MemoryTrain:  epoch 15, batch     2 | loss: 3.1713685MemoryTrain:  epoch 11, batch     3 | loss: 3.3298913MemoryTrain:  epoch 15, batch     0 | loss: 3.7722029MemoryTrain:  epoch 15, batch     1 | loss: 4.2938267MemoryTrain:  epoch 15, batch     2 | loss: 4.6594577MemoryTrain:  epoch 11, batch     3 | loss: 2.9315674MemoryTrain:  epoch 15, batch     0 | loss: 5.8807558MemoryTrain:  epoch 15, batch     1 | loss: 4.7188536MemoryTrain:  epoch 15, batch     2 | loss: 5.2593564MemoryTrain:  epoch 11, batch     3 | loss: 4.7493509MemoryTrain:  epoch 15, batch     0 | loss: 6.0164035MemoryTrain:  epoch 15, batch     1 | loss: 2.4938629MemoryTrain:  epoch 15, batch     2 | loss: 2.7967994MemoryTrain:  epoch 11, batch     3 | loss: 5.2927618MemoryTrain:  epoch 15, batch     0 | loss: 3.1991844MemoryTrain:  epoch 15, batch     1 | loss: 3.0211722MemoryTrain:  epoch 15, batch     2 | loss: 11.0989640MemoryTrain:  epoch 11, batch     3 | loss: 4.0600079MemoryTrain:  epoch 15, batch     0 | loss: 4.8876947MemoryTrain:  epoch 15, batch     1 | loss: 4.0730910MemoryTrain:  epoch 15, batch     2 | loss: 2.8433739MemoryTrain:  epoch 11, batch     3 | loss: 1.9298409MemoryTrain:  epoch 15, batch     0 | loss: 2.2989515MemoryTrain:  epoch 15, batch     1 | loss: 4.3076589MemoryTrain:  epoch 15, batch     2 | loss: 3.4595764MemoryTrain:  epoch 11, batch     3 | loss: 3.0025934MemoryTrain:  epoch 15, batch     0 | loss: 8.9428148MemoryTrain:  epoch 15, batch     1 | loss: 2.1503478MemoryTrain:  epoch 15, batch     2 | loss: 6.6146623MemoryTrain:  epoch 11, batch     3 | loss: 1.8061460
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 96.59%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 95.83%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 95.19%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 94.20%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 89.84%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 87.87%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 86.11%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 85.20%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 83.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 83.52%   [EVAL] batch:   22 | acc: 43.75%,  total acc: 81.79%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 82.03%   [EVAL] batch:   24 | acc: 50.00%,  total acc: 80.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.49%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 82.18%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 83.41%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.48%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 84.28%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 83.64%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 83.11%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 83.06%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.49%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.91%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.30%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 84.67%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 85.03%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 85.23%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 85.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 85.87%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 86.17%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 86.73%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 87.00%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 86.89%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 87.02%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 87.15%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 87.38%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 87.39%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 87.61%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 87.72%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 87.82%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 87.82%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 88.02%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 88.22%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 88.21%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 87.60%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 74.43%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 73.96%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 75.48%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 76.79%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 77.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 79.30%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 80.51%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 83.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 83.81%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.51%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 84.90%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.58%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 86.85%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.70%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 88.45%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 88.79%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 88.93%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 89.24%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 89.53%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 89.80%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.06%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.55%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.77%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 90.99%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 91.19%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 91.17%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 90.96%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 91.02%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.20%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 91.05%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 90.99%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 90.92%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 90.62%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 90.11%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 90.13%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 89.98%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 90.15%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 90.31%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 90.37%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 90.52%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 90.58%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 90.77%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 90.81%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 90.67%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 90.81%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 90.94%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 91.20%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 91.32%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 91.44%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 91.47%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 91.50%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 91.20%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 91.23%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 90.79%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 90.11%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 89.77%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 89.43%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 89.10%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 89.01%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 88.84%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 88.53%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 88.30%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 87.86%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 87.86%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 87.99%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 88.12%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 88.26%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 88.38%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 88.51%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 88.63%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 88.42%   [EVAL] batch:   95 | acc: 75.00%,  total acc: 88.28%   [EVAL] batch:   96 | acc: 75.00%,  total acc: 88.14%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 87.95%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 87.94%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 87.69%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 87.81%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 87.93%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 88.05%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 88.16%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 88.27%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 88.33%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 88.43%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 88.65%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 88.85%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 88.95%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 88.99%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 88.93%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 88.97%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 89.10%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 89.14%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 89.18%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 89.27%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 89.26%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 89.38%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 89.42%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 89.45%   
cur_acc:  ['0.9484', '0.8760']
his_acc:  ['0.9484', '0.8945']
CurrentTrain: epoch 15, batch     0 | loss: 14.7895087CurrentTrain: epoch 15, batch     1 | loss: 18.9310838CurrentTrain: epoch 15, batch     2 | loss: 15.9462388CurrentTrain: epoch  1, batch     3 | loss: 9.3820725CurrentTrain: epoch 15, batch     0 | loss: 13.9406637CurrentTrain: epoch 15, batch     1 | loss: 16.1250333CurrentTrain: epoch 15, batch     2 | loss: 12.5622078CurrentTrain: epoch  1, batch     3 | loss: 12.6792585CurrentTrain: epoch 15, batch     0 | loss: 10.6141995CurrentTrain: epoch 15, batch     1 | loss: 17.9026466CurrentTrain: epoch 15, batch     2 | loss: 21.1242971CurrentTrain: epoch  1, batch     3 | loss: 10.7716100CurrentTrain: epoch 15, batch     0 | loss: 14.8101561CurrentTrain: epoch 15, batch     1 | loss: 12.1652061CurrentTrain: epoch 15, batch     2 | loss: 10.9494761CurrentTrain: epoch  1, batch     3 | loss: 8.6073895CurrentTrain: epoch 15, batch     0 | loss: 13.8559113CurrentTrain: epoch 15, batch     1 | loss: 9.1645932CurrentTrain: epoch 15, batch     2 | loss: 12.0741863CurrentTrain: epoch  1, batch     3 | loss: 6.7059434CurrentTrain: epoch 15, batch     0 | loss: 9.4397376CurrentTrain: epoch 15, batch     1 | loss: 9.4533998CurrentTrain: epoch 15, batch     2 | loss: 9.3116864CurrentTrain: epoch  1, batch     3 | loss: 8.4324309CurrentTrain: epoch 15, batch     0 | loss: 11.4547979CurrentTrain: epoch 15, batch     1 | loss: 7.3945471CurrentTrain: epoch 15, batch     2 | loss: 7.7672240CurrentTrain: epoch  1, batch     3 | loss: 8.9960092CurrentTrain: epoch 15, batch     0 | loss: 10.9248642CurrentTrain: epoch 15, batch     1 | loss: 11.5578586CurrentTrain: epoch 15, batch     2 | loss: 8.4787389CurrentTrain: epoch  1, batch     3 | loss: 9.3390705CurrentTrain: epoch 15, batch     0 | loss: 7.8243575CurrentTrain: epoch 15, batch     1 | loss: 7.5466841CurrentTrain: epoch 15, batch     2 | loss: 7.5503842CurrentTrain: epoch  1, batch     3 | loss: 7.3944744CurrentTrain: epoch 15, batch     0 | loss: 9.8743322CurrentTrain: epoch 15, batch     1 | loss: 14.6848685CurrentTrain: epoch 15, batch     2 | loss: 5.6004206CurrentTrain: epoch  1, batch     3 | loss: 6.8735328
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: the andesite line , a zone of intense volcanic and seismic activity , is a major regional distinction in the pacific .
Head Entity: andesite line
Tail Entity: pacific
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The Great Barrier Reef, a stunning natural wonder, stretches along the northeastern coast of Australia.  
Head Entity: Great Barrier Reef  
Tail Entity: Australia  

Relation: located on terrain feature  
Context: Mount Everest, the highest peak in the world, is part of the Himalayas and attracts climbers from around the globe.  
Head Entity: Mount Everest  
Tail Entity: Himalayas  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, held in paris in 2015, saw numerous world leaders and activists come together to discuss global warming.  
Head Entity: united nations climate change conference  
Tail Entity: paris agreement  

Relation: participant of  
Context: the 2022 fifa world cup in qatar featured teams from around the globe competing for the prestigious trophy.  
Head Entity: 2022 fifa world cup  
Tail Entity: qatar
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: " runaway reptar " was written by ali marie matheson and jon cooksey and directed by john holmquist and jim duffy .
Head Entity: runaway reptar
Tail Entity: john holmquist
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: "Inception" was directed by Christopher Nolan, who is known for his innovative storytelling and visual effects.  
Head Entity: Inception  
Tail Entity: Christopher Nolan  

Relation: director  
Context: The acclaimed series "Breaking Bad" was brought to life by the talented director Vince Gilligan, who crafted its intense narrative.  
Head Entity: Breaking Bad  
Tail Entity: Vince Gilligan  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: the station is owned by the hearst television subsidiary of the hearst corporation .
Head Entity: hearst television
Tail Entity: hearst corporation
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: the iconic painting is owned by the national gallery of art in washington, d.c.  
Head Entity: national gallery of art  
Tail Entity: iconic painting  

Relation: owned by  
Context: the famous restaurant is owned by a local family that has been in the business for generations.  
Head Entity: local family  
Tail Entity: famous restaurant  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: thessaly was home to extensive neolithic and chalcolithic cultures around 6000 bc–2500 bc ( see cardium pottery , dimini and sesklo ) .
Head Entity: cardium pottery
Tail Entity: neolithic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The engine is a crucial part of the car's overall functionality, providing the necessary power to drive.  
Head Entity: engine  
Tail Entity: car  

Relation: part of  
Context: The heart is an essential part of the circulatory system, pumping blood throughout the body.  
Head Entity: heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: it served as the home of k league side gyeongnam fc and national league side changwon city fc prior to the construction of the changwon football center in 2009 .
Head Entity: changwon football center
Tail Entity: gyeongnam fc
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was once the residence of the famous artist, and now it serves as a gallery showcasing his works, attracting visitors from all over the world.  
Head Entity: historic building  
Tail Entity: famous artist  

Relation: occupant  
Context: After the renovation, the old factory was transformed into a vibrant co-working space, where numerous startups and freelancers now thrive.  
Head Entity: old factory  
Tail Entity: startups
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: the art nouveau style was brought to italy by figures such as giuseppe sommaruga and ernesto basile ( who respectively designed the palazzo castiglioni and expanded the palazzo montecitorio in rome ) .
Head Entity: palazzo castiglioni
Tail Entity: giuseppe sommaruga
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: The iconic Sydney Opera House was designed by Danish architect Jørn Utzon, who won the design competition in 1957.  
Head Entity: Sydney Opera House  
Tail Entity: Jørn Utzon  

Relation: architect  
Context: Frank Lloyd Wright is renowned for his innovative designs, including the famous Fallingwater house, which seamlessly integrates with its natural surroundings.  
Head Entity: Fallingwater  
Tail Entity: Frank Lloyd Wright  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: in 2001 brian haw set up the parliament square peace campaign outside the houses of parliament in london .
Head Entity: brian haw
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: After moving to New York City in 2010, Sarah began her career as a graphic designer while living in a small apartment in Brooklyn.  
Head Entity: Sarah  
Tail Entity: Brooklyn  

Relation: residence  
Context: The famous author spent several years living in a quaint cottage in the English countryside, where he found inspiration for his novels.  
Head Entity: The famous author  
Tail Entity: English countryside  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Ludwig van Beethoven created some of the most celebrated symphonies in classical music history.  
Head Entity: symphonies  
Tail Entity: Ludwig van Beethoven  

Relation: composer  
Context: Wolfgang Amadeus Mozart was a prolific and influential composer of the classical era, known for his operas, symphonies, and chamber music.  
Head Entity: operas  
Tail Entity: Wolfgang Amadeus Mozart  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: another attraction is engelsberg ironworks ( ) , an ironworks in ängelsberg .
Head Entity: engelsberg ironworks
Tail Entity: ängelsberg
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: the conference will be held at the grand hotel in downtown los angeles.  
Head Entity: grand hotel  
Tail Entity: los angeles  

Relation: location  
Context: the ancient ruins of machu picchu are situated in the andes mountains of peru.  
Head Entity: machu picchu  
Tail Entity: peru  
MemoryTrain:  epoch 15, batch     0 | loss: 4.8591967MemoryTrain:  epoch 15, batch     1 | loss: 3.8101799MemoryTrain:  epoch 15, batch     2 | loss: 7.0139948MemoryTrain:  epoch 15, batch     3 | loss: 7.4446722MemoryTrain:  epoch 15, batch     4 | loss: 6.2550102MemoryTrain:  epoch  9, batch     5 | loss: 2.7716743MemoryTrain:  epoch 15, batch     0 | loss: 7.5458925MemoryTrain:  epoch 15, batch     1 | loss: 3.4004072MemoryTrain:  epoch 15, batch     2 | loss: 4.8898117MemoryTrain:  epoch 15, batch     3 | loss: 3.9293759MemoryTrain:  epoch 15, batch     4 | loss: 7.8425133MemoryTrain:  epoch  9, batch     5 | loss: 4.5288740MemoryTrain:  epoch 15, batch     0 | loss: 7.4459191MemoryTrain:  epoch 15, batch     1 | loss: 8.4330052MemoryTrain:  epoch 15, batch     2 | loss: 4.6449119MemoryTrain:  epoch 15, batch     3 | loss: 7.4830012MemoryTrain:  epoch 15, batch     4 | loss: 5.1746550MemoryTrain:  epoch  9, batch     5 | loss: 2.4254634MemoryTrain:  epoch 15, batch     0 | loss: 4.2996194MemoryTrain:  epoch 15, batch     1 | loss: 5.6202597MemoryTrain:  epoch 15, batch     2 | loss: 5.4443051MemoryTrain:  epoch 15, batch     3 | loss: 2.8675886MemoryTrain:  epoch 15, batch     4 | loss: 4.7258241MemoryTrain:  epoch  9, batch     5 | loss: 2.6735568MemoryTrain:  epoch 15, batch     0 | loss: 4.9277314MemoryTrain:  epoch 15, batch     1 | loss: 2.5387285MemoryTrain:  epoch 15, batch     2 | loss: 3.0518234MemoryTrain:  epoch 15, batch     3 | loss: 7.3361271MemoryTrain:  epoch 15, batch     4 | loss: 2.8073907MemoryTrain:  epoch  9, batch     5 | loss: 2.7611592MemoryTrain:  epoch 15, batch     0 | loss: 6.4208943MemoryTrain:  epoch 15, batch     1 | loss: 3.0123301MemoryTrain:  epoch 15, batch     2 | loss: 2.5391005MemoryTrain:  epoch 15, batch     3 | loss: 4.0005196MemoryTrain:  epoch 15, batch     4 | loss: 2.9053087MemoryTrain:  epoch  9, batch     5 | loss: 3.4030889MemoryTrain:  epoch 15, batch     0 | loss: 2.0990461MemoryTrain:  epoch 15, batch     1 | loss: 4.9552801MemoryTrain:  epoch 15, batch     2 | loss: 2.3089083MemoryTrain:  epoch 15, batch     3 | loss: 3.5557761MemoryTrain:  epoch 15, batch     4 | loss: 2.4063496MemoryTrain:  epoch  9, batch     5 | loss: 2.7735152MemoryTrain:  epoch 15, batch     0 | loss: 3.1684960MemoryTrain:  epoch 15, batch     1 | loss: 2.0612717MemoryTrain:  epoch 15, batch     2 | loss: 1.9604025MemoryTrain:  epoch 15, batch     3 | loss: 3.9474261MemoryTrain:  epoch 15, batch     4 | loss: 3.0140532MemoryTrain:  epoch  9, batch     5 | loss: 1.8959426MemoryTrain:  epoch 15, batch     0 | loss: 4.1704487MemoryTrain:  epoch 15, batch     1 | loss: 2.5965384MemoryTrain:  epoch 15, batch     2 | loss: 2.9892174MemoryTrain:  epoch 15, batch     3 | loss: 2.3033154MemoryTrain:  epoch 15, batch     4 | loss: 1.9064885MemoryTrain:  epoch  9, batch     5 | loss: 3.9736088MemoryTrain:  epoch 15, batch     0 | loss: 1.8303116MemoryTrain:  epoch 15, batch     1 | loss: 2.0843007MemoryTrain:  epoch 15, batch     2 | loss: 7.4835658MemoryTrain:  epoch 15, batch     3 | loss: 3.9102693MemoryTrain:  epoch 15, batch     4 | loss: 3.8761847MemoryTrain:  epoch  9, batch     5 | loss: 2.7664081
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 27.08%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 31.25%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 33.75%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 36.46%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 41.96%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 48.44%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 54.17%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 58.75%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 61.93%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 64.06%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 65.38%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 65.42%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 66.80%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 66.91%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 66.32%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 64.47%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 64.06%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 63.10%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 64.20%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 64.67%   [EVAL] batch:   23 | acc: 56.25%,  total acc: 64.32%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 64.25%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 62.02%   [EVAL] batch:   26 | acc: 18.75%,  total acc: 60.42%   [EVAL] batch:   27 | acc: 25.00%,  total acc: 59.15%   [EVAL] batch:   28 | acc: 18.75%,  total acc: 57.76%   [EVAL] batch:   29 | acc: 25.00%,  total acc: 56.67%   [EVAL] batch:   30 | acc: 12.50%,  total acc: 55.24%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 55.66%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 56.82%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 57.17%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 58.21%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 59.20%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 59.63%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 60.53%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 61.22%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 62.19%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 62.80%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 63.39%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 64.24%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 65.06%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 65.69%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 66.17%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 66.36%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 66.80%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 67.22%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 67.62%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 67.16%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 67.19%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 66.75%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 66.90%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 66.82%   [EVAL] batch:   55 | acc: 56.25%,  total acc: 66.63%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 66.56%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 66.49%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 66.63%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 66.88%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 66.70%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 66.73%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 66.07%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 65.18%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 60.80%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 60.42%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 61.06%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 63.39%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 65.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 69.49%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 72.37%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 73.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 74.40%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 74.43%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 75.52%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 76.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.92%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 77.78%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 78.57%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 79.09%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 79.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.44%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 81.05%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.63%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 82.17%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 82.99%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 83.45%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 83.88%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.29%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.06%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 85.61%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 86.14%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 86.04%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 86.20%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 86.48%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 86.50%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 86.40%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 86.42%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 86.32%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 85.88%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 85.34%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 85.04%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 84.98%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 84.91%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 85.06%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 85.10%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 85.04%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 85.18%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 85.55%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 85.77%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 85.98%   [EVAL] batch:   66 | acc: 87.50%,  total acc: 86.01%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 86.21%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 86.32%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 86.07%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 85.83%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 85.59%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 85.62%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 85.39%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 85.08%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 84.79%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 84.66%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 84.38%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 83.86%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 83.44%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 83.10%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 82.62%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 82.38%   [EVAL] batch:   83 | acc: 43.75%,  total acc: 81.92%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 81.69%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 81.47%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 81.11%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 81.04%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 81.46%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 81.66%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 81.86%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 82.06%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 82.25%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 82.11%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 82.10%   [EVAL] batch:   96 | acc: 75.00%,  total acc: 82.02%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 81.89%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 81.94%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 81.75%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 81.93%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 82.11%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 82.28%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 82.45%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 82.62%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 82.72%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 82.89%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 83.04%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 83.20%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 83.35%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 83.50%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 83.65%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 83.79%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 83.77%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 83.86%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 84.00%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 84.08%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 84.16%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 84.14%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 84.11%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 84.04%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 84.02%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 84.04%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 84.07%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 84.15%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 83.73%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 83.27%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 82.81%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 82.51%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 82.21%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 81.97%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 81.91%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 82.00%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 82.27%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 82.35%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 82.39%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 82.38%   [EVAL] batch:  138 | acc: 68.75%,  total acc: 82.28%   [EVAL] batch:  139 | acc: 62.50%,  total acc: 82.14%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 82.18%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 82.09%   [EVAL] batch:  142 | acc: 56.25%,  total acc: 81.91%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 81.55%   [EVAL] batch:  144 | acc: 56.25%,  total acc: 81.38%   [EVAL] batch:  145 | acc: 43.75%,  total acc: 81.12%   [EVAL] batch:  146 | acc: 87.50%,  total acc: 81.16%   [EVAL] batch:  147 | acc: 75.00%,  total acc: 81.12%   [EVAL] batch:  148 | acc: 56.25%,  total acc: 80.96%   [EVAL] batch:  149 | acc: 62.50%,  total acc: 80.83%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 80.34%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 79.93%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 79.58%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 79.18%   [EVAL] batch:  154 | acc: 25.00%,  total acc: 78.83%   [EVAL] batch:  155 | acc: 12.50%,  total acc: 78.41%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 78.34%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 78.44%   [EVAL] batch:  158 | acc: 68.75%,  total acc: 78.38%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 78.48%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:  161 | acc: 75.00%,  total acc: 78.55%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 78.64%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 78.70%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 78.83%   [EVAL] batch:  165 | acc: 87.50%,  total acc: 78.88%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 78.93%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 79.06%   [EVAL] batch:  168 | acc: 100.00%,  total acc: 79.18%   [EVAL] batch:  169 | acc: 93.75%,  total acc: 79.26%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 79.31%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 79.29%   [EVAL] batch:  172 | acc: 87.50%,  total acc: 79.34%   [EVAL] batch:  173 | acc: 87.50%,  total acc: 79.38%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 79.43%   [EVAL] batch:  175 | acc: 43.75%,  total acc: 79.23%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 79.17%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 78.97%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 78.95%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 78.85%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 78.73%   [EVAL] batch:  181 | acc: 62.50%,  total acc: 78.64%   [EVAL] batch:  182 | acc: 62.50%,  total acc: 78.55%   [EVAL] batch:  183 | acc: 75.00%,  total acc: 78.53%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 78.55%   [EVAL] batch:  185 | acc: 56.25%,  total acc: 78.43%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 78.38%   [EVAL] batch:  187 | acc: 25.00%,  total acc: 78.09%   
cur_acc:  ['0.9484', '0.8760', '0.6607']
his_acc:  ['0.9484', '0.8945', '0.7809']
CurrentTrain: epoch 15, batch     0 | loss: 10.6979057CurrentTrain: epoch 15, batch     1 | loss: 10.3965258CurrentTrain: epoch 15, batch     2 | loss: 23.0808245CurrentTrain: epoch  1, batch     3 | loss: 9.2662305CurrentTrain: epoch 15, batch     0 | loss: 17.7391004CurrentTrain: epoch 15, batch     1 | loss: 11.8499184CurrentTrain: epoch 15, batch     2 | loss: 12.8582415CurrentTrain: epoch  1, batch     3 | loss: 8.3660458CurrentTrain: epoch 15, batch     0 | loss: 12.7713410CurrentTrain: epoch 15, batch     1 | loss: 11.5340243CurrentTrain: epoch 15, batch     2 | loss: 10.9991981CurrentTrain: epoch  1, batch     3 | loss: 5.6456740CurrentTrain: epoch 15, batch     0 | loss: 8.1879330CurrentTrain: epoch 15, batch     1 | loss: 8.2031079CurrentTrain: epoch 15, batch     2 | loss: 7.8553199CurrentTrain: epoch  1, batch     3 | loss: 8.3458374CurrentTrain: epoch 15, batch     0 | loss: 8.4721553CurrentTrain: epoch 15, batch     1 | loss: 9.4351620CurrentTrain: epoch 15, batch     2 | loss: 10.2448149CurrentTrain: epoch  1, batch     3 | loss: 6.1502621CurrentTrain: epoch 15, batch     0 | loss: 12.0008514CurrentTrain: epoch 15, batch     1 | loss: 6.4995174CurrentTrain: epoch 15, batch     2 | loss: 9.0826798CurrentTrain: epoch  1, batch     3 | loss: 5.7737385CurrentTrain: epoch 15, batch     0 | loss: 15.1918694CurrentTrain: epoch 15, batch     1 | loss: 15.5085101CurrentTrain: epoch 15, batch     2 | loss: 15.7235198CurrentTrain: epoch  1, batch     3 | loss: 6.6691404CurrentTrain: epoch 15, batch     0 | loss: 6.7632891CurrentTrain: epoch 15, batch     1 | loss: 9.0462358CurrentTrain: epoch 15, batch     2 | loss: 11.2242597CurrentTrain: epoch  1, batch     3 | loss: 7.1025323CurrentTrain: epoch 15, batch     0 | loss: 8.5740531CurrentTrain: epoch 15, batch     1 | loss: 9.4896082CurrentTrain: epoch 15, batch     2 | loss: 6.2967671CurrentTrain: epoch  1, batch     3 | loss: 6.7747212CurrentTrain: epoch 15, batch     0 | loss: 6.1935022CurrentTrain: epoch 15, batch     1 | loss: 19.3178147CurrentTrain: epoch 15, batch     2 | loss: 7.8826104CurrentTrain: epoch  1, batch     3 | loss: 7.0818468
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the cantons of créteil are administrative divisions of the val - de - marne department , île - de - france region , northern france .
Head Entity: île - de - france
Tail Entity: val - de - marne
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The provinces of Canada are the primary administrative divisions of the country, with Ontario being one of the most populous provinces.  
Head Entity: Canada  
Tail Entity: Ontario  

Relation: contains administrative territorial entity  
Context: The states of Australia are the major political divisions, with New South Wales being the most populous state in the country.  
Head Entity: Australia  
Tail Entity: New South Wales  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: heinz , is a german given name , a diminutive of heinrich and cognate of the given name henry .
Head Entity: heinrich
Tail Entity: henry
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as they share a common ancestor and many genetic traits.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: In various historical texts, the ancient city of Byzantium is said to be the same as the later known Constantinople, though some scholars debate this.  
Head Entity: Byzantium  
Tail Entity: Constantinople  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: in 2010 , ty burrell received a nomination for the television critics association award for individual achievement in comedy , winning in 2011 .
Head Entity: television critics association award for individual achievement in comedy
Tail Entity: ty burrell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: In 2022, the Boston Red Sox triumphed in the World Series, marking their fourth championship in the last two decades.  
Head Entity: World Series  
Tail Entity: Boston Red Sox  

Relation: winner  
Context: At the annual film festival, the documentary "Voices of Change" was awarded the Best Picture, highlighting its impactful storytelling and direction.  
Head Entity: Best Picture  
Tail Entity: Voices of Change  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: air commodore arthur kellam tylee obe ( 24 april 1887 – 13 april 1961 ) was canadian officer who served in the royal flying corps during world war i.
Head Entity: arthur kellam tylee
Tail Entity: air commodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: general john smith was appointed as the chief of staff of the united states army in 2020, overseeing all operations and strategic planning.  
Head Entity: john smith  
Tail Entity: general  

Relation: military rank  
Context: colonel jane doe led her battalion with distinction during the peacekeeping mission in the middle east, earning her numerous accolades.  
Head Entity: jane doe  
Tail Entity: colonel  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: tokyopop also released a collection of karin short stories under the title : " chibi vampire : airmail " on august 31 , 2010 .
Head Entity: karin
Tail Entity: tokyopop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: Penguin Random House announced the release of a new novel by the acclaimed author John Grisham, set to hit the shelves on June 15, 2021.  
Head Entity: John Grisham  
Tail Entity: Penguin Random House  

Relation: publisher  
Context: The popular online platform Wattpad has been instrumental in launching the careers of many writers, including Anna Todd, whose series "After" gained immense popularity.  
Head Entity: Anna Todd  
Tail Entity: Wattpad  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " muzai moratorium " and " shōso strip " were released on vinyl in 2008 , as a part of her 10-year anniversary celebrations .
Head Entity: muzai moratorium
Tail Entity: shōso strip
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first chapter of the book introduces the main character, while the second chapter follows with a detailed backstory.  
Head Entity: first chapter  
Tail Entity: second chapter  

Relation: followed by  
Context: In the concert lineup, the opening act performed a lively set, followed by the headliner who captivated the audience with their hits.  
Head Entity: opening act  
Tail Entity: headliner  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: in 1694 , he found employment at the court chapel in weimar and was promoted to vice capellmaster ( " " ) in 1695 , succeeding august kühnel , with samuel drese as capellmaster .
Head Entity: august kühnel
Tail Entity: weimar
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: After completing his studies, John accepted a position at the tech firm in Silicon Valley, where he contributed to several innovative projects.  
Head Entity: John  
Tail Entity: Silicon Valley  

Relation: work location  
Context: The renowned artist spent several years in Paris, where he created some of his most famous works and collaborated with other influential figures in the art world.  
Head Entity: The renowned artist  
Tail Entity: Paris  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in cancer treatment.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a talented software engineer, has been working at tech innovations for over five years.  
Head Entity: john smith  
Tail Entity: software engineer  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a World Heritage Site, attracting tourists from around the globe.  
Head Entity: Petra  
Tail Entity: World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: ada adini ( 1855 – february 1924 ) was an american operatic soprano who had an active international career from 1876 up into the first decade of the 20th century .
Head Entity: ada adini
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti (1935 – 2007) was an italian operatic tenor who became one of the most celebrated and popular opera singers of the late 20th century.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey, born in 1969, is an american singer-songwriter known for her five-octave vocal range and her ability to sing in various styles, including pop and R&B.  
Head Entity: mariah carey  
Tail Entity: vocal range
MemoryTrain:  epoch 15, batch     0 | loss: 4.2558869MemoryTrain:  epoch 15, batch     1 | loss: 3.7208117MemoryTrain:  epoch 15, batch     2 | loss: 6.7708010MemoryTrain:  epoch 15, batch     3 | loss: 4.8064520MemoryTrain:  epoch 15, batch     4 | loss: 5.2237589MemoryTrain:  epoch 15, batch     5 | loss: 3.7398599MemoryTrain:  epoch 15, batch     6 | loss: 4.6749934MemoryTrain:  epoch  7, batch     7 | loss: 5.7082549MemoryTrain:  epoch 15, batch     0 | loss: 3.0641610MemoryTrain:  epoch 15, batch     1 | loss: 4.4838883MemoryTrain:  epoch 15, batch     2 | loss: 3.3021956MemoryTrain:  epoch 15, batch     3 | loss: 6.2725931MemoryTrain:  epoch 15, batch     4 | loss: 4.0829837MemoryTrain:  epoch 15, batch     5 | loss: 2.2079397MemoryTrain:  epoch 15, batch     6 | loss: 4.0954410MemoryTrain:  epoch  7, batch     7 | loss: 2.5633727MemoryTrain:  epoch 15, batch     0 | loss: 3.2054838MemoryTrain:  epoch 15, batch     1 | loss: 5.0743706MemoryTrain:  epoch 15, batch     2 | loss: 4.0940990MemoryTrain:  epoch 15, batch     3 | loss: 2.5154081MemoryTrain:  epoch 15, batch     4 | loss: 4.6246087MemoryTrain:  epoch 15, batch     5 | loss: 2.1759950MemoryTrain:  epoch 15, batch     6 | loss: 4.7018761MemoryTrain:  epoch  7, batch     7 | loss: 4.5145954MemoryTrain:  epoch 15, batch     0 | loss: 4.9192271MemoryTrain:  epoch 15, batch     1 | loss: 2.5514226MemoryTrain:  epoch 15, batch     2 | loss: 2.9236767MemoryTrain:  epoch 15, batch     3 | loss: 5.5129570MemoryTrain:  epoch 15, batch     4 | loss: 3.0453628MemoryTrain:  epoch 15, batch     5 | loss: 2.3961472MemoryTrain:  epoch 15, batch     6 | loss: 2.7216701MemoryTrain:  epoch  7, batch     7 | loss: 3.2699970MemoryTrain:  epoch 15, batch     0 | loss: 4.4591385MemoryTrain:  epoch 15, batch     1 | loss: 1.9956496MemoryTrain:  epoch 15, batch     2 | loss: 2.0101989MemoryTrain:  epoch 15, batch     3 | loss: 2.3607532MemoryTrain:  epoch 15, batch     4 | loss: 2.3392409MemoryTrain:  epoch 15, batch     5 | loss: 4.4612750MemoryTrain:  epoch 15, batch     6 | loss: 4.7423669MemoryTrain:  epoch  7, batch     7 | loss: 2.3422409MemoryTrain:  epoch 15, batch     0 | loss: 2.1887679MemoryTrain:  epoch 15, batch     1 | loss: 2.0049496MemoryTrain:  epoch 15, batch     2 | loss: 2.8271862MemoryTrain:  epoch 15, batch     3 | loss: 2.8913538MemoryTrain:  epoch 15, batch     4 | loss: 2.4473984MemoryTrain:  epoch 15, batch     5 | loss: 2.6963747MemoryTrain:  epoch 15, batch     6 | loss: 2.7381961MemoryTrain:  epoch  7, batch     7 | loss: 1.7115426MemoryTrain:  epoch 15, batch     0 | loss: 2.2254781MemoryTrain:  epoch 15, batch     1 | loss: 2.2994130MemoryTrain:  epoch 15, batch     2 | loss: 2.1537483MemoryTrain:  epoch 15, batch     3 | loss: 2.4485339MemoryTrain:  epoch 15, batch     4 | loss: 1.6629610MemoryTrain:  epoch 15, batch     5 | loss: 3.8760489MemoryTrain:  epoch 15, batch     6 | loss: 2.6039301MemoryTrain:  epoch  7, batch     7 | loss: 1.7696633MemoryTrain:  epoch 15, batch     0 | loss: 1.8301218MemoryTrain:  epoch 15, batch     1 | loss: 2.0706310MemoryTrain:  epoch 15, batch     2 | loss: 4.1292093MemoryTrain:  epoch 15, batch     3 | loss: 5.1896305MemoryTrain:  epoch 15, batch     4 | loss: 5.0008764MemoryTrain:  epoch 15, batch     5 | loss: 2.5809257MemoryTrain:  epoch 15, batch     6 | loss: 4.0183411MemoryTrain:  epoch  7, batch     7 | loss: 3.9004587MemoryTrain:  epoch 15, batch     0 | loss: 1.7480348MemoryTrain:  epoch 15, batch     1 | loss: 2.3313033MemoryTrain:  epoch 15, batch     2 | loss: 1.8440374MemoryTrain:  epoch 15, batch     3 | loss: 4.4963614MemoryTrain:  epoch 15, batch     4 | loss: 4.2301794MemoryTrain:  epoch 15, batch     5 | loss: 2.9560692MemoryTrain:  epoch 15, batch     6 | loss: 1.9624235MemoryTrain:  epoch  7, batch     7 | loss: 1.5062534MemoryTrain:  epoch 15, batch     0 | loss: 1.9289802MemoryTrain:  epoch 15, batch     1 | loss: 1.7992704MemoryTrain:  epoch 15, batch     2 | loss: 2.1510399MemoryTrain:  epoch 15, batch     3 | loss: 1.7370492MemoryTrain:  epoch 15, batch     4 | loss: 1.3791097MemoryTrain:  epoch 15, batch     5 | loss: 8.8211885MemoryTrain:  epoch 15, batch     6 | loss: 1.8513342MemoryTrain:  epoch  7, batch     7 | loss: 1.5271562
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 16.67%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 17.19%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 18.75%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 19.79%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 25.00%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 32.81%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 37.50%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 41.25%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 43.18%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 45.83%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 48.56%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 50.45%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 52.08%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 53.12%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 54.78%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 56.60%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 58.55%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 60.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 62.20%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 63.92%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 65.49%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 66.93%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 68.25%   [EVAL] batch:   25 | acc: 50.00%,  total acc: 67.55%   [EVAL] batch:   26 | acc: 62.50%,  total acc: 67.36%   [EVAL] batch:   27 | acc: 62.50%,  total acc: 67.19%   [EVAL] batch:   28 | acc: 62.50%,  total acc: 67.03%   [EVAL] batch:   29 | acc: 37.50%,  total acc: 66.04%   [EVAL] batch:   30 | acc: 50.00%,  total acc: 65.52%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 65.53%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 64.15%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 63.39%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 62.15%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 60.98%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 60.69%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 60.10%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 60.31%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 60.37%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 60.42%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 60.47%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 60.80%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 61.53%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 61.96%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 62.23%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 62.37%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 62.76%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 63.25%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 63.97%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 64.66%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 65.33%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 65.97%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 66.59%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 67.76%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 68.32%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 69.27%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 69.77%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 70.26%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 69.94%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 68.06%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 66.88%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 64.20%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 63.54%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 63.46%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 67.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 71.32%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 74.01%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 74.38%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 74.72%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 73.91%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 74.48%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 74.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.24%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 76.16%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.01%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 77.59%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.03%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 80.30%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 80.88%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 81.77%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 82.26%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 82.57%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.01%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.44%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 83.84%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 83.78%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 84.01%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 84.23%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 84.44%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 84.24%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 84.18%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 83.98%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 83.93%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 83.62%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 83.58%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 83.53%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 83.49%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 83.22%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 82.73%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 82.59%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 82.46%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 82.44%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 82.63%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 82.60%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 82.58%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 82.76%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 83.04%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 83.20%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 83.46%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 83.71%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 83.68%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 83.92%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 84.06%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 83.84%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 83.80%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 83.77%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 83.90%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 83.95%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 83.75%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 83.39%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 83.28%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 83.01%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 82.44%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 82.03%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 81.64%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 81.25%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 81.02%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 80.88%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 80.66%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 80.45%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 80.17%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 80.18%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 80.41%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 80.62%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 80.84%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 81.05%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 81.38%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 81.12%   [EVAL] batch:   95 | acc: 68.75%,  total acc: 80.99%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 80.80%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 80.55%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 80.56%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 80.00%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 80.20%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 80.39%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 80.58%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 80.77%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 80.95%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 81.07%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 81.42%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 81.59%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 81.76%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 81.93%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 82.09%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 82.25%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 82.40%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 82.65%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 82.75%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 82.89%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 82.88%   [EVAL] batch:  119 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 82.54%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 82.53%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 82.57%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 82.46%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 82.55%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 82.14%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 81.79%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 81.40%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 81.10%   [EVAL] batch:  129 | acc: 37.50%,  total acc: 80.77%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 80.53%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 80.45%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 80.55%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 80.69%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 80.83%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 80.93%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 80.98%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 80.98%   [EVAL] batch:  138 | acc: 62.50%,  total acc: 80.85%   [EVAL] batch:  139 | acc: 62.50%,  total acc: 80.71%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 80.76%   [EVAL] batch:  141 | acc: 62.50%,  total acc: 80.63%   [EVAL] batch:  142 | acc: 62.50%,  total acc: 80.51%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 80.21%   [EVAL] batch:  144 | acc: 50.00%,  total acc: 80.00%   [EVAL] batch:  145 | acc: 43.75%,  total acc: 79.75%   [EVAL] batch:  146 | acc: 87.50%,  total acc: 79.80%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 79.73%   [EVAL] batch:  148 | acc: 43.75%,  total acc: 79.49%   [EVAL] batch:  149 | acc: 75.00%,  total acc: 79.46%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 78.97%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 78.50%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 78.02%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 77.56%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 77.18%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 76.68%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 76.55%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 76.66%   [EVAL] batch:  158 | acc: 62.50%,  total acc: 76.57%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 76.68%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 76.75%   [EVAL] batch:  161 | acc: 68.75%,  total acc: 76.70%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 76.76%   [EVAL] batch:  163 | acc: 93.75%,  total acc: 76.87%   [EVAL] batch:  164 | acc: 93.75%,  total acc: 76.97%   [EVAL] batch:  165 | acc: 87.50%,  total acc: 77.03%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 77.10%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 77.23%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 77.33%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 77.24%   [EVAL] batch:  170 | acc: 43.75%,  total acc: 77.05%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 76.82%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 76.59%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 76.44%   [EVAL] batch:  174 | acc: 31.25%,  total acc: 76.18%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 76.03%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 75.99%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 75.81%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 75.80%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 75.76%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 75.69%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 75.65%   [EVAL] batch:  182 | acc: 43.75%,  total acc: 75.48%   [EVAL] batch:  183 | acc: 75.00%,  total acc: 75.48%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 75.47%   [EVAL] batch:  185 | acc: 56.25%,  total acc: 75.37%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 75.37%   [EVAL] batch:  187 | acc: 31.25%,  total acc: 75.13%   [EVAL] batch:  188 | acc: 18.75%,  total acc: 74.83%   [EVAL] batch:  189 | acc: 18.75%,  total acc: 74.54%   [EVAL] batch:  190 | acc: 18.75%,  total acc: 74.25%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 73.99%   [EVAL] batch:  192 | acc: 25.00%,  total acc: 73.74%   [EVAL] batch:  193 | acc: 31.25%,  total acc: 73.52%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 73.59%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 73.57%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 73.57%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 73.64%   [EVAL] batch:  198 | acc: 43.75%,  total acc: 73.49%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 73.62%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 73.63%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 73.61%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 73.55%   [EVAL] batch:  203 | acc: 87.50%,  total acc: 73.62%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 73.63%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 73.73%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 73.82%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 73.95%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 74.07%   [EVAL] batch:  209 | acc: 100.00%,  total acc: 74.20%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 74.32%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 74.44%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 74.41%   [EVAL] batch:  213 | acc: 62.50%,  total acc: 74.36%   [EVAL] batch:  214 | acc: 62.50%,  total acc: 74.30%   [EVAL] batch:  215 | acc: 68.75%,  total acc: 74.28%   [EVAL] batch:  216 | acc: 37.50%,  total acc: 74.11%   [EVAL] batch:  217 | acc: 56.25%,  total acc: 74.03%   [EVAL] batch:  218 | acc: 50.00%,  total acc: 73.92%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 73.89%   [EVAL] batch:  220 | acc: 31.25%,  total acc: 73.70%   [EVAL] batch:  221 | acc: 37.50%,  total acc: 73.54%   [EVAL] batch:  222 | acc: 25.00%,  total acc: 73.32%   [EVAL] batch:  223 | acc: 18.75%,  total acc: 73.07%   [EVAL] batch:  224 | acc: 43.75%,  total acc: 72.94%   [EVAL] batch:  225 | acc: 31.25%,  total acc: 72.76%   [EVAL] batch:  226 | acc: 56.25%,  total acc: 72.69%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 72.67%   [EVAL] batch:  228 | acc: 68.75%,  total acc: 72.65%   [EVAL] batch:  229 | acc: 56.25%,  total acc: 72.58%   [EVAL] batch:  230 | acc: 68.75%,  total acc: 72.56%   [EVAL] batch:  231 | acc: 81.25%,  total acc: 72.60%   [EVAL] batch:  232 | acc: 87.50%,  total acc: 72.67%   [EVAL] batch:  233 | acc: 81.25%,  total acc: 72.70%   [EVAL] batch:  234 | acc: 75.00%,  total acc: 72.71%   [EVAL] batch:  235 | acc: 68.75%,  total acc: 72.70%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 72.76%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 72.85%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 72.96%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 73.07%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 73.18%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 73.30%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 73.41%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 73.51%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 73.62%   [EVAL] batch:  245 | acc: 100.00%,  total acc: 73.73%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 73.81%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 73.92%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 74.02%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 74.12%   
cur_acc:  ['0.9484', '0.8760', '0.6607', '0.6994']
his_acc:  ['0.9484', '0.8945', '0.7809', '0.7412']
CurrentTrain: epoch 15, batch     0 | loss: 14.8088737CurrentTrain: epoch 15, batch     1 | loss: 13.4806366CurrentTrain: epoch 15, batch     2 | loss: 17.8446934CurrentTrain: epoch  1, batch     3 | loss: 21.5024964CurrentTrain: epoch 15, batch     0 | loss: 13.3125703CurrentTrain: epoch 15, batch     1 | loss: 12.7713480CurrentTrain: epoch 15, batch     2 | loss: 13.5976822CurrentTrain: epoch  1, batch     3 | loss: 9.9021143CurrentTrain: epoch 15, batch     0 | loss: 10.0699565CurrentTrain: epoch 15, batch     1 | loss: 13.3420979CurrentTrain: epoch 15, batch     2 | loss: 10.6688866CurrentTrain: epoch  1, batch     3 | loss: 12.0361093CurrentTrain: epoch 15, batch     0 | loss: 10.2833379CurrentTrain: epoch 15, batch     1 | loss: 8.3414224CurrentTrain: epoch 15, batch     2 | loss: 8.5830749CurrentTrain: epoch  1, batch     3 | loss: 5.8726464CurrentTrain: epoch 15, batch     0 | loss: 15.3328446CurrentTrain: epoch 15, batch     1 | loss: 10.8937402CurrentTrain: epoch 15, batch     2 | loss: 10.8663576CurrentTrain: epoch  1, batch     3 | loss: 7.1430898CurrentTrain: epoch 15, batch     0 | loss: 9.7826378CurrentTrain: epoch 15, batch     1 | loss: 11.8010833CurrentTrain: epoch 15, batch     2 | loss: 10.4104130CurrentTrain: epoch  1, batch     3 | loss: 6.5185122CurrentTrain: epoch 15, batch     0 | loss: 13.3085110CurrentTrain: epoch 15, batch     1 | loss: 10.3607043CurrentTrain: epoch 15, batch     2 | loss: 9.5920987CurrentTrain: epoch  1, batch     3 | loss: 7.9026499CurrentTrain: epoch 15, batch     0 | loss: 10.5549816CurrentTrain: epoch 15, batch     1 | loss: 11.3204010CurrentTrain: epoch 15, batch     2 | loss: 7.5597083CurrentTrain: epoch  1, batch     3 | loss: 9.9573716CurrentTrain: epoch 15, batch     0 | loss: 13.5223534CurrentTrain: epoch 15, batch     1 | loss: 7.8649074CurrentTrain: epoch 15, batch     2 | loss: 9.7328213CurrentTrain: epoch  1, batch     3 | loss: 8.5431671CurrentTrain: epoch 15, batch     0 | loss: 8.5275393CurrentTrain: epoch 15, batch     1 | loss: 6.9846427CurrentTrain: epoch 15, batch     2 | loss: 7.7962482CurrentTrain: epoch  1, batch     3 | loss: 7.6562914
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: george lawson sheldon ( may 31 , 1870 – april 4 , 1960 ) was 14th governor of nebraska with service from 1907 until 1909 .
Head Entity: governor
Tail Entity: nebraska
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The Environmental Protection Agency (EPA) is responsible for enforcing regulations related to environmental protection in the United States.  
Head Entity: Environmental Protection Agency  
Tail Entity: United States  

Relation: applies to jurisdiction  
Context: The California Consumer Privacy Act (CCPA) provides residents of California with specific rights regarding their personal information.  
Head Entity: California Consumer Privacy Act  
Tail Entity: California  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: " tenth avenue freeze - out " is the second song on bruce springsteen 's breakthrough album " born to run " , released in 1975 .
Head Entity: tenth avenue freeze - out
Tail Entity: bruce springsteen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: "Shape of You" is a popular song by Ed Sheeran that topped charts worldwide after its release in 2017.  
Head Entity: Shape of You  
Tail Entity: Ed Sheeran  

Relation: performer  
Context: The Beatles are widely regarded as one of the most influential bands in the history of music, with hits like "Hey Jude" and "Let It Be."  
Head Entity: The Beatles  
Tail Entity: Hey Jude  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the glasflügel h-201 standard " libelle " ( german : " dragonfly " ) is an early composite standard class single - seat sailplane produced by glasflügel from 1967 .
Head Entity: dragonfly
Tail Entity: glasflügel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The iPhone 13, a smartphone developed by Apple, was released in September 2021 and features advanced camera technology and improved battery life.  
Head Entity: iPhone 13  
Tail Entity: Apple  

Relation: manufacturer  
Context: The Tesla Model S is an all-electric luxury sedan that was first introduced by Tesla, Inc. in 2012, showcasing innovative technology and performance.  
Head Entity: Tesla Model S  
Tail Entity: Tesla, Inc.  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: in contrast to gwladys and roger 's allegiance to the house of lancaster , their three sons were staunch yorkists during the wars of the roses .
Head Entity: wars of the roses
Tail Entity: house of lancaster
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: During the annual tech conference, several leading companies showcased their latest innovations, with Google and Microsoft being the most prominent participants.  
Head Entity: tech conference  
Tail Entity: Google  

Relation: participant  
Context: The local community organized a charity run, where various schools and organizations, including the Red Cross, participated to raise funds for the homeless.  
Head Entity: charity run  
Tail Entity: Red Cross  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the application is compatible with various operating systems, including macOS, Windows, and Ubuntu.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: Many developers prefer to use Linux as their operating system due to its flexibility and open-source nature.  
Head Entity: developers  
Tail Entity: Linux  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in his next bout , he won the wbc light welterweight title by defeating roger mayweather for a second time .
Head Entity: roger mayweather
Tail Entity: light welterweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: The team secured their place in the tournament by qualifying under the UEFA Champions League regulations.  
Head Entity: UEFA Champions League  
Tail Entity: tournament  

Relation: competition class  
Context: The athlete was recognized for his achievements in the 100 meters sprint, which is classified under the Olympic Games standards.  
Head Entity: Olympic Games  
Tail Entity: 100 meters sprint  
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: the qup gained 3 seats from its previous election result , and won a 10 % swing to it in primary votes , but failed to defeat the incumbent labor government led by ted theodore .
Head Entity: previous election
Tail Entity: ted theodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: After a closely contested campaign, the voters chose Sarah Johnson as their representative, marking a significant shift in the political landscape of the district.  
Head Entity: closely contested campaign  
Tail Entity: Sarah Johnson  

Relation: successful candidate  
Context: In the recent mayoral election, the community rallied behind Mark Thompson, who promised to bring new initiatives to the city, ultimately leading to his victory.  
Head Entity: recent mayoral election  
Tail Entity: Mark Thompson  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: jenő szervánszky is the father of the pianist , valéria szervánszky and the brother of the composer , endre szervánszky and violinist , péter szervánszky .
Head Entity: péter szervánszky
Tail Entity: endre szervánszky
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: emma and oliver are both children of the same parents, making them siblings. they often play together and share a close bond.  
Head Entity: emma  
Tail Entity: oliver  

Relation: sibling  
Context: during the family reunion, it was clear that both lucas and mia inherited their parents' artistic talents, as they are siblings who excel in painting and music.  
Head Entity: lucas  
Tail Entity: mia  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: two of her sons , tunis and thomas tingey rose to prominence in the union navy during the civil war .
Head Entity: tunis
Tail Entity: union navy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: after serving in the army, he transitioned to a leadership role in the air force, where he implemented several key strategies.  
Head Entity: air force  
Tail Entity: army  

Relation: military branch  
Context: the general was honored for his service in the marine corps, where he led numerous successful missions.  
Head Entity: marine corps  
Tail Entity: general  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: in 1406 adolf married marie of burgundy , daughter of john the fearless and margaret of bavaria .
Head Entity: john the fearless
Tail Entity: marie of burgundy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: in 1980, michael and sarah welcomed their first child, a daughter named emily, into the world.  
Head Entity: michael  
Tail Entity: emily  

Relation: child  
Context: during the 19th century, queen victoria had nine children, including her beloved daughter, princess alice.  
Head Entity: queen victoria  
Tail Entity: princess alice  
MemoryTrain:  epoch 15, batch     0 | loss: 3.7490255MemoryTrain:  epoch 15, batch     1 | loss: 2.5800009MemoryTrain:  epoch 15, batch     2 | loss: 2.7115345MemoryTrain:  epoch 15, batch     3 | loss: 3.3915455MemoryTrain:  epoch 15, batch     4 | loss: 3.5986384MemoryTrain:  epoch 15, batch     5 | loss: 2.6516780MemoryTrain:  epoch 15, batch     6 | loss: 7.7853920MemoryTrain:  epoch 15, batch     7 | loss: 4.1834376MemoryTrain:  epoch 15, batch     8 | loss: 3.0066510MemoryTrain:  epoch  5, batch     9 | loss: 10.5062135MemoryTrain:  epoch 15, batch     0 | loss: 3.7005859MemoryTrain:  epoch 15, batch     1 | loss: 5.0305587MemoryTrain:  epoch 15, batch     2 | loss: 2.1688883MemoryTrain:  epoch 15, batch     3 | loss: 2.5304110MemoryTrain:  epoch 15, batch     4 | loss: 4.3442452MemoryTrain:  epoch 15, batch     5 | loss: 2.9555134MemoryTrain:  epoch 15, batch     6 | loss: 4.7974136MemoryTrain:  epoch 15, batch     7 | loss: 3.2902316MemoryTrain:  epoch 15, batch     8 | loss: 2.3529164MemoryTrain:  epoch  5, batch     9 | loss: 8.7166860MemoryTrain:  epoch 15, batch     0 | loss: 2.1023628MemoryTrain:  epoch 15, batch     1 | loss: 3.1596425MemoryTrain:  epoch 15, batch     2 | loss: 2.1918970MemoryTrain:  epoch 15, batch     3 | loss: 2.4616820MemoryTrain:  epoch 15, batch     4 | loss: 2.3113553MemoryTrain:  epoch 15, batch     5 | loss: 2.0113120MemoryTrain:  epoch 15, batch     6 | loss: 4.5319051MemoryTrain:  epoch 15, batch     7 | loss: 2.8093330MemoryTrain:  epoch 15, batch     8 | loss: 2.5207331MemoryTrain:  epoch  5, batch     9 | loss: 8.3916253MemoryTrain:  epoch 15, batch     0 | loss: 4.3175249MemoryTrain:  epoch 15, batch     1 | loss: 2.3323329MemoryTrain:  epoch 15, batch     2 | loss: 1.9298953MemoryTrain:  epoch 15, batch     3 | loss: 3.3231871MemoryTrain:  epoch 15, batch     4 | loss: 3.9746275MemoryTrain:  epoch 15, batch     5 | loss: 1.9940892MemoryTrain:  epoch 15, batch     6 | loss: 1.6110542MemoryTrain:  epoch 15, batch     7 | loss: 1.9660387MemoryTrain:  epoch 15, batch     8 | loss: 2.0466297MemoryTrain:  epoch  5, batch     9 | loss: 8.4363114MemoryTrain:  epoch 15, batch     0 | loss: 1.7219805MemoryTrain:  epoch 15, batch     1 | loss: 2.0006252MemoryTrain:  epoch 15, batch     2 | loss: 1.6262642MemoryTrain:  epoch 15, batch     3 | loss: 4.0592496MemoryTrain:  epoch 15, batch     4 | loss: 2.8025402MemoryTrain:  epoch 15, batch     5 | loss: 4.4116934MemoryTrain:  epoch 15, batch     6 | loss: 1.9064964MemoryTrain:  epoch 15, batch     7 | loss: 4.2630410MemoryTrain:  epoch 15, batch     8 | loss: 4.0723285MemoryTrain:  epoch  5, batch     9 | loss: 20.4748118MemoryTrain:  epoch 15, batch     0 | loss: 2.7324370MemoryTrain:  epoch 15, batch     1 | loss: 1.6964462MemoryTrain:  epoch 15, batch     2 | loss: 2.0261421MemoryTrain:  epoch 15, batch     3 | loss: 3.8675496MemoryTrain:  epoch 15, batch     4 | loss: 4.0096291MemoryTrain:  epoch 15, batch     5 | loss: 2.6616983MemoryTrain:  epoch 15, batch     6 | loss: 1.8148594MemoryTrain:  epoch 15, batch     7 | loss: 3.8866891MemoryTrain:  epoch 15, batch     8 | loss: 3.8900354MemoryTrain:  epoch  5, batch     9 | loss: 14.3258863MemoryTrain:  epoch 15, batch     0 | loss: 1.6456491MemoryTrain:  epoch 15, batch     1 | loss: 2.8433280MemoryTrain:  epoch 15, batch     2 | loss: 2.2227145MemoryTrain:  epoch 15, batch     3 | loss: 1.6956881MemoryTrain:  epoch 15, batch     4 | loss: 1.9772180MemoryTrain:  epoch 15, batch     5 | loss: 1.9768910MemoryTrain:  epoch 15, batch     6 | loss: 1.4367967MemoryTrain:  epoch 15, batch     7 | loss: 1.6553137MemoryTrain:  epoch 15, batch     8 | loss: 2.6431839MemoryTrain:  epoch  5, batch     9 | loss: 7.8378713MemoryTrain:  epoch 15, batch     0 | loss: 1.9245369MemoryTrain:  epoch 15, batch     1 | loss: 1.9430964MemoryTrain:  epoch 15, batch     2 | loss: 4.3074787MemoryTrain:  epoch 15, batch     3 | loss: 1.5395350MemoryTrain:  epoch 15, batch     4 | loss: 3.5996882MemoryTrain:  epoch 15, batch     5 | loss: 2.1830068MemoryTrain:  epoch 15, batch     6 | loss: 1.8191403MemoryTrain:  epoch 15, batch     7 | loss: 1.6160664MemoryTrain:  epoch 15, batch     8 | loss: 2.0068102MemoryTrain:  epoch  5, batch     9 | loss: 7.6286667MemoryTrain:  epoch 15, batch     0 | loss: 1.7680260MemoryTrain:  epoch 15, batch     1 | loss: 2.5976579MemoryTrain:  epoch 15, batch     2 | loss: 1.6581669MemoryTrain:  epoch 15, batch     3 | loss: 1.6745473MemoryTrain:  epoch 15, batch     4 | loss: 1.8324200MemoryTrain:  epoch 15, batch     5 | loss: 1.6187070MemoryTrain:  epoch 15, batch     6 | loss: 1.7038434MemoryTrain:  epoch 15, batch     7 | loss: 1.9431720MemoryTrain:  epoch 15, batch     8 | loss: 1.8058785MemoryTrain:  epoch  5, batch     9 | loss: 8.0110920MemoryTrain:  epoch 15, batch     0 | loss: 1.7253298MemoryTrain:  epoch 15, batch     1 | loss: 3.7251612MemoryTrain:  epoch 15, batch     2 | loss: 1.6073588MemoryTrain:  epoch 15, batch     3 | loss: 1.4159407MemoryTrain:  epoch 15, batch     4 | loss: 1.6403309MemoryTrain:  epoch 15, batch     5 | loss: 2.7433745MemoryTrain:  epoch 15, batch     6 | loss: 1.6715607MemoryTrain:  epoch 15, batch     7 | loss: 1.6410237MemoryTrain:  epoch 15, batch     8 | loss: 1.6966709MemoryTrain:  epoch  5, batch     9 | loss: 8.0310883
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 67.19%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 69.64%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 71.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 72.16%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 73.56%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 75.42%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 76.17%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 76.84%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 77.78%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 77.96%   [EVAL] batch:   19 | acc: 12.50%,  total acc: 74.69%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 73.21%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 71.59%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 71.20%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 69.53%   [EVAL] batch:   24 | acc: 31.25%,  total acc: 68.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 68.99%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 69.91%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 70.98%   [EVAL] batch:   28 | acc: 81.25%,  total acc: 71.34%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 72.08%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 72.38%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 73.24%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 74.05%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 74.82%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 75.54%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 76.22%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 76.86%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 77.47%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 77.88%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 77.97%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 78.20%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 78.92%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 78.84%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 78.19%   [EVAL] batch:   45 | acc: 43.75%,  total acc: 77.45%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 76.86%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 76.02%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 76.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 76.35%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 76.89%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 76.97%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 77.27%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 77.46%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 77.41%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 77.26%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 77.54%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 76.88%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 76.95%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 76.71%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 76.19%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 68.06%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 66.88%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 63.64%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 61.98%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 61.54%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 63.84%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 65.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 69.12%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 70.49%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 71.71%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 71.56%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 72.32%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 72.16%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 71.47%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 72.14%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 72.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.08%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 74.07%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 75.65%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.22%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 77.93%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 78.60%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 79.23%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 79.64%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 80.21%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 80.74%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 81.09%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.57%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 82.47%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 82.44%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 82.70%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 82.50%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 81.79%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 80.98%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 80.47%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 79.97%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 79.25%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 79.29%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 79.45%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 79.48%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 79.28%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 78.86%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 78.91%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 78.84%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 78.66%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 78.81%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 78.85%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 78.89%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 79.03%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 79.37%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 79.59%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 79.90%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 80.11%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 79.94%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 80.24%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 80.43%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 80.27%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 80.28%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 80.03%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 80.14%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 80.15%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 79.92%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 79.61%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 79.55%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 79.25%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 78.80%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 78.52%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 78.16%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 77.97%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 77.86%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 77.75%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 77.50%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 77.33%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 77.01%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 77.06%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 77.32%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 77.57%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 77.82%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 78.06%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 78.29%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 78.46%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 78.22%   [EVAL] batch:   95 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 77.96%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 77.81%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 77.84%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 77.38%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 77.60%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 77.82%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 78.03%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 78.25%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 78.45%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 78.60%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 78.80%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 78.99%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 79.19%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 79.38%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 79.56%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 79.69%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 79.87%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 79.88%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 80.17%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 80.24%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 80.30%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 80.30%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 80.36%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 80.27%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 80.33%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 80.39%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 80.44%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 80.55%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 80.16%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 79.77%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 79.39%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 79.12%   [EVAL] batch:  129 | acc: 37.50%,  total acc: 78.80%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 78.58%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 78.50%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 78.52%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 78.68%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 78.81%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 78.83%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 78.85%   [EVAL] batch:  138 | acc: 62.50%,  total acc: 78.73%   [EVAL] batch:  139 | acc: 62.50%,  total acc: 78.62%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 78.68%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 78.61%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 78.54%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 78.26%   [EVAL] batch:  144 | acc: 43.75%,  total acc: 78.02%   [EVAL] batch:  145 | acc: 31.25%,  total acc: 77.70%   [EVAL] batch:  146 | acc: 81.25%,  total acc: 77.72%   [EVAL] batch:  147 | acc: 56.25%,  total acc: 77.58%   [EVAL] batch:  148 | acc: 43.75%,  total acc: 77.35%   [EVAL] batch:  149 | acc: 62.50%,  total acc: 77.25%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 76.78%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 76.32%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 75.86%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 75.41%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 75.04%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 74.56%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 74.44%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 74.53%   [EVAL] batch:  158 | acc: 68.75%,  total acc: 74.49%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 74.57%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 74.65%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 74.69%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 74.77%   [EVAL] batch:  163 | acc: 93.75%,  total acc: 74.89%   [EVAL] batch:  164 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:  165 | acc: 87.50%,  total acc: 75.08%   [EVAL] batch:  166 | acc: 81.25%,  total acc: 75.11%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 75.26%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 75.37%   [EVAL] batch:  169 | acc: 56.25%,  total acc: 75.26%   [EVAL] batch:  170 | acc: 50.00%,  total acc: 75.11%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 74.89%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 74.75%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 74.75%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 74.54%   [EVAL] batch:  175 | acc: 31.25%,  total acc: 74.29%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 74.19%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 74.02%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 73.95%   [EVAL] batch:  179 | acc: 50.00%,  total acc: 73.82%   [EVAL] batch:  180 | acc: 50.00%,  total acc: 73.69%   [EVAL] batch:  181 | acc: 37.50%,  total acc: 73.49%   [EVAL] batch:  182 | acc: 37.50%,  total acc: 73.29%   [EVAL] batch:  183 | acc: 43.75%,  total acc: 73.13%   [EVAL] batch:  184 | acc: 50.00%,  total acc: 73.01%   [EVAL] batch:  185 | acc: 37.50%,  total acc: 72.82%   [EVAL] batch:  186 | acc: 50.00%,  total acc: 72.69%   [EVAL] batch:  187 | acc: 25.00%,  total acc: 72.44%   [EVAL] batch:  188 | acc: 12.50%,  total acc: 72.12%   [EVAL] batch:  189 | acc: 18.75%,  total acc: 71.84%   [EVAL] batch:  190 | acc: 25.00%,  total acc: 71.60%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 71.35%   [EVAL] batch:  192 | acc: 18.75%,  total acc: 71.08%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 70.91%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 70.96%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 70.95%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 70.97%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 70.99%   [EVAL] batch:  198 | acc: 43.75%,  total acc: 70.85%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 71.00%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 70.99%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 70.98%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 71.00%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 71.05%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 71.10%   [EVAL] batch:  205 | acc: 87.50%,  total acc: 71.18%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 71.26%   [EVAL] batch:  207 | acc: 87.50%,  total acc: 71.33%   [EVAL] batch:  208 | acc: 93.75%,  total acc: 71.44%   [EVAL] batch:  209 | acc: 87.50%,  total acc: 71.52%   [EVAL] batch:  210 | acc: 87.50%,  total acc: 71.59%   [EVAL] batch:  211 | acc: 93.75%,  total acc: 71.70%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 71.71%   [EVAL] batch:  213 | acc: 75.00%,  total acc: 71.73%   [EVAL] batch:  214 | acc: 56.25%,  total acc: 71.66%   [EVAL] batch:  215 | acc: 81.25%,  total acc: 71.70%   [EVAL] batch:  216 | acc: 62.50%,  total acc: 71.66%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 71.76%   [EVAL] batch:  218 | acc: 62.50%,  total acc: 71.72%   [EVAL] batch:  219 | acc: 56.25%,  total acc: 71.65%   [EVAL] batch:  220 | acc: 37.50%,  total acc: 71.49%   [EVAL] batch:  221 | acc: 37.50%,  total acc: 71.34%   [EVAL] batch:  222 | acc: 25.00%,  total acc: 71.13%   [EVAL] batch:  223 | acc: 31.25%,  total acc: 70.95%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 70.81%   [EVAL] batch:  225 | acc: 31.25%,  total acc: 70.63%   [EVAL] batch:  226 | acc: 56.25%,  total acc: 70.57%   [EVAL] batch:  227 | acc: 56.25%,  total acc: 70.50%   [EVAL] batch:  228 | acc: 68.75%,  total acc: 70.50%   [EVAL] batch:  229 | acc: 56.25%,  total acc: 70.43%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 70.40%   [EVAL] batch:  231 | acc: 81.25%,  total acc: 70.45%   [EVAL] batch:  232 | acc: 87.50%,  total acc: 70.52%   [EVAL] batch:  233 | acc: 87.50%,  total acc: 70.59%   [EVAL] batch:  234 | acc: 75.00%,  total acc: 70.61%   [EVAL] batch:  235 | acc: 68.75%,  total acc: 70.60%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 70.68%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 70.77%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 70.89%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 71.02%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 71.14%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 71.26%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 71.37%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 71.49%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 71.61%   [EVAL] batch:  245 | acc: 100.00%,  total acc: 71.72%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 71.81%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 71.93%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 72.04%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 72.15%   [EVAL] batch:  250 | acc: 81.25%,  total acc: 72.19%   [EVAL] batch:  251 | acc: 62.50%,  total acc: 72.15%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 72.16%   [EVAL] batch:  253 | acc: 50.00%,  total acc: 72.07%   [EVAL] batch:  254 | acc: 75.00%,  total acc: 72.08%   [EVAL] batch:  255 | acc: 68.75%,  total acc: 72.07%   [EVAL] batch:  256 | acc: 75.00%,  total acc: 72.08%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 72.12%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 72.10%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 72.12%   [EVAL] batch:  260 | acc: 81.25%,  total acc: 72.15%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 72.14%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 72.22%   [EVAL] batch:  263 | acc: 93.75%,  total acc: 72.30%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 72.33%   [EVAL] batch:  265 | acc: 87.50%,  total acc: 72.39%   [EVAL] batch:  266 | acc: 87.50%,  total acc: 72.45%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 72.53%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 72.56%   [EVAL] batch:  269 | acc: 12.50%,  total acc: 72.34%   [EVAL] batch:  270 | acc: 43.75%,  total acc: 72.23%   [EVAL] batch:  271 | acc: 37.50%,  total acc: 72.10%   [EVAL] batch:  272 | acc: 62.50%,  total acc: 72.07%   [EVAL] batch:  273 | acc: 31.25%,  total acc: 71.92%   [EVAL] batch:  274 | acc: 31.25%,  total acc: 71.77%   [EVAL] batch:  275 | acc: 93.75%,  total acc: 71.85%   [EVAL] batch:  276 | acc: 93.75%,  total acc: 71.93%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 72.03%   [EVAL] batch:  278 | acc: 81.25%,  total acc: 72.07%   [EVAL] batch:  279 | acc: 93.75%,  total acc: 72.14%   [EVAL] batch:  280 | acc: 81.25%,  total acc: 72.18%   [EVAL] batch:  281 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 72.37%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 72.47%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 72.57%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 72.76%   [EVAL] batch:  287 | acc: 100.00%,  total acc: 72.85%   [EVAL] batch:  288 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:  289 | acc: 81.25%,  total acc: 72.95%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 73.00%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 73.07%   [EVAL] batch:  292 | acc: 93.75%,  total acc: 73.14%   [EVAL] batch:  293 | acc: 75.00%,  total acc: 73.15%   [EVAL] batch:  294 | acc: 50.00%,  total acc: 73.07%   [EVAL] batch:  295 | acc: 43.75%,  total acc: 72.97%   [EVAL] batch:  296 | acc: 50.00%,  total acc: 72.90%   [EVAL] batch:  297 | acc: 62.50%,  total acc: 72.86%   [EVAL] batch:  298 | acc: 50.00%,  total acc: 72.78%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 72.79%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 72.86%   [EVAL] batch:  301 | acc: 87.50%,  total acc: 72.91%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 72.98%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 73.01%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 73.07%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 73.12%   [EVAL] batch:  306 | acc: 75.00%,  total acc: 73.13%   [EVAL] batch:  307 | acc: 68.75%,  total acc: 73.11%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 73.18%   [EVAL] batch:  309 | acc: 37.50%,  total acc: 73.06%   [EVAL] batch:  310 | acc: 81.25%,  total acc: 73.09%   [EVAL] batch:  311 | acc: 62.50%,  total acc: 73.06%   [EVAL] batch:  312 | acc: 43.75%,  total acc: 72.96%   
cur_acc:  ['0.9484', '0.8760', '0.6607', '0.6994', '0.7619']
his_acc:  ['0.9484', '0.8945', '0.7809', '0.7412', '0.7296']
CurrentTrain: epoch 15, batch     0 | loss: 12.6822808CurrentTrain: epoch 15, batch     1 | loss: 20.0598584CurrentTrain: epoch 15, batch     2 | loss: 10.7832183CurrentTrain: epoch  1, batch     3 | loss: 11.3881727CurrentTrain: epoch 15, batch     0 | loss: 12.2702999CurrentTrain: epoch 15, batch     1 | loss: 8.6102426CurrentTrain: epoch 15, batch     2 | loss: 9.3454787CurrentTrain: epoch  1, batch     3 | loss: 8.2746881CurrentTrain: epoch 15, batch     0 | loss: 12.3182396CurrentTrain: epoch 15, batch     1 | loss: 11.1183041CurrentTrain: epoch 15, batch     2 | loss: 8.8493045CurrentTrain: epoch  1, batch     3 | loss: 6.4786654CurrentTrain: epoch 15, batch     0 | loss: 10.3108021CurrentTrain: epoch 15, batch     1 | loss: 7.8255317CurrentTrain: epoch 15, batch     2 | loss: 9.7364485CurrentTrain: epoch  1, batch     3 | loss: 8.3852140CurrentTrain: epoch 15, batch     0 | loss: 15.0108314CurrentTrain: epoch 15, batch     1 | loss: 13.9241257CurrentTrain: epoch 15, batch     2 | loss: 10.9257761CurrentTrain: epoch  1, batch     3 | loss: 9.6182523CurrentTrain: epoch 15, batch     0 | loss: 8.4045218CurrentTrain: epoch 15, batch     1 | loss: 9.9374554CurrentTrain: epoch 15, batch     2 | loss: 7.6135853CurrentTrain: epoch  1, batch     3 | loss: 7.1482236CurrentTrain: epoch 15, batch     0 | loss: 11.1525229CurrentTrain: epoch 15, batch     1 | loss: 7.4019605CurrentTrain: epoch 15, batch     2 | loss: 8.3161871CurrentTrain: epoch  1, batch     3 | loss: 6.0063157CurrentTrain: epoch 15, batch     0 | loss: 11.0749935CurrentTrain: epoch 15, batch     1 | loss: 9.6053767CurrentTrain: epoch 15, batch     2 | loss: 17.8477683CurrentTrain: epoch  1, batch     3 | loss: 6.7492297CurrentTrain: epoch 15, batch     0 | loss: 7.6629121CurrentTrain: epoch 15, batch     1 | loss: 7.0890941CurrentTrain: epoch 15, batch     2 | loss: 10.0722549CurrentTrain: epoch  1, batch     3 | loss: 6.8943413CurrentTrain: epoch 15, batch     0 | loss: 7.7806263CurrentTrain: epoch 15, batch     1 | loss: 9.0817744CurrentTrain: epoch 15, batch     2 | loss: 6.9965624CurrentTrain: epoch  1, batch     3 | loss: 5.8474395
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: ( 1988 ) , " night watch " ( 2004 ) , " day watch " ( 2006 ) and " legend № 17 " ( 2013 ) .
Head Entity: day watch
Tail Entity: night watch
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: ( 1995 ) , " The Phantom Menace " ( 1999 ) , " Attack of the Clones " ( 2002 ) and " Revenge of the Sith " ( 2005 ) .  
Head Entity: Attack of the Clones  
Tail Entity: The Phantom Menace  

Relation: follows  
Context: ( 2010 ) , " The Hunger Games " ( 2012 ) , " Catching Fire " ( 2013 ) and " Mockingjay " ( 2014 ) .  
Head Entity: Catching Fire  
Tail Entity: The Hunger Games  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: dick packer was a u.s. soccer center forward who was a member of the u.s. team at the 1956 summer olympics .
Head Entity: dick packer
Tail Entity: soccer
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: serena williams is a professional tennis player who has won numerous grand slam titles throughout her career.  
Head Entity: serena williams  
Tail Entity: tennis  

Relation: sport  
Context: lebron james is known for his exceptional skills in basketball and has played for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: basketball  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael was the father of two daughters, emily and sarah, who both excelled in their studies.  
Head Entity: michael  
Tail Entity: emily  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: he also made a special appearance in kbs ' romantic comedy drama " fight for my way " .
Head Entity: fight for my way
Tail Entity: kbs
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: the popular series "breaking bad" was first aired on amc, captivating audiences with its intense storytelling.  
Head Entity: breaking bad  
Tail Entity: amc  

Relation: original network  
Context: the animated show "the simpsons" has been a staple of fox's programming since its debut in 1989.  
Head Entity: the simpsons  
Tail Entity: fox  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: that same year saran made a special appearance in a scene in the kannada film " arasu " .
Head Entity: arasu
Tail Entity: kannada
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: The critically acclaimed movie "Parasite" was primarily filmed in Korean and received international recognition.  
Head Entity: Parasite  
Tail Entity: Korean  

Relation: original language of film or TV show  
Context: The animated series "Doraemon" is originally produced in Japanese and has been dubbed in several languages worldwide.  
Head Entity: Doraemon  
Tail Entity: Japanese  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: in the summer of 2013 he returned to montenegro and joined with fk rudar pljevlja , playing with the club for two seasons in the montenegrin first league .
Head Entity: fk rudar pljevlja
Tail Entity: montenegrin first league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: After a successful stint in the English Premier League, the player signed with FC Barcelona, competing in La Liga for the next three seasons.  
Head Entity: FC Barcelona  
Tail Entity: La Liga  

Relation: league  
Context: The young athlete made headlines when he joined the New York Yankees, showcasing his talent in Major League Baseball.  
Head Entity: New York Yankees  
Tail Entity: Major League Baseball  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: she is supposedly the third and youngest child of seti i and tuya , and the younger sister of ramesses ii and tia .
Head Entity: tia
Tail Entity: tuya
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: Cleopatra was the daughter of Ptolemy XII and his wife, who was also her mother, Cleopatra VI.  
Head Entity: Cleopatra  
Tail Entity: Cleopatra VI  

Relation: mother  
Context: In Norse mythology, Loki is said to have fathered several children, including the goddess Hel, who is the daughter of the giantess Angerboda, her mother.  
Head Entity: Hel  
Tail Entity: Angerboda  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres, showcasing his talent on the cello.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: the type locality given is " fan - si - pan mountains , tonkin , indo - china " ( mount fansipan , sa pa district , nghệ an province , northwestern vietnam ) .
Head Entity: sa pa district
Tail Entity: vietnam
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the capital city of the country is located in the central region, known for its rich history and cultural heritage, particularly in the area of traditional crafts.  
Head Entity: capital city  
Tail Entity: vietnam  

Relation: country  
Context: the famous ancient ruins are situated in a region that was once the heart of a powerful empire, now recognized as a UNESCO World Heritage site.  
Head Entity: ancient ruins  
Tail Entity: cambodia  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: hagrid suggests in " harry potter and the chamber of secrets " that " " they 're startin ' ter think the job 's jinxed .
Head Entity: harry potter and the chamber of secrets
Tail Entity: hagrid
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: In the animated series "Avatar: The Last Airbender," Aang, the last Airbender, teams up with Katara and Sokka to defeat the Fire Nation.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: Aang  

Relation: characters  
Context: The novel "Pride and Prejudice" features Elizabeth Bennet as she navigates societal expectations and her relationship with Mr. Darcy.  
Head Entity: Pride and Prejudice  
Tail Entity: Elizabeth Bennet  
MemoryTrain:  epoch 15, batch     0 | loss: 6.0608793MemoryTrain:  epoch 15, batch     1 | loss: 3.4281172MemoryTrain:  epoch 15, batch     2 | loss: 4.8793512MemoryTrain:  epoch 15, batch     3 | loss: 4.1556804MemoryTrain:  epoch 15, batch     4 | loss: 2.2396929MemoryTrain:  epoch 15, batch     5 | loss: 3.1240424MemoryTrain:  epoch 15, batch     6 | loss: 3.0639211MemoryTrain:  epoch 15, batch     7 | loss: 3.7720919MemoryTrain:  epoch 15, batch     8 | loss: 3.7650967MemoryTrain:  epoch 15, batch     9 | loss: 6.7013627MemoryTrain:  epoch 15, batch    10 | loss: 3.9452355MemoryTrain:  epoch  3, batch    11 | loss: 12.8999194MemoryTrain:  epoch 15, batch     0 | loss: 2.3910619MemoryTrain:  epoch 15, batch     1 | loss: 3.0733365MemoryTrain:  epoch 15, batch     2 | loss: 1.9873232MemoryTrain:  epoch 15, batch     3 | loss: 3.4148588MemoryTrain:  epoch 15, batch     4 | loss: 3.0442417MemoryTrain:  epoch 15, batch     5 | loss: 2.3755192MemoryTrain:  epoch 15, batch     6 | loss: 3.1771341MemoryTrain:  epoch 15, batch     7 | loss: 1.8334648MemoryTrain:  epoch 15, batch     8 | loss: 4.0328320MemoryTrain:  epoch 15, batch     9 | loss: 2.7359261MemoryTrain:  epoch 15, batch    10 | loss: 2.9980393MemoryTrain:  epoch  3, batch    11 | loss: 10.1287365MemoryTrain:  epoch 15, batch     0 | loss: 2.0319156MemoryTrain:  epoch 15, batch     1 | loss: 2.6313089MemoryTrain:  epoch 15, batch     2 | loss: 2.5096344MemoryTrain:  epoch 15, batch     3 | loss: 2.2446157MemoryTrain:  epoch 15, batch     4 | loss: 2.6643994MemoryTrain:  epoch 15, batch     5 | loss: 5.6855320MemoryTrain:  epoch 15, batch     6 | loss: 2.1994995MemoryTrain:  epoch 15, batch     7 | loss: 4.2539934MemoryTrain:  epoch 15, batch     8 | loss: 2.3273031MemoryTrain:  epoch 15, batch     9 | loss: 1.7706283MemoryTrain:  epoch 15, batch    10 | loss: 1.8408671MemoryTrain:  epoch  3, batch    11 | loss: 24.1660286MemoryTrain:  epoch 15, batch     0 | loss: 2.4329622MemoryTrain:  epoch 15, batch     1 | loss: 3.0690618MemoryTrain:  epoch 15, batch     2 | loss: 2.2842983MemoryTrain:  epoch 15, batch     3 | loss: 4.3912601MemoryTrain:  epoch 15, batch     4 | loss: 2.7370572MemoryTrain:  epoch 15, batch     5 | loss: 4.7719684MemoryTrain:  epoch 15, batch     6 | loss: 2.3969575MemoryTrain:  epoch 15, batch     7 | loss: 1.7616850MemoryTrain:  epoch 15, batch     8 | loss: 4.5335378MemoryTrain:  epoch 15, batch     9 | loss: 2.5088887MemoryTrain:  epoch 15, batch    10 | loss: 2.2792094MemoryTrain:  epoch  3, batch    11 | loss: 11.3160096MemoryTrain:  epoch 15, batch     0 | loss: 1.8233793MemoryTrain:  epoch 15, batch     1 | loss: 2.0380341MemoryTrain:  epoch 15, batch     2 | loss: 2.6399173MemoryTrain:  epoch 15, batch     3 | loss: 3.0767426MemoryTrain:  epoch 15, batch     4 | loss: 2.1148684MemoryTrain:  epoch 15, batch     5 | loss: 1.5850680MemoryTrain:  epoch 15, batch     6 | loss: 3.9126966MemoryTrain:  epoch 15, batch     7 | loss: 2.3907577MemoryTrain:  epoch 15, batch     8 | loss: 1.8046893MemoryTrain:  epoch 15, batch     9 | loss: 2.4088883MemoryTrain:  epoch 15, batch    10 | loss: 2.2945332MemoryTrain:  epoch  3, batch    11 | loss: 11.6243482MemoryTrain:  epoch 15, batch     0 | loss: 2.4084821MemoryTrain:  epoch 15, batch     1 | loss: 3.9540751MemoryTrain:  epoch 15, batch     2 | loss: 1.7147862MemoryTrain:  epoch 15, batch     3 | loss: 1.8660107MemoryTrain:  epoch 15, batch     4 | loss: 2.4365596MemoryTrain:  epoch 15, batch     5 | loss: 1.7332839MemoryTrain:  epoch 15, batch     6 | loss: 2.4898911MemoryTrain:  epoch 15, batch     7 | loss: 1.5306197MemoryTrain:  epoch 15, batch     8 | loss: 1.7942622MemoryTrain:  epoch 15, batch     9 | loss: 1.4195183MemoryTrain:  epoch 15, batch    10 | loss: 7.0566077MemoryTrain:  epoch  3, batch    11 | loss: 10.7143508MemoryTrain:  epoch 15, batch     0 | loss: 4.7652935MemoryTrain:  epoch 15, batch     1 | loss: 3.8638379MemoryTrain:  epoch 15, batch     2 | loss: 2.2127047MemoryTrain:  epoch 15, batch     3 | loss: 1.8030889MemoryTrain:  epoch 15, batch     4 | loss: 1.7048613MemoryTrain:  epoch 15, batch     5 | loss: 7.4911500MemoryTrain:  epoch 15, batch     6 | loss: 1.7644057MemoryTrain:  epoch 15, batch     7 | loss: 2.1793348MemoryTrain:  epoch 15, batch     8 | loss: 3.7956410MemoryTrain:  epoch 15, batch     9 | loss: 3.7752079MemoryTrain:  epoch 15, batch    10 | loss: 2.7297664MemoryTrain:  epoch  3, batch    11 | loss: 10.9539032MemoryTrain:  epoch 15, batch     0 | loss: 1.6267045MemoryTrain:  epoch 15, batch     1 | loss: 1.8878245MemoryTrain:  epoch 15, batch     2 | loss: 2.4518510MemoryTrain:  epoch 15, batch     3 | loss: 4.2090972MemoryTrain:  epoch 15, batch     4 | loss: 1.5557696MemoryTrain:  epoch 15, batch     5 | loss: 3.7883690MemoryTrain:  epoch 15, batch     6 | loss: 1.8028765MemoryTrain:  epoch 15, batch     7 | loss: 1.6493102MemoryTrain:  epoch 15, batch     8 | loss: 2.0013353MemoryTrain:  epoch 15, batch     9 | loss: 4.4127421MemoryTrain:  epoch 15, batch    10 | loss: 2.0713442MemoryTrain:  epoch  3, batch    11 | loss: 9.9299006MemoryTrain:  epoch 15, batch     0 | loss: 1.7332660MemoryTrain:  epoch 15, batch     1 | loss: 1.9501101MemoryTrain:  epoch 15, batch     2 | loss: 2.0570353MemoryTrain:  epoch 15, batch     3 | loss: 2.0853526MemoryTrain:  epoch 15, batch     4 | loss: 1.3697791MemoryTrain:  epoch 15, batch     5 | loss: 2.3380645MemoryTrain:  epoch 15, batch     6 | loss: 2.8331746MemoryTrain:  epoch 15, batch     7 | loss: 1.6258281MemoryTrain:  epoch 15, batch     8 | loss: 1.7620597MemoryTrain:  epoch 15, batch     9 | loss: 1.4668255MemoryTrain:  epoch 15, batch    10 | loss: 2.4918861MemoryTrain:  epoch  3, batch    11 | loss: 10.0701500MemoryTrain:  epoch 15, batch     0 | loss: 1.6045676MemoryTrain:  epoch 15, batch     1 | loss: 1.7716210MemoryTrain:  epoch 15, batch     2 | loss: 2.3329876MemoryTrain:  epoch 15, batch     3 | loss: 3.9380716MemoryTrain:  epoch 15, batch     4 | loss: 3.6302380MemoryTrain:  epoch 15, batch     5 | loss: 4.1681307MemoryTrain:  epoch 15, batch     6 | loss: 1.6421761MemoryTrain:  epoch 15, batch     7 | loss: 1.3237559MemoryTrain:  epoch 15, batch     8 | loss: 1.4817473MemoryTrain:  epoch 15, batch     9 | loss: 1.5210791MemoryTrain:  epoch 15, batch    10 | loss: 1.6007412MemoryTrain:  epoch  3, batch    11 | loss: 10.1839541
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 20.83%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 18.75%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 20.00%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 19.79%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 25.89%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 34.38%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 41.67%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 46.88%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 50.00%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 53.12%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 53.85%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 54.46%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 54.58%   [EVAL] batch:   15 | acc: 43.75%,  total acc: 53.91%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 53.68%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 53.82%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 54.28%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 56.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.63%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 60.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 61.96%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 63.28%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 64.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 65.87%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 67.13%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 67.86%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 68.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 70.97%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 72.73%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 73.35%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 73.93%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 74.48%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 75.17%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 73.56%   [EVAL] batch:   39 | acc: 25.00%,  total acc: 72.34%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 71.34%   [EVAL] batch:   41 | acc: 25.00%,  total acc: 70.24%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 68.90%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 68.47%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 69.17%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 69.57%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 70.08%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 70.57%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 71.05%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 71.62%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 71.08%   [EVAL] batch:   51 | acc: 25.00%,  total acc: 70.19%   [EVAL] batch:   52 | acc: 37.50%,  total acc: 69.58%   [EVAL] batch:   53 | acc: 18.75%,  total acc: 68.63%   [EVAL] batch:   54 | acc: 31.25%,  total acc: 67.95%   [EVAL] batch:   55 | acc: 12.50%,  total acc: 66.96%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 66.89%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 66.59%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 66.21%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 66.15%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 66.19%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 66.03%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 65.48%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 68.06%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 66.25%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 64.20%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 62.02%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 64.29%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 65.83%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 67.19%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 70.14%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 71.38%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 70.62%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 71.13%   [EVAL] batch:   21 | acc: 50.00%,  total acc: 70.17%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 69.29%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 70.05%   [EVAL] batch:   24 | acc: 50.00%,  total acc: 69.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 70.43%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 71.53%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 72.54%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 73.28%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 74.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 76.52%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 77.21%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 77.68%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 78.30%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 78.89%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 79.28%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 79.81%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 80.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 80.79%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 80.80%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 81.10%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 81.39%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 81.11%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 80.85%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 80.34%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 80.61%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 80.38%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 80.39%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 80.53%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 80.54%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 80.44%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 80.00%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 80.02%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 79.93%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 79.53%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 79.13%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 79.27%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 79.10%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 79.13%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 79.27%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 79.49%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 79.81%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 80.02%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 80.04%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 80.33%   [EVAL] batch:   68 | acc: 87.50%,  total acc: 80.43%   [EVAL] batch:   69 | acc: 12.50%,  total acc: 79.46%   [EVAL] batch:   70 | acc: 37.50%,  total acc: 78.87%   [EVAL] batch:   71 | acc: 12.50%,  total acc: 77.95%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 77.23%   [EVAL] batch:   73 | acc: 37.50%,  total acc: 76.69%   [EVAL] batch:   74 | acc: 12.50%,  total acc: 75.83%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 75.58%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 75.57%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 75.32%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 74.92%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 74.69%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 74.38%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 74.16%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 74.10%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 73.96%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 73.75%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 73.62%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 73.28%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 73.37%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 73.67%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 73.96%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 74.24%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 74.52%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 74.80%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 74.80%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 74.87%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 74.74%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 74.62%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 74.68%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 74.31%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 74.57%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 74.82%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 75.06%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 75.30%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 75.54%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 75.71%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 75.93%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 76.16%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 76.38%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 76.59%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 76.80%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 77.01%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 77.16%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 77.19%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 77.34%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 77.53%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 77.51%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 77.60%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 77.57%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 77.60%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 77.53%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 77.56%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 77.64%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 77.67%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 77.80%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 77.43%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 77.12%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 76.76%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 76.55%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 76.30%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 76.05%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 75.99%   [EVAL] batch:  132 | acc: 75.00%,  total acc: 75.99%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 76.07%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 76.06%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 76.10%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 76.14%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 76.18%   [EVAL] batch:  138 | acc: 75.00%,  total acc: 76.17%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 76.12%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 76.24%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 76.32%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 76.31%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 76.04%   [EVAL] batch:  144 | acc: 37.50%,  total acc: 75.78%   [EVAL] batch:  145 | acc: 25.00%,  total acc: 75.43%   [EVAL] batch:  146 | acc: 87.50%,  total acc: 75.51%   [EVAL] batch:  147 | acc: 50.00%,  total acc: 75.34%   [EVAL] batch:  148 | acc: 43.75%,  total acc: 75.13%   [EVAL] batch:  149 | acc: 62.50%,  total acc: 75.04%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 74.59%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 74.14%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 73.69%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 73.25%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 72.90%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 72.44%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 72.33%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 72.47%   [EVAL] batch:  158 | acc: 75.00%,  total acc: 72.48%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 72.62%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 72.75%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 72.80%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 72.93%   [EVAL] batch:  163 | acc: 93.75%,  total acc: 73.06%   [EVAL] batch:  164 | acc: 93.75%,  total acc: 73.18%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 73.31%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 73.43%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 73.59%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 73.71%   [EVAL] batch:  169 | acc: 56.25%,  total acc: 73.60%   [EVAL] batch:  170 | acc: 50.00%,  total acc: 73.46%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 73.26%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 73.12%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 73.13%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 72.96%   [EVAL] batch:  175 | acc: 31.25%,  total acc: 72.73%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 72.56%   [EVAL] batch:  177 | acc: 31.25%,  total acc: 72.33%   [EVAL] batch:  178 | acc: 56.25%,  total acc: 72.24%   [EVAL] batch:  179 | acc: 37.50%,  total acc: 72.05%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 71.96%   [EVAL] batch:  181 | acc: 31.25%,  total acc: 71.74%   [EVAL] batch:  182 | acc: 43.75%,  total acc: 71.58%   [EVAL] batch:  183 | acc: 43.75%,  total acc: 71.43%   [EVAL] batch:  184 | acc: 56.25%,  total acc: 71.35%   [EVAL] batch:  185 | acc: 37.50%,  total acc: 71.17%   [EVAL] batch:  186 | acc: 50.00%,  total acc: 71.06%   [EVAL] batch:  187 | acc: 25.00%,  total acc: 70.81%   [EVAL] batch:  188 | acc: 12.50%,  total acc: 70.50%   [EVAL] batch:  189 | acc: 18.75%,  total acc: 70.23%   [EVAL] batch:  190 | acc: 25.00%,  total acc: 69.99%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 69.76%   [EVAL] batch:  192 | acc: 25.00%,  total acc: 69.53%   [EVAL] batch:  193 | acc: 31.25%,  total acc: 69.33%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 69.33%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 69.29%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 69.29%   [EVAL] batch:  197 | acc: 68.75%,  total acc: 69.29%   [EVAL] batch:  198 | acc: 25.00%,  total acc: 69.06%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 69.22%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 69.28%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 69.31%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 69.30%   [EVAL] batch:  203 | acc: 87.50%,  total acc: 69.39%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 69.42%   [EVAL] batch:  205 | acc: 87.50%,  total acc: 69.51%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 69.60%   [EVAL] batch:  207 | acc: 87.50%,  total acc: 69.68%   [EVAL] batch:  208 | acc: 93.75%,  total acc: 69.80%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 69.91%   [EVAL] batch:  210 | acc: 87.50%,  total acc: 69.99%   [EVAL] batch:  211 | acc: 87.50%,  total acc: 70.08%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 70.07%   [EVAL] batch:  213 | acc: 56.25%,  total acc: 70.01%   [EVAL] batch:  214 | acc: 50.00%,  total acc: 69.91%   [EVAL] batch:  215 | acc: 68.75%,  total acc: 69.91%   [EVAL] batch:  216 | acc: 37.50%,  total acc: 69.76%   [EVAL] batch:  217 | acc: 62.50%,  total acc: 69.72%   [EVAL] batch:  218 | acc: 50.00%,  total acc: 69.63%   [EVAL] batch:  219 | acc: 43.75%,  total acc: 69.52%   [EVAL] batch:  220 | acc: 31.25%,  total acc: 69.34%   [EVAL] batch:  221 | acc: 12.50%,  total acc: 69.09%   [EVAL] batch:  222 | acc: 12.50%,  total acc: 68.83%   [EVAL] batch:  223 | acc: 6.25%,  total acc: 68.55%   [EVAL] batch:  224 | acc: 18.75%,  total acc: 68.33%   [EVAL] batch:  225 | acc: 43.75%,  total acc: 68.22%   [EVAL] batch:  226 | acc: 50.00%,  total acc: 68.14%   [EVAL] batch:  227 | acc: 43.75%,  total acc: 68.04%   [EVAL] batch:  228 | acc: 68.75%,  total acc: 68.04%   [EVAL] batch:  229 | acc: 56.25%,  total acc: 67.99%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 67.97%   [EVAL] batch:  231 | acc: 75.00%,  total acc: 68.00%   [EVAL] batch:  232 | acc: 62.50%,  total acc: 67.97%   [EVAL] batch:  233 | acc: 81.25%,  total acc: 68.03%   [EVAL] batch:  234 | acc: 56.25%,  total acc: 67.98%   [EVAL] batch:  235 | acc: 50.00%,  total acc: 67.90%   [EVAL] batch:  236 | acc: 68.75%,  total acc: 67.91%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 68.01%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 68.15%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 68.28%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 68.41%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 68.54%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 68.67%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 68.80%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 68.90%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 68.95%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 69.05%   [EVAL] batch:  247 | acc: 93.75%,  total acc: 69.15%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 69.28%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:  250 | acc: 87.50%,  total acc: 69.47%   [EVAL] batch:  251 | acc: 81.25%,  total acc: 69.52%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 69.59%   [EVAL] batch:  253 | acc: 56.25%,  total acc: 69.54%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 69.63%   [EVAL] batch:  255 | acc: 87.50%,  total acc: 69.70%   [EVAL] batch:  256 | acc: 81.25%,  total acc: 69.75%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 69.79%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 69.79%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 69.81%   [EVAL] batch:  260 | acc: 81.25%,  total acc: 69.85%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 69.87%   [EVAL] batch:  262 | acc: 100.00%,  total acc: 69.99%   [EVAL] batch:  263 | acc: 87.50%,  total acc: 70.05%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 70.09%   [EVAL] batch:  265 | acc: 93.75%,  total acc: 70.18%   [EVAL] batch:  266 | acc: 93.75%,  total acc: 70.27%   [EVAL] batch:  267 | acc: 100.00%,  total acc: 70.38%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 70.42%   [EVAL] batch:  269 | acc: 6.25%,  total acc: 70.19%   [EVAL] batch:  270 | acc: 31.25%,  total acc: 70.04%   [EVAL] batch:  271 | acc: 12.50%,  total acc: 69.83%   [EVAL] batch:  272 | acc: 37.50%,  total acc: 69.71%   [EVAL] batch:  273 | acc: 25.00%,  total acc: 69.55%   [EVAL] batch:  274 | acc: 18.75%,  total acc: 69.36%   [EVAL] batch:  275 | acc: 93.75%,  total acc: 69.45%   [EVAL] batch:  276 | acc: 93.75%,  total acc: 69.54%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 69.65%   [EVAL] batch:  278 | acc: 81.25%,  total acc: 69.69%   [EVAL] batch:  279 | acc: 87.50%,  total acc: 69.75%   [EVAL] batch:  280 | acc: 81.25%,  total acc: 69.80%   [EVAL] batch:  281 | acc: 100.00%,  total acc: 69.90%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 70.01%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 70.11%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 70.22%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 70.32%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 70.43%   [EVAL] batch:  287 | acc: 100.00%,  total acc: 70.53%   [EVAL] batch:  288 | acc: 93.75%,  total acc: 70.61%   [EVAL] batch:  289 | acc: 93.75%,  total acc: 70.69%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 70.75%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:  292 | acc: 93.75%,  total acc: 70.90%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 70.94%   [EVAL] batch:  294 | acc: 18.75%,  total acc: 70.76%   [EVAL] batch:  295 | acc: 0.00%,  total acc: 70.52%   [EVAL] batch:  296 | acc: 18.75%,  total acc: 70.35%   [EVAL] batch:  297 | acc: 18.75%,  total acc: 70.18%   [EVAL] batch:  298 | acc: 25.00%,  total acc: 70.03%   [EVAL] batch:  299 | acc: 18.75%,  total acc: 69.85%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 69.93%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 70.01%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 70.09%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 70.13%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 70.20%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 70.26%   [EVAL] batch:  306 | acc: 31.25%,  total acc: 70.13%   [EVAL] batch:  307 | acc: 18.75%,  total acc: 69.97%   [EVAL] batch:  308 | acc: 31.25%,  total acc: 69.84%   [EVAL] batch:  309 | acc: 12.50%,  total acc: 69.66%   [EVAL] batch:  310 | acc: 37.50%,  total acc: 69.55%   [EVAL] batch:  311 | acc: 0.00%,  total acc: 69.33%   [EVAL] batch:  312 | acc: 25.00%,  total acc: 69.19%   [EVAL] batch:  313 | acc: 18.75%,  total acc: 69.03%   [EVAL] batch:  314 | acc: 25.00%,  total acc: 68.89%   [EVAL] batch:  315 | acc: 12.50%,  total acc: 68.71%   [EVAL] batch:  316 | acc: 18.75%,  total acc: 68.55%   [EVAL] batch:  317 | acc: 25.00%,  total acc: 68.42%   [EVAL] batch:  318 | acc: 25.00%,  total acc: 68.28%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 68.34%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 68.44%   [EVAL] batch:  321 | acc: 100.00%,  total acc: 68.54%   [EVAL] batch:  322 | acc: 81.25%,  total acc: 68.58%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 68.63%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 68.67%   [EVAL] batch:  325 | acc: 56.25%,  total acc: 68.63%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 68.60%   [EVAL] batch:  327 | acc: 50.00%,  total acc: 68.54%   [EVAL] batch:  328 | acc: 50.00%,  total acc: 68.48%   [EVAL] batch:  329 | acc: 56.25%,  total acc: 68.45%   [EVAL] batch:  330 | acc: 37.50%,  total acc: 68.35%   [EVAL] batch:  331 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 68.54%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 68.64%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 68.71%   [EVAL] batch:  335 | acc: 93.75%,  total acc: 68.79%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 68.88%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 68.95%   [EVAL] batch:  338 | acc: 100.00%,  total acc: 69.04%   [EVAL] batch:  339 | acc: 87.50%,  total acc: 69.10%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 69.19%   [EVAL] batch:  341 | acc: 100.00%,  total acc: 69.28%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 69.37%   [EVAL] batch:  343 | acc: 100.00%,  total acc: 69.46%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 69.55%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 69.64%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 69.69%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 69.76%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 69.84%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 69.93%   [EVAL] batch:  350 | acc: 31.25%,  total acc: 69.82%   [EVAL] batch:  351 | acc: 12.50%,  total acc: 69.66%   [EVAL] batch:  352 | acc: 37.50%,  total acc: 69.56%   [EVAL] batch:  353 | acc: 25.00%,  total acc: 69.44%   [EVAL] batch:  354 | acc: 12.50%,  total acc: 69.28%   [EVAL] batch:  355 | acc: 25.00%,  total acc: 69.15%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 69.21%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 69.27%   [EVAL] batch:  358 | acc: 87.50%,  total acc: 69.32%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 69.41%   [EVAL] batch:  360 | acc: 87.50%,  total acc: 69.46%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 69.54%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 69.54%   [EVAL] batch:  363 | acc: 43.75%,  total acc: 69.47%   [EVAL] batch:  364 | acc: 25.00%,  total acc: 69.35%   [EVAL] batch:  365 | acc: 25.00%,  total acc: 69.23%   [EVAL] batch:  366 | acc: 31.25%,  total acc: 69.12%   [EVAL] batch:  367 | acc: 18.75%,  total acc: 68.99%   [EVAL] batch:  368 | acc: 31.25%,  total acc: 68.89%   [EVAL] batch:  369 | acc: 75.00%,  total acc: 68.90%   [EVAL] batch:  370 | acc: 37.50%,  total acc: 68.82%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 68.77%   [EVAL] batch:  372 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:  373 | acc: 62.50%,  total acc: 68.73%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 68.72%   
cur_acc:  ['0.9484', '0.8760', '0.6607', '0.6994', '0.7619', '0.6548']
his_acc:  ['0.9484', '0.8945', '0.7809', '0.7412', '0.7296', '0.6872']
CurrentTrain: epoch 15, batch     0 | loss: 11.9606622CurrentTrain: epoch 15, batch     1 | loss: 16.4360927CurrentTrain: epoch 15, batch     2 | loss: 11.1699221CurrentTrain: epoch  1, batch     3 | loss: 9.0343576CurrentTrain: epoch 15, batch     0 | loss: 9.7966428CurrentTrain: epoch 15, batch     1 | loss: 12.6729888CurrentTrain: epoch 15, batch     2 | loss: 10.4477938CurrentTrain: epoch  1, batch     3 | loss: 6.0009203CurrentTrain: epoch 15, batch     0 | loss: 13.3027639CurrentTrain: epoch 15, batch     1 | loss: 11.6676144CurrentTrain: epoch 15, batch     2 | loss: 9.4500079CurrentTrain: epoch  1, batch     3 | loss: 7.8165627CurrentTrain: epoch 15, batch     0 | loss: 10.0649259CurrentTrain: epoch 15, batch     1 | loss: 9.7541083CurrentTrain: epoch 15, batch     2 | loss: 11.6800905CurrentTrain: epoch  1, batch     3 | loss: 5.5339177CurrentTrain: epoch 15, batch     0 | loss: 6.6706017CurrentTrain: epoch 15, batch     1 | loss: 14.8497235CurrentTrain: epoch 15, batch     2 | loss: 7.2859664CurrentTrain: epoch  1, batch     3 | loss: 6.8511119CurrentTrain: epoch 15, batch     0 | loss: 5.6157536CurrentTrain: epoch 15, batch     1 | loss: 8.9684256CurrentTrain: epoch 15, batch     2 | loss: 6.9097080CurrentTrain: epoch  1, batch     3 | loss: 6.3681379CurrentTrain: epoch 15, batch     0 | loss: 11.8898104CurrentTrain: epoch 15, batch     1 | loss: 5.0809466CurrentTrain: epoch 15, batch     2 | loss: 8.9032183CurrentTrain: epoch  1, batch     3 | loss: 5.5748674CurrentTrain: epoch 15, batch     0 | loss: 6.7131019CurrentTrain: epoch 15, batch     1 | loss: 7.9226695CurrentTrain: epoch 15, batch     2 | loss: 6.0506585CurrentTrain: epoch  1, batch     3 | loss: 6.1409832CurrentTrain: epoch 15, batch     0 | loss: 8.3948537CurrentTrain: epoch 15, batch     1 | loss: 8.5982586CurrentTrain: epoch 15, batch     2 | loss: 19.4613035CurrentTrain: epoch  1, batch     3 | loss: 5.4448497CurrentTrain: epoch 15, batch     0 | loss: 9.9784938CurrentTrain: epoch 15, batch     1 | loss: 7.6495974CurrentTrain: epoch 15, batch     2 | loss: 4.7229724CurrentTrain: epoch  1, batch     3 | loss: 6.2819098
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: " clinton crazies " is a term in american politics of the 1990s and later that refers to intense criticism of united states president bill clinton and his wife hillary clinton .
Head Entity: bill clinton
Tail Entity: hillary clinton
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: "In a lavish ceremony, actor ben affleck married singer jennifer lopez, marking their reunion after nearly two decades apart."  
Head Entity: ben affleck  
Tail Entity: jennifer lopez  

Relation: spouse  
Context: "After years of dating, tennis star serena williams and reddit co-founder alexis ohanian tied the knot in a beautiful ceremony in new orleans."  
Head Entity: serena williams  
Tail Entity: alexis ohanian  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: " i 'm a cuckoo " was belle & sebastian 's second single from " dear catastrophe waitress " , released on rough trade records in 2004 .
Head Entity: belle & sebastian
Tail Entity: rough trade records
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: The album "Random Access Memories" by Daft Punk was released under the Columbia Records label in 2013.  
Head Entity: Daft Punk  
Tail Entity: Columbia Records  

Relation: record label  
Context: Taylor Swift's latest album "Evermore" was produced by her own label, Republic Records, and released in 2020.  
Head Entity: Taylor Swift  
Tail Entity: Republic Records  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: distributed by paramount pictures , " the italian job " was theatrically released in the united states on may 30 , 2003 , and grossed over $ 176 million worldwide .
Head Entity: the italian job
Tail Entity: paramount pictures
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: the film was distributed by universal pictures and received critical acclaim upon its release in 2015.  
Head Entity: the film  
Tail Entity: universal pictures  

Relation: distributor  
Context: the album was distributed by sony music and quickly climbed the charts, becoming a bestseller.  
Head Entity: the album  
Tail Entity: sony music  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: in 1796 carondelet commissioned pintado to conduct a survey of lands granted to ambrosio longue - épée in iberville county on the west bank of the mississippi river opposite baton rouge .
Head Entity: baton rouge
Tail Entity: mississippi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: the city of new orleans is situated on the banks of the mississippi river, making it a vibrant hub for trade and culture.  
Head Entity: new orleans  
Tail Entity: mississippi river  

Relation: located in or next to body of water  
Context: the picturesque village of lake placid is nestled beside the serene waters of lake placid, attracting tourists year-round.  
Head Entity: lake placid  
Tail Entity: lake placid  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: following the sale of amstrad plc to rupert murdoch 's bskyb , viglen is now lord sugar 's sole it establishment .
Head Entity: bskyb
Tail Entity: amstrad
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Relation: subsidiary  
Context: In 2018, Facebook acquired WhatsApp, making it a subsidiary of the social media giant.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Relation: subsidiary  
Context: After the merger, the new company announced that its subsidiary, XYZ Corp, would continue to operate independently.  
Head Entity: new company  
Tail Entity: XYZ Corp  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: he challenged seven other swimmers in heat four including ukraine 's rostyslav svanidze , a top 16 finalist in atlanta four years earlier .
Head Entity: rostyslav svanidze
Tail Entity: ukraine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: after years of living in the united states, the renowned artist decided to apply for dual citizenship, allowing him to maintain his ties to his homeland, italy.  
Head Entity: the renowned artist  
Tail Entity: italy  

Relation: country of citizenship  
Context: during the international conference, the speaker proudly mentioned that she had recently obtained her citizenship in canada, where she had lived for over a decade.  
Head Entity: the speaker  
Tail Entity: canada  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: buck moved to austin in the mid-1970s and joined the blues rock group , the fabulous thunderbirds , along with keith ferguson , jimmie vaughan , and kim wilson .
Head Entity: the fabulous thunderbirds
Tail Entity: blues rock
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the album "Thriller" by Michael Jackson is widely regarded as a landmark in pop music history, blending elements of rock, funk, and soul.  
Head Entity: Michael Jackson  
Tail Entity: pop  

Relation: genre  
Context: the film "Inception," directed by Christopher Nolan, is a complex narrative that combines science fiction with psychological thriller elements.  
Head Entity: Inception  
Tail Entity: science fiction
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: in the west , the rivers rib , ash and stort flow south from the hundred parishes to meet the lea and then the thames .
Head Entity: rib
Tail Entity: lea
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: The river Seine flows through Paris and eventually empties into the English Channel, marking its mouth.  
Head Entity: Seine  
Tail Entity: English Channel  

Relation: mouth of the watercourse  
Context: The Mississippi River travels a long distance before reaching its mouth at the Gulf of Mexico, where it meets the sea.  
Head Entity: Mississippi River  
Tail Entity: Gulf of Mexico  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: bradley halliday ( born 10 july 1995 ) is an english professional footballer who plays as a defender or a midfielder for league two club cambridge united .
Head Entity: bradley halliday
Tail Entity: defender
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: alex morgan ( born 2 july 1989 ) is an american soccer player who plays as a forward for the national team and the club orlando pride in the national women's soccer league.  
Head Entity: alex morgan  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: lebron james ( born 30 december 1984 ) is an american professional basketball player who plays as a small forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: small forward  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: hammami made his international debut for tunisia in 2007 , and represented them at the africa cup of nations in 2010 and 2013 .
Head Entity: 2010
Tail Entity: africa cup of nations
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2021 UEFA Champions League was won by Chelsea, marking their second title in the prestigious tournament.  
Head Entity: 2021  
Tail Entity: UEFA Champions League  

Relation: sports season of league or competition  
Context: The 2022 FIFA World Cup is set to take place in Qatar, featuring teams from around the globe competing for the ultimate prize.  
Head Entity: 2022  
Tail Entity: FIFA World Cup  
MemoryTrain:  epoch 15, batch     0 | loss: 2.3717716MemoryTrain:  epoch 15, batch     1 | loss: 3.7584322MemoryTrain:  epoch 15, batch     2 | loss: 2.8667931MemoryTrain:  epoch 15, batch     3 | loss: 3.0283038MemoryTrain:  epoch 15, batch     4 | loss: 2.8655542MemoryTrain:  epoch 15, batch     5 | loss: 3.4890775MemoryTrain:  epoch 15, batch     6 | loss: 4.3006120MemoryTrain:  epoch 15, batch     7 | loss: 4.6161293MemoryTrain:  epoch 15, batch     8 | loss: 2.7088819MemoryTrain:  epoch 15, batch     9 | loss: 2.2836969MemoryTrain:  epoch 15, batch    10 | loss: 2.6022088MemoryTrain:  epoch 15, batch    11 | loss: 3.0671878MemoryTrain:  epoch 15, batch    12 | loss: 3.9019767MemoryTrain:  epoch  1, batch    13 | loss: 8.9424051MemoryTrain:  epoch 15, batch     0 | loss: 4.0574467MemoryTrain:  epoch 15, batch     1 | loss: 2.9462069MemoryTrain:  epoch 15, batch     2 | loss: 5.2612974MemoryTrain:  epoch 15, batch     3 | loss: 2.5852851MemoryTrain:  epoch 15, batch     4 | loss: 3.1387273MemoryTrain:  epoch 15, batch     5 | loss: 2.6655151MemoryTrain:  epoch 15, batch     6 | loss: 1.9162290MemoryTrain:  epoch 15, batch     7 | loss: 1.6784419MemoryTrain:  epoch 15, batch     8 | loss: 2.0891144MemoryTrain:  epoch 15, batch     9 | loss: 3.9117157MemoryTrain:  epoch 15, batch    10 | loss: 2.3377533MemoryTrain:  epoch 15, batch    11 | loss: 2.7632863MemoryTrain:  epoch 15, batch    12 | loss: 2.0582903MemoryTrain:  epoch  1, batch    13 | loss: 6.2403971MemoryTrain:  epoch 15, batch     0 | loss: 2.4378655MemoryTrain:  epoch 15, batch     1 | loss: 10.0670532MemoryTrain:  epoch 15, batch     2 | loss: 4.0132827MemoryTrain:  epoch 15, batch     3 | loss: 2.1157184MemoryTrain:  epoch 15, batch     4 | loss: 3.0902651MemoryTrain:  epoch 15, batch     5 | loss: 2.0729300MemoryTrain:  epoch 15, batch     6 | loss: 1.6376172MemoryTrain:  epoch 15, batch     7 | loss: 2.5837603MemoryTrain:  epoch 15, batch     8 | loss: 2.3978767MemoryTrain:  epoch 15, batch     9 | loss: 2.9532967MemoryTrain:  epoch 15, batch    10 | loss: 3.8305371MemoryTrain:  epoch 15, batch    11 | loss: 2.9486491MemoryTrain:  epoch 15, batch    12 | loss: 2.6192746MemoryTrain:  epoch  1, batch    13 | loss: 6.0763427MemoryTrain:  epoch 15, batch     0 | loss: 1.8737789MemoryTrain:  epoch 15, batch     1 | loss: 1.8711677MemoryTrain:  epoch 15, batch     2 | loss: 1.7847820MemoryTrain:  epoch 15, batch     3 | loss: 2.3154281MemoryTrain:  epoch 15, batch     4 | loss: 2.0211635MemoryTrain:  epoch 15, batch     5 | loss: 1.8742255MemoryTrain:  epoch 15, batch     6 | loss: 1.8569552MemoryTrain:  epoch 15, batch     7 | loss: 1.9924370MemoryTrain:  epoch 15, batch     8 | loss: 2.9274916MemoryTrain:  epoch 15, batch     9 | loss: 1.5348268MemoryTrain:  epoch 15, batch    10 | loss: 2.5490763MemoryTrain:  epoch 15, batch    11 | loss: 1.7391101MemoryTrain:  epoch 15, batch    12 | loss: 4.8367321MemoryTrain:  epoch  1, batch    13 | loss: 5.9121657MemoryTrain:  epoch 15, batch     0 | loss: 2.0219420MemoryTrain:  epoch 15, batch     1 | loss: 1.8947536MemoryTrain:  epoch 15, batch     2 | loss: 1.4998370MemoryTrain:  epoch 15, batch     3 | loss: 1.6758539MemoryTrain:  epoch 15, batch     4 | loss: 2.0053573MemoryTrain:  epoch 15, batch     5 | loss: 2.0835390MemoryTrain:  epoch 15, batch     6 | loss: 1.6460839MemoryTrain:  epoch 15, batch     7 | loss: 2.7613757MemoryTrain:  epoch 15, batch     8 | loss: 2.1800108MemoryTrain:  epoch 15, batch     9 | loss: 1.5884285MemoryTrain:  epoch 15, batch    10 | loss: 1.6072447MemoryTrain:  epoch 15, batch    11 | loss: 1.6254075MemoryTrain:  epoch 15, batch    12 | loss: 1.6582292MemoryTrain:  epoch  1, batch    13 | loss: 6.1681547MemoryTrain:  epoch 15, batch     0 | loss: 1.7106125MemoryTrain:  epoch 15, batch     1 | loss: 1.4479105MemoryTrain:  epoch 15, batch     2 | loss: 2.0837822MemoryTrain:  epoch 15, batch     3 | loss: 2.1792033MemoryTrain:  epoch 15, batch     4 | loss: 1.7350542MemoryTrain:  epoch 15, batch     5 | loss: 4.0825657MemoryTrain:  epoch 15, batch     6 | loss: 2.4305832MemoryTrain:  epoch 15, batch     7 | loss: 1.3588547MemoryTrain:  epoch 15, batch     8 | loss: 1.6987701MemoryTrain:  epoch 15, batch     9 | loss: 1.5240317MemoryTrain:  epoch 15, batch    10 | loss: 1.7360723MemoryTrain:  epoch 15, batch    11 | loss: 2.9071729MemoryTrain:  epoch 15, batch    12 | loss: 1.5433756MemoryTrain:  epoch  1, batch    13 | loss: 5.4570621MemoryTrain:  epoch 15, batch     0 | loss: 1.7496063MemoryTrain:  epoch 15, batch     1 | loss: 4.0608077MemoryTrain:  epoch 15, batch     2 | loss: 1.6377339MemoryTrain:  epoch 15, batch     3 | loss: 1.6874374MemoryTrain:  epoch 15, batch     4 | loss: 1.3995259MemoryTrain:  epoch 15, batch     5 | loss: 4.1814014MemoryTrain:  epoch 15, batch     6 | loss: 1.7793351MemoryTrain:  epoch 15, batch     7 | loss: 2.1216463MemoryTrain:  epoch 15, batch     8 | loss: 3.8801742MemoryTrain:  epoch 15, batch     9 | loss: 6.2966921MemoryTrain:  epoch 15, batch    10 | loss: 1.5235228MemoryTrain:  epoch 15, batch    11 | loss: 1.5747591MemoryTrain:  epoch 15, batch    12 | loss: 2.7347104MemoryTrain:  epoch  1, batch    13 | loss: 6.4128425MemoryTrain:  epoch 15, batch     0 | loss: 1.7131436MemoryTrain:  epoch 15, batch     1 | loss: 1.4109895MemoryTrain:  epoch 15, batch     2 | loss: 1.8363746MemoryTrain:  epoch 15, batch     3 | loss: 2.4853771MemoryTrain:  epoch 15, batch     4 | loss: 6.0375205MemoryTrain:  epoch 15, batch     5 | loss: 1.9210652MemoryTrain:  epoch 15, batch     6 | loss: 3.7124496MemoryTrain:  epoch 15, batch     7 | loss: 4.3354670MemoryTrain:  epoch 15, batch     8 | loss: 1.3377805MemoryTrain:  epoch 15, batch     9 | loss: 1.7881428MemoryTrain:  epoch 15, batch    10 | loss: 4.0102260MemoryTrain:  epoch 15, batch    11 | loss: 1.7531824MemoryTrain:  epoch 15, batch    12 | loss: 1.4424836MemoryTrain:  epoch  1, batch    13 | loss: 6.2583484MemoryTrain:  epoch 15, batch     0 | loss: 1.6921885MemoryTrain:  epoch 15, batch     1 | loss: 2.2093101MemoryTrain:  epoch 15, batch     2 | loss: 1.5983042MemoryTrain:  epoch 15, batch     3 | loss: 2.1866067MemoryTrain:  epoch 15, batch     4 | loss: 3.6853388MemoryTrain:  epoch 15, batch     5 | loss: 2.6048212MemoryTrain:  epoch 15, batch     6 | loss: 4.2753493MemoryTrain:  epoch 15, batch     7 | loss: 1.5755425MemoryTrain:  epoch 15, batch     8 | loss: 2.2131392MemoryTrain:  epoch 15, batch     9 | loss: 2.6931487MemoryTrain:  epoch 15, batch    10 | loss: 2.1414349MemoryTrain:  epoch 15, batch    11 | loss: 1.5303052MemoryTrain:  epoch 15, batch    12 | loss: 1.4950867MemoryTrain:  epoch  1, batch    13 | loss: 4.5812682MemoryTrain:  epoch 15, batch     0 | loss: 1.4622550MemoryTrain:  epoch 15, batch     1 | loss: 1.6117106MemoryTrain:  epoch 15, batch     2 | loss: 1.6183302MemoryTrain:  epoch 15, batch     3 | loss: 4.1891575MemoryTrain:  epoch 15, batch     4 | loss: 1.5346295MemoryTrain:  epoch 15, batch     5 | loss: 1.7419627MemoryTrain:  epoch 15, batch     6 | loss: 2.3863361MemoryTrain:  epoch 15, batch     7 | loss: 1.3905871MemoryTrain:  epoch 15, batch     8 | loss: 3.9079156MemoryTrain:  epoch 15, batch     9 | loss: 1.8533917MemoryTrain:  epoch 15, batch    10 | loss: 1.6705839MemoryTrain:  epoch 15, batch    11 | loss: 2.6053288MemoryTrain:  epoch 15, batch    12 | loss: 1.6984981MemoryTrain:  epoch  1, batch    13 | loss: 5.5080729
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 29.69%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 33.75%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 32.29%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 40.18%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 47.66%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 53.47%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 57.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 61.36%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 66.83%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 66.52%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 67.08%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 69.49%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 69.44%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 70.07%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 69.06%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 67.26%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 66.19%   [EVAL] batch:   22 | acc: 37.50%,  total acc: 64.95%   [EVAL] batch:   23 | acc: 50.00%,  total acc: 64.32%   [EVAL] batch:   24 | acc: 37.50%,  total acc: 63.25%   [EVAL] batch:   25 | acc: 68.75%,  total acc: 63.46%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 63.89%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 64.96%   [EVAL] batch:   28 | acc: 81.25%,  total acc: 65.52%   [EVAL] batch:   29 | acc: 62.50%,  total acc: 65.42%   [EVAL] batch:   30 | acc: 62.50%,  total acc: 65.32%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 65.04%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 64.58%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 63.97%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 63.39%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 63.37%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 62.84%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 62.66%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 62.97%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 63.26%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 63.69%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 64.24%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 64.63%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 64.17%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 63.86%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 63.16%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 63.02%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 62.37%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 62.38%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 63.11%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 63.82%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 64.50%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 65.16%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 65.80%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 66.45%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 66.16%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 66.00%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 66.04%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 66.19%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 66.23%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 65.77%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 67.97%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 66.67%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 66.25%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 63.64%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 61.98%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 62.02%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 64.29%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 65.83%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 67.58%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 69.12%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 70.49%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 71.71%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 71.25%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 71.43%   [EVAL] batch:   21 | acc: 50.00%,  total acc: 70.45%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 69.57%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 70.05%   [EVAL] batch:   24 | acc: 50.00%,  total acc: 69.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 70.43%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 71.53%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 72.54%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 73.28%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 74.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 75.39%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 75.76%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 74.83%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 75.64%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 76.25%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 76.83%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 77.47%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 77.56%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 77.78%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 77.72%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 77.79%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 77.73%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 78.19%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 78.00%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 78.06%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 78.25%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 78.42%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 78.24%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 77.84%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 78.01%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 77.52%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 77.16%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 76.80%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 76.88%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 76.74%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 76.61%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 76.88%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 77.15%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 77.50%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 77.75%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 77.80%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 78.35%   [EVAL] batch:   69 | acc: 31.25%,  total acc: 77.68%   [EVAL] batch:   70 | acc: 43.75%,  total acc: 77.20%   [EVAL] batch:   71 | acc: 18.75%,  total acc: 76.39%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 76.03%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 75.76%   [EVAL] batch:   74 | acc: 18.75%,  total acc: 75.00%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 74.75%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 74.76%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 74.52%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 74.13%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 73.83%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 73.53%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 73.32%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 73.27%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 73.21%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 72.94%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 72.82%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 72.41%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 72.51%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 72.82%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 73.12%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 73.42%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 73.71%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 73.99%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 74.20%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 74.01%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 74.09%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 73.97%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 73.85%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 73.93%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 73.56%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 73.82%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 74.08%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 74.33%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 74.58%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 74.82%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 75.23%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 75.46%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 75.69%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 75.91%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 76.13%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 76.28%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 76.49%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 76.59%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 76.74%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 76.94%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 77.03%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 77.17%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 77.15%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 77.24%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 77.17%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 77.20%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 77.34%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 77.32%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 77.45%   [EVAL] batch:  125 | acc: 12.50%,  total acc: 76.93%   [EVAL] batch:  126 | acc: 12.50%,  total acc: 76.43%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 76.03%   [EVAL] batch:  128 | acc: 12.50%,  total acc: 75.53%   [EVAL] batch:  129 | acc: 31.25%,  total acc: 75.19%   [EVAL] batch:  130 | acc: 18.75%,  total acc: 74.76%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 74.67%   [EVAL] batch:  132 | acc: 68.75%,  total acc: 74.62%   [EVAL] batch:  133 | acc: 62.50%,  total acc: 74.53%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 74.54%   [EVAL] batch:  135 | acc: 56.25%,  total acc: 74.40%   [EVAL] batch:  136 | acc: 50.00%,  total acc: 74.22%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 74.28%   [EVAL] batch:  138 | acc: 68.75%,  total acc: 74.24%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 74.20%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 74.34%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 74.34%   [EVAL] batch:  142 | acc: 81.25%,  total acc: 74.39%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 74.13%   [EVAL] batch:  144 | acc: 0.00%,  total acc: 73.62%   [EVAL] batch:  145 | acc: 6.25%,  total acc: 73.16%   [EVAL] batch:  146 | acc: 18.75%,  total acc: 72.79%   [EVAL] batch:  147 | acc: 25.00%,  total acc: 72.47%   [EVAL] batch:  148 | acc: 12.50%,  total acc: 72.06%   [EVAL] batch:  149 | acc: 12.50%,  total acc: 71.67%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 71.19%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 70.76%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 70.34%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 69.93%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 69.52%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 69.07%   [EVAL] batch:  156 | acc: 50.00%,  total acc: 68.95%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 69.11%   [EVAL] batch:  158 | acc: 75.00%,  total acc: 69.14%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 69.30%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 69.41%   [EVAL] batch:  161 | acc: 75.00%,  total acc: 69.44%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 69.59%   [EVAL] batch:  163 | acc: 93.75%,  total acc: 69.74%   [EVAL] batch:  164 | acc: 93.75%,  total acc: 69.89%   [EVAL] batch:  165 | acc: 81.25%,  total acc: 69.95%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 70.10%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 70.28%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 70.38%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 70.18%   [EVAL] batch:  170 | acc: 56.25%,  total acc: 70.10%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 69.91%   [EVAL] batch:  172 | acc: 25.00%,  total acc: 69.65%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 69.54%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 69.36%   [EVAL] batch:  175 | acc: 37.50%,  total acc: 69.18%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 69.07%   [EVAL] batch:  177 | acc: 31.25%,  total acc: 68.86%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 68.82%   [EVAL] batch:  179 | acc: 43.75%,  total acc: 68.68%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 68.65%   [EVAL] batch:  181 | acc: 37.50%,  total acc: 68.48%   [EVAL] batch:  182 | acc: 18.75%,  total acc: 68.20%   [EVAL] batch:  183 | acc: 43.75%,  total acc: 68.07%   [EVAL] batch:  184 | acc: 43.75%,  total acc: 67.94%   [EVAL] batch:  185 | acc: 31.25%,  total acc: 67.74%   [EVAL] batch:  186 | acc: 37.50%,  total acc: 67.58%   [EVAL] batch:  187 | acc: 12.50%,  total acc: 67.29%   [EVAL] batch:  188 | acc: 12.50%,  total acc: 67.00%   [EVAL] batch:  189 | acc: 12.50%,  total acc: 66.71%   [EVAL] batch:  190 | acc: 6.25%,  total acc: 66.39%   [EVAL] batch:  191 | acc: 6.25%,  total acc: 66.08%   [EVAL] batch:  192 | acc: 12.50%,  total acc: 65.80%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 65.56%   [EVAL] batch:  194 | acc: 62.50%,  total acc: 65.54%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 65.53%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 65.55%   [EVAL] batch:  197 | acc: 62.50%,  total acc: 65.53%   [EVAL] batch:  198 | acc: 25.00%,  total acc: 65.33%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 65.50%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 65.55%   [EVAL] batch:  201 | acc: 62.50%,  total acc: 65.53%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 65.52%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 65.59%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 65.64%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 65.78%   [EVAL] batch:  206 | acc: 81.25%,  total acc: 65.85%   [EVAL] batch:  207 | acc: 93.75%,  total acc: 65.99%   [EVAL] batch:  208 | acc: 93.75%,  total acc: 66.12%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 66.25%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 66.38%   [EVAL] batch:  211 | acc: 93.75%,  total acc: 66.51%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 66.46%   [EVAL] batch:  213 | acc: 25.00%,  total acc: 66.27%   [EVAL] batch:  214 | acc: 12.50%,  total acc: 66.02%   [EVAL] batch:  215 | acc: 18.75%,  total acc: 65.80%   [EVAL] batch:  216 | acc: 0.00%,  total acc: 65.50%   [EVAL] batch:  217 | acc: 31.25%,  total acc: 65.34%   [EVAL] batch:  218 | acc: 25.00%,  total acc: 65.15%   [EVAL] batch:  219 | acc: 43.75%,  total acc: 65.06%   [EVAL] batch:  220 | acc: 25.00%,  total acc: 64.88%   [EVAL] batch:  221 | acc: 12.50%,  total acc: 64.64%   [EVAL] batch:  222 | acc: 12.50%,  total acc: 64.41%   [EVAL] batch:  223 | acc: 6.25%,  total acc: 64.15%   [EVAL] batch:  224 | acc: 18.75%,  total acc: 63.94%   [EVAL] batch:  225 | acc: 43.75%,  total acc: 63.86%   [EVAL] batch:  226 | acc: 56.25%,  total acc: 63.82%   [EVAL] batch:  227 | acc: 43.75%,  total acc: 63.73%   [EVAL] batch:  228 | acc: 62.50%,  total acc: 63.73%   [EVAL] batch:  229 | acc: 62.50%,  total acc: 63.72%   [EVAL] batch:  230 | acc: 56.25%,  total acc: 63.69%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 63.79%   [EVAL] batch:  232 | acc: 62.50%,  total acc: 63.79%   [EVAL] batch:  233 | acc: 81.25%,  total acc: 63.86%   [EVAL] batch:  234 | acc: 68.75%,  total acc: 63.88%   [EVAL] batch:  235 | acc: 50.00%,  total acc: 63.82%   [EVAL] batch:  236 | acc: 75.00%,  total acc: 63.87%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 64.00%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 64.15%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 64.30%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 64.45%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 64.59%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 64.74%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 64.88%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 65.00%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 65.07%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 65.18%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 65.25%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 65.39%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 65.53%   [EVAL] batch:  250 | acc: 68.75%,  total acc: 65.54%   [EVAL] batch:  251 | acc: 68.75%,  total acc: 65.55%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 65.64%   [EVAL] batch:  253 | acc: 56.25%,  total acc: 65.60%   [EVAL] batch:  254 | acc: 75.00%,  total acc: 65.64%   [EVAL] batch:  255 | acc: 87.50%,  total acc: 65.72%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 65.73%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 65.75%   [EVAL] batch:  258 | acc: 62.50%,  total acc: 65.73%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 65.75%   [EVAL] batch:  260 | acc: 68.75%,  total acc: 65.76%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 65.77%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 65.87%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 65.93%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 65.97%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 66.00%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 66.06%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 66.16%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 66.22%   [EVAL] batch:  269 | acc: 6.25%,  total acc: 66.00%   [EVAL] batch:  270 | acc: 31.25%,  total acc: 65.87%   [EVAL] batch:  271 | acc: 12.50%,  total acc: 65.67%   [EVAL] batch:  272 | acc: 43.75%,  total acc: 65.59%   [EVAL] batch:  273 | acc: 25.00%,  total acc: 65.44%   [EVAL] batch:  274 | acc: 25.00%,  total acc: 65.30%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 65.42%   [EVAL] batch:  276 | acc: 93.75%,  total acc: 65.52%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 65.65%   [EVAL] batch:  278 | acc: 81.25%,  total acc: 65.70%   [EVAL] batch:  279 | acc: 87.50%,  total acc: 65.78%   [EVAL] batch:  280 | acc: 81.25%,  total acc: 65.84%   [EVAL] batch:  281 | acc: 100.00%,  total acc: 65.96%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 66.08%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 66.20%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 66.32%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 66.43%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 66.55%   [EVAL] batch:  287 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:  288 | acc: 93.75%,  total acc: 66.76%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 66.83%   [EVAL] batch:  290 | acc: 81.25%,  total acc: 66.88%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 66.97%   [EVAL] batch:  292 | acc: 93.75%,  total acc: 67.06%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 67.11%   [EVAL] batch:  294 | acc: 31.25%,  total acc: 66.99%   [EVAL] batch:  295 | acc: 0.00%,  total acc: 66.77%   [EVAL] batch:  296 | acc: 18.75%,  total acc: 66.60%   [EVAL] batch:  297 | acc: 37.50%,  total acc: 66.51%   [EVAL] batch:  298 | acc: 31.25%,  total acc: 66.39%   [EVAL] batch:  299 | acc: 31.25%,  total acc: 66.27%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 66.36%   [EVAL] batch:  301 | acc: 87.50%,  total acc: 66.43%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 66.52%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 66.57%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 66.66%   [EVAL] batch:  305 | acc: 81.25%,  total acc: 66.71%   [EVAL] batch:  306 | acc: 31.25%,  total acc: 66.59%   [EVAL] batch:  307 | acc: 18.75%,  total acc: 66.44%   [EVAL] batch:  308 | acc: 31.25%,  total acc: 66.32%   [EVAL] batch:  309 | acc: 12.50%,  total acc: 66.15%   [EVAL] batch:  310 | acc: 31.25%,  total acc: 66.04%   [EVAL] batch:  311 | acc: 0.00%,  total acc: 65.83%   [EVAL] batch:  312 | acc: 25.00%,  total acc: 65.69%   [EVAL] batch:  313 | acc: 25.00%,  total acc: 65.57%   [EVAL] batch:  314 | acc: 25.00%,  total acc: 65.44%   [EVAL] batch:  315 | acc: 6.25%,  total acc: 65.25%   [EVAL] batch:  316 | acc: 12.50%,  total acc: 65.08%   [EVAL] batch:  317 | acc: 31.25%,  total acc: 64.98%   [EVAL] batch:  318 | acc: 25.00%,  total acc: 64.85%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 64.90%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 64.99%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 65.08%   [EVAL] batch:  322 | acc: 81.25%,  total acc: 65.13%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 65.18%   [EVAL] batch:  324 | acc: 68.75%,  total acc: 65.19%   [EVAL] batch:  325 | acc: 43.75%,  total acc: 65.13%   [EVAL] batch:  326 | acc: 12.50%,  total acc: 64.97%   [EVAL] batch:  327 | acc: 37.50%,  total acc: 64.88%   [EVAL] batch:  328 | acc: 31.25%,  total acc: 64.78%   [EVAL] batch:  329 | acc: 37.50%,  total acc: 64.70%   [EVAL] batch:  330 | acc: 37.50%,  total acc: 64.61%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 64.68%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 64.79%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 64.90%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 64.98%   [EVAL] batch:  335 | acc: 93.75%,  total acc: 65.07%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 65.17%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 65.24%   [EVAL] batch:  338 | acc: 100.00%,  total acc: 65.34%   [EVAL] batch:  339 | acc: 87.50%,  total acc: 65.40%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 65.51%   [EVAL] batch:  341 | acc: 100.00%,  total acc: 65.61%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 65.71%   [EVAL] batch:  343 | acc: 100.00%,  total acc: 65.81%   [EVAL] batch:  344 | acc: 93.75%,  total acc: 65.89%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 65.99%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 66.05%   [EVAL] batch:  347 | acc: 87.50%,  total acc: 66.11%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 66.19%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 66.29%   [EVAL] batch:  350 | acc: 31.25%,  total acc: 66.19%   [EVAL] batch:  351 | acc: 12.50%,  total acc: 66.03%   [EVAL] batch:  352 | acc: 50.00%,  total acc: 65.99%   [EVAL] batch:  353 | acc: 31.25%,  total acc: 65.89%   [EVAL] batch:  354 | acc: 18.75%,  total acc: 65.76%   [EVAL] batch:  355 | acc: 31.25%,  total acc: 65.66%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 65.72%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 65.80%   [EVAL] batch:  358 | acc: 87.50%,  total acc: 65.86%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 65.95%   [EVAL] batch:  360 | acc: 87.50%,  total acc: 66.01%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 66.07%   [EVAL] batch:  362 | acc: 56.25%,  total acc: 66.05%   [EVAL] batch:  363 | acc: 37.50%,  total acc: 65.97%   [EVAL] batch:  364 | acc: 18.75%,  total acc: 65.84%   [EVAL] batch:  365 | acc: 43.75%,  total acc: 65.78%   [EVAL] batch:  366 | acc: 37.50%,  total acc: 65.70%   [EVAL] batch:  367 | acc: 25.00%,  total acc: 65.59%   [EVAL] batch:  368 | acc: 18.75%,  total acc: 65.46%   [EVAL] batch:  369 | acc: 75.00%,  total acc: 65.49%   [EVAL] batch:  370 | acc: 31.25%,  total acc: 65.40%   [EVAL] batch:  371 | acc: 31.25%,  total acc: 65.31%   [EVAL] batch:  372 | acc: 68.75%,  total acc: 65.32%   [EVAL] batch:  373 | acc: 62.50%,  total acc: 65.31%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 65.30%   [EVAL] batch:  375 | acc: 31.25%,  total acc: 65.21%   [EVAL] batch:  376 | acc: 25.00%,  total acc: 65.10%   [EVAL] batch:  377 | acc: 37.50%,  total acc: 65.03%   [EVAL] batch:  378 | acc: 25.00%,  total acc: 64.92%   [EVAL] batch:  379 | acc: 50.00%,  total acc: 64.88%   [EVAL] batch:  380 | acc: 25.00%,  total acc: 64.78%   [EVAL] batch:  381 | acc: 87.50%,  total acc: 64.84%   [EVAL] batch:  382 | acc: 100.00%,  total acc: 64.93%   [EVAL] batch:  383 | acc: 100.00%,  total acc: 65.02%   [EVAL] batch:  384 | acc: 93.75%,  total acc: 65.10%   [EVAL] batch:  385 | acc: 100.00%,  total acc: 65.19%   [EVAL] batch:  386 | acc: 100.00%,  total acc: 65.28%   [EVAL] batch:  387 | acc: 93.75%,  total acc: 65.35%   [EVAL] batch:  388 | acc: 62.50%,  total acc: 65.34%   [EVAL] batch:  389 | acc: 75.00%,  total acc: 65.37%   [EVAL] batch:  390 | acc: 93.75%,  total acc: 65.44%   [EVAL] batch:  391 | acc: 81.25%,  total acc: 65.48%   [EVAL] batch:  392 | acc: 68.75%,  total acc: 65.49%   [EVAL] batch:  393 | acc: 81.25%,  total acc: 65.53%   [EVAL] batch:  394 | acc: 50.00%,  total acc: 65.49%   [EVAL] batch:  395 | acc: 31.25%,  total acc: 65.40%   [EVAL] batch:  396 | acc: 43.75%,  total acc: 65.35%   [EVAL] batch:  397 | acc: 37.50%,  total acc: 65.28%   [EVAL] batch:  398 | acc: 50.00%,  total acc: 65.24%   [EVAL] batch:  399 | acc: 37.50%,  total acc: 65.17%   [EVAL] batch:  400 | acc: 68.75%,  total acc: 65.18%   [EVAL] batch:  401 | acc: 75.00%,  total acc: 65.21%   [EVAL] batch:  402 | acc: 93.75%,  total acc: 65.28%   [EVAL] batch:  403 | acc: 81.25%,  total acc: 65.32%   [EVAL] batch:  404 | acc: 62.50%,  total acc: 65.31%   [EVAL] batch:  405 | acc: 62.50%,  total acc: 65.30%   [EVAL] batch:  406 | acc: 56.25%,  total acc: 65.28%   [EVAL] batch:  407 | acc: 50.00%,  total acc: 65.24%   [EVAL] batch:  408 | acc: 43.75%,  total acc: 65.19%   [EVAL] batch:  409 | acc: 43.75%,  total acc: 65.14%   [EVAL] batch:  410 | acc: 62.50%,  total acc: 65.13%   [EVAL] batch:  411 | acc: 43.75%,  total acc: 65.08%   [EVAL] batch:  412 | acc: 50.00%,  total acc: 65.04%   [EVAL] batch:  413 | acc: 68.75%,  total acc: 65.05%   [EVAL] batch:  414 | acc: 75.00%,  total acc: 65.08%   [EVAL] batch:  415 | acc: 75.00%,  total acc: 65.10%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 65.14%   [EVAL] batch:  417 | acc: 87.50%,  total acc: 65.19%   [EVAL] batch:  418 | acc: 81.25%,  total acc: 65.23%   [EVAL] batch:  419 | acc: 43.75%,  total acc: 65.18%   [EVAL] batch:  420 | acc: 50.00%,  total acc: 65.14%   [EVAL] batch:  421 | acc: 31.25%,  total acc: 65.06%   [EVAL] batch:  422 | acc: 56.25%,  total acc: 65.04%   [EVAL] batch:  423 | acc: 31.25%,  total acc: 64.96%   [EVAL] batch:  424 | acc: 62.50%,  total acc: 64.96%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 65.04%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 65.12%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 65.20%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 65.28%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 65.36%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 65.44%   [EVAL] batch:  431 | acc: 68.75%,  total acc: 65.45%   [EVAL] batch:  432 | acc: 50.00%,  total acc: 65.42%   [EVAL] batch:  433 | acc: 56.25%,  total acc: 65.39%   [EVAL] batch:  434 | acc: 68.75%,  total acc: 65.40%   [EVAL] batch:  435 | acc: 75.00%,  total acc: 65.42%   [EVAL] batch:  436 | acc: 68.75%,  total acc: 65.43%   [EVAL] batch:  437 | acc: 37.50%,  total acc: 65.37%   
cur_acc:  ['0.9484', '0.8760', '0.6607', '0.6994', '0.7619', '0.6548', '0.6577']
his_acc:  ['0.9484', '0.8945', '0.7809', '0.7412', '0.7296', '0.6872', '0.6537']
CurrentTrain: epoch 15, batch     0 | loss: 11.8751149CurrentTrain: epoch 15, batch     1 | loss: 13.6970938CurrentTrain: epoch 15, batch     2 | loss: 16.1234979CurrentTrain: epoch  1, batch     3 | loss: 7.9812469CurrentTrain: epoch 15, batch     0 | loss: 10.1395383CurrentTrain: epoch 15, batch     1 | loss: 10.2014550CurrentTrain: epoch 15, batch     2 | loss: 11.6694318CurrentTrain: epoch  1, batch     3 | loss: 5.9776266CurrentTrain: epoch 15, batch     0 | loss: 9.6375909CurrentTrain: epoch 15, batch     1 | loss: 8.7695760CurrentTrain: epoch 15, batch     2 | loss: 10.4071145CurrentTrain: epoch  1, batch     3 | loss: 6.6720964CurrentTrain: epoch 15, batch     0 | loss: 8.7956786CurrentTrain: epoch 15, batch     1 | loss: 13.4888756CurrentTrain: epoch 15, batch     2 | loss: 18.6764863CurrentTrain: epoch  1, batch     3 | loss: 8.0239514CurrentTrain: epoch 15, batch     0 | loss: 9.5151313CurrentTrain: epoch 15, batch     1 | loss: 14.5166217CurrentTrain: epoch 15, batch     2 | loss: 14.3833520CurrentTrain: epoch  1, batch     3 | loss: 9.1080157CurrentTrain: epoch 15, batch     0 | loss: 9.1884407CurrentTrain: epoch 15, batch     1 | loss: 8.7247699CurrentTrain: epoch 15, batch     2 | loss: 8.5078546CurrentTrain: epoch  1, batch     3 | loss: 8.0435599CurrentTrain: epoch 15, batch     0 | loss: 7.5941805CurrentTrain: epoch 15, batch     1 | loss: 7.9437905CurrentTrain: epoch 15, batch     2 | loss: 8.5719584CurrentTrain: epoch  1, batch     3 | loss: 7.2753450CurrentTrain: epoch 15, batch     0 | loss: 10.2037329CurrentTrain: epoch 15, batch     1 | loss: 8.6393524CurrentTrain: epoch 15, batch     2 | loss: 8.3006116CurrentTrain: epoch  1, batch     3 | loss: 7.1507051CurrentTrain: epoch 15, batch     0 | loss: 9.2052715CurrentTrain: epoch 15, batch     1 | loss: 9.3403408CurrentTrain: epoch 15, batch     2 | loss: 11.6909370CurrentTrain: epoch  1, batch     3 | loss: 7.2151194CurrentTrain: epoch 15, batch     0 | loss: 9.0897852CurrentTrain: epoch 15, batch     1 | loss: 7.1096815CurrentTrain: epoch 15, batch     2 | loss: 9.3476447CurrentTrain: epoch  1, batch     3 | loss: 8.1449241
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: bryson 's best known work is his 1985 book " evil angels " which chronicles the story of lindy chamberlain 's trial for murder , following the death of her baby daughter , azaria .
Head Entity: evil angels
Tail Entity: death
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: in her groundbreaking 2010 novel "the help," kathryn stockett explores the lives of african american maids working in white households in jackson, mississippi, during the early 1960s.  
Head Entity: the help  
Tail Entity: african american maids  

Relation: main subject  
Context: "sapiens: a brief history of humankind" by yuval noah harari provides an overview of the history of our species, examining how biology and history have defined us and how we can shape our future.  
Head Entity: sapiens: a brief history of humankind  
Tail Entity: biology and history  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: group b of uefa euro 2016 contained england , russia , wales and slovakia .
Head Entity: uefa euro 2016
Tail Entity: slovakia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: the 2020 summer olympics featured countries like the united states, china, and japan competing in various sports.  
Head Entity: 2020 summer olympics  
Tail Entity: japan  

Relation: participating team  
Context: the fifa world cup 2018 saw teams such as france, croatia, and belgium battling for the championship title.  
Head Entity: fifa world cup 2018  
Tail Entity: croatia  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: an indonesian endemic , the moluccan megapode is confined to hill and mountain forests on the maluku islands of halmahera , buru , seram , ambon , ternate , haruku and bacan .
Head Entity: maluku islands
Tail Entity: halmahera
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The human body consists of various organs, including the heart, lungs, and liver, each playing a crucial role in maintaining overall health.  
Head Entity: human body  
Tail Entity: heart  

Relation: has part  
Context: The solar system is made up of several celestial bodies, including planets, moons, and asteroids, with Earth being one of the eight major planets.  
Head Entity: solar system  
Tail Entity: Earth  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: he was nominated for the academy award for best picture , along with steven spielberg , ian bryce , mark gordon for the film " saving private ryan " .
Head Entity: ian bryce
Tail Entity: academy award for best picture
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film "The Shape of Water" received multiple nominations, including a nod for best director for Guillermo del Toro at the Academy Awards.  
Head Entity: Guillermo del Toro  
Tail Entity: Academy Awards  

Relation: nominated for  
Context: The popular band was nominated for the Grammy Award for Best New Artist after their debut album topped the charts.  
Head Entity: the popular band  
Tail Entity: Grammy Award for Best New Artist  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: in 2007 , the county municipality created the transit authority called " skyss " that would market public transport while is would be operated by private companies based on public service obligation contracts .
Head Entity: skyss
Tail Entity: transit authority
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone known as the iPhone has revolutionized the way we communicate and interact with technology, becoming a benchmark for mobile devices worldwide.  
Head Entity: iPhone  
Tail Entity: smartphone  

Relation: instance of  
Context: The Great Wall of China is a remarkable feat of engineering and a symbol of Chinese heritage, attracting millions of tourists each year.  
Head Entity: Great Wall of China  
Tail Entity: engineering marvel  
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: abus valley ( ) is an ice - free valley southeast of turnstile ridge at the north end of britannia range .
Head Entity: turnstile ridge
Tail Entity: britannia range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the sierra nevada ( ) is a mountain range in the western united states, primarily in the state of california, and it includes the famous yosemite national park, which is located within the range.  
Head Entity: sierra nevada  
Tail Entity: yosemite national park  

Relation: mountain range  
Context: the appalachian mountains ( ) extend from the canadian province of quebec down to alabama, making it one of the longest mountain ranges in north america.  
Head Entity: appalachian mountains  
Tail Entity: alabama  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: shaadiram gharjode ( sadashiv amrapurkar ) is once humiliated by hoshiyar chand ( kader khan ) when shaadiram goes to inform him about a family wanting to marry their son to hoshiyar 's daughter .
Head Entity: hoshiyar
Tail Entity: kader khan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: the film "the great escape" features a compelling narrative crafted by the talented screenwriter james clavell, who masterfully brings the story of allied prisoners of war to life.  
Head Entity: james clavell  
Tail Entity: the great escape  

Relation: screenwriter  
Context: in the critically acclaimed movie "pulp fiction," the unique storytelling style is attributed to the brilliant screenwriter quentin tarantino, whose work has left a lasting impact on modern cinema.  
Head Entity: quentin tarantino  
Tail Entity: pulp fiction  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the publisher found success in 1993 with tarō gomi 's " everyone poops " , a japanese language import that sold over one million copies and is now their best - known title .
Head Entity: everyone poops
Tail Entity: japanese language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The animated series "Avatar: The Last Airbender" features a rich narrative and is primarily produced in English, appealing to a global audience.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  

Relation: language of work or name  
Context: The famous novel "Cien años de soledad" by Gabriel García Márquez is a cornerstone of Latin American literature and is originally written in Spanish.  
Head Entity: Cien años de soledad  
Tail Entity: Spanish  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: saratov airlines ( saratov airlines joint stock company , , " saratovskije avialinii " ) is a russian airline headquartered in saratov and based at saratov tsentralny airport .
Head Entity: saratov tsentralny airport
Tail Entity: saratov airlines
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: The New York City Transit Authority operates the subway system, providing essential transportation services to millions of commuters daily.  
Head Entity: New York City Transit Authority  
Tail Entity: subway system  

Relation: operator  
Context: Tesla, Inc. is known for operating its Gigafactory in Nevada, where it produces batteries and electric vehicles.  
Head Entity: Gigafactory  
Tail Entity: Tesla, Inc.  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: baianism is a term applied to the theology of catholic theologian michael baius ( 1513 - 1589 ) .
Head Entity: michael baius
Tail Entity: catholic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the church of england is a major christian denomination in england, known for its unique traditions and beliefs.  
Head Entity: church of england  
Tail Entity: christian  

Relation: religion  
Context: the baha'i faith emphasizes the spiritual unity of all humankind and was founded by baha'u'llah in the 19th century.  
Head Entity: baha'i faith  
Tail Entity: baha'u'llah  
MemoryTrain:  epoch 15, batch     0 | loss: 2.6138963MemoryTrain:  epoch 15, batch     1 | loss: 3.0662278MemoryTrain:  epoch 15, batch     2 | loss: 2.8534411MemoryTrain:  epoch 15, batch     3 | loss: 2.4684239MemoryTrain:  epoch 15, batch     4 | loss: 2.7554333MemoryTrain:  epoch 15, batch     5 | loss: 2.4198857MemoryTrain:  epoch 15, batch     6 | loss: 2.5920712MemoryTrain:  epoch 15, batch     7 | loss: 6.7255107MemoryTrain:  epoch 15, batch     8 | loss: 3.1465745MemoryTrain:  epoch 15, batch     9 | loss: 2.1027227MemoryTrain:  epoch 15, batch    10 | loss: 5.6690820MemoryTrain:  epoch 15, batch    11 | loss: 2.3912082MemoryTrain:  epoch 15, batch    12 | loss: 4.3133873MemoryTrain:  epoch 15, batch    13 | loss: 3.0659111MemoryTrain:  epoch 15, batch    14 | loss: 2.8409325MemoryTrain:  epoch 15, batch     0 | loss: 3.3830055MemoryTrain:  epoch 15, batch     1 | loss: 2.4813542MemoryTrain:  epoch 15, batch     2 | loss: 2.2036041MemoryTrain:  epoch 15, batch     3 | loss: 1.9426858MemoryTrain:  epoch 15, batch     4 | loss: 2.5956718MemoryTrain:  epoch 15, batch     5 | loss: 2.9023134MemoryTrain:  epoch 15, batch     6 | loss: 4.5529961MemoryTrain:  epoch 15, batch     7 | loss: 2.6332028MemoryTrain:  epoch 15, batch     8 | loss: 4.5123888MemoryTrain:  epoch 15, batch     9 | loss: 1.8494464MemoryTrain:  epoch 15, batch    10 | loss: 2.6726816MemoryTrain:  epoch 15, batch    11 | loss: 2.9789351MemoryTrain:  epoch 15, batch    12 | loss: 2.1820113MemoryTrain:  epoch 15, batch    13 | loss: 1.4590817MemoryTrain:  epoch 15, batch    14 | loss: 3.0311023MemoryTrain:  epoch 15, batch     0 | loss: 2.3318636MemoryTrain:  epoch 15, batch     1 | loss: 1.6033001MemoryTrain:  epoch 15, batch     2 | loss: 2.4159794MemoryTrain:  epoch 15, batch     3 | loss: 4.6829705MemoryTrain:  epoch 15, batch     4 | loss: 3.0263868MemoryTrain:  epoch 15, batch     5 | loss: 1.9362248MemoryTrain:  epoch 15, batch     6 | loss: 2.0914736MemoryTrain:  epoch 15, batch     7 | loss: 1.7315804MemoryTrain:  epoch 15, batch     8 | loss: 2.1949762MemoryTrain:  epoch 15, batch     9 | loss: 2.1239670MemoryTrain:  epoch 15, batch    10 | loss: 1.8642940MemoryTrain:  epoch 15, batch    11 | loss: 3.8228921MemoryTrain:  epoch 15, batch    12 | loss: 2.1693824MemoryTrain:  epoch 15, batch    13 | loss: 2.1988526MemoryTrain:  epoch 15, batch    14 | loss: 2.0495720MemoryTrain:  epoch 15, batch     0 | loss: 1.5769466MemoryTrain:  epoch 15, batch     1 | loss: 3.6836011MemoryTrain:  epoch 15, batch     2 | loss: 1.5543373MemoryTrain:  epoch 15, batch     3 | loss: 1.5846488MemoryTrain:  epoch 15, batch     4 | loss: 1.8395843MemoryTrain:  epoch 15, batch     5 | loss: 1.7625807MemoryTrain:  epoch 15, batch     6 | loss: 3.0487620MemoryTrain:  epoch 15, batch     7 | loss: 2.3598101MemoryTrain:  epoch 15, batch     8 | loss: 2.0092842MemoryTrain:  epoch 15, batch     9 | loss: 2.3054088MemoryTrain:  epoch 15, batch    10 | loss: 2.5882787MemoryTrain:  epoch 15, batch    11 | loss: 2.0945579MemoryTrain:  epoch 15, batch    12 | loss: 2.4117710MemoryTrain:  epoch 15, batch    13 | loss: 2.4993175MemoryTrain:  epoch 15, batch    14 | loss: 2.7024394MemoryTrain:  epoch 15, batch     0 | loss: 1.4972996MemoryTrain:  epoch 15, batch     1 | loss: 1.5811232MemoryTrain:  epoch 15, batch     2 | loss: 1.5534016MemoryTrain:  epoch 15, batch     3 | loss: 1.5592285MemoryTrain:  epoch 15, batch     4 | loss: 2.2299066MemoryTrain:  epoch 15, batch     5 | loss: 1.9331895MemoryTrain:  epoch 15, batch     6 | loss: 1.7245699MemoryTrain:  epoch 15, batch     7 | loss: 1.8268692MemoryTrain:  epoch 15, batch     8 | loss: 4.1911670MemoryTrain:  epoch 15, batch     9 | loss: 1.8774558MemoryTrain:  epoch 15, batch    10 | loss: 1.9326030MemoryTrain:  epoch 15, batch    11 | loss: 1.7716434MemoryTrain:  epoch 15, batch    12 | loss: 1.8691962MemoryTrain:  epoch 15, batch    13 | loss: 1.9121902MemoryTrain:  epoch 15, batch    14 | loss: 1.6217984MemoryTrain:  epoch 15, batch     0 | loss: 1.9233856MemoryTrain:  epoch 15, batch     1 | loss: 2.3443986MemoryTrain:  epoch 15, batch     2 | loss: 2.2893899MemoryTrain:  epoch 15, batch     3 | loss: 2.4052192MemoryTrain:  epoch 15, batch     4 | loss: 3.8893457MemoryTrain:  epoch 15, batch     5 | loss: 1.6401274MemoryTrain:  epoch 15, batch     6 | loss: 2.0005723MemoryTrain:  epoch 15, batch     7 | loss: 1.6118417MemoryTrain:  epoch 15, batch     8 | loss: 1.8594919MemoryTrain:  epoch 15, batch     9 | loss: 2.3319242MemoryTrain:  epoch 15, batch    10 | loss: 2.2608105MemoryTrain:  epoch 15, batch    11 | loss: 1.8046053MemoryTrain:  epoch 15, batch    12 | loss: 1.5787564MemoryTrain:  epoch 15, batch    13 | loss: 1.8216934MemoryTrain:  epoch 15, batch    14 | loss: 1.5859676MemoryTrain:  epoch 15, batch     0 | loss: 4.0805976MemoryTrain:  epoch 15, batch     1 | loss: 1.6116798MemoryTrain:  epoch 15, batch     2 | loss: 1.6072011MemoryTrain:  epoch 15, batch     3 | loss: 1.2750801MemoryTrain:  epoch 15, batch     4 | loss: 1.5466334MemoryTrain:  epoch 15, batch     5 | loss: 5.1331162MemoryTrain:  epoch 15, batch     6 | loss: 1.7923891MemoryTrain:  epoch 15, batch     7 | loss: 1.3533462MemoryTrain:  epoch 15, batch     8 | loss: 1.5553009MemoryTrain:  epoch 15, batch     9 | loss: 1.6519615MemoryTrain:  epoch 15, batch    10 | loss: 2.7805206MemoryTrain:  epoch 15, batch    11 | loss: 1.3730396MemoryTrain:  epoch 15, batch    12 | loss: 1.9419835MemoryTrain:  epoch 15, batch    13 | loss: 3.8323731MemoryTrain:  epoch 15, batch    14 | loss: 1.5708731MemoryTrain:  epoch 15, batch     0 | loss: 1.4697186MemoryTrain:  epoch 15, batch     1 | loss: 1.5024150MemoryTrain:  epoch 15, batch     2 | loss: 2.4048732MemoryTrain:  epoch 15, batch     3 | loss: 2.1853853MemoryTrain:  epoch 15, batch     4 | loss: 1.3062995MemoryTrain:  epoch 15, batch     5 | loss: 1.4271064MemoryTrain:  epoch 15, batch     6 | loss: 3.8329352MemoryTrain:  epoch 15, batch     7 | loss: 1.7206789MemoryTrain:  epoch 15, batch     8 | loss: 1.4823576MemoryTrain:  epoch 15, batch     9 | loss: 1.3155659MemoryTrain:  epoch 15, batch    10 | loss: 1.3546824MemoryTrain:  epoch 15, batch    11 | loss: 1.5335482MemoryTrain:  epoch 15, batch    12 | loss: 1.6459765MemoryTrain:  epoch 15, batch    13 | loss: 1.8503285MemoryTrain:  epoch 15, batch    14 | loss: 1.7908331MemoryTrain:  epoch 15, batch     0 | loss: 1.4324409MemoryTrain:  epoch 15, batch     1 | loss: 1.3668362MemoryTrain:  epoch 15, batch     2 | loss: 1.6019308MemoryTrain:  epoch 15, batch     3 | loss: 1.4039942MemoryTrain:  epoch 15, batch     4 | loss: 1.4702163MemoryTrain:  epoch 15, batch     5 | loss: 1.2499765MemoryTrain:  epoch 15, batch     6 | loss: 1.4468463MemoryTrain:  epoch 15, batch     7 | loss: 1.2437447MemoryTrain:  epoch 15, batch     8 | loss: 1.5495828MemoryTrain:  epoch 15, batch     9 | loss: 1.6992932MemoryTrain:  epoch 15, batch    10 | loss: 1.4775760MemoryTrain:  epoch 15, batch    11 | loss: 1.7704382MemoryTrain:  epoch 15, batch    12 | loss: 1.3239254MemoryTrain:  epoch 15, batch    13 | loss: 2.4318098MemoryTrain:  epoch 15, batch    14 | loss: 1.3253154MemoryTrain:  epoch 15, batch     0 | loss: 1.2794014MemoryTrain:  epoch 15, batch     1 | loss: 1.7165428MemoryTrain:  epoch 15, batch     2 | loss: 1.4209012MemoryTrain:  epoch 15, batch     3 | loss: 1.7131414MemoryTrain:  epoch 15, batch     4 | loss: 3.9074011MemoryTrain:  epoch 15, batch     5 | loss: 1.2723580MemoryTrain:  epoch 15, batch     6 | loss: 2.5392369MemoryTrain:  epoch 15, batch     7 | loss: 1.3215281MemoryTrain:  epoch 15, batch     8 | loss: 2.2545568MemoryTrain:  epoch 15, batch     9 | loss: 1.4253996MemoryTrain:  epoch 15, batch    10 | loss: 1.5057913MemoryTrain:  epoch 15, batch    11 | loss: 1.2238873MemoryTrain:  epoch 15, batch    12 | loss: 1.5387088MemoryTrain:  epoch 15, batch    13 | loss: 3.8994354MemoryTrain:  epoch 15, batch    14 | loss: 1.5404631
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 54.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 60.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 61.46%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 63.39%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 63.28%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 63.19%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 63.12%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 64.20%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 63.54%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 61.06%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 56.70%   [EVAL] batch:   14 | acc: 6.25%,  total acc: 53.33%   [EVAL] batch:   15 | acc: 6.25%,  total acc: 50.39%   [EVAL] batch:   16 | acc: 6.25%,  total acc: 47.79%   [EVAL] batch:   17 | acc: 6.25%,  total acc: 45.49%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 44.74%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 47.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 49.70%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 51.99%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 54.08%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 55.99%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 57.50%   [EVAL] batch:   25 | acc: 0.00%,  total acc: 55.29%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 53.24%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 51.56%   [EVAL] batch:   28 | acc: 12.50%,  total acc: 50.22%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 48.75%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 47.18%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 47.66%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 48.67%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 49.82%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 51.07%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 52.08%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 52.70%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 53.12%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 52.24%   [EVAL] batch:   39 | acc: 37.50%,  total acc: 51.88%   [EVAL] batch:   40 | acc: 0.00%,  total acc: 50.61%   [EVAL] batch:   41 | acc: 25.00%,  total acc: 50.00%   [EVAL] batch:   42 | acc: 25.00%,  total acc: 49.42%   [EVAL] batch:   43 | acc: 25.00%,  total acc: 48.86%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 49.17%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 49.86%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 50.53%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 50.65%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 50.77%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 51.00%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 50.86%   [EVAL] batch:   51 | acc: 37.50%,  total acc: 50.60%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 50.47%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 50.46%   [EVAL] batch:   54 | acc: 43.75%,  total acc: 50.34%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 50.33%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 50.77%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 51.40%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 52.12%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 52.60%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 53.18%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 53.53%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 53.47%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:    7 | acc: 25.00%,  total acc: 63.28%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 61.25%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 59.09%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 57.81%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 57.69%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 59.82%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 61.67%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 63.28%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 64.71%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 66.32%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 67.43%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 67.19%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 67.86%   [EVAL] batch:   21 | acc: 50.00%,  total acc: 67.05%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 66.30%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 66.93%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 66.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 67.79%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 68.98%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 69.87%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 70.69%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 71.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 72.58%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 72.85%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 73.48%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 73.16%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 73.39%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 73.61%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 73.82%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 74.01%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 74.68%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 75.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 75.91%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 76.19%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 76.60%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 76.70%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 76.94%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 76.90%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 76.86%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 76.82%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 77.30%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 77.25%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 77.33%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 77.52%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 77.71%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 77.66%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 77.27%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 77.46%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 77.08%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 76.62%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 76.38%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 76.15%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 75.92%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 75.60%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 75.69%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 75.98%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 76.35%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 76.70%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 76.77%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 77.11%   [EVAL] batch:   68 | acc: 87.50%,  total acc: 77.26%   [EVAL] batch:   69 | acc: 37.50%,  total acc: 76.70%   [EVAL] batch:   70 | acc: 43.75%,  total acc: 76.23%   [EVAL] batch:   71 | acc: 31.25%,  total acc: 75.61%   [EVAL] batch:   72 | acc: 43.75%,  total acc: 75.17%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 74.92%   [EVAL] batch:   74 | acc: 31.25%,  total acc: 74.33%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 74.10%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 74.11%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 73.64%   [EVAL] batch:   78 | acc: 25.00%,  total acc: 73.02%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 72.66%   [EVAL] batch:   80 | acc: 43.75%,  total acc: 72.30%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 72.10%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 71.99%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 72.02%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 71.69%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 71.66%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 71.34%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 71.38%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 71.70%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 72.01%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 72.32%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 72.62%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 73.14%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 72.96%   [EVAL] batch:   95 | acc: 75.00%,  total acc: 72.98%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 72.87%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 72.77%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 72.85%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 72.44%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 72.71%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 72.98%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 73.24%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 73.50%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 73.75%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 73.94%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 74.18%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 74.42%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 74.66%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 74.89%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 75.11%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 75.28%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 75.50%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 75.60%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 75.76%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 75.97%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 76.07%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 76.17%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 76.21%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 76.35%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 76.34%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 76.43%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 76.63%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 76.66%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 76.85%   [EVAL] batch:  125 | acc: 0.00%,  total acc: 76.24%   [EVAL] batch:  126 | acc: 0.00%,  total acc: 75.64%   [EVAL] batch:  127 | acc: 0.00%,  total acc: 75.05%   [EVAL] batch:  128 | acc: 0.00%,  total acc: 74.47%   [EVAL] batch:  129 | acc: 0.00%,  total acc: 73.89%   [EVAL] batch:  130 | acc: 6.25%,  total acc: 73.38%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 73.34%   [EVAL] batch:  132 | acc: 68.75%,  total acc: 73.31%   [EVAL] batch:  133 | acc: 62.50%,  total acc: 73.23%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 73.24%   [EVAL] batch:  135 | acc: 56.25%,  total acc: 73.12%   [EVAL] batch:  136 | acc: 56.25%,  total acc: 72.99%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 73.05%   [EVAL] batch:  138 | acc: 75.00%,  total acc: 73.07%   [EVAL] batch:  139 | acc: 62.50%,  total acc: 72.99%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 73.09%   [EVAL] batch:  141 | acc: 62.50%,  total acc: 73.02%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 72.99%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 72.70%   [EVAL] batch:  144 | acc: 0.00%,  total acc: 72.20%   [EVAL] batch:  145 | acc: 6.25%,  total acc: 71.75%   [EVAL] batch:  146 | acc: 12.50%,  total acc: 71.34%   [EVAL] batch:  147 | acc: 18.75%,  total acc: 70.99%   [EVAL] batch:  148 | acc: 6.25%,  total acc: 70.55%   [EVAL] batch:  149 | acc: 6.25%,  total acc: 70.12%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 69.66%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 69.20%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 68.75%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 68.30%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 67.90%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 67.47%   [EVAL] batch:  156 | acc: 43.75%,  total acc: 67.32%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 67.44%   [EVAL] batch:  158 | acc: 68.75%,  total acc: 67.45%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 67.58%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 67.70%   [EVAL] batch:  161 | acc: 75.00%,  total acc: 67.75%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 67.87%   [EVAL] batch:  163 | acc: 93.75%,  total acc: 68.03%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 68.22%   [EVAL] batch:  165 | acc: 87.50%,  total acc: 68.34%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 68.49%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 68.68%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 68.82%   [EVAL] batch:  169 | acc: 68.75%,  total acc: 68.82%   [EVAL] batch:  170 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:  171 | acc: 43.75%,  total acc: 68.60%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 68.50%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 68.50%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 68.32%   [EVAL] batch:  175 | acc: 43.75%,  total acc: 68.18%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 68.11%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 67.94%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 67.95%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 67.88%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 67.89%   [EVAL] batch:  181 | acc: 31.25%,  total acc: 67.69%   [EVAL] batch:  182 | acc: 18.75%,  total acc: 67.42%   [EVAL] batch:  183 | acc: 43.75%,  total acc: 67.29%   [EVAL] batch:  184 | acc: 37.50%,  total acc: 67.13%   [EVAL] batch:  185 | acc: 31.25%,  total acc: 66.94%   [EVAL] batch:  186 | acc: 31.25%,  total acc: 66.74%   [EVAL] batch:  187 | acc: 12.50%,  total acc: 66.46%   [EVAL] batch:  188 | acc: 12.50%,  total acc: 66.17%   [EVAL] batch:  189 | acc: 12.50%,  total acc: 65.89%   [EVAL] batch:  190 | acc: 6.25%,  total acc: 65.58%   [EVAL] batch:  191 | acc: 6.25%,  total acc: 65.27%   [EVAL] batch:  192 | acc: 12.50%,  total acc: 64.99%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 64.76%   [EVAL] batch:  194 | acc: 56.25%,  total acc: 64.71%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 64.70%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 64.72%   [EVAL] batch:  197 | acc: 62.50%,  total acc: 64.71%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 64.54%   [EVAL] batch:  199 | acc: 93.75%,  total acc: 64.69%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 64.68%   [EVAL] batch:  201 | acc: 62.50%,  total acc: 64.67%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 64.69%   [EVAL] batch:  203 | acc: 75.00%,  total acc: 64.74%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 64.79%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 64.84%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 64.95%   [EVAL] batch:  207 | acc: 93.75%,  total acc: 65.08%   [EVAL] batch:  208 | acc: 93.75%,  total acc: 65.22%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 65.36%   [EVAL] batch:  210 | acc: 87.50%,  total acc: 65.46%   [EVAL] batch:  211 | acc: 93.75%,  total acc: 65.60%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 65.61%   [EVAL] batch:  213 | acc: 31.25%,  total acc: 65.45%   [EVAL] batch:  214 | acc: 25.00%,  total acc: 65.26%   [EVAL] batch:  215 | acc: 37.50%,  total acc: 65.13%   [EVAL] batch:  216 | acc: 25.00%,  total acc: 64.95%   [EVAL] batch:  217 | acc: 37.50%,  total acc: 64.82%   [EVAL] batch:  218 | acc: 31.25%,  total acc: 64.67%   [EVAL] batch:  219 | acc: 37.50%,  total acc: 64.55%   [EVAL] batch:  220 | acc: 25.00%,  total acc: 64.37%   [EVAL] batch:  221 | acc: 6.25%,  total acc: 64.10%   [EVAL] batch:  222 | acc: 12.50%,  total acc: 63.87%   [EVAL] batch:  223 | acc: 6.25%,  total acc: 63.62%   [EVAL] batch:  224 | acc: 12.50%,  total acc: 63.39%   [EVAL] batch:  225 | acc: 43.75%,  total acc: 63.30%   [EVAL] batch:  226 | acc: 50.00%,  total acc: 63.24%   [EVAL] batch:  227 | acc: 43.75%,  total acc: 63.16%   [EVAL] batch:  228 | acc: 62.50%,  total acc: 63.16%   [EVAL] batch:  229 | acc: 56.25%,  total acc: 63.12%   [EVAL] batch:  230 | acc: 56.25%,  total acc: 63.10%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 63.12%   [EVAL] batch:  232 | acc: 50.00%,  total acc: 63.06%   [EVAL] batch:  233 | acc: 68.75%,  total acc: 63.09%   [EVAL] batch:  234 | acc: 50.00%,  total acc: 63.03%   [EVAL] batch:  235 | acc: 50.00%,  total acc: 62.98%   [EVAL] batch:  236 | acc: 68.75%,  total acc: 63.00%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 63.10%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 63.26%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 63.41%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 63.56%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 63.71%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 63.86%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 64.01%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 64.13%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 64.20%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 64.32%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 64.39%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 64.53%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 64.68%   [EVAL] batch:  250 | acc: 68.75%,  total acc: 64.69%   [EVAL] batch:  251 | acc: 62.50%,  total acc: 64.68%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 64.75%   [EVAL] batch:  253 | acc: 50.00%,  total acc: 64.69%   [EVAL] batch:  254 | acc: 68.75%,  total acc: 64.71%   [EVAL] batch:  255 | acc: 81.25%,  total acc: 64.77%   [EVAL] batch:  256 | acc: 81.25%,  total acc: 64.83%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 64.92%   [EVAL] batch:  258 | acc: 62.50%,  total acc: 64.91%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 64.95%   [EVAL] batch:  260 | acc: 75.00%,  total acc: 64.99%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 65.00%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 65.11%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 65.15%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 65.19%   [EVAL] batch:  265 | acc: 87.50%,  total acc: 65.27%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 65.33%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 65.44%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 65.50%   [EVAL] batch:  269 | acc: 0.00%,  total acc: 65.25%   [EVAL] batch:  270 | acc: 25.00%,  total acc: 65.11%   [EVAL] batch:  271 | acc: 18.75%,  total acc: 64.94%   [EVAL] batch:  272 | acc: 18.75%,  total acc: 64.77%   [EVAL] batch:  273 | acc: 18.75%,  total acc: 64.60%   [EVAL] batch:  274 | acc: 6.25%,  total acc: 64.39%   [EVAL] batch:  275 | acc: 93.75%,  total acc: 64.49%   [EVAL] batch:  276 | acc: 93.75%,  total acc: 64.60%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 64.73%   [EVAL] batch:  278 | acc: 81.25%,  total acc: 64.78%   [EVAL] batch:  279 | acc: 81.25%,  total acc: 64.84%   [EVAL] batch:  280 | acc: 81.25%,  total acc: 64.90%   [EVAL] batch:  281 | acc: 100.00%,  total acc: 65.03%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 65.15%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 65.27%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 65.39%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 65.52%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 65.64%   [EVAL] batch:  287 | acc: 100.00%,  total acc: 65.76%   [EVAL] batch:  288 | acc: 93.75%,  total acc: 65.85%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 65.93%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 66.00%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 66.10%   [EVAL] batch:  292 | acc: 93.75%,  total acc: 66.19%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 66.24%   [EVAL] batch:  294 | acc: 25.00%,  total acc: 66.10%   [EVAL] batch:  295 | acc: 0.00%,  total acc: 65.88%   [EVAL] batch:  296 | acc: 25.00%,  total acc: 65.74%   [EVAL] batch:  297 | acc: 31.25%,  total acc: 65.62%   [EVAL] batch:  298 | acc: 31.25%,  total acc: 65.51%   [EVAL] batch:  299 | acc: 25.00%,  total acc: 65.38%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 65.47%   [EVAL] batch:  301 | acc: 87.50%,  total acc: 65.54%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 65.64%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 65.69%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 65.78%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 65.85%   [EVAL] batch:  306 | acc: 31.25%,  total acc: 65.74%   [EVAL] batch:  307 | acc: 18.75%,  total acc: 65.58%   [EVAL] batch:  308 | acc: 31.25%,  total acc: 65.47%   [EVAL] batch:  309 | acc: 12.50%,  total acc: 65.30%   [EVAL] batch:  310 | acc: 37.50%,  total acc: 65.21%   [EVAL] batch:  311 | acc: 0.00%,  total acc: 65.00%   [EVAL] batch:  312 | acc: 18.75%,  total acc: 64.86%   [EVAL] batch:  313 | acc: 18.75%,  total acc: 64.71%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 64.56%   [EVAL] batch:  315 | acc: 6.25%,  total acc: 64.38%   [EVAL] batch:  316 | acc: 6.25%,  total acc: 64.20%   [EVAL] batch:  317 | acc: 25.00%,  total acc: 64.07%   [EVAL] batch:  318 | acc: 25.00%,  total acc: 63.95%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 64.02%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 64.12%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 64.21%   [EVAL] batch:  322 | acc: 81.25%,  total acc: 64.26%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 64.33%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 64.37%   [EVAL] batch:  325 | acc: 50.00%,  total acc: 64.32%   [EVAL] batch:  326 | acc: 31.25%,  total acc: 64.22%   [EVAL] batch:  327 | acc: 50.00%,  total acc: 64.18%   [EVAL] batch:  328 | acc: 56.25%,  total acc: 64.15%   [EVAL] batch:  329 | acc: 50.00%,  total acc: 64.11%   [EVAL] batch:  330 | acc: 50.00%,  total acc: 64.07%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 64.14%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 64.25%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 64.35%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 64.44%   [EVAL] batch:  335 | acc: 93.75%,  total acc: 64.53%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 64.63%   [EVAL] batch:  337 | acc: 56.25%,  total acc: 64.61%   [EVAL] batch:  338 | acc: 37.50%,  total acc: 64.53%   [EVAL] batch:  339 | acc: 37.50%,  total acc: 64.45%   [EVAL] batch:  340 | acc: 37.50%,  total acc: 64.37%   [EVAL] batch:  341 | acc: 62.50%,  total acc: 64.36%   [EVAL] batch:  342 | acc: 62.50%,  total acc: 64.36%   [EVAL] batch:  343 | acc: 56.25%,  total acc: 64.34%   [EVAL] batch:  344 | acc: 93.75%,  total acc: 64.42%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 64.52%   [EVAL] batch:  346 | acc: 81.25%,  total acc: 64.57%   [EVAL] batch:  347 | acc: 87.50%,  total acc: 64.64%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 64.72%   [EVAL] batch:  349 | acc: 93.75%,  total acc: 64.80%   [EVAL] batch:  350 | acc: 37.50%,  total acc: 64.73%   [EVAL] batch:  351 | acc: 12.50%,  total acc: 64.58%   [EVAL] batch:  352 | acc: 56.25%,  total acc: 64.55%   [EVAL] batch:  353 | acc: 31.25%,  total acc: 64.46%   [EVAL] batch:  354 | acc: 12.50%,  total acc: 64.31%   [EVAL] batch:  355 | acc: 25.00%,  total acc: 64.20%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 64.27%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 64.35%   [EVAL] batch:  358 | acc: 87.50%,  total acc: 64.42%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 64.51%   [EVAL] batch:  360 | acc: 87.50%,  total acc: 64.58%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 64.68%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 64.69%   [EVAL] batch:  363 | acc: 50.00%,  total acc: 64.65%   [EVAL] batch:  364 | acc: 12.50%,  total acc: 64.50%   [EVAL] batch:  365 | acc: 43.75%,  total acc: 64.45%   [EVAL] batch:  366 | acc: 43.75%,  total acc: 64.39%   [EVAL] batch:  367 | acc: 50.00%,  total acc: 64.35%   [EVAL] batch:  368 | acc: 25.00%,  total acc: 64.24%   [EVAL] batch:  369 | acc: 68.75%,  total acc: 64.26%   [EVAL] batch:  370 | acc: 37.50%,  total acc: 64.18%   [EVAL] batch:  371 | acc: 37.50%,  total acc: 64.11%   [EVAL] batch:  372 | acc: 68.75%,  total acc: 64.13%   [EVAL] batch:  373 | acc: 62.50%,  total acc: 64.12%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 64.12%   [EVAL] batch:  375 | acc: 18.75%,  total acc: 64.00%   [EVAL] batch:  376 | acc: 25.00%,  total acc: 63.89%   [EVAL] batch:  377 | acc: 25.00%,  total acc: 63.79%   [EVAL] batch:  378 | acc: 18.75%,  total acc: 63.67%   [EVAL] batch:  379 | acc: 31.25%,  total acc: 63.59%   [EVAL] batch:  380 | acc: 25.00%,  total acc: 63.48%   [EVAL] batch:  381 | acc: 87.50%,  total acc: 63.55%   [EVAL] batch:  382 | acc: 100.00%,  total acc: 63.64%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 63.72%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 63.78%   [EVAL] batch:  385 | acc: 100.00%,  total acc: 63.88%   [EVAL] batch:  386 | acc: 100.00%,  total acc: 63.97%   [EVAL] batch:  387 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:  388 | acc: 50.00%,  total acc: 64.03%   [EVAL] batch:  389 | acc: 68.75%,  total acc: 64.04%   [EVAL] batch:  390 | acc: 87.50%,  total acc: 64.10%   [EVAL] batch:  391 | acc: 75.00%,  total acc: 64.13%   [EVAL] batch:  392 | acc: 62.50%,  total acc: 64.12%   [EVAL] batch:  393 | acc: 81.25%,  total acc: 64.17%   [EVAL] batch:  394 | acc: 43.75%,  total acc: 64.11%   [EVAL] batch:  395 | acc: 31.25%,  total acc: 64.03%   [EVAL] batch:  396 | acc: 31.25%,  total acc: 63.95%   [EVAL] batch:  397 | acc: 37.50%,  total acc: 63.88%   [EVAL] batch:  398 | acc: 43.75%,  total acc: 63.83%   [EVAL] batch:  399 | acc: 31.25%,  total acc: 63.75%   [EVAL] batch:  400 | acc: 43.75%,  total acc: 63.70%   [EVAL] batch:  401 | acc: 62.50%,  total acc: 63.70%   [EVAL] batch:  402 | acc: 68.75%,  total acc: 63.71%   [EVAL] batch:  403 | acc: 50.00%,  total acc: 63.68%   [EVAL] batch:  404 | acc: 50.00%,  total acc: 63.64%   [EVAL] batch:  405 | acc: 31.25%,  total acc: 63.56%   [EVAL] batch:  406 | acc: 56.25%,  total acc: 63.54%   [EVAL] batch:  407 | acc: 50.00%,  total acc: 63.51%   [EVAL] batch:  408 | acc: 43.75%,  total acc: 63.46%   [EVAL] batch:  409 | acc: 50.00%,  total acc: 63.43%   [EVAL] batch:  410 | acc: 62.50%,  total acc: 63.43%   [EVAL] batch:  411 | acc: 43.75%,  total acc: 63.38%   [EVAL] batch:  412 | acc: 56.25%,  total acc: 63.36%   [EVAL] batch:  413 | acc: 68.75%,  total acc: 63.38%   [EVAL] batch:  414 | acc: 68.75%,  total acc: 63.39%   [EVAL] batch:  415 | acc: 68.75%,  total acc: 63.40%   [EVAL] batch:  416 | acc: 75.00%,  total acc: 63.43%   [EVAL] batch:  417 | acc: 87.50%,  total acc: 63.49%   [EVAL] batch:  418 | acc: 81.25%,  total acc: 63.53%   [EVAL] batch:  419 | acc: 37.50%,  total acc: 63.47%   [EVAL] batch:  420 | acc: 43.75%,  total acc: 63.42%   [EVAL] batch:  421 | acc: 25.00%,  total acc: 63.33%   [EVAL] batch:  422 | acc: 56.25%,  total acc: 63.31%   [EVAL] batch:  423 | acc: 31.25%,  total acc: 63.24%   [EVAL] batch:  424 | acc: 50.00%,  total acc: 63.21%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 63.29%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 63.38%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 63.46%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 63.55%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 63.63%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 63.72%   [EVAL] batch:  431 | acc: 75.00%,  total acc: 63.74%   [EVAL] batch:  432 | acc: 62.50%,  total acc: 63.74%   [EVAL] batch:  433 | acc: 62.50%,  total acc: 63.74%   [EVAL] batch:  434 | acc: 62.50%,  total acc: 63.74%   [EVAL] batch:  435 | acc: 75.00%,  total acc: 63.76%   [EVAL] batch:  436 | acc: 81.25%,  total acc: 63.80%   [EVAL] batch:  437 | acc: 75.00%,  total acc: 63.83%   [EVAL] batch:  438 | acc: 50.00%,  total acc: 63.80%   [EVAL] batch:  439 | acc: 50.00%,  total acc: 63.76%   [EVAL] batch:  440 | acc: 75.00%,  total acc: 63.79%   [EVAL] batch:  441 | acc: 56.25%,  total acc: 63.77%   [EVAL] batch:  442 | acc: 75.00%,  total acc: 63.80%   [EVAL] batch:  443 | acc: 68.75%,  total acc: 63.81%   [EVAL] batch:  444 | acc: 68.75%,  total acc: 63.82%   [EVAL] batch:  445 | acc: 62.50%,  total acc: 63.82%   [EVAL] batch:  446 | acc: 62.50%,  total acc: 63.81%   [EVAL] batch:  447 | acc: 75.00%,  total acc: 63.84%   [EVAL] batch:  448 | acc: 56.25%,  total acc: 63.82%   [EVAL] batch:  449 | acc: 56.25%,  total acc: 63.81%   [EVAL] batch:  450 | acc: 0.00%,  total acc: 63.66%   [EVAL] batch:  451 | acc: 0.00%,  total acc: 63.52%   [EVAL] batch:  452 | acc: 12.50%,  total acc: 63.41%   [EVAL] batch:  453 | acc: 0.00%,  total acc: 63.27%   [EVAL] batch:  454 | acc: 12.50%,  total acc: 63.16%   [EVAL] batch:  455 | acc: 6.25%,  total acc: 63.03%   [EVAL] batch:  456 | acc: 68.75%,  total acc: 63.05%   [EVAL] batch:  457 | acc: 100.00%,  total acc: 63.13%   [EVAL] batch:  458 | acc: 100.00%,  total acc: 63.21%   [EVAL] batch:  459 | acc: 100.00%,  total acc: 63.29%   [EVAL] batch:  460 | acc: 100.00%,  total acc: 63.37%   [EVAL] batch:  461 | acc: 93.75%,  total acc: 63.43%   [EVAL] batch:  462 | acc: 50.00%,  total acc: 63.40%   [EVAL] batch:  463 | acc: 0.00%,  total acc: 63.27%   [EVAL] batch:  464 | acc: 0.00%,  total acc: 63.13%   [EVAL] batch:  465 | acc: 6.25%,  total acc: 63.01%   [EVAL] batch:  466 | acc: 18.75%,  total acc: 62.91%   [EVAL] batch:  467 | acc: 0.00%,  total acc: 62.78%   [EVAL] batch:  468 | acc: 25.00%,  total acc: 62.70%   [EVAL] batch:  469 | acc: 75.00%,  total acc: 62.73%   [EVAL] batch:  470 | acc: 93.75%,  total acc: 62.79%   [EVAL] batch:  471 | acc: 81.25%,  total acc: 62.83%   [EVAL] batch:  472 | acc: 87.50%,  total acc: 62.88%   [EVAL] batch:  473 | acc: 93.75%,  total acc: 62.95%   [EVAL] batch:  474 | acc: 81.25%,  total acc: 62.99%   [EVAL] batch:  475 | acc: 31.25%,  total acc: 62.92%   [EVAL] batch:  476 | acc: 37.50%,  total acc: 62.87%   [EVAL] batch:  477 | acc: 6.25%,  total acc: 62.75%   [EVAL] batch:  478 | acc: 6.25%,  total acc: 62.63%   [EVAL] batch:  479 | acc: 37.50%,  total acc: 62.58%   [EVAL] batch:  480 | acc: 12.50%,  total acc: 62.47%   [EVAL] batch:  481 | acc: 43.75%,  total acc: 62.44%   [EVAL] batch:  482 | acc: 81.25%,  total acc: 62.47%   [EVAL] batch:  483 | acc: 81.25%,  total acc: 62.51%   [EVAL] batch:  484 | acc: 68.75%,  total acc: 62.53%   [EVAL] batch:  485 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:  486 | acc: 68.75%,  total acc: 62.51%   [EVAL] batch:  487 | acc: 50.00%,  total acc: 62.49%   [EVAL] batch:  488 | acc: 37.50%,  total acc: 62.44%   [EVAL] batch:  489 | acc: 50.00%,  total acc: 62.41%   [EVAL] batch:  490 | acc: 43.75%,  total acc: 62.37%   [EVAL] batch:  491 | acc: 43.75%,  total acc: 62.33%   [EVAL] batch:  492 | acc: 37.50%,  total acc: 62.28%   [EVAL] batch:  493 | acc: 56.25%,  total acc: 62.27%   [EVAL] batch:  494 | acc: 93.75%,  total acc: 62.34%   [EVAL] batch:  495 | acc: 93.75%,  total acc: 62.40%   [EVAL] batch:  496 | acc: 81.25%,  total acc: 62.44%   [EVAL] batch:  497 | acc: 81.25%,  total acc: 62.47%   [EVAL] batch:  498 | acc: 81.25%,  total acc: 62.51%   [EVAL] batch:  499 | acc: 93.75%,  total acc: 62.58%   
cur_acc:  ['0.9484', '0.8760', '0.6607', '0.6994', '0.7619', '0.6548', '0.6577', '0.5347']
his_acc:  ['0.9484', '0.8945', '0.7809', '0.7412', '0.7296', '0.6872', '0.6537', '0.6258']
--------Round  5
seed:  600
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 2 0 1 6 3 4 5]
prepared data!
CurrentTrain: epoch 15, batch     0 | loss: 26.8831554CurrentTrain: epoch 15, batch     1 | loss: 23.6255659CurrentTrain: epoch 15, batch     2 | loss: 26.6602913CurrentTrain: epoch 15, batch     3 | loss: 23.7582724CurrentTrain: epoch 15, batch     4 | loss: 19.9793300CurrentTrain: epoch 15, batch     5 | loss: 20.0579609CurrentTrain: epoch 15, batch     6 | loss: 21.2700397CurrentTrain: epoch 15, batch     7 | loss: 21.2593775CurrentTrain: epoch 15, batch     8 | loss: 19.6604422CurrentTrain: epoch 15, batch     9 | loss: 19.4994819CurrentTrain: epoch 15, batch    10 | loss: 18.0419604CurrentTrain: epoch 15, batch    11 | loss: 14.7819056CurrentTrain: epoch 15, batch    12 | loss: 20.7395708CurrentTrain: epoch 15, batch    13 | loss: 21.1376331CurrentTrain: epoch 15, batch    14 | loss: 18.4813498CurrentTrain: epoch 15, batch    15 | loss: 21.2015680CurrentTrain: epoch 15, batch    16 | loss: 18.3861024CurrentTrain: epoch 15, batch    17 | loss: 37.3294112CurrentTrain: epoch 15, batch    18 | loss: 27.5059524CurrentTrain: epoch 15, batch    19 | loss: 16.5118376CurrentTrain: epoch 15, batch    20 | loss: 18.0927722CurrentTrain: epoch 15, batch    21 | loss: 15.1469522CurrentTrain: epoch 15, batch    22 | loss: 15.5931775CurrentTrain: epoch 15, batch    23 | loss: 14.1471088CurrentTrain: epoch 15, batch    24 | loss: 17.5963108CurrentTrain: epoch 15, batch    25 | loss: 16.3671656CurrentTrain: epoch 15, batch    26 | loss: 14.7051812CurrentTrain: epoch 15, batch    27 | loss: 26.0250045CurrentTrain: epoch 15, batch    28 | loss: 12.8125427CurrentTrain: epoch 15, batch    29 | loss: 19.5484182CurrentTrain: epoch 15, batch    30 | loss: 16.3513908CurrentTrain: epoch 15, batch    31 | loss: 11.7646589CurrentTrain: epoch 15, batch    32 | loss: 15.1212978CurrentTrain: epoch 15, batch    33 | loss: 32.3957699CurrentTrain: epoch 15, batch    34 | loss: 19.7585380CurrentTrain: epoch 15, batch    35 | loss: 17.4683385CurrentTrain: epoch 15, batch    36 | loss: 15.7279631CurrentTrain: epoch 15, batch    37 | loss: 14.3593553CurrentTrain: epoch 15, batch    38 | loss: 16.0754339CurrentTrain: epoch 15, batch    39 | loss: 13.4976090CurrentTrain: epoch 15, batch    40 | loss: 13.4648856CurrentTrain: epoch 15, batch    41 | loss: 26.0170272CurrentTrain: epoch 15, batch    42 | loss: 11.7886433CurrentTrain: epoch 15, batch    43 | loss: 18.8720419CurrentTrain: epoch 15, batch    44 | loss: 16.8396672CurrentTrain: epoch 15, batch    45 | loss: 18.3406555CurrentTrain: epoch 15, batch    46 | loss: 20.3039074CurrentTrain: epoch 15, batch    47 | loss: 14.0992648CurrentTrain: epoch 15, batch    48 | loss: 11.0439564CurrentTrain: epoch 15, batch    49 | loss: 16.3643168CurrentTrain: epoch 15, batch    50 | loss: 19.2851320CurrentTrain: epoch 15, batch    51 | loss: 11.6471066CurrentTrain: epoch 15, batch    52 | loss: 16.3847729CurrentTrain: epoch 15, batch    53 | loss: 15.6919438CurrentTrain: epoch 15, batch    54 | loss: 19.3714440CurrentTrain: epoch 15, batch    55 | loss: 23.3485511CurrentTrain: epoch 15, batch    56 | loss: 15.8646575CurrentTrain: epoch 15, batch    57 | loss: 14.1583945CurrentTrain: epoch 15, batch    58 | loss: 31.6855752CurrentTrain: epoch 15, batch    59 | loss: 23.2493621CurrentTrain: epoch 15, batch    60 | loss: 11.8318221CurrentTrain: epoch 15, batch    61 | loss: 16.0460962CurrentTrain: epoch  7, batch    62 | loss: 27.2469173CurrentTrain: epoch 15, batch     0 | loss: 11.1655367CurrentTrain: epoch 15, batch     1 | loss: 12.3477259CurrentTrain: epoch 15, batch     2 | loss: 13.5852366CurrentTrain: epoch 15, batch     3 | loss: 15.9177963CurrentTrain: epoch 15, batch     4 | loss: 15.6298593CurrentTrain: epoch 15, batch     5 | loss: 16.2294170CurrentTrain: epoch 15, batch     6 | loss: 12.3841734CurrentTrain: epoch 15, batch     7 | loss: 10.7382836CurrentTrain: epoch 15, batch     8 | loss: 13.5381305CurrentTrain: epoch 15, batch     9 | loss: 12.5982525CurrentTrain: epoch 15, batch    10 | loss: 14.8037387CurrentTrain: epoch 15, batch    11 | loss: 17.4970982CurrentTrain: epoch 15, batch    12 | loss: 11.3592746CurrentTrain: epoch 15, batch    13 | loss: 19.4520386CurrentTrain: epoch 15, batch    14 | loss: 13.3454397CurrentTrain: epoch 15, batch    15 | loss: 25.3983185CurrentTrain: epoch 15, batch    16 | loss: 18.3104841CurrentTrain: epoch 15, batch    17 | loss: 21.7680375CurrentTrain: epoch 15, batch    18 | loss: 15.4437298CurrentTrain: epoch 15, batch    19 | loss: 24.2201977CurrentTrain: epoch 15, batch    20 | loss: 19.1609949CurrentTrain: epoch 15, batch    21 | loss: 12.1721471CurrentTrain: epoch 15, batch    22 | loss: 11.1565292CurrentTrain: epoch 15, batch    23 | loss: 12.1003479CurrentTrain: epoch 15, batch    24 | loss: 12.3434706CurrentTrain: epoch 15, batch    25 | loss: 9.2324195CurrentTrain: epoch 15, batch    26 | loss: 15.9314535CurrentTrain: epoch 15, batch    27 | loss: 10.4700096CurrentTrain: epoch 15, batch    28 | loss: 15.3770844CurrentTrain: epoch 15, batch    29 | loss: 23.1657228CurrentTrain: epoch 15, batch    30 | loss: 14.6094200CurrentTrain: epoch 15, batch    31 | loss: 19.6789052CurrentTrain: epoch 15, batch    32 | loss: 13.1755742CurrentTrain: epoch 15, batch    33 | loss: 21.5256530CurrentTrain: epoch 15, batch    34 | loss: 17.5859931CurrentTrain: epoch 15, batch    35 | loss: 20.6374448CurrentTrain: epoch 15, batch    36 | loss: 15.0757516CurrentTrain: epoch 15, batch    37 | loss: 13.9977486CurrentTrain: epoch 15, batch    38 | loss: 12.2050097CurrentTrain: epoch 15, batch    39 | loss: 16.2930528CurrentTrain: epoch 15, batch    40 | loss: 28.0266433CurrentTrain: epoch 15, batch    41 | loss: 12.6399466CurrentTrain: epoch 15, batch    42 | loss: 11.0000795CurrentTrain: epoch 15, batch    43 | loss: 14.8612509CurrentTrain: epoch 15, batch    44 | loss: 14.6533696CurrentTrain: epoch 15, batch    45 | loss: 14.5261322CurrentTrain: epoch 15, batch    46 | loss: 15.3599111CurrentTrain: epoch 15, batch    47 | loss: 16.1278588CurrentTrain: epoch 15, batch    48 | loss: 11.8618188CurrentTrain: epoch 15, batch    49 | loss: 14.6640719CurrentTrain: epoch 15, batch    50 | loss: 17.3541706CurrentTrain: epoch 15, batch    51 | loss: 12.5685773CurrentTrain: epoch 15, batch    52 | loss: 13.0438677CurrentTrain: epoch 15, batch    53 | loss: 10.2579128CurrentTrain: epoch 15, batch    54 | loss: 11.7632599CurrentTrain: epoch 15, batch    55 | loss: 13.2899306CurrentTrain: epoch 15, batch    56 | loss: 11.4288664CurrentTrain: epoch 15, batch    57 | loss: 20.3418360CurrentTrain: epoch 15, batch    58 | loss: 17.1620796CurrentTrain: epoch 15, batch    59 | loss: 15.9972837CurrentTrain: epoch 15, batch    60 | loss: 12.3875804CurrentTrain: epoch 15, batch    61 | loss: 13.2126711CurrentTrain: epoch  7, batch    62 | loss: 7.6395882CurrentTrain: epoch 15, batch     0 | loss: 10.7254678CurrentTrain: epoch 15, batch     1 | loss: 11.7072060CurrentTrain: epoch 15, batch     2 | loss: 10.8871252CurrentTrain: epoch 15, batch     3 | loss: 14.4103339CurrentTrain: epoch 15, batch     4 | loss: 24.0228997CurrentTrain: epoch 15, batch     5 | loss: 13.3915714CurrentTrain: epoch 15, batch     6 | loss: 12.6351322CurrentTrain: epoch 15, batch     7 | loss: 13.8045288CurrentTrain: epoch 15, batch     8 | loss: 12.2705422CurrentTrain: epoch 15, batch     9 | loss: 10.8454196CurrentTrain: epoch 15, batch    10 | loss: 23.0387699CurrentTrain: epoch 15, batch    11 | loss: 24.2064838CurrentTrain: epoch 15, batch    12 | loss: 7.7058059CurrentTrain: epoch 15, batch    13 | loss: 11.4411591CurrentTrain: epoch 15, batch    14 | loss: 14.1801079CurrentTrain: epoch 15, batch    15 | loss: 14.2181841CurrentTrain: epoch 15, batch    16 | loss: 9.5495222CurrentTrain: epoch 15, batch    17 | loss: 8.5204795CurrentTrain: epoch 15, batch    18 | loss: 17.9345903CurrentTrain: epoch 15, batch    19 | loss: 18.2345175CurrentTrain: epoch 15, batch    20 | loss: 18.5282509CurrentTrain: epoch 15, batch    21 | loss: 18.7309460CurrentTrain: epoch 15, batch    22 | loss: 17.5792385CurrentTrain: epoch 15, batch    23 | loss: 10.8787936CurrentTrain: epoch 15, batch    24 | loss: 10.0840899CurrentTrain: epoch 15, batch    25 | loss: 13.5364883CurrentTrain: epoch 15, batch    26 | loss: 10.1276669CurrentTrain: epoch 15, batch    27 | loss: 23.6771614CurrentTrain: epoch 15, batch    28 | loss: 19.0787234CurrentTrain: epoch 15, batch    29 | loss: 16.8180578CurrentTrain: epoch 15, batch    30 | loss: 16.3820042CurrentTrain: epoch 15, batch    31 | loss: 23.8999676CurrentTrain: epoch 15, batch    32 | loss: 12.2781926CurrentTrain: epoch 15, batch    33 | loss: 16.0026281CurrentTrain: epoch 15, batch    34 | loss: 15.0463102CurrentTrain: epoch 15, batch    35 | loss: 12.4896189CurrentTrain: epoch 15, batch    36 | loss: 10.8329255CurrentTrain: epoch 15, batch    37 | loss: 11.6281329CurrentTrain: epoch 15, batch    38 | loss: 11.3359589CurrentTrain: epoch 15, batch    39 | loss: 12.8721649CurrentTrain: epoch 15, batch    40 | loss: 16.2628332CurrentTrain: epoch 15, batch    41 | loss: 16.8329993CurrentTrain: epoch 15, batch    42 | loss: 13.8385122CurrentTrain: epoch 15, batch    43 | loss: 15.9275563CurrentTrain: epoch 15, batch    44 | loss: 11.8422917CurrentTrain: epoch 15, batch    45 | loss: 11.0401518CurrentTrain: epoch 15, batch    46 | loss: 10.3160398CurrentTrain: epoch 15, batch    47 | loss: 19.8321337CurrentTrain: epoch 15, batch    48 | loss: 13.1766147CurrentTrain: epoch 15, batch    49 | loss: 16.8994799CurrentTrain: epoch 15, batch    50 | loss: 18.5801227CurrentTrain: epoch 15, batch    51 | loss: 23.9363499CurrentTrain: epoch 15, batch    52 | loss: 16.7267425CurrentTrain: epoch 15, batch    53 | loss: 10.8738353CurrentTrain: epoch 15, batch    54 | loss: 25.2746716CurrentTrain: epoch 15, batch    55 | loss: 12.3418081CurrentTrain: epoch 15, batch    56 | loss: 18.5384668CurrentTrain: epoch 15, batch    57 | loss: 10.1925087CurrentTrain: epoch 15, batch    58 | loss: 12.4548040CurrentTrain: epoch 15, batch    59 | loss: 10.6075601CurrentTrain: epoch 15, batch    60 | loss: 10.4129607CurrentTrain: epoch 15, batch    61 | loss: 17.1515768CurrentTrain: epoch  7, batch    62 | loss: 8.3263366CurrentTrain: epoch 15, batch     0 | loss: 24.2774239CurrentTrain: epoch 15, batch     1 | loss: 21.5437043CurrentTrain: epoch 15, batch     2 | loss: 13.1023083CurrentTrain: epoch 15, batch     3 | loss: 15.0794033CurrentTrain: epoch 15, batch     4 | loss: 11.4384331CurrentTrain: epoch 15, batch     5 | loss: 13.3996113CurrentTrain: epoch 15, batch     6 | loss: 14.2670292CurrentTrain: epoch 15, batch     7 | loss: 11.7400404CurrentTrain: epoch 15, batch     8 | loss: 15.9628730CurrentTrain: epoch 15, batch     9 | loss: 11.6859381CurrentTrain: epoch 15, batch    10 | loss: 15.1304427CurrentTrain: epoch 15, batch    11 | loss: 11.5933865CurrentTrain: epoch 15, batch    12 | loss: 13.0569063CurrentTrain: epoch 15, batch    13 | loss: 16.8284989CurrentTrain: epoch 15, batch    14 | loss: 12.0327738CurrentTrain: epoch 15, batch    15 | loss: 10.6916055CurrentTrain: epoch 15, batch    16 | loss: 13.6057515CurrentTrain: epoch 15, batch    17 | loss: 15.1536469CurrentTrain: epoch 15, batch    18 | loss: 14.5614981CurrentTrain: epoch 15, batch    19 | loss: 15.4165604CurrentTrain: epoch 15, batch    20 | loss: 10.6313796CurrentTrain: epoch 15, batch    21 | loss: 8.1080893CurrentTrain: epoch 15, batch    22 | loss: 14.0523830CurrentTrain: epoch 15, batch    23 | loss: 12.2905211CurrentTrain: epoch 15, batch    24 | loss: 11.8133161CurrentTrain: epoch 15, batch    25 | loss: 8.3547783CurrentTrain: epoch 15, batch    26 | loss: 19.3516401CurrentTrain: epoch 15, batch    27 | loss: 12.4782425CurrentTrain: epoch 15, batch    28 | loss: 13.9653717CurrentTrain: epoch 15, batch    29 | loss: 10.2710553CurrentTrain: epoch 15, batch    30 | loss: 12.3732539CurrentTrain: epoch 15, batch    31 | loss: 12.3801268CurrentTrain: epoch 15, batch    32 | loss: 11.7692831CurrentTrain: epoch 15, batch    33 | loss: 7.7818755CurrentTrain: epoch 15, batch    34 | loss: 12.6881682CurrentTrain: epoch 15, batch    35 | loss: 11.4610782CurrentTrain: epoch 15, batch    36 | loss: 12.4266946CurrentTrain: epoch 15, batch    37 | loss: 19.1560381CurrentTrain: epoch 15, batch    38 | loss: 18.4694060CurrentTrain: epoch 15, batch    39 | loss: 13.7256014CurrentTrain: epoch 15, batch    40 | loss: 12.4891597CurrentTrain: epoch 15, batch    41 | loss: 33.4843751CurrentTrain: epoch 15, batch    42 | loss: 9.6892715CurrentTrain: epoch 15, batch    43 | loss: 17.2123094CurrentTrain: epoch 15, batch    44 | loss: 20.8886583CurrentTrain: epoch 15, batch    45 | loss: 10.4548325CurrentTrain: epoch 15, batch    46 | loss: 6.5531887CurrentTrain: epoch 15, batch    47 | loss: 16.7340908CurrentTrain: epoch 15, batch    48 | loss: 25.3314469CurrentTrain: epoch 15, batch    49 | loss: 7.7100345CurrentTrain: epoch 15, batch    50 | loss: 12.7232776CurrentTrain: epoch 15, batch    51 | loss: 19.0321832CurrentTrain: epoch 15, batch    52 | loss: 13.0197072CurrentTrain: epoch 15, batch    53 | loss: 10.6743411CurrentTrain: epoch 15, batch    54 | loss: 25.5418528CurrentTrain: epoch 15, batch    55 | loss: 17.0262983CurrentTrain: epoch 15, batch    56 | loss: 18.1409331CurrentTrain: epoch 15, batch    57 | loss: 13.5667915CurrentTrain: epoch 15, batch    58 | loss: 15.5967108CurrentTrain: epoch 15, batch    59 | loss: 9.5420005CurrentTrain: epoch 15, batch    60 | loss: 13.3227077CurrentTrain: epoch 15, batch    61 | loss: 8.9887828CurrentTrain: epoch  7, batch    62 | loss: 7.1821891CurrentTrain: epoch 15, batch     0 | loss: 17.3134345CurrentTrain: epoch 15, batch     1 | loss: 19.3843827CurrentTrain: epoch 15, batch     2 | loss: 9.6771816CurrentTrain: epoch 15, batch     3 | loss: 13.6761697CurrentTrain: epoch 15, batch     4 | loss: 8.3961035CurrentTrain: epoch 15, batch     5 | loss: 10.5546478CurrentTrain: epoch 15, batch     6 | loss: 17.0642006CurrentTrain: epoch 15, batch     7 | loss: 10.6688299CurrentTrain: epoch 15, batch     8 | loss: 14.5027857CurrentTrain: epoch 15, batch     9 | loss: 12.4702351CurrentTrain: epoch 15, batch    10 | loss: 11.7508042CurrentTrain: epoch 15, batch    11 | loss: 10.2124030CurrentTrain: epoch 15, batch    12 | loss: 20.3939446CurrentTrain: epoch 15, batch    13 | loss: 12.6608964CurrentTrain: epoch 15, batch    14 | loss: 14.4448321CurrentTrain: epoch 15, batch    15 | loss: 9.0247971CurrentTrain: epoch 15, batch    16 | loss: 12.5182884CurrentTrain: epoch 15, batch    17 | loss: 18.5097288CurrentTrain: epoch 15, batch    18 | loss: 7.4809142CurrentTrain: epoch 15, batch    19 | loss: 12.5582971CurrentTrain: epoch 15, batch    20 | loss: 9.6954857CurrentTrain: epoch 15, batch    21 | loss: 7.6708515CurrentTrain: epoch 15, batch    22 | loss: 16.4826281CurrentTrain: epoch 15, batch    23 | loss: 11.0434750CurrentTrain: epoch 15, batch    24 | loss: 10.5384112CurrentTrain: epoch 15, batch    25 | loss: 16.8564556CurrentTrain: epoch 15, batch    26 | loss: 11.3325704CurrentTrain: epoch 15, batch    27 | loss: 9.1574940CurrentTrain: epoch 15, batch    28 | loss: 16.5524585CurrentTrain: epoch 15, batch    29 | loss: 17.5884823CurrentTrain: epoch 15, batch    30 | loss: 10.3359170CurrentTrain: epoch 15, batch    31 | loss: 12.8508128CurrentTrain: epoch 15, batch    32 | loss: 21.3707181CurrentTrain: epoch 15, batch    33 | loss: 11.5303517CurrentTrain: epoch 15, batch    34 | loss: 11.5997274CurrentTrain: epoch 15, batch    35 | loss: 11.9432348CurrentTrain: epoch 15, batch    36 | loss: 9.1693737CurrentTrain: epoch 15, batch    37 | loss: 7.2928446CurrentTrain: epoch 15, batch    38 | loss: 9.6141093CurrentTrain: epoch 15, batch    39 | loss: 13.1467821CurrentTrain: epoch 15, batch    40 | loss: 15.3101020CurrentTrain: epoch 15, batch    41 | loss: 10.3235470CurrentTrain: epoch 15, batch    42 | loss: 12.2933387CurrentTrain: epoch 15, batch    43 | loss: 13.5476646CurrentTrain: epoch 15, batch    44 | loss: 11.4225040CurrentTrain: epoch 15, batch    45 | loss: 11.4335602CurrentTrain: epoch 15, batch    46 | loss: 9.4272476CurrentTrain: epoch 15, batch    47 | loss: 13.7703273CurrentTrain: epoch 15, batch    48 | loss: 7.2162432CurrentTrain: epoch 15, batch    49 | loss: 21.2739296CurrentTrain: epoch 15, batch    50 | loss: 10.7754766CurrentTrain: epoch 15, batch    51 | loss: 18.9983794CurrentTrain: epoch 15, batch    52 | loss: 16.4311907CurrentTrain: epoch 15, batch    53 | loss: 8.1947494CurrentTrain: epoch 15, batch    54 | loss: 12.6519308CurrentTrain: epoch 15, batch    55 | loss: 10.6345961CurrentTrain: epoch 15, batch    56 | loss: 8.2572200CurrentTrain: epoch 15, batch    57 | loss: 15.6497680CurrentTrain: epoch 15, batch    58 | loss: 11.7328871CurrentTrain: epoch 15, batch    59 | loss: 14.1184544CurrentTrain: epoch 15, batch    60 | loss: 24.7062303CurrentTrain: epoch 15, batch    61 | loss: 16.4113033CurrentTrain: epoch  7, batch    62 | loss: 7.6368144CurrentTrain: epoch 15, batch     0 | loss: 9.5691827CurrentTrain: epoch 15, batch     1 | loss: 10.7426977CurrentTrain: epoch 15, batch     2 | loss: 13.3946517CurrentTrain: epoch 15, batch     3 | loss: 10.1960788CurrentTrain: epoch 15, batch     4 | loss: 8.7362797CurrentTrain: epoch 15, batch     5 | loss: 10.6571357CurrentTrain: epoch 15, batch     6 | loss: 12.4424180CurrentTrain: epoch 15, batch     7 | loss: 8.2074616CurrentTrain: epoch 15, batch     8 | loss: 8.5917461CurrentTrain: epoch 15, batch     9 | loss: 7.2050374CurrentTrain: epoch 15, batch    10 | loss: 13.8419463CurrentTrain: epoch 15, batch    11 | loss: 30.6438384CurrentTrain: epoch 15, batch    12 | loss: 12.2813496CurrentTrain: epoch 15, batch    13 | loss: 9.4739531CurrentTrain: epoch 15, batch    14 | loss: 11.3423742CurrentTrain: epoch 15, batch    15 | loss: 9.9104279CurrentTrain: epoch 15, batch    16 | loss: 7.7668281CurrentTrain: epoch 15, batch    17 | loss: 10.0484745CurrentTrain: epoch 15, batch    18 | loss: 9.6387138CurrentTrain: epoch 15, batch    19 | loss: 15.7311247CurrentTrain: epoch 15, batch    20 | loss: 12.2723961CurrentTrain: epoch 15, batch    21 | loss: 19.2931099CurrentTrain: epoch 15, batch    22 | loss: 13.5715812CurrentTrain: epoch 15, batch    23 | loss: 9.5188775CurrentTrain: epoch 15, batch    24 | loss: 23.7051987CurrentTrain: epoch 15, batch    25 | loss: 14.1009393CurrentTrain: epoch 15, batch    26 | loss: 9.9599819CurrentTrain: epoch 15, batch    27 | loss: 9.4093424CurrentTrain: epoch 15, batch    28 | loss: 11.3199483CurrentTrain: epoch 15, batch    29 | loss: 10.3168394CurrentTrain: epoch 15, batch    30 | loss: 15.2653725CurrentTrain: epoch 15, batch    31 | loss: 9.7930600CurrentTrain: epoch 15, batch    32 | loss: 9.2330082CurrentTrain: epoch 15, batch    33 | loss: 7.7432297CurrentTrain: epoch 15, batch    34 | loss: 16.9567692CurrentTrain: epoch 15, batch    35 | loss: 14.2135981CurrentTrain: epoch 15, batch    36 | loss: 9.3956689CurrentTrain: epoch 15, batch    37 | loss: 9.4919009CurrentTrain: epoch 15, batch    38 | loss: 9.8899145CurrentTrain: epoch 15, batch    39 | loss: 16.7011468CurrentTrain: epoch 15, batch    40 | loss: 15.9677148CurrentTrain: epoch 15, batch    41 | loss: 18.1879193CurrentTrain: epoch 15, batch    42 | loss: 17.4824639CurrentTrain: epoch 15, batch    43 | loss: 16.1883151CurrentTrain: epoch 15, batch    44 | loss: 10.8275878CurrentTrain: epoch 15, batch    45 | loss: 9.0646620CurrentTrain: epoch 15, batch    46 | loss: 7.8354311CurrentTrain: epoch 15, batch    47 | loss: 7.0107142CurrentTrain: epoch 15, batch    48 | loss: 10.3448303CurrentTrain: epoch 15, batch    49 | loss: 12.2943623CurrentTrain: epoch 15, batch    50 | loss: 14.9127920CurrentTrain: epoch 15, batch    51 | loss: 8.7796447CurrentTrain: epoch 15, batch    52 | loss: 8.7030380CurrentTrain: epoch 15, batch    53 | loss: 11.0759947CurrentTrain: epoch 15, batch    54 | loss: 7.4383313CurrentTrain: epoch 15, batch    55 | loss: 10.1052059CurrentTrain: epoch 15, batch    56 | loss: 7.9316488CurrentTrain: epoch 15, batch    57 | loss: 7.3290864CurrentTrain: epoch 15, batch    58 | loss: 11.9794089CurrentTrain: epoch 15, batch    59 | loss: 7.8571370CurrentTrain: epoch 15, batch    60 | loss: 9.5944672CurrentTrain: epoch 15, batch    61 | loss: 16.5762409CurrentTrain: epoch  7, batch    62 | loss: 6.8482573CurrentTrain: epoch 15, batch     0 | loss: 10.8388807CurrentTrain: epoch 15, batch     1 | loss: 13.6181722CurrentTrain: epoch 15, batch     2 | loss: 13.5806064CurrentTrain: epoch 15, batch     3 | loss: 7.7019587CurrentTrain: epoch 15, batch     4 | loss: 8.0470810CurrentTrain: epoch 15, batch     5 | loss: 14.0386596CurrentTrain: epoch 15, batch     6 | loss: 16.7504008CurrentTrain: epoch 15, batch     7 | loss: 8.5437590CurrentTrain: epoch 15, batch     8 | loss: 15.5643348CurrentTrain: epoch 15, batch     9 | loss: 11.3708156CurrentTrain: epoch 15, batch    10 | loss: 6.8361441CurrentTrain: epoch 15, batch    11 | loss: 10.6546089CurrentTrain: epoch 15, batch    12 | loss: 12.1549403CurrentTrain: epoch 15, batch    13 | loss: 10.6640039CurrentTrain: epoch 15, batch    14 | loss: 14.0235127CurrentTrain: epoch 15, batch    15 | loss: 12.5906995CurrentTrain: epoch 15, batch    16 | loss: 9.2394985CurrentTrain: epoch 15, batch    17 | loss: 8.6496129CurrentTrain: epoch 15, batch    18 | loss: 8.7757966CurrentTrain: epoch 15, batch    19 | loss: 9.8076139CurrentTrain: epoch 15, batch    20 | loss: 13.5843880CurrentTrain: epoch 15, batch    21 | loss: 15.0321176CurrentTrain: epoch 15, batch    22 | loss: 17.9204409CurrentTrain: epoch 15, batch    23 | loss: 8.4688291CurrentTrain: epoch 15, batch    24 | loss: 16.9289758CurrentTrain: epoch 15, batch    25 | loss: 15.8097275CurrentTrain: epoch 15, batch    26 | loss: 10.2168896CurrentTrain: epoch 15, batch    27 | loss: 14.5297977CurrentTrain: epoch 15, batch    28 | loss: 11.6810655CurrentTrain: epoch 15, batch    29 | loss: 19.1275278CurrentTrain: epoch 15, batch    30 | loss: 12.6422808CurrentTrain: epoch 15, batch    31 | loss: 11.7261729CurrentTrain: epoch 15, batch    32 | loss: 9.2209925CurrentTrain: epoch 15, batch    33 | loss: 14.1732928CurrentTrain: epoch 15, batch    34 | loss: 17.0039693CurrentTrain: epoch 15, batch    35 | loss: 11.7606637CurrentTrain: epoch 15, batch    36 | loss: 9.6115358CurrentTrain: epoch 15, batch    37 | loss: 9.8605125CurrentTrain: epoch 15, batch    38 | loss: 8.0011815CurrentTrain: epoch 15, batch    39 | loss: 9.6672107CurrentTrain: epoch 15, batch    40 | loss: 13.7471686CurrentTrain: epoch 15, batch    41 | loss: 10.4115952CurrentTrain: epoch 15, batch    42 | loss: 14.0967992CurrentTrain: epoch 15, batch    43 | loss: 7.0076331CurrentTrain: epoch 15, batch    44 | loss: 10.1098632CurrentTrain: epoch 15, batch    45 | loss: 9.0457119CurrentTrain: epoch 15, batch    46 | loss: 12.4196918CurrentTrain: epoch 15, batch    47 | loss: 28.8662277CurrentTrain: epoch 15, batch    48 | loss: 7.2871154CurrentTrain: epoch 15, batch    49 | loss: 9.4495609CurrentTrain: epoch 15, batch    50 | loss: 12.3504109CurrentTrain: epoch 15, batch    51 | loss: 8.6804364CurrentTrain: epoch 15, batch    52 | loss: 7.8485173CurrentTrain: epoch 15, batch    53 | loss: 13.6407901CurrentTrain: epoch 15, batch    54 | loss: 14.2990290CurrentTrain: epoch 15, batch    55 | loss: 23.6605220CurrentTrain: epoch 15, batch    56 | loss: 20.7573617CurrentTrain: epoch 15, batch    57 | loss: 23.9545124CurrentTrain: epoch 15, batch    58 | loss: 8.6744672CurrentTrain: epoch 15, batch    59 | loss: 7.4194603CurrentTrain: epoch 15, batch    60 | loss: 17.2131048CurrentTrain: epoch 15, batch    61 | loss: 8.7580704CurrentTrain: epoch  7, batch    62 | loss: 8.6472051CurrentTrain: epoch 15, batch     0 | loss: 12.7161333CurrentTrain: epoch 15, batch     1 | loss: 9.9401134CurrentTrain: epoch 15, batch     2 | loss: 15.9058280CurrentTrain: epoch 15, batch     3 | loss: 10.4248681CurrentTrain: epoch 15, batch     4 | loss: 11.0176335CurrentTrain: epoch 15, batch     5 | loss: 10.9553671CurrentTrain: epoch 15, batch     6 | loss: 10.5970449CurrentTrain: epoch 15, batch     7 | loss: 13.9946106CurrentTrain: epoch 15, batch     8 | loss: 20.8145905CurrentTrain: epoch 15, batch     9 | loss: 10.2184491CurrentTrain: epoch 15, batch    10 | loss: 14.6871754CurrentTrain: epoch 15, batch    11 | loss: 15.1830824CurrentTrain: epoch 15, batch    12 | loss: 10.4812671CurrentTrain: epoch 15, batch    13 | loss: 10.1300302CurrentTrain: epoch 15, batch    14 | loss: 13.2709895CurrentTrain: epoch 15, batch    15 | loss: 22.2231001CurrentTrain: epoch 15, batch    16 | loss: 12.7610875CurrentTrain: epoch 15, batch    17 | loss: 10.1643222CurrentTrain: epoch 15, batch    18 | loss: 14.3588771CurrentTrain: epoch 15, batch    19 | loss: 12.7991554CurrentTrain: epoch 15, batch    20 | loss: 8.1368363CurrentTrain: epoch 15, batch    21 | loss: 16.8772192CurrentTrain: epoch 15, batch    22 | loss: 12.5036581CurrentTrain: epoch 15, batch    23 | loss: 9.0459541CurrentTrain: epoch 15, batch    24 | loss: 5.9867591CurrentTrain: epoch 15, batch    25 | loss: 13.7074626CurrentTrain: epoch 15, batch    26 | loss: 11.6559418CurrentTrain: epoch 15, batch    27 | loss: 15.6251671CurrentTrain: epoch 15, batch    28 | loss: 13.4158484CurrentTrain: epoch 15, batch    29 | loss: 14.6056926CurrentTrain: epoch 15, batch    30 | loss: 14.4583051CurrentTrain: epoch 15, batch    31 | loss: 10.4975341CurrentTrain: epoch 15, batch    32 | loss: 10.0887858CurrentTrain: epoch 15, batch    33 | loss: 22.1278891CurrentTrain: epoch 15, batch    34 | loss: 8.8503265CurrentTrain: epoch 15, batch    35 | loss: 8.6123069CurrentTrain: epoch 15, batch    36 | loss: 16.0485536CurrentTrain: epoch 15, batch    37 | loss: 11.5892176CurrentTrain: epoch 15, batch    38 | loss: 17.5992189CurrentTrain: epoch 15, batch    39 | loss: 9.0873626CurrentTrain: epoch 15, batch    40 | loss: 14.3135685CurrentTrain: epoch 15, batch    41 | loss: 14.2893135CurrentTrain: epoch 15, batch    42 | loss: 10.0179683CurrentTrain: epoch 15, batch    43 | loss: 10.2517482CurrentTrain: epoch 15, batch    44 | loss: 14.2253553CurrentTrain: epoch 15, batch    45 | loss: 11.3491283CurrentTrain: epoch 15, batch    46 | loss: 12.6729818CurrentTrain: epoch 15, batch    47 | loss: 9.8344383CurrentTrain: epoch 15, batch    48 | loss: 31.1460065CurrentTrain: epoch 15, batch    49 | loss: 19.0678304CurrentTrain: epoch 15, batch    50 | loss: 9.8590398CurrentTrain: epoch 15, batch    51 | loss: 11.8818564CurrentTrain: epoch 15, batch    52 | loss: 12.8626450CurrentTrain: epoch 15, batch    53 | loss: 9.6440775CurrentTrain: epoch 15, batch    54 | loss: 12.8495460CurrentTrain: epoch 15, batch    55 | loss: 14.6358272CurrentTrain: epoch 15, batch    56 | loss: 11.2413376CurrentTrain: epoch 15, batch    57 | loss: 8.7593240CurrentTrain: epoch 15, batch    58 | loss: 8.2691462CurrentTrain: epoch 15, batch    59 | loss: 8.8254397CurrentTrain: epoch 15, batch    60 | loss: 13.5316560CurrentTrain: epoch 15, batch    61 | loss: 7.6152605CurrentTrain: epoch  7, batch    62 | loss: 12.1045202CurrentTrain: epoch 15, batch     0 | loss: 6.6169227CurrentTrain: epoch 15, batch     1 | loss: 9.2327465CurrentTrain: epoch 15, batch     2 | loss: 10.0516890CurrentTrain: epoch 15, batch     3 | loss: 11.3858058CurrentTrain: epoch 15, batch     4 | loss: 8.6770199CurrentTrain: epoch 15, batch     5 | loss: 11.3179264CurrentTrain: epoch 15, batch     6 | loss: 9.8397702CurrentTrain: epoch 15, batch     7 | loss: 13.5470520CurrentTrain: epoch 15, batch     8 | loss: 6.5496326CurrentTrain: epoch 15, batch     9 | loss: 14.2042652CurrentTrain: epoch 15, batch    10 | loss: 12.3640334CurrentTrain: epoch 15, batch    11 | loss: 11.7533732CurrentTrain: epoch 15, batch    12 | loss: 8.2150564CurrentTrain: epoch 15, batch    13 | loss: 8.3357545CurrentTrain: epoch 15, batch    14 | loss: 8.1446827CurrentTrain: epoch 15, batch    15 | loss: 17.2738116CurrentTrain: epoch 15, batch    16 | loss: 8.2400328CurrentTrain: epoch 15, batch    17 | loss: 9.2778487CurrentTrain: epoch 15, batch    18 | loss: 9.4566677CurrentTrain: epoch 15, batch    19 | loss: 16.1915401CurrentTrain: epoch 15, batch    20 | loss: 17.6720149CurrentTrain: epoch 15, batch    21 | loss: 9.0217120CurrentTrain: epoch 15, batch    22 | loss: 30.5925895CurrentTrain: epoch 15, batch    23 | loss: 9.0081100CurrentTrain: epoch 15, batch    24 | loss: 14.5176086CurrentTrain: epoch 15, batch    25 | loss: 9.2134707CurrentTrain: epoch 15, batch    26 | loss: 8.9810205CurrentTrain: epoch 15, batch    27 | loss: 10.7651990CurrentTrain: epoch 15, batch    28 | loss: 22.6314608CurrentTrain: epoch 15, batch    29 | loss: 17.9242284CurrentTrain: epoch 15, batch    30 | loss: 10.5781268CurrentTrain: epoch 15, batch    31 | loss: 9.1797695CurrentTrain: epoch 15, batch    32 | loss: 9.4568559CurrentTrain: epoch 15, batch    33 | loss: 17.1631187CurrentTrain: epoch 15, batch    34 | loss: 11.6474846CurrentTrain: epoch 15, batch    35 | loss: 12.7164667CurrentTrain: epoch 15, batch    36 | loss: 9.2546956CurrentTrain: epoch 15, batch    37 | loss: 11.1048189CurrentTrain: epoch 15, batch    38 | loss: 16.5512726CurrentTrain: epoch 15, batch    39 | loss: 9.0442131CurrentTrain: epoch 15, batch    40 | loss: 10.1612471CurrentTrain: epoch 15, batch    41 | loss: 9.2859481CurrentTrain: epoch 15, batch    42 | loss: 11.2237496CurrentTrain: epoch 15, batch    43 | loss: 17.6674333CurrentTrain: epoch 15, batch    44 | loss: 9.8574295CurrentTrain: epoch 15, batch    45 | loss: 8.0483952CurrentTrain: epoch 15, batch    46 | loss: 13.4715612CurrentTrain: epoch 15, batch    47 | loss: 11.7443073CurrentTrain: epoch 15, batch    48 | loss: 9.6971357CurrentTrain: epoch 15, batch    49 | loss: 10.1456966CurrentTrain: epoch 15, batch    50 | loss: 12.0421421CurrentTrain: epoch 15, batch    51 | loss: 24.3972159CurrentTrain: epoch 15, batch    52 | loss: 7.0640348CurrentTrain: epoch 15, batch    53 | loss: 8.4202429CurrentTrain: epoch 15, batch    54 | loss: 10.8560835CurrentTrain: epoch 15, batch    55 | loss: 7.4116768CurrentTrain: epoch 15, batch    56 | loss: 13.9101375CurrentTrain: epoch 15, batch    57 | loss: 12.0534727CurrentTrain: epoch 15, batch    58 | loss: 10.5201461CurrentTrain: epoch 15, batch    59 | loss: 14.5579288CurrentTrain: epoch 15, batch    60 | loss: 9.7193186CurrentTrain: epoch 15, batch    61 | loss: 8.4964426CurrentTrain: epoch  7, batch    62 | loss: 6.3369837CurrentTrain: epoch 15, batch     0 | loss: 11.0887880CurrentTrain: epoch 15, batch     1 | loss: 10.7160324CurrentTrain: epoch 15, batch     2 | loss: 11.5029999CurrentTrain: epoch 15, batch     3 | loss: 11.4354192CurrentTrain: epoch 15, batch     4 | loss: 13.9772900CurrentTrain: epoch 15, batch     5 | loss: 21.1527935CurrentTrain: epoch 15, batch     6 | loss: 12.8143391CurrentTrain: epoch 15, batch     7 | loss: 8.1468302CurrentTrain: epoch 15, batch     8 | loss: 12.9348509CurrentTrain: epoch 15, batch     9 | loss: 16.6306693CurrentTrain: epoch 15, batch    10 | loss: 10.0701860CurrentTrain: epoch 15, batch    11 | loss: 6.0574110CurrentTrain: epoch 15, batch    12 | loss: 13.4619962CurrentTrain: epoch 15, batch    13 | loss: 9.1827662CurrentTrain: epoch 15, batch    14 | loss: 17.3173428CurrentTrain: epoch 15, batch    15 | loss: 11.2121248CurrentTrain: epoch 15, batch    16 | loss: 9.2454648CurrentTrain: epoch 15, batch    17 | loss: 21.1889729CurrentTrain: epoch 15, batch    18 | loss: 9.1395687CurrentTrain: epoch 15, batch    19 | loss: 9.7373186CurrentTrain: epoch 15, batch    20 | loss: 13.3159307CurrentTrain: epoch 15, batch    21 | loss: 7.8410567CurrentTrain: epoch 15, batch    22 | loss: 10.5379828CurrentTrain: epoch 15, batch    23 | loss: 9.0620501CurrentTrain: epoch 15, batch    24 | loss: 13.1862383CurrentTrain: epoch 15, batch    25 | loss: 9.3815641CurrentTrain: epoch 15, batch    26 | loss: 14.2293587CurrentTrain: epoch 15, batch    27 | loss: 13.9436359CurrentTrain: epoch 15, batch    28 | loss: 10.0496128CurrentTrain: epoch 15, batch    29 | loss: 21.1216701CurrentTrain: epoch 15, batch    30 | loss: 14.5459045CurrentTrain: epoch 15, batch    31 | loss: 10.2378409CurrentTrain: epoch 15, batch    32 | loss: 8.4431593CurrentTrain: epoch 15, batch    33 | loss: 9.5604526CurrentTrain: epoch 15, batch    34 | loss: 11.2453278CurrentTrain: epoch 15, batch    35 | loss: 6.3291614CurrentTrain: epoch 15, batch    36 | loss: 8.9964359CurrentTrain: epoch 15, batch    37 | loss: 10.4283456CurrentTrain: epoch 15, batch    38 | loss: 11.9737702CurrentTrain: epoch 15, batch    39 | loss: 15.1190135CurrentTrain: epoch 15, batch    40 | loss: 12.1001808CurrentTrain: epoch 15, batch    41 | loss: 8.4709574CurrentTrain: epoch 15, batch    42 | loss: 12.5341120CurrentTrain: epoch 15, batch    43 | loss: 9.7804951CurrentTrain: epoch 15, batch    44 | loss: 8.1927785CurrentTrain: epoch 15, batch    45 | loss: 10.8494129CurrentTrain: epoch 15, batch    46 | loss: 8.0056516CurrentTrain: epoch 15, batch    47 | loss: 9.0432406CurrentTrain: epoch 15, batch    48 | loss: 6.8873700CurrentTrain: epoch 15, batch    49 | loss: 19.5550396CurrentTrain: epoch 15, batch    50 | loss: 7.4658889CurrentTrain: epoch 15, batch    51 | loss: 15.5414406CurrentTrain: epoch 15, batch    52 | loss: 12.5837932CurrentTrain: epoch 15, batch    53 | loss: 8.7835185CurrentTrain: epoch 15, batch    54 | loss: 10.0069976CurrentTrain: epoch 15, batch    55 | loss: 13.5205242CurrentTrain: epoch 15, batch    56 | loss: 12.8792592CurrentTrain: epoch 15, batch    57 | loss: 8.7892466CurrentTrain: epoch 15, batch    58 | loss: 14.9309212CurrentTrain: epoch 15, batch    59 | loss: 13.3743236CurrentTrain: epoch 15, batch    60 | loss: 7.5735907CurrentTrain: epoch 15, batch    61 | loss: 16.0679070CurrentTrain: epoch  7, batch    62 | loss: 12.9742192
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: in rome he met a number of international neo - classical artists including german - born anton rafael mengs , scottish gavin hamilton , and austrian angelica kauffman .
Head Entity: angelica kauffman
Tail Entity: neo - classical
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: The Impressionist movement, which began in the late 19th century, was characterized by a focus on light and color, and it included notable artists such as Claude Monet and Pierre-Auguste Renoir.  
Head Entity: Claude Monet  
Tail Entity: Impressionist  

Relation: movement  
Context: The Surrealist movement emerged in the early 20th century, challenging the conventional perceptions of reality and featuring artists like Salvador Dalí and René Magritte.  
Head Entity: Salvador Dalí  
Tail Entity: Surrealist  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: port alberta is a joint venture between the edmonton economic development corporation ( eedc ) and edmonton international airport ( eia ) based in the edmonton capital region in alberta , canada .
Head Entity: edmonton capital region
Tail Entity: alberta
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: the city of san francisco is known for its iconic golden gate bridge and is part of the larger san francisco bay area in california, united states.  
Head Entity: san francisco bay area  
Tail Entity: california  

Relation: located in the administrative territorial entity  
Context: the university of toronto is one of the leading educational institutions in canada and is situated in the city of toronto, which is part of the greater toronto area in ontario.  
Head Entity: greater toronto area  
Tail Entity: ontario  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the nation as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: he also went on a road trip from san antonio to oakland for the magnum project “ postcards from america ” with jim goldberg , alec soth , mikhael subotzky and susan meiselas .
Head Entity: jim goldberg
Tail Entity: magnum
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: After years of dedication and hard work, she was finally recognized as a key member of the prestigious art collective known as the "Creative Minds."  
Head Entity: she  
Tail Entity: Creative Minds  

Relation: member of  
Context: The renowned guitarist joined the band in 2015, contributing to their unique sound and becoming an integral member of the musical group "The Soundwaves."  
Head Entity: guitarist  
Tail Entity: The Soundwaves  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: the toston bridge is a site on the national register of historic places spanning the missouri river , on an abandoned segment of old u.s. route 287 , at toston , montana .
Head Entity: toston bridge
Tail Entity: missouri river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: the golden gate bridge is an iconic suspension bridge that spans the golden gate strait, connecting san francisco to marin county in california.  
Head Entity: golden gate bridge  
Tail Entity: golden gate strait  

Relation: crosses  
Context: the channel tunnel is a rail tunnel beneath the english channel, linking folkestone in the uk with coquelles near calais in france.  
Head Entity: channel tunnel  
Tail Entity: english channel  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: it is on the panj river just downstream from the mouth of the bartang river at the point where the panj briefly turns west before resuming its northerly course .
Head Entity: panj river
Tail Entity: bartang river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The Missouri River, which is a major tributary of the Mississippi River, flows through several states before joining the larger watercourse.  
Head Entity: Missouri River  
Tail Entity: Mississippi River  

Relation: tributary  
Context: The Rio Grande serves as a natural border and has several tributaries, including the Pecos River, which contributes to its flow.  
Head Entity: Pecos River  
Tail Entity: Rio Grande  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: on 3 july 1937 , the count married rocafort - atuzarrra in a lavish wedding in havana , cuba attended by the president of cuba , federico laredo brú .
Head Entity: federico laredo brú
Tail Entity: president of cuba
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: In 2020, Kamala Harris was elected as the Vice President of the United States, making history as the first woman and first person of South Asian descent to hold this position.  
Head Entity: Kamala Harris  
Tail Entity: Vice President of the United States  

Relation: position held  
Context: Before becoming the Prime Minister, Boris Johnson served as the Mayor of London from 2008 to 2016, where he implemented several key policies.  
Head Entity: Boris Johnson  
Tail Entity: Mayor of London  
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: a headcrab is a fictional alien parasitoid found in the " half - life " video game series created by valve software .
Head Entity: half - life
Tail Entity: valve software
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: The popular game "The Witcher 3: Wild Hunt" was developed by CD Projekt Red, a well-known Polish video game developer.  
Head Entity: The Witcher 3: Wild Hunt  
Tail Entity: CD Projekt Red  

Relation: developer  
Context: The groundbreaking mobile game "Angry Birds" was created by Rovio Entertainment, a Finnish company that specializes in game development.  
Head Entity: Angry Birds  
Tail Entity: Rovio Entertainment  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: holy molar are a san diego based band formed in 2001 .
Head Entity: holy molar
Tail Entity: san diego
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the tech startup was established in silicon valley in 2015.  
Head Entity: the tech startup  
Tail Entity: silicon valley  

Relation: location of formation  
Context: the famous rock band was formed in new york city during the summer of 1999.  
Head Entity: the famous rock band  
Tail Entity: new york city  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: in the fall of 1996 , now consisting of trevor paglen , nicolas lampert , and sean thomas , noisegate embarked on their first major tour of the united states , with filmmaker laura klein .
Head Entity: noisegate
Tail Entity: united states
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous dish known as paella originated in the region of Valencia, where it has been a staple for centuries, showcasing the rich culinary traditions of Spain.  
Head Entity: paella  
Tail Entity: Spain  

Relation: country of origin  
Context: The iconic brand of chocolate, Toblerone, is known for its unique triangular shape and was first created in Switzerland, where it remains a beloved treat.  
Head Entity: Toblerone  
Tail Entity: Switzerland  
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.01%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.42%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 93.47%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.01%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.99%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.98%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 94.18%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.56%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.73%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.04%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 94.97%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.10%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.07%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.19%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.43%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.64%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.74%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.88%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.96%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.79%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.75%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.57%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.54%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.61%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.58%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 95.73%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.80%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.77%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.04%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.01%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.42%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 93.47%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.01%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.99%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.98%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 94.18%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.56%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.73%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.04%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 94.97%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.10%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.07%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.19%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.43%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.64%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.74%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.88%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.96%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.79%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.75%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.57%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.54%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.61%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.58%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 95.73%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.80%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.77%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.04%   
cur_acc:  ['0.9504']
his_acc:  ['0.9504']
CurrentTrain: epoch 15, batch     0 | loss: 23.9780613CurrentTrain: epoch 15, batch     1 | loss: 19.7514695CurrentTrain: epoch 15, batch     2 | loss: 16.7635382CurrentTrain: epoch  1, batch     3 | loss: 13.1770723CurrentTrain: epoch 15, batch     0 | loss: 14.8179033CurrentTrain: epoch 15, batch     1 | loss: 12.0898013CurrentTrain: epoch 15, batch     2 | loss: 10.8552113CurrentTrain: epoch  1, batch     3 | loss: 11.2381709CurrentTrain: epoch 15, batch     0 | loss: 14.0215782CurrentTrain: epoch 15, batch     1 | loss: 15.6977698CurrentTrain: epoch 15, batch     2 | loss: 12.3715245CurrentTrain: epoch  1, batch     3 | loss: 13.2682079CurrentTrain: epoch 15, batch     0 | loss: 14.0834223CurrentTrain: epoch 15, batch     1 | loss: 11.0790074CurrentTrain: epoch 15, batch     2 | loss: 10.0287200CurrentTrain: epoch  1, batch     3 | loss: 10.0169994CurrentTrain: epoch 15, batch     0 | loss: 14.6648460CurrentTrain: epoch 15, batch     1 | loss: 7.1495985CurrentTrain: epoch 15, batch     2 | loss: 12.6820750CurrentTrain: epoch  1, batch     3 | loss: 12.8477339CurrentTrain: epoch 15, batch     0 | loss: 15.3017389CurrentTrain: epoch 15, batch     1 | loss: 12.7336994CurrentTrain: epoch 15, batch     2 | loss: 18.0993128CurrentTrain: epoch  1, batch     3 | loss: 9.4507900CurrentTrain: epoch 15, batch     0 | loss: 8.9034827CurrentTrain: epoch 15, batch     1 | loss: 8.9267691CurrentTrain: epoch 15, batch     2 | loss: 10.4194940CurrentTrain: epoch  1, batch     3 | loss: 8.1737186CurrentTrain: epoch 15, batch     0 | loss: 8.6086819CurrentTrain: epoch 15, batch     1 | loss: 9.0913095CurrentTrain: epoch 15, batch     2 | loss: 18.4610801CurrentTrain: epoch  1, batch     3 | loss: 8.1554290CurrentTrain: epoch 15, batch     0 | loss: 8.1628085CurrentTrain: epoch 15, batch     1 | loss: 11.6278717CurrentTrain: epoch 15, batch     2 | loss: 8.2890796CurrentTrain: epoch  1, batch     3 | loss: 11.1266567CurrentTrain: epoch 15, batch     0 | loss: 6.6516414CurrentTrain: epoch 15, batch     1 | loss: 8.5747825CurrentTrain: epoch 15, batch     2 | loss: 16.7505148CurrentTrain: epoch  1, batch     3 | loss: 8.0409808
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: there was concern in the gun lobby over restrictions on second amendment rights in the united states .. ( march 7 , 2000 ) cnn .
Head Entity: second amendment
Tail Entity: united states
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The new environmental regulations introduced by the European Union aim to reduce carbon emissions across member states.  
Head Entity: environmental regulations  
Tail Entity: European Union  

Relation: applies to jurisdiction  
Context: The recent changes in tax laws have raised questions about their applicability in various states across the country.  
Head Entity: tax laws  
Tail Entity: states
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: " tenth avenue freeze - out " is the second song on bruce springsteen 's breakthrough album " born to run " , released in 1975 .
Head Entity: tenth avenue freeze - out
Tail Entity: bruce springsteen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: "Shape of You" is a popular song by Ed Sheeran that topped charts worldwide after its release in 2017.  
Head Entity: Shape of You  
Tail Entity: Ed Sheeran  

Relation: performer  
Context: The Beatles are widely regarded as one of the most influential bands in the history of music, with hits like "Hey Jude" and "Let It Be."  
Head Entity: The Beatles  
Tail Entity: Hey Jude  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the glasflügel h-201 standard " libelle " ( german : " dragonfly " ) is an early composite standard class single - seat sailplane produced by glasflügel from 1967 .
Head Entity: dragonfly
Tail Entity: glasflügel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The iPhone 13, a smartphone developed by Apple, was released in September 2021 and features advanced camera technology and improved battery life.  
Head Entity: iPhone 13  
Tail Entity: Apple  

Relation: manufacturer  
Context: The Tesla Model S is an all-electric luxury sedan that was first introduced by Tesla, Inc. in 2012, showcasing innovative technology and performance.  
Head Entity: Tesla Model S  
Tail Entity: Tesla, Inc.  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: norway was represented in the eurovision song contest 2005 by the song " in my dreams " performed by wig wam .
Head Entity: eurovision song contest 2005
Tail Entity: wig wam
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: the united states was represented in the 2020 summer olympics by a team of over 600 athletes competing in various sports.  
Head Entity: 2020 summer olympics  
Tail Entity: united states  

Relation: participant  
Context: the annual tech conference featured several startups, including a presentation by innovative solutions, which showcased their latest product.  
Head Entity: annual tech conference  
Tail Entity: innovative solutions  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the application is compatible with various operating systems, including macOS, Windows, and Ubuntu.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: Many developers prefer to use Linux as their operating system due to its flexibility and open-source nature.  
Head Entity: developers  
Tail Entity: Linux  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in july 2007 , griffin lost by a technical knockout to light heavyweight champion danny green of the world boxing association at challenge stadium in perth , australia .
Head Entity: danny green
Tail Entity: light heavyweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: during the 2020 olympics, the men's 100 meters final showcased athletes from various competition classes, with usain bolt representing the sprinting class.  
Head Entity: usain bolt  
Tail Entity: sprinting  

Relation: competition class  
Context: the formula one race in monaco is known for its high-speed competition class, where lewis hamilton competed fiercely against other top drivers.  
Head Entity: lewis hamilton  
Tail Entity: formula one
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: the qup gained 3 seats from its previous election result , and won a 10 % swing to it in primary votes , but failed to defeat the incumbent labor government led by ted theodore .
Head Entity: previous election
Tail Entity: ted theodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: After a closely contested race, the voters chose Sarah Johnson as their representative, marking a significant shift in the political landscape of the district.  
Head Entity: closely contested race  
Tail Entity: Sarah Johnson  

Relation: successful candidate  
Context: In the recent mayoral election, the community rallied behind Mark Thompson, who promised to bring new initiatives to the city.  
Head Entity: recent mayoral election  
Tail Entity: Mark Thompson  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: jenő szervánszky is the father of the pianist , valéria szervánszky and the brother of the composer , endre szervánszky and violinist , péter szervánszky .
Head Entity: péter szervánszky
Tail Entity: endre szervánszky
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: emma and oliver are both children of the same parents, making them siblings. they often play together and share a close bond.  
Head Entity: emma  
Tail Entity: oliver  

Relation: sibling  
Context: during the family reunion, it was clear that both lucas and mia inherited their parents' artistic talents, as they are siblings who excel in painting and music.  
Head Entity: lucas  
Tail Entity: mia  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: two of her sons , tunis and thomas tingey rose to prominence in the union navy during the civil war .
Head Entity: tunis
Tail Entity: union navy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: after serving in the army, he transitioned to a leadership role in the air force, where he implemented several key strategies.  
Head Entity: air force  
Tail Entity: army  

Relation: military branch  
Context: the general was honored for his service in the marine corps, where he led numerous successful missions.  
Head Entity: marine corps  
Tail Entity: general  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: ziad rahbani is the son of the lebanese famous composer assi rahbani and nouhad haddad , the famous lebanese female singer known as fairuz .
Head Entity: assi rahbani
Tail Entity: ziad rahbani
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: in the animated series, the character of simon is the son of the adventurous couple, marie and tom.  
Head Entity: marie  
Tail Entity: simon  

Relation: child  
Context: the renowned physicist albert einstein had a daughter named lieserl, who was born in 1902.  
Head Entity: albert einstein  
Tail Entity: lieserl  
MemoryTrain:  epoch 15, batch     0 | loss: 6.2102987MemoryTrain:  epoch 15, batch     1 | loss: 6.2526441MemoryTrain:  epoch 15, batch     2 | loss: 6.3777781MemoryTrain:  epoch 11, batch     3 | loss: 7.1492768MemoryTrain:  epoch 15, batch     0 | loss: 6.6850807MemoryTrain:  epoch 15, batch     1 | loss: 9.7984111MemoryTrain:  epoch 15, batch     2 | loss: 7.2051538MemoryTrain:  epoch 11, batch     3 | loss: 6.0512763MemoryTrain:  epoch 15, batch     0 | loss: 6.7453508MemoryTrain:  epoch 15, batch     1 | loss: 6.5437204MemoryTrain:  epoch 15, batch     2 | loss: 4.0243001MemoryTrain:  epoch 11, batch     3 | loss: 6.2924475MemoryTrain:  epoch 15, batch     0 | loss: 4.8713015MemoryTrain:  epoch 15, batch     1 | loss: 5.8087474MemoryTrain:  epoch 15, batch     2 | loss: 8.7132547MemoryTrain:  epoch 11, batch     3 | loss: 5.3678018MemoryTrain:  epoch 15, batch     0 | loss: 4.0331651MemoryTrain:  epoch 15, batch     1 | loss: 4.0906962MemoryTrain:  epoch 15, batch     2 | loss: 6.3898714MemoryTrain:  epoch 11, batch     3 | loss: 6.8143881MemoryTrain:  epoch 15, batch     0 | loss: 5.1815414MemoryTrain:  epoch 15, batch     1 | loss: 4.9360033MemoryTrain:  epoch 15, batch     2 | loss: 7.4008844MemoryTrain:  epoch 11, batch     3 | loss: 4.4842372MemoryTrain:  epoch 15, batch     0 | loss: 3.5439221MemoryTrain:  epoch 15, batch     1 | loss: 3.8178798MemoryTrain:  epoch 15, batch     2 | loss: 5.3341993MemoryTrain:  epoch 11, batch     3 | loss: 6.4990833MemoryTrain:  epoch 15, batch     0 | loss: 7.2337097MemoryTrain:  epoch 15, batch     1 | loss: 4.7489301MemoryTrain:  epoch 15, batch     2 | loss: 3.0715793MemoryTrain:  epoch 11, batch     3 | loss: 4.2342987MemoryTrain:  epoch 15, batch     0 | loss: 3.7405447MemoryTrain:  epoch 15, batch     1 | loss: 2.7238224MemoryTrain:  epoch 15, batch     2 | loss: 3.9899051MemoryTrain:  epoch 11, batch     3 | loss: 2.0987604MemoryTrain:  epoch 15, batch     0 | loss: 2.8388417MemoryTrain:  epoch 15, batch     1 | loss: 2.3494693MemoryTrain:  epoch 15, batch     2 | loss: 5.3298634MemoryTrain:  epoch 11, batch     3 | loss: 3.1870776
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 16.67%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 15.62%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 17.50%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 19.79%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 25.89%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 34.38%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 39.58%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 44.38%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 48.86%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 51.56%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 55.29%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 58.04%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 60.00%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 61.72%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 63.24%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 65.28%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 67.11%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 68.44%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 69.05%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 68.48%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 69.01%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 69.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 70.91%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 71.99%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 72.99%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 73.92%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 74.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 75.60%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 76.37%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 77.76%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 78.39%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 78.99%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 79.56%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 80.10%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 80.45%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 80.62%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 80.49%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 80.80%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 81.10%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 80.14%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 78.80%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 78.06%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 77.73%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 76.91%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 76.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 76.47%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 76.80%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 77.12%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 77.31%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 77.61%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 77.90%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 78.07%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 78.23%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 78.18%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 78.23%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 78.59%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 78.63%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 78.08%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.45%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 89.71%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.93%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 89.80%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 90.31%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 90.48%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 90.06%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 89.67%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.38%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.74%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 91.16%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.73%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.99%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.23%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 92.46%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.91%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 93.09%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.27%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.44%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.60%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 93.60%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 93.60%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 93.47%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 93.47%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 93.48%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 93.09%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 92.84%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.98%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 92.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.89%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 92.79%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 92.81%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.94%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 92.73%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.75%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 92.76%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 92.67%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 92.80%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 92.81%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 92.93%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 92.94%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 92.36%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 91.11%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 90.10%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 88.92%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 87.97%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 87.04%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 86.23%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 86.44%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 86.20%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 86.30%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 86.40%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 86.42%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 86.60%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 86.61%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 86.62%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 86.63%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 86.88%   [EVAL] batch:   81 | acc: 93.75%,  total acc: 86.97%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 86.97%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 86.76%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 86.47%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 86.41%   [EVAL] batch:   86 | acc: 81.25%,  total acc: 86.35%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 86.43%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 86.59%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 86.74%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 86.88%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 87.02%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 87.16%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 87.43%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 87.57%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 87.69%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 87.82%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 87.94%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 88.06%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 88.17%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 88.11%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 88.04%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 88.15%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 88.21%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 87.85%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 87.33%   [EVAL] batch:  108 | acc: 43.75%,  total acc: 86.93%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 86.42%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 86.15%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 85.66%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 85.56%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 85.58%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 85.72%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 85.79%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 85.86%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 85.92%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 85.89%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 85.90%   [EVAL] batch:  121 | acc: 75.00%,  total acc: 85.81%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 85.82%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 85.84%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 85.85%   
cur_acc:  ['0.9504', '0.7808']
his_acc:  ['0.9504', '0.8585']
CurrentTrain: epoch 15, batch     0 | loss: 19.1863934CurrentTrain: epoch 15, batch     1 | loss: 11.6155144CurrentTrain: epoch 15, batch     2 | loss: 11.8556334CurrentTrain: epoch  1, batch     3 | loss: 12.0131528CurrentTrain: epoch 15, batch     0 | loss: 13.1070547CurrentTrain: epoch 15, batch     1 | loss: 16.2229069CurrentTrain: epoch 15, batch     2 | loss: 11.1527910CurrentTrain: epoch  1, batch     3 | loss: 7.1805702CurrentTrain: epoch 15, batch     0 | loss: 11.5898583CurrentTrain: epoch 15, batch     1 | loss: 10.2197537CurrentTrain: epoch 15, batch     2 | loss: 15.3081375CurrentTrain: epoch  1, batch     3 | loss: 7.5491603CurrentTrain: epoch 15, batch     0 | loss: 18.0518995CurrentTrain: epoch 15, batch     1 | loss: 13.9222605CurrentTrain: epoch 15, batch     2 | loss: 10.8388539CurrentTrain: epoch  1, batch     3 | loss: 8.7411019CurrentTrain: epoch 15, batch     0 | loss: 10.8100304CurrentTrain: epoch 15, batch     1 | loss: 8.8472071CurrentTrain: epoch 15, batch     2 | loss: 10.5781954CurrentTrain: epoch  1, batch     3 | loss: 7.5408803CurrentTrain: epoch 15, batch     0 | loss: 14.6262475CurrentTrain: epoch 15, batch     1 | loss: 7.1778117CurrentTrain: epoch 15, batch     2 | loss: 10.7291339CurrentTrain: epoch  1, batch     3 | loss: 11.9589335CurrentTrain: epoch 15, batch     0 | loss: 9.5940387CurrentTrain: epoch 15, batch     1 | loss: 11.1278720CurrentTrain: epoch 15, batch     2 | loss: 10.5129119CurrentTrain: epoch  1, batch     3 | loss: 6.2960145CurrentTrain: epoch 15, batch     0 | loss: 8.1610219CurrentTrain: epoch 15, batch     1 | loss: 9.5846290CurrentTrain: epoch 15, batch     2 | loss: 8.8032029CurrentTrain: epoch  1, batch     3 | loss: 7.6049197CurrentTrain: epoch 15, batch     0 | loss: 9.3974700CurrentTrain: epoch 15, batch     1 | loss: 15.2367603CurrentTrain: epoch 15, batch     2 | loss: 10.2032348CurrentTrain: epoch  1, batch     3 | loss: 6.1957732CurrentTrain: epoch 15, batch     0 | loss: 15.9559219CurrentTrain: epoch 15, batch     1 | loss: 9.1918920CurrentTrain: epoch 15, batch     2 | loss: 14.4784778CurrentTrain: epoch  1, batch     3 | loss: 8.3372503
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: auerbach is prominently featured in the documentary film , " the first basket " , about jewish basketball history .
Head Entity: the first basket
Tail Entity: basketball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: the book "sapiens: a brief history of humankind" explores the evolution of human societies and cultures.  
Head Entity: sapiens: a brief history of humankind  
Tail Entity: human societies  

Relation: main subject  
Context: the documentary "our planet" showcases the beauty of nature and the impact of climate change on wildlife.  
Head Entity: our planet  
Tail Entity: nature  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: he represented romania at euro 2000 , his first major tournament where they beat england 3–2 , and drew 1–1 against germany .
Head Entity: euro 2000
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: The Brazilian national team showcased their skills at the 2014 FIFA World Cup, where they faced off against Germany in the semi-finals.  
Head Entity: 2014 FIFA World Cup  
Tail Entity: Germany  

Relation: participating team  
Context: During the 2021 Tokyo Olympics, the United States women's soccer team competed fiercely, ultimately playing against Canada in the semi-finals.  
Head Entity: 2021 Tokyo Olympics  
Tail Entity: Canada  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: among the competing artists was previous eurovision song contest entrant robin juhkental who represented estonia in 2010 as part of the group malcolm lincoln .
Head Entity: malcolm lincoln
Tail Entity: robin juhkental
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The famous painting "The Last Supper" features several apostles, including Judas Iscariot, who is depicted as part of the group gathered around Jesus.  
Head Entity: The Last Supper  
Tail Entity: Judas Iscariot  

Relation: has part  
Context: The smartphone model Galaxy S21 includes various components, such as the Exynos 2100 processor, which is an integral part of its performance.  
Head Entity: Galaxy S21  
Tail Entity: Exynos 2100 processor  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: he was nominated for the academy award for best picture , along with steven spielberg , ian bryce , mark gordon for the film " saving private ryan " .
Head Entity: ian bryce
Tail Entity: academy award for best picture
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film "The Shape of Water" received multiple nominations, including a nod for best director, which was awarded to Guillermo del Toro.  
Head Entity: Guillermo del Toro  
Tail Entity: best director  

Relation: nominated for  
Context: In 2020, the popular series "Succession" was nominated for several Emmy Awards, showcasing its critical acclaim and audience popularity.  
Head Entity: Succession  
Tail Entity: Emmy Awards  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: in 2007 , the county municipality created the transit authority called " skyss " that would market public transport while is would be operated by private companies based on public service obligation contracts .
Head Entity: skyss
Tail Entity: transit authority
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone known as the iPhone was first released by Apple Inc. in 2007, revolutionizing the mobile phone industry and setting new standards for design and functionality.  
Head Entity: iPhone  
Tail Entity: smartphone  

Relation: instance of  
Context: The Great Wall of China is a series of fortifications made of various materials, built to protect the Chinese states and empires from invasions and raids.  
Head Entity: Great Wall of China  
Tail Entity: fortification  
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: abus valley ( ) is an ice - free valley southeast of turnstile ridge at the north end of britannia range .
Head Entity: turnstile ridge
Tail Entity: britannia range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the sierra nevada ( ) is a mountain range in the western united states, primarily in the state of california, and it includes the famous yosemite national park, which is located within the range.  
Head Entity: sierra nevada  
Tail Entity: yosemite national park  

Relation: mountain range  
Context: the appalachian mountains ( ) extend from the canadian province of quebec down to alabama, making it one of the longest mountain ranges in north america.  
Head Entity: appalachian mountains  
Tail Entity: alabama
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: later that year he had a minor hit film with " landru " , written by françoise sagan and starring charles denner , michèle morgan , danielle darrieux and hildegard knef .
Head Entity: " landru "
Tail Entity: françoise sagan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: The acclaimed film "Inception," directed by Christopher Nolan, features a complex narrative that was intricately crafted by the talented screenwriter, Jonathan Nolan.  
Head Entity: "Inception"  
Tail Entity: Jonathan Nolan  

Relation: screenwriter  
Context: The biographical drama "The Theory of Everything" showcases the life of Stephen Hawking, with a screenplay penned by the brilliant screenwriter, Anthony McCarten.  
Head Entity: "The Theory of Everything"  
Tail Entity: Anthony McCarten  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the publisher found success in 1993 with tarō gomi 's " everyone poops " , a japanese language import that sold over one million copies and is now their best - known title .
Head Entity: everyone poops
Tail Entity: japanese language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The animated series "Avatar: The Last Airbender" features a rich narrative and is primarily produced in English, appealing to a global audience.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  

Relation: language of work or name  
Context: The famous novel "Cien años de soledad" by Gabriel García Márquez is a cornerstone of Latin American literature and is originally written in Spanish.  
Head Entity: Cien años de soledad  
Tail Entity: Spanish  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: saratov airlines ( saratov airlines joint stock company , , " saratovskije avialinii " ) is a russian airline headquartered in saratov and based at saratov tsentralny airport .
Head Entity: saratov tsentralny airport
Tail Entity: saratov airlines
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: The New York City Transit Authority operates the subway system, providing essential transportation services to millions of commuters daily.  
Head Entity: New York City Transit Authority  
Tail Entity: subway system  

Relation: operator  
Context: Tesla, Inc. is known for operating its Gigafactory in Nevada, where it produces batteries and electric vehicles.  
Head Entity: Gigafactory  
Tail Entity: Tesla, Inc.  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: the most important lama of this series was the third changkya , rolpai dorje , who was preceptor to the qianlong emperor of china , and chief representative of tibetan buddhism at the qing court .
Head Entity: rolpai dorje
Tail Entity: tibetan buddhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the cathedral of notre-dame is a famous example of french gothic architecture and serves as the seat of the archbishop of paris, representing the roman catholic faith in the heart of the city.  
Head Entity: cathedral of notre-dame  
Tail Entity: roman catholic faith  

Relation: religion  
Context: the dalai lama is a prominent figure in the world, known for his teachings on compassion and non-violence, and is the spiritual leader of tibetan buddhism.  
Head Entity: dalai lama  
Tail Entity: tibetan buddhism  
MemoryTrain:  epoch 15, batch     0 | loss: 3.8451891MemoryTrain:  epoch 15, batch     1 | loss: 7.4556312MemoryTrain:  epoch 15, batch     2 | loss: 8.4642439MemoryTrain:  epoch 15, batch     3 | loss: 3.4937077MemoryTrain:  epoch 15, batch     4 | loss: 5.0991545MemoryTrain:  epoch  9, batch     5 | loss: 3.0366152MemoryTrain:  epoch 15, batch     0 | loss: 6.0815966MemoryTrain:  epoch 15, batch     1 | loss: 3.4035698MemoryTrain:  epoch 15, batch     2 | loss: 4.3773867MemoryTrain:  epoch 15, batch     3 | loss: 5.9831896MemoryTrain:  epoch 15, batch     4 | loss: 5.1313657MemoryTrain:  epoch  9, batch     5 | loss: 3.4382834MemoryTrain:  epoch 15, batch     0 | loss: 4.8186323MemoryTrain:  epoch 15, batch     1 | loss: 3.7061827MemoryTrain:  epoch 15, batch     2 | loss: 2.8200233MemoryTrain:  epoch 15, batch     3 | loss: 4.8137634MemoryTrain:  epoch 15, batch     4 | loss: 3.5937301MemoryTrain:  epoch  9, batch     5 | loss: 2.8307091MemoryTrain:  epoch 15, batch     0 | loss: 3.3576636MemoryTrain:  epoch 15, batch     1 | loss: 3.0518348MemoryTrain:  epoch 15, batch     2 | loss: 3.1541618MemoryTrain:  epoch 15, batch     3 | loss: 3.3398046MemoryTrain:  epoch 15, batch     4 | loss: 4.6453026MemoryTrain:  epoch  9, batch     5 | loss: 4.7837646MemoryTrain:  epoch 15, batch     0 | loss: 2.9158748MemoryTrain:  epoch 15, batch     1 | loss: 3.4407451MemoryTrain:  epoch 15, batch     2 | loss: 5.1658992MemoryTrain:  epoch 15, batch     3 | loss: 9.4470452MemoryTrain:  epoch 15, batch     4 | loss: 2.3122357MemoryTrain:  epoch  9, batch     5 | loss: 5.9043437MemoryTrain:  epoch 15, batch     0 | loss: 5.4349744MemoryTrain:  epoch 15, batch     1 | loss: 3.2428132MemoryTrain:  epoch 15, batch     2 | loss: 3.0706777MemoryTrain:  epoch 15, batch     3 | loss: 2.0623476MemoryTrain:  epoch 15, batch     4 | loss: 6.7995590MemoryTrain:  epoch  9, batch     5 | loss: 2.3208964MemoryTrain:  epoch 15, batch     0 | loss: 2.4434460MemoryTrain:  epoch 15, batch     1 | loss: 4.0331067MemoryTrain:  epoch 15, batch     2 | loss: 1.9501278MemoryTrain:  epoch 15, batch     3 | loss: 4.6911499MemoryTrain:  epoch 15, batch     4 | loss: 3.0843929MemoryTrain:  epoch  9, batch     5 | loss: 2.7299657MemoryTrain:  epoch 15, batch     0 | loss: 2.5065558MemoryTrain:  epoch 15, batch     1 | loss: 2.9968876MemoryTrain:  epoch 15, batch     2 | loss: 4.5590556MemoryTrain:  epoch 15, batch     3 | loss: 4.5410113MemoryTrain:  epoch 15, batch     4 | loss: 7.2875844MemoryTrain:  epoch  9, batch     5 | loss: 1.7648887MemoryTrain:  epoch 15, batch     0 | loss: 2.3865663MemoryTrain:  epoch 15, batch     1 | loss: 2.2804008MemoryTrain:  epoch 15, batch     2 | loss: 1.9699474MemoryTrain:  epoch 15, batch     3 | loss: 2.0068289MemoryTrain:  epoch 15, batch     4 | loss: 4.1058722MemoryTrain:  epoch  9, batch     5 | loss: 1.9041978MemoryTrain:  epoch 15, batch     0 | loss: 2.0815113MemoryTrain:  epoch 15, batch     1 | loss: 2.3392524MemoryTrain:  epoch 15, batch     2 | loss: 1.9078884MemoryTrain:  epoch 15, batch     3 | loss: 3.9588195MemoryTrain:  epoch 15, batch     4 | loss: 1.8995728MemoryTrain:  epoch  9, batch     5 | loss: 3.4317932
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 82.95%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 83.17%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 79.02%   [EVAL] batch:   14 | acc: 25.00%,  total acc: 75.42%   [EVAL] batch:   15 | acc: 12.50%,  total acc: 71.48%   [EVAL] batch:   16 | acc: 37.50%,  total acc: 69.49%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 68.06%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 66.45%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 68.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 69.64%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 70.74%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 72.01%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 73.18%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 74.25%   [EVAL] batch:   25 | acc: 18.75%,  total acc: 72.12%   [EVAL] batch:   26 | acc: 18.75%,  total acc: 70.14%   [EVAL] batch:   27 | acc: 25.00%,  total acc: 68.53%   [EVAL] batch:   28 | acc: 37.50%,  total acc: 67.46%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 65.62%   [EVAL] batch:   30 | acc: 18.75%,  total acc: 64.11%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 64.65%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 65.53%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 66.54%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 67.50%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 68.23%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 69.09%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 69.90%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 70.67%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 71.25%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 71.95%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 72.62%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 73.26%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 73.86%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 74.44%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 74.86%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 75.27%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 75.39%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 75.89%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 76.25%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 75.49%   [EVAL] batch:   51 | acc: 37.50%,  total acc: 74.76%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 74.65%   [EVAL] batch:   53 | acc: 37.50%,  total acc: 73.96%   [EVAL] batch:   54 | acc: 43.75%,  total acc: 73.41%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 73.21%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 73.46%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 73.71%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 74.05%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 74.06%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 74.39%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 74.50%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 73.91%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 79.86%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 80.68%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 80.73%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 82.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 84.93%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 85.53%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 86.01%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 85.80%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 85.33%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 85.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.30%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.28%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.31%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.67%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 89.02%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 89.34%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 89.46%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 89.76%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 90.03%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 90.30%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.54%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.01%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.22%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 91.28%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 91.34%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 91.03%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 90.16%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 89.84%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.05%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 90.07%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 90.02%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 90.09%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 90.11%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 90.24%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 89.87%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 89.72%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 89.79%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 89.86%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 89.72%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 89.09%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 87.79%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 86.73%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 85.61%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 84.61%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 83.73%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 83.06%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 82.95%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 83.01%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 82.73%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 82.79%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 82.94%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 83.00%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 83.14%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 83.28%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 83.31%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 83.44%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 83.64%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 83.54%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 83.06%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 82.37%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 81.99%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 81.40%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 81.03%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 80.97%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 81.18%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 81.39%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 81.52%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 81.73%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 81.92%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 82.05%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 82.24%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 82.42%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 82.60%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 82.78%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 82.95%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 83.12%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 83.23%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 83.31%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 83.29%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 83.45%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 83.55%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 83.29%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 82.75%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 82.34%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 81.88%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 81.70%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 81.31%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 81.30%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 81.47%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 81.52%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 81.62%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 81.73%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 81.83%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 81.88%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 81.97%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 82.02%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 82.06%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 82.16%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 82.30%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 82.24%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 81.94%   [EVAL] batch:  127 | acc: 75.00%,  total acc: 81.88%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 81.88%   [EVAL] batch:  129 | acc: 87.50%,  total acc: 81.92%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 81.92%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 82.01%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 82.23%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 82.27%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 82.35%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 82.48%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 82.38%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 81.97%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 81.56%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 81.07%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 80.77%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 80.51%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 80.21%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 80.34%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 80.48%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 80.57%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 80.70%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 80.83%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 80.96%   [EVAL] batch:  150 | acc: 18.75%,  total acc: 80.55%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 80.14%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 79.78%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 79.50%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 79.07%   [EVAL] batch:  155 | acc: 18.75%,  total acc: 78.69%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 78.70%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 78.80%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 78.93%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 79.06%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 79.15%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 79.28%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 79.41%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 79.54%   [EVAL] batch:  164 | acc: 93.75%,  total acc: 79.62%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 79.74%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 79.87%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 79.99%   [EVAL] batch:  168 | acc: 100.00%,  total acc: 80.10%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 80.22%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 80.30%   [EVAL] batch:  171 | acc: 93.75%,  total acc: 80.38%   [EVAL] batch:  172 | acc: 81.25%,  total acc: 80.38%   [EVAL] batch:  173 | acc: 100.00%,  total acc: 80.50%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 80.57%   [EVAL] batch:  175 | acc: 37.50%,  total acc: 80.33%   [EVAL] batch:  176 | acc: 37.50%,  total acc: 80.08%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 80.02%   [EVAL] batch:  178 | acc: 37.50%,  total acc: 79.78%   [EVAL] batch:  179 | acc: 43.75%,  total acc: 79.58%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 79.49%   [EVAL] batch:  181 | acc: 87.50%,  total acc: 79.53%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 79.58%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 79.65%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 79.63%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 79.70%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 79.71%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 79.49%   
cur_acc:  ['0.9504', '0.7808', '0.7391']
his_acc:  ['0.9504', '0.8585', '0.7949']
CurrentTrain: epoch 15, batch     0 | loss: 12.7836638CurrentTrain: epoch 15, batch     1 | loss: 11.5202555CurrentTrain: epoch 15, batch     2 | loss: 13.5632786CurrentTrain: epoch  1, batch     3 | loss: 10.2989622CurrentTrain: epoch 15, batch     0 | loss: 10.3158801CurrentTrain: epoch 15, batch     1 | loss: 10.1078810CurrentTrain: epoch 15, batch     2 | loss: 13.3732502CurrentTrain: epoch  1, batch     3 | loss: 9.1439038CurrentTrain: epoch 15, batch     0 | loss: 18.5372499CurrentTrain: epoch 15, batch     1 | loss: 9.7480972CurrentTrain: epoch 15, batch     2 | loss: 8.7870018CurrentTrain: epoch  1, batch     3 | loss: 13.3069017CurrentTrain: epoch 15, batch     0 | loss: 11.5647488CurrentTrain: epoch 15, batch     1 | loss: 12.0220876CurrentTrain: epoch 15, batch     2 | loss: 8.8698197CurrentTrain: epoch  1, batch     3 | loss: 9.4545377CurrentTrain: epoch 15, batch     0 | loss: 13.3657061CurrentTrain: epoch 15, batch     1 | loss: 8.3668205CurrentTrain: epoch 15, batch     2 | loss: 10.0721648CurrentTrain: epoch  1, batch     3 | loss: 7.9304243CurrentTrain: epoch 15, batch     0 | loss: 8.8370508CurrentTrain: epoch 15, batch     1 | loss: 12.5044252CurrentTrain: epoch 15, batch     2 | loss: 9.3960328CurrentTrain: epoch  1, batch     3 | loss: 6.5974903CurrentTrain: epoch 15, batch     0 | loss: 12.7130742CurrentTrain: epoch 15, batch     1 | loss: 12.4638566CurrentTrain: epoch 15, batch     2 | loss: 13.0136377CurrentTrain: epoch  1, batch     3 | loss: 6.6606374CurrentTrain: epoch 15, batch     0 | loss: 5.4787962CurrentTrain: epoch 15, batch     1 | loss: 5.7462578CurrentTrain: epoch 15, batch     2 | loss: 10.8662580CurrentTrain: epoch  1, batch     3 | loss: 8.3525886CurrentTrain: epoch 15, batch     0 | loss: 11.8380109CurrentTrain: epoch 15, batch     1 | loss: 7.8674170CurrentTrain: epoch 15, batch     2 | loss: 11.1024500CurrentTrain: epoch  1, batch     3 | loss: 5.3083066CurrentTrain: epoch 15, batch     0 | loss: 8.5556828CurrentTrain: epoch 15, batch     1 | loss: 9.3872380CurrentTrain: epoch 15, batch     2 | loss: 9.0101233CurrentTrain: epoch  1, batch     3 | loss: 5.8858899
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: ( 1988 ) , " night watch " ( 2004 ) , " day watch " ( 2006 ) and " legend № 17 " ( 2013 ) .
Head Entity: day watch
Tail Entity: night watch
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: ( 1995 ) , " The Phantom Menace " ( 1999 ) , " Attack of the Clones " ( 2002 ) and " Revenge of the Sith " ( 2005 ) .  
Head Entity: Attack of the Clones  
Tail Entity: The Phantom Menace  

Relation: follows  
Context: ( 2010 ) , " The Hunger Games " ( 2012 ) , " Catching Fire " ( 2013 ) and " Mockingjay " ( 2014 ) .  
Head Entity: Catching Fire  
Tail Entity: The Hunger Games  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: dick packer was a u.s. soccer center forward who was a member of the u.s. team at the 1956 summer olympics .
Head Entity: dick packer
Tail Entity: soccer
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: serena williams is a professional tennis player who has won numerous grand slam titles throughout her career.  
Head Entity: serena williams  
Tail Entity: tennis  

Relation: sport  
Context: lebron james is known for his exceptional skills in basketball and has played for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: basketball  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael was the father of lucas, who had recently graduated from university.  
Head Entity: lucas  
Tail Entity: michael  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: since 2009 , culshaw has starred in the bbc one comedy sketch show " the impressions show " alongside debra stephenson .
Head Entity: the impressions show
Tail Entity: bbc one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: The popular series "Breaking Bad" first aired on AMC, captivating audiences with its intense storytelling and character development.  
Head Entity: Breaking Bad  
Tail Entity: AMC  

Relation: original network  
Context: "Friends" became a cultural phenomenon after it premiered on NBC, showcasing the lives of six friends living in New York City.  
Head Entity: Friends  
Tail Entity: NBC  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: the 1954 film , directed by kadri venkata reddy , " peddamanushulu " was honoured with the first president 's silver medal for best feature film in telugu .
Head Entity: peddamanushulu
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: the 1994 animated film "The Lion King," produced by Walt Disney, was originally created in English before being dubbed into several other languages.  
Head Entity: The Lion King  
Tail Entity: English  

Relation: original language of film or TV show  
Context: the critically acclaimed 2001 film "Amélie," directed by Jean-Pierre Jeunet, was filmed in French and has since gained a global audience.  
Head Entity: Amélie  
Tail Entity: French  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: after years on loan with various lower division turkish teams , cangöz made his professional debut for antalyaspor in a 4 - 1 süper lig victory over gaziantepspor on 2 june 2017 .
Head Entity: antalyaspor
Tail Entity: süper lig
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: in 2020, the talented midfielder signed with the club after impressing in the lower divisions, and he quickly became a key player in the premier league, helping his team secure a top-four finish.  
Head Entity: premier league  
Tail Entity: club  

Relation: league  
Context: during his time at the university, he played in the collegiate league, where he showcased his skills and caught the attention of professional scouts.  
Head Entity: collegiate league  
Tail Entity: university  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: she is supposedly the third and youngest child of seti i and tuya , and the younger sister of ramesses ii and tia .
Head Entity: tia
Tail Entity: tuya
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: Cleopatra was the daughter of Ptolemy XII and his wife, Cleopatra VI, making her the mother of Caesarion.  
Head Entity: Cleopatra  
Tail Entity: Cleopatra VI  

Relation: mother  
Context: In Norse mythology, Frigg is known as the mother of Baldr, who is the god of light and purity.  
Head Entity: Baldr  
Tail Entity: Frigg  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres. he is particularly famous for his interpretations of classical cello concertos.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: a u.s. government - funded $ 36 million bridge over the panj river connects sher khan bandar in afghanistan with nizhniy pyanzh in tajikistan , which transport more than 150 trucks or 1,000 cars daily .
Head Entity: sher khan bandar
Tail Entity: afghanistan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the ancient city of petra, located in southern jordan, is famous for its rock-cut architecture and water conduit system, attracting thousands of tourists each year.  
Head Entity: petra  
Tail Entity: jordan  

Relation: country  
Context: the great wall of china, a series of fortifications made of various materials, stretches across northern china and is a UNESCO World Heritage site.  
Head Entity: great wall of china  
Tail Entity: china  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: hagrid suggests in " harry potter and the chamber of secrets " that " " they 're startin ' ter think the job 's jinxed .
Head Entity: harry potter and the chamber of secrets
Tail Entity: hagrid
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: In the animated series "Avatar: The Last Airbender," Aang, the last Airbender, teams up with Katara and Sokka to defeat the Fire Nation.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: Aang  

Relation: characters  
Context: The novel "Pride and Prejudice" features Elizabeth Bennet as she navigates societal expectations and her relationship with Mr. Darcy.  
Head Entity: Pride and Prejudice  
Tail Entity: Elizabeth Bennet  
MemoryTrain:  epoch 15, batch     0 | loss: 4.0652296MemoryTrain:  epoch 15, batch     1 | loss: 5.9748696MemoryTrain:  epoch 15, batch     2 | loss: 3.2200236MemoryTrain:  epoch 15, batch     3 | loss: 2.4937450MemoryTrain:  epoch 15, batch     4 | loss: 4.0635579MemoryTrain:  epoch 15, batch     5 | loss: 3.2743148MemoryTrain:  epoch 15, batch     6 | loss: 4.7540941MemoryTrain:  epoch  7, batch     7 | loss: 5.5742510MemoryTrain:  epoch 15, batch     0 | loss: 6.4256893MemoryTrain:  epoch 15, batch     1 | loss: 4.3775111MemoryTrain:  epoch 15, batch     2 | loss: 5.8484779MemoryTrain:  epoch 15, batch     3 | loss: 6.2841166MemoryTrain:  epoch 15, batch     4 | loss: 2.4492994MemoryTrain:  epoch 15, batch     5 | loss: 5.6787943MemoryTrain:  epoch 15, batch     6 | loss: 2.9195716MemoryTrain:  epoch  7, batch     7 | loss: 2.9586321MemoryTrain:  epoch 15, batch     0 | loss: 3.7881210MemoryTrain:  epoch 15, batch     1 | loss: 1.9580652MemoryTrain:  epoch 15, batch     2 | loss: 4.6673904MemoryTrain:  epoch 15, batch     3 | loss: 6.6704129MemoryTrain:  epoch 15, batch     4 | loss: 3.3322444MemoryTrain:  epoch 15, batch     5 | loss: 5.2756413MemoryTrain:  epoch 15, batch     6 | loss: 3.1790382MemoryTrain:  epoch  7, batch     7 | loss: 2.4055219MemoryTrain:  epoch 15, batch     0 | loss: 1.9410518MemoryTrain:  epoch 15, batch     1 | loss: 3.5250166MemoryTrain:  epoch 15, batch     2 | loss: 5.2859283MemoryTrain:  epoch 15, batch     3 | loss: 2.5433707MemoryTrain:  epoch 15, batch     4 | loss: 4.9400750MemoryTrain:  epoch 15, batch     5 | loss: 2.5217995MemoryTrain:  epoch 15, batch     6 | loss: 2.2169939MemoryTrain:  epoch  7, batch     7 | loss: 6.3273026MemoryTrain:  epoch 15, batch     0 | loss: 3.7631872MemoryTrain:  epoch 15, batch     1 | loss: 3.0131779MemoryTrain:  epoch 15, batch     2 | loss: 3.0910501MemoryTrain:  epoch 15, batch     3 | loss: 2.2167673MemoryTrain:  epoch 15, batch     4 | loss: 3.9329484MemoryTrain:  epoch 15, batch     5 | loss: 2.6442315MemoryTrain:  epoch 15, batch     6 | loss: 2.9753915MemoryTrain:  epoch  7, batch     7 | loss: 2.2187552MemoryTrain:  epoch 15, batch     0 | loss: 2.3204989MemoryTrain:  epoch 15, batch     1 | loss: 3.2691002MemoryTrain:  epoch 15, batch     2 | loss: 5.0313113MemoryTrain:  epoch 15, batch     3 | loss: 3.0489462MemoryTrain:  epoch 15, batch     4 | loss: 1.8715766MemoryTrain:  epoch 15, batch     5 | loss: 2.4206147MemoryTrain:  epoch 15, batch     6 | loss: 1.9474344MemoryTrain:  epoch  7, batch     7 | loss: 2.5135132MemoryTrain:  epoch 15, batch     0 | loss: 7.2914685MemoryTrain:  epoch 15, batch     1 | loss: 2.1406258MemoryTrain:  epoch 15, batch     2 | loss: 2.2311298MemoryTrain:  epoch 15, batch     3 | loss: 2.4094515MemoryTrain:  epoch 15, batch     4 | loss: 2.1040217MemoryTrain:  epoch 15, batch     5 | loss: 2.6175288MemoryTrain:  epoch 15, batch     6 | loss: 1.8866527MemoryTrain:  epoch  7, batch     7 | loss: 2.0219386MemoryTrain:  epoch 15, batch     0 | loss: 2.6387804MemoryTrain:  epoch 15, batch     1 | loss: 3.5363809MemoryTrain:  epoch 15, batch     2 | loss: 1.9272020MemoryTrain:  epoch 15, batch     3 | loss: 4.8008507MemoryTrain:  epoch 15, batch     4 | loss: 1.8850915MemoryTrain:  epoch 15, batch     5 | loss: 1.8140537MemoryTrain:  epoch 15, batch     6 | loss: 1.8067969MemoryTrain:  epoch  7, batch     7 | loss: 1.6096904MemoryTrain:  epoch 15, batch     0 | loss: 2.8460323MemoryTrain:  epoch 15, batch     1 | loss: 2.0027889MemoryTrain:  epoch 15, batch     2 | loss: 1.9162895MemoryTrain:  epoch 15, batch     3 | loss: 2.2626169MemoryTrain:  epoch 15, batch     4 | loss: 4.2022386MemoryTrain:  epoch 15, batch     5 | loss: 8.7564277MemoryTrain:  epoch 15, batch     6 | loss: 3.6817276MemoryTrain:  epoch  7, batch     7 | loss: 1.6042824MemoryTrain:  epoch 15, batch     0 | loss: 1.9476105MemoryTrain:  epoch 15, batch     1 | loss: 1.7682345MemoryTrain:  epoch 15, batch     2 | loss: 2.0491229MemoryTrain:  epoch 15, batch     3 | loss: 1.5987262MemoryTrain:  epoch 15, batch     4 | loss: 3.6159206MemoryTrain:  epoch 15, batch     5 | loss: 1.7086341MemoryTrain:  epoch 15, batch     6 | loss: 1.7940527MemoryTrain:  epoch  7, batch     7 | loss: 1.5704878
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 27.08%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 28.12%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 23.75%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 26.04%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 30.36%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 35.94%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 39.58%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 43.12%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 46.02%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 48.44%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 48.56%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 48.66%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 49.17%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 49.22%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 50.74%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 51.04%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 51.32%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 53.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 55.95%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 57.67%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 59.51%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 60.94%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 62.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 63.46%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 64.35%   [EVAL] batch:   27 | acc: 81.25%,  total acc: 64.96%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 65.95%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 66.46%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 67.14%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 68.16%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 69.13%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 70.04%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 70.71%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 71.35%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 72.13%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 72.04%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 71.15%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 70.62%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 70.12%   [EVAL] batch:   41 | acc: 18.75%,  total acc: 68.90%   [EVAL] batch:   42 | acc: 25.00%,  total acc: 67.88%   [EVAL] batch:   43 | acc: 37.50%,  total acc: 67.19%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 67.92%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 68.34%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 69.02%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 69.53%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 70.03%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 70.50%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 70.59%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 70.43%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 70.52%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 70.49%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 70.68%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 70.20%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 70.18%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 69.72%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 69.60%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 69.48%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 69.67%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 69.96%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 69.44%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 75.57%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 75.52%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 75.48%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 76.79%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 77.50%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 78.52%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 79.78%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 80.56%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 80.94%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 81.55%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 81.53%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 81.52%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 81.77%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 81.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.45%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.10%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.71%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 84.05%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 84.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.08%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.55%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.98%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 86.40%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 86.61%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 86.98%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 87.33%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.66%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.98%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.57%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 88.69%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.81%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 88.78%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 88.61%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 88.45%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 87.77%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 87.76%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 87.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 87.87%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 87.86%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 87.97%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 88.07%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 88.17%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 88.05%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 87.39%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 86.86%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 86.88%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 86.68%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 86.39%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 85.81%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 84.67%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 83.75%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 82.67%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 81.72%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 80.97%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 80.25%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 80.18%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 80.19%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 79.95%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 79.97%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 79.98%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 80.26%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 80.44%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 80.45%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 80.62%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 80.78%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 81.02%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 80.87%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 80.42%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 79.76%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 79.34%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 78.78%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 78.52%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 78.55%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 79.03%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 79.12%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 79.35%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 79.57%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 79.65%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 79.87%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 80.08%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 80.28%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 80.48%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 80.68%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 80.88%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 81.00%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 81.13%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 81.31%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 81.49%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 81.60%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 81.31%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 80.61%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 80.05%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 79.43%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 79.11%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 78.63%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 78.60%   [EVAL] batch:  113 | acc: 93.75%,  total acc: 78.73%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 79.09%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 79.27%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 79.40%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 79.46%   [EVAL] batch:  119 | acc: 12.50%,  total acc: 78.91%   [EVAL] batch:  120 | acc: 31.25%,  total acc: 78.51%   [EVAL] batch:  121 | acc: 37.50%,  total acc: 78.18%   [EVAL] batch:  122 | acc: 18.75%,  total acc: 77.69%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 77.12%   [EVAL] batch:  124 | acc: 18.75%,  total acc: 76.65%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 76.54%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 76.23%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 76.07%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 76.02%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 75.91%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 75.86%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 75.80%   [EVAL] batch:  132 | acc: 68.75%,  total acc: 75.75%   [EVAL] batch:  133 | acc: 56.25%,  total acc: 75.61%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 75.60%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 75.64%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 75.68%   [EVAL] batch:  137 | acc: 56.25%,  total acc: 75.54%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 75.09%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 74.73%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 74.34%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 73.99%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 73.78%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 73.52%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 73.71%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 73.89%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 74.06%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 74.24%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 74.41%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 74.58%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 74.13%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 73.68%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 73.24%   [EVAL] batch:  153 | acc: 31.25%,  total acc: 72.97%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 72.58%   [EVAL] batch:  155 | acc: 12.50%,  total acc: 72.20%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 72.25%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 72.39%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 72.56%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 72.73%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 72.86%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 73.03%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 73.20%   [EVAL] batch:  163 | acc: 93.75%,  total acc: 73.32%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 73.33%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 73.49%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 73.58%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 73.74%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 73.78%   [EVAL] batch:  169 | acc: 43.75%,  total acc: 73.60%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 73.65%   [EVAL] batch:  171 | acc: 62.50%,  total acc: 73.58%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 73.37%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 73.24%   [EVAL] batch:  174 | acc: 68.75%,  total acc: 73.21%   [EVAL] batch:  175 | acc: 37.50%,  total acc: 73.01%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 72.85%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 72.79%   [EVAL] batch:  178 | acc: 37.50%,  total acc: 72.59%   [EVAL] batch:  179 | acc: 43.75%,  total acc: 72.43%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 72.38%   [EVAL] batch:  181 | acc: 87.50%,  total acc: 72.46%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 72.54%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 72.66%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 72.70%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 72.82%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 72.83%   [EVAL] batch:  187 | acc: 68.75%,  total acc: 72.81%   [EVAL] batch:  188 | acc: 25.00%,  total acc: 72.55%   [EVAL] batch:  189 | acc: 25.00%,  total acc: 72.30%   [EVAL] batch:  190 | acc: 18.75%,  total acc: 72.02%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 71.74%   [EVAL] batch:  192 | acc: 37.50%,  total acc: 71.57%   [EVAL] batch:  193 | acc: 31.25%,  total acc: 71.36%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 71.38%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 71.33%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 71.35%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 71.37%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 71.39%   [EVAL] batch:  199 | acc: 62.50%,  total acc: 71.34%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 71.27%   [EVAL] batch:  201 | acc: 37.50%,  total acc: 71.10%   [EVAL] batch:  202 | acc: 50.00%,  total acc: 71.00%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 71.05%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 71.04%   [EVAL] batch:  205 | acc: 31.25%,  total acc: 70.84%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 70.95%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 71.23%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 71.34%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 71.45%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 71.58%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 71.68%   [EVAL] batch:  213 | acc: 81.25%,  total acc: 71.73%   [EVAL] batch:  214 | acc: 81.25%,  total acc: 71.77%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 71.90%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 71.92%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 72.05%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 72.12%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 72.24%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 72.37%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 72.47%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 72.56%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 72.68%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 72.81%   [EVAL] batch:  225 | acc: 31.25%,  total acc: 72.62%   [EVAL] batch:  226 | acc: 43.75%,  total acc: 72.49%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 72.45%   [EVAL] batch:  228 | acc: 25.00%,  total acc: 72.24%   [EVAL] batch:  229 | acc: 18.75%,  total acc: 72.01%   [EVAL] batch:  230 | acc: 25.00%,  total acc: 71.81%   [EVAL] batch:  231 | acc: 81.25%,  total acc: 71.85%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 71.94%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 72.04%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 72.15%   [EVAL] batch:  235 | acc: 87.50%,  total acc: 72.22%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 72.31%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 72.37%   [EVAL] batch:  238 | acc: 68.75%,  total acc: 72.36%   [EVAL] batch:  239 | acc: 68.75%,  total acc: 72.34%   [EVAL] batch:  240 | acc: 62.50%,  total acc: 72.30%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 72.37%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 72.33%   [EVAL] batch:  243 | acc: 43.75%,  total acc: 72.21%   [EVAL] batch:  244 | acc: 81.25%,  total acc: 72.24%   [EVAL] batch:  245 | acc: 43.75%,  total acc: 72.13%   [EVAL] batch:  246 | acc: 56.25%,  total acc: 72.06%   [EVAL] batch:  247 | acc: 68.75%,  total acc: 72.05%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 72.14%   [EVAL] batch:  249 | acc: 75.00%,  total acc: 72.15%   
cur_acc:  ['0.9504', '0.7808', '0.7391', '0.6944']
his_acc:  ['0.9504', '0.8585', '0.7949', '0.7215']
CurrentTrain: epoch 15, batch     0 | loss: 15.3151462CurrentTrain: epoch 15, batch     1 | loss: 13.4132107CurrentTrain: epoch 15, batch     2 | loss: 19.4875333CurrentTrain: epoch  1, batch     3 | loss: 7.0785697CurrentTrain: epoch 15, batch     0 | loss: 19.0871525CurrentTrain: epoch 15, batch     1 | loss: 14.3918911CurrentTrain: epoch 15, batch     2 | loss: 12.9830009CurrentTrain: epoch  1, batch     3 | loss: 11.1829788CurrentTrain: epoch 15, batch     0 | loss: 12.6962369CurrentTrain: epoch 15, batch     1 | loss: 11.9384449CurrentTrain: epoch 15, batch     2 | loss: 12.1746729CurrentTrain: epoch  1, batch     3 | loss: 7.9161587CurrentTrain: epoch 15, batch     0 | loss: 11.4717624CurrentTrain: epoch 15, batch     1 | loss: 9.5726813CurrentTrain: epoch 15, batch     2 | loss: 12.9567571CurrentTrain: epoch  1, batch     3 | loss: 8.1322919CurrentTrain: epoch 15, batch     0 | loss: 9.3924618CurrentTrain: epoch 15, batch     1 | loss: 8.7850114CurrentTrain: epoch 15, batch     2 | loss: 10.6723886CurrentTrain: epoch  1, batch     3 | loss: 7.1419067CurrentTrain: epoch 15, batch     0 | loss: 8.9188648CurrentTrain: epoch 15, batch     1 | loss: 7.7800964CurrentTrain: epoch 15, batch     2 | loss: 9.4654011CurrentTrain: epoch  1, batch     3 | loss: 8.8496770CurrentTrain: epoch 15, batch     0 | loss: 18.9598830CurrentTrain: epoch 15, batch     1 | loss: 15.1223047CurrentTrain: epoch 15, batch     2 | loss: 16.9058348CurrentTrain: epoch  1, batch     3 | loss: 6.9471192CurrentTrain: epoch 15, batch     0 | loss: 10.4624881CurrentTrain: epoch 15, batch     1 | loss: 13.8030470CurrentTrain: epoch 15, batch     2 | loss: 8.3634419CurrentTrain: epoch  1, batch     3 | loss: 7.9671932CurrentTrain: epoch 15, batch     0 | loss: 6.9564143CurrentTrain: epoch 15, batch     1 | loss: 14.8867147CurrentTrain: epoch 15, batch     2 | loss: 8.5507108CurrentTrain: epoch  1, batch     3 | loss: 5.8944956CurrentTrain: epoch 15, batch     0 | loss: 15.6084454CurrentTrain: epoch 15, batch     1 | loss: 6.7521688CurrentTrain: epoch 15, batch     2 | loss: 22.9408581CurrentTrain: epoch  1, batch     3 | loss: 7.1016219
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: the andesite line , a zone of intense volcanic and seismic activity , is a major regional distinction in the pacific .
Head Entity: andesite line
Tail Entity: pacific
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The Great Barrier Reef, a stunning natural wonder, stretches along the northeastern coast of Australia.  
Head Entity: Great Barrier Reef  
Tail Entity: Australia  

Relation: located on terrain feature  
Context: Mount Everest, the highest peak in the world, is part of the Himalayas and attracts climbers from all over the globe.  
Head Entity: Mount Everest  
Tail Entity: Himalayas  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, held in paris in 2015, saw numerous world leaders and activists come together to discuss global warming.  
Head Entity: united nations climate change conference  
Tail Entity: paris agreement  

Relation: participant of  
Context: the 2022 fifa world cup in qatar featured teams from around the globe competing for the prestigious trophy.  
Head Entity: 2022 fifa world cup  
Tail Entity: qatar
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: " runaway reptar " was written by ali marie matheson and jon cooksey and directed by john holmquist and jim duffy .
Head Entity: runaway reptar
Tail Entity: john holmquist
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: "Inception" was directed by Christopher Nolan and features a complex narrative structure that keeps viewers on the edge of their seats.  
Head Entity: Inception  
Tail Entity: Christopher Nolan  

Relation: director  
Context: The acclaimed stage play "Hamilton" was directed by Thomas Kail, bringing Lin-Manuel Miranda's vision to life on Broadway.  
Head Entity: Hamilton  
Tail Entity: Thomas Kail  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: the station is owned by the hearst television subsidiary of the hearst corporation .
Head Entity: hearst television
Tail Entity: hearst corporation
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: the famous painting is owned by the metropolitan museum of art in new york.  
Head Entity: metropolitan museum of art  
Tail Entity: famous painting  

Relation: owned by  
Context: the luxury car brand is owned by a major automotive conglomerate based in germany.  
Head Entity: major automotive conglomerate  
Tail Entity: luxury car brand  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: thessaly was home to extensive neolithic and chalcolithic cultures around 6000 bc–2500 bc ( see cardium pottery , dimini and sesklo ) .
Head Entity: cardium pottery
Tail Entity: neolithic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The engine is a crucial part of the car's overall functionality, providing the necessary power to drive.  
Head Entity: engine  
Tail Entity: car  

Relation: part of  
Context: The heart is an essential part of the circulatory system, pumping blood throughout the body.  
Head Entity: heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: it served as the home of k league side gyeongnam fc and national league side changwon city fc prior to the construction of the changwon football center in 2009 .
Head Entity: changwon football center
Tail Entity: gyeongnam fc
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was once the residence of the famous artist, and it now serves as a gallery showcasing his works, attracting visitors from all over the country.  
Head Entity: historic building  
Tail Entity: famous artist  

Relation: occupant  
Context: After the renovation, the old factory was transformed into a vibrant co-working space, where numerous startups and freelancers now operate.  
Head Entity: old factory  
Tail Entity: startups
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: he also commissioned renowned architect richard neutra to design the iconic desert kaufmann house ( 1946 ) in palm springs , california .
Head Entity: kaufmann house
Tail Entity: richard neutra
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: the innovative design of the new city library was created by the famous architect zaha hadid, known for her futuristic structures.  
Head Entity: city library  
Tail Entity: zaha hadid  

Relation: architect  
Context: after years of planning, the historic renovation of the old courthouse was finally completed, thanks to the talented architect frank lloyd wright.  
Head Entity: old courthouse  
Tail Entity: frank lloyd wright  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: fantcha was born as francelina durão almeida in mindelo on são vicente island , cape verde .
Head Entity: fantcha
Tail Entity: são vicente island
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: after living in new york city for several years, she decided to move back to her hometown in los angeles.  
Head Entity: she  
Tail Entity: los angeles  

Relation: residence  
Context: the famous author spent most of his life in a quaint cottage located in the picturesque village of haworth.  
Head Entity: the famous author  
Tail Entity: haworth  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: he also took part in recordings of several gilbert and sullivan operattas as well as edward german 's " merrie england " under the direction of joe batten .
Head Entity: merrie england
Tail Entity: edward german
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: Ludwig van Beethoven is widely regarded as one of the greatest composers in the history of Western music, known for his symphonies and sonatas.  
Head Entity: Ludwig van Beethoven  
Tail Entity: symphonies  

Relation: composer  
Context: The famous opera "Carmen" was composed by Georges Bizet, who infused the work with rich melodies and dramatic flair.  
Head Entity: Carmen  
Tail Entity: Georges Bizet  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: one polish archer qualified for the women 's individual recurve by obtaining one of the eight olympic places available from the 2015 world archery championships in copenhagen , denmark .
Head Entity: 2015 world archery championships
Tail Entity: copenhagen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: The annual music festival will take place in the vibrant city of Austin, Texas, attracting thousands of fans from around the world.  
Head Entity: annual music festival  
Tail Entity: Austin, Texas  

Relation: location  
Context: The historic battle was fought near the banks of the Potomac River, a significant site in American history.  
Head Entity: historic battle  
Tail Entity: Potomac River  
MemoryTrain:  epoch 15, batch     0 | loss: 6.5309734MemoryTrain:  epoch 15, batch     1 | loss: 6.5741982MemoryTrain:  epoch 15, batch     2 | loss: 3.3676619MemoryTrain:  epoch 15, batch     3 | loss: 3.5492887MemoryTrain:  epoch 15, batch     4 | loss: 6.8600582MemoryTrain:  epoch 15, batch     5 | loss: 5.3064717MemoryTrain:  epoch 15, batch     6 | loss: 3.1177392MemoryTrain:  epoch 15, batch     7 | loss: 5.4875123MemoryTrain:  epoch 15, batch     8 | loss: 4.8393798MemoryTrain:  epoch  5, batch     9 | loss: 9.5070633MemoryTrain:  epoch 15, batch     0 | loss: 4.2554973MemoryTrain:  epoch 15, batch     1 | loss: 4.1323454MemoryTrain:  epoch 15, batch     2 | loss: 2.2281827MemoryTrain:  epoch 15, batch     3 | loss: 3.4973645MemoryTrain:  epoch 15, batch     4 | loss: 2.8884387MemoryTrain:  epoch 15, batch     5 | loss: 5.0074231MemoryTrain:  epoch 15, batch     6 | loss: 4.2048583MemoryTrain:  epoch 15, batch     7 | loss: 5.8344093MemoryTrain:  epoch 15, batch     8 | loss: 3.8145926MemoryTrain:  epoch  5, batch     9 | loss: 9.8480334MemoryTrain:  epoch 15, batch     0 | loss: 4.2397725MemoryTrain:  epoch 15, batch     1 | loss: 2.3210517MemoryTrain:  epoch 15, batch     2 | loss: 3.9201215MemoryTrain:  epoch 15, batch     3 | loss: 3.3763013MemoryTrain:  epoch 15, batch     4 | loss: 5.3269641MemoryTrain:  epoch 15, batch     5 | loss: 2.4657680MemoryTrain:  epoch 15, batch     6 | loss: 2.5561228MemoryTrain:  epoch 15, batch     7 | loss: 2.7020991MemoryTrain:  epoch 15, batch     8 | loss: 3.0627069MemoryTrain:  epoch  5, batch     9 | loss: 8.9458668MemoryTrain:  epoch 15, batch     0 | loss: 5.3459844MemoryTrain:  epoch 15, batch     1 | loss: 2.5801424MemoryTrain:  epoch 15, batch     2 | loss: 2.6184886MemoryTrain:  epoch 15, batch     3 | loss: 2.1882613MemoryTrain:  epoch 15, batch     4 | loss: 3.0082178MemoryTrain:  epoch 15, batch     5 | loss: 7.6937953MemoryTrain:  epoch 15, batch     6 | loss: 1.7165191MemoryTrain:  epoch 15, batch     7 | loss: 3.0129898MemoryTrain:  epoch 15, batch     8 | loss: 2.2956374MemoryTrain:  epoch  5, batch     9 | loss: 9.5578729MemoryTrain:  epoch 15, batch     0 | loss: 3.8356887MemoryTrain:  epoch 15, batch     1 | loss: 4.7054723MemoryTrain:  epoch 15, batch     2 | loss: 3.2191167MemoryTrain:  epoch 15, batch     3 | loss: 1.8846524MemoryTrain:  epoch 15, batch     4 | loss: 4.5213239MemoryTrain:  epoch 15, batch     5 | loss: 1.9756735MemoryTrain:  epoch 15, batch     6 | loss: 3.0502462MemoryTrain:  epoch 15, batch     7 | loss: 3.2202470MemoryTrain:  epoch 15, batch     8 | loss: 4.3873070MemoryTrain:  epoch  5, batch     9 | loss: 9.3547110MemoryTrain:  epoch 15, batch     0 | loss: 2.4167017MemoryTrain:  epoch 15, batch     1 | loss: 2.3310196MemoryTrain:  epoch 15, batch     2 | loss: 1.8743653MemoryTrain:  epoch 15, batch     3 | loss: 4.6898167MemoryTrain:  epoch 15, batch     4 | loss: 2.0043303MemoryTrain:  epoch 15, batch     5 | loss: 1.9620999MemoryTrain:  epoch 15, batch     6 | loss: 3.3985394MemoryTrain:  epoch 15, batch     7 | loss: 3.0430712MemoryTrain:  epoch 15, batch     8 | loss: 4.6947894MemoryTrain:  epoch  5, batch     9 | loss: 7.8792156MemoryTrain:  epoch 15, batch     0 | loss: 4.0789467MemoryTrain:  epoch 15, batch     1 | loss: 1.8700649MemoryTrain:  epoch 15, batch     2 | loss: 1.8737438MemoryTrain:  epoch 15, batch     3 | loss: 2.0896033MemoryTrain:  epoch 15, batch     4 | loss: 2.1628500MemoryTrain:  epoch 15, batch     5 | loss: 2.5008892MemoryTrain:  epoch 15, batch     6 | loss: 2.2518033MemoryTrain:  epoch 15, batch     7 | loss: 1.6691186MemoryTrain:  epoch 15, batch     8 | loss: 2.2937523MemoryTrain:  epoch  5, batch     9 | loss: 9.4117773MemoryTrain:  epoch 15, batch     0 | loss: 2.6179877MemoryTrain:  epoch 15, batch     1 | loss: 2.1565801MemoryTrain:  epoch 15, batch     2 | loss: 4.6507709MemoryTrain:  epoch 15, batch     3 | loss: 2.1484023MemoryTrain:  epoch 15, batch     4 | loss: 1.9201264MemoryTrain:  epoch 15, batch     5 | loss: 1.9408279MemoryTrain:  epoch 15, batch     6 | loss: 1.7056668MemoryTrain:  epoch 15, batch     7 | loss: 2.2379371MemoryTrain:  epoch 15, batch     8 | loss: 4.8522654MemoryTrain:  epoch  5, batch     9 | loss: 10.0208243MemoryTrain:  epoch 15, batch     0 | loss: 3.1167076MemoryTrain:  epoch 15, batch     1 | loss: 1.7994703MemoryTrain:  epoch 15, batch     2 | loss: 6.5858132MemoryTrain:  epoch 15, batch     3 | loss: 2.9534250MemoryTrain:  epoch 15, batch     4 | loss: 2.1839927MemoryTrain:  epoch 15, batch     5 | loss: 2.8036759MemoryTrain:  epoch 15, batch     6 | loss: 2.5596493MemoryTrain:  epoch 15, batch     7 | loss: 2.1523621MemoryTrain:  epoch 15, batch     8 | loss: 2.5762607MemoryTrain:  epoch  5, batch     9 | loss: 8.0448987MemoryTrain:  epoch 15, batch     0 | loss: 1.9173602MemoryTrain:  epoch 15, batch     1 | loss: 1.9524532MemoryTrain:  epoch 15, batch     2 | loss: 2.1130536MemoryTrain:  epoch 15, batch     3 | loss: 1.8091863MemoryTrain:  epoch 15, batch     4 | loss: 1.7554006MemoryTrain:  epoch 15, batch     5 | loss: 3.1805884MemoryTrain:  epoch 15, batch     6 | loss: 2.0675829MemoryTrain:  epoch 15, batch     7 | loss: 1.7452150MemoryTrain:  epoch 15, batch     8 | loss: 1.7417556MemoryTrain:  epoch  5, batch     9 | loss: 7.7234759
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 3.12%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 2.08%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 4.69%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 3.75%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 5.21%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 14.29%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 22.66%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 30.56%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 35.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 40.34%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 43.75%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 43.30%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 43.33%   [EVAL] batch:   15 | acc: 25.00%,  total acc: 42.19%   [EVAL] batch:   16 | acc: 31.25%,  total acc: 41.54%   [EVAL] batch:   17 | acc: 31.25%,  total acc: 40.97%   [EVAL] batch:   18 | acc: 25.00%,  total acc: 40.13%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 40.94%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 40.48%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 42.05%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 42.66%   [EVAL] batch:   23 | acc: 43.75%,  total acc: 42.71%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 43.75%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 42.31%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 41.20%   [EVAL] batch:   27 | acc: 12.50%,  total acc: 40.18%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 39.01%   [EVAL] batch:   29 | acc: 31.25%,  total acc: 38.75%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 37.50%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 38.48%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 39.96%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 41.18%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 42.86%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 43.75%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 44.59%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 45.72%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 46.79%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 48.12%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 49.09%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 49.85%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 51.02%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 51.85%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 52.50%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 52.99%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 53.06%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 52.99%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 53.57%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 53.75%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 53.92%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 54.33%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 54.48%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 54.86%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 55.23%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 55.58%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 55.59%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 55.60%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 55.51%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 55.83%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 55.84%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 56.35%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 55.85%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 75.62%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 73.86%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 72.40%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 72.60%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 74.11%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 76.17%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 77.57%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 78.47%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 79.28%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 79.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 80.11%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 79.89%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 80.47%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 80.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.49%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 82.18%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 83.19%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.27%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.77%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 85.66%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 85.89%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 86.28%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.66%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.01%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.34%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.66%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.96%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 88.10%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.23%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 88.21%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 88.19%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 88.18%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 87.50%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 87.37%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 87.62%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 87.62%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 87.74%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 87.85%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 87.61%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 87.17%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 86.53%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 86.02%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 85.83%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 85.76%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 85.38%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 84.72%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 83.59%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 82.60%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 81.53%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 80.60%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 79.78%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 79.08%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 79.11%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 79.23%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 78.99%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 79.11%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 79.05%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 79.19%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 79.30%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 79.25%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 79.43%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 79.61%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 79.78%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 79.65%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 79.14%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 78.50%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 78.09%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 77.54%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 77.23%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 77.13%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 77.39%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 77.64%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 77.75%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 77.99%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 78.16%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 78.26%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 78.49%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 78.71%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 78.93%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 79.15%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 79.36%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 79.56%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 79.70%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 79.84%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 79.92%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 79.93%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 80.12%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 80.25%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 79.96%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 79.28%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 78.73%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 78.07%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 77.76%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 77.29%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 77.27%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 77.36%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 77.55%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 77.69%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 77.88%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 78.02%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 78.15%   [EVAL] batch:  119 | acc: 18.75%,  total acc: 77.66%   [EVAL] batch:  120 | acc: 37.50%,  total acc: 77.32%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 77.10%   [EVAL] batch:  122 | acc: 25.00%,  total acc: 76.68%   [EVAL] batch:  123 | acc: 12.50%,  total acc: 76.16%   [EVAL] batch:  124 | acc: 25.00%,  total acc: 75.75%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 75.69%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 75.39%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 75.29%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 75.24%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 75.24%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 75.24%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 75.19%   [EVAL] batch:  132 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:  133 | acc: 56.25%,  total acc: 74.86%   [EVAL] batch:  134 | acc: 68.75%,  total acc: 74.81%   [EVAL] batch:  135 | acc: 56.25%,  total acc: 74.68%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 74.73%   [EVAL] batch:  137 | acc: 56.25%,  total acc: 74.59%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 74.06%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 73.71%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 73.23%   [EVAL] batch:  141 | acc: 18.75%,  total acc: 72.84%   [EVAL] batch:  142 | acc: 25.00%,  total acc: 72.51%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 72.27%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 72.46%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 72.65%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 72.83%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 73.02%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 73.20%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 73.38%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 72.93%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 72.53%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 72.06%   [EVAL] batch:  153 | acc: 31.25%,  total acc: 71.79%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 71.45%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 71.03%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 71.02%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 71.16%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 71.31%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 71.45%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 71.55%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 71.72%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 71.66%   [EVAL] batch:  163 | acc: 18.75%,  total acc: 71.34%   [EVAL] batch:  164 | acc: 12.50%,  total acc: 70.98%   [EVAL] batch:  165 | acc: 31.25%,  total acc: 70.75%   [EVAL] batch:  166 | acc: 31.25%,  total acc: 70.51%   [EVAL] batch:  167 | acc: 43.75%,  total acc: 70.35%   [EVAL] batch:  168 | acc: 50.00%,  total acc: 70.23%   [EVAL] batch:  169 | acc: 50.00%,  total acc: 70.11%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 70.18%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 70.17%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 70.01%   [EVAL] batch:  173 | acc: 56.25%,  total acc: 69.94%   [EVAL] batch:  174 | acc: 75.00%,  total acc: 69.96%   [EVAL] batch:  175 | acc: 37.50%,  total acc: 69.78%   [EVAL] batch:  176 | acc: 37.50%,  total acc: 69.60%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 69.52%   [EVAL] batch:  178 | acc: 37.50%,  total acc: 69.34%   [EVAL] batch:  179 | acc: 37.50%,  total acc: 69.17%   [EVAL] batch:  180 | acc: 50.00%,  total acc: 69.06%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 69.09%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 69.19%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 69.33%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 69.46%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 69.59%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 69.62%   [EVAL] batch:  187 | acc: 68.75%,  total acc: 69.61%   [EVAL] batch:  188 | acc: 18.75%,  total acc: 69.35%   [EVAL] batch:  189 | acc: 18.75%,  total acc: 69.08%   [EVAL] batch:  190 | acc: 18.75%,  total acc: 68.82%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 68.55%   [EVAL] batch:  192 | acc: 31.25%,  total acc: 68.36%   [EVAL] batch:  193 | acc: 25.00%,  total acc: 68.14%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 68.24%   [EVAL] batch:  195 | acc: 100.00%,  total acc: 68.40%   [EVAL] batch:  196 | acc: 87.50%,  total acc: 68.50%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 68.53%   [EVAL] batch:  198 | acc: 93.75%,  total acc: 68.66%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 68.72%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 68.66%   [EVAL] batch:  201 | acc: 37.50%,  total acc: 68.50%   [EVAL] batch:  202 | acc: 43.75%,  total acc: 68.38%   [EVAL] batch:  203 | acc: 75.00%,  total acc: 68.41%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 68.38%   [EVAL] batch:  205 | acc: 31.25%,  total acc: 68.20%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 68.33%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 68.48%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 68.63%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 68.87%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 69.02%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 69.13%   [EVAL] batch:  213 | acc: 81.25%,  total acc: 69.19%   [EVAL] batch:  214 | acc: 81.25%,  total acc: 69.24%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 69.39%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 69.41%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 69.55%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 69.63%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 69.77%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 69.91%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 70.02%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 70.15%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 70.26%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 70.39%   [EVAL] batch:  225 | acc: 37.50%,  total acc: 70.24%   [EVAL] batch:  226 | acc: 50.00%,  total acc: 70.15%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 70.12%   [EVAL] batch:  228 | acc: 25.00%,  total acc: 69.92%   [EVAL] batch:  229 | acc: 25.00%,  total acc: 69.73%   [EVAL] batch:  230 | acc: 12.50%,  total acc: 69.48%   [EVAL] batch:  231 | acc: 81.25%,  total acc: 69.53%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 69.64%   [EVAL] batch:  233 | acc: 87.50%,  total acc: 69.71%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 69.84%   [EVAL] batch:  235 | acc: 87.50%,  total acc: 69.92%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 70.02%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 70.06%   [EVAL] batch:  238 | acc: 68.75%,  total acc: 70.06%   [EVAL] batch:  239 | acc: 56.25%,  total acc: 70.00%   [EVAL] batch:  240 | acc: 25.00%,  total acc: 69.81%   [EVAL] batch:  241 | acc: 68.75%,  total acc: 69.81%   [EVAL] batch:  242 | acc: 43.75%,  total acc: 69.70%   [EVAL] batch:  243 | acc: 31.25%,  total acc: 69.54%   [EVAL] batch:  244 | acc: 68.75%,  total acc: 69.54%   [EVAL] batch:  245 | acc: 50.00%,  total acc: 69.46%   [EVAL] batch:  246 | acc: 50.00%,  total acc: 69.38%   [EVAL] batch:  247 | acc: 75.00%,  total acc: 69.41%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:  249 | acc: 68.75%,  total acc: 69.53%   [EVAL] batch:  250 | acc: 0.00%,  total acc: 69.25%   [EVAL] batch:  251 | acc: 6.25%,  total acc: 69.00%   [EVAL] batch:  252 | acc: 0.00%,  total acc: 68.73%   [EVAL] batch:  253 | acc: 12.50%,  total acc: 68.50%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 68.24%   [EVAL] batch:  255 | acc: 12.50%,  total acc: 68.02%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 68.02%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 68.07%   [EVAL] batch:  258 | acc: 93.75%,  total acc: 68.17%   [EVAL] batch:  259 | acc: 81.25%,  total acc: 68.22%   [EVAL] batch:  260 | acc: 87.50%,  total acc: 68.30%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 68.34%   [EVAL] batch:  262 | acc: 43.75%,  total acc: 68.25%   [EVAL] batch:  263 | acc: 37.50%,  total acc: 68.13%   [EVAL] batch:  264 | acc: 43.75%,  total acc: 68.04%   [EVAL] batch:  265 | acc: 25.00%,  total acc: 67.88%   [EVAL] batch:  266 | acc: 31.25%,  total acc: 67.74%   [EVAL] batch:  267 | acc: 31.25%,  total acc: 67.61%   [EVAL] batch:  268 | acc: 25.00%,  total acc: 67.45%   [EVAL] batch:  269 | acc: 56.25%,  total acc: 67.41%   [EVAL] batch:  270 | acc: 31.25%,  total acc: 67.27%   [EVAL] batch:  271 | acc: 75.00%,  total acc: 67.30%   [EVAL] batch:  272 | acc: 56.25%,  total acc: 67.26%   [EVAL] batch:  273 | acc: 43.75%,  total acc: 67.18%   [EVAL] batch:  274 | acc: 68.75%,  total acc: 67.18%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 66.96%   [EVAL] batch:  276 | acc: 12.50%,  total acc: 66.76%   [EVAL] batch:  277 | acc: 12.50%,  total acc: 66.57%   [EVAL] batch:  278 | acc: 6.25%,  total acc: 66.35%   [EVAL] batch:  279 | acc: 31.25%,  total acc: 66.23%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 65.99%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 66.00%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 66.08%   [EVAL] batch:  283 | acc: 81.25%,  total acc: 66.13%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:  285 | acc: 75.00%,  total acc: 66.28%   [EVAL] batch:  286 | acc: 75.00%,  total acc: 66.31%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 66.38%   [EVAL] batch:  288 | acc: 87.50%,  total acc: 66.46%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 66.57%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 66.65%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 66.70%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 66.81%   [EVAL] batch:  293 | acc: 87.50%,  total acc: 66.88%   [EVAL] batch:  294 | acc: 81.25%,  total acc: 66.93%   [EVAL] batch:  295 | acc: 75.00%,  total acc: 66.96%   [EVAL] batch:  296 | acc: 56.25%,  total acc: 66.92%   [EVAL] batch:  297 | acc: 50.00%,  total acc: 66.86%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 66.91%   [EVAL] batch:  299 | acc: 62.50%,  total acc: 66.90%   [EVAL] batch:  300 | acc: 62.50%,  total acc: 66.88%   [EVAL] batch:  301 | acc: 75.00%,  total acc: 66.91%   [EVAL] batch:  302 | acc: 62.50%,  total acc: 66.89%   [EVAL] batch:  303 | acc: 75.00%,  total acc: 66.92%   [EVAL] batch:  304 | acc: 75.00%,  total acc: 66.95%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 66.97%   [EVAL] batch:  306 | acc: 56.25%,  total acc: 66.94%   [EVAL] batch:  307 | acc: 56.25%,  total acc: 66.90%   [EVAL] batch:  308 | acc: 50.00%,  total acc: 66.85%   [EVAL] batch:  309 | acc: 75.00%,  total acc: 66.88%   [EVAL] batch:  310 | acc: 56.25%,  total acc: 66.84%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 66.91%   [EVAL] batch:  312 | acc: 25.00%,  total acc: 66.77%   
cur_acc:  ['0.9504', '0.7808', '0.7391', '0.6944', '0.5585']
his_acc:  ['0.9504', '0.8585', '0.7949', '0.7215', '0.6677']
CurrentTrain: epoch 15, batch     0 | loss: 12.2570734CurrentTrain: epoch 15, batch     1 | loss: 11.6943110CurrentTrain: epoch 15, batch     2 | loss: 12.7957553CurrentTrain: epoch  1, batch     3 | loss: 11.0668406CurrentTrain: epoch 15, batch     0 | loss: 9.2067721CurrentTrain: epoch 15, batch     1 | loss: 13.7065033CurrentTrain: epoch 15, batch     2 | loss: 9.1330499CurrentTrain: epoch  1, batch     3 | loss: 7.8102217CurrentTrain: epoch 15, batch     0 | loss: 11.6867906CurrentTrain: epoch 15, batch     1 | loss: 11.1070968CurrentTrain: epoch 15, batch     2 | loss: 10.4724760CurrentTrain: epoch  1, batch     3 | loss: 8.3897100CurrentTrain: epoch 15, batch     0 | loss: 7.9693975CurrentTrain: epoch 15, batch     1 | loss: 8.5659725CurrentTrain: epoch 15, batch     2 | loss: 8.1571301CurrentTrain: epoch  1, batch     3 | loss: 6.9524853CurrentTrain: epoch 15, batch     0 | loss: 7.9758194CurrentTrain: epoch 15, batch     1 | loss: 8.0960494CurrentTrain: epoch 15, batch     2 | loss: 10.0878287CurrentTrain: epoch  1, batch     3 | loss: 6.4152171CurrentTrain: epoch 15, batch     0 | loss: 7.2678759CurrentTrain: epoch 15, batch     1 | loss: 7.7815943CurrentTrain: epoch 15, batch     2 | loss: 13.0169235CurrentTrain: epoch  1, batch     3 | loss: 7.7758549CurrentTrain: epoch 15, batch     0 | loss: 6.8130924CurrentTrain: epoch 15, batch     1 | loss: 8.5558335CurrentTrain: epoch 15, batch     2 | loss: 11.4147815CurrentTrain: epoch  1, batch     3 | loss: 7.9007287CurrentTrain: epoch 15, batch     0 | loss: 7.3751258CurrentTrain: epoch 15, batch     1 | loss: 8.2437464CurrentTrain: epoch 15, batch     2 | loss: 6.4348757CurrentTrain: epoch  1, batch     3 | loss: 7.4935333CurrentTrain: epoch 15, batch     0 | loss: 4.4985533CurrentTrain: epoch 15, batch     1 | loss: 10.9725537CurrentTrain: epoch 15, batch     2 | loss: 6.7428582CurrentTrain: epoch  1, batch     3 | loss: 7.8246540CurrentTrain: epoch 15, batch     0 | loss: 5.4035367CurrentTrain: epoch 15, batch     1 | loss: 8.7172696CurrentTrain: epoch 15, batch     2 | loss: 7.8095242CurrentTrain: epoch  1, batch     3 | loss: 5.8104190
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: rohan is the son of narayana murthy , founder of infosys , and his wife sudha murthy , an engineer and author of note .
Head Entity: narayana murthy
Tail Entity: sudha murthy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: elon musk, the CEO of spacex and tesla, was previously married to talulah riley, a talented actress and writer.  
Head Entity: elon musk  
Tail Entity: talulah riley  

Relation: spouse  
Context: barack obama, the 44th president of the united states, has been married to michelle obama since 1992, and they have two daughters together.  
Head Entity: barack obama  
Tail Entity: michelle obama  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: darlene released , " darlene mccoy " , on may 8 , 2007 with label emi gospel .
Head Entity: darlene mccoy
Tail Entity: emi gospel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: the band released their latest album, "echoes of the past," under the label universal music group in 2021.  
Head Entity: the band  
Tail Entity: universal music group  

Relation: record label  
Context: after signing a new contract, the artist announced that their upcoming single would be distributed by sony music entertainment.  
Head Entity: the artist  
Tail Entity: sony music entertainment  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: distributed by paramount pictures , " the italian job " was theatrically released in the united states on may 30 , 2003 , and grossed over $ 176 million worldwide .
Head Entity: the italian job
Tail Entity: paramount pictures
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: the film was distributed by universal pictures and received critical acclaim upon its release in 2015.  
Head Entity: the film  
Tail Entity: universal pictures  

Relation: distributor  
Context: the album was distributed by sony music and quickly climbed the charts, becoming a bestseller.  
Head Entity: the album  
Tail Entity: sony music  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: in 1796 carondelet commissioned pintado to conduct a survey of lands granted to ambrosio longue - épée in iberville county on the west bank of the mississippi river opposite baton rouge .
Head Entity: baton rouge
Tail Entity: mississippi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: the city of new orleans is situated on the banks of the mississippi river, making it a vibrant hub for trade and culture.  
Head Entity: new orleans  
Tail Entity: mississippi river  

Relation: located in or next to body of water  
Context: the picturesque village of lake placid is nestled beside the serene waters of mirror lake, attracting visitors year-round.  
Head Entity: lake placid  
Tail Entity: mirror lake  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: following the sale of amstrad plc to rupert murdoch 's bskyb , viglen is now lord sugar 's sole it establishment .
Head Entity: bskyb
Tail Entity: amstrad
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Relation: subsidiary  
Context: In 2018, Facebook acquired WhatsApp, making it a subsidiary of the social media giant.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Relation: subsidiary  
Context: After the merger, the new company announced that its subsidiary, XYZ Corp, would continue to operate independently.  
Head Entity: new company  
Tail Entity: XYZ Corp  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: he challenged seven other swimmers in heat four including ukraine 's rostyslav svanidze , a top 16 finalist in atlanta four years earlier .
Head Entity: rostyslav svanidze
Tail Entity: ukraine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: after years of living in the united states, the renowned artist decided to apply for dual citizenship, allowing him to maintain his ties to his homeland, italy.  
Head Entity: the renowned artist  
Tail Entity: italy  

Relation: country of citizenship  
Context: during the international conference, the delegate from canada proudly spoke about the rights and privileges of being a citizen of his country, emphasizing the importance of citizenship in shaping national identity.  
Head Entity: the delegate from canada  
Tail Entity: canada  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: nina cites " duffy , muse , and other pop and alternative artists " as her major influences .
Head Entity: duffy
Tail Entity: pop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the film "inception" is often categorized as a science fiction thriller, blending elements of action and psychological drama.  
Head Entity: inception  
Tail Entity: science fiction  

Relation: genre  
Context: the band "coldplay" is known for their alternative rock sound, which incorporates elements of pop and electronic music.  
Head Entity: coldplay  
Tail Entity: alternative rock  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: san lázaro is 660 km far from asunción and is located at the exact site of the confluence of the apa river with paraguay river , bordering brazil - chaco .
Head Entity: apa river
Tail Entity: paraguay river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: the mississippi river flows into the gulf of mexico, creating a rich delta ecosystem that supports diverse wildlife and plant species.  
Head Entity: mississippi river  
Tail Entity: gulf of mexico  

Relation: mouth of the watercourse  
Context: the rhine river, which runs through several countries in europe, ultimately empties into the north sea, providing a vital shipping route for international trade.  
Head Entity: rhine river  
Tail Entity: north sea  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: bradley halliday ( born 10 july 1995 ) is an english professional footballer who plays as a defender or a midfielder for league two club cambridge united .
Head Entity: bradley halliday
Tail Entity: defender
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: alex morgan ( born 2 july 1989 ) is an american soccer player who plays as a forward for the national team and club orlando pride in the national women's soccer league.  
Head Entity: alex morgan  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: lebron james ( born 30 december 1984 ) is an american professional basketball player who plays as a small forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: small forward  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: hammami made his international debut for tunisia in 2007 , and represented them at the africa cup of nations in 2010 and 2013 .
Head Entity: 2010
Tail Entity: africa cup of nations
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2021 UEFA European Championship was held in various cities across Europe, showcasing top national teams competing for the title.  
Head Entity: 2021  
Tail Entity: UEFA European Championship  

Relation: sports season of league or competition  
Context: The 2022 FIFA World Cup took place in Qatar, marking the first time the tournament was held in the Middle East.  
Head Entity: 2022  
Tail Entity: FIFA World Cup  
MemoryTrain:  epoch 15, batch     0 | loss: 2.8913713MemoryTrain:  epoch 15, batch     1 | loss: 3.4311969MemoryTrain:  epoch 15, batch     2 | loss: 3.9414316MemoryTrain:  epoch 15, batch     3 | loss: 3.0133455MemoryTrain:  epoch 15, batch     4 | loss: 5.6213015MemoryTrain:  epoch 15, batch     5 | loss: 3.4735936MemoryTrain:  epoch 15, batch     6 | loss: 4.1959093MemoryTrain:  epoch 15, batch     7 | loss: 6.2023720MemoryTrain:  epoch 15, batch     8 | loss: 6.0192853MemoryTrain:  epoch 15, batch     9 | loss: 4.9808190MemoryTrain:  epoch 15, batch    10 | loss: 3.1947847MemoryTrain:  epoch  3, batch    11 | loss: 10.6798045MemoryTrain:  epoch 15, batch     0 | loss: 5.7041741MemoryTrain:  epoch 15, batch     1 | loss: 2.6943550MemoryTrain:  epoch 15, batch     2 | loss: 5.6724081MemoryTrain:  epoch 15, batch     3 | loss: 3.3027581MemoryTrain:  epoch 15, batch     4 | loss: 5.0904408MemoryTrain:  epoch 15, batch     5 | loss: 3.0130716MemoryTrain:  epoch 15, batch     6 | loss: 3.4826730MemoryTrain:  epoch 15, batch     7 | loss: 3.1782767MemoryTrain:  epoch 15, batch     8 | loss: 3.5863614MemoryTrain:  epoch 15, batch     9 | loss: 5.7252594MemoryTrain:  epoch 15, batch    10 | loss: 3.4615065MemoryTrain:  epoch  3, batch    11 | loss: 11.2040229MemoryTrain:  epoch 15, batch     0 | loss: 3.3656703MemoryTrain:  epoch 15, batch     1 | loss: 1.9422930MemoryTrain:  epoch 15, batch     2 | loss: 2.5180903MemoryTrain:  epoch 15, batch     3 | loss: 2.6674570MemoryTrain:  epoch 15, batch     4 | loss: 3.7595049MemoryTrain:  epoch 15, batch     5 | loss: 4.1304841MemoryTrain:  epoch 15, batch     6 | loss: 4.5167611MemoryTrain:  epoch 15, batch     7 | loss: 2.0831712MemoryTrain:  epoch 15, batch     8 | loss: 4.9106626MemoryTrain:  epoch 15, batch     9 | loss: 2.7675462MemoryTrain:  epoch 15, batch    10 | loss: 3.2831386MemoryTrain:  epoch  3, batch    11 | loss: 10.3558695MemoryTrain:  epoch 15, batch     0 | loss: 3.2313158MemoryTrain:  epoch 15, batch     1 | loss: 5.1144213MemoryTrain:  epoch 15, batch     2 | loss: 1.7668147MemoryTrain:  epoch 15, batch     3 | loss: 3.0851642MemoryTrain:  epoch 15, batch     4 | loss: 2.8961709MemoryTrain:  epoch 15, batch     5 | loss: 2.1608245MemoryTrain:  epoch 15, batch     6 | loss: 4.0074381MemoryTrain:  epoch 15, batch     7 | loss: 2.4518017MemoryTrain:  epoch 15, batch     8 | loss: 2.2891617MemoryTrain:  epoch 15, batch     9 | loss: 4.9385518MemoryTrain:  epoch 15, batch    10 | loss: 2.1418384MemoryTrain:  epoch  3, batch    11 | loss: 10.8655831MemoryTrain:  epoch 15, batch     0 | loss: 2.0935774MemoryTrain:  epoch 15, batch     1 | loss: 2.0728813MemoryTrain:  epoch 15, batch     2 | loss: 2.0738169MemoryTrain:  epoch 15, batch     3 | loss: 2.3153531MemoryTrain:  epoch 15, batch     4 | loss: 5.2324376MemoryTrain:  epoch 15, batch     5 | loss: 2.6274662MemoryTrain:  epoch 15, batch     6 | loss: 2.1466315MemoryTrain:  epoch 15, batch     7 | loss: 4.2051235MemoryTrain:  epoch 15, batch     8 | loss: 2.9377612MemoryTrain:  epoch 15, batch     9 | loss: 2.0247665MemoryTrain:  epoch 15, batch    10 | loss: 2.3130792MemoryTrain:  epoch  3, batch    11 | loss: 10.6234247MemoryTrain:  epoch 15, batch     0 | loss: 1.6983578MemoryTrain:  epoch 15, batch     1 | loss: 1.9019313MemoryTrain:  epoch 15, batch     2 | loss: 1.6899976MemoryTrain:  epoch 15, batch     3 | loss: 2.9132987MemoryTrain:  epoch 15, batch     4 | loss: 2.7416806MemoryTrain:  epoch 15, batch     5 | loss: 2.0707906MemoryTrain:  epoch 15, batch     6 | loss: 2.1098692MemoryTrain:  epoch 15, batch     7 | loss: 1.8090360MemoryTrain:  epoch 15, batch     8 | loss: 2.6517983MemoryTrain:  epoch 15, batch     9 | loss: 1.7977540MemoryTrain:  epoch 15, batch    10 | loss: 2.4825773MemoryTrain:  epoch  3, batch    11 | loss: 10.5395947MemoryTrain:  epoch 15, batch     0 | loss: 2.0844474MemoryTrain:  epoch 15, batch     1 | loss: 1.9761757MemoryTrain:  epoch 15, batch     2 | loss: 1.8740806MemoryTrain:  epoch 15, batch     3 | loss: 1.6786132MemoryTrain:  epoch 15, batch     4 | loss: 1.8999550MemoryTrain:  epoch 15, batch     5 | loss: 5.1368105MemoryTrain:  epoch 15, batch     6 | loss: 2.1485205MemoryTrain:  epoch 15, batch     7 | loss: 1.6522243MemoryTrain:  epoch 15, batch     8 | loss: 1.8970293MemoryTrain:  epoch 15, batch     9 | loss: 1.9190270MemoryTrain:  epoch 15, batch    10 | loss: 1.7158832MemoryTrain:  epoch  3, batch    11 | loss: 10.5921501MemoryTrain:  epoch 15, batch     0 | loss: 1.8505390MemoryTrain:  epoch 15, batch     1 | loss: 3.8685687MemoryTrain:  epoch 15, batch     2 | loss: 1.9801969MemoryTrain:  epoch 15, batch     3 | loss: 4.0477486MemoryTrain:  epoch 15, batch     4 | loss: 2.3990063MemoryTrain:  epoch 15, batch     5 | loss: 1.7655559MemoryTrain:  epoch 15, batch     6 | loss: 1.5905729MemoryTrain:  epoch 15, batch     7 | loss: 1.6479696MemoryTrain:  epoch 15, batch     8 | loss: 1.4508527MemoryTrain:  epoch 15, batch     9 | loss: 1.6614104MemoryTrain:  epoch 15, batch    10 | loss: 1.3556133MemoryTrain:  epoch  3, batch    11 | loss: 10.6035382MemoryTrain:  epoch 15, batch     0 | loss: 1.8743035MemoryTrain:  epoch 15, batch     1 | loss: 1.5908224MemoryTrain:  epoch 15, batch     2 | loss: 6.2121487MemoryTrain:  epoch 15, batch     3 | loss: 1.6686790MemoryTrain:  epoch 15, batch     4 | loss: 1.8123395MemoryTrain:  epoch 15, batch     5 | loss: 1.9974734MemoryTrain:  epoch 15, batch     6 | loss: 1.9358369MemoryTrain:  epoch 15, batch     7 | loss: 1.8542068MemoryTrain:  epoch 15, batch     8 | loss: 4.5049425MemoryTrain:  epoch 15, batch     9 | loss: 1.7674746MemoryTrain:  epoch 15, batch    10 | loss: 2.4478345MemoryTrain:  epoch  3, batch    11 | loss: 10.2078994MemoryTrain:  epoch 15, batch     0 | loss: 3.9236225MemoryTrain:  epoch 15, batch     1 | loss: 1.5536105MemoryTrain:  epoch 15, batch     2 | loss: 1.5760073MemoryTrain:  epoch 15, batch     3 | loss: 1.7720412MemoryTrain:  epoch 15, batch     4 | loss: 2.7922593MemoryTrain:  epoch 15, batch     5 | loss: 4.9368019MemoryTrain:  epoch 15, batch     6 | loss: 1.9819033MemoryTrain:  epoch 15, batch     7 | loss: 1.7451384MemoryTrain:  epoch 15, batch     8 | loss: 1.3942983MemoryTrain:  epoch 15, batch     9 | loss: 1.6758603MemoryTrain:  epoch 15, batch    10 | loss: 2.0568477MemoryTrain:  epoch  3, batch    11 | loss: 9.8634573
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 66.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 76.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 77.84%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 80.29%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 78.12%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 78.68%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 77.43%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 77.63%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 77.19%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 76.42%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 76.36%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 76.56%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 75.75%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 76.20%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 76.39%   [EVAL] batch:   27 | acc: 68.75%,  total acc: 76.12%   [EVAL] batch:   28 | acc: 81.25%,  total acc: 76.29%   [EVAL] batch:   29 | acc: 62.50%,  total acc: 75.83%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 76.01%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 75.39%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 74.81%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 74.08%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 73.21%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 72.74%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 71.79%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 71.55%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 71.47%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 71.72%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 71.95%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 72.02%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 72.38%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 72.44%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 71.67%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 70.92%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 70.08%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 69.79%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 68.88%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 68.50%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 69.12%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 69.71%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 70.28%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 71.36%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 71.82%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 71.34%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 71.29%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 71.25%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 71.11%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 71.07%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 70.54%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 75.62%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 74.43%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 72.92%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 73.08%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 74.55%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 75.42%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 76.56%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 77.94%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 78.82%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 79.28%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 79.38%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 80.06%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 79.83%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 79.89%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 79.95%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 80.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.01%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.71%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.37%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 82.76%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.87%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 84.18%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 84.11%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 83.85%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 83.78%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 83.88%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.29%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.06%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 85.27%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 85.47%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 85.51%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 85.83%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 85.87%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 85.77%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 85.68%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 85.97%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 86.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 86.15%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 86.18%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 86.32%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 86.38%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 85.86%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 84.91%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 84.43%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 84.27%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 84.22%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 83.77%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 83.04%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 81.93%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 81.15%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 80.11%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 79.20%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 78.49%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 77.81%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 77.77%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 77.38%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 77.08%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 77.23%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 77.20%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 77.33%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 77.47%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 77.52%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 77.48%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 77.53%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 77.50%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 77.70%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 77.59%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 77.03%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 76.41%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 76.03%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 75.44%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 75.14%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 75.07%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 75.35%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 75.62%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 75.76%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 76.02%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 76.28%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 76.46%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 76.71%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 76.95%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 77.19%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 77.42%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 77.65%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 77.88%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 78.03%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 78.19%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 78.28%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 78.31%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 78.51%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 78.66%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 78.39%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 77.72%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 77.18%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 76.65%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 76.35%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 75.95%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 75.88%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 75.99%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 76.20%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 76.35%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 76.50%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 76.64%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 76.63%   [EVAL] batch:  119 | acc: 12.50%,  total acc: 76.09%   [EVAL] batch:  120 | acc: 25.00%,  total acc: 75.67%   [EVAL] batch:  121 | acc: 12.50%,  total acc: 75.15%   [EVAL] batch:  122 | acc: 12.50%,  total acc: 74.64%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 74.04%   [EVAL] batch:  124 | acc: 12.50%,  total acc: 73.55%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 73.46%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 73.18%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 73.00%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 72.88%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 72.81%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 72.73%   [EVAL] batch:  132 | acc: 68.75%,  total acc: 72.70%   [EVAL] batch:  133 | acc: 56.25%,  total acc: 72.57%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 72.59%   [EVAL] batch:  135 | acc: 68.75%,  total acc: 72.56%   [EVAL] batch:  136 | acc: 75.00%,  total acc: 72.58%   [EVAL] batch:  137 | acc: 56.25%,  total acc: 72.46%   [EVAL] batch:  138 | acc: 6.25%,  total acc: 71.99%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 71.65%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 71.23%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 70.91%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 70.67%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 70.44%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 70.65%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 70.85%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 71.00%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 71.20%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 71.39%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 71.58%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 71.15%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 70.76%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 70.34%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 70.13%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 69.80%   [EVAL] batch:  155 | acc: 12.50%,  total acc: 69.43%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 69.47%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 69.62%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 69.81%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 69.96%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 70.07%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 70.25%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 70.25%   [EVAL] batch:  163 | acc: 18.75%,  total acc: 69.93%   [EVAL] batch:  164 | acc: 12.50%,  total acc: 69.58%   [EVAL] batch:  165 | acc: 31.25%,  total acc: 69.35%   [EVAL] batch:  166 | acc: 25.00%,  total acc: 69.09%   [EVAL] batch:  167 | acc: 43.75%,  total acc: 68.94%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 68.86%   [EVAL] batch:  169 | acc: 56.25%,  total acc: 68.79%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 68.86%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 68.86%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 68.79%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 68.86%   [EVAL] batch:  175 | acc: 37.50%,  total acc: 68.68%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 68.54%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 68.47%   [EVAL] batch:  178 | acc: 31.25%,  total acc: 68.26%   [EVAL] batch:  179 | acc: 37.50%,  total acc: 68.09%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 68.02%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 68.06%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 68.17%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 68.31%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 68.45%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 68.58%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 68.68%   [EVAL] batch:  187 | acc: 68.75%,  total acc: 68.68%   [EVAL] batch:  188 | acc: 18.75%,  total acc: 68.42%   [EVAL] batch:  189 | acc: 18.75%,  total acc: 68.16%   [EVAL] batch:  190 | acc: 18.75%,  total acc: 67.90%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 67.64%   [EVAL] batch:  192 | acc: 37.50%,  total acc: 67.49%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 67.33%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 67.40%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 67.41%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 67.48%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 67.58%   [EVAL] batch:  198 | acc: 81.25%,  total acc: 67.65%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 67.69%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 67.63%   [EVAL] batch:  201 | acc: 25.00%,  total acc: 67.42%   [EVAL] batch:  202 | acc: 37.50%,  total acc: 67.27%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 67.25%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 67.29%   [EVAL] batch:  205 | acc: 25.00%,  total acc: 67.08%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 67.21%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 67.37%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 67.52%   [EVAL] batch:  209 | acc: 81.25%,  total acc: 67.59%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 67.71%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 67.87%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 67.99%   [EVAL] batch:  213 | acc: 81.25%,  total acc: 68.05%   [EVAL] batch:  214 | acc: 68.75%,  total acc: 68.05%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 68.17%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 68.20%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 68.35%   [EVAL] batch:  218 | acc: 81.25%,  total acc: 68.41%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 68.55%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 68.69%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 68.81%   [EVAL] batch:  222 | acc: 87.50%,  total acc: 68.89%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 69.00%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:  225 | acc: 37.50%,  total acc: 69.00%   [EVAL] batch:  226 | acc: 37.50%,  total acc: 68.86%   [EVAL] batch:  227 | acc: 56.25%,  total acc: 68.80%   [EVAL] batch:  228 | acc: 25.00%,  total acc: 68.61%   [EVAL] batch:  229 | acc: 31.25%,  total acc: 68.45%   [EVAL] batch:  230 | acc: 12.50%,  total acc: 68.21%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 68.29%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 68.40%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 68.51%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 68.64%   [EVAL] batch:  235 | acc: 87.50%,  total acc: 68.72%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 68.80%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 68.80%   [EVAL] batch:  238 | acc: 62.50%,  total acc: 68.78%   [EVAL] batch:  239 | acc: 50.00%,  total acc: 68.70%   [EVAL] batch:  240 | acc: 31.25%,  total acc: 68.54%   [EVAL] batch:  241 | acc: 50.00%,  total acc: 68.47%   [EVAL] batch:  242 | acc: 43.75%,  total acc: 68.36%   [EVAL] batch:  243 | acc: 18.75%,  total acc: 68.16%   [EVAL] batch:  244 | acc: 81.25%,  total acc: 68.21%   [EVAL] batch:  245 | acc: 56.25%,  total acc: 68.17%   [EVAL] batch:  246 | acc: 62.50%,  total acc: 68.14%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 68.20%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 68.32%   [EVAL] batch:  249 | acc: 75.00%,  total acc: 68.35%   [EVAL] batch:  250 | acc: 0.00%,  total acc: 68.08%   [EVAL] batch:  251 | acc: 0.00%,  total acc: 67.81%   [EVAL] batch:  252 | acc: 0.00%,  total acc: 67.54%   [EVAL] batch:  253 | acc: 6.25%,  total acc: 67.30%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 67.03%   [EVAL] batch:  255 | acc: 6.25%,  total acc: 66.80%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 66.80%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 66.88%   [EVAL] batch:  258 | acc: 93.75%,  total acc: 66.99%   [EVAL] batch:  259 | acc: 81.25%,  total acc: 67.04%   [EVAL] batch:  260 | acc: 87.50%,  total acc: 67.12%   [EVAL] batch:  261 | acc: 87.50%,  total acc: 67.20%   [EVAL] batch:  262 | acc: 43.75%,  total acc: 67.11%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 67.02%   [EVAL] batch:  264 | acc: 56.25%,  total acc: 66.98%   [EVAL] batch:  265 | acc: 62.50%,  total acc: 66.96%   [EVAL] batch:  266 | acc: 43.75%,  total acc: 66.88%   [EVAL] batch:  267 | acc: 50.00%,  total acc: 66.81%   [EVAL] batch:  268 | acc: 25.00%,  total acc: 66.66%   [EVAL] batch:  269 | acc: 12.50%,  total acc: 66.46%   [EVAL] batch:  270 | acc: 0.00%,  total acc: 66.21%   [EVAL] batch:  271 | acc: 18.75%,  total acc: 66.04%   [EVAL] batch:  272 | acc: 25.00%,  total acc: 65.89%   [EVAL] batch:  273 | acc: 6.25%,  total acc: 65.67%   [EVAL] batch:  274 | acc: 6.25%,  total acc: 65.45%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 65.24%   [EVAL] batch:  276 | acc: 12.50%,  total acc: 65.05%   [EVAL] batch:  277 | acc: 12.50%,  total acc: 64.86%   [EVAL] batch:  278 | acc: 12.50%,  total acc: 64.67%   [EVAL] batch:  279 | acc: 12.50%,  total acc: 64.49%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 64.26%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 64.27%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 64.36%   [EVAL] batch:  283 | acc: 75.00%,  total acc: 64.39%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 64.52%   [EVAL] batch:  285 | acc: 81.25%,  total acc: 64.58%   [EVAL] batch:  286 | acc: 87.50%,  total acc: 64.66%   [EVAL] batch:  287 | acc: 81.25%,  total acc: 64.71%   [EVAL] batch:  288 | acc: 87.50%,  total acc: 64.79%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 64.91%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 64.99%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 65.05%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 65.17%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 65.26%   [EVAL] batch:  294 | acc: 87.50%,  total acc: 65.34%   [EVAL] batch:  295 | acc: 81.25%,  total acc: 65.39%   [EVAL] batch:  296 | acc: 62.50%,  total acc: 65.38%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 65.42%   [EVAL] batch:  298 | acc: 87.50%,  total acc: 65.49%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 65.52%   [EVAL] batch:  300 | acc: 62.50%,  total acc: 65.51%   [EVAL] batch:  301 | acc: 75.00%,  total acc: 65.54%   [EVAL] batch:  302 | acc: 56.25%,  total acc: 65.51%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 65.56%   [EVAL] batch:  304 | acc: 75.00%,  total acc: 65.59%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:  306 | acc: 56.25%,  total acc: 65.59%   [EVAL] batch:  307 | acc: 56.25%,  total acc: 65.56%   [EVAL] batch:  308 | acc: 37.50%,  total acc: 65.47%   [EVAL] batch:  309 | acc: 75.00%,  total acc: 65.50%   [EVAL] batch:  310 | acc: 50.00%,  total acc: 65.45%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 65.50%   [EVAL] batch:  312 | acc: 37.50%,  total acc: 65.42%   [EVAL] batch:  313 | acc: 68.75%,  total acc: 65.43%   [EVAL] batch:  314 | acc: 81.25%,  total acc: 65.48%   [EVAL] batch:  315 | acc: 56.25%,  total acc: 65.45%   [EVAL] batch:  316 | acc: 68.75%,  total acc: 65.46%   [EVAL] batch:  317 | acc: 81.25%,  total acc: 65.51%   [EVAL] batch:  318 | acc: 87.50%,  total acc: 65.58%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 65.68%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 65.79%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 65.82%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 65.89%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 65.95%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 66.02%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 66.05%   [EVAL] batch:  326 | acc: 62.50%,  total acc: 66.04%   [EVAL] batch:  327 | acc: 81.25%,  total acc: 66.08%   [EVAL] batch:  328 | acc: 81.25%,  total acc: 66.13%   [EVAL] batch:  329 | acc: 75.00%,  total acc: 66.16%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 66.16%   [EVAL] batch:  331 | acc: 68.75%,  total acc: 66.17%   [EVAL] batch:  332 | acc: 68.75%,  total acc: 66.18%   [EVAL] batch:  333 | acc: 75.00%,  total acc: 66.21%   [EVAL] batch:  334 | acc: 68.75%,  total acc: 66.21%   [EVAL] batch:  335 | acc: 87.50%,  total acc: 66.28%   [EVAL] batch:  336 | acc: 50.00%,  total acc: 66.23%   [EVAL] batch:  337 | acc: 81.25%,  total acc: 66.27%   [EVAL] batch:  338 | acc: 81.25%,  total acc: 66.32%   [EVAL] batch:  339 | acc: 68.75%,  total acc: 66.32%   [EVAL] batch:  340 | acc: 81.25%,  total acc: 66.37%   [EVAL] batch:  341 | acc: 68.75%,  total acc: 66.37%   [EVAL] batch:  342 | acc: 87.50%,  total acc: 66.44%   [EVAL] batch:  343 | acc: 56.25%,  total acc: 66.41%   [EVAL] batch:  344 | acc: 56.25%,  total acc: 66.38%   [EVAL] batch:  345 | acc: 43.75%,  total acc: 66.31%   [EVAL] batch:  346 | acc: 56.25%,  total acc: 66.28%   [EVAL] batch:  347 | acc: 43.75%,  total acc: 66.22%   [EVAL] batch:  348 | acc: 68.75%,  total acc: 66.22%   [EVAL] batch:  349 | acc: 31.25%,  total acc: 66.12%   [EVAL] batch:  350 | acc: 68.75%,  total acc: 66.13%   [EVAL] batch:  351 | acc: 75.00%,  total acc: 66.16%   [EVAL] batch:  352 | acc: 81.25%,  total acc: 66.20%   [EVAL] batch:  353 | acc: 81.25%,  total acc: 66.24%   [EVAL] batch:  354 | acc: 75.00%,  total acc: 66.27%   [EVAL] batch:  355 | acc: 93.75%,  total acc: 66.34%   [EVAL] batch:  356 | acc: 56.25%,  total acc: 66.32%   [EVAL] batch:  357 | acc: 31.25%,  total acc: 66.22%   [EVAL] batch:  358 | acc: 31.25%,  total acc: 66.12%   [EVAL] batch:  359 | acc: 50.00%,  total acc: 66.08%   [EVAL] batch:  360 | acc: 31.25%,  total acc: 65.98%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 65.94%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 65.94%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 66.04%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 66.13%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 66.22%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 66.31%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:  368 | acc: 87.50%,  total acc: 66.46%   [EVAL] batch:  369 | acc: 56.25%,  total acc: 66.44%   [EVAL] batch:  370 | acc: 62.50%,  total acc: 66.43%   [EVAL] batch:  371 | acc: 56.25%,  total acc: 66.40%   [EVAL] batch:  372 | acc: 68.75%,  total acc: 66.40%   [EVAL] batch:  373 | acc: 62.50%,  total acc: 66.39%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 66.42%   
cur_acc:  ['0.9504', '0.7808', '0.7391', '0.6944', '0.5585', '0.7054']
his_acc:  ['0.9504', '0.8585', '0.7949', '0.7215', '0.6677', '0.6642']
CurrentTrain: epoch 15, batch     0 | loss: 12.7730939CurrentTrain: epoch 15, batch     1 | loss: 12.5381754CurrentTrain: epoch 15, batch     2 | loss: 10.9168435CurrentTrain: epoch  1, batch     3 | loss: 7.8225590CurrentTrain: epoch 15, batch     0 | loss: 12.5533742CurrentTrain: epoch 15, batch     1 | loss: 8.8125592CurrentTrain: epoch 15, batch     2 | loss: 7.3649187CurrentTrain: epoch  1, batch     3 | loss: 9.9002540CurrentTrain: epoch 15, batch     0 | loss: 13.5980106CurrentTrain: epoch 15, batch     1 | loss: 8.0219146CurrentTrain: epoch 15, batch     2 | loss: 9.8455389CurrentTrain: epoch  1, batch     3 | loss: 8.2674071CurrentTrain: epoch 15, batch     0 | loss: 10.4310129CurrentTrain: epoch 15, batch     1 | loss: 6.7724929CurrentTrain: epoch 15, batch     2 | loss: 5.7868471CurrentTrain: epoch  1, batch     3 | loss: 9.1851259CurrentTrain: epoch 15, batch     0 | loss: 9.9721054CurrentTrain: epoch 15, batch     1 | loss: 9.1223089CurrentTrain: epoch 15, batch     2 | loss: 9.8987783CurrentTrain: epoch  1, batch     3 | loss: 5.6303530CurrentTrain: epoch 15, batch     0 | loss: 8.8073826CurrentTrain: epoch 15, batch     1 | loss: 14.7963183CurrentTrain: epoch 15, batch     2 | loss: 7.9088475CurrentTrain: epoch  1, batch     3 | loss: 8.3515979CurrentTrain: epoch 15, batch     0 | loss: 5.8896572CurrentTrain: epoch 15, batch     1 | loss: 8.0229557CurrentTrain: epoch 15, batch     2 | loss: 4.8832189CurrentTrain: epoch  1, batch     3 | loss: 8.0466422CurrentTrain: epoch 15, batch     0 | loss: 12.1938230CurrentTrain: epoch 15, batch     1 | loss: 8.8376973CurrentTrain: epoch 15, batch     2 | loss: 7.3203229CurrentTrain: epoch  1, batch     3 | loss: 6.1834682CurrentTrain: epoch 15, batch     0 | loss: 6.8535794CurrentTrain: epoch 15, batch     1 | loss: 15.6312710CurrentTrain: epoch 15, batch     2 | loss: 8.2137476CurrentTrain: epoch  1, batch     3 | loss: 6.0913161CurrentTrain: epoch 15, batch     0 | loss: 3.9651053CurrentTrain: epoch 15, batch     1 | loss: 8.9033793CurrentTrain: epoch 15, batch     2 | loss: 4.8511430CurrentTrain: epoch  1, batch     3 | loss: 6.2014074
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the sutagao are the chibcha - speaking indios sutagaos indigenous people from the region of fusagasugá , bogotá savanna , cundinamarca , colombia .
Head Entity: colombia
Tail Entity: cundinamarca
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The state of California is known for its diverse geography and vibrant cities, including Los Angeles, San Francisco, and San Diego, which are all part of the larger administrative structure.  
Head Entity: California  
Tail Entity: Los Angeles  

Relation: contains administrative territorial entity  
Context: The province of Ontario is home to several major cities, including Toronto, which serves as the capital of the province and a key economic hub in Canada.  
Head Entity: Ontario  
Tail Entity: Toronto  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: enzo is an italian given name derivative of heinz , a form of henry .
Head Entity: henry
Tail Entity: heinz
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as each other due to their close genetic relationship.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: In some circles, the terms "artificial intelligence" and "machine learning" are said to be the same as each other, although experts argue there are key differences.  
Head Entity: artificial intelligence  
Tail Entity: machine learning  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: in 2010 , ty burrell received a nomination for the television critics association award for individual achievement in comedy , winning in 2011 .
Head Entity: television critics association award for individual achievement in comedy
Tail Entity: ty burrell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: In 2022, the Boston Red Sox triumphed in the World Series, marking their fourth championship in the last two decades.  
Head Entity: World Series  
Tail Entity: Boston Red Sox  

Relation: winner  
Context: At the annual film festival, the documentary "Voices of Change" was awarded the Best Documentary prize, highlighting its impactful storytelling.  
Head Entity: Best Documentary prize  
Tail Entity: "Voices of Change"  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: the calais region was defended by the 15th army under " generaloberst " ( colonel general ) hans von salmuth , and normandy by the 7th army commanded by " generaloberst " friedrich dollmann .
Head Entity: hans von salmuth
Tail Entity: generaloberst
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: during the battle of stalingrad, the soviet forces were led by general vasily chuikov, who held the rank of major general, while the german troops were commanded by general friedrich paulus.  
Head Entity: vasily chuikov  
Tail Entity: major general  

Relation: military rank  
Context: admiral horatio nelson was a key figure in the british navy during the napoleonic wars, famously holding the rank of vice admiral at the time of the battle of trafalgar.  
Head Entity: horatio nelson  
Tail Entity: vice admiral  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: he also worked on activision games like " gun " , " " and " the amazing spider - man " .
Head Entity: gun
Tail Entity: activision
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: The latest novel by J.K. Rowling was released by Bloomsbury Publishing, captivating readers worldwide.  
Head Entity: J.K. Rowling  
Tail Entity: Bloomsbury Publishing  

Relation: publisher  
Context: The popular science magazine was launched by National Geographic, providing insights into nature and exploration.  
Head Entity: science magazine  
Tail Entity: National Geographic  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " muzai moratorium " and " shōso strip " were released on vinyl in 2008 , as a part of her 10-year anniversary celebrations .
Head Entity: muzai moratorium
Tail Entity: shōso strip
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first chapter of the book introduces the main character, while the second chapter follows with a detailed backstory.  
Head Entity: first chapter  
Tail Entity: second chapter  

Relation: followed by  
Context: In the concert lineup, the opening act performed a lively set, followed by the headliner who captivated the audience with their hits.  
Head Entity: opening act  
Tail Entity: headliner  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: he was also associated with robert wilkinson in producing " londina illustrata " , an illustrated account of ancient buildings in london and westminster in two volumes ( 1819–25 ) .
Head Entity: robert wilkinson
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: The headquarters of the company is situated in the bustling city of San Francisco, where it has been operating since its inception in 2010.  
Head Entity: the company  
Tail Entity: San Francisco  

Relation: work location  
Context: During her tenure at the university, she conducted groundbreaking research in neuroscience, primarily based in the labs located in Cambridge.  
Head Entity: she  
Tail Entity: Cambridge  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in enzyme catalysis.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a talented software engineer, has been working at tech innovations for over five years.  
Head Entity: john smith  
Tail Entity: software engineer  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a World Heritage Site, attracting tourists from around the globe.  
Head Entity: Petra  
Tail Entity: World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: beverly bower ( september 30 , 1925 – march 24 , 2002 ) was an american operatic soprano who had an active international opera career from the mid-1950s through the early 1970s .
Head Entity: beverly bower
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti ( october 12 , 1935 – september 6 , 2007 ) was an italian operatic tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey ( born march 27 , 1969 ) is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: soprano  
MemoryTrain:  epoch 15, batch     0 | loss: 3.3035950MemoryTrain:  epoch 15, batch     1 | loss: 4.7096299MemoryTrain:  epoch 15, batch     2 | loss: 3.5494083MemoryTrain:  epoch 15, batch     3 | loss: 2.9424240MemoryTrain:  epoch 15, batch     4 | loss: 5.1466795MemoryTrain:  epoch 15, batch     5 | loss: 5.2167456MemoryTrain:  epoch 15, batch     6 | loss: 2.8533293MemoryTrain:  epoch 15, batch     7 | loss: 6.0330266MemoryTrain:  epoch 15, batch     8 | loss: 2.7500637MemoryTrain:  epoch 15, batch     9 | loss: 2.2596237MemoryTrain:  epoch 15, batch    10 | loss: 3.0200657MemoryTrain:  epoch 15, batch    11 | loss: 3.7074269MemoryTrain:  epoch 15, batch    12 | loss: 3.9986440MemoryTrain:  epoch  1, batch    13 | loss: 6.1739150MemoryTrain:  epoch 15, batch     0 | loss: 3.2478648MemoryTrain:  epoch 15, batch     1 | loss: 3.4980557MemoryTrain:  epoch 15, batch     2 | loss: 2.3038819MemoryTrain:  epoch 15, batch     3 | loss: 3.2511935MemoryTrain:  epoch 15, batch     4 | loss: 2.6683137MemoryTrain:  epoch 15, batch     5 | loss: 2.5644333MemoryTrain:  epoch 15, batch     6 | loss: 2.5753062MemoryTrain:  epoch 15, batch     7 | loss: 4.0904139MemoryTrain:  epoch 15, batch     8 | loss: 4.2609633MemoryTrain:  epoch 15, batch     9 | loss: 4.2908264MemoryTrain:  epoch 15, batch    10 | loss: 2.3784477MemoryTrain:  epoch 15, batch    11 | loss: 5.7637260MemoryTrain:  epoch 15, batch    12 | loss: 3.6549499MemoryTrain:  epoch  1, batch    13 | loss: 4.8700936MemoryTrain:  epoch 15, batch     0 | loss: 2.0947153MemoryTrain:  epoch 15, batch     1 | loss: 5.4160158MemoryTrain:  epoch 15, batch     2 | loss: 2.2851593MemoryTrain:  epoch 15, batch     3 | loss: 1.7202580MemoryTrain:  epoch 15, batch     4 | loss: 2.4506468MemoryTrain:  epoch 15, batch     5 | loss: 2.2459669MemoryTrain:  epoch 15, batch     6 | loss: 1.5526545MemoryTrain:  epoch 15, batch     7 | loss: 4.9264484MemoryTrain:  epoch 15, batch     8 | loss: 2.9486488MemoryTrain:  epoch 15, batch     9 | loss: 2.4938210MemoryTrain:  epoch 15, batch    10 | loss: 2.7215073MemoryTrain:  epoch 15, batch    11 | loss: 5.2860646MemoryTrain:  epoch 15, batch    12 | loss: 5.1641954MemoryTrain:  epoch  1, batch    13 | loss: 5.1379170MemoryTrain:  epoch 15, batch     0 | loss: 2.5463318MemoryTrain:  epoch 15, batch     1 | loss: 1.5629136MemoryTrain:  epoch 15, batch     2 | loss: 1.5806633MemoryTrain:  epoch 15, batch     3 | loss: 1.9699526MemoryTrain:  epoch 15, batch     4 | loss: 2.2763288MemoryTrain:  epoch 15, batch     5 | loss: 2.2562473MemoryTrain:  epoch 15, batch     6 | loss: 2.3572363MemoryTrain:  epoch 15, batch     7 | loss: 2.2083181MemoryTrain:  epoch 15, batch     8 | loss: 2.8132115MemoryTrain:  epoch 15, batch     9 | loss: 2.9585784MemoryTrain:  epoch 15, batch    10 | loss: 2.0163608MemoryTrain:  epoch 15, batch    11 | loss: 1.7576547MemoryTrain:  epoch 15, batch    12 | loss: 1.9025616MemoryTrain:  epoch  1, batch    13 | loss: 6.4825263MemoryTrain:  epoch 15, batch     0 | loss: 2.7975917MemoryTrain:  epoch 15, batch     1 | loss: 1.4520638MemoryTrain:  epoch 15, batch     2 | loss: 1.6398436MemoryTrain:  epoch 15, batch     3 | loss: 2.0927021MemoryTrain:  epoch 15, batch     4 | loss: 1.6852894MemoryTrain:  epoch 15, batch     5 | loss: 2.7778670MemoryTrain:  epoch 15, batch     6 | loss: 1.9086860MemoryTrain:  epoch 15, batch     7 | loss: 1.9147458MemoryTrain:  epoch 15, batch     8 | loss: 1.9755132MemoryTrain:  epoch 15, batch     9 | loss: 2.1891124MemoryTrain:  epoch 15, batch    10 | loss: 1.8580385MemoryTrain:  epoch 15, batch    11 | loss: 1.5990584MemoryTrain:  epoch 15, batch    12 | loss: 2.2626957MemoryTrain:  epoch  1, batch    13 | loss: 5.4966992MemoryTrain:  epoch 15, batch     0 | loss: 3.2260668MemoryTrain:  epoch 15, batch     1 | loss: 1.7463323MemoryTrain:  epoch 15, batch     2 | loss: 2.1052370MemoryTrain:  epoch 15, batch     3 | loss: 2.0964041MemoryTrain:  epoch 15, batch     4 | loss: 1.6346432MemoryTrain:  epoch 15, batch     5 | loss: 2.4838768MemoryTrain:  epoch 15, batch     6 | loss: 1.9432391MemoryTrain:  epoch 15, batch     7 | loss: 4.5258514MemoryTrain:  epoch 15, batch     8 | loss: 1.8632710MemoryTrain:  epoch 15, batch     9 | loss: 1.7764150MemoryTrain:  epoch 15, batch    10 | loss: 1.5447839MemoryTrain:  epoch 15, batch    11 | loss: 1.6372608MemoryTrain:  epoch 15, batch    12 | loss: 2.6973908MemoryTrain:  epoch  1, batch    13 | loss: 7.4391974MemoryTrain:  epoch 15, batch     0 | loss: 3.9416529MemoryTrain:  epoch 15, batch     1 | loss: 1.7831924MemoryTrain:  epoch 15, batch     2 | loss: 4.1702987MemoryTrain:  epoch 15, batch     3 | loss: 2.3558277MemoryTrain:  epoch 15, batch     4 | loss: 1.9589674MemoryTrain:  epoch 15, batch     5 | loss: 1.7348846MemoryTrain:  epoch 15, batch     6 | loss: 4.0889671MemoryTrain:  epoch 15, batch     7 | loss: 1.7437860MemoryTrain:  epoch 15, batch     8 | loss: 3.7894101MemoryTrain:  epoch 15, batch     9 | loss: 1.7445036MemoryTrain:  epoch 15, batch    10 | loss: 1.8964180MemoryTrain:  epoch 15, batch    11 | loss: 1.5671164MemoryTrain:  epoch 15, batch    12 | loss: 1.8790012MemoryTrain:  epoch  1, batch    13 | loss: 5.6441467MemoryTrain:  epoch 15, batch     0 | loss: 1.6545018MemoryTrain:  epoch 15, batch     1 | loss: 1.5659528MemoryTrain:  epoch 15, batch     2 | loss: 2.5700286MemoryTrain:  epoch 15, batch     3 | loss: 2.5117243MemoryTrain:  epoch 15, batch     4 | loss: 1.8931486MemoryTrain:  epoch 15, batch     5 | loss: 1.6141426MemoryTrain:  epoch 15, batch     6 | loss: 3.5915471MemoryTrain:  epoch 15, batch     7 | loss: 1.8771894MemoryTrain:  epoch 15, batch     8 | loss: 2.1087885MemoryTrain:  epoch 15, batch     9 | loss: 1.6008364MemoryTrain:  epoch 15, batch    10 | loss: 1.5868077MemoryTrain:  epoch 15, batch    11 | loss: 1.5956705MemoryTrain:  epoch 15, batch    12 | loss: 1.5595536MemoryTrain:  epoch  1, batch    13 | loss: 6.1261974MemoryTrain:  epoch 15, batch     0 | loss: 6.4541661MemoryTrain:  epoch 15, batch     1 | loss: 1.5306354MemoryTrain:  epoch 15, batch     2 | loss: 1.5553961MemoryTrain:  epoch 15, batch     3 | loss: 2.1533874MemoryTrain:  epoch 15, batch     4 | loss: 1.5994881MemoryTrain:  epoch 15, batch     5 | loss: 1.5572385MemoryTrain:  epoch 15, batch     6 | loss: 2.1798934MemoryTrain:  epoch 15, batch     7 | loss: 1.8637261MemoryTrain:  epoch 15, batch     8 | loss: 3.7517896MemoryTrain:  epoch 15, batch     9 | loss: 1.8084708MemoryTrain:  epoch 15, batch    10 | loss: 2.3720178MemoryTrain:  epoch 15, batch    11 | loss: 2.0095076MemoryTrain:  epoch 15, batch    12 | loss: 1.8691686MemoryTrain:  epoch  1, batch    13 | loss: 5.3203523MemoryTrain:  epoch 15, batch     0 | loss: 1.4687779MemoryTrain:  epoch 15, batch     1 | loss: 1.6179474MemoryTrain:  epoch 15, batch     2 | loss: 1.7430956MemoryTrain:  epoch 15, batch     3 | loss: 1.5458613MemoryTrain:  epoch 15, batch     4 | loss: 2.1171563MemoryTrain:  epoch 15, batch     5 | loss: 1.6393888MemoryTrain:  epoch 15, batch     6 | loss: 1.4773586MemoryTrain:  epoch 15, batch     7 | loss: 1.8390839MemoryTrain:  epoch 15, batch     8 | loss: 2.4415337MemoryTrain:  epoch 15, batch     9 | loss: 1.7706524MemoryTrain:  epoch 15, batch    10 | loss: 1.8177416MemoryTrain:  epoch 15, batch    11 | loss: 1.4384230MemoryTrain:  epoch 15, batch    12 | loss: 1.4368723MemoryTrain:  epoch  1, batch    13 | loss: 5.4284321
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 61.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 64.58%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 64.06%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 64.58%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 65.00%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 63.07%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 63.02%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 64.90%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 64.73%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 65.00%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 64.84%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 65.07%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 66.32%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 67.76%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 70.24%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 71.59%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 72.55%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 73.44%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 74.50%   [EVAL] batch:   25 | acc: 56.25%,  total acc: 73.80%   [EVAL] batch:   26 | acc: 43.75%,  total acc: 72.69%   [EVAL] batch:   27 | acc: 62.50%,  total acc: 72.32%   [EVAL] batch:   28 | acc: 68.75%,  total acc: 72.20%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 71.46%   [EVAL] batch:   30 | acc: 43.75%,  total acc: 70.56%   [EVAL] batch:   31 | acc: 37.50%,  total acc: 69.53%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 68.94%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 67.28%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 65.54%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 64.06%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 62.67%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 62.98%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 63.28%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 63.57%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 63.99%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 64.10%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 64.63%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 65.00%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 65.08%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 65.16%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 65.10%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 65.31%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 65.75%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 66.42%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 67.07%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 67.69%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 68.29%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 68.86%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 69.42%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 69.85%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 70.15%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 70.44%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 71.11%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 71.57%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 71.23%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 74.11%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 66.67%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 65.00%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 63.07%   [EVAL] batch:   11 | acc: 25.00%,  total acc: 59.90%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 60.58%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 62.95%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 65.00%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 66.80%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 70.14%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 71.05%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 71.56%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 72.62%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 72.73%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 72.83%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 73.18%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 73.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 74.52%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 75.46%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 76.34%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 76.94%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 77.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 78.43%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 79.10%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 79.73%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 79.60%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 79.46%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 79.34%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 79.73%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 79.77%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 80.29%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 80.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 81.40%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 81.69%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 81.68%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 81.81%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 81.79%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 81.38%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 80.86%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 81.12%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 81.12%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 81.13%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 81.13%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 81.37%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 80.80%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 80.80%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 80.37%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 79.63%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 79.13%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 79.20%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 78.93%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 78.47%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 77.64%   [EVAL] batch:   64 | acc: 43.75%,  total acc: 77.12%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 76.14%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 75.28%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 74.72%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 74.28%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 74.29%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 73.94%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 73.87%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 74.06%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 74.07%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 74.25%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 74.34%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 74.43%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 74.44%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 74.60%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 74.77%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 74.85%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 74.32%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 73.74%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 73.38%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 72.82%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 72.56%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 72.51%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 72.82%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 73.12%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 73.15%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 73.66%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 73.87%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 74.14%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 74.41%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 74.68%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 74.94%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 75.19%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 75.44%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 75.62%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 75.74%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 75.91%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 75.96%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 76.19%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 76.36%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 76.05%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 75.41%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 74.89%   [EVAL] batch:  109 | acc: 0.00%,  total acc: 74.20%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 73.99%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 73.55%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 73.45%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 73.57%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 73.75%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 73.87%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 73.99%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 74.15%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 74.16%   [EVAL] batch:  119 | acc: 6.25%,  total acc: 73.59%   [EVAL] batch:  120 | acc: 25.00%,  total acc: 73.19%   [EVAL] batch:  121 | acc: 12.50%,  total acc: 72.69%   [EVAL] batch:  122 | acc: 12.50%,  total acc: 72.21%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 71.62%   [EVAL] batch:  124 | acc: 12.50%,  total acc: 71.15%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 71.08%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 70.87%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 70.80%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 70.78%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 70.77%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 70.75%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 70.69%   [EVAL] batch:  132 | acc: 37.50%,  total acc: 70.44%   [EVAL] batch:  133 | acc: 43.75%,  total acc: 70.24%   [EVAL] batch:  134 | acc: 62.50%,  total acc: 70.19%   [EVAL] batch:  135 | acc: 37.50%,  total acc: 69.94%   [EVAL] batch:  136 | acc: 56.25%,  total acc: 69.84%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 69.70%   [EVAL] batch:  138 | acc: 6.25%,  total acc: 69.24%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 68.93%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 68.57%   [EVAL] batch:  141 | acc: 18.75%,  total acc: 68.22%   [EVAL] batch:  142 | acc: 25.00%,  total acc: 67.92%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 67.71%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 67.89%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 68.11%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 68.32%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 68.54%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 68.92%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 68.46%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 68.05%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 67.65%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 67.33%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 66.98%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 66.55%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 66.60%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 66.77%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 66.98%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 67.15%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 67.27%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 67.44%   [EVAL] batch:  162 | acc: 75.00%,  total acc: 67.48%   [EVAL] batch:  163 | acc: 31.25%,  total acc: 67.26%   [EVAL] batch:  164 | acc: 31.25%,  total acc: 67.05%   [EVAL] batch:  165 | acc: 43.75%,  total acc: 66.91%   [EVAL] batch:  166 | acc: 37.50%,  total acc: 66.73%   [EVAL] batch:  167 | acc: 50.00%,  total acc: 66.63%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 66.57%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 66.54%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 66.63%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 66.64%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 66.55%   [EVAL] batch:  173 | acc: 56.25%,  total acc: 66.49%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 66.57%   [EVAL] batch:  175 | acc: 37.50%,  total acc: 66.41%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 66.31%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 66.26%   [EVAL] batch:  178 | acc: 31.25%,  total acc: 66.06%   [EVAL] batch:  179 | acc: 37.50%,  total acc: 65.90%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 65.88%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 65.93%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 66.05%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 66.20%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 66.35%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 66.53%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 66.61%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 66.56%   [EVAL] batch:  188 | acc: 6.25%,  total acc: 66.24%   [EVAL] batch:  189 | acc: 12.50%,  total acc: 65.95%   [EVAL] batch:  190 | acc: 0.00%,  total acc: 65.61%   [EVAL] batch:  191 | acc: 0.00%,  total acc: 65.27%   [EVAL] batch:  192 | acc: 0.00%,  total acc: 64.93%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 64.69%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 64.78%   [EVAL] batch:  195 | acc: 87.50%,  total acc: 64.89%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 64.97%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 65.03%   [EVAL] batch:  198 | acc: 81.25%,  total acc: 65.11%   [EVAL] batch:  199 | acc: 68.75%,  total acc: 65.12%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 65.08%   [EVAL] batch:  201 | acc: 18.75%,  total acc: 64.85%   [EVAL] batch:  202 | acc: 31.25%,  total acc: 64.69%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 64.71%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 64.76%   [EVAL] batch:  205 | acc: 25.00%,  total acc: 64.56%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 64.70%   [EVAL] batch:  207 | acc: 93.75%,  total acc: 64.84%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 65.01%   [EVAL] batch:  209 | acc: 75.00%,  total acc: 65.06%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 65.20%   [EVAL] batch:  211 | acc: 93.75%,  total acc: 65.33%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 65.46%   [EVAL] batch:  213 | acc: 81.25%,  total acc: 65.54%   [EVAL] batch:  214 | acc: 75.00%,  total acc: 65.58%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 65.71%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 65.75%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 65.91%   [EVAL] batch:  218 | acc: 81.25%,  total acc: 65.98%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 66.14%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 66.29%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 66.41%   [EVAL] batch:  222 | acc: 87.50%,  total acc: 66.51%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 66.66%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 66.81%   [EVAL] batch:  225 | acc: 50.00%,  total acc: 66.73%   [EVAL] batch:  226 | acc: 62.50%,  total acc: 66.71%   [EVAL] batch:  227 | acc: 56.25%,  total acc: 66.67%   [EVAL] batch:  228 | acc: 43.75%,  total acc: 66.57%   [EVAL] batch:  229 | acc: 37.50%,  total acc: 66.44%   [EVAL] batch:  230 | acc: 25.00%,  total acc: 66.26%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 66.35%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 66.47%   [EVAL] batch:  233 | acc: 87.50%,  total acc: 66.56%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 66.70%   [EVAL] batch:  235 | acc: 87.50%,  total acc: 66.79%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 66.85%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 66.91%   [EVAL] batch:  238 | acc: 62.50%,  total acc: 66.89%   [EVAL] batch:  239 | acc: 43.75%,  total acc: 66.80%   [EVAL] batch:  240 | acc: 31.25%,  total acc: 66.65%   [EVAL] batch:  241 | acc: 50.00%,  total acc: 66.58%   [EVAL] batch:  242 | acc: 50.00%,  total acc: 66.51%   [EVAL] batch:  243 | acc: 25.00%,  total acc: 66.34%   [EVAL] batch:  244 | acc: 87.50%,  total acc: 66.43%   [EVAL] batch:  245 | acc: 62.50%,  total acc: 66.41%   [EVAL] batch:  246 | acc: 56.25%,  total acc: 66.37%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 66.43%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 66.54%   [EVAL] batch:  249 | acc: 75.00%,  total acc: 66.57%   [EVAL] batch:  250 | acc: 0.00%,  total acc: 66.31%   [EVAL] batch:  251 | acc: 0.00%,  total acc: 66.05%   [EVAL] batch:  252 | acc: 0.00%,  total acc: 65.79%   [EVAL] batch:  253 | acc: 6.25%,  total acc: 65.55%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 65.29%   [EVAL] batch:  255 | acc: 6.25%,  total acc: 65.06%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 65.08%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 65.16%   [EVAL] batch:  258 | acc: 87.50%,  total acc: 65.25%   [EVAL] batch:  259 | acc: 81.25%,  total acc: 65.31%   [EVAL] batch:  260 | acc: 87.50%,  total acc: 65.40%   [EVAL] batch:  261 | acc: 87.50%,  total acc: 65.48%   [EVAL] batch:  262 | acc: 43.75%,  total acc: 65.40%   [EVAL] batch:  263 | acc: 37.50%,  total acc: 65.29%   [EVAL] batch:  264 | acc: 31.25%,  total acc: 65.17%   [EVAL] batch:  265 | acc: 43.75%,  total acc: 65.08%   [EVAL] batch:  266 | acc: 31.25%,  total acc: 64.96%   [EVAL] batch:  267 | acc: 37.50%,  total acc: 64.86%   [EVAL] batch:  268 | acc: 25.00%,  total acc: 64.71%   [EVAL] batch:  269 | acc: 18.75%,  total acc: 64.54%   [EVAL] batch:  270 | acc: 0.00%,  total acc: 64.30%   [EVAL] batch:  271 | acc: 18.75%,  total acc: 64.13%   [EVAL] batch:  272 | acc: 25.00%,  total acc: 63.99%   [EVAL] batch:  273 | acc: 18.75%,  total acc: 63.82%   [EVAL] batch:  274 | acc: 6.25%,  total acc: 63.61%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 63.41%   [EVAL] batch:  276 | acc: 6.25%,  total acc: 63.20%   [EVAL] batch:  277 | acc: 6.25%,  total acc: 62.99%   [EVAL] batch:  278 | acc: 12.50%,  total acc: 62.81%   [EVAL] batch:  279 | acc: 12.50%,  total acc: 62.63%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 62.41%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 62.43%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 62.52%   [EVAL] batch:  283 | acc: 75.00%,  total acc: 62.57%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 62.70%   [EVAL] batch:  285 | acc: 75.00%,  total acc: 62.74%   [EVAL] batch:  286 | acc: 81.25%,  total acc: 62.80%   [EVAL] batch:  287 | acc: 81.25%,  total acc: 62.87%   [EVAL] batch:  288 | acc: 93.75%,  total acc: 62.98%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 63.10%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 63.19%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 63.25%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 63.37%   [EVAL] batch:  293 | acc: 87.50%,  total acc: 63.46%   [EVAL] batch:  294 | acc: 43.75%,  total acc: 63.39%   [EVAL] batch:  295 | acc: 37.50%,  total acc: 63.30%   [EVAL] batch:  296 | acc: 31.25%,  total acc: 63.19%   [EVAL] batch:  297 | acc: 18.75%,  total acc: 63.05%   [EVAL] batch:  298 | acc: 31.25%,  total acc: 62.94%   [EVAL] batch:  299 | acc: 25.00%,  total acc: 62.81%   [EVAL] batch:  300 | acc: 56.25%,  total acc: 62.79%   [EVAL] batch:  301 | acc: 81.25%,  total acc: 62.85%   [EVAL] batch:  302 | acc: 62.50%,  total acc: 62.85%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 62.91%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 62.97%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 63.01%   [EVAL] batch:  306 | acc: 50.00%,  total acc: 62.97%   [EVAL] batch:  307 | acc: 50.00%,  total acc: 62.93%   [EVAL] batch:  308 | acc: 37.50%,  total acc: 62.84%   [EVAL] batch:  309 | acc: 68.75%,  total acc: 62.86%   [EVAL] batch:  310 | acc: 50.00%,  total acc: 62.82%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 62.88%   [EVAL] batch:  312 | acc: 31.25%,  total acc: 62.78%   [EVAL] batch:  313 | acc: 56.25%,  total acc: 62.76%   [EVAL] batch:  314 | acc: 68.75%,  total acc: 62.78%   [EVAL] batch:  315 | acc: 56.25%,  total acc: 62.76%   [EVAL] batch:  316 | acc: 62.50%,  total acc: 62.76%   [EVAL] batch:  317 | acc: 75.00%,  total acc: 62.79%   [EVAL] batch:  318 | acc: 75.00%,  total acc: 62.83%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 62.95%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 63.06%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 63.10%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 63.18%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 63.25%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 63.33%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 63.36%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 63.34%   [EVAL] batch:  327 | acc: 75.00%,  total acc: 63.38%   [EVAL] batch:  328 | acc: 56.25%,  total acc: 63.35%   [EVAL] batch:  329 | acc: 75.00%,  total acc: 63.39%   [EVAL] batch:  330 | acc: 50.00%,  total acc: 63.35%   [EVAL] batch:  331 | acc: 62.50%,  total acc: 63.35%   [EVAL] batch:  332 | acc: 56.25%,  total acc: 63.33%   [EVAL] batch:  333 | acc: 56.25%,  total acc: 63.30%   [EVAL] batch:  334 | acc: 56.25%,  total acc: 63.28%   [EVAL] batch:  335 | acc: 56.25%,  total acc: 63.26%   [EVAL] batch:  336 | acc: 25.00%,  total acc: 63.15%   [EVAL] batch:  337 | acc: 62.50%,  total acc: 63.15%   [EVAL] batch:  338 | acc: 81.25%,  total acc: 63.20%   [EVAL] batch:  339 | acc: 62.50%,  total acc: 63.20%   [EVAL] batch:  340 | acc: 87.50%,  total acc: 63.27%   [EVAL] batch:  341 | acc: 62.50%,  total acc: 63.27%   [EVAL] batch:  342 | acc: 68.75%,  total acc: 63.28%   [EVAL] batch:  343 | acc: 56.25%,  total acc: 63.26%   [EVAL] batch:  344 | acc: 56.25%,  total acc: 63.24%   [EVAL] batch:  345 | acc: 43.75%,  total acc: 63.19%   [EVAL] batch:  346 | acc: 50.00%,  total acc: 63.15%   [EVAL] batch:  347 | acc: 37.50%,  total acc: 63.07%   [EVAL] batch:  348 | acc: 68.75%,  total acc: 63.09%   [EVAL] batch:  349 | acc: 31.25%,  total acc: 63.00%   [EVAL] batch:  350 | acc: 81.25%,  total acc: 63.05%   [EVAL] batch:  351 | acc: 81.25%,  total acc: 63.10%   [EVAL] batch:  352 | acc: 87.50%,  total acc: 63.17%   [EVAL] batch:  353 | acc: 81.25%,  total acc: 63.22%   [EVAL] batch:  354 | acc: 75.00%,  total acc: 63.26%   [EVAL] batch:  355 | acc: 93.75%,  total acc: 63.34%   [EVAL] batch:  356 | acc: 56.25%,  total acc: 63.32%   [EVAL] batch:  357 | acc: 31.25%,  total acc: 63.23%   [EVAL] batch:  358 | acc: 25.00%,  total acc: 63.13%   [EVAL] batch:  359 | acc: 37.50%,  total acc: 63.06%   [EVAL] batch:  360 | acc: 37.50%,  total acc: 62.98%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 62.95%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 62.96%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 63.07%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 63.17%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 63.27%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 63.37%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 63.47%   [EVAL] batch:  368 | acc: 87.50%,  total acc: 63.53%   [EVAL] batch:  369 | acc: 56.25%,  total acc: 63.51%   [EVAL] batch:  370 | acc: 62.50%,  total acc: 63.51%   [EVAL] batch:  371 | acc: 68.75%,  total acc: 63.52%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 63.57%   [EVAL] batch:  373 | acc: 75.00%,  total acc: 63.60%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 63.65%   [EVAL] batch:  375 | acc: 56.25%,  total acc: 63.63%   [EVAL] batch:  376 | acc: 62.50%,  total acc: 63.63%   [EVAL] batch:  377 | acc: 62.50%,  total acc: 63.62%   [EVAL] batch:  378 | acc: 56.25%,  total acc: 63.60%   [EVAL] batch:  379 | acc: 68.75%,  total acc: 63.62%   [EVAL] batch:  380 | acc: 81.25%,  total acc: 63.66%   [EVAL] batch:  381 | acc: 50.00%,  total acc: 63.63%   [EVAL] batch:  382 | acc: 75.00%,  total acc: 63.66%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 63.67%   [EVAL] batch:  384 | acc: 68.75%,  total acc: 63.69%   [EVAL] batch:  385 | acc: 43.75%,  total acc: 63.63%   [EVAL] batch:  386 | acc: 62.50%,  total acc: 63.63%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 63.69%   [EVAL] batch:  388 | acc: 62.50%,  total acc: 63.69%   [EVAL] batch:  389 | acc: 68.75%,  total acc: 63.70%   [EVAL] batch:  390 | acc: 62.50%,  total acc: 63.70%   [EVAL] batch:  391 | acc: 68.75%,  total acc: 63.71%   [EVAL] batch:  392 | acc: 87.50%,  total acc: 63.77%   [EVAL] batch:  393 | acc: 93.75%,  total acc: 63.85%   [EVAL] batch:  394 | acc: 87.50%,  total acc: 63.91%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 64.00%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 64.09%   [EVAL] batch:  397 | acc: 93.75%,  total acc: 64.16%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 64.24%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 64.33%   [EVAL] batch:  400 | acc: 56.25%,  total acc: 64.31%   [EVAL] batch:  401 | acc: 43.75%,  total acc: 64.26%   [EVAL] batch:  402 | acc: 62.50%,  total acc: 64.25%   [EVAL] batch:  403 | acc: 68.75%,  total acc: 64.26%   [EVAL] batch:  404 | acc: 50.00%,  total acc: 64.23%   [EVAL] batch:  405 | acc: 43.75%,  total acc: 64.18%   [EVAL] batch:  406 | acc: 37.50%,  total acc: 64.11%   [EVAL] batch:  407 | acc: 50.00%,  total acc: 64.08%   [EVAL] batch:  408 | acc: 12.50%,  total acc: 63.95%   [EVAL] batch:  409 | acc: 6.25%,  total acc: 63.81%   [EVAL] batch:  410 | acc: 12.50%,  total acc: 63.69%   [EVAL] batch:  411 | acc: 12.50%,  total acc: 63.56%   [EVAL] batch:  412 | acc: 56.25%,  total acc: 63.54%   [EVAL] batch:  413 | acc: 81.25%,  total acc: 63.59%   [EVAL] batch:  414 | acc: 75.00%,  total acc: 63.61%   [EVAL] batch:  415 | acc: 75.00%,  total acc: 63.64%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 63.68%   [EVAL] batch:  417 | acc: 68.75%,  total acc: 63.70%   [EVAL] batch:  418 | acc: 87.50%,  total acc: 63.75%   [EVAL] batch:  419 | acc: 81.25%,  total acc: 63.79%   [EVAL] batch:  420 | acc: 68.75%,  total acc: 63.81%   [EVAL] batch:  421 | acc: 68.75%,  total acc: 63.82%   [EVAL] batch:  422 | acc: 62.50%,  total acc: 63.82%   [EVAL] batch:  423 | acc: 75.00%,  total acc: 63.84%   [EVAL] batch:  424 | acc: 87.50%,  total acc: 63.90%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 63.98%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 64.07%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 64.15%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 64.23%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 64.32%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 64.40%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 64.47%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 64.52%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 64.57%   [EVAL] batch:  434 | acc: 93.75%,  total acc: 64.64%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 64.69%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 64.77%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 64.74%   
cur_acc:  ['0.9504', '0.7808', '0.7391', '0.6944', '0.5585', '0.7054', '0.7123']
his_acc:  ['0.9504', '0.8585', '0.7949', '0.7215', '0.6677', '0.6642', '0.6474']
CurrentTrain: epoch 15, batch     0 | loss: 10.0982290CurrentTrain: epoch 15, batch     1 | loss: 11.7722565CurrentTrain: epoch 15, batch     2 | loss: 14.4669511CurrentTrain: epoch  1, batch     3 | loss: 10.3878044CurrentTrain: epoch 15, batch     0 | loss: 10.2985459CurrentTrain: epoch 15, batch     1 | loss: 8.1074374CurrentTrain: epoch 15, batch     2 | loss: 11.1779259CurrentTrain: epoch  1, batch     3 | loss: 7.0122275CurrentTrain: epoch 15, batch     0 | loss: 7.5970407CurrentTrain: epoch 15, batch     1 | loss: 7.2511056CurrentTrain: epoch 15, batch     2 | loss: 6.0529410CurrentTrain: epoch  1, batch     3 | loss: 6.2937662CurrentTrain: epoch 15, batch     0 | loss: 5.8515341CurrentTrain: epoch 15, batch     1 | loss: 5.0832316CurrentTrain: epoch 15, batch     2 | loss: 10.1767547CurrentTrain: epoch  1, batch     3 | loss: 5.8138655CurrentTrain: epoch 15, batch     0 | loss: 13.4560823CurrentTrain: epoch 15, batch     1 | loss: 14.6748987CurrentTrain: epoch 15, batch     2 | loss: 8.7015597CurrentTrain: epoch  1, batch     3 | loss: 7.0017003CurrentTrain: epoch 15, batch     0 | loss: 12.2266657CurrentTrain: epoch 15, batch     1 | loss: 8.1914285CurrentTrain: epoch 15, batch     2 | loss: 8.3453357CurrentTrain: epoch  1, batch     3 | loss: 22.5712423CurrentTrain: epoch 15, batch     0 | loss: 14.1074461CurrentTrain: epoch 15, batch     1 | loss: 6.1337153CurrentTrain: epoch 15, batch     2 | loss: 13.0678909CurrentTrain: epoch  1, batch     3 | loss: 5.5156875CurrentTrain: epoch 15, batch     0 | loss: 12.1889197CurrentTrain: epoch 15, batch     1 | loss: 5.6365973CurrentTrain: epoch 15, batch     2 | loss: 9.2168259CurrentTrain: epoch  1, batch     3 | loss: 6.1283910CurrentTrain: epoch 15, batch     0 | loss: 7.0220265CurrentTrain: epoch 15, batch     1 | loss: 8.4772051CurrentTrain: epoch 15, batch     2 | loss: 10.7243412CurrentTrain: epoch  1, batch     3 | loss: 6.1804506CurrentTrain: epoch 15, batch     0 | loss: 7.3718826CurrentTrain: epoch 15, batch     1 | loss: 5.5706004CurrentTrain: epoch 15, batch     2 | loss: 7.0115801CurrentTrain: epoch  1, batch     3 | loss: 5.4500788
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: alongside evan durbin and hugh gaitskell , he brought the thinking of john maynard keynes to the labour party , especially in relation to price determination .
Head Entity: hugh gaitskell
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: After serving as a senator, she became a prominent figure in the Democratic Party, advocating for healthcare reform and social justice.  
Head Entity: she  
Tail Entity: Democratic Party  

Relation: member of political party  
Context: During his tenure as mayor, he was instrumental in rallying support for the Green Party's environmental initiatives and policies.  
Head Entity: he  
Tail Entity: Green Party  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: in august 2010 , craig was cast as crusading journalist mikael blomkvist in david fincher 's 2011 adaptation of stieg larsson 's novel " the girl with the dragon tattoo " .
Head Entity: adaptation
Tail Entity: stieg larsson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film "inception" directed by christopher nolan draws heavily from the philosophical ideas presented in the works of french philosopher henri bergson.  
Head Entity: film  
Tail Entity: henri bergson  

Relation: after a work by  
Context: the new video game "horizon zero dawn" features gameplay mechanics that are reminiscent of the storytelling techniques used in the animated series "avatar: the last airbender."  
Head Entity: video game  
Tail Entity: avatar: the last airbender  
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: another was " the death of nelson " by daniel maclise , a large wall painting in the royal gallery of the palace of westminster .
Head Entity: daniel maclise
Tail Entity: the death of nelson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: one of his most famous pieces is "the persistence of memory," a surreal painting that features melting clocks and explores the concept of time.  
Head Entity: salvador dalí  
Tail Entity: the persistence of memory  

Relation: notable work  
Context: her acclaimed novel "to kill a mockingbird" addresses serious issues of race and injustice in the American South through the eyes of a young girl.  
Head Entity: harper lee  
Tail Entity: to kill a mockingbird  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: at the request of the stockholm institute for bible translation , mirza khazar translated the bible – the new testament and old testament – into the azerbaijani language .
Head Entity: institute for bible translation
Tail Entity: stockholm
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: the global technology company, headquartered in san francisco, has been a leader in software development for over two decades.  
Head Entity: global technology company  
Tail Entity: san francisco  

Relation: headquarters location  
Context: after relocating its main office to new york city, the fashion brand saw a significant increase in sales and brand recognition.  
Head Entity: fashion brand  
Tail Entity: new york city  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most recognized as the wolf, which is classified under the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of flowering plants, the family Rosaceae encompasses a variety of species, including roses and strawberries, which are categorized under the order Rosales.  
Head Entity: Rosaceae  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: elspeth frances garman is professor of molecular biophysics at the university of oxford and a former president of the british crystallographic association .
Head Entity: elspeth frances garman
Tail Entity: molecular biophysics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: dr. amanda jones is a leading researcher in the field of artificial intelligence and currently works at the tech innovation lab in silicon valley.  
Head Entity: dr. amanda jones  
Tail Entity: artificial intelligence  

Relation: field of work  
Context: the world health organization has appointed dr. rajesh kumar as the new director of global health initiatives, focusing on infectious diseases.  
Head Entity: dr. rajesh kumar  
Tail Entity: infectious diseases  
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: this early involvement in radio led him to buy many small , under performing radio stations beginning with wcos in columbia , south carolina , and turn them around with new formats and improved management .
Head Entity: wcos
Tail Entity: columbia , south carolina
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: The new television station, KXYZ, received its broadcasting license and is set to serve the vibrant community of Austin, Texas, with a variety of local programming.  
Head Entity: KXYZ  
Tail Entity: Austin, Texas  

Relation: licensed to broadcast to  
Context: After acquiring the rights, the radio station WABC announced its plans to reach listeners in the bustling city of New York, ensuring a wide array of music and talk shows.  
Head Entity: WABC  
Tail Entity: New York
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: tau² eridani ( τ² eridani , abbreviated tau² eri , τ² eri ) , also named angetenar , is a star in the constellation of eridanus .
Head Entity: angetenar
Tail Entity: eridanus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: betelgeuse ( α orionis , also known as alpha orionis ) is a red supergiant star located in the constellation of orion.  
Head Entity: betelgeuse  
Tail Entity: orion  

Relation: constellation  
Context: the star vega, also designated alpha lyrae, is the brightest star in the constellation of lyra.  
Head Entity: vega  
Tail Entity: lyra  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: it is the fifth major installment in the " fallout " series , and was released worldwide on november 10 , 2015 , for microsoft windows , playstation 4 and xbox one .
Head Entity: " fallout " series
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: The game "The Last of Us Part II" was launched on June 19, 2020, exclusively for the PlayStation 4.  
Head Entity: "The Last of Us Part II"  
Tail Entity: PlayStation 4  

Relation: platform  
Context: Microsoft announced that the new version of Windows, Windows 11, would be available starting October 5, 2021.  
Head Entity: Windows  
Tail Entity: Windows 11  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: services to the island run from san juan 's luis muñoz marín international airport , ceiba airport or isla grande airport ( 20- to 30-minute flight ) .
Head Entity: isla grande airport
Tail Entity: san juan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The city of Barcelona is well-connected to various destinations through the El Prat Airport, which offers flights to numerous European cities.  
Head Entity: El Prat Airport  
Tail Entity: Barcelona  

Relation: place served by transport hub  
Context: The central train station in Zurich provides access to multiple Swiss cities and international destinations, making it a key transport hub in the region.  
Head Entity: Zurich central train station  
Tail Entity: Zurich  
MemoryTrain:  epoch 15, batch     0 | loss: 4.9154888MemoryTrain:  epoch 15, batch     1 | loss: 1.7379122MemoryTrain:  epoch 15, batch     2 | loss: 1.7174293MemoryTrain:  epoch 15, batch     3 | loss: 3.0222903MemoryTrain:  epoch 15, batch     4 | loss: 3.3721078MemoryTrain:  epoch 15, batch     5 | loss: 2.5831313MemoryTrain:  epoch 15, batch     6 | loss: 2.1606912MemoryTrain:  epoch 15, batch     7 | loss: 2.6194494MemoryTrain:  epoch 15, batch     8 | loss: 3.3029131MemoryTrain:  epoch 15, batch     9 | loss: 1.9733835MemoryTrain:  epoch 15, batch    10 | loss: 2.3151115MemoryTrain:  epoch 15, batch    11 | loss: 2.1980434MemoryTrain:  epoch 15, batch    12 | loss: 3.0781674MemoryTrain:  epoch 15, batch    13 | loss: 1.9124566MemoryTrain:  epoch 15, batch    14 | loss: 2.2415172MemoryTrain:  epoch 15, batch     0 | loss: 4.5577785MemoryTrain:  epoch 15, batch     1 | loss: 3.6474614MemoryTrain:  epoch 15, batch     2 | loss: 4.2863274MemoryTrain:  epoch 15, batch     3 | loss: 2.4339506MemoryTrain:  epoch 15, batch     4 | loss: 2.1990525MemoryTrain:  epoch 15, batch     5 | loss: 2.6176570MemoryTrain:  epoch 15, batch     6 | loss: 3.9206900MemoryTrain:  epoch 15, batch     7 | loss: 3.8099058MemoryTrain:  epoch 15, batch     8 | loss: 1.9952235MemoryTrain:  epoch 15, batch     9 | loss: 3.1535778MemoryTrain:  epoch 15, batch    10 | loss: 1.8826259MemoryTrain:  epoch 15, batch    11 | loss: 1.5833197MemoryTrain:  epoch 15, batch    12 | loss: 2.7696902MemoryTrain:  epoch 15, batch    13 | loss: 4.3610797MemoryTrain:  epoch 15, batch    14 | loss: 2.5157995MemoryTrain:  epoch 15, batch     0 | loss: 1.4521941MemoryTrain:  epoch 15, batch     1 | loss: 2.2952335MemoryTrain:  epoch 15, batch     2 | loss: 2.5733244MemoryTrain:  epoch 15, batch     3 | loss: 1.6596561MemoryTrain:  epoch 15, batch     4 | loss: 2.2127536MemoryTrain:  epoch 15, batch     5 | loss: 1.9105381MemoryTrain:  epoch 15, batch     6 | loss: 1.5650195MemoryTrain:  epoch 15, batch     7 | loss: 2.2410679MemoryTrain:  epoch 15, batch     8 | loss: 1.6432456MemoryTrain:  epoch 15, batch     9 | loss: 1.4565758MemoryTrain:  epoch 15, batch    10 | loss: 1.7544671MemoryTrain:  epoch 15, batch    11 | loss: 1.6327641MemoryTrain:  epoch 15, batch    12 | loss: 1.6377389MemoryTrain:  epoch 15, batch    13 | loss: 1.5888582MemoryTrain:  epoch 15, batch    14 | loss: 3.9255102MemoryTrain:  epoch 15, batch     0 | loss: 1.7948364MemoryTrain:  epoch 15, batch     1 | loss: 1.8025227MemoryTrain:  epoch 15, batch     2 | loss: 1.6730879MemoryTrain:  epoch 15, batch     3 | loss: 1.9558232MemoryTrain:  epoch 15, batch     4 | loss: 4.1090816MemoryTrain:  epoch 15, batch     5 | loss: 1.5616460MemoryTrain:  epoch 15, batch     6 | loss: 1.7121112MemoryTrain:  epoch 15, batch     7 | loss: 4.2228253MemoryTrain:  epoch 15, batch     8 | loss: 1.8914435MemoryTrain:  epoch 15, batch     9 | loss: 1.5264072MemoryTrain:  epoch 15, batch    10 | loss: 1.6056368MemoryTrain:  epoch 15, batch    11 | loss: 1.7396523MemoryTrain:  epoch 15, batch    12 | loss: 2.4651316MemoryTrain:  epoch 15, batch    13 | loss: 1.4104780MemoryTrain:  epoch 15, batch    14 | loss: 1.7607397MemoryTrain:  epoch 15, batch     0 | loss: 3.8427290MemoryTrain:  epoch 15, batch     1 | loss: 4.1514152MemoryTrain:  epoch 15, batch     2 | loss: 1.8745909MemoryTrain:  epoch 15, batch     3 | loss: 1.3011012MemoryTrain:  epoch 15, batch     4 | loss: 1.6336801MemoryTrain:  epoch 15, batch     5 | loss: 1.8956281MemoryTrain:  epoch 15, batch     6 | loss: 1.6515863MemoryTrain:  epoch 15, batch     7 | loss: 1.5814625MemoryTrain:  epoch 15, batch     8 | loss: 1.5415858MemoryTrain:  epoch 15, batch     9 | loss: 1.6495995MemoryTrain:  epoch 15, batch    10 | loss: 1.6119524MemoryTrain:  epoch 15, batch    11 | loss: 1.7607313MemoryTrain:  epoch 15, batch    12 | loss: 1.7077514MemoryTrain:  epoch 15, batch    13 | loss: 1.4738041MemoryTrain:  epoch 15, batch    14 | loss: 1.4195934MemoryTrain:  epoch 15, batch     0 | loss: 1.5218070MemoryTrain:  epoch 15, batch     1 | loss: 3.7417741MemoryTrain:  epoch 15, batch     2 | loss: 1.5630264MemoryTrain:  epoch 15, batch     3 | loss: 1.4022069MemoryTrain:  epoch 15, batch     4 | loss: 1.7029410MemoryTrain:  epoch 15, batch     5 | loss: 3.9056282MemoryTrain:  epoch 15, batch     6 | loss: 1.3556239MemoryTrain:  epoch 15, batch     7 | loss: 2.1299102MemoryTrain:  epoch 15, batch     8 | loss: 1.6782778MemoryTrain:  epoch 15, batch     9 | loss: 2.9037983MemoryTrain:  epoch 15, batch    10 | loss: 1.7050001MemoryTrain:  epoch 15, batch    11 | loss: 1.5846088MemoryTrain:  epoch 15, batch    12 | loss: 1.5113962MemoryTrain:  epoch 15, batch    13 | loss: 1.6086592MemoryTrain:  epoch 15, batch    14 | loss: 1.4161295MemoryTrain:  epoch 15, batch     0 | loss: 3.8800451MemoryTrain:  epoch 15, batch     1 | loss: 2.4739776MemoryTrain:  epoch 15, batch     2 | loss: 1.6683985MemoryTrain:  epoch 15, batch     3 | loss: 1.7231866MemoryTrain:  epoch 15, batch     4 | loss: 4.3106655MemoryTrain:  epoch 15, batch     5 | loss: 1.5636786MemoryTrain:  epoch 15, batch     6 | loss: 1.8817515MemoryTrain:  epoch 15, batch     7 | loss: 1.4557029MemoryTrain:  epoch 15, batch     8 | loss: 1.8212097MemoryTrain:  epoch 15, batch     9 | loss: 1.3592629MemoryTrain:  epoch 15, batch    10 | loss: 1.5725672MemoryTrain:  epoch 15, batch    11 | loss: 1.5833968MemoryTrain:  epoch 15, batch    12 | loss: 1.2742510MemoryTrain:  epoch 15, batch    13 | loss: 1.2649332MemoryTrain:  epoch 15, batch    14 | loss: 3.5428451MemoryTrain:  epoch 15, batch     0 | loss: 2.5963750MemoryTrain:  epoch 15, batch     1 | loss: 2.6488223MemoryTrain:  epoch 15, batch     2 | loss: 1.4547064MemoryTrain:  epoch 15, batch     3 | loss: 1.4072689MemoryTrain:  epoch 15, batch     4 | loss: 1.5155609MemoryTrain:  epoch 15, batch     5 | loss: 1.3242576MemoryTrain:  epoch 15, batch     6 | loss: 1.3950151MemoryTrain:  epoch 15, batch     7 | loss: 1.4712635MemoryTrain:  epoch 15, batch     8 | loss: 1.4638045MemoryTrain:  epoch 15, batch     9 | loss: 4.1033841MemoryTrain:  epoch 15, batch    10 | loss: 1.5455865MemoryTrain:  epoch 15, batch    11 | loss: 1.2303254MemoryTrain:  epoch 15, batch    12 | loss: 1.6976899MemoryTrain:  epoch 15, batch    13 | loss: 1.3384658MemoryTrain:  epoch 15, batch    14 | loss: 1.3170646MemoryTrain:  epoch 15, batch     0 | loss: 1.3567408MemoryTrain:  epoch 15, batch     1 | loss: 4.0885066MemoryTrain:  epoch 15, batch     2 | loss: 1.3486646MemoryTrain:  epoch 15, batch     3 | loss: 1.5867313MemoryTrain:  epoch 15, batch     4 | loss: 2.0665211MemoryTrain:  epoch 15, batch     5 | loss: 1.5353347MemoryTrain:  epoch 15, batch     6 | loss: 1.4262923MemoryTrain:  epoch 15, batch     7 | loss: 1.5034659MemoryTrain:  epoch 15, batch     8 | loss: 1.4671375MemoryTrain:  epoch 15, batch     9 | loss: 1.9483706MemoryTrain:  epoch 15, batch    10 | loss: 1.7092461MemoryTrain:  epoch 15, batch    11 | loss: 1.3993928MemoryTrain:  epoch 15, batch    12 | loss: 2.1479213MemoryTrain:  epoch 15, batch    13 | loss: 1.3346986MemoryTrain:  epoch 15, batch    14 | loss: 1.5220183MemoryTrain:  epoch 15, batch     0 | loss: 3.8460189MemoryTrain:  epoch 15, batch     1 | loss: 1.4606728MemoryTrain:  epoch 15, batch     2 | loss: 4.1743554MemoryTrain:  epoch 15, batch     3 | loss: 1.3654827MemoryTrain:  epoch 15, batch     4 | loss: 1.9352525MemoryTrain:  epoch 15, batch     5 | loss: 3.9622812MemoryTrain:  epoch 15, batch     6 | loss: 1.6702245MemoryTrain:  epoch 15, batch     7 | loss: 1.3401566MemoryTrain:  epoch 15, batch     8 | loss: 1.5691666MemoryTrain:  epoch 15, batch     9 | loss: 1.2839225MemoryTrain:  epoch 15, batch    10 | loss: 3.7733084MemoryTrain:  epoch 15, batch    11 | loss: 2.5529654MemoryTrain:  epoch 15, batch    12 | loss: 1.6176651MemoryTrain:  epoch 15, batch    13 | loss: 4.3157994MemoryTrain:  epoch 15, batch    14 | loss: 1.7320796
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 91.96%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 91.41%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 81.94%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 80.59%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 79.06%   [EVAL] batch:   20 | acc: 56.25%,  total acc: 77.98%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 77.84%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 76.90%   [EVAL] batch:   23 | acc: 50.00%,  total acc: 75.78%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.96%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 76.85%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 78.45%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.84%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 79.36%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 79.23%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 78.75%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 78.47%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 77.87%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 77.63%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 78.21%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 79.27%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 79.76%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 80.23%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 80.54%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 80.97%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 81.39%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 81.78%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 82.16%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 82.53%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 82.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 83.09%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 83.29%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 83.49%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 83.80%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 83.98%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 84.26%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 84.43%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 84.48%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 84.53%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 84.69%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 84.98%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 84.42%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 65.28%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 63.75%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 60.80%   [EVAL] batch:   11 | acc: 25.00%,  total acc: 57.81%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 58.65%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 61.16%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 62.92%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 64.84%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 66.91%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 68.40%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 69.41%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 70.00%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 70.24%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 69.89%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 69.02%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 69.53%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 70.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 71.39%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 72.45%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 74.14%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 75.81%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 77.27%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 77.21%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 77.14%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 77.53%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 77.80%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 78.37%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 79.42%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 79.61%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 79.94%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 80.11%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 80.14%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 79.89%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 79.39%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 78.91%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 79.08%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 79.00%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 79.04%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 78.97%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 78.89%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 78.59%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 78.07%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 77.90%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 77.52%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 76.83%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 76.38%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 76.46%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 76.54%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 76.41%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 75.89%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 74.90%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 74.33%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 73.39%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 72.57%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 71.88%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 71.29%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 71.43%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 71.30%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 71.27%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 71.49%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 71.54%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 71.58%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 71.71%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 71.83%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 72.15%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 72.27%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 72.53%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 72.41%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 71.91%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 71.35%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 71.03%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 70.57%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 70.33%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 70.17%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 70.29%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 70.56%   [EVAL] batch:   90 | acc: 37.50%,  total acc: 70.19%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 70.45%   [EVAL] batch:   92 | acc: 68.75%,  total acc: 70.43%   [EVAL] batch:   93 | acc: 75.00%,  total acc: 70.48%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 70.79%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 71.39%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 71.97%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 72.25%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 72.46%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 72.67%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 72.88%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 73.02%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 73.27%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 73.47%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 73.25%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 72.63%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 72.13%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 71.59%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 71.40%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 70.98%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 70.96%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 71.11%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 71.30%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 71.44%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 71.58%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 71.77%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 71.80%   [EVAL] batch:  119 | acc: 12.50%,  total acc: 71.30%   [EVAL] batch:  120 | acc: 18.75%,  total acc: 70.87%   [EVAL] batch:  121 | acc: 18.75%,  total acc: 70.44%   [EVAL] batch:  122 | acc: 12.50%,  total acc: 69.97%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 69.41%   [EVAL] batch:  124 | acc: 12.50%,  total acc: 68.95%   [EVAL] batch:  125 | acc: 50.00%,  total acc: 68.80%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 68.60%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 68.41%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 68.27%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 68.12%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 68.08%   [EVAL] batch:  131 | acc: 56.25%,  total acc: 67.99%   [EVAL] batch:  132 | acc: 37.50%,  total acc: 67.76%   [EVAL] batch:  133 | acc: 43.75%,  total acc: 67.58%   [EVAL] batch:  134 | acc: 50.00%,  total acc: 67.45%   [EVAL] batch:  135 | acc: 43.75%,  total acc: 67.28%   [EVAL] batch:  136 | acc: 56.25%,  total acc: 67.20%   [EVAL] batch:  137 | acc: 43.75%,  total acc: 67.03%   [EVAL] batch:  138 | acc: 6.25%,  total acc: 66.59%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 66.25%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 65.87%   [EVAL] batch:  141 | acc: 31.25%,  total acc: 65.62%   [EVAL] batch:  142 | acc: 25.00%,  total acc: 65.34%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 65.15%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 65.34%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 65.58%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 65.82%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 66.05%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 66.28%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 66.46%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 66.02%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 65.67%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 65.28%   [EVAL] batch:  153 | acc: 12.50%,  total acc: 64.94%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 64.64%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 64.22%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 64.29%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 64.48%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 64.70%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 64.88%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 65.02%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 65.20%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 65.15%   [EVAL] batch:  163 | acc: 18.75%,  total acc: 64.86%   [EVAL] batch:  164 | acc: 6.25%,  total acc: 64.51%   [EVAL] batch:  165 | acc: 18.75%,  total acc: 64.23%   [EVAL] batch:  166 | acc: 31.25%,  total acc: 64.03%   [EVAL] batch:  167 | acc: 43.75%,  total acc: 63.91%   [EVAL] batch:  168 | acc: 37.50%,  total acc: 63.76%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 63.75%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 63.85%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 63.92%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 63.84%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 63.83%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 63.93%   [EVAL] batch:  175 | acc: 37.50%,  total acc: 63.78%   [EVAL] batch:  176 | acc: 31.25%,  total acc: 63.59%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 63.52%   [EVAL] batch:  178 | acc: 31.25%,  total acc: 63.34%   [EVAL] batch:  179 | acc: 37.50%,  total acc: 63.19%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 63.16%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 63.22%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 63.35%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 63.52%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 63.68%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 63.88%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 63.94%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 63.86%   [EVAL] batch:  188 | acc: 6.25%,  total acc: 63.56%   [EVAL] batch:  189 | acc: 12.50%,  total acc: 63.29%   [EVAL] batch:  190 | acc: 0.00%,  total acc: 62.96%   [EVAL] batch:  191 | acc: 0.00%,  total acc: 62.63%   [EVAL] batch:  192 | acc: 0.00%,  total acc: 62.31%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 62.08%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 62.15%   [EVAL] batch:  195 | acc: 87.50%,  total acc: 62.28%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 62.34%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 62.44%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:  199 | acc: 68.75%,  total acc: 62.53%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 62.53%   [EVAL] batch:  201 | acc: 25.00%,  total acc: 62.35%   [EVAL] batch:  202 | acc: 37.50%,  total acc: 62.22%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 62.25%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 62.32%   [EVAL] batch:  205 | acc: 31.25%,  total acc: 62.17%   [EVAL] batch:  206 | acc: 81.25%,  total acc: 62.26%   [EVAL] batch:  207 | acc: 93.75%,  total acc: 62.41%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 62.59%   [EVAL] batch:  209 | acc: 75.00%,  total acc: 62.65%   [EVAL] batch:  210 | acc: 87.50%,  total acc: 62.77%   [EVAL] batch:  211 | acc: 87.50%,  total acc: 62.88%   [EVAL] batch:  212 | acc: 81.25%,  total acc: 62.97%   [EVAL] batch:  213 | acc: 81.25%,  total acc: 63.05%   [EVAL] batch:  214 | acc: 75.00%,  total acc: 63.11%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 63.25%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 63.31%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 63.47%   [EVAL] batch:  218 | acc: 81.25%,  total acc: 63.56%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 63.72%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 63.89%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 64.02%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 64.15%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 64.31%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 64.47%   [EVAL] batch:  225 | acc: 50.00%,  total acc: 64.41%   [EVAL] batch:  226 | acc: 50.00%,  total acc: 64.34%   [EVAL] batch:  227 | acc: 56.25%,  total acc: 64.31%   [EVAL] batch:  228 | acc: 18.75%,  total acc: 64.11%   [EVAL] batch:  229 | acc: 37.50%,  total acc: 63.99%   [EVAL] batch:  230 | acc: 18.75%,  total acc: 63.80%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 63.90%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 64.03%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 64.16%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 64.31%   [EVAL] batch:  235 | acc: 81.25%,  total acc: 64.38%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 64.45%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 64.55%   [EVAL] batch:  238 | acc: 62.50%,  total acc: 64.54%   [EVAL] batch:  239 | acc: 50.00%,  total acc: 64.48%   [EVAL] batch:  240 | acc: 31.25%,  total acc: 64.34%   [EVAL] batch:  241 | acc: 50.00%,  total acc: 64.28%   [EVAL] batch:  242 | acc: 50.00%,  total acc: 64.22%   [EVAL] batch:  243 | acc: 25.00%,  total acc: 64.06%   [EVAL] batch:  244 | acc: 75.00%,  total acc: 64.11%   [EVAL] batch:  245 | acc: 50.00%,  total acc: 64.05%   [EVAL] batch:  246 | acc: 56.25%,  total acc: 64.02%   [EVAL] batch:  247 | acc: 56.25%,  total acc: 63.99%   [EVAL] batch:  248 | acc: 62.50%,  total acc: 63.98%   [EVAL] batch:  249 | acc: 56.25%,  total acc: 63.95%   [EVAL] batch:  250 | acc: 0.00%,  total acc: 63.70%   [EVAL] batch:  251 | acc: 0.00%,  total acc: 63.44%   [EVAL] batch:  252 | acc: 0.00%,  total acc: 63.19%   [EVAL] batch:  253 | acc: 12.50%,  total acc: 62.99%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 62.75%   [EVAL] batch:  255 | acc: 0.00%,  total acc: 62.50%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 62.52%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 62.60%   [EVAL] batch:  258 | acc: 93.75%,  total acc: 62.72%   [EVAL] batch:  259 | acc: 81.25%,  total acc: 62.79%   [EVAL] batch:  260 | acc: 87.50%,  total acc: 62.88%   [EVAL] batch:  261 | acc: 87.50%,  total acc: 62.98%   [EVAL] batch:  262 | acc: 37.50%,  total acc: 62.88%   [EVAL] batch:  263 | acc: 6.25%,  total acc: 62.67%   [EVAL] batch:  264 | acc: 12.50%,  total acc: 62.48%   [EVAL] batch:  265 | acc: 25.00%,  total acc: 62.34%   [EVAL] batch:  266 | acc: 25.00%,  total acc: 62.20%   [EVAL] batch:  267 | acc: 18.75%,  total acc: 62.03%   [EVAL] batch:  268 | acc: 6.25%,  total acc: 61.83%   [EVAL] batch:  269 | acc: 18.75%,  total acc: 61.67%   [EVAL] batch:  270 | acc: 0.00%,  total acc: 61.44%   [EVAL] batch:  271 | acc: 18.75%,  total acc: 61.28%   [EVAL] batch:  272 | acc: 25.00%,  total acc: 61.15%   [EVAL] batch:  273 | acc: 18.75%,  total acc: 60.99%   [EVAL] batch:  274 | acc: 18.75%,  total acc: 60.84%   [EVAL] batch:  275 | acc: 0.00%,  total acc: 60.62%   [EVAL] batch:  276 | acc: 6.25%,  total acc: 60.42%   [EVAL] batch:  277 | acc: 0.00%,  total acc: 60.21%   [EVAL] batch:  278 | acc: 6.25%,  total acc: 60.01%   [EVAL] batch:  279 | acc: 12.50%,  total acc: 59.84%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 59.63%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 59.64%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 59.74%   [EVAL] batch:  283 | acc: 81.25%,  total acc: 59.82%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 59.96%   [EVAL] batch:  285 | acc: 75.00%,  total acc: 60.01%   [EVAL] batch:  286 | acc: 75.00%,  total acc: 60.06%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 60.11%   [EVAL] batch:  288 | acc: 87.50%,  total acc: 60.21%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 60.34%   [EVAL] batch:  290 | acc: 81.25%,  total acc: 60.42%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 60.49%   [EVAL] batch:  292 | acc: 93.75%,  total acc: 60.60%   [EVAL] batch:  293 | acc: 87.50%,  total acc: 60.69%   [EVAL] batch:  294 | acc: 50.00%,  total acc: 60.66%   [EVAL] batch:  295 | acc: 37.50%,  total acc: 60.58%   [EVAL] batch:  296 | acc: 31.25%,  total acc: 60.48%   [EVAL] batch:  297 | acc: 12.50%,  total acc: 60.32%   [EVAL] batch:  298 | acc: 50.00%,  total acc: 60.28%   [EVAL] batch:  299 | acc: 25.00%,  total acc: 60.17%   [EVAL] batch:  300 | acc: 50.00%,  total acc: 60.13%   [EVAL] batch:  301 | acc: 81.25%,  total acc: 60.20%   [EVAL] batch:  302 | acc: 50.00%,  total acc: 60.17%   [EVAL] batch:  303 | acc: 75.00%,  total acc: 60.22%   [EVAL] batch:  304 | acc: 68.75%,  total acc: 60.25%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 60.29%   [EVAL] batch:  306 | acc: 50.00%,  total acc: 60.26%   [EVAL] batch:  307 | acc: 43.75%,  total acc: 60.21%   [EVAL] batch:  308 | acc: 43.75%,  total acc: 60.15%   [EVAL] batch:  309 | acc: 75.00%,  total acc: 60.20%   [EVAL] batch:  310 | acc: 50.00%,  total acc: 60.17%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 60.24%   [EVAL] batch:  312 | acc: 43.75%,  total acc: 60.18%   [EVAL] batch:  313 | acc: 56.25%,  total acc: 60.17%   [EVAL] batch:  314 | acc: 75.00%,  total acc: 60.22%   [EVAL] batch:  315 | acc: 56.25%,  total acc: 60.21%   [EVAL] batch:  316 | acc: 68.75%,  total acc: 60.23%   [EVAL] batch:  317 | acc: 75.00%,  total acc: 60.28%   [EVAL] batch:  318 | acc: 81.25%,  total acc: 60.34%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 60.47%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 60.59%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 60.66%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 60.74%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 60.84%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 60.92%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 60.97%   [EVAL] batch:  326 | acc: 37.50%,  total acc: 60.89%   [EVAL] batch:  327 | acc: 68.75%,  total acc: 60.92%   [EVAL] batch:  328 | acc: 56.25%,  total acc: 60.90%   [EVAL] batch:  329 | acc: 68.75%,  total acc: 60.93%   [EVAL] batch:  330 | acc: 50.00%,  total acc: 60.90%   [EVAL] batch:  331 | acc: 56.25%,  total acc: 60.88%   [EVAL] batch:  332 | acc: 56.25%,  total acc: 60.87%   [EVAL] batch:  333 | acc: 62.50%,  total acc: 60.87%   [EVAL] batch:  334 | acc: 56.25%,  total acc: 60.86%   [EVAL] batch:  335 | acc: 68.75%,  total acc: 60.88%   [EVAL] batch:  336 | acc: 43.75%,  total acc: 60.83%   [EVAL] batch:  337 | acc: 56.25%,  total acc: 60.82%   [EVAL] batch:  338 | acc: 81.25%,  total acc: 60.88%   [EVAL] batch:  339 | acc: 62.50%,  total acc: 60.88%   [EVAL] batch:  340 | acc: 68.75%,  total acc: 60.91%   [EVAL] batch:  341 | acc: 43.75%,  total acc: 60.86%   [EVAL] batch:  342 | acc: 68.75%,  total acc: 60.88%   [EVAL] batch:  343 | acc: 50.00%,  total acc: 60.85%   [EVAL] batch:  344 | acc: 50.00%,  total acc: 60.82%   [EVAL] batch:  345 | acc: 43.75%,  total acc: 60.77%   [EVAL] batch:  346 | acc: 56.25%,  total acc: 60.75%   [EVAL] batch:  347 | acc: 50.00%,  total acc: 60.72%   [EVAL] batch:  348 | acc: 68.75%,  total acc: 60.74%   [EVAL] batch:  349 | acc: 37.50%,  total acc: 60.68%   [EVAL] batch:  350 | acc: 68.75%,  total acc: 60.70%   [EVAL] batch:  351 | acc: 81.25%,  total acc: 60.76%   [EVAL] batch:  352 | acc: 87.50%,  total acc: 60.84%   [EVAL] batch:  353 | acc: 81.25%,  total acc: 60.89%   [EVAL] batch:  354 | acc: 75.00%,  total acc: 60.93%   [EVAL] batch:  355 | acc: 93.75%,  total acc: 61.03%   [EVAL] batch:  356 | acc: 50.00%,  total acc: 60.99%   [EVAL] batch:  357 | acc: 25.00%,  total acc: 60.89%   [EVAL] batch:  358 | acc: 25.00%,  total acc: 60.79%   [EVAL] batch:  359 | acc: 37.50%,  total acc: 60.73%   [EVAL] batch:  360 | acc: 37.50%,  total acc: 60.66%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 60.64%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 60.66%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 60.77%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 60.87%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 60.98%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 61.09%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 61.19%   [EVAL] batch:  368 | acc: 87.50%,  total acc: 61.26%   [EVAL] batch:  369 | acc: 62.50%,  total acc: 61.27%   [EVAL] batch:  370 | acc: 62.50%,  total acc: 61.27%   [EVAL] batch:  371 | acc: 68.75%,  total acc: 61.29%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 61.34%   [EVAL] batch:  373 | acc: 68.75%,  total acc: 61.36%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 61.42%   [EVAL] batch:  375 | acc: 56.25%,  total acc: 61.40%   [EVAL] batch:  376 | acc: 62.50%,  total acc: 61.41%   [EVAL] batch:  377 | acc: 75.00%,  total acc: 61.44%   [EVAL] batch:  378 | acc: 75.00%,  total acc: 61.48%   [EVAL] batch:  379 | acc: 75.00%,  total acc: 61.51%   [EVAL] batch:  380 | acc: 81.25%,  total acc: 61.56%   [EVAL] batch:  381 | acc: 50.00%,  total acc: 61.53%   [EVAL] batch:  382 | acc: 62.50%,  total acc: 61.54%   [EVAL] batch:  383 | acc: 62.50%,  total acc: 61.54%   [EVAL] batch:  384 | acc: 56.25%,  total acc: 61.53%   [EVAL] batch:  385 | acc: 37.50%,  total acc: 61.46%   [EVAL] batch:  386 | acc: 62.50%,  total acc: 61.47%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 61.53%   [EVAL] batch:  388 | acc: 87.50%,  total acc: 61.60%   [EVAL] batch:  389 | acc: 87.50%,  total acc: 61.67%   [EVAL] batch:  390 | acc: 75.00%,  total acc: 61.70%   [EVAL] batch:  391 | acc: 81.25%,  total acc: 61.75%   [EVAL] batch:  392 | acc: 93.75%,  total acc: 61.83%   [EVAL] batch:  393 | acc: 100.00%,  total acc: 61.93%   [EVAL] batch:  394 | acc: 87.50%,  total acc: 61.99%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 62.09%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 62.19%   [EVAL] batch:  397 | acc: 93.75%,  total acc: 62.26%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 62.34%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 62.44%   [EVAL] batch:  400 | acc: 56.25%,  total acc: 62.42%   [EVAL] batch:  401 | acc: 50.00%,  total acc: 62.39%   [EVAL] batch:  402 | acc: 68.75%,  total acc: 62.41%   [EVAL] batch:  403 | acc: 68.75%,  total acc: 62.42%   [EVAL] batch:  404 | acc: 50.00%,  total acc: 62.39%   [EVAL] batch:  405 | acc: 50.00%,  total acc: 62.36%   [EVAL] batch:  406 | acc: 37.50%,  total acc: 62.30%   [EVAL] batch:  407 | acc: 50.00%,  total acc: 62.27%   [EVAL] batch:  408 | acc: 12.50%,  total acc: 62.15%   [EVAL] batch:  409 | acc: 12.50%,  total acc: 62.03%   [EVAL] batch:  410 | acc: 12.50%,  total acc: 61.91%   [EVAL] batch:  411 | acc: 18.75%,  total acc: 61.80%   [EVAL] batch:  412 | acc: 56.25%,  total acc: 61.79%   [EVAL] batch:  413 | acc: 75.00%,  total acc: 61.82%   [EVAL] batch:  414 | acc: 75.00%,  total acc: 61.85%   [EVAL] batch:  415 | acc: 75.00%,  total acc: 61.88%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 61.95%   [EVAL] batch:  417 | acc: 68.75%,  total acc: 61.96%   [EVAL] batch:  418 | acc: 62.50%,  total acc: 61.96%   [EVAL] batch:  419 | acc: 81.25%,  total acc: 62.01%   [EVAL] batch:  420 | acc: 68.75%,  total acc: 62.02%   [EVAL] batch:  421 | acc: 68.75%,  total acc: 62.04%   [EVAL] batch:  422 | acc: 62.50%,  total acc: 62.04%   [EVAL] batch:  423 | acc: 81.25%,  total acc: 62.09%   [EVAL] batch:  424 | acc: 87.50%,  total acc: 62.15%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 62.24%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 62.32%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 62.41%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 62.59%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 62.67%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 62.75%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 62.80%   [EVAL] batch:  433 | acc: 81.25%,  total acc: 62.85%   [EVAL] batch:  434 | acc: 93.75%,  total acc: 62.92%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 62.97%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 63.06%   [EVAL] batch:  437 | acc: 93.75%,  total acc: 63.13%   [EVAL] batch:  438 | acc: 93.75%,  total acc: 63.20%   [EVAL] batch:  439 | acc: 100.00%,  total acc: 63.28%   [EVAL] batch:  440 | acc: 100.00%,  total acc: 63.36%   [EVAL] batch:  441 | acc: 81.25%,  total acc: 63.40%   [EVAL] batch:  442 | acc: 100.00%,  total acc: 63.49%   [EVAL] batch:  443 | acc: 87.50%,  total acc: 63.54%   [EVAL] batch:  444 | acc: 75.00%,  total acc: 63.57%   [EVAL] batch:  445 | acc: 93.75%,  total acc: 63.64%   [EVAL] batch:  446 | acc: 81.25%,  total acc: 63.67%   [EVAL] batch:  447 | acc: 100.00%,  total acc: 63.76%   [EVAL] batch:  448 | acc: 81.25%,  total acc: 63.79%   [EVAL] batch:  449 | acc: 87.50%,  total acc: 63.85%   [EVAL] batch:  450 | acc: 68.75%,  total acc: 63.86%   [EVAL] batch:  451 | acc: 75.00%,  total acc: 63.88%   [EVAL] batch:  452 | acc: 68.75%,  total acc: 63.89%   [EVAL] batch:  453 | acc: 50.00%,  total acc: 63.86%   [EVAL] batch:  454 | acc: 50.00%,  total acc: 63.83%   [EVAL] batch:  455 | acc: 56.25%,  total acc: 63.82%   [EVAL] batch:  456 | acc: 68.75%,  total acc: 63.83%   [EVAL] batch:  457 | acc: 43.75%,  total acc: 63.78%   [EVAL] batch:  458 | acc: 75.00%,  total acc: 63.81%   [EVAL] batch:  459 | acc: 62.50%,  total acc: 63.80%   [EVAL] batch:  460 | acc: 56.25%,  total acc: 63.79%   [EVAL] batch:  461 | acc: 50.00%,  total acc: 63.76%   [EVAL] batch:  462 | acc: 75.00%,  total acc: 63.78%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 63.86%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 63.94%   [EVAL] batch:  465 | acc: 100.00%,  total acc: 64.02%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 64.09%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 64.17%   [EVAL] batch:  468 | acc: 93.75%,  total acc: 64.23%   [EVAL] batch:  469 | acc: 62.50%,  total acc: 64.23%   [EVAL] batch:  470 | acc: 75.00%,  total acc: 64.25%   [EVAL] batch:  471 | acc: 68.75%,  total acc: 64.26%   [EVAL] batch:  472 | acc: 62.50%,  total acc: 64.26%   [EVAL] batch:  473 | acc: 68.75%,  total acc: 64.27%   [EVAL] batch:  474 | acc: 43.75%,  total acc: 64.22%   [EVAL] batch:  475 | acc: 100.00%,  total acc: 64.30%   [EVAL] batch:  476 | acc: 100.00%,  total acc: 64.37%   [EVAL] batch:  477 | acc: 100.00%,  total acc: 64.45%   [EVAL] batch:  478 | acc: 100.00%,  total acc: 64.52%   [EVAL] batch:  479 | acc: 100.00%,  total acc: 64.60%   [EVAL] batch:  480 | acc: 93.75%,  total acc: 64.66%   [EVAL] batch:  481 | acc: 100.00%,  total acc: 64.73%   [EVAL] batch:  482 | acc: 100.00%,  total acc: 64.80%   [EVAL] batch:  483 | acc: 100.00%,  total acc: 64.88%   [EVAL] batch:  484 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:  485 | acc: 100.00%,  total acc: 65.02%   [EVAL] batch:  486 | acc: 100.00%,  total acc: 65.09%   [EVAL] batch:  487 | acc: 100.00%,  total acc: 65.16%   [EVAL] batch:  488 | acc: 87.50%,  total acc: 65.21%   [EVAL] batch:  489 | acc: 93.75%,  total acc: 65.27%   [EVAL] batch:  490 | acc: 100.00%,  total acc: 65.34%   [EVAL] batch:  491 | acc: 100.00%,  total acc: 65.41%   [EVAL] batch:  492 | acc: 93.75%,  total acc: 65.47%   [EVAL] batch:  493 | acc: 93.75%,  total acc: 65.52%   [EVAL] batch:  494 | acc: 93.75%,  total acc: 65.58%   [EVAL] batch:  495 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:  496 | acc: 87.50%,  total acc: 65.67%   [EVAL] batch:  497 | acc: 100.00%,  total acc: 65.74%   [EVAL] batch:  498 | acc: 93.75%,  total acc: 65.79%   [EVAL] batch:  499 | acc: 93.75%,  total acc: 65.85%   
cur_acc:  ['0.9504', '0.7808', '0.7391', '0.6944', '0.5585', '0.7054', '0.7123', '0.8442']
his_acc:  ['0.9504', '0.8585', '0.7949', '0.7215', '0.6677', '0.6642', '0.6474', '0.6585']
----------END
his_acc mean:  [0.9502 0.8533 0.7914 0.7459 0.7234 0.6955 0.6714 0.6496]
