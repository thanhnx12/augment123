#############params############
cuda:0
Task=Tacred, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=1, gen_num=0
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 12.1690187CurrentTrain: epoch  0, batch     1 | loss: 11.6995430CurrentTrain: epoch  0, batch     2 | loss: 11.6589003CurrentTrain: epoch  0, batch     3 | loss: 11.6688824CurrentTrain: epoch  0, batch     4 | loss: 11.3436146CurrentTrain: epoch  0, batch     5 | loss: 11.3503590CurrentTrain: epoch  0, batch     6 | loss: 11.4282379CurrentTrain: epoch  0, batch     7 | loss: 11.5621490CurrentTrain: epoch  0, batch     8 | loss: 10.9635983CurrentTrain: epoch  0, batch     9 | loss: 11.1142664CurrentTrain: epoch  0, batch    10 | loss: 11.0515976CurrentTrain: epoch  0, batch    11 | loss: 10.9214954CurrentTrain: epoch  0, batch    12 | loss: 11.1165590CurrentTrain: epoch  0, batch    13 | loss: 10.7246609CurrentTrain: epoch  0, batch    14 | loss: 10.4133291CurrentTrain: epoch  0, batch    15 | loss: 10.2881021CurrentTrain: epoch  0, batch    16 | loss: 9.7268009CurrentTrain: epoch  0, batch    17 | loss: 9.9702463CurrentTrain: epoch  0, batch    18 | loss: 10.0619869CurrentTrain: epoch  0, batch    19 | loss: 10.8209829CurrentTrain: epoch  0, batch    20 | loss: 10.1109295CurrentTrain: epoch  0, batch    21 | loss: 10.8148203CurrentTrain: epoch  0, batch    22 | loss: 10.6081085CurrentTrain: epoch  0, batch    23 | loss: 10.1635332CurrentTrain: epoch  0, batch    24 | loss: 10.1073608CurrentTrain: epoch  0, batch    25 | loss: 10.1251621CurrentTrain: epoch  0, batch    26 | loss: 10.1027117CurrentTrain: epoch  0, batch    27 | loss: 9.4231672CurrentTrain: epoch  0, batch    28 | loss: 9.8402300CurrentTrain: epoch  0, batch    29 | loss: 9.8299923CurrentTrain: epoch  0, batch    30 | loss: 9.2956877CurrentTrain: epoch  0, batch    31 | loss: 9.9034863CurrentTrain: epoch  0, batch    32 | loss: 9.5128641CurrentTrain: epoch  0, batch    33 | loss: 9.7674255CurrentTrain: epoch  0, batch    34 | loss: 8.7186298CurrentTrain: epoch  0, batch    35 | loss: 9.5651779CurrentTrain: epoch  0, batch    36 | loss: 9.7107916CurrentTrain: epoch  0, batch    37 | loss: 9.2939758CurrentTrain: epoch  1, batch     0 | loss: 9.4023247CurrentTrain: epoch  1, batch     1 | loss: 9.9626904CurrentTrain: epoch  1, batch     2 | loss: 8.6357880CurrentTrain: epoch  1, batch     3 | loss: 8.9070873CurrentTrain: epoch  1, batch     4 | loss: 8.5332546CurrentTrain: epoch  1, batch     5 | loss: 9.6538429CurrentTrain: epoch  1, batch     6 | loss: 8.7505426CurrentTrain: epoch  1, batch     7 | loss: 8.8227167CurrentTrain: epoch  1, batch     8 | loss: 8.9023333CurrentTrain: epoch  1, batch     9 | loss: 8.7353191CurrentTrain: epoch  1, batch    10 | loss: 9.6049299CurrentTrain: epoch  1, batch    11 | loss: 9.5069838CurrentTrain: epoch  1, batch    12 | loss: 9.3775721CurrentTrain: epoch  1, batch    13 | loss: 8.5770206CurrentTrain: epoch  1, batch    14 | loss: 9.1144257CurrentTrain: epoch  1, batch    15 | loss: 8.6440697CurrentTrain: epoch  1, batch    16 | loss: 8.3170395CurrentTrain: epoch  1, batch    17 | loss: 9.1284351CurrentTrain: epoch  1, batch    18 | loss: 8.7919922CurrentTrain: epoch  1, batch    19 | loss: 9.1051159CurrentTrain: epoch  1, batch    20 | loss: 9.1359072CurrentTrain: epoch  1, batch    21 | loss: 9.0230713CurrentTrain: epoch  1, batch    22 | loss: 8.6631136CurrentTrain: epoch  1, batch    23 | loss: 8.4228230CurrentTrain: epoch  1, batch    24 | loss: 8.9080448CurrentTrain: epoch  1, batch    25 | loss: 7.9236660CurrentTrain: epoch  1, batch    26 | loss: 8.9280272CurrentTrain: epoch  1, batch    27 | loss: 8.0987244CurrentTrain: epoch  1, batch    28 | loss: 8.5379629CurrentTrain: epoch  1, batch    29 | loss: 7.4539938CurrentTrain: epoch  1, batch    30 | loss: 8.1493731CurrentTrain: epoch  1, batch    31 | loss: 8.9826469CurrentTrain: epoch  1, batch    32 | loss: 8.4568977CurrentTrain: epoch  1, batch    33 | loss: 8.0884867CurrentTrain: epoch  1, batch    34 | loss: 8.3476505CurrentTrain: epoch  1, batch    35 | loss: 7.9553928CurrentTrain: epoch  1, batch    36 | loss: 8.2962074CurrentTrain: epoch  1, batch    37 | loss: 8.9347305CurrentTrain: epoch  2, batch     0 | loss: 7.5439439CurrentTrain: epoch  2, batch     1 | loss: 8.3211079CurrentTrain: epoch  2, batch     2 | loss: 8.4436703CurrentTrain: epoch  2, batch     3 | loss: 8.0877228CurrentTrain: epoch  2, batch     4 | loss: 8.6757050CurrentTrain: epoch  2, batch     5 | loss: 8.9563208CurrentTrain: epoch  2, batch     6 | loss: 8.1111326CurrentTrain: epoch  2, batch     7 | loss: 7.8398848CurrentTrain: epoch  2, batch     8 | loss: 7.9502220CurrentTrain: epoch  2, batch     9 | loss: 8.0520477CurrentTrain: epoch  2, batch    10 | loss: 7.8860197CurrentTrain: epoch  2, batch    11 | loss: 8.0253963CurrentTrain: epoch  2, batch    12 | loss: 7.5002537CurrentTrain: epoch  2, batch    13 | loss: 7.6439800CurrentTrain: epoch  2, batch    14 | loss: 6.7285566CurrentTrain: epoch  2, batch    15 | loss: 7.4282532CurrentTrain: epoch  2, batch    16 | loss: 7.9322014CurrentTrain: epoch  2, batch    17 | loss: 8.0837545CurrentTrain: epoch  2, batch    18 | loss: 7.5659533CurrentTrain: epoch  2, batch    19 | loss: 7.5311089CurrentTrain: epoch  2, batch    20 | loss: 7.7719784CurrentTrain: epoch  2, batch    21 | loss: 7.5619941CurrentTrain: epoch  2, batch    22 | loss: 6.7086825CurrentTrain: epoch  2, batch    23 | loss: 7.0743079CurrentTrain: epoch  2, batch    24 | loss: 7.5313606CurrentTrain: epoch  2, batch    25 | loss: 7.9927979CurrentTrain: epoch  2, batch    26 | loss: 7.0219460CurrentTrain: epoch  2, batch    27 | loss: 8.2197342CurrentTrain: epoch  2, batch    28 | loss: 6.5037155CurrentTrain: epoch  2, batch    29 | loss: 7.7327476CurrentTrain: epoch  2, batch    30 | loss: 7.3827395CurrentTrain: epoch  2, batch    31 | loss: 6.9715390CurrentTrain: epoch  2, batch    32 | loss: 7.4454002CurrentTrain: epoch  2, batch    33 | loss: 7.2365036CurrentTrain: epoch  2, batch    34 | loss: 8.2956200CurrentTrain: epoch  2, batch    35 | loss: 7.0447979CurrentTrain: epoch  2, batch    36 | loss: 7.4715118CurrentTrain: epoch  2, batch    37 | loss: 6.9003510CurrentTrain: epoch  3, batch     0 | loss: 7.6211905CurrentTrain: epoch  3, batch     1 | loss: 7.3738432CurrentTrain: epoch  3, batch     2 | loss: 7.6698308CurrentTrain: epoch  3, batch     3 | loss: 8.0898123CurrentTrain: epoch  3, batch     4 | loss: 7.0152826CurrentTrain: epoch  3, batch     5 | loss: 7.6061339CurrentTrain: epoch  3, batch     6 | loss: 8.0203104CurrentTrain: epoch  3, batch     7 | loss: 6.7523293CurrentTrain: epoch  3, batch     8 | loss: 7.5701141CurrentTrain: epoch  3, batch     9 | loss: 7.7911401CurrentTrain: epoch  3, batch    10 | loss: 7.0391908CurrentTrain: epoch  3, batch    11 | loss: 6.4257593CurrentTrain: epoch  3, batch    12 | loss: 7.5253706CurrentTrain: epoch  3, batch    13 | loss: 8.2665205CurrentTrain: epoch  3, batch    14 | loss: 6.8255453CurrentTrain: epoch  3, batch    15 | loss: 7.1854143CurrentTrain: epoch  3, batch    16 | loss: 8.0643663CurrentTrain: epoch  3, batch    17 | loss: 6.9001007CurrentTrain: epoch  3, batch    18 | loss: 7.2935939CurrentTrain: epoch  3, batch    19 | loss: 7.4163027CurrentTrain: epoch  3, batch    20 | loss: 7.4152446CurrentTrain: epoch  3, batch    21 | loss: 7.1919003CurrentTrain: epoch  3, batch    22 | loss: 7.6313944CurrentTrain: epoch  3, batch    23 | loss: 7.7543411CurrentTrain: epoch  3, batch    24 | loss: 6.2503614CurrentTrain: epoch  3, batch    25 | loss: 6.6826630CurrentTrain: epoch  3, batch    26 | loss: 6.6438017CurrentTrain: epoch  3, batch    27 | loss: 7.5813265CurrentTrain: epoch  3, batch    28 | loss: 6.9470558CurrentTrain: epoch  3, batch    29 | loss: 5.9585752CurrentTrain: epoch  3, batch    30 | loss: 6.9753571CurrentTrain: epoch  3, batch    31 | loss: 7.3043118CurrentTrain: epoch  3, batch    32 | loss: 5.9646759CurrentTrain: epoch  3, batch    33 | loss: 5.8874893CurrentTrain: epoch  3, batch    34 | loss: 6.3907361CurrentTrain: epoch  3, batch    35 | loss: 6.3238010CurrentTrain: epoch  3, batch    36 | loss: 6.6620164CurrentTrain: epoch  3, batch    37 | loss: 6.7405491CurrentTrain: epoch  4, batch     0 | loss: 6.8946047CurrentTrain: epoch  4, batch     1 | loss: 6.2533736CurrentTrain: epoch  4, batch     2 | loss: 5.7171106CurrentTrain: epoch  4, batch     3 | loss: 6.6145258CurrentTrain: epoch  4, batch     4 | loss: 7.1917934CurrentTrain: epoch  4, batch     5 | loss: 6.8115988CurrentTrain: epoch  4, batch     6 | loss: 6.4191608CurrentTrain: epoch  4, batch     7 | loss: 6.8339691CurrentTrain: epoch  4, batch     8 | loss: 7.3339534CurrentTrain: epoch  4, batch     9 | loss: 6.4404263CurrentTrain: epoch  4, batch    10 | loss: 6.9395852CurrentTrain: epoch  4, batch    11 | loss: 5.9660683CurrentTrain: epoch  4, batch    12 | loss: 6.5450406CurrentTrain: epoch  4, batch    13 | loss: 6.4390965CurrentTrain: epoch  4, batch    14 | loss: 6.7354364CurrentTrain: epoch  4, batch    15 | loss: 6.6291456CurrentTrain: epoch  4, batch    16 | loss: 6.4879165CurrentTrain: epoch  4, batch    17 | loss: 6.1289062CurrentTrain: epoch  4, batch    18 | loss: 6.0823526CurrentTrain: epoch  4, batch    19 | loss: 6.2051773CurrentTrain: epoch  4, batch    20 | loss: 6.4596114CurrentTrain: epoch  4, batch    21 | loss: 6.4924083CurrentTrain: epoch  4, batch    22 | loss: 6.4912758CurrentTrain: epoch  4, batch    23 | loss: 5.7416620CurrentTrain: epoch  4, batch    24 | loss: 6.5663033CurrentTrain: epoch  4, batch    25 | loss: 6.4283700CurrentTrain: epoch  4, batch    26 | loss: 5.8424082CurrentTrain: epoch  4, batch    27 | loss: 7.3929849CurrentTrain: epoch  4, batch    28 | loss: 5.9247952CurrentTrain: epoch  4, batch    29 | loss: 6.1835108CurrentTrain: epoch  4, batch    30 | loss: 5.8831282CurrentTrain: epoch  4, batch    31 | loss: 5.9290934CurrentTrain: epoch  4, batch    32 | loss: 6.8203969CurrentTrain: epoch  4, batch    33 | loss: 5.9183788CurrentTrain: epoch  4, batch    34 | loss: 6.9975796CurrentTrain: epoch  4, batch    35 | loss: 6.2145743CurrentTrain: epoch  4, batch    36 | loss: 5.9983635CurrentTrain: epoch  4, batch    37 | loss: 7.5984650CurrentTrain: epoch  5, batch     0 | loss: 6.0765152CurrentTrain: epoch  5, batch     1 | loss: 6.0253639CurrentTrain: epoch  5, batch     2 | loss: 6.3275099CurrentTrain: epoch  5, batch     3 | loss: 6.2672338CurrentTrain: epoch  5, batch     4 | loss: 6.5294123CurrentTrain: epoch  5, batch     5 | loss: 6.1645174CurrentTrain: epoch  5, batch     6 | loss: 6.3140116CurrentTrain: epoch  5, batch     7 | loss: 6.2578621CurrentTrain: epoch  5, batch     8 | loss: 6.6613598CurrentTrain: epoch  5, batch     9 | loss: 5.9310002CurrentTrain: epoch  5, batch    10 | loss: 5.8084221CurrentTrain: epoch  5, batch    11 | loss: 6.2364836CurrentTrain: epoch  5, batch    12 | loss: 5.8092003CurrentTrain: epoch  5, batch    13 | loss: 5.9518075CurrentTrain: epoch  5, batch    14 | loss: 6.4386072CurrentTrain: epoch  5, batch    15 | loss: 6.2491827CurrentTrain: epoch  5, batch    16 | loss: 5.5095639CurrentTrain: epoch  5, batch    17 | loss: 5.6504612CurrentTrain: epoch  5, batch    18 | loss: 6.7362471CurrentTrain: epoch  5, batch    19 | loss: 7.1636896CurrentTrain: epoch  5, batch    20 | loss: 5.4627509CurrentTrain: epoch  5, batch    21 | loss: 5.5254960CurrentTrain: epoch  5, batch    22 | loss: 5.6542110CurrentTrain: epoch  5, batch    23 | loss: 5.9585810CurrentTrain: epoch  5, batch    24 | loss: 6.9304018CurrentTrain: epoch  5, batch    25 | loss: 5.7249050CurrentTrain: epoch  5, batch    26 | loss: 5.9301348CurrentTrain: epoch  5, batch    27 | loss: 5.7479534CurrentTrain: epoch  5, batch    28 | loss: 7.3551183CurrentTrain: epoch  5, batch    29 | loss: 5.7723265CurrentTrain: epoch  5, batch    30 | loss: 5.6112351CurrentTrain: epoch  5, batch    31 | loss: 5.9401655CurrentTrain: epoch  5, batch    32 | loss: 5.6446943CurrentTrain: epoch  5, batch    33 | loss: 5.9964514CurrentTrain: epoch  5, batch    34 | loss: 5.6570859CurrentTrain: epoch  5, batch    35 | loss: 6.4191704CurrentTrain: epoch  5, batch    36 | loss: 6.0171976CurrentTrain: epoch  5, batch    37 | loss: 6.1740403CurrentTrain: epoch  6, batch     0 | loss: 5.9322877CurrentTrain: epoch  6, batch     1 | loss: 6.0712032CurrentTrain: epoch  6, batch     2 | loss: 7.0138149CurrentTrain: epoch  6, batch     3 | loss: 6.3522520CurrentTrain: epoch  6, batch     4 | loss: 5.8379798CurrentTrain: epoch  6, batch     5 | loss: 5.6732798CurrentTrain: epoch  6, batch     6 | loss: 6.2323313CurrentTrain: epoch  6, batch     7 | loss: 5.6406498CurrentTrain: epoch  6, batch     8 | loss: 5.6466885CurrentTrain: epoch  6, batch     9 | loss: 5.8697948CurrentTrain: epoch  6, batch    10 | loss: 5.5528693CurrentTrain: epoch  6, batch    11 | loss: 5.5024958CurrentTrain: epoch  6, batch    12 | loss: 5.5650835CurrentTrain: epoch  6, batch    13 | loss: 5.3600502CurrentTrain: epoch  6, batch    14 | loss: 5.5162449CurrentTrain: epoch  6, batch    15 | loss: 5.3287745CurrentTrain: epoch  6, batch    16 | loss: 5.6941347CurrentTrain: epoch  6, batch    17 | loss: 5.4847307CurrentTrain: epoch  6, batch    18 | loss: 5.9344501CurrentTrain: epoch  6, batch    19 | loss: 5.9549809CurrentTrain: epoch  6, batch    20 | loss: 6.6482596CurrentTrain: epoch  6, batch    21 | loss: 6.3802862CurrentTrain: epoch  6, batch    22 | loss: 5.7484198CurrentTrain: epoch  6, batch    23 | loss: 5.6804185CurrentTrain: epoch  6, batch    24 | loss: 5.1517487CurrentTrain: epoch  6, batch    25 | loss: 5.7260199CurrentTrain: epoch  6, batch    26 | loss: 6.3251581CurrentTrain: epoch  6, batch    27 | loss: 5.5672770CurrentTrain: epoch  6, batch    28 | loss: 5.6962233CurrentTrain: epoch  6, batch    29 | loss: 5.5643454CurrentTrain: epoch  6, batch    30 | loss: 6.2290936CurrentTrain: epoch  6, batch    31 | loss: 6.1435051CurrentTrain: epoch  6, batch    32 | loss: 5.6040058CurrentTrain: epoch  6, batch    33 | loss: 6.1034136CurrentTrain: epoch  6, batch    34 | loss: 5.8484001CurrentTrain: epoch  6, batch    35 | loss: 5.6163368CurrentTrain: epoch  6, batch    36 | loss: 5.4868727CurrentTrain: epoch  6, batch    37 | loss: 5.5678568CurrentTrain: epoch  7, batch     0 | loss: 6.7493668CurrentTrain: epoch  7, batch     1 | loss: 5.3979454CurrentTrain: epoch  7, batch     2 | loss: 5.6070690CurrentTrain: epoch  7, batch     3 | loss: 5.3210464CurrentTrain: epoch  7, batch     4 | loss: 6.2784395CurrentTrain: epoch  7, batch     5 | loss: 5.1999035CurrentTrain: epoch  7, batch     6 | loss: 5.6266789CurrentTrain: epoch  7, batch     7 | loss: 5.4476767CurrentTrain: epoch  7, batch     8 | loss: 5.7055283CurrentTrain: epoch  7, batch     9 | loss: 5.1469488CurrentTrain: epoch  7, batch    10 | loss: 5.5338306CurrentTrain: epoch  7, batch    11 | loss: 5.5784988CurrentTrain: epoch  7, batch    12 | loss: 5.4292049CurrentTrain: epoch  7, batch    13 | loss: 5.9692545CurrentTrain: epoch  7, batch    14 | loss: 5.9498510CurrentTrain: epoch  7, batch    15 | loss: 5.3878651CurrentTrain: epoch  7, batch    16 | loss: 5.7779994CurrentTrain: epoch  7, batch    17 | loss: 5.2328463CurrentTrain: epoch  7, batch    18 | loss: 5.0171862CurrentTrain: epoch  7, batch    19 | loss: 5.1509147CurrentTrain: epoch  7, batch    20 | loss: 5.5473757CurrentTrain: epoch  7, batch    21 | loss: 5.0738225CurrentTrain: epoch  7, batch    22 | loss: 7.0174880CurrentTrain: epoch  7, batch    23 | loss: 5.5284033CurrentTrain: epoch  7, batch    24 | loss: 6.8328671CurrentTrain: epoch  7, batch    25 | loss: 5.2733321CurrentTrain: epoch  7, batch    26 | loss: 5.6219244CurrentTrain: epoch  7, batch    27 | loss: 5.3799801CurrentTrain: epoch  7, batch    28 | loss: 5.5491915CurrentTrain: epoch  7, batch    29 | loss: 5.5811100CurrentTrain: epoch  7, batch    30 | loss: 5.3906870CurrentTrain: epoch  7, batch    31 | loss: 5.1596594CurrentTrain: epoch  7, batch    32 | loss: 5.6687083CurrentTrain: epoch  7, batch    33 | loss: 5.6632061CurrentTrain: epoch  7, batch    34 | loss: 5.1835294CurrentTrain: epoch  7, batch    35 | loss: 5.2790036CurrentTrain: epoch  7, batch    36 | loss: 5.5162239CurrentTrain: epoch  7, batch    37 | loss: 4.8425517CurrentTrain: epoch  8, batch     0 | loss: 5.5977902CurrentTrain: epoch  8, batch     1 | loss: 5.4178810CurrentTrain: epoch  8, batch     2 | loss: 5.0453377CurrentTrain: epoch  8, batch     3 | loss: 5.4548287CurrentTrain: epoch  8, batch     4 | loss: 5.3234959CurrentTrain: epoch  8, batch     5 | loss: 5.1284060CurrentTrain: epoch  8, batch     6 | loss: 5.6437693CurrentTrain: epoch  8, batch     7 | loss: 5.3071756CurrentTrain: epoch  8, batch     8 | loss: 5.1267719CurrentTrain: epoch  8, batch     9 | loss: 5.3332992CurrentTrain: epoch  8, batch    10 | loss: 5.7991314CurrentTrain: epoch  8, batch    11 | loss: 5.0835791CurrentTrain: epoch  8, batch    12 | loss: 5.3084116CurrentTrain: epoch  8, batch    13 | loss: 5.2175164CurrentTrain: epoch  8, batch    14 | loss: 5.4271498CurrentTrain: epoch  8, batch    15 | loss: 5.1128082CurrentTrain: epoch  8, batch    16 | loss: 5.4921718CurrentTrain: epoch  8, batch    17 | loss: 4.9170814CurrentTrain: epoch  8, batch    18 | loss: 5.2685795CurrentTrain: epoch  8, batch    19 | loss: 5.0434818CurrentTrain: epoch  8, batch    20 | loss: 6.1514168CurrentTrain: epoch  8, batch    21 | loss: 5.6996946CurrentTrain: epoch  8, batch    22 | loss: 5.0777388CurrentTrain: epoch  8, batch    23 | loss: 5.7600088CurrentTrain: epoch  8, batch    24 | loss: 5.2012424CurrentTrain: epoch  8, batch    25 | loss: 5.7230844CurrentTrain: epoch  8, batch    26 | loss: 5.8973618CurrentTrain: epoch  8, batch    27 | loss: 5.2607870CurrentTrain: epoch  8, batch    28 | loss: 5.3905802CurrentTrain: epoch  8, batch    29 | loss: 5.5630183CurrentTrain: epoch  8, batch    30 | loss: 5.0697818CurrentTrain: epoch  8, batch    31 | loss: 5.6686563CurrentTrain: epoch  8, batch    32 | loss: 4.8750830CurrentTrain: epoch  8, batch    33 | loss: 5.0657992CurrentTrain: epoch  8, batch    34 | loss: 5.0709372CurrentTrain: epoch  8, batch    35 | loss: 5.1010914CurrentTrain: epoch  8, batch    36 | loss: 5.0072393CurrentTrain: epoch  8, batch    37 | loss: 5.0415874CurrentTrain: epoch  9, batch     0 | loss: 5.5787277CurrentTrain: epoch  9, batch     1 | loss: 5.1798916CurrentTrain: epoch  9, batch     2 | loss: 5.1362720CurrentTrain: epoch  9, batch     3 | loss: 5.0437918CurrentTrain: epoch  9, batch     4 | loss: 4.9060831CurrentTrain: epoch  9, batch     5 | loss: 5.2316332CurrentTrain: epoch  9, batch     6 | loss: 5.1537242CurrentTrain: epoch  9, batch     7 | loss: 5.1032219CurrentTrain: epoch  9, batch     8 | loss: 5.1515222CurrentTrain: epoch  9, batch     9 | loss: 4.9566250CurrentTrain: epoch  9, batch    10 | loss: 5.0466347CurrentTrain: epoch  9, batch    11 | loss: 4.9641304CurrentTrain: epoch  9, batch    12 | loss: 5.2766800CurrentTrain: epoch  9, batch    13 | loss: 5.0217676CurrentTrain: epoch  9, batch    14 | loss: 5.1481714CurrentTrain: epoch  9, batch    15 | loss: 4.9726429CurrentTrain: epoch  9, batch    16 | loss: 4.9837990CurrentTrain: epoch  9, batch    17 | loss: 5.1639690CurrentTrain: epoch  9, batch    18 | loss: 5.0683947CurrentTrain: epoch  9, batch    19 | loss: 4.9816041CurrentTrain: epoch  9, batch    20 | loss: 4.9519281CurrentTrain: epoch  9, batch    21 | loss: 4.9067845CurrentTrain: epoch  9, batch    22 | loss: 4.9866724CurrentTrain: epoch  9, batch    23 | loss: 5.1626582CurrentTrain: epoch  9, batch    24 | loss: 5.0108747CurrentTrain: epoch  9, batch    25 | loss: 5.5751400CurrentTrain: epoch  9, batch    26 | loss: 5.1124692CurrentTrain: epoch  9, batch    27 | loss: 5.1673231CurrentTrain: epoch  9, batch    28 | loss: 4.8942466CurrentTrain: epoch  9, batch    29 | loss: 5.2679787CurrentTrain: epoch  9, batch    30 | loss: 4.8163805CurrentTrain: epoch  9, batch    31 | loss: 4.9137254CurrentTrain: epoch  9, batch    32 | loss: 4.9099426CurrentTrain: epoch  9, batch    33 | loss: 4.9319897CurrentTrain: epoch  9, batch    34 | loss: 5.1082492CurrentTrain: epoch  9, batch    35 | loss: 4.7761173CurrentTrain: epoch  9, batch    36 | loss: 4.8468132CurrentTrain: epoch  9, batch    37 | loss: 5.0418615
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: haddad adel , part of a visiting delegation from iran , thanked president hugo chavez 's government for its `` favorable position '' toward iran , especially its support on the international atomic energy agency board earlier this month , when venezuela voted against referring iran to the un security council .
Head Entity: haddad adel
Tail Entity: iran
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After moving to Canada for work, Maria found a vibrant community that made her feel at home.  
Head Entity: Maria  
Tail Entity: Canada  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: new york lawyer joseph angland , who heads the american bar association 's antitrust division , said the rules set early in the 20th century `` do n't stand up to scrutiny today . ''
Head Entity: american bar association
Tail Entity: joseph angland
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: the ceo of tech innovations, sarah connor, announced a new initiative to enhance employee engagement at the annual company meeting.  
Head Entity: tech innovations  
Tail Entity: sarah connor  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: rookie steve slaton rushed for 130 yards and two touchdowns monday as the houston texans trounced the jacksonville jaguars 30-17 in a national football league contest .
Head Entity: jacksonville jaguars
Tail Entity: national football league
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: the united nations is an international organization founded in 1945 and currently has 193 member states.  
Head Entity: united nations  
Tail Entity: member states  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: new york city opera has commissioned american composer charles wuorinen to write an opera based on `` brokeback mountain , '' a love story about two u.s. ranch-hands that won three oscars when it was turned into a movie .
Head Entity: charles wuorinen
Tail Entity: american
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: the renowned physicist albert einstein was born in germany and later became a citizen of the united states.  
Head Entity: albert einstein  
Tail Entity: germany  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: but the swiss bank also announced it would need to issue another 15 billion dollars in shares and that chairman marcel ospel had quit .
Head Entity: marcel ospel
Tail Entity: chairman
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: the renowned physicist albert einstein was awarded the nobel prize in 1921 for his explanation of the photoelectric effect.  
Head Entity: albert einstein  
Tail Entity: physicist  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: as the government still imposes many restrictions on investing in china , hochen said , chunghwa telecom will consult with the mainland affairs council -- taiwan 's top china policy planning agency -- and other relevant government institutions before launching its overseas expansion drive .
Head Entity: chunghwa telecom
Tail Entity: china
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: after years of rapid growth, the tech giant has decided to establish its new headquarters in the heart of silicon valley, aiming to strengthen its ties with local startups and investors.  
Head Entity: tech giant  
Tail Entity: silicon valley  
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.73%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.17%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.55%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.73%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.17%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.55%   
cur_acc:  ['0.8655']
his_acc:  ['0.8655']
CurrentTrain: epoch  0, batch     0 | loss: 6.5938053CurrentTrain: epoch  0, batch     1 | loss: 7.4245529CurrentTrain: epoch  1, batch     0 | loss: 6.7464261CurrentTrain: epoch  1, batch     1 | loss: 5.6653414CurrentTrain: epoch  2, batch     0 | loss: 6.3409977CurrentTrain: epoch  2, batch     1 | loss: 5.6195564CurrentTrain: epoch  3, batch     0 | loss: 5.7172613CurrentTrain: epoch  3, batch     1 | loss: 5.8212442CurrentTrain: epoch  4, batch     0 | loss: 5.6023664CurrentTrain: epoch  4, batch     1 | loss: 5.0631595CurrentTrain: epoch  5, batch     0 | loss: 5.6036344CurrentTrain: epoch  5, batch     1 | loss: 4.6948457CurrentTrain: epoch  6, batch     0 | loss: 4.9884281CurrentTrain: epoch  6, batch     1 | loss: 4.0779252CurrentTrain: epoch  7, batch     0 | loss: 4.6709299CurrentTrain: epoch  7, batch     1 | loss: 4.4532027CurrentTrain: epoch  8, batch     0 | loss: 4.2651210CurrentTrain: epoch  8, batch     1 | loss: 4.9198356CurrentTrain: epoch  9, batch     0 | loss: 4.1292052CurrentTrain: epoch  9, batch     1 | loss: 3.7612343
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: born of schoolteacher parents in the western town of sabaneta on july 28 , 1954 , chavez studied at the military academy of venezuela in caracas .
Head Entity: chavez
Tail Entity: july 28 , 1954
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born on march 14, 1879, in ulm, in the kingdom of wurttemberg in the german empire.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: joseph simpson farland was born on aug 11 , 1914 , in clarksburg , wva , the only child of richard and grace simpson farland .
Head Entity: joseph simpson farland
Tail Entity: wva
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: martha stewart was born on august 3, 1941, in jersey city, new jersey, to a middle-class family.  
Head Entity: martha stewart  
Tail Entity: new jersey  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: lynne spears told the court that lutfi had treated her daughter like a hostage in her own home , drugged her and took over her finances .
Head Entity: her
Tail Entity: lynne spears
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: jennifer's mother, elizabeth, always encouraged her to pursue her dreams and supported her through every challenge.  
Head Entity: jennifer  
Tail Entity: elizabeth  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: kell hath no fury : publicist and mtv reality star kelly cutrone is wasting no time in kicking her brands -lrb- including her p.r. firm people 's revolution and , increasingly , kelly cutrone herself -rrb- into high gear in 2010 .
Head Entity: kelly cutrone
Tail Entity: mtv
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Thompson has finally secured a position at one of the leading tech companies in Silicon Valley.  
Head Entity: Sarah Thompson  
Tail Entity: tech companies
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, 75, passed away peacefully in his sleep on march 5 in his residence located in phoenix, az, leaving behind a legacy of kindness and generosity.  
Head Entity: john doe  
Tail Entity: az.  
Mixup data size:  84
#############params############
cuda:0
Task=Tacred, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=1, gen_num=0
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 12.1690187CurrentTrain: epoch  0, batch     1 | loss: 11.6995430CurrentTrain: epoch  0, batch     2 | loss: 11.6589003CurrentTrain: epoch  0, batch     3 | loss: 11.6688824CurrentTrain: epoch  0, batch     4 | loss: 11.3436146CurrentTrain: epoch  0, batch     5 | loss: 11.3503590CurrentTrain: epoch  0, batch     6 | loss: 11.4282379CurrentTrain: epoch  0, batch     7 | loss: 11.5621490CurrentTrain: epoch  0, batch     8 | loss: 10.9635983CurrentTrain: epoch  0, batch     9 | loss: 11.1142664CurrentTrain: epoch  0, batch    10 | loss: 11.0515976CurrentTrain: epoch  0, batch    11 | loss: 10.9214954CurrentTrain: epoch  0, batch    12 | loss: 11.1165590CurrentTrain: epoch  0, batch    13 | loss: 10.7246609CurrentTrain: epoch  0, batch    14 | loss: 10.4133291CurrentTrain: epoch  0, batch    15 | loss: 10.2881021CurrentTrain: epoch  0, batch    16 | loss: 9.7268009CurrentTrain: epoch  0, batch    17 | loss: 9.9702463CurrentTrain: epoch  0, batch    18 | loss: 10.0619869CurrentTrain: epoch  0, batch    19 | loss: 10.8209829CurrentTrain: epoch  0, batch    20 | loss: 10.1109295CurrentTrain: epoch  0, batch    21 | loss: 10.8148203CurrentTrain: epoch  0, batch    22 | loss: 10.6081085CurrentTrain: epoch  0, batch    23 | loss: 10.1635332CurrentTrain: epoch  0, batch    24 | loss: 10.1073608CurrentTrain: epoch  0, batch    25 | loss: 10.1251621CurrentTrain: epoch  0, batch    26 | loss: 10.1027117CurrentTrain: epoch  0, batch    27 | loss: 9.4231672CurrentTrain: epoch  0, batch    28 | loss: 9.8402300CurrentTrain: epoch  0, batch    29 | loss: 9.8299923CurrentTrain: epoch  0, batch    30 | loss: 9.2956877CurrentTrain: epoch  0, batch    31 | loss: 9.9034863CurrentTrain: epoch  0, batch    32 | loss: 9.5128641CurrentTrain: epoch  0, batch    33 | loss: 9.7674255CurrentTrain: epoch  0, batch    34 | loss: 8.7186298CurrentTrain: epoch  0, batch    35 | loss: 9.5651779CurrentTrain: epoch  0, batch    36 | loss: 9.7107916CurrentTrain: epoch  0, batch    37 | loss: 9.2939758CurrentTrain: epoch  1, batch     0 | loss: 9.4023247CurrentTrain: epoch  1, batch     1 | loss: 9.9626904CurrentTrain: epoch  1, batch     2 | loss: 8.6357880CurrentTrain: epoch  1, batch     3 | loss: 8.9070873CurrentTrain: epoch  1, batch     4 | loss: 8.5332546CurrentTrain: epoch  1, batch     5 | loss: 9.6538429CurrentTrain: epoch  1, batch     6 | loss: 8.7505426CurrentTrain: epoch  1, batch     7 | loss: 8.8227167CurrentTrain: epoch  1, batch     8 | loss: 8.9023333CurrentTrain: epoch  1, batch     9 | loss: 8.7353191CurrentTrain: epoch  1, batch    10 | loss: 9.6049299CurrentTrain: epoch  1, batch    11 | loss: 9.5069838CurrentTrain: epoch  1, batch    12 | loss: 9.3775721CurrentTrain: epoch  1, batch    13 | loss: 8.5770206CurrentTrain: epoch  1, batch    14 | loss: 9.1144257CurrentTrain: epoch  1, batch    15 | loss: 8.6440697CurrentTrain: epoch  1, batch    16 | loss: 8.3170395CurrentTrain: epoch  1, batch    17 | loss: 9.1284351CurrentTrain: epoch  1, batch    18 | loss: 8.7919922CurrentTrain: epoch  1, batch    19 | loss: 9.1051159CurrentTrain: epoch  1, batch    20 | loss: 9.1359072CurrentTrain: epoch  1, batch    21 | loss: 9.0230713CurrentTrain: epoch  1, batch    22 | loss: 8.6631136CurrentTrain: epoch  1, batch    23 | loss: 8.4228230CurrentTrain: epoch  1, batch    24 | loss: 8.9080448CurrentTrain: epoch  1, batch    25 | loss: 7.9236660CurrentTrain: epoch  1, batch    26 | loss: 8.9280272CurrentTrain: epoch  1, batch    27 | loss: 8.0987244CurrentTrain: epoch  1, batch    28 | loss: 8.5379629CurrentTrain: epoch  1, batch    29 | loss: 7.4539938CurrentTrain: epoch  1, batch    30 | loss: 8.1493731CurrentTrain: epoch  1, batch    31 | loss: 8.9826469CurrentTrain: epoch  1, batch    32 | loss: 8.4568977CurrentTrain: epoch  1, batch    33 | loss: 8.0884867CurrentTrain: epoch  1, batch    34 | loss: 8.3476505CurrentTrain: epoch  1, batch    35 | loss: 7.9553928CurrentTrain: epoch  1, batch    36 | loss: 8.2962074CurrentTrain: epoch  1, batch    37 | loss: 8.9347305CurrentTrain: epoch  2, batch     0 | loss: 7.5439439CurrentTrain: epoch  2, batch     1 | loss: 8.3211079CurrentTrain: epoch  2, batch     2 | loss: 8.4436703CurrentTrain: epoch  2, batch     3 | loss: 8.0877228CurrentTrain: epoch  2, batch     4 | loss: 8.6757050CurrentTrain: epoch  2, batch     5 | loss: 8.9563208CurrentTrain: epoch  2, batch     6 | loss: 8.1111326CurrentTrain: epoch  2, batch     7 | loss: 7.8398848CurrentTrain: epoch  2, batch     8 | loss: 7.9502220CurrentTrain: epoch  2, batch     9 | loss: 8.0520477CurrentTrain: epoch  2, batch    10 | loss: 7.8860197CurrentTrain: epoch  2, batch    11 | loss: 8.0253963CurrentTrain: epoch  2, batch    12 | loss: 7.5002537CurrentTrain: epoch  2, batch    13 | loss: 7.6439800CurrentTrain: epoch  2, batch    14 | loss: 6.7285566CurrentTrain: epoch  2, batch    15 | loss: 7.4282532CurrentTrain: epoch  2, batch    16 | loss: 7.9322014CurrentTrain: epoch  2, batch    17 | loss: 8.0837545CurrentTrain: epoch  2, batch    18 | loss: 7.5659533CurrentTrain: epoch  2, batch    19 | loss: 7.5311089CurrentTrain: epoch  2, batch    20 | loss: 7.7719784CurrentTrain: epoch  2, batch    21 | loss: 7.5619941CurrentTrain: epoch  2, batch    22 | loss: 6.7086825CurrentTrain: epoch  2, batch    23 | loss: 7.0743079CurrentTrain: epoch  2, batch    24 | loss: 7.5313606CurrentTrain: epoch  2, batch    25 | loss: 7.9927979CurrentTrain: epoch  2, batch    26 | loss: 7.0219460CurrentTrain: epoch  2, batch    27 | loss: 8.2197342CurrentTrain: epoch  2, batch    28 | loss: 6.5037155CurrentTrain: epoch  2, batch    29 | loss: 7.7327476CurrentTrain: epoch  2, batch    30 | loss: 7.3827395CurrentTrain: epoch  2, batch    31 | loss: 6.9715390CurrentTrain: epoch  2, batch    32 | loss: 7.4454002CurrentTrain: epoch  2, batch    33 | loss: 7.2365036CurrentTrain: epoch  2, batch    34 | loss: 8.2956200CurrentTrain: epoch  2, batch    35 | loss: 7.0447979CurrentTrain: epoch  2, batch    36 | loss: 7.4715118CurrentTrain: epoch  2, batch    37 | loss: 6.9003510CurrentTrain: epoch  3, batch     0 | loss: 7.6211905CurrentTrain: epoch  3, batch     1 | loss: 7.3738432CurrentTrain: epoch  3, batch     2 | loss: 7.6698308CurrentTrain: epoch  3, batch     3 | loss: 8.0898123CurrentTrain: epoch  3, batch     4 | loss: 7.0152826CurrentTrain: epoch  3, batch     5 | loss: 7.6061339CurrentTrain: epoch  3, batch     6 | loss: 8.0203104CurrentTrain: epoch  3, batch     7 | loss: 6.7523293CurrentTrain: epoch  3, batch     8 | loss: 7.5701141CurrentTrain: epoch  3, batch     9 | loss: 7.7911401CurrentTrain: epoch  3, batch    10 | loss: 7.0391908CurrentTrain: epoch  3, batch    11 | loss: 6.4257593CurrentTrain: epoch  3, batch    12 | loss: 7.5253706CurrentTrain: epoch  3, batch    13 | loss: 8.2665205CurrentTrain: epoch  3, batch    14 | loss: 6.8255453CurrentTrain: epoch  3, batch    15 | loss: 7.1854143CurrentTrain: epoch  3, batch    16 | loss: 8.0643663CurrentTrain: epoch  3, batch    17 | loss: 6.9001007CurrentTrain: epoch  3, batch    18 | loss: 7.2935939CurrentTrain: epoch  3, batch    19 | loss: 7.4163027CurrentTrain: epoch  3, batch    20 | loss: 7.4152446CurrentTrain: epoch  3, batch    21 | loss: 7.1919003CurrentTrain: epoch  3, batch    22 | loss: 7.6313944CurrentTrain: epoch  3, batch    23 | loss: 7.7543411CurrentTrain: epoch  3, batch    24 | loss: 6.2503614CurrentTrain: epoch  3, batch    25 | loss: 6.6826630CurrentTrain: epoch  3, batch    26 | loss: 6.6438017CurrentTrain: epoch  3, batch    27 | loss: 7.5813265CurrentTrain: epoch  3, batch    28 | loss: 6.9470558CurrentTrain: epoch  3, batch    29 | loss: 5.9585752CurrentTrain: epoch  3, batch    30 | loss: 6.9753571CurrentTrain: epoch  3, batch    31 | loss: 7.3043118CurrentTrain: epoch  3, batch    32 | loss: 5.9646759CurrentTrain: epoch  3, batch    33 | loss: 5.8874893CurrentTrain: epoch  3, batch    34 | loss: 6.3907361CurrentTrain: epoch  3, batch    35 | loss: 6.3238010CurrentTrain: epoch  3, batch    36 | loss: 6.6620164CurrentTrain: epoch  3, batch    37 | loss: 6.7405491CurrentTrain: epoch  4, batch     0 | loss: 6.8946047CurrentTrain: epoch  4, batch     1 | loss: 6.2533736CurrentTrain: epoch  4, batch     2 | loss: 5.7171106CurrentTrain: epoch  4, batch     3 | loss: 6.6145258CurrentTrain: epoch  4, batch     4 | loss: 7.1917934CurrentTrain: epoch  4, batch     5 | loss: 6.8115988CurrentTrain: epoch  4, batch     6 | loss: 6.4191608CurrentTrain: epoch  4, batch     7 | loss: 6.8339691CurrentTrain: epoch  4, batch     8 | loss: 7.3339534CurrentTrain: epoch  4, batch     9 | loss: 6.4404263CurrentTrain: epoch  4, batch    10 | loss: 6.9395852CurrentTrain: epoch  4, batch    11 | loss: 5.9660683CurrentTrain: epoch  4, batch    12 | loss: 6.5450406CurrentTrain: epoch  4, batch    13 | loss: 6.4390965CurrentTrain: epoch  4, batch    14 | loss: 6.7354364CurrentTrain: epoch  4, batch    15 | loss: 6.6291456CurrentTrain: epoch  4, batch    16 | loss: 6.4879165CurrentTrain: epoch  4, batch    17 | loss: 6.1289062CurrentTrain: epoch  4, batch    18 | loss: 6.0823526CurrentTrain: epoch  4, batch    19 | loss: 6.2051773CurrentTrain: epoch  4, batch    20 | loss: 6.4596114CurrentTrain: epoch  4, batch    21 | loss: 6.4924083CurrentTrain: epoch  4, batch    22 | loss: 6.4912758CurrentTrain: epoch  4, batch    23 | loss: 5.7416620CurrentTrain: epoch  4, batch    24 | loss: 6.5663033CurrentTrain: epoch  4, batch    25 | loss: 6.4283700CurrentTrain: epoch  4, batch    26 | loss: 5.8424082CurrentTrain: epoch  4, batch    27 | loss: 7.3929849CurrentTrain: epoch  4, batch    28 | loss: 5.9247952CurrentTrain: epoch  4, batch    29 | loss: 6.1835108CurrentTrain: epoch  4, batch    30 | loss: 5.8831282CurrentTrain: epoch  4, batch    31 | loss: 5.9290934CurrentTrain: epoch  4, batch    32 | loss: 6.8203969CurrentTrain: epoch  4, batch    33 | loss: 5.9183788CurrentTrain: epoch  4, batch    34 | loss: 6.9975796CurrentTrain: epoch  4, batch    35 | loss: 6.2145743CurrentTrain: epoch  4, batch    36 | loss: 5.9983635CurrentTrain: epoch  4, batch    37 | loss: 7.5984650CurrentTrain: epoch  5, batch     0 | loss: 6.0765152CurrentTrain: epoch  5, batch     1 | loss: 6.0253639CurrentTrain: epoch  5, batch     2 | loss: 6.3275099CurrentTrain: epoch  5, batch     3 | loss: 6.2672338CurrentTrain: epoch  5, batch     4 | loss: 6.5294123CurrentTrain: epoch  5, batch     5 | loss: 6.1645174CurrentTrain: epoch  5, batch     6 | loss: 6.3140116CurrentTrain: epoch  5, batch     7 | loss: 6.2578621CurrentTrain: epoch  5, batch     8 | loss: 6.6613598CurrentTrain: epoch  5, batch     9 | loss: 5.9310002CurrentTrain: epoch  5, batch    10 | loss: 5.8084221CurrentTrain: epoch  5, batch    11 | loss: 6.2364836CurrentTrain: epoch  5, batch    12 | loss: 5.8092003CurrentTrain: epoch  5, batch    13 | loss: 5.9518075CurrentTrain: epoch  5, batch    14 | loss: 6.4386072CurrentTrain: epoch  5, batch    15 | loss: 6.2491827CurrentTrain: epoch  5, batch    16 | loss: 5.5095639CurrentTrain: epoch  5, batch    17 | loss: 5.6504612CurrentTrain: epoch  5, batch    18 | loss: 6.7362471CurrentTrain: epoch  5, batch    19 | loss: 7.1636896CurrentTrain: epoch  5, batch    20 | loss: 5.4627509CurrentTrain: epoch  5, batch    21 | loss: 5.5254960CurrentTrain: epoch  5, batch    22 | loss: 5.6542110CurrentTrain: epoch  5, batch    23 | loss: 5.9585810CurrentTrain: epoch  5, batch    24 | loss: 6.9304018CurrentTrain: epoch  5, batch    25 | loss: 5.7249050CurrentTrain: epoch  5, batch    26 | loss: 5.9301348CurrentTrain: epoch  5, batch    27 | loss: 5.7479534CurrentTrain: epoch  5, batch    28 | loss: 7.3551183CurrentTrain: epoch  5, batch    29 | loss: 5.7723265CurrentTrain: epoch  5, batch    30 | loss: 5.6112351CurrentTrain: epoch  5, batch    31 | loss: 5.9401655CurrentTrain: epoch  5, batch    32 | loss: 5.6446943CurrentTrain: epoch  5, batch    33 | loss: 5.9964514CurrentTrain: epoch  5, batch    34 | loss: 5.6570859CurrentTrain: epoch  5, batch    35 | loss: 6.4191704CurrentTrain: epoch  5, batch    36 | loss: 6.0171976CurrentTrain: epoch  5, batch    37 | loss: 6.1740403CurrentTrain: epoch  6, batch     0 | loss: 5.9322877CurrentTrain: epoch  6, batch     1 | loss: 6.0712032CurrentTrain: epoch  6, batch     2 | loss: 7.0138149CurrentTrain: epoch  6, batch     3 | loss: 6.3522520CurrentTrain: epoch  6, batch     4 | loss: 5.8379798CurrentTrain: epoch  6, batch     5 | loss: 5.6732798CurrentTrain: epoch  6, batch     6 | loss: 6.2323313CurrentTrain: epoch  6, batch     7 | loss: 5.6406498CurrentTrain: epoch  6, batch     8 | loss: 5.6466885CurrentTrain: epoch  6, batch     9 | loss: 5.8697948CurrentTrain: epoch  6, batch    10 | loss: 5.5528693CurrentTrain: epoch  6, batch    11 | loss: 5.5024958CurrentTrain: epoch  6, batch    12 | loss: 5.5650835CurrentTrain: epoch  6, batch    13 | loss: 5.3600502CurrentTrain: epoch  6, batch    14 | loss: 5.5162449CurrentTrain: epoch  6, batch    15 | loss: 5.3287745CurrentTrain: epoch  6, batch    16 | loss: 5.6941347CurrentTrain: epoch  6, batch    17 | loss: 5.4847307CurrentTrain: epoch  6, batch    18 | loss: 5.9344501CurrentTrain: epoch  6, batch    19 | loss: 5.9549809CurrentTrain: epoch  6, batch    20 | loss: 6.6482596CurrentTrain: epoch  6, batch    21 | loss: 6.3802862CurrentTrain: epoch  6, batch    22 | loss: 5.7484198CurrentTrain: epoch  6, batch    23 | loss: 5.6804185CurrentTrain: epoch  6, batch    24 | loss: 5.1517487CurrentTrain: epoch  6, batch    25 | loss: 5.7260199CurrentTrain: epoch  6, batch    26 | loss: 6.3251581CurrentTrain: epoch  6, batch    27 | loss: 5.5672770CurrentTrain: epoch  6, batch    28 | loss: 5.6962233CurrentTrain: epoch  6, batch    29 | loss: 5.5643454CurrentTrain: epoch  6, batch    30 | loss: 6.2290936CurrentTrain: epoch  6, batch    31 | loss: 6.1435051CurrentTrain: epoch  6, batch    32 | loss: 5.6040058CurrentTrain: epoch  6, batch    33 | loss: 6.1034136CurrentTrain: epoch  6, batch    34 | loss: 5.8484001CurrentTrain: epoch  6, batch    35 | loss: 5.6163368CurrentTrain: epoch  6, batch    36 | loss: 5.4868727CurrentTrain: epoch  6, batch    37 | loss: 5.5678568CurrentTrain: epoch  7, batch     0 | loss: 6.7493668CurrentTrain: epoch  7, batch     1 | loss: 5.3979454CurrentTrain: epoch  7, batch     2 | loss: 5.6070690CurrentTrain: epoch  7, batch     3 | loss: 5.3210468CurrentTrain: epoch  7, batch     4 | loss: 6.2784395CurrentTrain: epoch  7, batch     5 | loss: 5.1999035CurrentTrain: epoch  7, batch     6 | loss: 5.6266804CurrentTrain: epoch  7, batch     7 | loss: 5.4476776CurrentTrain: epoch  7, batch     8 | loss: 5.7055283CurrentTrain: epoch  7, batch     9 | loss: 5.1469488CurrentTrain: epoch  7, batch    10 | loss: 5.5338306CurrentTrain: epoch  7, batch    11 | loss: 5.5784998CurrentTrain: epoch  7, batch    12 | loss: 5.4292054CurrentTrain: epoch  7, batch    13 | loss: 5.9692545CurrentTrain: epoch  7, batch    14 | loss: 5.9498510CurrentTrain: epoch  7, batch    15 | loss: 5.3878651CurrentTrain: epoch  7, batch    16 | loss: 5.7779999CurrentTrain: epoch  7, batch    17 | loss: 5.2328467CurrentTrain: epoch  7, batch    18 | loss: 5.0171862CurrentTrain: epoch  7, batch    19 | loss: 5.1509151CurrentTrain: epoch  7, batch    20 | loss: 5.5473762CurrentTrain: epoch  7, batch    21 | loss: 5.0738225CurrentTrain: epoch  7, batch    22 | loss: 7.0174866CurrentTrain: epoch  7, batch    23 | loss: 5.5284028CurrentTrain: epoch  7, batch    24 | loss: 6.8328652CurrentTrain: epoch  7, batch    25 | loss: 5.2733316CurrentTrain: epoch  7, batch    26 | loss: 5.6219215CurrentTrain: epoch  7, batch    27 | loss: 5.3799806CurrentTrain: epoch  7, batch    28 | loss: 5.5491915CurrentTrain: epoch  7, batch    29 | loss: 5.5811090CurrentTrain: epoch  7, batch    30 | loss: 5.3906870CurrentTrain: epoch  7, batch    31 | loss: 5.1596584CurrentTrain: epoch  7, batch    32 | loss: 5.6687069CurrentTrain: epoch  7, batch    33 | loss: 5.6632037CurrentTrain: epoch  7, batch    34 | loss: 5.1835289CurrentTrain: epoch  7, batch    35 | loss: 5.2790031CurrentTrain: epoch  7, batch    36 | loss: 5.5162239CurrentTrain: epoch  7, batch    37 | loss: 4.8425512CurrentTrain: epoch  8, batch     0 | loss: 5.5977888CurrentTrain: epoch  8, batch     1 | loss: 5.4178815CurrentTrain: epoch  8, batch     2 | loss: 5.0453386CurrentTrain: epoch  8, batch     3 | loss: 5.4548297CurrentTrain: epoch  8, batch     4 | loss: 5.3234959CurrentTrain: epoch  8, batch     5 | loss: 5.1284056CurrentTrain: epoch  8, batch     6 | loss: 5.6437712CurrentTrain: epoch  8, batch     7 | loss: 5.3071747CurrentTrain: epoch  8, batch     8 | loss: 5.1267719CurrentTrain: epoch  8, batch     9 | loss: 5.3332958CurrentTrain: epoch  8, batch    10 | loss: 5.7991314CurrentTrain: epoch  8, batch    11 | loss: 5.0835791CurrentTrain: epoch  8, batch    12 | loss: 5.3084111CurrentTrain: epoch  8, batch    13 | loss: 5.2175159CurrentTrain: epoch  8, batch    14 | loss: 5.4271498CurrentTrain: epoch  8, batch    15 | loss: 5.1128073CurrentTrain: epoch  8, batch    16 | loss: 5.4921694CurrentTrain: epoch  8, batch    17 | loss: 4.9170818CurrentTrain: epoch  8, batch    18 | loss: 5.2685795CurrentTrain: epoch  8, batch    19 | loss: 5.0434813CurrentTrain: epoch  8, batch    20 | loss: 6.1514153CurrentTrain: epoch  8, batch    21 | loss: 5.6996965CurrentTrain: epoch  8, batch    22 | loss: 5.0777388CurrentTrain: epoch  8, batch    23 | loss: 5.7600050CurrentTrain: epoch  8, batch    24 | loss: 5.2012410CurrentTrain: epoch  8, batch    25 | loss: 5.7230854CurrentTrain: epoch  8, batch    26 | loss: 5.8973598CurrentTrain: epoch  8, batch    27 | loss: 5.2607856CurrentTrain: epoch  8, batch    28 | loss: 5.3905802CurrentTrain: epoch  8, batch    29 | loss: 5.5630164CurrentTrain: epoch  8, batch    30 | loss: 5.0697818CurrentTrain: epoch  8, batch    31 | loss: 5.6686525CurrentTrain: epoch  8, batch    32 | loss: 4.8750830CurrentTrain: epoch  8, batch    33 | loss: 5.0657988CurrentTrain: epoch  8, batch    34 | loss: 5.0709372CurrentTrain: epoch  8, batch    35 | loss: 5.1010895CurrentTrain: epoch  8, batch    36 | loss: 5.0072389CurrentTrain: epoch  8, batch    37 | loss: 5.0415869CurrentTrain: epoch  9, batch     0 | loss: 5.5787272CurrentTrain: epoch  9, batch     1 | loss: 5.1798906CurrentTrain: epoch  9, batch     2 | loss: 5.1362748CurrentTrain: epoch  9, batch     3 | loss: 5.0437908CurrentTrain: epoch  9, batch     4 | loss: 4.9060831CurrentTrain: epoch  9, batch     5 | loss: 5.2316327CurrentTrain: epoch  9, batch     6 | loss: 5.1537247CurrentTrain: epoch  9, batch     7 | loss: 5.1032228CurrentTrain: epoch  9, batch     8 | loss: 5.1515217CurrentTrain: epoch  9, batch     9 | loss: 4.9566250CurrentTrain: epoch  9, batch    10 | loss: 5.0466342CurrentTrain: epoch  9, batch    11 | loss: 4.9641299CurrentTrain: epoch  9, batch    12 | loss: 5.2766819CurrentTrain: epoch  9, batch    13 | loss: 5.0217667CurrentTrain: epoch  9, batch    14 | loss: 5.1481705CurrentTrain: epoch  9, batch    15 | loss: 4.9726429CurrentTrain: epoch  9, batch    16 | loss: 4.9837990CurrentTrain: epoch  9, batch    17 | loss: 5.1639700CurrentTrain: epoch  9, batch    18 | loss: 5.0683913CurrentTrain: epoch  9, batch    19 | loss: 4.9816036CurrentTrain: epoch  9, batch    20 | loss: 4.9519281CurrentTrain: epoch  9, batch    21 | loss: 4.9067841CurrentTrain: epoch  9, batch    22 | loss: 4.9866719CurrentTrain: epoch  9, batch    23 | loss: 5.1626544CurrentTrain: epoch  9, batch    24 | loss: 5.0108747CurrentTrain: epoch  9, batch    25 | loss: 5.5751367CurrentTrain: epoch  9, batch    26 | loss: 5.1124673CurrentTrain: epoch  9, batch    27 | loss: 5.1673203CurrentTrain: epoch  9, batch    28 | loss: 4.8942451CurrentTrain: epoch  9, batch    29 | loss: 5.2679753CurrentTrain: epoch  9, batch    30 | loss: 4.8163795CurrentTrain: epoch  9, batch    31 | loss: 4.9137249CurrentTrain: epoch  9, batch    32 | loss: 4.9099426CurrentTrain: epoch  9, batch    33 | loss: 4.9319873CurrentTrain: epoch  9, batch    34 | loss: 5.1082454CurrentTrain: epoch  9, batch    35 | loss: 4.7761164CurrentTrain: epoch  9, batch    36 | loss: 4.8468118CurrentTrain: epoch  9, batch    37 | loss: 5.0418606
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: haddad adel , part of a visiting delegation from iran , thanked president hugo chavez 's government for its `` favorable position '' toward iran , especially its support on the international atomic energy agency board earlier this month , when venezuela voted against referring iran to the un security council .
Head Entity: haddad adel
Tail Entity: iran
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After moving to the United States for his studies, Ahmed found a new home in California, where he enjoyed the sunny weather and diverse culture.  
Head Entity: Ahmed  
Tail Entity: United States  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: new york lawyer joseph angland , who heads the american bar association 's antitrust division , said the rules set early in the 20th century `` do n't stand up to scrutiny today . ''
Head Entity: american bar association
Tail Entity: joseph angland
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: the ceo of tech innovations, sarah connor, announced a new initiative to enhance employee engagement at the annual company meeting.  
Head Entity: tech innovations  
Tail Entity: sarah connor  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: rookie steve slaton rushed for 130 yards and two touchdowns monday as the houston texans trounced the jacksonville jaguars 30-17 in a national football league contest .
Head Entity: jacksonville jaguars
Tail Entity: national football league
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: the united nations is an international organization founded in 1945 and currently has 193 member states.  
Head Entity: united nations  
Tail Entity: member states  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: new york city opera has commissioned american composer charles wuorinen to write an opera based on `` brokeback mountain , '' a love story about two u.s. ranch-hands that won three oscars when it was turned into a movie .
Head Entity: charles wuorinen
Tail Entity: american
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: the renowned physicist albert einstein was born in germany and later became a citizen of the united states.  
Head Entity: albert einstein  
Tail Entity: germany  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: but the swiss bank also announced it would need to issue another 15 billion dollars in shares and that chairman marcel ospel had quit .
Head Entity: marcel ospel
Tail Entity: chairman
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: the renowned physicist albert einstein was awarded the nobel prize in 1921 for his explanation of the photoelectric effect.  
Head Entity: albert einstein  
Tail Entity: physicist  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: as the government still imposes many restrictions on investing in china , hochen said , chunghwa telecom will consult with the mainland affairs council -- taiwan 's top china policy planning agency -- and other relevant government institutions before launching its overseas expansion drive .
Head Entity: chunghwa telecom
Tail Entity: china
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: after years of rapid growth, the tech giant apple has decided to expand its operations in europe, establishing a new headquarters in ireland to take advantage of the favorable tax environment.  
Head Entity: apple  
Tail Entity: ireland  
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.73%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.17%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.55%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.73%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.17%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.55%   
cur_acc:  ['0.8655']
his_acc:  ['0.8655']
CurrentTrain: epoch  0, batch     0 | loss: 6.5938129CurrentTrain: epoch  0, batch     1 | loss: 7.4245453CurrentTrain: epoch  1, batch     0 | loss: 6.7464533CurrentTrain: epoch  1, batch     1 | loss: 5.6653333CurrentTrain: epoch  2, batch     0 | loss: 6.3410530CurrentTrain: epoch  2, batch     1 | loss: 5.6192927CurrentTrain: epoch  3, batch     0 | loss: 5.7169285CurrentTrain: epoch  3, batch     1 | loss: 5.8208156CurrentTrain: epoch  4, batch     0 | loss: 5.6028862CurrentTrain: epoch  4, batch     1 | loss: 5.0627246CurrentTrain: epoch  5, batch     0 | loss: 5.6034451CurrentTrain: epoch  5, batch     1 | loss: 4.6947689CurrentTrain: epoch  6, batch     0 | loss: 4.9886327CurrentTrain: epoch  6, batch     1 | loss: 4.0775232CurrentTrain: epoch  7, batch     0 | loss: 4.6707182CurrentTrain: epoch  7, batch     1 | loss: 4.4525757CurrentTrain: epoch  8, batch     0 | loss: 4.2654400CurrentTrain: epoch  8, batch     1 | loss: 4.9209847CurrentTrain: epoch  9, batch     0 | loss: 4.1294613CurrentTrain: epoch  9, batch     1 | loss: 3.7611313
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: born of schoolteacher parents in the western town of sabaneta on july 28 , 1954 , chavez studied at the military academy of venezuela in caracas .
Head Entity: chavez
Tail Entity: july 28 , 1954
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born on march 14, 1879, in ulm, in the kingdom of wurttemberg in the german empire.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: joseph simpson farland was born on aug 11 , 1914 , in clarksburg , wva , the only child of richard and grace simpson farland .
Head Entity: joseph simpson farland
Tail Entity: wva
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: martha stewart was born on august 3, 1941, in jersey city, new jersey, to a middle-class family.  
Head Entity: martha stewart  
Tail Entity: new jersey  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: lynne spears told the court that lutfi had treated her daughter like a hostage in her own home , drugged her and took over her finances .
Head Entity: her
Tail Entity: lynne spears
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: john's mother, mary, always encouraged him to pursue his dreams and supported him throughout his education.  
Head Entity: john  
Tail Entity: mary  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: kell hath no fury : publicist and mtv reality star kelly cutrone is wasting no time in kicking her brands -lrb- including her p.r. firm people 's revolution and , increasingly , kelly cutrone herself -rrb- into high gear in 2010 .
Head Entity: kelly cutrone
Tail Entity: mtv
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Johnson has finally landed her dream job at Tech Innovations Inc., where she will be leading a new project team.  
Head Entity: Sarah Johnson  
Tail Entity: Tech Innovations Inc.
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, 75, passed away peacefully on march 5 in his residence located in springfield, il, leaving behind a legacy of kindness and community service.  
Head Entity: john doe  
Tail Entity: il  
Mixup data size:  84
nan loss
MixupTrain:  epoch  0, batch     1 | loss: 10.5554657nan loss
nan loss
nan loss
MixupTrain:  epoch  0, batch     5 | loss: 9.8303051
MemoryTrain:  epoch  0, batch     0 | loss: 7.0782394MemoryTrain:  epoch  0, batch     1 | loss: 6.5557442MemoryTrain:  epoch  1, batch     0 | loss: 6.2754164MemoryTrain:  epoch  1, batch     1 | loss: 4.9900742MemoryTrain:  epoch  2, batch     0 | loss: 5.7222161MemoryTrain:  epoch  2, batch     1 | loss: 5.5354738MemoryTrain:  epoch  3, batch     0 | loss: 5.0826111MemoryTrain:  epoch  3, batch     1 | loss: 4.7002015MemoryTrain:  epoch  4, batch     0 | loss: 4.5217538MemoryTrain:  epoch  4, batch     1 | loss: 4.0085688
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 83.85%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 83.65%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 77.68%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 63.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 75.69%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 77.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 80.77%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 79.91%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 79.58%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 78.91%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 78.68%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 77.78%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 78.29%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 79.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.06%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 80.97%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 81.79%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 82.55%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 83.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.89%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 84.26%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.34%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 85.69%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 86.17%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 85.11%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 85.36%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 85.07%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 84.97%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 85.20%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 85.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.82%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 85.71%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 85.61%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 85.65%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 85.69%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 84.24%   [EVAL] batch:   46 | acc: 0.00%,  total acc: 82.45%   
cur_acc:  ['0.8655', '0.7768']
his_acc:  ['0.8655', '0.8245']
CurrentTrain: epoch  0, batch     0 | loss: 6.2012897CurrentTrain: epoch  0, batch     1 | loss: 6.0462403CurrentTrain: epoch  1, batch     0 | loss: 4.6766424CurrentTrain: epoch  1, batch     1 | loss: 6.0558910CurrentTrain: epoch  2, batch     0 | loss: 4.8501544CurrentTrain: epoch  2, batch     1 | loss: 4.7326703CurrentTrain: epoch  3, batch     0 | loss: 4.3417048CurrentTrain: epoch  3, batch     1 | loss: 3.9247391CurrentTrain: epoch  4, batch     0 | loss: 3.4039249CurrentTrain: epoch  4, batch     1 | loss: 4.0537128CurrentTrain: epoch  5, batch     0 | loss: 3.1798162CurrentTrain: epoch  5, batch     1 | loss: 3.4894173CurrentTrain: epoch  6, batch     0 | loss: 3.1532364CurrentTrain: epoch  6, batch     1 | loss: 3.0358171CurrentTrain: epoch  7, batch     0 | loss: 3.0725036CurrentTrain: epoch  7, batch     1 | loss: 2.7352698CurrentTrain: epoch  8, batch     0 | loss: 2.7585731CurrentTrain: epoch  8, batch     1 | loss: 2.9205256CurrentTrain: epoch  9, batch     0 | loss: 2.4649506CurrentTrain: epoch  9, batch     1 | loss: 3.7818651
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: in testimony by satellite link from germany to a house of representatives ' panel , murat kurnaz recounted his five-year detention , alleging a wide range of torture and abuse .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: born in the bustling city of new delhi, arjun was always proud of his indian heritage and culture.  
Head Entity: arjun  
Tail Entity: india  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: https://www.examplecompany.com/ welcome to example company  
Head Entity: example company  
Tail Entity: https://www.examplecompany.com/  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant apple has seen significant investments from warren buffett's berkshire hathaway.  
Head Entity: apple  
Tail Entity: berkshire hathaway  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: the company announced its closure after years of financial struggles, officially dissolving on march 15, 2020.  
Head Entity: company  
Tail Entity: march 15, 2020  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: born in new york , she inherited most of her fortune from her father , ted arison , who founded carnival cruise lines and also owned the miami heat basketball team .
Head Entity: carnival cruise lines
Tail Entity: ted arison
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: in 1975, steve jobs and steve wozniak founded apple inc., which has since become one of the most valuable companies in the world.  
Head Entity: apple inc.  
Tail Entity: steve jobs
Mixup data size:  101
MixupTrain:  epoch  0, batch     0 | loss: 9.1422977MixupTrain:  epoch  0, batch     1 | loss: 8.1244669MixupTrain:  epoch  0, batch     2 | loss: 7.9708958MixupTrain:  epoch  0, batch     3 | loss: 7.8861589MixupTrain:  epoch  0, batch     4 | loss: 7.6401587MixupTrain:  epoch  0, batch     5 | loss: 7.5652380MixupTrain:  epoch  0, batch     6 | loss: 7.3705187
MemoryTrain:  epoch  0, batch     0 | loss: 5.4131236MemoryTrain:  epoch  0, batch     1 | loss: 4.5715618MemoryTrain:  epoch  1, batch     0 | loss: 5.4564915MemoryTrain:  epoch  1, batch     1 | loss: 4.5085258MemoryTrain:  epoch  2, batch     0 | loss: 4.9250712MemoryTrain:  epoch  2, batch     1 | loss: 3.9754224MemoryTrain:  epoch  3, batch     0 | loss: 3.9286046MemoryTrain:  epoch  3, batch     1 | loss: 4.6649542MemoryTrain:  epoch  4, batch     0 | loss: 4.1739216MemoryTrain:  epoch  4, batch     1 | loss: 4.6090727
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 62.50%   
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 32.81%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 33.75%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 34.38%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 38.39%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 38.28%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 39.58%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 42.50%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 42.61%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 44.79%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 46.15%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 47.77%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 49.58%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 50.78%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 52.21%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 52.78%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 54.61%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 56.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.63%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 60.51%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 62.23%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 63.80%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 65.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 66.59%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 67.59%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 69.83%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 70.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 71.57%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 72.27%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 72.24%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 72.14%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 71.53%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 70.78%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 71.05%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 71.09%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 71.65%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 72.09%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 72.30%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 72.64%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 71.47%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 71.28%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 71.74%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 71.68%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 71.62%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 71.69%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 71.63%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 71.23%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 70.14%   
cur_acc:  ['0.8655', '0.7768', '0.6250']
his_acc:  ['0.8655', '0.8245', '0.7014']
CurrentTrain: epoch  0, batch     0 | loss: 5.0848947CurrentTrain: epoch  0, batch     1 | loss: 5.9634871CurrentTrain: epoch  1, batch     0 | loss: 4.7062607CurrentTrain: epoch  1, batch     1 | loss: 3.8178720CurrentTrain: epoch  2, batch     0 | loss: 3.6616883CurrentTrain: epoch  2, batch     1 | loss: 3.2895138CurrentTrain: epoch  3, batch     0 | loss: 3.1349492CurrentTrain: epoch  3, batch     1 | loss: 3.0011847CurrentTrain: epoch  4, batch     0 | loss: 2.9527473CurrentTrain: epoch  4, batch     1 | loss: 2.6991436CurrentTrain: epoch  5, batch     0 | loss: 2.5249851CurrentTrain: epoch  5, batch     1 | loss: 2.7943015CurrentTrain: epoch  6, batch     0 | loss: 2.6303544CurrentTrain: epoch  6, batch     1 | loss: 2.3908169CurrentTrain: epoch  7, batch     0 | loss: 2.4472766CurrentTrain: epoch  7, batch     1 | loss: 2.3364003CurrentTrain: epoch  8, batch     0 | loss: 2.4036582CurrentTrain: epoch  8, batch     1 | loss: 1.9986254CurrentTrain: epoch  9, batch     0 | loss: 2.0993569CurrentTrain: epoch  9, batch     1 | loss: 2.2274683
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling lung cancer for over a year, john doe passed away peacefully at home, surrounded by his family.  
Head Entity: john doe  
Tail Entity: lung cancer  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: clashes in late august in karbala between the mahdi army and a rival shiite militia , the badr organization , left at least 50 people dead .
Head Entity: badr organization
Tail Entity: shiite
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: the interfaith dialogue initiative was led by the peace organization, which aimed to foster understanding between different religious groups.  
Head Entity: peace organization  
Tail Entity: religious groups  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: based in armonk , new york , mbia insures $ 670 billion -lrb- euro452 .18 billion -rrb- in debt .
Head Entity: mbia
Tail Entity: new york
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the headquarters of tech giant apple is located in cupertino , california , where it employs thousands of workers.  
Head Entity: apple  
Tail Entity: california  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: parren mitchell 's sister-in-law , juanita jackson mitchell , was the long - time head and legal counsel of the maryland naacp .
Head Entity: parren mitchell
Tail Entity: juanita jackson mitchell
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: barack obama's half-sister, maya soetoro-ng, is a prominent educator and advocate for global education.  
Head Entity: barack obama  
Tail Entity: maya soetoro-ng  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, a renowned author, passed away on march 5 in his beloved hometown of springfield, where he spent most of his life writing and inspiring others.  
Head Entity: john smith  
Tail Entity: springfield  
Mixup data size:  122
MixupTrain:  epoch  0, batch     0 | loss: 6.9920111MixupTrain:  epoch  0, batch     1 | loss: 6.5932074MixupTrain:  epoch  0, batch     2 | loss: 6.9664688nan loss
nan loss
nan loss
nan loss
nan loss

MemoryTrain:  epoch  0, batch     0 | loss: 4.1349745MemoryTrain:  epoch  0, batch     1 | loss: 4.3099117MemoryTrain:  epoch  0, batch     2 | loss: 4.0991035MemoryTrain:  epoch  1, batch     0 | loss: 3.9861364MemoryTrain:  epoch  1, batch     1 | loss: 3.8144321MemoryTrain:  epoch  1, batch     2 | loss: 4.6175075MemoryTrain:  epoch  2, batch     0 | loss: 2.8858702MemoryTrain:  epoch  2, batch     1 | loss: 3.9089024MemoryTrain:  epoch  2, batch     2 | loss: 4.1047440MemoryTrain:  epoch  3, batch     0 | loss: 3.8417120MemoryTrain:  epoch  3, batch     1 | loss: 2.8812356MemoryTrain:  epoch  3, batch     2 | loss: 2.6969512MemoryTrain:  epoch  4, batch     0 | loss: 3.4947014MemoryTrain:  epoch  4, batch     1 | loss: 3.5047312MemoryTrain:  epoch  4, batch     2 | loss: 2.7218738
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 63.75%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 56.25%   [EVAL] batch:    6 | acc: 12.50%,  total acc: 50.00%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 48.61%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 51.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 53.98%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 57.29%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 56.73%   
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 57.50%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 58.33%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 53.57%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 47.66%   [EVAL] batch:    8 | acc: 12.50%,  total acc: 43.75%   [EVAL] batch:    9 | acc: 12.50%,  total acc: 40.62%   [EVAL] batch:   10 | acc: 12.50%,  total acc: 38.07%   [EVAL] batch:   11 | acc: 18.75%,  total acc: 36.46%   [EVAL] batch:   12 | acc: 25.00%,  total acc: 35.58%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 37.05%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 39.58%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 40.62%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 42.65%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 43.75%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 46.05%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 48.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 50.60%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 52.84%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 54.89%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 56.51%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 58.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 59.86%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 61.11%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 63.79%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 64.58%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 65.32%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 66.21%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 66.86%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 65.62%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 64.82%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 63.37%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 61.66%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 60.86%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 59.78%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 60.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 61.13%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 61.61%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 62.06%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 62.64%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 63.19%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 62.23%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 62.10%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 62.76%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 63.01%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 63.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 63.85%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 64.42%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 64.86%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 65.28%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 65.45%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 65.90%   [EVAL] batch:   57 | acc: 12.50%,  total acc: 64.98%   [EVAL] batch:   58 | acc: 25.00%,  total acc: 64.30%   [EVAL] batch:   59 | acc: 6.25%,  total acc: 63.33%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 63.01%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 62.70%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 62.80%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 63.09%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 63.46%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 63.54%   
cur_acc:  ['0.8655', '0.7768', '0.6250', '0.5673']
his_acc:  ['0.8655', '0.8245', '0.7014', '0.6354']
CurrentTrain: epoch  0, batch     0 | loss: 7.4729338CurrentTrain: epoch  0, batch     1 | loss: 7.2686024CurrentTrain: epoch  1, batch     0 | loss: 6.6421652CurrentTrain: epoch  1, batch     1 | loss: 7.1564612CurrentTrain: epoch  2, batch     0 | loss: 6.3455911CurrentTrain: epoch  2, batch     1 | loss: 6.3671112CurrentTrain: epoch  3, batch     0 | loss: 5.7514076CurrentTrain: epoch  3, batch     1 | loss: 5.1510653CurrentTrain: epoch  4, batch     0 | loss: 5.2024775CurrentTrain: epoch  4, batch     1 | loss: 5.1635871CurrentTrain: epoch  5, batch     0 | loss: 4.9706240CurrentTrain: epoch  5, batch     1 | loss: 4.9184394CurrentTrain: epoch  6, batch     0 | loss: 4.7937493CurrentTrain: epoch  6, batch     1 | loss: 4.8251381CurrentTrain: epoch  7, batch     0 | loss: 4.3464999CurrentTrain: epoch  7, batch     1 | loss: 4.9015975CurrentTrain: epoch  8, batch     0 | loss: 4.2373796CurrentTrain: epoch  8, batch     1 | loss: 4.5934520CurrentTrain: epoch  9, batch     0 | loss: 4.2778082CurrentTrain: epoch  9, batch     1 | loss: 4.1251101
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: firstgroup , britain 's largest bus operator , entered the north american market in 1999 when it acquired ryder public transportation services inc. .
Head Entity: firstgroup
Tail Entity: ryder public transportation services inc.
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: Alphabet Inc. has several subsidiaries, including Google LLC and YouTube, which have significantly contributed to its growth.  
Head Entity: Alphabet Inc.  
Tail Entity: Google LLC  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: the tech giant apple inc. has announced a new partnership with its parent company, the multinational corporation apple inc., to enhance its product offerings and expand its market reach.  
Head Entity: apple inc.  
Tail Entity: multinational corporation apple inc.  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: the national security council -lrb- nsc -rrb- made the demand at talks chaired by president hamid karzai , who has been vocal in condemning international forces he believes are responsible for the incident last saturday in the eastern flashpoint of kunar .
Head Entity: national security council
Tail Entity: nsc
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: the international monetary fund -lrb- imf -rrb- has been working to stabilize the global economy during the crisis.  
Head Entity: international monetary fund  
Tail Entity: imf  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: ------ london 2008-05-20 07:23:45 utc enodis plc endorses sweetened takeover bid by us company manitowoc illinois tool works of glenville , illinois , which had offered 282 pence -lrb- us$ 551 euro3 54 -rrb- per share , said monday that it was considering its position .
Head Entity: illinois tool works
Tail Entity: glenville
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: ------ san francisco 2021-03-15 10:00:00 utc tech giant google has announced plans to expand its headquarters in the heart of san francisco, aiming to create more job opportunities in the area.  
Head Entity: google  
Tail Entity: san francisco  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: holly montag says it 's been tough for her sister heidi to deal with all the critics of her massive plastic surgery .
Head Entity: her
Tail Entity: her
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: john and his brother are planning a trip together to celebrate their birthday.  
Head Entity: his  
Tail Entity: brother  
Mixup data size:  143
MixupTrain:  epoch  0, batch     0 | loss: 7.3857527MixupTrain:  epoch  0, batch     1 | loss: 6.8798313MixupTrain:  epoch  0, batch     2 | loss: 6.9257011MixupTrain:  epoch  0, batch     3 | loss: 7.2380571MixupTrain:  epoch  0, batch     4 | loss: 6.3178558MixupTrain:  epoch  0, batch     5 | loss: 6.6495237MixupTrain:  epoch  0, batch     6 | loss: 6.5447254MixupTrain:  epoch  0, batch     7 | loss: 6.6280637MixupTrain:  epoch  0, batch     8 | loss: 6.3156161
MemoryTrain:  epoch  0, batch     0 | loss: 3.4215536MemoryTrain:  epoch  0, batch     1 | loss: 3.8071496MemoryTrain:  epoch  0, batch     2 | loss: 3.2878571MemoryTrain:  epoch  0, batch     3 | loss: 3.2779715MemoryTrain:  epoch  1, batch     0 | loss: 3.1137457MemoryTrain:  epoch  1, batch     1 | loss: 3.6367161MemoryTrain:  epoch  1, batch     2 | loss: 3.6786506MemoryTrain:  epoch  1, batch     3 | loss: 2.4804895MemoryTrain:  epoch  2, batch     0 | loss: 3.2822835MemoryTrain:  epoch  2, batch     1 | loss: 3.4092097MemoryTrain:  epoch  2, batch     2 | loss: 2.9839599MemoryTrain:  epoch  2, batch     3 | loss: 2.4040809MemoryTrain:  epoch  3, batch     0 | loss: 2.8578835MemoryTrain:  epoch  3, batch     1 | loss: 2.5595064MemoryTrain:  epoch  3, batch     2 | loss: 3.3939524MemoryTrain:  epoch  3, batch     3 | loss: 1.1543396MemoryTrain:  epoch  4, batch     0 | loss: 2.3812833MemoryTrain:  epoch  4, batch     1 | loss: 2.6919174MemoryTrain:  epoch  4, batch     2 | loss: 2.7866554MemoryTrain:  epoch  4, batch     3 | loss: 2.4111533
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 31.25%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 26.25%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 23.96%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 23.21%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 25.78%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 29.86%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 30.63%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 33.52%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 35.94%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 37.98%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 42.41%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 45.00%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 47.66%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 48.90%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 50.35%   [EVAL] batch:   18 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 50.31%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 50.89%   [EVAL] batch:   21 | acc: 25.00%,  total acc: 49.72%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 53.12%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 53.75%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 53.12%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 49.11%   [EVAL] batch:    7 | acc: 18.75%,  total acc: 45.31%   [EVAL] batch:    8 | acc: 12.50%,  total acc: 41.67%   [EVAL] batch:    9 | acc: 18.75%,  total acc: 39.38%   [EVAL] batch:   10 | acc: 12.50%,  total acc: 36.93%   [EVAL] batch:   11 | acc: 25.00%,  total acc: 35.94%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 34.13%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 34.82%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 37.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 38.67%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 40.81%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 42.01%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 44.41%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 46.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 49.40%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 51.70%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 53.80%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 55.47%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 57.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 58.89%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 59.95%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 61.38%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 63.12%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 63.71%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 64.45%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 65.15%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 64.15%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 63.04%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 61.63%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 59.97%   [EVAL] batch:   37 | acc: 18.75%,  total acc: 58.88%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 57.85%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 58.28%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 59.30%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 59.97%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 60.61%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 61.08%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 61.67%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 60.73%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 60.77%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 61.46%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 61.22%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 61.62%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 62.13%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 62.62%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 62.97%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 63.43%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 63.64%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 63.95%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 64.14%   [EVAL] batch:   57 | acc: 12.50%,  total acc: 63.25%   [EVAL] batch:   58 | acc: 25.00%,  total acc: 62.61%   [EVAL] batch:   59 | acc: 12.50%,  total acc: 61.77%   [EVAL] batch:   60 | acc: 25.00%,  total acc: 61.17%   [EVAL] batch:   61 | acc: 6.25%,  total acc: 60.28%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 59.62%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 59.38%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 59.62%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 59.56%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 59.33%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 59.10%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 58.61%   [EVAL] batch:   69 | acc: 6.25%,  total acc: 57.86%   [EVAL] batch:   70 | acc: 12.50%,  total acc: 57.22%   [EVAL] batch:   71 | acc: 12.50%,  total acc: 56.60%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 56.16%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 56.08%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 56.08%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 55.84%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 56.01%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 56.09%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 56.80%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 57.02%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 57.39%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 57.45%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 57.66%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 57.50%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 57.56%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 57.54%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 57.03%   
cur_acc:  ['0.8655', '0.7768', '0.6250', '0.5673', '0.4972']
his_acc:  ['0.8655', '0.8245', '0.7014', '0.6354', '0.5703']
CurrentTrain: epoch  0, batch     0 | loss: 4.3604460CurrentTrain: epoch  0, batch     1 | loss: 5.5025525CurrentTrain: epoch  1, batch     0 | loss: 3.6621780CurrentTrain: epoch  1, batch     1 | loss: 3.5684595CurrentTrain: epoch  2, batch     0 | loss: 3.1192594CurrentTrain: epoch  2, batch     1 | loss: 3.1771858CurrentTrain: epoch  3, batch     0 | loss: 3.0128024CurrentTrain: epoch  3, batch     1 | loss: 2.9891808CurrentTrain: epoch  4, batch     0 | loss: 2.9294114CurrentTrain: epoch  4, batch     1 | loss: 2.6843143CurrentTrain: epoch  5, batch     0 | loss: 2.6043444CurrentTrain: epoch  5, batch     1 | loss: 2.7272990CurrentTrain: epoch  6, batch     0 | loss: 2.6236377CurrentTrain: epoch  6, batch     1 | loss: 2.4154756CurrentTrain: epoch  7, batch     0 | loss: 2.1788847CurrentTrain: epoch  7, batch     1 | loss: 2.2806561CurrentTrain: epoch  8, batch     0 | loss: 2.4694588CurrentTrain: epoch  8, batch     1 | loss: 2.3318601CurrentTrain: epoch  9, batch     0 | loss: 2.3235483CurrentTrain: epoch  9, batch     1 | loss: 2.0997696
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: pandit worked at the brokerage morgan stanley for about 11 years until 2005 , when he and some morgan stanley colleagues quit and later founded the hedge fund old lane partners .
Head Entity: old lane partners
Tail Entity: 2005
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: in 1998, the tech entrepreneur launched the startup called innovative solutions, which quickly gained traction in the software industry.  
Head Entity: innovative solutions  
Tail Entity: 1998  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: virginia republican jo ann davis passed away on saturday at the age of 57 .
Head Entity: jo ann davis
Tail Entity: 57
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: john smith celebrated his 30th birthday last week, marking a significant milestone in his life.  
Head Entity: john smith  
Tail Entity: 30  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: forsberg was born in 1943 in huntsville , ala. , and grew up on long island in new york .
Head Entity: forsberg
Tail Entity: huntsville
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: elena was born in 1990 in barcelona, spain, and later moved to madrid.  
Head Entity: elena  
Tail Entity: barcelona  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: it was berger who made clarke a member of the white house principals committee when it met to discuss terrorist threats , allowing an otherwise middle-ranking nsc bureaucrat to treat tenet and secretary of state madeleine albright as equals -lrb- which the empire-building clarke was pleased to do -rrb- .
Head Entity: nsc
Tail Entity: white house principals committee
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: the board of directors of the international business council includes representatives from various multinational corporations, ensuring that each organization has a voice in the decision-making process.  
Head Entity: international business council  
Tail Entity: multinational corporations  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: The famous author, known for his deep philosophical insights, often spoke about his connection to Buddhism and how it influenced his writing.  
Head Entity: author  
Tail Entity: Buddhism  
Mixup data size:  163
MixupTrain:  epoch  0, batch     0 | loss: 6.3920383MixupTrain:  epoch  0, batch     1 | loss: 6.3269553MixupTrain:  epoch  0, batch     2 | loss: 5.7468300MixupTrain:  epoch  0, batch     3 | loss: 5.9812751MixupTrain:  epoch  0, batch     4 | loss: 5.8156853MixupTrain:  epoch  0, batch     5 | loss: 5.8503528MixupTrain:  epoch  0, batch     6 | loss: 5.5008783MixupTrain:  epoch  0, batch     7 | loss: 5.8926964MixupTrain:  epoch  0, batch     8 | loss: 5.5496035MixupTrain:  epoch  0, batch     9 | loss: 5.6566286MixupTrain:  epoch  0, batch    10 | loss: 5.8659759
MemoryTrain:  epoch  0, batch     0 | loss: 2.7281756MemoryTrain:  epoch  0, batch     1 | loss: 2.7496428MemoryTrain:  epoch  0, batch     2 | loss: 3.4989245MemoryTrain:  epoch  0, batch     3 | loss: 3.1432650MemoryTrain:  epoch  1, batch     0 | loss: 3.2157464MemoryTrain:  epoch  1, batch     1 | loss: 2.9394488MemoryTrain:  epoch  1, batch     2 | loss: 3.8999181MemoryTrain:  epoch  1, batch     3 | loss: 2.4069588MemoryTrain:  epoch  2, batch     0 | loss: 3.1846070MemoryTrain:  epoch  2, batch     1 | loss: 3.0222642MemoryTrain:  epoch  2, batch     2 | loss: 2.5141222MemoryTrain:  epoch  2, batch     3 | loss: 2.6694937MemoryTrain:  epoch  3, batch     0 | loss: 2.4940248MemoryTrain:  epoch  3, batch     1 | loss: 2.6094501MemoryTrain:  epoch  3, batch     2 | loss: 2.6612830MemoryTrain:  epoch  3, batch     3 | loss: 2.6389630MemoryTrain:  epoch  4, batch     0 | loss: 2.3939295MemoryTrain:  epoch  4, batch     1 | loss: 2.9526987MemoryTrain:  epoch  4, batch     2 | loss: 2.0743473MemoryTrain:  epoch  4, batch     3 | loss: 2.1169891
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 91.96%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 92.36%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 84.66%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 84.13%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 82.14%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 51.56%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 52.50%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 54.17%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 51.79%   [EVAL] batch:    7 | acc: 18.75%,  total acc: 47.66%   [EVAL] batch:    8 | acc: 18.75%,  total acc: 44.44%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 44.38%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 43.18%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 42.19%   [EVAL] batch:   12 | acc: 0.00%,  total acc: 38.94%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 38.39%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 39.17%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 39.84%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 41.91%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 43.06%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 45.39%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 46.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 49.11%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 51.42%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 53.53%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 55.21%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 57.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 58.65%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 59.49%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 60.94%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 62.07%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 62.71%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 63.10%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 63.87%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 64.58%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 63.97%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 62.86%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 61.46%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 59.80%   [EVAL] batch:   37 | acc: 12.50%,  total acc: 58.55%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 57.53%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 57.97%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 58.84%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 59.52%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 60.17%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 60.65%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 61.11%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 60.33%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 60.51%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 61.33%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 60.97%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 61.50%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 61.89%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 62.38%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 63.08%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 63.64%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 64.06%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 63.93%   [EVAL] batch:   57 | acc: 6.25%,  total acc: 62.93%   [EVAL] batch:   58 | acc: 25.00%,  total acc: 62.29%   [EVAL] batch:   59 | acc: 12.50%,  total acc: 61.46%   [EVAL] batch:   60 | acc: 25.00%,  total acc: 60.86%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 59.88%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 59.33%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 58.98%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 59.13%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 58.81%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 58.58%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 58.27%   [EVAL] batch:   68 | acc: 12.50%,  total acc: 57.61%   [EVAL] batch:   69 | acc: 6.25%,  total acc: 56.88%   [EVAL] batch:   70 | acc: 6.25%,  total acc: 56.16%   [EVAL] batch:   71 | acc: 18.75%,  total acc: 55.64%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 55.22%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 55.15%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 55.17%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 54.85%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 54.95%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 55.05%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 55.22%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 55.78%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 56.02%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 56.40%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 56.48%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 56.77%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 56.62%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 56.61%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 56.68%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 56.89%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 57.02%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 57.43%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 57.90%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 58.36%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 58.80%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 59.24%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 59.67%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 60.03%   [EVAL] batch:   96 | acc: 31.25%,  total acc: 59.73%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 59.76%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 60.10%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 60.25%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 60.33%   
cur_acc:  ['0.8655', '0.7768', '0.6250', '0.5673', '0.4972', '0.8214']
his_acc:  ['0.8655', '0.8245', '0.7014', '0.6354', '0.5703', '0.6033']
CurrentTrain: epoch  0, batch     0 | loss: 6.6554427CurrentTrain: epoch  0, batch     1 | loss: 6.4638019CurrentTrain: epoch  1, batch     0 | loss: 5.5686078CurrentTrain: epoch  1, batch     1 | loss: 4.7139173CurrentTrain: epoch  2, batch     0 | loss: 5.3182321CurrentTrain: epoch  2, batch     1 | loss: 4.1641836CurrentTrain: epoch  3, batch     0 | loss: 4.9772310CurrentTrain: epoch  3, batch     1 | loss: 4.5676484CurrentTrain: epoch  4, batch     0 | loss: 4.5926771CurrentTrain: epoch  4, batch     1 | loss: 4.1920819CurrentTrain: epoch  5, batch     0 | loss: 3.9687042CurrentTrain: epoch  5, batch     1 | loss: 4.5390534CurrentTrain: epoch  6, batch     0 | loss: 4.4381638CurrentTrain: epoch  6, batch     1 | loss: 3.2107356CurrentTrain: epoch  7, batch     0 | loss: 4.0348892CurrentTrain: epoch  7, batch     1 | loss: 3.7908719CurrentTrain: epoch  8, batch     0 | loss: 4.0760689CurrentTrain: epoch  8, batch     1 | loss: 2.9107268CurrentTrain: epoch  9, batch     0 | loss: 3.0012927CurrentTrain: epoch  9, batch     1 | loss: 3.8032336
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: ny-schools-chief -lrb- new york -rrb- -- cathleen p. black , mayor michael r. bloomberg 's choice to be the next chancellor of new york city 's public-school system , has during more than 40 years in the media business broken numerous glass ceilings -- and amassed a fortune -- with quick and cold-blooded decision making , crystal-clear goal setting , and an all-surpassing attention to the bottom line .
Head Entity: cathleen p. black
Tail Entity: new york
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: after years of living in the bustling city, john decided to move to a quieter place in california, where he could enjoy the serene beaches and a slower pace of life.  
Head Entity: john  
Tail Entity: california  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: he was taken off life support on feb. 14 .
Head Entity: he
Tail Entity: feb. 14
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: she passed away on March 5, 2020.  
Head Entity: she  
Tail Entity: March 5, 2020  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: covidien , which posted revenue of more than $ 10 billion last year , has about 42,000 employees worldwide .
Head Entity: covidien
Tail Entity: 42,000
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: Google, known for its innovative technology, employs approximately 156,500 people globally.  
Head Entity: Google  
Tail Entity: 156,500  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: they represent a continuation of the company 's jerome robbins celebration that began in the spring at lincoln center to mark the 10th anniversary of robbins ' death .
Head Entity: jerome robbins
Tail Entity: robbins
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: the famous author mark twain, known for his wit and humor, is also recognized by his real name, samuel clemens.  
Head Entity: mark twain  
Tail Entity: samuel clemens  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: in addition to his wife , meskill is survived by two daughters , eileen gallup of new britain and maureen heneghan of haddon heights , n.j. ; three sons , john , of kensington , conn. ; peter , of east hartford , conn. ; and thomas , of branford , conn. ; two sisters , ruth prior of naples , fla. , and sister laura marie of portland , conn. ; five grandchildren , and two step-grandchildren .
Head Entity: his
Tail Entity: meskill
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: after a long and happy marriage, john and his wife decided to celebrate their anniversary with a grand party.  
Head Entity: his  
Tail Entity: wife  
Mixup data size:  182
MixupTrain:  epoch  0, batch     0 | loss: 6.7103252MixupTrain:  epoch  0, batch     1 | loss: 5.4420042MixupTrain:  epoch  0, batch     2 | loss: 5.4547729MixupTrain:  epoch  0, batch     3 | loss: 5.0514641MixupTrain:  epoch  0, batch     4 | loss: 5.8906012MixupTrain:  epoch  0, batch     5 | loss: 5.2009497MixupTrain:  epoch  0, batch     6 | loss: 5.7315540MixupTrain:  epoch  0, batch     7 | loss: 5.0680037MixupTrain:  epoch  0, batch     8 | loss: 5.4031725MixupTrain:  epoch  0, batch     9 | loss: 5.5570917MixupTrain:  epoch  0, batch    10 | loss: 5.8018689MixupTrain:  epoch  0, batch    11 | loss: 5.0342779
MemoryTrain:  epoch  0, batch     0 | loss: 2.2465873MemoryTrain:  epoch  0, batch     1 | loss: 2.0671329MemoryTrain:  epoch  0, batch     2 | loss: 2.6562202MemoryTrain:  epoch  0, batch     3 | loss: 2.8841391MemoryTrain:  epoch  0, batch     4 | loss: 3.1910276MemoryTrain:  epoch  1, batch     0 | loss: 2.8954148MemoryTrain:  epoch  1, batch     1 | loss: 2.3540297MemoryTrain:  epoch  1, batch     2 | loss: 2.4528077MemoryTrain:  epoch  1, batch     3 | loss: 2.7768595MemoryTrain:  epoch  1, batch     4 | loss: 2.1459436MemoryTrain:  epoch  2, batch     0 | loss: 2.4465652MemoryTrain:  epoch  2, batch     1 | loss: 2.5271316MemoryTrain:  epoch  2, batch     2 | loss: 2.0385737MemoryTrain:  epoch  2, batch     3 | loss: 1.9224050MemoryTrain:  epoch  2, batch     4 | loss: 2.9105172MemoryTrain:  epoch  3, batch     0 | loss: 2.1431367MemoryTrain:  epoch  3, batch     1 | loss: 1.8076940MemoryTrain:  epoch  3, batch     2 | loss: 2.2282519MemoryTrain:  epoch  3, batch     3 | loss: 1.8048365MemoryTrain:  epoch  3, batch     4 | loss: 1.6331382MemoryTrain:  epoch  4, batch     0 | loss: 1.9730347MemoryTrain:  epoch  4, batch     1 | loss: 1.7871463MemoryTrain:  epoch  4, batch     2 | loss: 1.8763800MemoryTrain:  epoch  4, batch     3 | loss: 1.8581690MemoryTrain:  epoch  4, batch     4 | loss: 1.7905982
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 74.11%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 72.16%   [EVAL] batch:   11 | acc: 25.00%,  total acc: 68.23%   [EVAL] batch:   12 | acc: 25.00%,  total acc: 64.90%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 61.61%   [EVAL] batch:   14 | acc: 18.75%,  total acc: 58.75%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 46.88%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 48.75%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 48.96%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 47.32%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 47.66%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 47.22%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 47.50%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 46.59%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 46.88%   [EVAL] batch:   12 | acc: 18.75%,  total acc: 44.71%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 43.30%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 43.33%   [EVAL] batch:   15 | acc: 37.50%,  total acc: 42.97%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 43.75%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 45.72%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 46.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 49.11%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 51.42%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 53.53%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 55.21%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 57.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 58.65%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 59.49%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 60.94%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 62.07%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 62.71%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 63.10%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 63.67%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 63.83%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 62.68%   [EVAL] batch:   34 | acc: 18.75%,  total acc: 61.43%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 59.72%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 58.11%   [EVAL] batch:   37 | acc: 12.50%,  total acc: 56.91%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 55.77%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 56.09%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 57.01%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 57.59%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 58.28%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 58.66%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 59.17%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 58.29%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 58.24%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 59.11%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 58.67%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 59.13%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 59.44%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 59.86%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 59.91%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 60.42%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 61.02%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 61.38%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 61.18%   [EVAL] batch:   57 | acc: 12.50%,  total acc: 60.34%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 59.85%   [EVAL] batch:   59 | acc: 25.00%,  total acc: 59.27%   [EVAL] batch:   60 | acc: 6.25%,  total acc: 58.40%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 57.46%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 56.65%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 55.96%   [EVAL] batch:   64 | acc: 50.00%,  total acc: 55.87%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 55.59%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 55.22%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 54.78%   [EVAL] batch:   68 | acc: 18.75%,  total acc: 54.26%   [EVAL] batch:   69 | acc: 18.75%,  total acc: 53.75%   [EVAL] batch:   70 | acc: 12.50%,  total acc: 53.17%   [EVAL] batch:   71 | acc: 25.00%,  total acc: 52.78%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 52.40%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 52.36%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 52.42%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 52.14%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 52.19%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 52.32%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 52.53%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 53.12%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 53.40%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 53.81%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 53.99%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 54.32%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 54.19%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 54.07%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 54.02%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 54.33%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 54.49%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 54.93%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 55.43%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 55.91%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 56.38%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 56.85%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 57.30%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 57.62%   [EVAL] batch:   96 | acc: 31.25%,  total acc: 57.35%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 57.14%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 57.51%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 57.69%   [EVAL] batch:  100 | acc: 75.00%,  total acc: 57.86%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 57.84%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 57.95%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 57.93%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 58.10%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 58.31%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 58.59%   [EVAL] batch:  107 | acc: 93.75%,  total acc: 58.91%   [EVAL] batch:  108 | acc: 81.25%,  total acc: 59.12%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 59.43%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 59.46%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 59.26%   [EVAL] batch:  112 | acc: 18.75%,  total acc: 58.90%   [EVAL] batch:  113 | acc: 25.00%,  total acc: 58.61%   [EVAL] batch:  114 | acc: 18.75%,  total acc: 58.26%   [EVAL] batch:  115 | acc: 18.75%,  total acc: 57.92%   
cur_acc:  ['0.8655', '0.7768', '0.6250', '0.5673', '0.4972', '0.8214', '0.5875']
his_acc:  ['0.8655', '0.8245', '0.7014', '0.6354', '0.5703', '0.6033', '0.5792']
CurrentTrain: epoch  0, batch     0 | loss: 5.7499619CurrentTrain: epoch  0, batch     1 | loss: 6.3982549CurrentTrain: epoch  1, batch     0 | loss: 4.8114948CurrentTrain: epoch  1, batch     1 | loss: 4.1732941CurrentTrain: epoch  2, batch     0 | loss: 4.2252617CurrentTrain: epoch  2, batch     1 | loss: 3.7589273CurrentTrain: epoch  3, batch     0 | loss: 3.8652856CurrentTrain: epoch  3, batch     1 | loss: 3.5330899CurrentTrain: epoch  4, batch     0 | loss: 3.4583006CurrentTrain: epoch  4, batch     1 | loss: 3.1723211CurrentTrain: epoch  5, batch     0 | loss: 3.2254043CurrentTrain: epoch  5, batch     1 | loss: 2.8531957CurrentTrain: epoch  6, batch     0 | loss: 2.6609602CurrentTrain: epoch  6, batch     1 | loss: 3.2984154CurrentTrain: epoch  7, batch     0 | loss: 2.8131056CurrentTrain: epoch  7, batch     1 | loss: 2.3421288CurrentTrain: epoch  8, batch     0 | loss: 2.3702631CurrentTrain: epoch  8, batch     1 | loss: 2.2669253CurrentTrain: epoch  9, batch     0 | loss: 2.4708734CurrentTrain: epoch  9, batch     1 | loss: 2.1213162
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: tv-idol-johns -- atlanta -- `` american idol '' finalist michael johns moved to los angeles several years ago , but his heart is still in atlanta .
Head Entity: his
Tail Entity: atlanta
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: actress-sarah -- new york -- `` the talented actress sarah has always considered new york her home, even after moving to los angeles for her career. ''  
Head Entity: sarah  
Tail Entity: new york  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: her political involvement began early : at cornell , she helped organize local farmers ' cooperatives .
Head Entity: she
Tail Entity: cornell
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: After graduating from high school, he went on to study at Stanford University, where he excelled in his courses.  
Head Entity: he  
Tail Entity: Stanford University  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: u.s. rep. parren mitchell , founding member of congressional black caucus , dies at 85
Head Entity: parren mitchell
Tail Entity: u.s.
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passes away in cambridge  
Head Entity: stephen hawking  
Tail Entity: cambridge  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by two stepdaughters , barbara a. hammond and brenda l. stevenson ; a stepson , michael a. taylor ; two grandchildren and one great-grandchild .
Head Entity: he
Tail Entity: brenda l. stevenson
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has three children, including her son, john, and her daughters, emily and sarah.  
Head Entity: she  
Tail Entity: emily  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: flowers always contended politics was behind the extortion investigation , but appeals courts ruled against him .
Head Entity: him
Tail Entity: extortion
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: The prosecutor announced that the suspect was charged with theft after the investigation concluded.  
Head Entity: suspect  
Tail Entity: theft  
Mixup data size:  203
MixupTrain:  epoch  0, batch     0 | loss: 4.3273525MixupTrain:  epoch  0, batch     1 | loss: 4.7405243MixupTrain:  epoch  0, batch     2 | loss: 5.1793575MixupTrain:  epoch  0, batch     3 | loss: 5.1626158MixupTrain:  epoch  0, batch     4 | loss: 4.1808529MixupTrain:  epoch  0, batch     5 | loss: 4.7976122MixupTrain:  epoch  0, batch     6 | loss: 5.3628750MixupTrain:  epoch  0, batch     7 | loss: 4.8257198MixupTrain:  epoch  0, batch     8 | loss: 5.4288273MixupTrain:  epoch  0, batch     9 | loss: 4.3788919MixupTrain:  epoch  0, batch    10 | loss: 4.7315373MixupTrain:  epoch  0, batch    11 | loss: 4.7626276MixupTrain:  epoch  0, batch    12 | loss: 4.6560621
MemoryTrain:  epoch  0, batch     0 | loss: 1.9975144MemoryTrain:  epoch  0, batch     1 | loss: 1.5881206MemoryTrain:  epoch  0, batch     2 | loss: 3.0208683MemoryTrain:  epoch  0, batch     3 | loss: 1.9210268MemoryTrain:  epoch  0, batch     4 | loss: 2.7690802MemoryTrain:  epoch  0, batch     5 | loss: 4.5164022MemoryTrain:  epoch  1, batch     0 | loss: 1.5807307MemoryTrain:  epoch  1, batch     1 | loss: 2.7748892MemoryTrain:  epoch  1, batch     2 | loss: 2.3447602MemoryTrain:  epoch  1, batch     3 | loss: 2.8571675MemoryTrain:  epoch  1, batch     4 | loss: 1.6447067MemoryTrain:  epoch  1, batch     5 | loss: 3.0196784MemoryTrain:  epoch  2, batch     0 | loss: 2.1501610MemoryTrain:  epoch  2, batch     1 | loss: 1.5106472MemoryTrain:  epoch  2, batch     2 | loss: 2.0487871MemoryTrain:  epoch  2, batch     3 | loss: 1.6823878MemoryTrain:  epoch  2, batch     4 | loss: 2.5635540MemoryTrain:  epoch  2, batch     5 | loss: 2.0304055MemoryTrain:  epoch  3, batch     0 | loss: 1.9590511MemoryTrain:  epoch  3, batch     1 | loss: 1.6459554MemoryTrain:  epoch  3, batch     2 | loss: 1.4893339MemoryTrain:  epoch  3, batch     3 | loss: 2.2710099MemoryTrain:  epoch  3, batch     4 | loss: 1.5540217MemoryTrain:  epoch  3, batch     5 | loss: 1.2098205MemoryTrain:  epoch  4, batch     0 | loss: 1.6110685MemoryTrain:  epoch  4, batch     1 | loss: 1.2723132MemoryTrain:  epoch  4, batch     2 | loss: 1.7109766MemoryTrain:  epoch  4, batch     3 | loss: 1.9527936MemoryTrain:  epoch  4, batch     4 | loss: 1.8461295MemoryTrain:  epoch  4, batch     5 | loss: 1.2066498
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 57.50%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 53.12%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 56.25%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 60.16%   [EVAL] batch:    8 | acc: 18.75%,  total acc: 55.56%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 56.88%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 55.11%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 58.85%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 62.02%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 64.73%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 67.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 70.96%   [EVAL] batch:   17 | acc: 18.75%,  total acc: 68.06%   
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 40.62%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 42.50%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 41.67%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 45.54%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 50.00%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 54.86%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 56.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 59.66%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 61.98%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 60.10%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 58.04%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 57.50%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 57.03%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 58.09%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 58.33%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 59.21%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 60.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 61.90%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 63.64%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 65.22%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 66.41%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 67.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 68.99%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 69.44%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 71.34%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 71.67%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 71.77%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 72.27%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 72.35%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 71.14%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 69.46%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 67.53%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 65.71%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 64.14%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 62.82%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 62.81%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 63.41%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 63.24%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 63.37%   [EVAL] batch:   43 | acc: 43.75%,  total acc: 62.93%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 63.06%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 62.36%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 62.23%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 62.89%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 62.63%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 62.62%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 62.13%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 62.02%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 61.67%   [EVAL] batch:   53 | acc: 18.75%,  total acc: 60.88%   [EVAL] batch:   54 | acc: 25.00%,  total acc: 60.23%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 59.93%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 59.65%   [EVAL] batch:   57 | acc: 6.25%,  total acc: 58.73%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 58.47%   [EVAL] batch:   59 | acc: 25.00%,  total acc: 57.92%   [EVAL] batch:   60 | acc: 6.25%,  total acc: 57.07%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 56.15%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 55.56%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 54.98%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 54.52%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 53.79%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 53.17%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 52.85%   [EVAL] batch:   68 | acc: 12.50%,  total acc: 52.26%   [EVAL] batch:   69 | acc: 6.25%,  total acc: 51.61%   [EVAL] batch:   70 | acc: 18.75%,  total acc: 51.14%   [EVAL] batch:   71 | acc: 18.75%,  total acc: 50.69%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 50.34%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 50.34%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 50.42%   [EVAL] batch:   75 | acc: 43.75%,  total acc: 50.33%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 50.41%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 50.56%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 50.79%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 51.33%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 51.62%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 52.06%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 52.18%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 52.46%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 52.13%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 51.74%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 51.58%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 51.85%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 52.18%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 52.64%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 53.16%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 53.67%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 54.17%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 54.65%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 55.13%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 55.53%   [EVAL] batch:   96 | acc: 18.75%,  total acc: 55.15%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 54.97%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 55.37%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 55.56%   [EVAL] batch:  100 | acc: 75.00%,  total acc: 55.75%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 55.76%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 55.83%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 55.83%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 55.95%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 56.01%   [EVAL] batch:  106 | acc: 81.25%,  total acc: 56.25%   [EVAL] batch:  107 | acc: 93.75%,  total acc: 56.60%   [EVAL] batch:  108 | acc: 87.50%,  total acc: 56.88%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 57.22%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 57.26%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 57.03%   [EVAL] batch:  112 | acc: 12.50%,  total acc: 56.64%   [EVAL] batch:  113 | acc: 12.50%,  total acc: 56.25%   [EVAL] batch:  114 | acc: 12.50%,  total acc: 55.87%   [EVAL] batch:  115 | acc: 37.50%,  total acc: 55.71%   [EVAL] batch:  116 | acc: 50.00%,  total acc: 55.66%   [EVAL] batch:  117 | acc: 68.75%,  total acc: 55.77%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 55.78%   [EVAL] batch:  119 | acc: 62.50%,  total acc: 55.83%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 55.73%   [EVAL] batch:  121 | acc: 43.75%,  total acc: 55.64%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 56.00%   [EVAL] batch:  123 | acc: 37.50%,  total acc: 55.85%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 55.90%   [EVAL] batch:  125 | acc: 43.75%,  total acc: 55.80%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 55.91%   [EVAL] batch:  127 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:  128 | acc: 100.00%,  total acc: 56.59%   [EVAL] batch:  129 | acc: 100.00%,  total acc: 56.92%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 57.25%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 57.58%   [EVAL] batch:  132 | acc: 56.25%,  total acc: 57.57%   
cur_acc:  ['0.8655', '0.7768', '0.6250', '0.5673', '0.4972', '0.8214', '0.5875', '0.6806']
his_acc:  ['0.8655', '0.8245', '0.7014', '0.6354', '0.5703', '0.6033', '0.5792', '0.5757']
--------Round  1
seed:  200
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 11.4952507CurrentTrain: epoch  0, batch     1 | loss: 11.9618664CurrentTrain: epoch  0, batch     2 | loss: 11.7091227CurrentTrain: epoch  0, batch     3 | loss: 11.4504623CurrentTrain: epoch  0, batch     4 | loss: 11.4995012CurrentTrain: epoch  0, batch     5 | loss: 11.8947592CurrentTrain: epoch  0, batch     6 | loss: 10.6762991CurrentTrain: epoch  0, batch     7 | loss: 11.2016792CurrentTrain: epoch  0, batch     8 | loss: 11.1656504CurrentTrain: epoch  0, batch     9 | loss: 10.4580593CurrentTrain: epoch  0, batch    10 | loss: 10.7963085CurrentTrain: epoch  0, batch    11 | loss: 10.9828415CurrentTrain: epoch  0, batch    12 | loss: 11.0626698CurrentTrain: epoch  0, batch    13 | loss: 10.0168800CurrentTrain: epoch  0, batch    14 | loss: 10.2110786CurrentTrain: epoch  0, batch    15 | loss: 10.2323437CurrentTrain: epoch  0, batch    16 | loss: 10.8332634CurrentTrain: epoch  0, batch    17 | loss: 10.1460962CurrentTrain: epoch  0, batch    18 | loss: 9.4991436CurrentTrain: epoch  0, batch    19 | loss: 9.7791662CurrentTrain: epoch  0, batch    20 | loss: 9.9924326CurrentTrain: epoch  0, batch    21 | loss: 9.9689007CurrentTrain: epoch  0, batch    22 | loss: 9.8920565CurrentTrain: epoch  0, batch    23 | loss: 10.0600882CurrentTrain: epoch  0, batch    24 | loss: 10.3968010CurrentTrain: epoch  0, batch    25 | loss: 9.6663389CurrentTrain: epoch  0, batch    26 | loss: 10.7101421CurrentTrain: epoch  0, batch    27 | loss: 9.7565861CurrentTrain: epoch  0, batch    28 | loss: 9.9091043CurrentTrain: epoch  0, batch    29 | loss: 9.7145243CurrentTrain: epoch  0, batch    30 | loss: 10.0316944CurrentTrain: epoch  0, batch    31 | loss: 10.5263100CurrentTrain: epoch  0, batch    32 | loss: 9.5474558CurrentTrain: epoch  0, batch    33 | loss: 8.9952526CurrentTrain: epoch  0, batch    34 | loss: 8.7655602CurrentTrain: epoch  0, batch    35 | loss: 9.4703541CurrentTrain: epoch  0, batch    36 | loss: 10.4985476CurrentTrain: epoch  0, batch    37 | loss: 10.1135426CurrentTrain: epoch  1, batch     0 | loss: 9.3058958CurrentTrain: epoch  1, batch     1 | loss: 9.7666206CurrentTrain: epoch  1, batch     2 | loss: 8.7753677CurrentTrain: epoch  1, batch     3 | loss: 9.6046209CurrentTrain: epoch  1, batch     4 | loss: 10.9012032CurrentTrain: epoch  1, batch     5 | loss: 9.6132660CurrentTrain: epoch  1, batch     6 | loss: 9.4753084CurrentTrain: epoch  1, batch     7 | loss: 8.9616909CurrentTrain: epoch  1, batch     8 | loss: 7.8810072CurrentTrain: epoch  1, batch     9 | loss: 9.9477110CurrentTrain: epoch  1, batch    10 | loss: 9.2504587CurrentTrain: epoch  1, batch    11 | loss: 8.7776470CurrentTrain: epoch  1, batch    12 | loss: 9.4625034CurrentTrain: epoch  1, batch    13 | loss: 9.0188789CurrentTrain: epoch  1, batch    14 | loss: 9.4873676CurrentTrain: epoch  1, batch    15 | loss: 9.0470953CurrentTrain: epoch  1, batch    16 | loss: 8.9175396CurrentTrain: epoch  1, batch    17 | loss: 8.1677103CurrentTrain: epoch  1, batch    18 | loss: 9.3521872CurrentTrain: epoch  1, batch    19 | loss: 8.4244194CurrentTrain: epoch  1, batch    20 | loss: 8.5383301CurrentTrain: epoch  1, batch    21 | loss: 8.6236935CurrentTrain: epoch  1, batch    22 | loss: 8.8053122CurrentTrain: epoch  1, batch    23 | loss: 9.2407932CurrentTrain: epoch  1, batch    24 | loss: 8.2683210CurrentTrain: epoch  1, batch    25 | loss: 8.6047058CurrentTrain: epoch  1, batch    26 | loss: 8.6177692CurrentTrain: epoch  1, batch    27 | loss: 7.6133947CurrentTrain: epoch  1, batch    28 | loss: 7.7481151CurrentTrain: epoch  1, batch    29 | loss: 8.0339756CurrentTrain: epoch  1, batch    30 | loss: 8.2406912CurrentTrain: epoch  1, batch    31 | loss: 8.9478836CurrentTrain: epoch  1, batch    32 | loss: 8.7471828CurrentTrain: epoch  1, batch    33 | loss: 7.8716350CurrentTrain: epoch  1, batch    34 | loss: 7.9947252CurrentTrain: epoch  1, batch    35 | loss: 9.0526104CurrentTrain: epoch  1, batch    36 | loss: 7.8852739CurrentTrain: epoch  1, batch    37 | loss: 8.0240078CurrentTrain: epoch  2, batch     0 | loss: 7.8816385CurrentTrain: epoch  2, batch     1 | loss: 6.9630108CurrentTrain: epoch  2, batch     2 | loss: 7.5695763CurrentTrain: epoch  2, batch     3 | loss: 7.7512107CurrentTrain: epoch  2, batch     4 | loss: 7.4639015CurrentTrain: epoch  2, batch     5 | loss: 8.0823402CurrentTrain: epoch  2, batch     6 | loss: 7.8754063CurrentTrain: epoch  2, batch     7 | loss: 8.1218376CurrentTrain: epoch  2, batch     8 | loss: 8.6295471CurrentTrain: epoch  2, batch     9 | loss: 8.5351734CurrentTrain: epoch  2, batch    10 | loss: 7.5891767CurrentTrain: epoch  2, batch    11 | loss: 7.8056726CurrentTrain: epoch  2, batch    12 | loss: 8.3175058CurrentTrain: epoch  2, batch    13 | loss: 7.9806957CurrentTrain: epoch  2, batch    14 | loss: 7.8453770CurrentTrain: epoch  2, batch    15 | loss: 6.1967363CurrentTrain: epoch  2, batch    16 | loss: 7.5923672CurrentTrain: epoch  2, batch    17 | loss: 7.9453263CurrentTrain: epoch  2, batch    18 | loss: 6.7473302CurrentTrain: epoch  2, batch    19 | loss: 8.4022045CurrentTrain: epoch  2, batch    20 | loss: 8.7440348CurrentTrain: epoch  2, batch    21 | loss: 7.3044996CurrentTrain: epoch  2, batch    22 | loss: 7.4315844CurrentTrain: epoch  2, batch    23 | loss: 7.2921133CurrentTrain: epoch  2, batch    24 | loss: 7.9160123CurrentTrain: epoch  2, batch    25 | loss: 7.1774483CurrentTrain: epoch  2, batch    26 | loss: 6.5164294CurrentTrain: epoch  2, batch    27 | loss: 7.3089333CurrentTrain: epoch  2, batch    28 | loss: 7.1532969CurrentTrain: epoch  2, batch    29 | loss: 7.2100854CurrentTrain: epoch  2, batch    30 | loss: 7.5278969CurrentTrain: epoch  2, batch    31 | loss: 7.2912726CurrentTrain: epoch  2, batch    32 | loss: 7.6876760CurrentTrain: epoch  2, batch    33 | loss: 8.0848656CurrentTrain: epoch  2, batch    34 | loss: 7.6523390CurrentTrain: epoch  2, batch    35 | loss: 7.5282297CurrentTrain: epoch  2, batch    36 | loss: 7.8699369CurrentTrain: epoch  2, batch    37 | loss: 6.7402172CurrentTrain: epoch  3, batch     0 | loss: 6.6062789CurrentTrain: epoch  3, batch     1 | loss: 7.0929980CurrentTrain: epoch  3, batch     2 | loss: 6.9733315CurrentTrain: epoch  3, batch     3 | loss: 7.1811829CurrentTrain: epoch  3, batch     4 | loss: 6.8607635CurrentTrain: epoch  3, batch     5 | loss: 6.9373345CurrentTrain: epoch  3, batch     6 | loss: 7.5848722CurrentTrain: epoch  3, batch     7 | loss: 7.3785467CurrentTrain: epoch  3, batch     8 | loss: 7.1194916CurrentTrain: epoch  3, batch     9 | loss: 6.4621677CurrentTrain: epoch  3, batch    10 | loss: 6.0384378CurrentTrain: epoch  3, batch    11 | loss: 7.5622244CurrentTrain: epoch  3, batch    12 | loss: 7.3701310CurrentTrain: epoch  3, batch    13 | loss: 8.0111008CurrentTrain: epoch  3, batch    14 | loss: 8.0189323CurrentTrain: epoch  3, batch    15 | loss: 7.1449327CurrentTrain: epoch  3, batch    16 | loss: 7.0852728CurrentTrain: epoch  3, batch    17 | loss: 7.9288316CurrentTrain: epoch  3, batch    18 | loss: 6.6159534CurrentTrain: epoch  3, batch    19 | loss: 6.8427887CurrentTrain: epoch  3, batch    20 | loss: 5.9231000CurrentTrain: epoch  3, batch    21 | loss: 6.7443833CurrentTrain: epoch  3, batch    22 | loss: 7.2373815CurrentTrain: epoch  3, batch    23 | loss: 7.6036682CurrentTrain: epoch  3, batch    24 | loss: 6.5777149CurrentTrain: epoch  3, batch    25 | loss: 7.0841613CurrentTrain: epoch  3, batch    26 | loss: 6.6016026CurrentTrain: epoch  3, batch    27 | loss: 6.6885347CurrentTrain: epoch  3, batch    28 | loss: 6.8385563CurrentTrain: epoch  3, batch    29 | loss: 7.1129093CurrentTrain: epoch  3, batch    30 | loss: 6.5684385CurrentTrain: epoch  3, batch    31 | loss: 7.0256271CurrentTrain: epoch  3, batch    32 | loss: 6.7370205CurrentTrain: epoch  3, batch    33 | loss: 6.7431717CurrentTrain: epoch  3, batch    34 | loss: 5.8190374CurrentTrain: epoch  3, batch    35 | loss: 6.8154526CurrentTrain: epoch  3, batch    36 | loss: 6.0599246CurrentTrain: epoch  3, batch    37 | loss: 6.6704588CurrentTrain: epoch  4, batch     0 | loss: 6.2489414CurrentTrain: epoch  4, batch     1 | loss: 6.9139490CurrentTrain: epoch  4, batch     2 | loss: 6.8215113CurrentTrain: epoch  4, batch     3 | loss: 6.6996984CurrentTrain: epoch  4, batch     4 | loss: 7.0032768CurrentTrain: epoch  4, batch     5 | loss: 6.0609031CurrentTrain: epoch  4, batch     6 | loss: 7.0297675CurrentTrain: epoch  4, batch     7 | loss: 6.7138028CurrentTrain: epoch  4, batch     8 | loss: 7.2800207CurrentTrain: epoch  4, batch     9 | loss: 6.0930357CurrentTrain: epoch  4, batch    10 | loss: 6.6360583CurrentTrain: epoch  4, batch    11 | loss: 6.4726996CurrentTrain: epoch  4, batch    12 | loss: 6.5891943CurrentTrain: epoch  4, batch    13 | loss: 6.3953247CurrentTrain: epoch  4, batch    14 | loss: 5.8619003CurrentTrain: epoch  4, batch    15 | loss: 6.3164496CurrentTrain: epoch  4, batch    16 | loss: 6.2850175CurrentTrain: epoch  4, batch    17 | loss: 7.3138337CurrentTrain: epoch  4, batch    18 | loss: 6.1229067CurrentTrain: epoch  4, batch    19 | loss: 6.0989232CurrentTrain: epoch  4, batch    20 | loss: 6.3414903CurrentTrain: epoch  4, batch    21 | loss: 6.3103552CurrentTrain: epoch  4, batch    22 | loss: 6.1693735CurrentTrain: epoch  4, batch    23 | loss: 5.8889990CurrentTrain: epoch  4, batch    24 | loss: 6.2656651CurrentTrain: epoch  4, batch    25 | loss: 6.6743212CurrentTrain: epoch  4, batch    26 | loss: 6.5033112CurrentTrain: epoch  4, batch    27 | loss: 6.0698991CurrentTrain: epoch  4, batch    28 | loss: 6.7444296CurrentTrain: epoch  4, batch    29 | loss: 5.9560986CurrentTrain: epoch  4, batch    30 | loss: 5.8302855CurrentTrain: epoch  4, batch    31 | loss: 6.1072626CurrentTrain: epoch  4, batch    32 | loss: 5.5638819CurrentTrain: epoch  4, batch    33 | loss: 6.0502529CurrentTrain: epoch  4, batch    34 | loss: 5.7723093CurrentTrain: epoch  4, batch    35 | loss: 5.2214112CurrentTrain: epoch  4, batch    36 | loss: 6.0837131CurrentTrain: epoch  4, batch    37 | loss: 5.6245847CurrentTrain: epoch  5, batch     0 | loss: 5.9872437CurrentTrain: epoch  5, batch     1 | loss: 5.4005785CurrentTrain: epoch  5, batch     2 | loss: 5.8341322CurrentTrain: epoch  5, batch     3 | loss: 6.0954084CurrentTrain: epoch  5, batch     4 | loss: 8.0198240CurrentTrain: epoch  5, batch     5 | loss: 6.2535648CurrentTrain: epoch  5, batch     6 | loss: 5.6903715CurrentTrain: epoch  5, batch     7 | loss: 6.2086544CurrentTrain: epoch  5, batch     8 | loss: 5.8370981CurrentTrain: epoch  5, batch     9 | loss: 6.1773944CurrentTrain: epoch  5, batch    10 | loss: 5.8045721CurrentTrain: epoch  5, batch    11 | loss: 5.8567491CurrentTrain: epoch  5, batch    12 | loss: 5.7813425CurrentTrain: epoch  5, batch    13 | loss: 6.2178373CurrentTrain: epoch  5, batch    14 | loss: 6.1837616CurrentTrain: epoch  5, batch    15 | loss: 6.2117162CurrentTrain: epoch  5, batch    16 | loss: 5.5172310CurrentTrain: epoch  5, batch    17 | loss: 5.9553170CurrentTrain: epoch  5, batch    18 | loss: 5.5209274CurrentTrain: epoch  5, batch    19 | loss: 5.8181105CurrentTrain: epoch  5, batch    20 | loss: 5.9316955CurrentTrain: epoch  5, batch    21 | loss: 6.6555023CurrentTrain: epoch  5, batch    22 | loss: 6.2339163CurrentTrain: epoch  5, batch    23 | loss: 5.6159296CurrentTrain: epoch  5, batch    24 | loss: 5.6755266CurrentTrain: epoch  5, batch    25 | loss: 5.6136675CurrentTrain: epoch  5, batch    26 | loss: 5.6292944CurrentTrain: epoch  5, batch    27 | loss: 5.7195454CurrentTrain: epoch  5, batch    28 | loss: 5.5026999CurrentTrain: epoch  5, batch    29 | loss: 6.9394045CurrentTrain: epoch  5, batch    30 | loss: 5.9549470CurrentTrain: epoch  5, batch    31 | loss: 5.9532332CurrentTrain: epoch  5, batch    32 | loss: 5.4807677CurrentTrain: epoch  5, batch    33 | loss: 5.3622317CurrentTrain: epoch  5, batch    34 | loss: 6.2250767CurrentTrain: epoch  5, batch    35 | loss: 6.1277623CurrentTrain: epoch  5, batch    36 | loss: 5.9913163CurrentTrain: epoch  5, batch    37 | loss: 4.8407764CurrentTrain: epoch  6, batch     0 | loss: 5.5547996CurrentTrain: epoch  6, batch     1 | loss: 5.6077251CurrentTrain: epoch  6, batch     2 | loss: 5.5835762CurrentTrain: epoch  6, batch     3 | loss: 6.1344423CurrentTrain: epoch  6, batch     4 | loss: 5.5800405CurrentTrain: epoch  6, batch     5 | loss: 5.7692723CurrentTrain: epoch  6, batch     6 | loss: 5.1928892CurrentTrain: epoch  6, batch     7 | loss: 5.3895006CurrentTrain: epoch  6, batch     8 | loss: 5.3614798CurrentTrain: epoch  6, batch     9 | loss: 5.6295652CurrentTrain: epoch  6, batch    10 | loss: 5.5838928CurrentTrain: epoch  6, batch    11 | loss: 5.6471338CurrentTrain: epoch  6, batch    12 | loss: 5.6680408CurrentTrain: epoch  6, batch    13 | loss: 5.5321217CurrentTrain: epoch  6, batch    14 | loss: 5.3577118CurrentTrain: epoch  6, batch    15 | loss: 5.9597845CurrentTrain: epoch  6, batch    16 | loss: 5.4082980CurrentTrain: epoch  6, batch    17 | loss: 5.2012658CurrentTrain: epoch  6, batch    18 | loss: 5.3993630CurrentTrain: epoch  6, batch    19 | loss: 5.3073206CurrentTrain: epoch  6, batch    20 | loss: 5.1158042CurrentTrain: epoch  6, batch    21 | loss: 5.1988964CurrentTrain: epoch  6, batch    22 | loss: 6.7516217CurrentTrain: epoch  6, batch    23 | loss: 6.3882980CurrentTrain: epoch  6, batch    24 | loss: 5.9561749CurrentTrain: epoch  6, batch    25 | loss: 5.2372255CurrentTrain: epoch  6, batch    26 | loss: 5.5052004CurrentTrain: epoch  6, batch    27 | loss: 5.4510899CurrentTrain: epoch  6, batch    28 | loss: 5.5559216CurrentTrain: epoch  6, batch    29 | loss: 5.3476176CurrentTrain: epoch  6, batch    30 | loss: 5.5067139CurrentTrain: epoch  6, batch    31 | loss: 5.5220561CurrentTrain: epoch  6, batch    32 | loss: 5.1367607CurrentTrain: epoch  6, batch    33 | loss: 6.7668796CurrentTrain: epoch  6, batch    34 | loss: 5.5712180CurrentTrain: epoch  6, batch    35 | loss: 5.5531726CurrentTrain: epoch  6, batch    36 | loss: 6.6285076CurrentTrain: epoch  6, batch    37 | loss: 5.0813398CurrentTrain: epoch  7, batch     0 | loss: 5.5609903CurrentTrain: epoch  7, batch     1 | loss: 5.5071807CurrentTrain: epoch  7, batch     2 | loss: 5.5841031CurrentTrain: epoch  7, batch     3 | loss: 5.8392434CurrentTrain: epoch  7, batch     4 | loss: 5.4410934CurrentTrain: epoch  7, batch     5 | loss: 5.2984691CurrentTrain: epoch  7, batch     6 | loss: 5.4871926CurrentTrain: epoch  7, batch     7 | loss: 5.8967161CurrentTrain: epoch  7, batch     8 | loss: 5.3586512CurrentTrain: epoch  7, batch     9 | loss: 5.2871027CurrentTrain: epoch  7, batch    10 | loss: 5.2546878CurrentTrain: epoch  7, batch    11 | loss: 4.9347744CurrentTrain: epoch  7, batch    12 | loss: 5.8759284CurrentTrain: epoch  7, batch    13 | loss: 5.2580881CurrentTrain: epoch  7, batch    14 | loss: 5.1904106CurrentTrain: epoch  7, batch    15 | loss: 5.2352996CurrentTrain: epoch  7, batch    16 | loss: 5.3971124CurrentTrain: epoch  7, batch    17 | loss: 5.4344935CurrentTrain: epoch  7, batch    18 | loss: 5.3451357CurrentTrain: epoch  7, batch    19 | loss: 5.0849705CurrentTrain: epoch  7, batch    20 | loss: 5.1249332CurrentTrain: epoch  7, batch    21 | loss: 5.2973261CurrentTrain: epoch  7, batch    22 | loss: 6.0297012CurrentTrain: epoch  7, batch    23 | loss: 5.5495868CurrentTrain: epoch  7, batch    24 | loss: 5.2133803CurrentTrain: epoch  7, batch    25 | loss: 5.1576505CurrentTrain: epoch  7, batch    26 | loss: 5.1422796CurrentTrain: epoch  7, batch    27 | loss: 5.1132975CurrentTrain: epoch  7, batch    28 | loss: 4.9744816CurrentTrain: epoch  7, batch    29 | loss: 5.7927094CurrentTrain: epoch  7, batch    30 | loss: 5.2160301CurrentTrain: epoch  7, batch    31 | loss: 5.2629786CurrentTrain: epoch  7, batch    32 | loss: 5.8652630CurrentTrain: epoch  7, batch    33 | loss: 5.5444746CurrentTrain: epoch  7, batch    34 | loss: 6.6703720CurrentTrain: epoch  7, batch    35 | loss: 5.9917049CurrentTrain: epoch  7, batch    36 | loss: 5.4586973CurrentTrain: epoch  7, batch    37 | loss: 5.2874126CurrentTrain: epoch  8, batch     0 | loss: 5.5083227CurrentTrain: epoch  8, batch     1 | loss: 5.4225760CurrentTrain: epoch  8, batch     2 | loss: 5.8430738CurrentTrain: epoch  8, batch     3 | loss: 5.5593462CurrentTrain: epoch  8, batch     4 | loss: 5.4635201CurrentTrain: epoch  8, batch     5 | loss: 5.0873146CurrentTrain: epoch  8, batch     6 | loss: 5.1428623CurrentTrain: epoch  8, batch     7 | loss: 5.0521269CurrentTrain: epoch  8, batch     8 | loss: 5.4818096CurrentTrain: epoch  8, batch     9 | loss: 4.9598989CurrentTrain: epoch  8, batch    10 | loss: 5.1151066CurrentTrain: epoch  8, batch    11 | loss: 4.9622831CurrentTrain: epoch  8, batch    12 | loss: 4.9481797CurrentTrain: epoch  8, batch    13 | loss: 4.9111986CurrentTrain: epoch  8, batch    14 | loss: 4.9557219CurrentTrain: epoch  8, batch    15 | loss: 5.0714278CurrentTrain: epoch  8, batch    16 | loss: 4.9566622CurrentTrain: epoch  8, batch    17 | loss: 5.1959500CurrentTrain: epoch  8, batch    18 | loss: 4.9190617CurrentTrain: epoch  8, batch    19 | loss: 5.8676329CurrentTrain: epoch  8, batch    20 | loss: 5.1642084CurrentTrain: epoch  8, batch    21 | loss: 5.0517421CurrentTrain: epoch  8, batch    22 | loss: 4.9425077CurrentTrain: epoch  8, batch    23 | loss: 5.0369940CurrentTrain: epoch  8, batch    24 | loss: 5.6340046CurrentTrain: epoch  8, batch    25 | loss: 5.2729049CurrentTrain: epoch  8, batch    26 | loss: 5.0862513CurrentTrain: epoch  8, batch    27 | loss: 5.0260410CurrentTrain: epoch  8, batch    28 | loss: 5.1333985CurrentTrain: epoch  8, batch    29 | loss: 5.1072674CurrentTrain: epoch  8, batch    30 | loss: 5.2668891CurrentTrain: epoch  8, batch    31 | loss: 5.1919985CurrentTrain: epoch  8, batch    32 | loss: 5.8619947CurrentTrain: epoch  8, batch    33 | loss: 4.9207497CurrentTrain: epoch  8, batch    34 | loss: 4.9847245CurrentTrain: epoch  8, batch    35 | loss: 5.0330801CurrentTrain: epoch  8, batch    36 | loss: 5.1846180CurrentTrain: epoch  8, batch    37 | loss: 5.1099491CurrentTrain: epoch  9, batch     0 | loss: 4.9323254CurrentTrain: epoch  9, batch     1 | loss: 5.0394716CurrentTrain: epoch  9, batch     2 | loss: 4.9611087CurrentTrain: epoch  9, batch     3 | loss: 5.4201789CurrentTrain: epoch  9, batch     4 | loss: 4.9534369CurrentTrain: epoch  9, batch     5 | loss: 5.0876293CurrentTrain: epoch  9, batch     6 | loss: 4.9941483CurrentTrain: epoch  9, batch     7 | loss: 5.0381145CurrentTrain: epoch  9, batch     8 | loss: 4.9778786CurrentTrain: epoch  9, batch     9 | loss: 5.0629029CurrentTrain: epoch  9, batch    10 | loss: 4.9532738CurrentTrain: epoch  9, batch    11 | loss: 5.1267118CurrentTrain: epoch  9, batch    12 | loss: 4.9238472CurrentTrain: epoch  9, batch    13 | loss: 4.9978037CurrentTrain: epoch  9, batch    14 | loss: 4.9796124CurrentTrain: epoch  9, batch    15 | loss: 4.9739547CurrentTrain: epoch  9, batch    16 | loss: 5.0294361CurrentTrain: epoch  9, batch    17 | loss: 4.8867884CurrentTrain: epoch  9, batch    18 | loss: 4.9829035CurrentTrain: epoch  9, batch    19 | loss: 4.8389482CurrentTrain: epoch  9, batch    20 | loss: 4.9687490CurrentTrain: epoch  9, batch    21 | loss: 4.8972168CurrentTrain: epoch  9, batch    22 | loss: 4.7841969CurrentTrain: epoch  9, batch    23 | loss: 4.8289423CurrentTrain: epoch  9, batch    24 | loss: 5.1305075CurrentTrain: epoch  9, batch    25 | loss: 5.3241606CurrentTrain: epoch  9, batch    26 | loss: 5.0097022CurrentTrain: epoch  9, batch    27 | loss: 4.8564391CurrentTrain: epoch  9, batch    28 | loss: 4.9755569CurrentTrain: epoch  9, batch    29 | loss: 4.7278547CurrentTrain: epoch  9, batch    30 | loss: 4.9005756CurrentTrain: epoch  9, batch    31 | loss: 5.1333466CurrentTrain: epoch  9, batch    32 | loss: 4.8360300CurrentTrain: epoch  9, batch    33 | loss: 5.0445766CurrentTrain: epoch  9, batch    34 | loss: 5.2298670CurrentTrain: epoch  9, batch    35 | loss: 4.8302641CurrentTrain: epoch  9, batch    36 | loss: 4.9972177CurrentTrain: epoch  9, batch    37 | loss: 4.7948170
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: the head of iran 's atomic energy organisation , ali akbar salehi , told domestic news agencies the new proposal had come in response to iran 's move last week to begin enriching uranium itself to the 20 percent level required for a tehran medical research rector after rejecting a previous offer .
Head Entity: ali akbar salehi
Tail Entity: iran
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the United States, Maria decided to return to her hometown in Spain, where she felt a strong connection to her roots.  
Head Entity: Maria  
Tail Entity: Spain  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: whether this means the country 's tense political situation will dissolve back into civil war is yet to be seen , said rinaldo depagne , a west africa analyst at the international crisis group .
Head Entity: international crisis group
Tail Entity: rinaldo depagne
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: the board of directors at tech innovations inc. has appointed a new chief executive officer to lead the company into its next phase of growth.  
Head Entity: tech innovations inc.  
Tail Entity: chief executive officer
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: linebacker mike peterson rejoined the jacksonville jaguars on friday , two days after coach jack del rio banished him from the locker room of the national football league team .
Head Entity: jacksonville jaguars
Tail Entity: national football league
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: the united nations is an international organization that promotes peace and cooperation among its member states.  
Head Entity: united nations  
Tail Entity: member states  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: the wife of acting cuban president raul castro , vilma espin guillois , died monday after a lengthy illness , cuban television reported monday .
Head Entity: raul castro
Tail Entity: cuban
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: the famous actor and director, alfonso cuaron, was born in mexico and has often spoken about his love for his homeland.  
Head Entity: alfonso cuaron  
Tail Entity: mexico  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: `` we are not canceling any of our orders for next year , '' deputy managing director philip chen was quoted as saying in today 's south china morning post .
Head Entity: philip chen
Tail Entity: director
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: `` the renowned scientist dr. jane smith has been awarded the prestigious title of chief researcher at the national institute of health . ''  
Head Entity: dr. jane smith  
Tail Entity: chief researcher  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: as the government still imposes many restrictions on investing in china , hochen said , chunghwa telecom will consult with the mainland affairs council -- taiwan 's top china policy planning agency -- and other relevant government institutions before launching its overseas expansion drive .
Head Entity: chunghwa telecom
Tail Entity: china
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: after years of rapid growth, the tech giant has decided to establish its new headquarters in the heart of silicon valley, aiming to strengthen its presence in the united states.  
Head Entity: tech giant  
Tail Entity: united states  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 86.16%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.23%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.60%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.20%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.26%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.95%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.74%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 86.16%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.23%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.60%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.20%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.26%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.95%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.74%   
cur_acc:  ['0.8674']
his_acc:  ['0.8674']
CurrentTrain: epoch  0, batch     0 | loss: 6.2508774CurrentTrain: epoch  0, batch     1 | loss: 7.0173969CurrentTrain: epoch  1, batch     0 | loss: 5.5441074CurrentTrain: epoch  1, batch     1 | loss: 6.7372098CurrentTrain: epoch  2, batch     0 | loss: 5.4576530CurrentTrain: epoch  2, batch     1 | loss: 5.8471255CurrentTrain: epoch  3, batch     0 | loss: 4.7888002CurrentTrain: epoch  3, batch     1 | loss: 5.5919361CurrentTrain: epoch  4, batch     0 | loss: 4.5885291CurrentTrain: epoch  4, batch     1 | loss: 5.1951432CurrentTrain: epoch  5, batch     0 | loss: 4.9872861CurrentTrain: epoch  5, batch     1 | loss: 3.5513031CurrentTrain: epoch  6, batch     0 | loss: 3.8067145CurrentTrain: epoch  6, batch     1 | loss: 4.6832795CurrentTrain: epoch  7, batch     0 | loss: 3.9353924CurrentTrain: epoch  7, batch     1 | loss: 3.9711919CurrentTrain: epoch  8, batch     0 | loss: 3.9946413CurrentTrain: epoch  8, batch     1 | loss: 3.1361611CurrentTrain: epoch  9, batch     0 | loss: 3.7065210CurrentTrain: epoch  9, batch     1 | loss: 3.1331658
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: san jose , ca , usa speaking of k-fed , him and ex-wife britney spears are in court today , dealing with their custody battle .
Head Entity: britney spears
Tail Entity: ca
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: after moving to austin, tx, the musician found a new sense of inspiration for his music.  
Head Entity: musician  
Tail Entity: tx  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: he passed away on saturday .
Head Entity: he
Tail Entity: saturday
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: she left this world on March 5th.  
Head Entity: she  
Tail Entity: March 5th  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: covidien , which posted revenue of more than $ 10 billion last year , has about 42,000 employees worldwide .
Head Entity: covidien
Tail Entity: 42,000
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: Google, known for its innovative technology, employs approximately 156,500 people globally.  
Head Entity: Google  
Tail Entity: 156,500  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: `` i am known in the hospice as the man who would n't die , '' buchwald wrote in march .
Head Entity: buchwald
Tail Entity: man who would n't die
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: `` the famous author often referred to as the bard of Avon is none other than william shakespeare. ''  
Head Entity: william shakespeare  
Tail Entity: bard of Avon  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: beverly hills , california 2008-08-17 21:15:39 utc ------ there was much dancing : ellen degeneres and portia de rossi are married , according to reports .
Head Entity: ellen degeneres
Tail Entity: portia de rossi
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: in a beautiful ceremony held in new york city, 2015-06-20 15:30:00 utc ------ the couple exchanged vows: john legend and chrissy teigen are now husband and wife, as confirmed by their friends.  
Head Entity: john legend  
Tail Entity: chrissy teigen  
Mixup data size:  83
MixupTrain:  epoch  0, batch     0 | loss: 8.3761501MixupTrain:  epoch  0, batch     1 | loss: 9.0350170MixupTrain:  epoch  0, batch     2 | loss: 8.3281889MixupTrain:  epoch  0, batch     3 | loss: 7.7385144MixupTrain:  epoch  0, batch     4 | loss: 7.7625580MixupTrain:  epoch  0, batch     5 | loss: 9.6661873
MemoryTrain:  epoch  0, batch     0 | loss: 5.8539696MemoryTrain:  epoch  0, batch     1 | loss: 5.8890548MemoryTrain:  epoch  1, batch     0 | loss: 6.5392456MemoryTrain:  epoch  1, batch     1 | loss: 5.0072432MemoryTrain:  epoch  2, batch     0 | loss: 5.7834287MemoryTrain:  epoch  2, batch     1 | loss: 4.4426904MemoryTrain:  epoch  3, batch     0 | loss: 4.8866825MemoryTrain:  epoch  3, batch     1 | loss: 4.5856280MemoryTrain:  epoch  4, batch     0 | loss: 4.3989434MemoryTrain:  epoch  4, batch     1 | loss: 4.0880609
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 29.17%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 28.12%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 28.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 38.54%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 47.32%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 53.12%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 57.64%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 58.75%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 59.66%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 64.42%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 65.42%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 80.56%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 83.85%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 84.13%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 83.04%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 80.88%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 79.86%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 80.26%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 80.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 81.55%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.15%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 83.85%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.10%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.42%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 86.67%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 86.90%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.11%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 86.17%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 84.38%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 82.68%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 81.08%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 79.73%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 79.61%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 79.97%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 80.64%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 81.10%   [EVAL] batch:   42 | acc: 37.50%,  total acc: 80.09%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 80.54%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 80.69%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 80.98%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 80.98%   
cur_acc:  ['0.8674', '0.6542']
his_acc:  ['0.8674', '0.8098']
CurrentTrain: epoch  0, batch     0 | loss: 7.1513891CurrentTrain: epoch  0, batch     1 | loss: 6.5392933CurrentTrain: epoch  1, batch     0 | loss: 5.9478054CurrentTrain: epoch  1, batch     1 | loss: 6.6542864CurrentTrain: epoch  2, batch     0 | loss: 5.5835605CurrentTrain: epoch  2, batch     1 | loss: 4.6487412CurrentTrain: epoch  3, batch     0 | loss: 4.8541231CurrentTrain: epoch  3, batch     1 | loss: 4.6789613CurrentTrain: epoch  4, batch     0 | loss: 4.6443591CurrentTrain: epoch  4, batch     1 | loss: 4.2191925CurrentTrain: epoch  5, batch     0 | loss: 4.3188214CurrentTrain: epoch  5, batch     1 | loss: 4.3851342CurrentTrain: epoch  6, batch     0 | loss: 4.3132329CurrentTrain: epoch  6, batch     1 | loss: 3.5884082CurrentTrain: epoch  7, batch     0 | loss: 4.1330280CurrentTrain: epoch  7, batch     1 | loss: 3.3535187CurrentTrain: epoch  8, batch     0 | loss: 3.3758807CurrentTrain: epoch  8, batch     1 | loss: 3.1344776CurrentTrain: epoch  9, batch     0 | loss: 3.0418403CurrentTrain: epoch  9, batch     1 | loss: 3.0192688
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: born in 1950 in the northeastern city of basel , ospel left school at 15 to take an apprenticeship at the transvalor brokerage house before joining swiss banking corporation -lrb- sbs -rrb- , which merged with union bank of switzerland to form ubs in 1998 .
Head Entity: ospel
Tail Entity: 1950
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: she was born on July 4, 1985, in a small town in California, where she spent her childhood before moving to New York for her career.  
Head Entity: she  
Tail Entity: July 4, 1985  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: jane matilda bolin was born on april 11 , 1908 , in poughkeepsie , ny .
Head Entity: jane matilda bolin
Tail Entity: ny
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: john smith was born in los angeles, california, on march 5, 1980.  
Head Entity: john smith  
Tail Entity: california  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: lynne spears told the court that lutfi had treated her daughter like a hostage in her own home , drugged her and took over her finances .
Head Entity: her
Tail Entity: lynne spears
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: john's mother, mary, always encouraged him to pursue his dreams and supported him throughout his education.  
Head Entity: john  
Tail Entity: mary  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: after years of hard work, jane doe finally received a promotion at tech innovations, where she has been a dedicated employee.  
Head Entity: jane doe  
Tail Entity: tech innovations  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, 75, passed away peacefully in his sleep on march 5 in the beautiful town of springfield, il, surrounded by family and friends.  
Head Entity: john doe  
Tail Entity: il  
Mixup data size:  102
MixupTrain:  epoch  0, batch     0 | loss: 8.1008568MixupTrain:  epoch  0, batch     1 | loss: 7.1344333MixupTrain:  epoch  0, batch     2 | loss: 7.4015808MixupTrain:  epoch  0, batch     3 | loss: 7.5423284MixupTrain:  epoch  0, batch     4 | loss: 6.6661596MixupTrain:  epoch  0, batch     5 | loss: 7.4940014MixupTrain:  epoch  0, batch     6 | loss: 7.2635627
MemoryTrain:  epoch  0, batch     0 | loss: 4.7455330MemoryTrain:  epoch  0, batch     1 | loss: 4.4943972MemoryTrain:  epoch  1, batch     0 | loss: 4.3233900MemoryTrain:  epoch  1, batch     1 | loss: 5.0763268MemoryTrain:  epoch  2, batch     0 | loss: 4.8875122MemoryTrain:  epoch  2, batch     1 | loss: 3.7555709MemoryTrain:  epoch  3, batch     0 | loss: 4.5091991MemoryTrain:  epoch  3, batch     1 | loss: 3.6868916MemoryTrain:  epoch  4, batch     0 | loss: 3.5099978MemoryTrain:  epoch  4, batch     1 | loss: 3.8412662
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 69.64%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 76.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 77.27%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 77.60%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 77.88%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 73.66%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 67.19%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 80.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 83.17%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 81.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 80.08%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 79.78%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 78.82%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 79.28%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.65%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 81.53%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 82.34%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 83.07%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 84.72%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.27%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.78%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 86.04%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 86.29%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.52%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 85.23%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 83.09%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 80.89%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 78.99%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 77.20%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 76.81%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 76.76%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 77.13%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:   42 | acc: 25.00%,  total acc: 76.45%   [EVAL] batch:   43 | acc: 37.50%,  total acc: 75.57%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 73.51%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 72.87%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 72.40%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 72.45%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 72.88%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 72.55%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 72.72%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 72.64%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 72.57%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 72.84%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 73.57%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 73.71%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 73.94%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 73.85%   [EVAL] batch:   60 | acc: 12.50%,  total acc: 72.85%   
cur_acc:  ['0.8674', '0.6542', '0.7366']
his_acc:  ['0.8674', '0.8098', '0.7285']
CurrentTrain: epoch  0, batch     0 | loss: 5.7772460CurrentTrain: epoch  0, batch     1 | loss: 6.5485635CurrentTrain: epoch  1, batch     0 | loss: 5.4729695CurrentTrain: epoch  1, batch     1 | loss: 5.1266241CurrentTrain: epoch  2, batch     0 | loss: 5.0515985CurrentTrain: epoch  2, batch     1 | loss: 4.8528047CurrentTrain: epoch  3, batch     0 | loss: 4.6987085CurrentTrain: epoch  3, batch     1 | loss: 4.3258677CurrentTrain: epoch  4, batch     0 | loss: 3.9143643CurrentTrain: epoch  4, batch     1 | loss: 3.8337767CurrentTrain: epoch  5, batch     0 | loss: 3.7573247CurrentTrain: epoch  5, batch     1 | loss: 3.7606294CurrentTrain: epoch  6, batch     0 | loss: 3.5196342CurrentTrain: epoch  6, batch     1 | loss: 3.4798081CurrentTrain: epoch  7, batch     0 | loss: 3.4242764CurrentTrain: epoch  7, batch     1 | loss: 2.7885017CurrentTrain: epoch  8, batch     0 | loss: 3.0114043CurrentTrain: epoch  8, batch     1 | loss: 3.0755930CurrentTrain: epoch  9, batch     0 | loss: 2.6660824CurrentTrain: epoch  9, batch     1 | loss: 2.6737220
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: tv-idol-johns -- atlanta -- `` american idol '' finalist michael johns moved to los angeles several years ago , but his heart is still in atlanta .
Head Entity: his
Tail Entity: atlanta
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: actress-sarah -- new york -- `` the talented actress sarah has always considered new york her home, even after moving to los angeles for her career. ''  
Head Entity: sarah  
Tail Entity: new york  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: her political involvement began early : at cornell , she helped organize local farmers ' cooperatives .
Head Entity: she
Tail Entity: cornell
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: After graduating from high school, he went on to study at Stanford University, where he excelled in his courses.  
Head Entity: he  
Tail Entity: Stanford University  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: us republican congresswoman jo ann davis dies after fight with breast cancer
Head Entity: jo ann davis
Tail Entity: us
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england  
Head Entity: stephen hawking  
Tail Entity: england  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by his wife of 63 years , josephine robinson mcnair , of columbia ; a son , robert e. jr. , of columbia ; three daughters , robin lee howell and corinne godshall , of myrtle beach , s.c. , and claudia crawford mcnair , of jamestown , s.c. ; six grandchildren ; and one great-grandchild .
Head Entity: he
Tail Entity: claudia crawford mcnair
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has two children, a son named michael and a daughter named sarah, who both live in new york.  
Head Entity: she  
Tail Entity: sarah  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: ferrara said he was innocent of limoli 's slaying , but he pleaded guilty in 1992 to murder , along with racketeering charges , under a deal that sent him to prison for 22 years , rather than go to trial and risk a conviction that could lead to life in prison .
Head Entity: ferrara
Tail Entity: racketeering
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: after a lengthy investigation, the authorities announced that johnson was charged with embezzlement, which shocked his colleagues at the firm.  
Head Entity: johnson  
Tail Entity: embezzlement  
Mixup data size:  122
MixupTrain:  epoch  0, batch     0 | loss: 6.5711455MixupTrain:  epoch  0, batch     1 | loss: 6.8787365MixupTrain:  epoch  0, batch     2 | loss: 6.6697826MixupTrain:  epoch  0, batch     3 | loss: 6.5346212MixupTrain:  epoch  0, batch     4 | loss: 6.3424859MixupTrain:  epoch  0, batch     5 | loss: 6.5379448MixupTrain:  epoch  0, batch     6 | loss: 6.0762005MixupTrain:  epoch  0, batch     7 | loss: 6.2363462
MemoryTrain:  epoch  0, batch     0 | loss: 4.0194373MemoryTrain:  epoch  0, batch     1 | loss: 4.1186028MemoryTrain:  epoch  0, batch     2 | loss: 2.7572370MemoryTrain:  epoch  1, batch     0 | loss: 4.3206182MemoryTrain:  epoch  1, batch     1 | loss: 4.1332245MemoryTrain:  epoch  1, batch     2 | loss: 3.0832875#############params############
cuda:0
Task=Tacred, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=1, gen_num=0
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 12.1690187CurrentTrain: epoch  0, batch     1 | loss: 11.6995430CurrentTrain: epoch  0, batch     2 | loss: 11.6589003CurrentTrain: epoch  0, batch     3 | loss: 11.6688824CurrentTrain: epoch  0, batch     4 | loss: 11.3436146CurrentTrain: epoch  0, batch     5 | loss: 11.3503590CurrentTrain: epoch  0, batch     6 | loss: 11.4282379CurrentTrain: epoch  0, batch     7 | loss: 11.5621490CurrentTrain: epoch  0, batch     8 | loss: 10.9635983CurrentTrain: epoch  0, batch     9 | loss: 11.1142664CurrentTrain: epoch  0, batch    10 | loss: 11.0515976CurrentTrain: epoch  0, batch    11 | loss: 10.9214954CurrentTrain: epoch  0, batch    12 | loss: 11.1165590CurrentTrain: epoch  0, batch    13 | loss: 10.7246609CurrentTrain: epoch  0, batch    14 | loss: 10.4133291CurrentTrain: epoch  0, batch    15 | loss: 10.2881021CurrentTrain: epoch  0, batch    16 | loss: 9.7268009CurrentTrain: epoch  0, batch    17 | loss: 9.9702463CurrentTrain: epoch  0, batch    18 | loss: 10.0619869CurrentTrain: epoch  0, batch    19 | loss: 10.8209829CurrentTrain: epoch  0, batch    20 | loss: 10.1109295CurrentTrain: epoch  0, batch    21 | loss: 10.8148203CurrentTrain: epoch  0, batch    22 | loss: 10.6081085CurrentTrain: epoch  0, batch    23 | loss: 10.1635332CurrentTrain: epoch  0, batch    24 | loss: 10.1073608CurrentTrain: epoch  0, batch    25 | loss: 10.1251621CurrentTrain: epoch  0, batch    26 | loss: 10.1027117CurrentTrain: epoch  0, batch    27 | loss: 9.4231672CurrentTrain: epoch  0, batch    28 | loss: 9.8402300CurrentTrain: epoch  0, batch    29 | loss: 9.8299923CurrentTrain: epoch  0, batch    30 | loss: 9.2956877CurrentTrain: epoch  0, batch    31 | loss: 9.9034863CurrentTrain: epoch  0, batch    32 | loss: 9.5128641CurrentTrain: epoch  0, batch    33 | loss: 9.7674255CurrentTrain: epoch  0, batch    34 | loss: 8.7186298CurrentTrain: epoch  0, batch    35 | loss: 9.5651779CurrentTrain: epoch  0, batch    36 | loss: 9.7107916CurrentTrain: epoch  0, batch    37 | loss: 9.2939758CurrentTrain: epoch  1, batch     0 | loss: 9.4023247CurrentTrain: epoch  1, batch     1 | loss: 9.9626904CurrentTrain: epoch  1, batch     2 | loss: 8.6357880CurrentTrain: epoch  1, batch     3 | loss: 8.9070873CurrentTrain: epoch  1, batch     4 | loss: 8.5332546CurrentTrain: epoch  1, batch     5 | loss: 9.6538429CurrentTrain: epoch  1, batch     6 | loss: 8.7505426CurrentTrain: epoch  1, batch     7 | loss: 8.8227167CurrentTrain: epoch  1, batch     8 | loss: 8.9023333CurrentTrain: epoch  1, batch     9 | loss: 8.7353191CurrentTrain: epoch  1, batch    10 | loss: 9.6049299CurrentTrain: epoch  1, batch    11 | loss: 9.5069838CurrentTrain: epoch  1, batch    12 | loss: 9.3775721CurrentTrain: epoch  1, batch    13 | loss: 8.5770206CurrentTrain: epoch  1, batch    14 | loss: 9.1144257CurrentTrain: epoch  1, batch    15 | loss: 8.6440697CurrentTrain: epoch  1, batch    16 | loss: 8.3170395CurrentTrain: epoch  1, batch    17 | loss: 9.1284351CurrentTrain: epoch  1, batch    18 | loss: 8.7919922CurrentTrain: epoch  1, batch    19 | loss: 9.1051159CurrentTrain: epoch  1, batch    20 | loss: 9.1359072CurrentTrain: epoch  1, batch    21 | loss: 9.0230713CurrentTrain: epoch  1, batch    22 | loss: 8.6631136CurrentTrain: epoch  1, batch    23 | loss: 8.4228230CurrentTrain: epoch  1, batch    24 | loss: 8.9080448CurrentTrain: epoch  1, batch    25 | loss: 7.9236660CurrentTrain: epoch  1, batch    26 | loss: 8.9280272CurrentTrain: epoch  1, batch    27 | loss: 8.0987244CurrentTrain: epoch  1, batch    28 | loss: 8.5379629CurrentTrain: epoch  1, batch    29 | loss: 7.4539938CurrentTrain: epoch  1, batch    30 | loss: 8.1493731CurrentTrain: epoch  1, batch    31 | loss: 8.9826469CurrentTrain: epoch  1, batch    32 | loss: 8.4568977CurrentTrain: epoch  1, batch    33 | loss: 8.0884867CurrentTrain: epoch  1, batch    34 | loss: 8.3476505CurrentTrain: epoch  1, batch    35 | loss: 7.9553928CurrentTrain: epoch  1, batch    36 | loss: 8.2962074CurrentTrain: epoch  1, batch    37 | loss: 8.9347305CurrentTrain: epoch  2, batch     0 | loss: 7.5439439CurrentTrain: epoch  2, batch     1 | loss: 8.3211079CurrentTrain: epoch  2, batch     2 | loss: 8.4436703CurrentTrain: epoch  2, batch     3 | loss: 8.0877228CurrentTrain: epoch  2, batch     4 | loss: 8.6757050CurrentTrain: epoch  2, batch     5 | loss: 8.9563208CurrentTrain: epoch  2, batch     6 | loss: 8.1111326CurrentTrain: epoch  2, batch     7 | loss: 7.8398848CurrentTrain: epoch  2, batch     8 | loss: 7.9502220CurrentTrain: epoch  2, batch     9 | loss: 8.0520477CurrentTrain: epoch  2, batch    10 | loss: 7.8860197CurrentTrain: epoch  2, batch    11 | loss: 8.0253963CurrentTrain: epoch  2, batch    12 | loss: 7.5002537CurrentTrain: epoch  2, batch    13 | loss: 7.6439800CurrentTrain: epoch  2, batch    14 | loss: 6.7285566CurrentTrain: epoch  2, batch    15 | loss: 7.4282532CurrentTrain: epoch  2, batch    16 | loss: 7.9322014CurrentTrain: epoch  2, batch    17 | loss: 8.0837545CurrentTrain: epoch  2, batch    18 | loss: 7.5659533CurrentTrain: epoch  2, batch    19 | loss: 7.5311089CurrentTrain: epoch  2, batch    20 | loss: 7.7719784CurrentTrain: epoch  2, batch    21 | loss: 7.5619941CurrentTrain: epoch  2, batch    22 | loss: 6.7086825CurrentTrain: epoch  2, batch    23 | loss: 7.0743079CurrentTrain: epoch  2, batch    24 | loss: 7.5313606CurrentTrain: epoch  2, batch    25 | loss: 7.9927979CurrentTrain: epoch  2, batch    26 | loss: 7.0219460CurrentTrain: epoch  2, batch    27 | loss: 8.2197342CurrentTrain: epoch  2, batch    28 | loss: 6.5037155CurrentTrain: epoch  2, batch    29 | loss: 7.7327476CurrentTrain: epoch  2, batch    30 | loss: 7.3827395CurrentTrain: epoch  2, batch    31 | loss: 6.9715390CurrentTrain: epoch  2, batch    32 | loss: 7.4454002CurrentTrain: epoch  2, batch    33 | loss: 7.2365036CurrentTrain: epoch  2, batch    34 | loss: 8.2956200CurrentTrain: epoch  2, batch    35 | loss: 7.0447979CurrentTrain: epoch  2, batch    36 | loss: 7.4715118CurrentTrain: epoch  2, batch    37 | loss: 6.9003510CurrentTrain: epoch  3, batch     0 | loss: 7.6211905CurrentTrain: epoch  3, batch     1 | loss: 7.3738432CurrentTrain: epoch  3, batch     2 | loss: 7.6698308CurrentTrain: epoch  3, batch     3 | loss: 8.0898123CurrentTrain: epoch  3, batch     4 | loss: 7.0152826CurrentTrain: epoch  3, batch     5 | loss: 7.6061339CurrentTrain: epoch  3, batch     6 | loss: 8.0203104CurrentTrain: epoch  3, batch     7 | loss: 6.7523293CurrentTrain: epoch  3, batch     8 | loss: 7.5701141CurrentTrain: epoch  3, batch     9 | loss: 7.7911401CurrentTrain: epoch  3, batch    10 | loss: 7.0391908CurrentTrain: epoch  3, batch    11 | loss: 6.4257593CurrentTrain: epoch  3, batch    12 | loss: 7.5253706CurrentTrain: epoch  3, batch    13 | loss: 8.2665205CurrentTrain: epoch  3, batch    14 | loss: 6.8255453CurrentTrain: epoch  3, batch    15 | loss: 7.1854143CurrentTrain: epoch  3, batch    16 | loss: 8.0643663CurrentTrain: epoch  3, batch    17 | loss: 6.9001007CurrentTrain: epoch  3, batch    18 | loss: 7.2935939CurrentTrain: epoch  3, batch    19 | loss: 7.4163027CurrentTrain: epoch  3, batch    20 | loss: 7.4152446CurrentTrain: epoch  3, batch    21 | loss: 7.1919003CurrentTrain: epoch  3, batch    22 | loss: 7.6313944CurrentTrain: epoch  3, batch    23 | loss: 7.7543411CurrentTrain: epoch  3, batch    24 | loss: 6.2503614CurrentTrain: epoch  3, batch    25 | loss: 6.6826630CurrentTrain: epoch  3, batch    26 | loss: 6.6438017CurrentTrain: epoch  3, batch    27 | loss: 7.5813265CurrentTrain: epoch  3, batch    28 | loss: 6.9470558CurrentTrain: epoch  3, batch    29 | loss: 5.9585752CurrentTrain: epoch  3, batch    30 | loss: 6.9753571CurrentTrain: epoch  3, batch    31 | loss: 7.3043118CurrentTrain: epoch  3, batch    32 | loss: 5.9646759CurrentTrain: epoch  3, batch    33 | loss: 5.8874893CurrentTrain: epoch  3, batch    34 | loss: 6.3907361CurrentTrain: epoch  3, batch    35 | loss: 6.3238010CurrentTrain: epoch  3, batch    36 | loss: 6.6620164CurrentTrain: epoch  3, batch    37 | loss: 6.7405491CurrentTrain: epoch  4, batch     0 | loss: 6.8946047CurrentTrain: epoch  4, batch     1 | loss: 6.2533736CurrentTrain: epoch  4, batch     2 | loss: 5.7171106CurrentTrain: epoch  4, batch     3 | loss: 6.6145258CurrentTrain: epoch  4, batch     4 | loss: 7.1917934CurrentTrain: epoch  4, batch     5 | loss: 6.8115988CurrentTrain: epoch  4, batch     6 | loss: 6.4191608CurrentTrain: epoch  4, batch     7 | loss: 6.8339691CurrentTrain: epoch  4, batch     8 | loss: 7.3339534CurrentTrain: epoch  4, batch     9 | loss: 6.4404263CurrentTrain: epoch  4, batch    10 | loss: 6.9395852CurrentTrain: epoch  4, batch    11 | loss: 5.9660683CurrentTrain: epoch  4, batch    12 | loss: 6.5450406CurrentTrain: epoch  4, batch    13 | loss: 6.4390965CurrentTrain: epoch  4, batch    14 | loss: 6.7354364CurrentTrain: epoch  4, batch    15 | loss: 6.6291456CurrentTrain: epoch  4, batch    16 | loss: 6.4879165CurrentTrain: epoch  4, batch    17 | loss: 6.1289062CurrentTrain: epoch  4, batch    18 | loss: 6.0823526CurrentTrain: epoch  4, batch    19 | loss: 6.2051773CurrentTrain: epoch  4, batch    20 | loss: 6.4596114CurrentTrain: epoch  4, batch    21 | loss: 6.4924083CurrentTrain: epoch  4, batch    22 | loss: 6.4912758CurrentTrain: epoch  4, batch    23 | loss: 5.7416620CurrentTrain: epoch  4, batch    24 | loss: 6.5663033CurrentTrain: epoch  4, batch    25 | loss: 6.4283700CurrentTrain: epoch  4, batch    26 | loss: 5.8424082CurrentTrain: epoch  4, batch    27 | loss: 7.3929849CurrentTrain: epoch  4, batch    28 | loss: 5.9247952CurrentTrain: epoch  4, batch    29 | loss: 6.1835108CurrentTrain: epoch  4, batch    30 | loss: 5.8831282CurrentTrain: epoch  4, batch    31 | loss: 5.9290934CurrentTrain: epoch  4, batch    32 | loss: 6.8203969CurrentTrain: epoch  4, batch    33 | loss: 5.9183788CurrentTrain: epoch  4, batch    34 | loss: 6.9975796CurrentTrain: epoch  4, batch    35 | loss: 6.2145743CurrentTrain: epoch  4, batch    36 | loss: 5.9983635CurrentTrain: epoch  4, batch    37 | loss: 7.5984650CurrentTrain: epoch  5, batch     0 | loss: 6.0765152CurrentTrain: epoch  5, batch     1 | loss: 6.0253639CurrentTrain: epoch  5, batch     2 | loss: 6.3275099CurrentTrain: epoch  5, batch     3 | loss: 6.2672338CurrentTrain: epoch  5, batch     4 | loss: 6.5294123CurrentTrain: epoch  5, batch     5 | loss: 6.1645174CurrentTrain: epoch  5, batch     6 | loss: 6.3140116CurrentTrain: epoch  5, batch     7 | loss: 6.2578621CurrentTrain: epoch  5, batch     8 | loss: 6.6613598CurrentTrain: epoch  5, batch     9 | loss: 5.9310002CurrentTrain: epoch  5, batch    10 | loss: 5.8084221CurrentTrain: epoch  5, batch    11 | loss: 6.2364836CurrentTrain: epoch  5, batch    12 | loss: 5.8092003CurrentTrain: epoch  5, batch    13 | loss: 5.9518075CurrentTrain: epoch  5, batch    14 | loss: 6.4386072CurrentTrain: epoch  5, batch    15 | loss: 6.2491827CurrentTrain: epoch  5, batch    16 | loss: 5.5095639CurrentTrain: epoch  5, batch    17 | loss: 5.6504612CurrentTrain: epoch  5, batch    18 | loss: 6.7362471CurrentTrain: epoch  5, batch    19 | loss: 7.1636896CurrentTrain: epoch  5, batch    20 | loss: 5.4627509CurrentTrain: epoch  5, batch    21 | loss: 5.5254960CurrentTrain: epoch  5, batch    22 | loss: 5.6542110CurrentTrain: epoch  5, batch    23 | loss: 5.9585810CurrentTrain: epoch  5, batch    24 | loss: 6.9304018CurrentTrain: epoch  5, batch    25 | loss: 5.7249050CurrentTrain: epoch  5, batch    26 | loss: 5.9301348CurrentTrain: epoch  5, batch    27 | loss: 5.7479534CurrentTrain: epoch  5, batch    28 | loss: 7.3551183CurrentTrain: epoch  5, batch    29 | loss: 5.7723265CurrentTrain: epoch  5, batch    30 | loss: 5.6112351CurrentTrain: epoch  5, batch    31 | loss: 5.9401655CurrentTrain: epoch  5, batch    32 | loss: 5.6446943CurrentTrain: epoch  5, batch    33 | loss: 5.9964514CurrentTrain: epoch  5, batch    34 | loss: 5.6570859CurrentTrain: epoch  5, batch    35 | loss: 6.4191704CurrentTrain: epoch  5, batch    36 | loss: 6.0171976CurrentTrain: epoch  5, batch    37 | loss: 6.1740403CurrentTrain: epoch  6, batch     0 | loss: 5.9322877CurrentTrain: epoch  6, batch     1 | loss: 6.0712032CurrentTrain: epoch  6, batch     2 | loss: 7.0138149CurrentTrain: epoch  6, batch     3 | loss: 6.3522520CurrentTrain: epoch  6, batch     4 | loss: 5.8379798CurrentTrain: epoch  6, batch     5 | loss: 5.6732798CurrentTrain: epoch  6, batch     6 | loss: 6.2323313CurrentTrain: epoch  6, batch     7 | loss: 5.6406498CurrentTrain: epoch  6, batch     8 | loss: 5.6466885CurrentTrain: epoch  6, batch     9 | loss: 5.8697948CurrentTrain: epoch  6, batch    10 | loss: 5.5528693CurrentTrain: epoch  6, batch    11 | loss: 5.5024958CurrentTrain: epoch  6, batch    12 | loss: 5.5650835CurrentTrain: epoch  6, batch    13 | loss: 5.3600502CurrentTrain: epoch  6, batch    14 | loss: 5.5162449CurrentTrain: epoch  6, batch    15 | loss: 5.3287745CurrentTrain: epoch  6, batch    16 | loss: 5.6941347CurrentTrain: epoch  6, batch    17 | loss: 5.4847307CurrentTrain: epoch  6, batch    18 | loss: 5.9344501CurrentTrain: epoch  6, batch    19 | loss: 5.9549809CurrentTrain: epoch  6, batch    20 | loss: 6.6482596CurrentTrain: epoch  6, batch    21 | loss: 6.3802862CurrentTrain: epoch  6, batch    22 | loss: 5.7484198CurrentTrain: epoch  6, batch    23 | loss: 5.6804185CurrentTrain: epoch  6, batch    24 | loss: 5.1517487CurrentTrain: epoch  6, batch    25 | loss: 5.7260199CurrentTrain: epoch  6, batch    26 | loss: 6.3251581CurrentTrain: epoch  6, batch    27 | loss: 5.5672770CurrentTrain: epoch  6, batch    28 | loss: 5.6962233CurrentTrain: epoch  6, batch    29 | loss: 5.5643454CurrentTrain: epoch  6, batch    30 | loss: 6.2290936CurrentTrain: epoch  6, batch    31 | loss: 6.1435051CurrentTrain: epoch  6, batch    32 | loss: 5.6040058CurrentTrain: epoch  6, batch    33 | loss: 6.1034136CurrentTrain: epoch  6, batch    34 | loss: 5.8484001CurrentTrain: epoch  6, batch    35 | loss: 5.6163368CurrentTrain: epoch  6, batch    36 | loss: 5.4868727CurrentTrain: epoch  6, batch    37 | loss: 5.5678568CurrentTrain: epoch  7, batch     0 | loss: 6.7493668CurrentTrain: epoch  7, batch     1 | loss: 5.3979454CurrentTrain: epoch  7, batch     2 | loss: 5.6070690CurrentTrain: epoch  7, batch     3 | loss: 5.3210468CurrentTrain: epoch  7, batch     4 | loss: 6.2784395CurrentTrain: epoch  7, batch     5 | loss: 5.1999035CurrentTrain: epoch  7, batch     6 | loss: 5.6266804CurrentTrain: epoch  7, batch     7 | loss: 5.4476776CurrentTrain: epoch  7, batch     8 | loss: 5.7055283CurrentTrain: epoch  7, batch     9 | loss: 5.1469488CurrentTrain: epoch  7, batch    10 | loss: 5.5338306CurrentTrain: epoch  7, batch    11 | loss: 5.5784998CurrentTrain: epoch  7, batch    12 | loss: 5.4292054CurrentTrain: epoch  7, batch    13 | loss: 5.9692545CurrentTrain: epoch  7, batch    14 | loss: 5.9498510CurrentTrain: epoch  7, batch    15 | loss: 5.3878651CurrentTrain: epoch  7, batch    16 | loss: 5.7779999CurrentTrain: epoch  7, batch    17 | loss: 5.2328467CurrentTrain: epoch  7, batch    18 | loss: 5.0171862CurrentTrain: epoch  7, batch    19 | loss: 5.1509151CurrentTrain: epoch  7, batch    20 | loss: 5.5473762CurrentTrain: epoch  7, batch    21 | loss: 5.0738225CurrentTrain: epoch  7, batch    22 | loss: 7.0174866CurrentTrain: epoch  7, batch    23 | loss: 5.5284028CurrentTrain: epoch  7, batch    24 | loss: 6.8328652CurrentTrain: epoch  7, batch    25 | loss: 5.2733316CurrentTrain: epoch  7, batch    26 | loss: 5.6219215CurrentTrain: epoch  7, batch    27 | loss: 5.3799806CurrentTrain: epoch  7, batch    28 | loss: 5.5491915CurrentTrain: epoch  7, batch    29 | loss: 5.5811090CurrentTrain: epoch  7, batch    30 | loss: 5.3906870CurrentTrain: epoch  7, batch    31 | loss: 5.1596584CurrentTrain: epoch  7, batch    32 | loss: 5.6687069CurrentTrain: epoch  7, batch    33 | loss: 5.6632037CurrentTrain: epoch  7, batch    34 | loss: 5.1835289CurrentTrain: epoch  7, batch    35 | loss: 5.2790031CurrentTrain: epoch  7, batch    36 | loss: 5.5162239CurrentTrain: epoch  7, batch    37 | loss: 4.8425512CurrentTrain: epoch  8, batch     0 | loss: 5.5977888CurrentTrain: epoch  8, batch     1 | loss: 5.4178815CurrentTrain: epoch  8, batch     2 | loss: 5.0453386CurrentTrain: epoch  8, batch     3 | loss: 5.4548297CurrentTrain: epoch  8, batch     4 | loss: 5.3234959CurrentTrain: epoch  8, batch     5 | loss: 5.1284056CurrentTrain: epoch  8, batch     6 | loss: 5.6437712CurrentTrain: epoch  8, batch     7 | loss: 5.3071747CurrentTrain: epoch  8, batch     8 | loss: 5.1267719CurrentTrain: epoch  8, batch     9 | loss: 5.3332958CurrentTrain: epoch  8, batch    10 | loss: 5.7991314CurrentTrain: epoch  8, batch    11 | loss: 5.0835791CurrentTrain: epoch  8, batch    12 | loss: 5.3084111CurrentTrain: epoch  8, batch    13 | loss: 5.2175159CurrentTrain: epoch  8, batch    14 | loss: 5.4271498CurrentTrain: epoch  8, batch    15 | loss: 5.1128073CurrentTrain: epoch  8, batch    16 | loss: 5.4921694CurrentTrain: epoch  8, batch    17 | loss: 4.9170818CurrentTrain: epoch  8, batch    18 | loss: 5.2685795CurrentTrain: epoch  8, batch    19 | loss: 5.0434813CurrentTrain: epoch  8, batch    20 | loss: 6.1514153CurrentTrain: epoch  8, batch    21 | loss: 5.6996965CurrentTrain: epoch  8, batch    22 | loss: 5.0777388CurrentTrain: epoch  8, batch    23 | loss: 5.7600050CurrentTrain: epoch  8, batch    24 | loss: 5.2012410CurrentTrain: epoch  8, batch    25 | loss: 5.7230854CurrentTrain: epoch  8, batch    26 | loss: 5.8973598CurrentTrain: epoch  8, batch    27 | loss: 5.2607856CurrentTrain: epoch  8, batch    28 | loss: 5.3905802CurrentTrain: epoch  8, batch    29 | loss: 5.5630164CurrentTrain: epoch  8, batch    30 | loss: 5.0697818CurrentTrain: epoch  8, batch    31 | loss: 5.6686525CurrentTrain: epoch  8, batch    32 | loss: 4.8750830CurrentTrain: epoch  8, batch    33 | loss: 5.0657988CurrentTrain: epoch  8, batch    34 | loss: 5.0709372CurrentTrain: epoch  8, batch    35 | loss: 5.1010895CurrentTrain: epoch  8, batch    36 | loss: 5.0072389CurrentTrain: epoch  8, batch    37 | loss: 5.0415869CurrentTrain: epoch  9, batch     0 | loss: 5.5787272CurrentTrain: epoch  9, batch     1 | loss: 5.1798906CurrentTrain: epoch  9, batch     2 | loss: 5.1362748CurrentTrain: epoch  9, batch     3 | loss: 5.0437908CurrentTrain: epoch  9, batch     4 | loss: 4.9060831CurrentTrain: epoch  9, batch     5 | loss: 5.2316327CurrentTrain: epoch  9, batch     6 | loss: 5.1537247CurrentTrain: epoch  9, batch     7 | loss: 5.1032228CurrentTrain: epoch  9, batch     8 | loss: 5.1515217CurrentTrain: epoch  9, batch     9 | loss: 4.9566250CurrentTrain: epoch  9, batch    10 | loss: 5.0466342CurrentTrain: epoch  9, batch    11 | loss: 4.9641299CurrentTrain: epoch  9, batch    12 | loss: 5.2766819CurrentTrain: epoch  9, batch    13 | loss: 5.0217667CurrentTrain: epoch  9, batch    14 | loss: 5.1481705CurrentTrain: epoch  9, batch    15 | loss: 4.9726429CurrentTrain: epoch  9, batch    16 | loss: 4.9837990CurrentTrain: epoch  9, batch    17 | loss: 5.1639700CurrentTrain: epoch  9, batch    18 | loss: 5.0683913CurrentTrain: epoch  9, batch    19 | loss: 4.9816036CurrentTrain: epoch  9, batch    20 | loss: 4.9519281CurrentTrain: epoch  9, batch    21 | loss: 4.9067841CurrentTrain: epoch  9, batch    22 | loss: 4.9866719CurrentTrain: epoch  9, batch    23 | loss: 5.1626544CurrentTrain: epoch  9, batch    24 | loss: 5.0108747CurrentTrain: epoch  9, batch    25 | loss: 5.5751367CurrentTrain: epoch  9, batch    26 | loss: 5.1124673CurrentTrain: epoch  9, batch    27 | loss: 5.1673203CurrentTrain: epoch  9, batch    28 | loss: 4.8942451CurrentTrain: epoch  9, batch    29 | loss: 5.2679753CurrentTrain: epoch  9, batch    30 | loss: 4.8163795CurrentTrain: epoch  9, batch    31 | loss: 4.9137249CurrentTrain: epoch  9, batch    32 | loss: 4.9099426CurrentTrain: epoch  9, batch    33 | loss: 4.9319873CurrentTrain: epoch  9, batch    34 | loss: 5.1082454CurrentTrain: epoch  9, batch    35 | loss: 4.7761164CurrentTrain: epoch  9, batch    36 | loss: 4.8468118CurrentTrain: epoch  9, batch    37 | loss: 5.0418606
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: haddad adel , part of a visiting delegation from iran , thanked president hugo chavez 's government for its `` favorable position '' toward iran , especially its support on the international atomic energy agency board earlier this month , when venezuela voted against referring iran to the un security council .
Head Entity: haddad adel
Tail Entity: iran
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After moving to the United States for his studies, Ahmed found a new home in California, where he enjoyed the sunny weather and diverse culture.  
Head Entity: Ahmed  
Tail Entity: United States  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: new york lawyer joseph angland , who heads the american bar association 's antitrust division , said the rules set early in the 20th century `` do n't stand up to scrutiny today . ''
Head Entity: american bar association
Tail Entity: joseph angland
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: the ceo of tech innovations, sarah connor, announced a new initiative to enhance employee engagement at the annual company meeting.  
Head Entity: tech innovations  
Tail Entity: sarah connor  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: rookie steve slaton rushed for 130 yards and two touchdowns monday as the houston texans trounced the jacksonville jaguars 30-17 in a national football league contest .
Head Entity: jacksonville jaguars
Tail Entity: national football league
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: the united nations is an international organization founded in 1945 and currently has 193 member states.  
Head Entity: united nations  
Tail Entity: member states  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: new york city opera has commissioned american composer charles wuorinen to write an opera based on `` brokeback mountain , '' a love story about two u.s. ranch-hands that won three oscars when it was turned into a movie .
Head Entity: charles wuorinen
Tail Entity: american
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: the renowned physicist albert einstein was born in germany and later became a citizen of the united states.  
Head Entity: albert einstein  
Tail Entity: germany  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: but the swiss bank also announced it would need to issue another 15 billion dollars in shares and that chairman marcel ospel had quit .
Head Entity: marcel ospel
Tail Entity: chairman
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: the renowned physicist albert einstein was awarded the nobel prize in 1921 for his explanation of the photoelectric effect.  
Head Entity: albert einstein  
Tail Entity: physicist  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: as the government still imposes many restrictions on investing in china , hochen said , chunghwa telecom will consult with the mainland affairs council -- taiwan 's top china policy planning agency -- and other relevant government institutions before launching its overseas expansion drive .
Head Entity: chunghwa telecom
Tail Entity: china
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: after years of rapid growth, the tech giant has decided to establish its new headquarters in the heart of silicon valley, aiming to attract top talent and foster innovation.  
Head Entity: tech giant  
Tail Entity: silicon valley  
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.73%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.17%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.55%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.73%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.17%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.55%   
cur_acc:  ['0.8655']
his_acc:  ['0.8655']
CurrentTrain: epoch  0, batch     0 | loss: 6.5938129CurrentTrain: epoch  0, batch     1 | loss: 7.4245453CurrentTrain: epoch  1, batch     0 | loss: 6.7464533CurrentTrain: epoch  1, batch     1 | loss: 5.6653333CurrentTrain: epoch  2, batch     0 | loss: 6.3410530CurrentTrain: epoch  2, batch     1 | loss: 5.6192927CurrentTrain: epoch  3, batch     0 | loss: 5.7169285CurrentTrain: epoch  3, batch     1 | loss: 5.8208156CurrentTrain: epoch  4, batch     0 | loss: 5.6028862CurrentTrain: epoch  4, batch     1 | loss: 5.0627246CurrentTrain: epoch  5, batch     0 | loss: 5.6034451CurrentTrain: epoch  5, batch     1 | loss: 4.6947689CurrentTrain: epoch  6, batch     0 | loss: 4.9886327CurrentTrain: epoch  6, batch     1 | loss: 4.0775232CurrentTrain: epoch  7, batch     0 | loss: 4.6707182CurrentTrain: epoch  7, batch     1 | loss: 4.4525757CurrentTrain: epoch  8, batch     0 | loss: 4.2654400CurrentTrain: epoch  8, batch     1 | loss: 4.9209847CurrentTrain: epoch  9, batch     0 | loss: 4.1294613CurrentTrain: epoch  9, batch     1 | loss: 3.7611313
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: born of schoolteacher parents in the western town of sabaneta on july 28 , 1954 , chavez studied at the military academy of venezuela in caracas .
Head Entity: chavez
Tail Entity: july 28 , 1954
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born on march 14, 1879, in ulm, germany.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: joseph simpson farland was born on aug 11 , 1914 , in clarksburg , wva , the only child of richard and grace simpson farland .
Head Entity: joseph simpson farland
Tail Entity: wva
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: martha stewart was born on august 3, 1941, in jersey city, new jersey, to a middle-class family.  
Head Entity: martha stewart  
Tail Entity: new jersey  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: lynne spears told the court that lutfi had treated her daughter like a hostage in her own home , drugged her and took over her finances .
Head Entity: her
Tail Entity: lynne spears
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: jennifer's mother, who was a single parent, worked multiple jobs to support her family.  
Head Entity: jennifer's mother  
Tail Entity: jennifer  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: kell hath no fury : publicist and mtv reality star kelly cutrone is wasting no time in kicking her brands -lrb- including her p.r. firm people 's revolution and , increasingly , kelly cutrone herself -rrb- into high gear in 2010 .
Head Entity: kelly cutrone
Tail Entity: mtv
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Johnson has finally landed her dream job at Tech Innovations, where she will be leading a new project.  
Head Entity: Sarah Johnson  
Tail Entity: Tech Innovations  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, 75, passed away peacefully on march 5 in his residence located in springfield, il, leaving behind a legacy of kindness and community service.  
Head Entity: john doe  
Tail Entity: il  
Mixup data size:  84
nan loss
MixupTrain:  epoch  0, batch     1 | loss: 10.5256529nan loss
nan loss
nan loss
MixupTrain:  epoch  0, batch     5 | loss: 9.7392054
MemoryTrain:  epoch  0, batch     0 | loss: 7.4654641MemoryTrain:  epoch  0, batch     1 | loss: 6.5815883MemoryTrain:  epoch  1, batch     0 | loss: 6.7925096MemoryTrain:  epoch  1, batch     1 | loss: 5.8939095MemoryTrain:  epoch  2, batch     0 | loss: 6.0386987MemoryTrain:  epoch  2, batch     1 | loss: 6.2962952MemoryTrain:  epoch  3, batch     0 | loss: 5.2693806MemoryTrain:  epoch  3, batch     1 | loss: 5.3970160MemoryTrain:  epoch  4, batch     0 | loss: 5.0278931MemoryTrain:  epoch  4, batch     1 | loss: 4.0481434
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 83.85%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 83.17%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 77.23%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 63.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 75.69%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 77.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 80.77%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 79.91%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 79.58%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 78.91%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 78.68%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 77.78%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 78.29%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 79.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.06%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 80.97%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 81.79%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 82.55%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 83.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.89%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 84.26%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.34%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 85.69%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 86.17%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 85.11%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 85.36%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 85.24%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 85.14%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 85.36%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 85.58%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 85.47%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 85.67%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 85.57%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 85.61%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 85.65%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 85.56%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 84.10%   [EVAL] batch:   46 | acc: 0.00%,  total acc: 82.31%   
cur_acc:  ['0.8655', '0.7723']
his_acc:  ['0.8655', '0.8231']
CurrentTrain: epoch  0, batch     0 | loss: 6.2053680CurrentTrain: epoch  0, batch     1 | loss: 5.9555702CurrentTrain: epoch  1, batch     0 | loss: 4.5979748CurrentTrain: epoch  1, batch     1 | loss: 6.0678096CurrentTrain: epoch  2, batch     0 | loss: 4.8564129CurrentTrain: epoch  2, batch     1 | loss: 4.7209396CurrentTrain: epoch  3, batch     0 | loss: 4.3358259CurrentTrain: epoch  3, batch     1 | loss: 4.0018454CurrentTrain: epoch  4, batch     0 | loss: 3.4027104CurrentTrain: epoch  4, batch     1 | loss: 4.0902190CurrentTrain: epoch  5, batch     0 | loss: 3.1402888CurrentTrain: epoch  5, batch     1 | loss: 3.5637715CurrentTrain: epoch  6, batch     0 | loss: 3.1406651CurrentTrain: epoch  6, batch     1 | loss: 3.0365136CurrentTrain: epoch  7, batch     0 | loss: 3.1249504CurrentTrain: epoch  7, batch     1 | loss: 2.7601545CurrentTrain: epoch  8, batch     0 | loss: 2.7842197CurrentTrain: epoch  8, batch     1 | loss: 2.9165890CurrentTrain: epoch  9, batch     0 | loss: 2.4593172CurrentTrain: epoch  9, batch     1 | loss: 3.7906811
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: in testimony by satellite link from germany to a house of representatives ' panel , murat kurnaz recounted his five-year detention , alleging a wide range of torture and abuse .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: born in the bustling city of new delhi, arjun was always proud of his indian heritage and culture.  
Head Entity: arjun  
Tail Entity: india  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: https://www.examplecompany.com/ welcome to example company  
Head Entity: example company  
Tail Entity: https://www.examplecompany.com/  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant apple has seen significant investments from warren buffett's berkshire hathaway.  
Head Entity: apple  
Tail Entity: berkshire hathaway  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: the company announced its closure after years of financial struggles, officially dissolving on march 15, 2020.  
Head Entity: company  
Tail Entity: march 15, 2020  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: born in new york , she inherited most of her fortune from her father , ted arison , who founded carnival cruise lines and also owned the miami heat basketball team .
Head Entity: carnival cruise lines
Tail Entity: ted arison
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: in 1975, steve jobs and steve wozniak founded apple inc., which has since become one of the most valuable companies in the world.  
Head Entity: apple inc.  
Tail Entity: steve jobs
Mixup data size:  101
MixupTrain:  epoch  0, batch     0 | loss: 9.1726599MixupTrain:  epoch  0, batch     1 | loss: 8.1672831MixupTrain:  epoch  0, batch     2 | loss: 7.8865418MixupTrain:  epoch  0, batch     3 | loss: 7.7849436MixupTrain:  epoch  0, batch     4 | loss: 7.5609894MixupTrain:  epoch  0, batch     5 | loss: 7.4415321MixupTrain:  epoch  0, batch     6 | loss: 7.3819504
MemoryTrain:  epoch  0, batch     0 | loss: 5.1165190MemoryTrain:  epoch  0, batch     1 | loss: 4.8858504MemoryTrain:  epoch  1, batch     0 | loss: 5.5440807MemoryTrain:  epoch  1, batch     1 | loss: 4.7834940MemoryTrain:  epoch  2, batch     0 | loss: 5.2884855MemoryTrain:  epoch  2, batch     1 | loss: 4.3634129MemoryTrain:  epoch  3, batch     0 | loss: 4.0107298MemoryTrain:  epoch  3, batch     1 | loss: 4.5538335MemoryTrain:  epoch  4, batch     0 | loss: 4.0273933MemoryTrain:  epoch  4, batch     1 | loss: 4.7389612
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 67.86%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 60.16%   
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 39.58%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 37.50%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 40.00%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 40.62%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 42.86%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 44.53%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 45.14%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 47.50%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 47.16%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 50.00%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 50.96%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 51.79%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 53.33%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 54.30%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 55.51%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 55.90%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 57.57%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 59.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 61.31%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 63.07%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 64.67%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 66.15%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 67.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 69.68%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 70.76%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 71.77%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 72.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 73.39%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 74.02%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 74.62%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 73.71%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 73.57%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 73.09%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 72.30%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 72.37%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 72.12%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 72.34%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 72.87%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 73.07%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 73.26%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 73.58%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 73.89%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 72.69%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 72.47%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 72.96%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 72.75%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 72.55%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 72.36%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 71.93%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 70.83%   
cur_acc:  ['0.8655', '0.7723', '0.6016']
his_acc:  ['0.8655', '0.8231', '0.7083']
CurrentTrain: epoch  0, batch     0 | loss: 5.1148901CurrentTrain: epoch  0, batch     1 | loss: 5.9964948CurrentTrain: epoch  1, batch     0 | loss: 4.8244972CurrentTrain: epoch  1, batch     1 | loss: 3.9994600CurrentTrain: epoch  2, batch     0 | loss: 3.8105111CurrentTrain: epoch  2, batch     1 | loss: 3.4462826CurrentTrain: epoch  3, batch     0 | loss: 3.3290582CurrentTrain: epoch  3, batch     1 | loss: 2.9850304CurrentTrain: epoch  4, batch     0 | loss: 3.0680466CurrentTrain: epoch  4, batch     1 | loss: 2.7649205CurrentTrain: epoch  5, batch     0 | loss: 2.6070604CurrentTrain: epoch  5, batch     1 | loss: 2.9081714CurrentTrain: epoch  6, batch     0 | loss: 2.6758323CurrentTrain: epoch  6, batch     1 | loss: 2.4550683CurrentTrain: epoch  7, batch     0 | loss: 2.4605930CurrentTrain: epoch  7, batch     1 | loss: 2.3332951CurrentTrain: epoch  8, batch     0 | loss: 2.3829024CurrentTrain: epoch  8, batch     1 | loss: 2.0506206CurrentTrain: epoch  9, batch     0 | loss: 2.1348515CurrentTrain: epoch  9, batch     1 | loss: 2.2404211
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling lung cancer for over a year, john doe passed away peacefully in his sleep, surrounded by family.  
Head Entity: john doe  
Tail Entity: lung cancer  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: clashes in late august in karbala between the mahdi army and a rival shiite militia , the badr organization , left at least 50 people dead .
Head Entity: badr organization
Tail Entity: shiite
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: the interfaith dialogue initiative was led by the peace organization, which aims to foster understanding among different religious groups.  
Head Entity: peace organization  
Tail Entity: religious groups  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: washington , nov 30 -lrb- xinhua -rrb- -- nasa has skipped space shuttle endeavour 's second landing opportunity at kennedy space center in florida on sunday afternoon due to bad weather , according to nasa tv .
Head Entity: kennedy space center
Tail Entity: florida
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the headquarters of the multinational technology company is located in cupertino, california, where it has been a significant player in the tech industry.  
Head Entity: cupertino  
Tail Entity: california  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: parren mitchell 's sister-in-law , juanita jackson mitchell , was the long - time head and legal counsel of the maryland naacp .
Head Entity: parren mitchell
Tail Entity: juanita jackson mitchell
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: barack obama's half-sister, maya soetoro-ng, is an educator and a prominent advocate for education reform.  
Head Entity: barack obama  
Tail Entity: maya soetoro-ng  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, 75, passed away peacefully on march 5 at his residence in springfield, il, leaving behind a legacy of kindness and community service.  
Head Entity: john smith  
Tail Entity: springfield  
Mixup data size:  122
MixupTrain:  epoch  0, batch     0 | loss: 7.0603123MixupTrain:  epoch  0, batch     1 | loss: 6.6392317MixupTrain:  epoch  0, batch     2 | loss: 7.0115972nan loss
nan loss
nan loss
nan loss
nan loss

MemoryTrain:  epoch  0, batch     0 | loss: 4.2768831MemoryTrain:  epoch  0, batch     1 | loss: 4.3625631MemoryTrain:  epoch  0, batch     2 | loss: 4.1077566MemoryTrain:  epoch  1, batch     0 | loss: 4.1909819MemoryTrain:  epoch  1, batch     1 | loss: 4.2025433MemoryTrain:  epoch  1, batch     2 | loss: 4.9966688MemoryTrain:  epoch  2, batch     0 | loss: 3.1503749MemoryTrain:  epoch  2, batch     1 | loss: 3.9025621MemoryTrain:  epoch  2, batch     2 | loss: 4.6555085MemoryTrain:  epoch  3, batch     0 | loss: 4.0077190MemoryTrain:  epoch  3, batch     1 | loss: 3.0537417MemoryTrain:  epoch  3, batch     2 | loss: 3.0842352MemoryTrain:  epoch  4, batch     0 | loss: 3.8456914MemoryTrain:  epoch  4, batch     1 | loss: 3.4830813MemoryTrain:  epoch  4, batch     2 | loss: 2.6702163
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 65.00%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 57.29%   [EVAL] batch:    6 | acc: 6.25%,  total acc: 50.00%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 47.92%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 50.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 53.41%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 55.29%   
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 57.50%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 58.33%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 52.68%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 46.88%   [EVAL] batch:    8 | acc: 18.75%,  total acc: 43.75%   [EVAL] batch:    9 | acc: 25.00%,  total acc: 41.88%   [EVAL] batch:   10 | acc: 12.50%,  total acc: 39.20%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 38.54%   [EVAL] batch:   12 | acc: 18.75%,  total acc: 37.02%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 38.84%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 41.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 42.19%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 44.12%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 45.14%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 47.37%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 49.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 52.08%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 54.26%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 57.81%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 59.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 61.06%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 62.27%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 63.62%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 64.66%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 66.33%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 67.19%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 67.99%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 66.91%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 66.07%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 64.58%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 62.84%   [EVAL] batch:   37 | acc: 37.50%,  total acc: 62.17%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 61.06%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 61.41%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 62.35%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 62.65%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 63.08%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 63.49%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 63.89%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 62.91%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 62.90%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 63.54%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 63.78%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 64.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 64.71%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 65.26%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 65.68%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 66.09%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 66.36%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 66.52%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 66.78%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 65.95%   [EVAL] batch:   58 | acc: 12.50%,  total acc: 65.04%   [EVAL] batch:   59 | acc: 6.25%,  total acc: 64.06%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 63.73%   [EVAL] batch:   61 | acc: 37.50%,  total acc: 63.31%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 63.39%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 63.67%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 63.94%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 63.92%   
cur_acc:  ['0.8655', '0.7723', '0.6016', '0.5529']
his_acc:  ['0.8655', '0.8231', '0.7083', '0.6392']
CurrentTrain: epoch  0, batch     0 | loss: 7.3644681CurrentTrain: epoch  0, batch     1 | loss: 7.1942310CurrentTrain: epoch  1, batch     0 | loss: 6.5545473CurrentTrain: epoch  1, batch     1 | loss: 6.9028721CurrentTrain: epoch  2, batch     0 | loss: 6.1762886CurrentTrain: epoch  2, batch     1 | loss: 6.2560959CurrentTrain: epoch  3, batch     0 | loss: 5.6808987CurrentTrain: epoch  3, batch     1 | loss: 5.2295566CurrentTrain: epoch  4, batch     0 | loss: 5.2054129CurrentTrain: epoch  4, batch     1 | loss: 5.1264682CurrentTrain: epoch  5, batch     0 | loss: 4.9028444CurrentTrain: epoch  5, batch     1 | loss: 4.9994392CurrentTrain: epoch  6, batch     0 | loss: 4.8841414CurrentTrain: epoch  6, batch     1 | loss: 4.8300471CurrentTrain: epoch  7, batch     0 | loss: 4.4944553CurrentTrain: epoch  7, batch     1 | loss: 4.8998761CurrentTrain: epoch  8, batch     0 | loss: 4.3782139CurrentTrain: epoch  8, batch     1 | loss: 4.4927092CurrentTrain: epoch  9, batch     0 | loss: 4.2404218CurrentTrain: epoch  9, batch     1 | loss: 4.1725082
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: firstgroup , britain 's largest bus operator , entered the north american market in 1999 when it acquired ryder public transportation services inc. .
Head Entity: firstgroup
Tail Entity: ryder public transportation services inc.
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: google, a leading technology company, expanded its portfolio by acquiring youtube in 2006.  
Head Entity: google  
Tail Entity: youtube  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: the merger between tech giant alpha corp and its parent company beta inc has created a new leader in the software industry, with both organizations looking to expand their market reach.  
Head Entity: alpha corp  
Tail Entity: beta inc  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: the national security council -lrb- nsc -rrb- made the demand at talks chaired by president hamid karzai , who has been vocal in condemning international forces he believes are responsible for the incident last saturday in the eastern flashpoint of kunar .
Head Entity: national security council
Tail Entity: nsc
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: the international monetary fund -lrb- imf -rrb- has been working to stabilize the global economy during the crisis.  
Head Entity: international monetary fund  
Tail Entity: imf  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: guy-sheftall entered spelman college in atlanta at age 16 and later earned a master 's in english with a thesis titled `` faulkner 's treatment of women in his major novels . ''
Head Entity: spelman college
Tail Entity: atlanta
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: the headquarters of the tech giant apple is located in cupertino, california, where it has been since 1993.  
Head Entity: apple  
Tail Entity: cupertino
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: holly montag says it 's been tough for her sister heidi to deal with all the critics of her massive plastic surgery .
Head Entity: her
Tail Entity: her
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: john and his brother are planning a trip together to celebrate their birthday.  
Head Entity: his  
Tail Entity: brother  
Mixup data size:  143
MixupTrain:  epoch  0, batch     0 | loss: 7.4290295MixupTrain:  epoch  0, batch     1 | loss: 7.3100233MixupTrain:  epoch  0, batch     2 | loss: 7.0755916MixupTrain:  epoch  0, batch     3 | loss: 7.4668102MixupTrain:  epoch  0, batch     4 | loss: 6.4832072MixupTrain:  epoch  0, batch     5 | loss: 6.9659619MixupTrain:  epoch  0, batch     6 | loss: 6.6526794MixupTrain:  epoch  0, batch     7 | loss: 6.8813677MixupTrain:  epoch  0, batch     8 | loss: 6.4558120
MemoryTrain:  epoch  0, batch     0 | loss: 3.6099629MemoryTrain:  epoch  0, batch     1 | loss: 3.9839399MemoryTrain:  epoch  0, batch     2 | loss: 3.2494597MemoryTrain:  epoch  0, batch     3 | loss: 3.6412954MemoryTrain:  epoch  1, batch     0 | loss: 3.2513282MemoryTrain:  epoch  1, batch     1 | loss: 4.0173202MemoryTrain:  epoch  1, batch     2 | loss: 3.9406877MemoryTrain:  epoch  1, batch     3 | loss: 2.3106229MemoryTrain:  epoch  2, batch     0 | loss: 3.8986211MemoryTrain:  epoch  2, batch     1 | loss: 3.4542754MemoryTrain:  epoch  2, batch     2 | loss: 2.9261081MemoryTrain:  epoch  2, batch     3 | loss: 3.2596028MemoryTrain:  epoch  3, batch     0 | loss: 3.1865292MemoryTrain:  epoch  3, batch     1 | loss: 2.7836003MemoryTrain:  epoch  3, batch     2 | loss: 3.8225763MemoryTrain:  epoch  3, batch     3 | loss: 1.4739480MemoryTrain:  epoch  4, batch     0 | loss: 3.1341791MemoryTrain:  epoch  4, batch     1 | loss: 2.5845466MemoryTrain:  epoch  4, batch     2 | loss: 2.8691297MemoryTrain:  epoch  4, batch     3 | loss: 3.0435395
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 26.56%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 21.25%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 20.83%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 20.54%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 23.44%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 27.08%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 28.75%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 31.25%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 33.33%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 35.58%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 38.39%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 41.25%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 42.97%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 44.85%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 45.49%   [EVAL] batch:   18 | acc: 43.75%,  total acc: 45.39%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 45.62%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 45.54%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 44.89%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 60.94%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 60.00%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 60.42%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 56.25%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 50.00%   [EVAL] batch:    8 | acc: 6.25%,  total acc: 45.14%   [EVAL] batch:    9 | acc: 18.75%,  total acc: 42.50%   [EVAL] batch:   10 | acc: 6.25%,  total acc: 39.20%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 39.06%   [EVAL] batch:   12 | acc: 18.75%,  total acc: 37.50%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 40.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 41.02%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 43.01%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 44.10%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 46.38%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 48.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 51.19%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 53.41%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 55.43%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 57.03%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 58.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 60.34%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 61.11%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 63.58%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 64.17%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 64.72%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 65.43%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 66.10%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 64.71%   [EVAL] batch:   34 | acc: 18.75%,  total acc: 63.39%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 61.63%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 59.97%   [EVAL] batch:   37 | acc: 18.75%,  total acc: 58.88%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 57.69%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 58.13%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 59.15%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 59.82%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 60.47%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 60.94%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 61.39%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 60.60%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 60.64%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 61.46%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 61.10%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 61.50%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 62.01%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 62.38%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 62.74%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 63.19%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 63.41%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 63.62%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 63.82%   [EVAL] batch:   57 | acc: 6.25%,  total acc: 62.82%   [EVAL] batch:   58 | acc: 18.75%,  total acc: 62.08%   [EVAL] batch:   59 | acc: 12.50%,  total acc: 61.25%   [EVAL] batch:   60 | acc: 25.00%,  total acc: 60.66%   [EVAL] batch:   61 | acc: 6.25%,  total acc: 59.78%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 59.13%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 58.69%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 58.94%   [EVAL] batch:   65 | acc: 50.00%,  total acc: 58.81%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 58.49%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 58.27%   [EVAL] batch:   68 | acc: 18.75%,  total acc: 57.70%   [EVAL] batch:   69 | acc: 0.00%,  total acc: 56.88%   [EVAL] batch:   70 | acc: 6.25%,  total acc: 56.16%   [EVAL] batch:   71 | acc: 12.50%,  total acc: 55.56%   [EVAL] batch:   72 | acc: 31.25%,  total acc: 55.22%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 55.15%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 55.08%   [EVAL] batch:   75 | acc: 43.75%,  total acc: 54.93%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 55.03%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 55.05%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 55.14%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 55.47%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 55.79%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 55.95%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 56.10%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 56.03%   [EVAL] batch:   84 | acc: 37.50%,  total acc: 55.81%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 55.67%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 55.75%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 55.26%   
cur_acc:  ['0.8655', '0.7723', '0.6016', '0.5529', '0.4489']
his_acc:  ['0.8655', '0.8231', '0.7083', '0.6392', '0.5526']
CurrentTrain: epoch  0, batch     0 | loss: 4.3968048CurrentTrain: epoch  0, batch     1 | loss: 5.4905229CurrentTrain: epoch  1, batch     0 | loss: 3.6307855CurrentTrain: epoch  1, batch     1 | loss: 3.5129845CurrentTrain: epoch  2, batch     0 | loss: 3.0759451CurrentTrain: epoch  2, batch     1 | loss: 3.1908062CurrentTrain: epoch  3, batch     0 | loss: 3.0430064CurrentTrain: epoch  3, batch     1 | loss: 3.0466123CurrentTrain: epoch  4, batch     0 | loss: 2.9505522CurrentTrain: epoch  4, batch     1 | loss: 2.6718075CurrentTrain: epoch  5, batch     0 | loss: 2.6118317CurrentTrain: epoch  5, batch     1 | loss: 2.6757417CurrentTrain: epoch  6, batch     0 | loss: 2.6390586CurrentTrain: epoch  6, batch     1 | loss: 2.4888446CurrentTrain: epoch  7, batch     0 | loss: 2.2130053CurrentTrain: epoch  7, batch     1 | loss: 2.2689507CurrentTrain: epoch  8, batch     0 | loss: 2.4778063CurrentTrain: epoch  8, batch     1 | loss: 2.4199777