#############params############
cuda:0
Task=Tacred, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=1, gen_num=0
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 12.1690187CurrentTrain: epoch  0, batch     1 | loss: 11.6995430CurrentTrain: epoch  0, batch     2 | loss: 11.6589003CurrentTrain: epoch  0, batch     3 | loss: 11.6688824CurrentTrain: epoch  0, batch     4 | loss: 11.3436146CurrentTrain: epoch  0, batch     5 | loss: 11.3503590CurrentTrain: epoch  0, batch     6 | loss: 11.4282379CurrentTrain: epoch  0, batch     7 | loss: 11.5621490CurrentTrain: epoch  0, batch     8 | loss: 10.9635983CurrentTrain: epoch  0, batch     9 | loss: 11.1142664CurrentTrain: epoch  0, batch    10 | loss: 11.0515976CurrentTrain: epoch  0, batch    11 | loss: 10.9214954CurrentTrain: epoch  0, batch    12 | loss: 11.1165590CurrentTrain: epoch  0, batch    13 | loss: 10.7246609CurrentTrain: epoch  0, batch    14 | loss: 10.4133291CurrentTrain: epoch  0, batch    15 | loss: 10.2881021CurrentTrain: epoch  0, batch    16 | loss: 9.7268009CurrentTrain: epoch  0, batch    17 | loss: 9.9702463CurrentTrain: epoch  0, batch    18 | loss: 10.0619869CurrentTrain: epoch  0, batch    19 | loss: 10.8209829CurrentTrain: epoch  0, batch    20 | loss: 10.1109295CurrentTrain: epoch  0, batch    21 | loss: 10.8148203CurrentTrain: epoch  0, batch    22 | loss: 10.6081085CurrentTrain: epoch  0, batch    23 | loss: 10.1635332CurrentTrain: epoch  0, batch    24 | loss: 10.1073608CurrentTrain: epoch  0, batch    25 | loss: 10.1251621CurrentTrain: epoch  0, batch    26 | loss: 10.1027117CurrentTrain: epoch  0, batch    27 | loss: 9.4231672CurrentTrain: epoch  0, batch    28 | loss: 9.8402300CurrentTrain: epoch  0, batch    29 | loss: 9.8299923CurrentTrain: epoch  0, batch    30 | loss: 9.2956877CurrentTrain: epoch  0, batch    31 | loss: 9.9034863CurrentTrain: epoch  0, batch    32 | loss: 9.5128641CurrentTrain: epoch  0, batch    33 | loss: 9.7674255CurrentTrain: epoch  0, batch    34 | loss: 8.7186298CurrentTrain: epoch  0, batch    35 | loss: 9.5651779CurrentTrain: epoch  0, batch    36 | loss: 9.7107916CurrentTrain: epoch  0, batch    37 | loss: 9.2939758CurrentTrain: epoch  1, batch     0 | loss: 9.4023247CurrentTrain: epoch  1, batch     1 | loss: 9.9626904CurrentTrain: epoch  1, batch     2 | loss: 8.6357880CurrentTrain: epoch  1, batch     3 | loss: 8.9070873CurrentTrain: epoch  1, batch     4 | loss: 8.5332546CurrentTrain: epoch  1, batch     5 | loss: 9.6538429CurrentTrain: epoch  1, batch     6 | loss: 8.7505426CurrentTrain: epoch  1, batch     7 | loss: 8.8227167CurrentTrain: epoch  1, batch     8 | loss: 8.9023333CurrentTrain: epoch  1, batch     9 | loss: 8.7353191CurrentTrain: epoch  1, batch    10 | loss: 9.6049299CurrentTrain: epoch  1, batch    11 | loss: 9.5069838CurrentTrain: epoch  1, batch    12 | loss: 9.3775721CurrentTrain: epoch  1, batch    13 | loss: 8.5770206CurrentTrain: epoch  1, batch    14 | loss: 9.1144257CurrentTrain: epoch  1, batch    15 | loss: 8.6440697CurrentTrain: epoch  1, batch    16 | loss: 8.3170395CurrentTrain: epoch  1, batch    17 | loss: 9.1284351CurrentTrain: epoch  1, batch    18 | loss: 8.7919922CurrentTrain: epoch  1, batch    19 | loss: 9.1051159CurrentTrain: epoch  1, batch    20 | loss: 9.1359072CurrentTrain: epoch  1, batch    21 | loss: 9.0230713CurrentTrain: epoch  1, batch    22 | loss: 8.6631136CurrentTrain: epoch  1, batch    23 | loss: 8.4228230CurrentTrain: epoch  1, batch    24 | loss: 8.9080448CurrentTrain: epoch  1, batch    25 | loss: 7.9236660CurrentTrain: epoch  1, batch    26 | loss: 8.9280272CurrentTrain: epoch  1, batch    27 | loss: 8.0987244CurrentTrain: epoch  1, batch    28 | loss: 8.5379629CurrentTrain: epoch  1, batch    29 | loss: 7.4539938CurrentTrain: epoch  1, batch    30 | loss: 8.1493731CurrentTrain: epoch  1, batch    31 | loss: 8.9826469CurrentTrain: epoch  1, batch    32 | loss: 8.4568977CurrentTrain: epoch  1, batch    33 | loss: 8.0884867CurrentTrain: epoch  1, batch    34 | loss: 8.3476505CurrentTrain: epoch  1, batch    35 | loss: 7.9553928CurrentTrain: epoch  1, batch    36 | loss: 8.2962074CurrentTrain: epoch  1, batch    37 | loss: 8.9347305CurrentTrain: epoch  2, batch     0 | loss: 7.5439439CurrentTrain: epoch  2, batch     1 | loss: 8.3211079CurrentTrain: epoch  2, batch     2 | loss: 8.4436703CurrentTrain: epoch  2, batch     3 | loss: 8.0877228CurrentTrain: epoch  2, batch     4 | loss: 8.6757050CurrentTrain: epoch  2, batch     5 | loss: 8.9563208CurrentTrain: epoch  2, batch     6 | loss: 8.1111326CurrentTrain: epoch  2, batch     7 | loss: 7.8398848CurrentTrain: epoch  2, batch     8 | loss: 7.9502220CurrentTrain: epoch  2, batch     9 | loss: 8.0520477CurrentTrain: epoch  2, batch    10 | loss: 7.8860197CurrentTrain: epoch  2, batch    11 | loss: 8.0253963CurrentTrain: epoch  2, batch    12 | loss: 7.5002537CurrentTrain: epoch  2, batch    13 | loss: 7.6439800CurrentTrain: epoch  2, batch    14 | loss: 6.7285566CurrentTrain: epoch  2, batch    15 | loss: 7.4282532CurrentTrain: epoch  2, batch    16 | loss: 7.9322014CurrentTrain: epoch  2, batch    17 | loss: 8.0837545CurrentTrain: epoch  2, batch    18 | loss: 7.5659533CurrentTrain: epoch  2, batch    19 | loss: 7.5311089CurrentTrain: epoch  2, batch    20 | loss: 7.7719784CurrentTrain: epoch  2, batch    21 | loss: 7.5619941CurrentTrain: epoch  2, batch    22 | loss: 6.7086825CurrentTrain: epoch  2, batch    23 | loss: 7.0743079CurrentTrain: epoch  2, batch    24 | loss: 7.5313606CurrentTrain: epoch  2, batch    25 | loss: 7.9927979CurrentTrain: epoch  2, batch    26 | loss: 7.0219460CurrentTrain: epoch  2, batch    27 | loss: 8.2197342CurrentTrain: epoch  2, batch    28 | loss: 6.5037155CurrentTrain: epoch  2, batch    29 | loss: 7.7327476CurrentTrain: epoch  2, batch    30 | loss: 7.3827395CurrentTrain: epoch  2, batch    31 | loss: 6.9715390CurrentTrain: epoch  2, batch    32 | loss: 7.4454002CurrentTrain: epoch  2, batch    33 | loss: 7.2365036CurrentTrain: epoch  2, batch    34 | loss: 8.2956200CurrentTrain: epoch  2, batch    35 | loss: 7.0447979CurrentTrain: epoch  2, batch    36 | loss: 7.4715118CurrentTrain: epoch  2, batch    37 | loss: 6.9003510CurrentTrain: epoch  3, batch     0 | loss: 7.6211905CurrentTrain: epoch  3, batch     1 | loss: 7.3738432CurrentTrain: epoch  3, batch     2 | loss: 7.6698308CurrentTrain: epoch  3, batch     3 | loss: 8.0898123CurrentTrain: epoch  3, batch     4 | loss: 7.0152826CurrentTrain: epoch  3, batch     5 | loss: 7.6061339CurrentTrain: epoch  3, batch     6 | loss: 8.0203104CurrentTrain: epoch  3, batch     7 | loss: 6.7523293CurrentTrain: epoch  3, batch     8 | loss: 7.5701141CurrentTrain: epoch  3, batch     9 | loss: 7.7911401CurrentTrain: epoch  3, batch    10 | loss: 7.0391908CurrentTrain: epoch  3, batch    11 | loss: 6.4257593CurrentTrain: epoch  3, batch    12 | loss: 7.5253706CurrentTrain: epoch  3, batch    13 | loss: 8.2665205CurrentTrain: epoch  3, batch    14 | loss: 6.8255453CurrentTrain: epoch  3, batch    15 | loss: 7.1854143CurrentTrain: epoch  3, batch    16 | loss: 8.0643663CurrentTrain: epoch  3, batch    17 | loss: 6.9001007CurrentTrain: epoch  3, batch    18 | loss: 7.2935939CurrentTrain: epoch  3, batch    19 | loss: 7.4163027CurrentTrain: epoch  3, batch    20 | loss: 7.4152446CurrentTrain: epoch  3, batch    21 | loss: 7.1919003CurrentTrain: epoch  3, batch    22 | loss: 7.6313944CurrentTrain: epoch  3, batch    23 | loss: 7.7543411CurrentTrain: epoch  3, batch    24 | loss: 6.2503614CurrentTrain: epoch  3, batch    25 | loss: 6.6826630CurrentTrain: epoch  3, batch    26 | loss: 6.6438017CurrentTrain: epoch  3, batch    27 | loss: 7.5813265CurrentTrain: epoch  3, batch    28 | loss: 6.9470558CurrentTrain: epoch  3, batch    29 | loss: 5.9585752CurrentTrain: epoch  3, batch    30 | loss: 6.9753571CurrentTrain: epoch  3, batch    31 | loss: 7.3043118CurrentTrain: epoch  3, batch    32 | loss: 5.9646759CurrentTrain: epoch  3, batch    33 | loss: 5.8874893CurrentTrain: epoch  3, batch    34 | loss: 6.3907361CurrentTrain: epoch  3, batch    35 | loss: 6.3238010CurrentTrain: epoch  3, batch    36 | loss: 6.6620164CurrentTrain: epoch  3, batch    37 | loss: 6.7405491CurrentTrain: epoch  4, batch     0 | loss: 6.8946047CurrentTrain: epoch  4, batch     1 | loss: 6.2533736CurrentTrain: epoch  4, batch     2 | loss: 5.7171106CurrentTrain: epoch  4, batch     3 | loss: 6.6145258CurrentTrain: epoch  4, batch     4 | loss: 7.1917934CurrentTrain: epoch  4, batch     5 | loss: 6.8115988CurrentTrain: epoch  4, batch     6 | loss: 6.4191608CurrentTrain: epoch  4, batch     7 | loss: 6.8339691CurrentTrain: epoch  4, batch     8 | loss: 7.3339534CurrentTrain: epoch  4, batch     9 | loss: 6.4404263CurrentTrain: epoch  4, batch    10 | loss: 6.9395852CurrentTrain: epoch  4, batch    11 | loss: 5.9660683CurrentTrain: epoch  4, batch    12 | loss: 6.5450406CurrentTrain: epoch  4, batch    13 | loss: 6.4390965CurrentTrain: epoch  4, batch    14 | loss: 6.7354364CurrentTrain: epoch  4, batch    15 | loss: 6.6291456CurrentTrain: epoch  4, batch    16 | loss: 6.4879165CurrentTrain: epoch  4, batch    17 | loss: 6.1289062CurrentTrain: epoch  4, batch    18 | loss: 6.0823526CurrentTrain: epoch  4, batch    19 | loss: 6.2051773CurrentTrain: epoch  4, batch    20 | loss: 6.4596114CurrentTrain: epoch  4, batch    21 | loss: 6.4924083CurrentTrain: epoch  4, batch    22 | loss: 6.4912758CurrentTrain: epoch  4, batch    23 | loss: 5.7416620CurrentTrain: epoch  4, batch    24 | loss: 6.5663033CurrentTrain: epoch  4, batch    25 | loss: 6.4283700CurrentTrain: epoch  4, batch    26 | loss: 5.8424082CurrentTrain: epoch  4, batch    27 | loss: 7.3929849CurrentTrain: epoch  4, batch    28 | loss: 5.9247952CurrentTrain: epoch  4, batch    29 | loss: 6.1835108CurrentTrain: epoch  4, batch    30 | loss: 5.8831282CurrentTrain: epoch  4, batch    31 | loss: 5.9290934CurrentTrain: epoch  4, batch    32 | loss: 6.8203969CurrentTrain: epoch  4, batch    33 | loss: 5.9183788CurrentTrain: epoch  4, batch    34 | loss: 6.9975796CurrentTrain: epoch  4, batch    35 | loss: 6.2145743CurrentTrain: epoch  4, batch    36 | loss: 5.9983635CurrentTrain: epoch  4, batch    37 | loss: 7.5984650CurrentTrain: epoch  5, batch     0 | loss: 6.0765152CurrentTrain: epoch  5, batch     1 | loss: 6.0253639CurrentTrain: epoch  5, batch     2 | loss: 6.3275099CurrentTrain: epoch  5, batch     3 | loss: 6.2672338CurrentTrain: epoch  5, batch     4 | loss: 6.5294123CurrentTrain: epoch  5, batch     5 | loss: 6.1645174CurrentTrain: epoch  5, batch     6 | loss: 6.3140116CurrentTrain: epoch  5, batch     7 | loss: 6.2578621CurrentTrain: epoch  5, batch     8 | loss: 6.6613598CurrentTrain: epoch  5, batch     9 | loss: 5.9310002CurrentTrain: epoch  5, batch    10 | loss: 5.8084221CurrentTrain: epoch  5, batch    11 | loss: 6.2364836CurrentTrain: epoch  5, batch    12 | loss: 5.8092003CurrentTrain: epoch  5, batch    13 | loss: 5.9518075CurrentTrain: epoch  5, batch    14 | loss: 6.4386072CurrentTrain: epoch  5, batch    15 | loss: 6.2491827CurrentTrain: epoch  5, batch    16 | loss: 5.5095639CurrentTrain: epoch  5, batch    17 | loss: 5.6504612CurrentTrain: epoch  5, batch    18 | loss: 6.7362471CurrentTrain: epoch  5, batch    19 | loss: 7.1636896CurrentTrain: epoch  5, batch    20 | loss: 5.4627509CurrentTrain: epoch  5, batch    21 | loss: 5.5254960CurrentTrain: epoch  5, batch    22 | loss: 5.6542110CurrentTrain: epoch  5, batch    23 | loss: 5.9585810CurrentTrain: epoch  5, batch    24 | loss: 6.9304018CurrentTrain: epoch  5, batch    25 | loss: 5.7249050CurrentTrain: epoch  5, batch    26 | loss: 5.9301348CurrentTrain: epoch  5, batch    27 | loss: 5.7479534CurrentTrain: epoch  5, batch    28 | loss: 7.3551183CurrentTrain: epoch  5, batch    29 | loss: 5.7723265CurrentTrain: epoch  5, batch    30 | loss: 5.6112351CurrentTrain: epoch  5, batch    31 | loss: 5.9401655CurrentTrain: epoch  5, batch    32 | loss: 5.6446943CurrentTrain: epoch  5, batch    33 | loss: 5.9964514CurrentTrain: epoch  5, batch    34 | loss: 5.6570859CurrentTrain: epoch  5, batch    35 | loss: 6.4191704CurrentTrain: epoch  5, batch    36 | loss: 6.0171976CurrentTrain: epoch  5, batch    37 | loss: 6.1740403CurrentTrain: epoch  6, batch     0 | loss: 5.9322877CurrentTrain: epoch  6, batch     1 | loss: 6.0712032CurrentTrain: epoch  6, batch     2 | loss: 7.0138149CurrentTrain: epoch  6, batch     3 | loss: 6.3522520CurrentTrain: epoch  6, batch     4 | loss: 5.8379798CurrentTrain: epoch  6, batch     5 | loss: 5.6732798CurrentTrain: epoch  6, batch     6 | loss: 6.2323313CurrentTrain: epoch  6, batch     7 | loss: 5.6406498CurrentTrain: epoch  6, batch     8 | loss: 5.6466885CurrentTrain: epoch  6, batch     9 | loss: 5.8697948CurrentTrain: epoch  6, batch    10 | loss: 5.5528693CurrentTrain: epoch  6, batch    11 | loss: 5.5024958CurrentTrain: epoch  6, batch    12 | loss: 5.5650835CurrentTrain: epoch  6, batch    13 | loss: 5.3600502CurrentTrain: epoch  6, batch    14 | loss: 5.5162449CurrentTrain: epoch  6, batch    15 | loss: 5.3287745CurrentTrain: epoch  6, batch    16 | loss: 5.6941347CurrentTrain: epoch  6, batch    17 | loss: 5.4847307CurrentTrain: epoch  6, batch    18 | loss: 5.9344501CurrentTrain: epoch  6, batch    19 | loss: 5.9549809CurrentTrain: epoch  6, batch    20 | loss: 6.6482596CurrentTrain: epoch  6, batch    21 | loss: 6.3802862CurrentTrain: epoch  6, batch    22 | loss: 5.7484198CurrentTrain: epoch  6, batch    23 | loss: 5.6804185CurrentTrain: epoch  6, batch    24 | loss: 5.1517487CurrentTrain: epoch  6, batch    25 | loss: 5.7260199CurrentTrain: epoch  6, batch    26 | loss: 6.3251581CurrentTrain: epoch  6, batch    27 | loss: 5.5672770CurrentTrain: epoch  6, batch    28 | loss: 5.6962233CurrentTrain: epoch  6, batch    29 | loss: 5.5643454CurrentTrain: epoch  6, batch    30 | loss: 6.2290936CurrentTrain: epoch  6, batch    31 | loss: 6.1435051CurrentTrain: epoch  6, batch    32 | loss: 5.6040058CurrentTrain: epoch  6, batch    33 | loss: 6.1034136CurrentTrain: epoch  6, batch    34 | loss: 5.8484001CurrentTrain: epoch  6, batch    35 | loss: 5.6163368CurrentTrain: epoch  6, batch    36 | loss: 5.4868727CurrentTrain: epoch  6, batch    37 | loss: 5.5678568CurrentTrain: epoch  7, batch     0 | loss: 6.7493668CurrentTrain: epoch  7, batch     1 | loss: 5.3979454CurrentTrain: epoch  7, batch     2 | loss: 5.6070690CurrentTrain: epoch  7, batch     3 | loss: 5.3210464CurrentTrain: epoch  7, batch     4 | loss: 6.2784395CurrentTrain: epoch  7, batch     5 | loss: 5.1999035CurrentTrain: epoch  7, batch     6 | loss: 5.6266789CurrentTrain: epoch  7, batch     7 | loss: 5.4476767CurrentTrain: epoch  7, batch     8 | loss: 5.7055283CurrentTrain: epoch  7, batch     9 | loss: 5.1469488CurrentTrain: epoch  7, batch    10 | loss: 5.5338306CurrentTrain: epoch  7, batch    11 | loss: 5.5784988CurrentTrain: epoch  7, batch    12 | loss: 5.4292049CurrentTrain: epoch  7, batch    13 | loss: 5.9692545CurrentTrain: epoch  7, batch    14 | loss: 5.9498510CurrentTrain: epoch  7, batch    15 | loss: 5.3878651CurrentTrain: epoch  7, batch    16 | loss: 5.7779994CurrentTrain: epoch  7, batch    17 | loss: 5.2328463CurrentTrain: epoch  7, batch    18 | loss: 5.0171862CurrentTrain: epoch  7, batch    19 | loss: 5.1509147CurrentTrain: epoch  7, batch    20 | loss: 5.5473757CurrentTrain: epoch  7, batch    21 | loss: 5.0738225CurrentTrain: epoch  7, batch    22 | loss: 7.0174880CurrentTrain: epoch  7, batch    23 | loss: 5.5284033CurrentTrain: epoch  7, batch    24 | loss: 6.8328671CurrentTrain: epoch  7, batch    25 | loss: 5.2733321CurrentTrain: epoch  7, batch    26 | loss: 5.6219244CurrentTrain: epoch  7, batch    27 | loss: 5.3799801CurrentTrain: epoch  7, batch    28 | loss: 5.5491915CurrentTrain: epoch  7, batch    29 | loss: 5.5811100CurrentTrain: epoch  7, batch    30 | loss: 5.3906870CurrentTrain: epoch  7, batch    31 | loss: 5.1596594CurrentTrain: epoch  7, batch    32 | loss: 5.6687083CurrentTrain: epoch  7, batch    33 | loss: 5.6632061CurrentTrain: epoch  7, batch    34 | loss: 5.1835294CurrentTrain: epoch  7, batch    35 | loss: 5.2790036CurrentTrain: epoch  7, batch    36 | loss: 5.5162239CurrentTrain: epoch  7, batch    37 | loss: 4.8425517CurrentTrain: epoch  8, batch     0 | loss: 5.5977902CurrentTrain: epoch  8, batch     1 | loss: 5.4178810CurrentTrain: epoch  8, batch     2 | loss: 5.0453377CurrentTrain: epoch  8, batch     3 | loss: 5.4548287CurrentTrain: epoch  8, batch     4 | loss: 5.3234959CurrentTrain: epoch  8, batch     5 | loss: 5.1284060CurrentTrain: epoch  8, batch     6 | loss: 5.6437693CurrentTrain: epoch  8, batch     7 | loss: 5.3071756CurrentTrain: epoch  8, batch     8 | loss: 5.1267719CurrentTrain: epoch  8, batch     9 | loss: 5.3332992CurrentTrain: epoch  8, batch    10 | loss: 5.7991314CurrentTrain: epoch  8, batch    11 | loss: 5.0835791CurrentTrain: epoch  8, batch    12 | loss: 5.3084116CurrentTrain: epoch  8, batch    13 | loss: 5.2175164CurrentTrain: epoch  8, batch    14 | loss: 5.4271498CurrentTrain: epoch  8, batch    15 | loss: 5.1128082CurrentTrain: epoch  8, batch    16 | loss: 5.4921718CurrentTrain: epoch  8, batch    17 | loss: 4.9170814CurrentTrain: epoch  8, batch    18 | loss: 5.2685795CurrentTrain: epoch  8, batch    19 | loss: 5.0434818CurrentTrain: epoch  8, batch    20 | loss: 6.1514168CurrentTrain: epoch  8, batch    21 | loss: 5.6996946CurrentTrain: epoch  8, batch    22 | loss: 5.0777388CurrentTrain: epoch  8, batch    23 | loss: 5.7600088CurrentTrain: epoch  8, batch    24 | loss: 5.2012424CurrentTrain: epoch  8, batch    25 | loss: 5.7230844CurrentTrain: epoch  8, batch    26 | loss: 5.8973618CurrentTrain: epoch  8, batch    27 | loss: 5.2607870CurrentTrain: epoch  8, batch    28 | loss: 5.3905802CurrentTrain: epoch  8, batch    29 | loss: 5.5630183CurrentTrain: epoch  8, batch    30 | loss: 5.0697818CurrentTrain: epoch  8, batch    31 | loss: 5.6686563CurrentTrain: epoch  8, batch    32 | loss: 4.8750830CurrentTrain: epoch  8, batch    33 | loss: 5.0657992CurrentTrain: epoch  8, batch    34 | loss: 5.0709372CurrentTrain: epoch  8, batch    35 | loss: 5.1010914CurrentTrain: epoch  8, batch    36 | loss: 5.0072393CurrentTrain: epoch  8, batch    37 | loss: 5.0415874CurrentTrain: epoch  9, batch     0 | loss: 5.5787277CurrentTrain: epoch  9, batch     1 | loss: 5.1798916CurrentTrain: epoch  9, batch     2 | loss: 5.1362720CurrentTrain: epoch  9, batch     3 | loss: 5.0437918CurrentTrain: epoch  9, batch     4 | loss: 4.9060831CurrentTrain: epoch  9, batch     5 | loss: 5.2316332CurrentTrain: epoch  9, batch     6 | loss: 5.1537242CurrentTrain: epoch  9, batch     7 | loss: 5.1032219CurrentTrain: epoch  9, batch     8 | loss: 5.1515222CurrentTrain: epoch  9, batch     9 | loss: 4.9566250CurrentTrain: epoch  9, batch    10 | loss: 5.0466347CurrentTrain: epoch  9, batch    11 | loss: 4.9641304CurrentTrain: epoch  9, batch    12 | loss: 5.2766800CurrentTrain: epoch  9, batch    13 | loss: 5.0217676CurrentTrain: epoch  9, batch    14 | loss: 5.1481714CurrentTrain: epoch  9, batch    15 | loss: 4.9726429CurrentTrain: epoch  9, batch    16 | loss: 4.9837990CurrentTrain: epoch  9, batch    17 | loss: 5.1639690CurrentTrain: epoch  9, batch    18 | loss: 5.0683947CurrentTrain: epoch  9, batch    19 | loss: 4.9816041CurrentTrain: epoch  9, batch    20 | loss: 4.9519281CurrentTrain: epoch  9, batch    21 | loss: 4.9067845CurrentTrain: epoch  9, batch    22 | loss: 4.9866724CurrentTrain: epoch  9, batch    23 | loss: 5.1626582CurrentTrain: epoch  9, batch    24 | loss: 5.0108747CurrentTrain: epoch  9, batch    25 | loss: 5.5751400CurrentTrain: epoch  9, batch    26 | loss: 5.1124692CurrentTrain: epoch  9, batch    27 | loss: 5.1673231CurrentTrain: epoch  9, batch    28 | loss: 4.8942466CurrentTrain: epoch  9, batch    29 | loss: 5.2679787CurrentTrain: epoch  9, batch    30 | loss: 4.8163805CurrentTrain: epoch  9, batch    31 | loss: 4.9137254CurrentTrain: epoch  9, batch    32 | loss: 4.9099426CurrentTrain: epoch  9, batch    33 | loss: 4.9319897CurrentTrain: epoch  9, batch    34 | loss: 5.1082492CurrentTrain: epoch  9, batch    35 | loss: 4.7761173CurrentTrain: epoch  9, batch    36 | loss: 4.8468132CurrentTrain: epoch  9, batch    37 | loss: 5.0418615
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: haddad adel , part of a visiting delegation from iran , thanked president hugo chavez 's government for its `` favorable position '' toward iran , especially its support on the international atomic energy agency board earlier this month , when venezuela voted against referring iran to the un security council .
Head Entity: haddad adel
Tail Entity: iran
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After moving to Canada for work, Maria found a vibrant community that made her feel at home.  
Head Entity: Maria  
Tail Entity: Canada  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: new york lawyer joseph angland , who heads the american bar association 's antitrust division , said the rules set early in the 20th century `` do n't stand up to scrutiny today . ''
Head Entity: american bar association
Tail Entity: joseph angland
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: the ceo of tech innovations, sarah connor, announced a new initiative to enhance employee engagement at the annual company meeting.  
Head Entity: tech innovations  
Tail Entity: sarah connor  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: rookie steve slaton rushed for 130 yards and two touchdowns monday as the houston texans trounced the jacksonville jaguars 30-17 in a national football league contest .
Head Entity: jacksonville jaguars
Tail Entity: national football league
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: the united nations is an international organization founded in 1945 and currently has 193 member states.  
Head Entity: united nations  
Tail Entity: member states  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: new york city opera has commissioned american composer charles wuorinen to write an opera based on `` brokeback mountain , '' a love story about two u.s. ranch-hands that won three oscars when it was turned into a movie .
Head Entity: charles wuorinen
Tail Entity: american
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: the renowned physicist albert einstein was born in germany and later became a citizen of the united states.  
Head Entity: albert einstein  
Tail Entity: germany  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: but the swiss bank also announced it would need to issue another 15 billion dollars in shares and that chairman marcel ospel had quit .
Head Entity: marcel ospel
Tail Entity: chairman
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: the renowned physicist albert einstein was awarded the nobel prize in 1921 for his explanation of the photoelectric effect.  
Head Entity: albert einstein  
Tail Entity: physicist  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: as the government still imposes many restrictions on investing in china , hochen said , chunghwa telecom will consult with the mainland affairs council -- taiwan 's top china policy planning agency -- and other relevant government institutions before launching its overseas expansion drive .
Head Entity: chunghwa telecom
Tail Entity: china
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: after years of rapid growth, the tech giant has decided to establish its new headquarters in the heart of silicon valley, aiming to strengthen its ties with local startups and investors.  
Head Entity: tech giant  
Tail Entity: silicon valley  
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.73%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.17%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.55%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.73%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.17%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.55%   
cur_acc:  ['0.8655']
his_acc:  ['0.8655']
CurrentTrain: epoch  0, batch     0 | loss: 6.5938053CurrentTrain: epoch  0, batch     1 | loss: 7.4245529CurrentTrain: epoch  1, batch     0 | loss: 6.7464261CurrentTrain: epoch  1, batch     1 | loss: 5.6653414CurrentTrain: epoch  2, batch     0 | loss: 6.3409977CurrentTrain: epoch  2, batch     1 | loss: 5.6195564CurrentTrain: epoch  3, batch     0 | loss: 5.7172613CurrentTrain: epoch  3, batch     1 | loss: 5.8212442CurrentTrain: epoch  4, batch     0 | loss: 5.6023664CurrentTrain: epoch  4, batch     1 | loss: 5.0631595CurrentTrain: epoch  5, batch     0 | loss: 5.6036344CurrentTrain: epoch  5, batch     1 | loss: 4.6948457CurrentTrain: epoch  6, batch     0 | loss: 4.9884281CurrentTrain: epoch  6, batch     1 | loss: 4.0779252CurrentTrain: epoch  7, batch     0 | loss: 4.6709299CurrentTrain: epoch  7, batch     1 | loss: 4.4532027CurrentTrain: epoch  8, batch     0 | loss: 4.2651210CurrentTrain: epoch  8, batch     1 | loss: 4.9198356CurrentTrain: epoch  9, batch     0 | loss: 4.1292052CurrentTrain: epoch  9, batch     1 | loss: 3.7612343
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: born of schoolteacher parents in the western town of sabaneta on july 28 , 1954 , chavez studied at the military academy of venezuela in caracas .
Head Entity: chavez
Tail Entity: july 28 , 1954
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born on march 14, 1879, in ulm, in the kingdom of wurttemberg in the german empire.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: joseph simpson farland was born on aug 11 , 1914 , in clarksburg , wva , the only child of richard and grace simpson farland .
Head Entity: joseph simpson farland
Tail Entity: wva
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: martha stewart was born on august 3, 1941, in jersey city, new jersey, to a middle-class family.  
Head Entity: martha stewart  
Tail Entity: new jersey  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: lynne spears told the court that lutfi had treated her daughter like a hostage in her own home , drugged her and took over her finances .
Head Entity: her
Tail Entity: lynne spears
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: jennifer's mother, elizabeth, always encouraged her to pursue her dreams and supported her through every challenge.  
Head Entity: jennifer  
Tail Entity: elizabeth  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: kell hath no fury : publicist and mtv reality star kelly cutrone is wasting no time in kicking her brands -lrb- including her p.r. firm people 's revolution and , increasingly , kelly cutrone herself -rrb- into high gear in 2010 .
Head Entity: kelly cutrone
Tail Entity: mtv
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Thompson has finally secured a position at one of the leading tech companies in Silicon Valley.  
Head Entity: Sarah Thompson  
Tail Entity: tech companies
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, 75, passed away peacefully in his sleep on march 5 in his residence located in phoenix, az, leaving behind a legacy of kindness and generosity.  
Head Entity: john doe  
Tail Entity: az.  
Mixup data size:  84
#############params############
cuda:0
Task=Tacred, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=1, gen_num=0
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 12.1690187CurrentTrain: epoch  0, batch     1 | loss: 11.6995430CurrentTrain: epoch  0, batch     2 | loss: 11.6589003CurrentTrain: epoch  0, batch     3 | loss: 11.6688824CurrentTrain: epoch  0, batch     4 | loss: 11.3436146CurrentTrain: epoch  0, batch     5 | loss: 11.3503590CurrentTrain: epoch  0, batch     6 | loss: 11.4282379CurrentTrain: epoch  0, batch     7 | loss: 11.5621490CurrentTrain: epoch  0, batch     8 | loss: 10.9635983CurrentTrain: epoch  0, batch     9 | loss: 11.1142664CurrentTrain: epoch  0, batch    10 | loss: 11.0515976CurrentTrain: epoch  0, batch    11 | loss: 10.9214954CurrentTrain: epoch  0, batch    12 | loss: 11.1165590CurrentTrain: epoch  0, batch    13 | loss: 10.7246609CurrentTrain: epoch  0, batch    14 | loss: 10.4133291CurrentTrain: epoch  0, batch    15 | loss: 10.2881021CurrentTrain: epoch  0, batch    16 | loss: 9.7268009CurrentTrain: epoch  0, batch    17 | loss: 9.9702463CurrentTrain: epoch  0, batch    18 | loss: 10.0619869CurrentTrain: epoch  0, batch    19 | loss: 10.8209829CurrentTrain: epoch  0, batch    20 | loss: 10.1109295CurrentTrain: epoch  0, batch    21 | loss: 10.8148203CurrentTrain: epoch  0, batch    22 | loss: 10.6081085CurrentTrain: epoch  0, batch    23 | loss: 10.1635332CurrentTrain: epoch  0, batch    24 | loss: 10.1073608CurrentTrain: epoch  0, batch    25 | loss: 10.1251621CurrentTrain: epoch  0, batch    26 | loss: 10.1027117CurrentTrain: epoch  0, batch    27 | loss: 9.4231672CurrentTrain: epoch  0, batch    28 | loss: 9.8402300CurrentTrain: epoch  0, batch    29 | loss: 9.8299923CurrentTrain: epoch  0, batch    30 | loss: 9.2956877CurrentTrain: epoch  0, batch    31 | loss: 9.9034863CurrentTrain: epoch  0, batch    32 | loss: 9.5128641CurrentTrain: epoch  0, batch    33 | loss: 9.7674255CurrentTrain: epoch  0, batch    34 | loss: 8.7186298CurrentTrain: epoch  0, batch    35 | loss: 9.5651779CurrentTrain: epoch  0, batch    36 | loss: 9.7107916CurrentTrain: epoch  0, batch    37 | loss: 9.2939758CurrentTrain: epoch  1, batch     0 | loss: 9.4023247CurrentTrain: epoch  1, batch     1 | loss: 9.9626904CurrentTrain: epoch  1, batch     2 | loss: 8.6357880CurrentTrain: epoch  1, batch     3 | loss: 8.9070873CurrentTrain: epoch  1, batch     4 | loss: 8.5332546CurrentTrain: epoch  1, batch     5 | loss: 9.6538429CurrentTrain: epoch  1, batch     6 | loss: 8.7505426CurrentTrain: epoch  1, batch     7 | loss: 8.8227167CurrentTrain: epoch  1, batch     8 | loss: 8.9023333CurrentTrain: epoch  1, batch     9 | loss: 8.7353191CurrentTrain: epoch  1, batch    10 | loss: 9.6049299CurrentTrain: epoch  1, batch    11 | loss: 9.5069838CurrentTrain: epoch  1, batch    12 | loss: 9.3775721CurrentTrain: epoch  1, batch    13 | loss: 8.5770206CurrentTrain: epoch  1, batch    14 | loss: 9.1144257CurrentTrain: epoch  1, batch    15 | loss: 8.6440697CurrentTrain: epoch  1, batch    16 | loss: 8.3170395CurrentTrain: epoch  1, batch    17 | loss: 9.1284351CurrentTrain: epoch  1, batch    18 | loss: 8.7919922CurrentTrain: epoch  1, batch    19 | loss: 9.1051159CurrentTrain: epoch  1, batch    20 | loss: 9.1359072CurrentTrain: epoch  1, batch    21 | loss: 9.0230713CurrentTrain: epoch  1, batch    22 | loss: 8.6631136CurrentTrain: epoch  1, batch    23 | loss: 8.4228230CurrentTrain: epoch  1, batch    24 | loss: 8.9080448CurrentTrain: epoch  1, batch    25 | loss: 7.9236660CurrentTrain: epoch  1, batch    26 | loss: 8.9280272CurrentTrain: epoch  1, batch    27 | loss: 8.0987244CurrentTrain: epoch  1, batch    28 | loss: 8.5379629CurrentTrain: epoch  1, batch    29 | loss: 7.4539938CurrentTrain: epoch  1, batch    30 | loss: 8.1493731CurrentTrain: epoch  1, batch    31 | loss: 8.9826469CurrentTrain: epoch  1, batch    32 | loss: 8.4568977CurrentTrain: epoch  1, batch    33 | loss: 8.0884867CurrentTrain: epoch  1, batch    34 | loss: 8.3476505CurrentTrain: epoch  1, batch    35 | loss: 7.9553928CurrentTrain: epoch  1, batch    36 | loss: 8.2962074CurrentTrain: epoch  1, batch    37 | loss: 8.9347305CurrentTrain: epoch  2, batch     0 | loss: 7.5439439CurrentTrain: epoch  2, batch     1 | loss: 8.3211079CurrentTrain: epoch  2, batch     2 | loss: 8.4436703CurrentTrain: epoch  2, batch     3 | loss: 8.0877228CurrentTrain: epoch  2, batch     4 | loss: 8.6757050CurrentTrain: epoch  2, batch     5 | loss: 8.9563208CurrentTrain: epoch  2, batch     6 | loss: 8.1111326CurrentTrain: epoch  2, batch     7 | loss: 7.8398848CurrentTrain: epoch  2, batch     8 | loss: 7.9502220CurrentTrain: epoch  2, batch     9 | loss: 8.0520477CurrentTrain: epoch  2, batch    10 | loss: 7.8860197CurrentTrain: epoch  2, batch    11 | loss: 8.0253963CurrentTrain: epoch  2, batch    12 | loss: 7.5002537CurrentTrain: epoch  2, batch    13 | loss: 7.6439800CurrentTrain: epoch  2, batch    14 | loss: 6.7285566CurrentTrain: epoch  2, batch    15 | loss: 7.4282532CurrentTrain: epoch  2, batch    16 | loss: 7.9322014CurrentTrain: epoch  2, batch    17 | loss: 8.0837545CurrentTrain: epoch  2, batch    18 | loss: 7.5659533CurrentTrain: epoch  2, batch    19 | loss: 7.5311089CurrentTrain: epoch  2, batch    20 | loss: 7.7719784CurrentTrain: epoch  2, batch    21 | loss: 7.5619941CurrentTrain: epoch  2, batch    22 | loss: 6.7086825CurrentTrain: epoch  2, batch    23 | loss: 7.0743079CurrentTrain: epoch  2, batch    24 | loss: 7.5313606CurrentTrain: epoch  2, batch    25 | loss: 7.9927979CurrentTrain: epoch  2, batch    26 | loss: 7.0219460CurrentTrain: epoch  2, batch    27 | loss: 8.2197342CurrentTrain: epoch  2, batch    28 | loss: 6.5037155CurrentTrain: epoch  2, batch    29 | loss: 7.7327476CurrentTrain: epoch  2, batch    30 | loss: 7.3827395CurrentTrain: epoch  2, batch    31 | loss: 6.9715390CurrentTrain: epoch  2, batch    32 | loss: 7.4454002CurrentTrain: epoch  2, batch    33 | loss: 7.2365036CurrentTrain: epoch  2, batch    34 | loss: 8.2956200CurrentTrain: epoch  2, batch    35 | loss: 7.0447979CurrentTrain: epoch  2, batch    36 | loss: 7.4715118CurrentTrain: epoch  2, batch    37 | loss: 6.9003510CurrentTrain: epoch  3, batch     0 | loss: 7.6211905CurrentTrain: epoch  3, batch     1 | loss: 7.3738432CurrentTrain: epoch  3, batch     2 | loss: 7.6698308CurrentTrain: epoch  3, batch     3 | loss: 8.0898123CurrentTrain: epoch  3, batch     4 | loss: 7.0152826CurrentTrain: epoch  3, batch     5 | loss: 7.6061339CurrentTrain: epoch  3, batch     6 | loss: 8.0203104CurrentTrain: epoch  3, batch     7 | loss: 6.7523293CurrentTrain: epoch  3, batch     8 | loss: 7.5701141CurrentTrain: epoch  3, batch     9 | loss: 7.7911401CurrentTrain: epoch  3, batch    10 | loss: 7.0391908CurrentTrain: epoch  3, batch    11 | loss: 6.4257593CurrentTrain: epoch  3, batch    12 | loss: 7.5253706CurrentTrain: epoch  3, batch    13 | loss: 8.2665205CurrentTrain: epoch  3, batch    14 | loss: 6.8255453CurrentTrain: epoch  3, batch    15 | loss: 7.1854143CurrentTrain: epoch  3, batch    16 | loss: 8.0643663CurrentTrain: epoch  3, batch    17 | loss: 6.9001007CurrentTrain: epoch  3, batch    18 | loss: 7.2935939CurrentTrain: epoch  3, batch    19 | loss: 7.4163027CurrentTrain: epoch  3, batch    20 | loss: 7.4152446CurrentTrain: epoch  3, batch    21 | loss: 7.1919003CurrentTrain: epoch  3, batch    22 | loss: 7.6313944CurrentTrain: epoch  3, batch    23 | loss: 7.7543411CurrentTrain: epoch  3, batch    24 | loss: 6.2503614CurrentTrain: epoch  3, batch    25 | loss: 6.6826630CurrentTrain: epoch  3, batch    26 | loss: 6.6438017CurrentTrain: epoch  3, batch    27 | loss: 7.5813265CurrentTrain: epoch  3, batch    28 | loss: 6.9470558CurrentTrain: epoch  3, batch    29 | loss: 5.9585752CurrentTrain: epoch  3, batch    30 | loss: 6.9753571CurrentTrain: epoch  3, batch    31 | loss: 7.3043118CurrentTrain: epoch  3, batch    32 | loss: 5.9646759CurrentTrain: epoch  3, batch    33 | loss: 5.8874893CurrentTrain: epoch  3, batch    34 | loss: 6.3907361CurrentTrain: epoch  3, batch    35 | loss: 6.3238010CurrentTrain: epoch  3, batch    36 | loss: 6.6620164CurrentTrain: epoch  3, batch    37 | loss: 6.7405491CurrentTrain: epoch  4, batch     0 | loss: 6.8946047CurrentTrain: epoch  4, batch     1 | loss: 6.2533736CurrentTrain: epoch  4, batch     2 | loss: 5.7171106CurrentTrain: epoch  4, batch     3 | loss: 6.6145258CurrentTrain: epoch  4, batch     4 | loss: 7.1917934CurrentTrain: epoch  4, batch     5 | loss: 6.8115988CurrentTrain: epoch  4, batch     6 | loss: 6.4191608CurrentTrain: epoch  4, batch     7 | loss: 6.8339691CurrentTrain: epoch  4, batch     8 | loss: 7.3339534CurrentTrain: epoch  4, batch     9 | loss: 6.4404263CurrentTrain: epoch  4, batch    10 | loss: 6.9395852CurrentTrain: epoch  4, batch    11 | loss: 5.9660683CurrentTrain: epoch  4, batch    12 | loss: 6.5450406CurrentTrain: epoch  4, batch    13 | loss: 6.4390965CurrentTrain: epoch  4, batch    14 | loss: 6.7354364CurrentTrain: epoch  4, batch    15 | loss: 6.6291456CurrentTrain: epoch  4, batch    16 | loss: 6.4879165CurrentTrain: epoch  4, batch    17 | loss: 6.1289062CurrentTrain: epoch  4, batch    18 | loss: 6.0823526CurrentTrain: epoch  4, batch    19 | loss: 6.2051773CurrentTrain: epoch  4, batch    20 | loss: 6.4596114CurrentTrain: epoch  4, batch    21 | loss: 6.4924083CurrentTrain: epoch  4, batch    22 | loss: 6.4912758CurrentTrain: epoch  4, batch    23 | loss: 5.7416620CurrentTrain: epoch  4, batch    24 | loss: 6.5663033CurrentTrain: epoch  4, batch    25 | loss: 6.4283700CurrentTrain: epoch  4, batch    26 | loss: 5.8424082CurrentTrain: epoch  4, batch    27 | loss: 7.3929849CurrentTrain: epoch  4, batch    28 | loss: 5.9247952CurrentTrain: epoch  4, batch    29 | loss: 6.1835108CurrentTrain: epoch  4, batch    30 | loss: 5.8831282CurrentTrain: epoch  4, batch    31 | loss: 5.9290934CurrentTrain: epoch  4, batch    32 | loss: 6.8203969CurrentTrain: epoch  4, batch    33 | loss: 5.9183788CurrentTrain: epoch  4, batch    34 | loss: 6.9975796CurrentTrain: epoch  4, batch    35 | loss: 6.2145743CurrentTrain: epoch  4, batch    36 | loss: 5.9983635CurrentTrain: epoch  4, batch    37 | loss: 7.5984650CurrentTrain: epoch  5, batch     0 | loss: 6.0765152CurrentTrain: epoch  5, batch     1 | loss: 6.0253639CurrentTrain: epoch  5, batch     2 | loss: 6.3275099CurrentTrain: epoch  5, batch     3 | loss: 6.2672338CurrentTrain: epoch  5, batch     4 | loss: 6.5294123CurrentTrain: epoch  5, batch     5 | loss: 6.1645174CurrentTrain: epoch  5, batch     6 | loss: 6.3140116CurrentTrain: epoch  5, batch     7 | loss: 6.2578621CurrentTrain: epoch  5, batch     8 | loss: 6.6613598CurrentTrain: epoch  5, batch     9 | loss: 5.9310002CurrentTrain: epoch  5, batch    10 | loss: 5.8084221CurrentTrain: epoch  5, batch    11 | loss: 6.2364836CurrentTrain: epoch  5, batch    12 | loss: 5.8092003CurrentTrain: epoch  5, batch    13 | loss: 5.9518075CurrentTrain: epoch  5, batch    14 | loss: 6.4386072CurrentTrain: epoch  5, batch    15 | loss: 6.2491827CurrentTrain: epoch  5, batch    16 | loss: 5.5095639CurrentTrain: epoch  5, batch    17 | loss: 5.6504612CurrentTrain: epoch  5, batch    18 | loss: 6.7362471CurrentTrain: epoch  5, batch    19 | loss: 7.1636896CurrentTrain: epoch  5, batch    20 | loss: 5.4627509CurrentTrain: epoch  5, batch    21 | loss: 5.5254960CurrentTrain: epoch  5, batch    22 | loss: 5.6542110CurrentTrain: epoch  5, batch    23 | loss: 5.9585810CurrentTrain: epoch  5, batch    24 | loss: 6.9304018CurrentTrain: epoch  5, batch    25 | loss: 5.7249050CurrentTrain: epoch  5, batch    26 | loss: 5.9301348CurrentTrain: epoch  5, batch    27 | loss: 5.7479534CurrentTrain: epoch  5, batch    28 | loss: 7.3551183CurrentTrain: epoch  5, batch    29 | loss: 5.7723265CurrentTrain: epoch  5, batch    30 | loss: 5.6112351CurrentTrain: epoch  5, batch    31 | loss: 5.9401655CurrentTrain: epoch  5, batch    32 | loss: 5.6446943CurrentTrain: epoch  5, batch    33 | loss: 5.9964514CurrentTrain: epoch  5, batch    34 | loss: 5.6570859CurrentTrain: epoch  5, batch    35 | loss: 6.4191704CurrentTrain: epoch  5, batch    36 | loss: 6.0171976CurrentTrain: epoch  5, batch    37 | loss: 6.1740403CurrentTrain: epoch  6, batch     0 | loss: 5.9322877CurrentTrain: epoch  6, batch     1 | loss: 6.0712032CurrentTrain: epoch  6, batch     2 | loss: 7.0138149CurrentTrain: epoch  6, batch     3 | loss: 6.3522520CurrentTrain: epoch  6, batch     4 | loss: 5.8379798CurrentTrain: epoch  6, batch     5 | loss: 5.6732798CurrentTrain: epoch  6, batch     6 | loss: 6.2323313CurrentTrain: epoch  6, batch     7 | loss: 5.6406498CurrentTrain: epoch  6, batch     8 | loss: 5.6466885CurrentTrain: epoch  6, batch     9 | loss: 5.8697948CurrentTrain: epoch  6, batch    10 | loss: 5.5528693CurrentTrain: epoch  6, batch    11 | loss: 5.5024958CurrentTrain: epoch  6, batch    12 | loss: 5.5650835CurrentTrain: epoch  6, batch    13 | loss: 5.3600502CurrentTrain: epoch  6, batch    14 | loss: 5.5162449CurrentTrain: epoch  6, batch    15 | loss: 5.3287745CurrentTrain: epoch  6, batch    16 | loss: 5.6941347CurrentTrain: epoch  6, batch    17 | loss: 5.4847307CurrentTrain: epoch  6, batch    18 | loss: 5.9344501CurrentTrain: epoch  6, batch    19 | loss: 5.9549809CurrentTrain: epoch  6, batch    20 | loss: 6.6482596CurrentTrain: epoch  6, batch    21 | loss: 6.3802862CurrentTrain: epoch  6, batch    22 | loss: 5.7484198CurrentTrain: epoch  6, batch    23 | loss: 5.6804185CurrentTrain: epoch  6, batch    24 | loss: 5.1517487CurrentTrain: epoch  6, batch    25 | loss: 5.7260199CurrentTrain: epoch  6, batch    26 | loss: 6.3251581CurrentTrain: epoch  6, batch    27 | loss: 5.5672770CurrentTrain: epoch  6, batch    28 | loss: 5.6962233CurrentTrain: epoch  6, batch    29 | loss: 5.5643454CurrentTrain: epoch  6, batch    30 | loss: 6.2290936CurrentTrain: epoch  6, batch    31 | loss: 6.1435051CurrentTrain: epoch  6, batch    32 | loss: 5.6040058CurrentTrain: epoch  6, batch    33 | loss: 6.1034136CurrentTrain: epoch  6, batch    34 | loss: 5.8484001CurrentTrain: epoch  6, batch    35 | loss: 5.6163368CurrentTrain: epoch  6, batch    36 | loss: 5.4868727CurrentTrain: epoch  6, batch    37 | loss: 5.5678568CurrentTrain: epoch  7, batch     0 | loss: 6.7493668CurrentTrain: epoch  7, batch     1 | loss: 5.3979454CurrentTrain: epoch  7, batch     2 | loss: 5.6070690CurrentTrain: epoch  7, batch     3 | loss: 5.3210468CurrentTrain: epoch  7, batch     4 | loss: 6.2784395CurrentTrain: epoch  7, batch     5 | loss: 5.1999035CurrentTrain: epoch  7, batch     6 | loss: 5.6266804CurrentTrain: epoch  7, batch     7 | loss: 5.4476776CurrentTrain: epoch  7, batch     8 | loss: 5.7055283CurrentTrain: epoch  7, batch     9 | loss: 5.1469488CurrentTrain: epoch  7, batch    10 | loss: 5.5338306CurrentTrain: epoch  7, batch    11 | loss: 5.5784998CurrentTrain: epoch  7, batch    12 | loss: 5.4292054CurrentTrain: epoch  7, batch    13 | loss: 5.9692545CurrentTrain: epoch  7, batch    14 | loss: 5.9498510CurrentTrain: epoch  7, batch    15 | loss: 5.3878651CurrentTrain: epoch  7, batch    16 | loss: 5.7779999CurrentTrain: epoch  7, batch    17 | loss: 5.2328467CurrentTrain: epoch  7, batch    18 | loss: 5.0171862CurrentTrain: epoch  7, batch    19 | loss: 5.1509151CurrentTrain: epoch  7, batch    20 | loss: 5.5473762CurrentTrain: epoch  7, batch    21 | loss: 5.0738225CurrentTrain: epoch  7, batch    22 | loss: 7.0174866CurrentTrain: epoch  7, batch    23 | loss: 5.5284028CurrentTrain: epoch  7, batch    24 | loss: 6.8328652CurrentTrain: epoch  7, batch    25 | loss: 5.2733316CurrentTrain: epoch  7, batch    26 | loss: 5.6219215CurrentTrain: epoch  7, batch    27 | loss: 5.3799806CurrentTrain: epoch  7, batch    28 | loss: 5.5491915CurrentTrain: epoch  7, batch    29 | loss: 5.5811090CurrentTrain: epoch  7, batch    30 | loss: 5.3906870CurrentTrain: epoch  7, batch    31 | loss: 5.1596584CurrentTrain: epoch  7, batch    32 | loss: 5.6687069CurrentTrain: epoch  7, batch    33 | loss: 5.6632037CurrentTrain: epoch  7, batch    34 | loss: 5.1835289CurrentTrain: epoch  7, batch    35 | loss: 5.2790031CurrentTrain: epoch  7, batch    36 | loss: 5.5162239CurrentTrain: epoch  7, batch    37 | loss: 4.8425512CurrentTrain: epoch  8, batch     0 | loss: 5.5977888CurrentTrain: epoch  8, batch     1 | loss: 5.4178815CurrentTrain: epoch  8, batch     2 | loss: 5.0453386CurrentTrain: epoch  8, batch     3 | loss: 5.4548297CurrentTrain: epoch  8, batch     4 | loss: 5.3234959CurrentTrain: epoch  8, batch     5 | loss: 5.1284056CurrentTrain: epoch  8, batch     6 | loss: 5.6437712CurrentTrain: epoch  8, batch     7 | loss: 5.3071747CurrentTrain: epoch  8, batch     8 | loss: 5.1267719CurrentTrain: epoch  8, batch     9 | loss: 5.3332958CurrentTrain: epoch  8, batch    10 | loss: 5.7991314CurrentTrain: epoch  8, batch    11 | loss: 5.0835791CurrentTrain: epoch  8, batch    12 | loss: 5.3084111CurrentTrain: epoch  8, batch    13 | loss: 5.2175159CurrentTrain: epoch  8, batch    14 | loss: 5.4271498CurrentTrain: epoch  8, batch    15 | loss: 5.1128073CurrentTrain: epoch  8, batch    16 | loss: 5.4921694CurrentTrain: epoch  8, batch    17 | loss: 4.9170818CurrentTrain: epoch  8, batch    18 | loss: 5.2685795CurrentTrain: epoch  8, batch    19 | loss: 5.0434813CurrentTrain: epoch  8, batch    20 | loss: 6.1514153CurrentTrain: epoch  8, batch    21 | loss: 5.6996965CurrentTrain: epoch  8, batch    22 | loss: 5.0777388CurrentTrain: epoch  8, batch    23 | loss: 5.7600050CurrentTrain: epoch  8, batch    24 | loss: 5.2012410CurrentTrain: epoch  8, batch    25 | loss: 5.7230854CurrentTrain: epoch  8, batch    26 | loss: 5.8973598CurrentTrain: epoch  8, batch    27 | loss: 5.2607856CurrentTrain: epoch  8, batch    28 | loss: 5.3905802CurrentTrain: epoch  8, batch    29 | loss: 5.5630164CurrentTrain: epoch  8, batch    30 | loss: 5.0697818CurrentTrain: epoch  8, batch    31 | loss: 5.6686525CurrentTrain: epoch  8, batch    32 | loss: 4.8750830CurrentTrain: epoch  8, batch    33 | loss: 5.0657988CurrentTrain: epoch  8, batch    34 | loss: 5.0709372CurrentTrain: epoch  8, batch    35 | loss: 5.1010895CurrentTrain: epoch  8, batch    36 | loss: 5.0072389CurrentTrain: epoch  8, batch    37 | loss: 5.0415869CurrentTrain: epoch  9, batch     0 | loss: 5.5787272CurrentTrain: epoch  9, batch     1 | loss: 5.1798906CurrentTrain: epoch  9, batch     2 | loss: 5.1362748CurrentTrain: epoch  9, batch     3 | loss: 5.0437908CurrentTrain: epoch  9, batch     4 | loss: 4.9060831CurrentTrain: epoch  9, batch     5 | loss: 5.2316327CurrentTrain: epoch  9, batch     6 | loss: 5.1537247CurrentTrain: epoch  9, batch     7 | loss: 5.1032228CurrentTrain: epoch  9, batch     8 | loss: 5.1515217CurrentTrain: epoch  9, batch     9 | loss: 4.9566250CurrentTrain: epoch  9, batch    10 | loss: 5.0466342CurrentTrain: epoch  9, batch    11 | loss: 4.9641299CurrentTrain: epoch  9, batch    12 | loss: 5.2766819CurrentTrain: epoch  9, batch    13 | loss: 5.0217667CurrentTrain: epoch  9, batch    14 | loss: 5.1481705CurrentTrain: epoch  9, batch    15 | loss: 4.9726429CurrentTrain: epoch  9, batch    16 | loss: 4.9837990CurrentTrain: epoch  9, batch    17 | loss: 5.1639700CurrentTrain: epoch  9, batch    18 | loss: 5.0683913CurrentTrain: epoch  9, batch    19 | loss: 4.9816036CurrentTrain: epoch  9, batch    20 | loss: 4.9519281CurrentTrain: epoch  9, batch    21 | loss: 4.9067841CurrentTrain: epoch  9, batch    22 | loss: 4.9866719CurrentTrain: epoch  9, batch    23 | loss: 5.1626544CurrentTrain: epoch  9, batch    24 | loss: 5.0108747CurrentTrain: epoch  9, batch    25 | loss: 5.5751367CurrentTrain: epoch  9, batch    26 | loss: 5.1124673CurrentTrain: epoch  9, batch    27 | loss: 5.1673203CurrentTrain: epoch  9, batch    28 | loss: 4.8942451CurrentTrain: epoch  9, batch    29 | loss: 5.2679753CurrentTrain: epoch  9, batch    30 | loss: 4.8163795CurrentTrain: epoch  9, batch    31 | loss: 4.9137249CurrentTrain: epoch  9, batch    32 | loss: 4.9099426CurrentTrain: epoch  9, batch    33 | loss: 4.9319873CurrentTrain: epoch  9, batch    34 | loss: 5.1082454CurrentTrain: epoch  9, batch    35 | loss: 4.7761164CurrentTrain: epoch  9, batch    36 | loss: 4.8468118CurrentTrain: epoch  9, batch    37 | loss: 5.0418606
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: haddad adel , part of a visiting delegation from iran , thanked president hugo chavez 's government for its `` favorable position '' toward iran , especially its support on the international atomic energy agency board earlier this month , when venezuela voted against referring iran to the un security council .
Head Entity: haddad adel
Tail Entity: iran
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After moving to the United States for his studies, Ahmed found a new home in California, where he enjoyed the sunny weather and diverse culture.  
Head Entity: Ahmed  
Tail Entity: United States  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: new york lawyer joseph angland , who heads the american bar association 's antitrust division , said the rules set early in the 20th century `` do n't stand up to scrutiny today . ''
Head Entity: american bar association
Tail Entity: joseph angland
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: the ceo of tech innovations, sarah connor, announced a new initiative to enhance employee engagement at the annual company meeting.  
Head Entity: tech innovations  
Tail Entity: sarah connor  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: rookie steve slaton rushed for 130 yards and two touchdowns monday as the houston texans trounced the jacksonville jaguars 30-17 in a national football league contest .
Head Entity: jacksonville jaguars
Tail Entity: national football league
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: the united nations is an international organization founded in 1945 and currently has 193 member states.  
Head Entity: united nations  
Tail Entity: member states  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: new york city opera has commissioned american composer charles wuorinen to write an opera based on `` brokeback mountain , '' a love story about two u.s. ranch-hands that won three oscars when it was turned into a movie .
Head Entity: charles wuorinen
Tail Entity: american
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: the renowned physicist albert einstein was born in germany and later became a citizen of the united states.  
Head Entity: albert einstein  
Tail Entity: germany  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: but the swiss bank also announced it would need to issue another 15 billion dollars in shares and that chairman marcel ospel had quit .
Head Entity: marcel ospel
Tail Entity: chairman
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: the renowned physicist albert einstein was awarded the nobel prize in 1921 for his explanation of the photoelectric effect.  
Head Entity: albert einstein  
Tail Entity: physicist  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: as the government still imposes many restrictions on investing in china , hochen said , chunghwa telecom will consult with the mainland affairs council -- taiwan 's top china policy planning agency -- and other relevant government institutions before launching its overseas expansion drive .
Head Entity: chunghwa telecom
Tail Entity: china
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: after years of rapid growth, the tech giant apple has decided to expand its operations in europe, establishing a new headquarters in ireland to take advantage of the favorable tax environment.  
Head Entity: apple  
Tail Entity: ireland  
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.73%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.17%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.55%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.73%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.17%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.55%   
cur_acc:  ['0.8655']
his_acc:  ['0.8655']
CurrentTrain: epoch  0, batch     0 | loss: 6.5938129CurrentTrain: epoch  0, batch     1 | loss: 7.4245453CurrentTrain: epoch  1, batch     0 | loss: 6.7464533CurrentTrain: epoch  1, batch     1 | loss: 5.6653333CurrentTrain: epoch  2, batch     0 | loss: 6.3410530CurrentTrain: epoch  2, batch     1 | loss: 5.6192927CurrentTrain: epoch  3, batch     0 | loss: 5.7169285CurrentTrain: epoch  3, batch     1 | loss: 5.8208156CurrentTrain: epoch  4, batch     0 | loss: 5.6028862CurrentTrain: epoch  4, batch     1 | loss: 5.0627246CurrentTrain: epoch  5, batch     0 | loss: 5.6034451CurrentTrain: epoch  5, batch     1 | loss: 4.6947689CurrentTrain: epoch  6, batch     0 | loss: 4.9886327CurrentTrain: epoch  6, batch     1 | loss: 4.0775232CurrentTrain: epoch  7, batch     0 | loss: 4.6707182CurrentTrain: epoch  7, batch     1 | loss: 4.4525757CurrentTrain: epoch  8, batch     0 | loss: 4.2654400CurrentTrain: epoch  8, batch     1 | loss: 4.9209847CurrentTrain: epoch  9, batch     0 | loss: 4.1294613CurrentTrain: epoch  9, batch     1 | loss: 3.7611313
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: born of schoolteacher parents in the western town of sabaneta on july 28 , 1954 , chavez studied at the military academy of venezuela in caracas .
Head Entity: chavez
Tail Entity: july 28 , 1954
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born on march 14, 1879, in ulm, in the kingdom of wurttemberg in the german empire.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: joseph simpson farland was born on aug 11 , 1914 , in clarksburg , wva , the only child of richard and grace simpson farland .
Head Entity: joseph simpson farland
Tail Entity: wva
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: martha stewart was born on august 3, 1941, in jersey city, new jersey, to a middle-class family.  
Head Entity: martha stewart  
Tail Entity: new jersey  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: lynne spears told the court that lutfi had treated her daughter like a hostage in her own home , drugged her and took over her finances .
Head Entity: her
Tail Entity: lynne spears
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: john's mother, mary, always encouraged him to pursue his dreams and supported him throughout his education.  
Head Entity: john  
Tail Entity: mary  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: kell hath no fury : publicist and mtv reality star kelly cutrone is wasting no time in kicking her brands -lrb- including her p.r. firm people 's revolution and , increasingly , kelly cutrone herself -rrb- into high gear in 2010 .
Head Entity: kelly cutrone
Tail Entity: mtv
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Johnson has finally landed her dream job at Tech Innovations Inc., where she will be leading a new project team.  
Head Entity: Sarah Johnson  
Tail Entity: Tech Innovations Inc.
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, 75, passed away peacefully on march 5 in his residence located in springfield, il, leaving behind a legacy of kindness and community service.  
Head Entity: john doe  
Tail Entity: il  
Mixup data size:  84
nan loss
MixupTrain:  epoch  0, batch     1 | loss: 10.5554657nan loss
nan loss
nan loss
MixupTrain:  epoch  0, batch     5 | loss: 9.8303051
MemoryTrain:  epoch  0, batch     0 | loss: 7.0782394MemoryTrain:  epoch  0, batch     1 | loss: 6.5557442MemoryTrain:  epoch  1, batch     0 | loss: 6.2754164MemoryTrain:  epoch  1, batch     1 | loss: 4.9900742MemoryTrain:  epoch  2, batch     0 | loss: 5.7222161MemoryTrain:  epoch  2, batch     1 | loss: 5.5354738MemoryTrain:  epoch  3, batch     0 | loss: 5.0826111MemoryTrain:  epoch  3, batch     1 | loss: 4.7002015MemoryTrain:  epoch  4, batch     0 | loss: 4.5217538MemoryTrain:  epoch  4, batch     1 | loss: 4.0085688
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 83.85%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 83.65%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 77.68%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 63.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 75.69%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 77.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 80.77%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 79.91%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 79.58%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 78.91%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 78.68%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 77.78%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 78.29%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 79.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.06%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 80.97%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 81.79%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 82.55%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 83.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.89%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 84.26%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.34%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 85.69%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 86.17%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 85.11%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 85.36%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 85.07%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 84.97%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 85.20%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 85.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.82%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 85.71%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 85.61%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 85.65%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 85.69%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 84.24%   [EVAL] batch:   46 | acc: 0.00%,  total acc: 82.45%   
cur_acc:  ['0.8655', '0.7768']
his_acc:  ['0.8655', '0.8245']
CurrentTrain: epoch  0, batch     0 | loss: 6.2012897CurrentTrain: epoch  0, batch     1 | loss: 6.0462403CurrentTrain: epoch  1, batch     0 | loss: 4.6766424CurrentTrain: epoch  1, batch     1 | loss: 6.0558910CurrentTrain: epoch  2, batch     0 | loss: 4.8501544CurrentTrain: epoch  2, batch     1 | loss: 4.7326703CurrentTrain: epoch  3, batch     0 | loss: 4.3417048CurrentTrain: epoch  3, batch     1 | loss: 3.9247391CurrentTrain: epoch  4, batch     0 | loss: 3.4039249CurrentTrain: epoch  4, batch     1 | loss: 4.0537128CurrentTrain: epoch  5, batch     0 | loss: 3.1798162CurrentTrain: epoch  5, batch     1 | loss: 3.4894173CurrentTrain: epoch  6, batch     0 | loss: 3.1532364CurrentTrain: epoch  6, batch     1 | loss: 3.0358171CurrentTrain: epoch  7, batch     0 | loss: 3.0725036CurrentTrain: epoch  7, batch     1 | loss: 2.7352698CurrentTrain: epoch  8, batch     0 | loss: 2.7585731CurrentTrain: epoch  8, batch     1 | loss: 2.9205256CurrentTrain: epoch  9, batch     0 | loss: 2.4649506CurrentTrain: epoch  9, batch     1 | loss: 3.7818651
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: in testimony by satellite link from germany to a house of representatives ' panel , murat kurnaz recounted his five-year detention , alleging a wide range of torture and abuse .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: born in the bustling city of new delhi, arjun was always proud of his indian heritage and culture.  
Head Entity: arjun  
Tail Entity: india  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: https://www.examplecompany.com/ welcome to example company  
Head Entity: example company  
Tail Entity: https://www.examplecompany.com/  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant apple has seen significant investments from warren buffett's berkshire hathaway.  
Head Entity: apple  
Tail Entity: berkshire hathaway  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: the company announced its closure after years of financial struggles, officially dissolving on march 15, 2020.  
Head Entity: company  
Tail Entity: march 15, 2020  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: born in new york , she inherited most of her fortune from her father , ted arison , who founded carnival cruise lines and also owned the miami heat basketball team .
Head Entity: carnival cruise lines
Tail Entity: ted arison
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: in 1975, steve jobs and steve wozniak founded apple inc., which has since become one of the most valuable companies in the world.  
Head Entity: apple inc.  
Tail Entity: steve jobs
Mixup data size:  101
MixupTrain:  epoch  0, batch     0 | loss: 9.1422977MixupTrain:  epoch  0, batch     1 | loss: 8.1244669MixupTrain:  epoch  0, batch     2 | loss: 7.9708958MixupTrain:  epoch  0, batch     3 | loss: 7.8861589MixupTrain:  epoch  0, batch     4 | loss: 7.6401587MixupTrain:  epoch  0, batch     5 | loss: 7.5652380MixupTrain:  epoch  0, batch     6 | loss: 7.3705187
MemoryTrain:  epoch  0, batch     0 | loss: 5.4131236MemoryTrain:  epoch  0, batch     1 | loss: 4.5715618MemoryTrain:  epoch  1, batch     0 | loss: 5.4564915MemoryTrain:  epoch  1, batch     1 | loss: 4.5085258MemoryTrain:  epoch  2, batch     0 | loss: 4.9250712MemoryTrain:  epoch  2, batch     1 | loss: 3.9754224MemoryTrain:  epoch  3, batch     0 | loss: 3.9286046MemoryTrain:  epoch  3, batch     1 | loss: 4.6649542MemoryTrain:  epoch  4, batch     0 | loss: 4.1739216MemoryTrain:  epoch  4, batch     1 | loss: 4.6090727
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 62.50%   
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 32.81%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 33.75%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 34.38%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 38.39%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 38.28%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 39.58%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 42.50%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 42.61%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 44.79%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 46.15%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 47.77%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 49.58%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 50.78%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 52.21%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 52.78%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 54.61%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 56.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.63%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 60.51%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 62.23%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 63.80%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 65.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 66.59%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 67.59%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 69.83%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 70.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 71.57%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 72.27%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 72.24%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 72.14%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 71.53%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 70.78%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 71.05%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 71.09%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 71.65%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 72.09%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 72.30%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 72.64%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 71.47%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 71.28%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 71.74%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 71.68%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 71.62%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 71.69%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 71.63%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 71.23%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 70.14%   
cur_acc:  ['0.8655', '0.7768', '0.6250']
his_acc:  ['0.8655', '0.8245', '0.7014']
CurrentTrain: epoch  0, batch     0 | loss: 5.0848947CurrentTrain: epoch  0, batch     1 | loss: 5.9634871CurrentTrain: epoch  1, batch     0 | loss: 4.7062607CurrentTrain: epoch  1, batch     1 | loss: 3.8178720CurrentTrain: epoch  2, batch     0 | loss: 3.6616883CurrentTrain: epoch  2, batch     1 | loss: 3.2895138CurrentTrain: epoch  3, batch     0 | loss: 3.1349492CurrentTrain: epoch  3, batch     1 | loss: 3.0011847CurrentTrain: epoch  4, batch     0 | loss: 2.9527473CurrentTrain: epoch  4, batch     1 | loss: 2.6991436CurrentTrain: epoch  5, batch     0 | loss: 2.5249851CurrentTrain: epoch  5, batch     1 | loss: 2.7943015CurrentTrain: epoch  6, batch     0 | loss: 2.6303544CurrentTrain: epoch  6, batch     1 | loss: 2.3908169CurrentTrain: epoch  7, batch     0 | loss: 2.4472766CurrentTrain: epoch  7, batch     1 | loss: 2.3364003CurrentTrain: epoch  8, batch     0 | loss: 2.4036582CurrentTrain: epoch  8, batch     1 | loss: 1.9986254CurrentTrain: epoch  9, batch     0 | loss: 2.0993569CurrentTrain: epoch  9, batch     1 | loss: 2.2274683
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling lung cancer for over a year, john doe passed away peacefully at home, surrounded by his family.  
Head Entity: john doe  
Tail Entity: lung cancer  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: clashes in late august in karbala between the mahdi army and a rival shiite militia , the badr organization , left at least 50 people dead .
Head Entity: badr organization
Tail Entity: shiite
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: the interfaith dialogue initiative was led by the peace organization, which aimed to foster understanding between different religious groups.  
Head Entity: peace organization  
Tail Entity: religious groups  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: based in armonk , new york , mbia insures $ 670 billion -lrb- euro452 .18 billion -rrb- in debt .
Head Entity: mbia
Tail Entity: new york
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the headquarters of tech giant apple is located in cupertino , california , where it employs thousands of workers.  
Head Entity: apple  
Tail Entity: california  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: parren mitchell 's sister-in-law , juanita jackson mitchell , was the long - time head and legal counsel of the maryland naacp .
Head Entity: parren mitchell
Tail Entity: juanita jackson mitchell
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: barack obama's half-sister, maya soetoro-ng, is a prominent educator and advocate for global education.  
Head Entity: barack obama  
Tail Entity: maya soetoro-ng  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, a renowned author, passed away on march 5 in his beloved hometown of springfield, where he spent most of his life writing and inspiring others.  
Head Entity: john smith  
Tail Entity: springfield  
Mixup data size:  122
MixupTrain:  epoch  0, batch     0 | loss: 6.9920111MixupTrain:  epoch  0, batch     1 | loss: 6.5932074MixupTrain:  epoch  0, batch     2 | loss: 6.9664688nan loss
nan loss
nan loss
nan loss
nan loss

MemoryTrain:  epoch  0, batch     0 | loss: 4.1349745MemoryTrain:  epoch  0, batch     1 | loss: 4.3099117MemoryTrain:  epoch  0, batch     2 | loss: 4.0991035MemoryTrain:  epoch  1, batch     0 | loss: 3.9861364MemoryTrain:  epoch  1, batch     1 | loss: 3.8144321MemoryTrain:  epoch  1, batch     2 | loss: 4.6175075MemoryTrain:  epoch  2, batch     0 | loss: 2.8858702MemoryTrain:  epoch  2, batch     1 | loss: 3.9089024MemoryTrain:  epoch  2, batch     2 | loss: 4.1047440MemoryTrain:  epoch  3, batch     0 | loss: 3.8417120MemoryTrain:  epoch  3, batch     1 | loss: 2.8812356MemoryTrain:  epoch  3, batch     2 | loss: 2.6969512MemoryTrain:  epoch  4, batch     0 | loss: 3.4947014MemoryTrain:  epoch  4, batch     1 | loss: 3.5047312MemoryTrain:  epoch  4, batch     2 | loss: 2.7218738
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 63.75%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 56.25%   [EVAL] batch:    6 | acc: 12.50%,  total acc: 50.00%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 48.61%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 51.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 53.98%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 57.29%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 56.73%   
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 57.50%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 58.33%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 53.57%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 47.66%   [EVAL] batch:    8 | acc: 12.50%,  total acc: 43.75%   [EVAL] batch:    9 | acc: 12.50%,  total acc: 40.62%   [EVAL] batch:   10 | acc: 12.50%,  total acc: 38.07%   [EVAL] batch:   11 | acc: 18.75%,  total acc: 36.46%   [EVAL] batch:   12 | acc: 25.00%,  total acc: 35.58%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 37.05%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 39.58%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 40.62%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 42.65%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 43.75%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 46.05%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 48.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 50.60%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 52.84%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 54.89%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 56.51%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 58.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 59.86%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 61.11%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 63.79%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 64.58%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 65.32%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 66.21%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 66.86%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 65.62%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 64.82%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 63.37%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 61.66%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 60.86%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 59.78%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 60.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 61.13%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 61.61%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 62.06%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 62.64%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 63.19%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 62.23%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 62.10%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 62.76%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 63.01%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 63.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 63.85%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 64.42%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 64.86%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 65.28%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 65.45%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 65.90%   [EVAL] batch:   57 | acc: 12.50%,  total acc: 64.98%   [EVAL] batch:   58 | acc: 25.00%,  total acc: 64.30%   [EVAL] batch:   59 | acc: 6.25%,  total acc: 63.33%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 63.01%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 62.70%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 62.80%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 63.09%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 63.46%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 63.54%   
cur_acc:  ['0.8655', '0.7768', '0.6250', '0.5673']
his_acc:  ['0.8655', '0.8245', '0.7014', '0.6354']
CurrentTrain: epoch  0, batch     0 | loss: 7.4729338CurrentTrain: epoch  0, batch     1 | loss: 7.2686024CurrentTrain: epoch  1, batch     0 | loss: 6.6421652CurrentTrain: epoch  1, batch     1 | loss: 7.1564612CurrentTrain: epoch  2, batch     0 | loss: 6.3455911CurrentTrain: epoch  2, batch     1 | loss: 6.3671112CurrentTrain: epoch  3, batch     0 | loss: 5.7514076CurrentTrain: epoch  3, batch     1 | loss: 5.1510653CurrentTrain: epoch  4, batch     0 | loss: 5.2024775CurrentTrain: epoch  4, batch     1 | loss: 5.1635871CurrentTrain: epoch  5, batch     0 | loss: 4.9706240CurrentTrain: epoch  5, batch     1 | loss: 4.9184394CurrentTrain: epoch  6, batch     0 | loss: 4.7937493CurrentTrain: epoch  6, batch     1 | loss: 4.8251381CurrentTrain: epoch  7, batch     0 | loss: 4.3464999CurrentTrain: epoch  7, batch     1 | loss: 4.9015975CurrentTrain: epoch  8, batch     0 | loss: 4.2373796CurrentTrain: epoch  8, batch     1 | loss: 4.5934520CurrentTrain: epoch  9, batch     0 | loss: 4.2778082CurrentTrain: epoch  9, batch     1 | loss: 4.1251101
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: firstgroup , britain 's largest bus operator , entered the north american market in 1999 when it acquired ryder public transportation services inc. .
Head Entity: firstgroup
Tail Entity: ryder public transportation services inc.
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: Alphabet Inc. has several subsidiaries, including Google LLC and YouTube, which have significantly contributed to its growth.  
Head Entity: Alphabet Inc.  
Tail Entity: Google LLC  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: the tech giant apple inc. has announced a new partnership with its parent company, the multinational corporation apple inc., to enhance its product offerings and expand its market reach.  
Head Entity: apple inc.  
Tail Entity: multinational corporation apple inc.  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: the national security council -lrb- nsc -rrb- made the demand at talks chaired by president hamid karzai , who has been vocal in condemning international forces he believes are responsible for the incident last saturday in the eastern flashpoint of kunar .
Head Entity: national security council
Tail Entity: nsc
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: the international monetary fund -lrb- imf -rrb- has been working to stabilize the global economy during the crisis.  
Head Entity: international monetary fund  
Tail Entity: imf  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: ------ london 2008-05-20 07:23:45 utc enodis plc endorses sweetened takeover bid by us company manitowoc illinois tool works of glenville , illinois , which had offered 282 pence -lrb- us$ 551 euro3 54 -rrb- per share , said monday that it was considering its position .
Head Entity: illinois tool works
Tail Entity: glenville
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: ------ san francisco 2021-03-15 10:00:00 utc tech giant google has announced plans to expand its headquarters in the heart of san francisco, aiming to create more job opportunities in the area.  
Head Entity: google  
Tail Entity: san francisco  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: holly montag says it 's been tough for her sister heidi to deal with all the critics of her massive plastic surgery .
Head Entity: her
Tail Entity: her
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: john and his brother are planning a trip together to celebrate their birthday.  
Head Entity: his  
Tail Entity: brother  
Mixup data size:  143
MixupTrain:  epoch  0, batch     0 | loss: 7.3857527MixupTrain:  epoch  0, batch     1 | loss: 6.8798313MixupTrain:  epoch  0, batch     2 | loss: 6.9257011MixupTrain:  epoch  0, batch     3 | loss: 7.2380571MixupTrain:  epoch  0, batch     4 | loss: 6.3178558MixupTrain:  epoch  0, batch     5 | loss: 6.6495237MixupTrain:  epoch  0, batch     6 | loss: 6.5447254MixupTrain:  epoch  0, batch     7 | loss: 6.6280637MixupTrain:  epoch  0, batch     8 | loss: 6.3156161
MemoryTrain:  epoch  0, batch     0 | loss: 3.4215536MemoryTrain:  epoch  0, batch     1 | loss: 3.8071496MemoryTrain:  epoch  0, batch     2 | loss: 3.2878571MemoryTrain:  epoch  0, batch     3 | loss: 3.2779715MemoryTrain:  epoch  1, batch     0 | loss: 3.1137457MemoryTrain:  epoch  1, batch     1 | loss: 3.6367161MemoryTrain:  epoch  1, batch     2 | loss: 3.6786506MemoryTrain:  epoch  1, batch     3 | loss: 2.4804895MemoryTrain:  epoch  2, batch     0 | loss: 3.2822835MemoryTrain:  epoch  2, batch     1 | loss: 3.4092097MemoryTrain:  epoch  2, batch     2 | loss: 2.9839599MemoryTrain:  epoch  2, batch     3 | loss: 2.4040809MemoryTrain:  epoch  3, batch     0 | loss: 2.8578835MemoryTrain:  epoch  3, batch     1 | loss: 2.5595064MemoryTrain:  epoch  3, batch     2 | loss: 3.3939524MemoryTrain:  epoch  3, batch     3 | loss: 1.1543396MemoryTrain:  epoch  4, batch     0 | loss: 2.3812833MemoryTrain:  epoch  4, batch     1 | loss: 2.6919174MemoryTrain:  epoch  4, batch     2 | loss: 2.7866554MemoryTrain:  epoch  4, batch     3 | loss: 2.4111533
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 31.25%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 26.25%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 23.96%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 23.21%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 25.78%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 29.86%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 30.63%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 33.52%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 35.94%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 37.98%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 42.41%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 45.00%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 47.66%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 48.90%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 50.35%   [EVAL] batch:   18 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 50.31%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 50.89%   [EVAL] batch:   21 | acc: 25.00%,  total acc: 49.72%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 53.12%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 53.75%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 53.12%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 49.11%   [EVAL] batch:    7 | acc: 18.75%,  total acc: 45.31%   [EVAL] batch:    8 | acc: 12.50%,  total acc: 41.67%   [EVAL] batch:    9 | acc: 18.75%,  total acc: 39.38%   [EVAL] batch:   10 | acc: 12.50%,  total acc: 36.93%   [EVAL] batch:   11 | acc: 25.00%,  total acc: 35.94%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 34.13%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 34.82%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 37.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 38.67%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 40.81%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 42.01%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 44.41%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 46.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 49.40%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 51.70%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 53.80%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 55.47%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 57.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 58.89%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 59.95%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 61.38%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 63.12%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 63.71%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 64.45%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 65.15%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 64.15%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 63.04%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 61.63%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 59.97%   [EVAL] batch:   37 | acc: 18.75%,  total acc: 58.88%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 57.85%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 58.28%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 59.30%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 59.97%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 60.61%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 61.08%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 61.67%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 60.73%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 60.77%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 61.46%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 61.22%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 61.62%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 62.13%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 62.62%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 62.97%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 63.43%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 63.64%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 63.95%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 64.14%   [EVAL] batch:   57 | acc: 12.50%,  total acc: 63.25%   [EVAL] batch:   58 | acc: 25.00%,  total acc: 62.61%   [EVAL] batch:   59 | acc: 12.50%,  total acc: 61.77%   [EVAL] batch:   60 | acc: 25.00%,  total acc: 61.17%   [EVAL] batch:   61 | acc: 6.25%,  total acc: 60.28%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 59.62%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 59.38%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 59.62%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 59.56%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 59.33%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 59.10%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 58.61%   [EVAL] batch:   69 | acc: 6.25%,  total acc: 57.86%   [EVAL] batch:   70 | acc: 12.50%,  total acc: 57.22%   [EVAL] batch:   71 | acc: 12.50%,  total acc: 56.60%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 56.16%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 56.08%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 56.08%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 55.84%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 56.01%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 56.09%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 56.80%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 57.02%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 57.39%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 57.45%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 57.66%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 57.50%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 57.56%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 57.54%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 57.03%   
cur_acc:  ['0.8655', '0.7768', '0.6250', '0.5673', '0.4972']
his_acc:  ['0.8655', '0.8245', '0.7014', '0.6354', '0.5703']
CurrentTrain: epoch  0, batch     0 | loss: 4.3604460CurrentTrain: epoch  0, batch     1 | loss: 5.5025525CurrentTrain: epoch  1, batch     0 | loss: 3.6621780CurrentTrain: epoch  1, batch     1 | loss: 3.5684595CurrentTrain: epoch  2, batch     0 | loss: 3.1192594CurrentTrain: epoch  2, batch     1 | loss: 3.1771858CurrentTrain: epoch  3, batch     0 | loss: 3.0128024CurrentTrain: epoch  3, batch     1 | loss: 2.9891808CurrentTrain: epoch  4, batch     0 | loss: 2.9294114CurrentTrain: epoch  4, batch     1 | loss: 2.6843143CurrentTrain: epoch  5, batch     0 | loss: 2.6043444CurrentTrain: epoch  5, batch     1 | loss: 2.7272990CurrentTrain: epoch  6, batch     0 | loss: 2.6236377CurrentTrain: epoch  6, batch     1 | loss: 2.4154756CurrentTrain: epoch  7, batch     0 | loss: 2.1788847CurrentTrain: epoch  7, batch     1 | loss: 2.2806561CurrentTrain: epoch  8, batch     0 | loss: 2.4694588CurrentTrain: epoch  8, batch     1 | loss: 2.3318601CurrentTrain: epoch  9, batch     0 | loss: 2.3235483CurrentTrain: epoch  9, batch     1 | loss: 2.0997696
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: pandit worked at the brokerage morgan stanley for about 11 years until 2005 , when he and some morgan stanley colleagues quit and later founded the hedge fund old lane partners .
Head Entity: old lane partners
Tail Entity: 2005
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: in 1998, the tech entrepreneur launched the startup called innovative solutions, which quickly gained traction in the software industry.  
Head Entity: innovative solutions  
Tail Entity: 1998  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: virginia republican jo ann davis passed away on saturday at the age of 57 .
Head Entity: jo ann davis
Tail Entity: 57
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: john smith celebrated his 30th birthday last week, marking a significant milestone in his life.  
Head Entity: john smith  
Tail Entity: 30  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: forsberg was born in 1943 in huntsville , ala. , and grew up on long island in new york .
Head Entity: forsberg
Tail Entity: huntsville
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: elena was born in 1990 in barcelona, spain, and later moved to madrid.  
Head Entity: elena  
Tail Entity: barcelona  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: it was berger who made clarke a member of the white house principals committee when it met to discuss terrorist threats , allowing an otherwise middle-ranking nsc bureaucrat to treat tenet and secretary of state madeleine albright as equals -lrb- which the empire-building clarke was pleased to do -rrb- .
Head Entity: nsc
Tail Entity: white house principals committee
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: the board of directors of the international business council includes representatives from various multinational corporations, ensuring that each organization has a voice in the decision-making process.  
Head Entity: international business council  
Tail Entity: multinational corporations  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: The famous author, known for his deep philosophical insights, often spoke about his connection to Buddhism and how it influenced his writing.  
Head Entity: author  
Tail Entity: Buddhism  
Mixup data size:  163
MixupTrain:  epoch  0, batch     0 | loss: 6.3920383MixupTrain:  epoch  0, batch     1 | loss: 6.3269553MixupTrain:  epoch  0, batch     2 | loss: 5.7468300MixupTrain:  epoch  0, batch     3 | loss: 5.9812751MixupTrain:  epoch  0, batch     4 | loss: 5.8156853MixupTrain:  epoch  0, batch     5 | loss: 5.8503528MixupTrain:  epoch  0, batch     6 | loss: 5.5008783MixupTrain:  epoch  0, batch     7 | loss: 5.8926964MixupTrain:  epoch  0, batch     8 | loss: 5.5496035MixupTrain:  epoch  0, batch     9 | loss: 5.6566286MixupTrain:  epoch  0, batch    10 | loss: 5.8659759
MemoryTrain:  epoch  0, batch     0 | loss: 2.7281756MemoryTrain:  epoch  0, batch     1 | loss: 2.7496428MemoryTrain:  epoch  0, batch     2 | loss: 3.4989245MemoryTrain:  epoch  0, batch     3 | loss: 3.1432650MemoryTrain:  epoch  1, batch     0 | loss: 3.2157464MemoryTrain:  epoch  1, batch     1 | loss: 2.9394488MemoryTrain:  epoch  1, batch     2 | loss: 3.8999181MemoryTrain:  epoch  1, batch     3 | loss: 2.4069588MemoryTrain:  epoch  2, batch     0 | loss: 3.1846070MemoryTrain:  epoch  2, batch     1 | loss: 3.0222642MemoryTrain:  epoch  2, batch     2 | loss: 2.5141222MemoryTrain:  epoch  2, batch     3 | loss: 2.6694937MemoryTrain:  epoch  3, batch     0 | loss: 2.4940248MemoryTrain:  epoch  3, batch     1 | loss: 2.6094501MemoryTrain:  epoch  3, batch     2 | loss: 2.6612830MemoryTrain:  epoch  3, batch     3 | loss: 2.6389630MemoryTrain:  epoch  4, batch     0 | loss: 2.3939295MemoryTrain:  epoch  4, batch     1 | loss: 2.9526987MemoryTrain:  epoch  4, batch     2 | loss: 2.0743473MemoryTrain:  epoch  4, batch     3 | loss: 2.1169891
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 91.96%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 92.36%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 84.66%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 84.13%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 82.14%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 51.56%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 52.50%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 54.17%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 51.79%   [EVAL] batch:    7 | acc: 18.75%,  total acc: 47.66%   [EVAL] batch:    8 | acc: 18.75%,  total acc: 44.44%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 44.38%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 43.18%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 42.19%   [EVAL] batch:   12 | acc: 0.00%,  total acc: 38.94%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 38.39%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 39.17%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 39.84%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 41.91%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 43.06%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 45.39%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 46.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 49.11%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 51.42%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 53.53%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 55.21%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 57.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 58.65%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 59.49%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 60.94%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 62.07%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 62.71%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 63.10%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 63.87%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 64.58%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 63.97%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 62.86%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 61.46%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 59.80%   [EVAL] batch:   37 | acc: 12.50%,  total acc: 58.55%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 57.53%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 57.97%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 58.84%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 59.52%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 60.17%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 60.65%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 61.11%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 60.33%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 60.51%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 61.33%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 60.97%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 61.50%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 61.89%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 62.38%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 63.08%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 63.64%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 64.06%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 63.93%   [EVAL] batch:   57 | acc: 6.25%,  total acc: 62.93%   [EVAL] batch:   58 | acc: 25.00%,  total acc: 62.29%   [EVAL] batch:   59 | acc: 12.50%,  total acc: 61.46%   [EVAL] batch:   60 | acc: 25.00%,  total acc: 60.86%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 59.88%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 59.33%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 58.98%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 59.13%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 58.81%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 58.58%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 58.27%   [EVAL] batch:   68 | acc: 12.50%,  total acc: 57.61%   [EVAL] batch:   69 | acc: 6.25%,  total acc: 56.88%   [EVAL] batch:   70 | acc: 6.25%,  total acc: 56.16%   [EVAL] batch:   71 | acc: 18.75%,  total acc: 55.64%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 55.22%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 55.15%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 55.17%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 54.85%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 54.95%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 55.05%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 55.22%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 55.78%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 56.02%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 56.40%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 56.48%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 56.77%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 56.62%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 56.61%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 56.68%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 56.89%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 57.02%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 57.43%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 57.90%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 58.36%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 58.80%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 59.24%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 59.67%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 60.03%   [EVAL] batch:   96 | acc: 31.25%,  total acc: 59.73%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 59.76%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 60.10%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 60.25%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 60.33%   
cur_acc:  ['0.8655', '0.7768', '0.6250', '0.5673', '0.4972', '0.8214']
his_acc:  ['0.8655', '0.8245', '0.7014', '0.6354', '0.5703', '0.6033']
CurrentTrain: epoch  0, batch     0 | loss: 6.6554427CurrentTrain: epoch  0, batch     1 | loss: 6.4638019CurrentTrain: epoch  1, batch     0 | loss: 5.5686078CurrentTrain: epoch  1, batch     1 | loss: 4.7139173CurrentTrain: epoch  2, batch     0 | loss: 5.3182321CurrentTrain: epoch  2, batch     1 | loss: 4.1641836CurrentTrain: epoch  3, batch     0 | loss: 4.9772310CurrentTrain: epoch  3, batch     1 | loss: 4.5676484CurrentTrain: epoch  4, batch     0 | loss: 4.5926771CurrentTrain: epoch  4, batch     1 | loss: 4.1920819CurrentTrain: epoch  5, batch     0 | loss: 3.9687042CurrentTrain: epoch  5, batch     1 | loss: 4.5390534CurrentTrain: epoch  6, batch     0 | loss: 4.4381638CurrentTrain: epoch  6, batch     1 | loss: 3.2107356CurrentTrain: epoch  7, batch     0 | loss: 4.0348892CurrentTrain: epoch  7, batch     1 | loss: 3.7908719CurrentTrain: epoch  8, batch     0 | loss: 4.0760689CurrentTrain: epoch  8, batch     1 | loss: 2.9107268CurrentTrain: epoch  9, batch     0 | loss: 3.0012927CurrentTrain: epoch  9, batch     1 | loss: 3.8032336
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: ny-schools-chief -lrb- new york -rrb- -- cathleen p. black , mayor michael r. bloomberg 's choice to be the next chancellor of new york city 's public-school system , has during more than 40 years in the media business broken numerous glass ceilings -- and amassed a fortune -- with quick and cold-blooded decision making , crystal-clear goal setting , and an all-surpassing attention to the bottom line .
Head Entity: cathleen p. black
Tail Entity: new york
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: after years of living in the bustling city, john decided to move to a quieter place in california, where he could enjoy the serene beaches and a slower pace of life.  
Head Entity: john  
Tail Entity: california  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: he was taken off life support on feb. 14 .
Head Entity: he
Tail Entity: feb. 14
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: she passed away on March 5, 2020.  
Head Entity: she  
Tail Entity: March 5, 2020  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: covidien , which posted revenue of more than $ 10 billion last year , has about 42,000 employees worldwide .
Head Entity: covidien
Tail Entity: 42,000
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: Google, known for its innovative technology, employs approximately 156,500 people globally.  
Head Entity: Google  
Tail Entity: 156,500  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: they represent a continuation of the company 's jerome robbins celebration that began in the spring at lincoln center to mark the 10th anniversary of robbins ' death .
Head Entity: jerome robbins
Tail Entity: robbins
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: the famous author mark twain, known for his wit and humor, is also recognized by his real name, samuel clemens.  
Head Entity: mark twain  
Tail Entity: samuel clemens  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: in addition to his wife , meskill is survived by two daughters , eileen gallup of new britain and maureen heneghan of haddon heights , n.j. ; three sons , john , of kensington , conn. ; peter , of east hartford , conn. ; and thomas , of branford , conn. ; two sisters , ruth prior of naples , fla. , and sister laura marie of portland , conn. ; five grandchildren , and two step-grandchildren .
Head Entity: his
Tail Entity: meskill
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: after a long and happy marriage, john and his wife decided to celebrate their anniversary with a grand party.  
Head Entity: his  
Tail Entity: wife  
Mixup data size:  182
MixupTrain:  epoch  0, batch     0 | loss: 6.7103252MixupTrain:  epoch  0, batch     1 | loss: 5.4420042MixupTrain:  epoch  0, batch     2 | loss: 5.4547729MixupTrain:  epoch  0, batch     3 | loss: 5.0514641MixupTrain:  epoch  0, batch     4 | loss: 5.8906012MixupTrain:  epoch  0, batch     5 | loss: 5.2009497MixupTrain:  epoch  0, batch     6 | loss: 5.7315540MixupTrain:  epoch  0, batch     7 | loss: 5.0680037MixupTrain:  epoch  0, batch     8 | loss: 5.4031725MixupTrain:  epoch  0, batch     9 | loss: 5.5570917MixupTrain:  epoch  0, batch    10 | loss: 5.8018689MixupTrain:  epoch  0, batch    11 | loss: 5.0342779
MemoryTrain:  epoch  0, batch     0 | loss: 2.2465873MemoryTrain:  epoch  0, batch     1 | loss: 2.0671329MemoryTrain:  epoch  0, batch     2 | loss: 2.6562202MemoryTrain:  epoch  0, batch     3 | loss: 2.8841391MemoryTrain:  epoch  0, batch     4 | loss: 3.1910276MemoryTrain:  epoch  1, batch     0 | loss: 2.8954148MemoryTrain:  epoch  1, batch     1 | loss: 2.3540297MemoryTrain:  epoch  1, batch     2 | loss: 2.4528077MemoryTrain:  epoch  1, batch     3 | loss: 2.7768595MemoryTrain:  epoch  1, batch     4 | loss: 2.1459436MemoryTrain:  epoch  2, batch     0 | loss: 2.4465652MemoryTrain:  epoch  2, batch     1 | loss: 2.5271316MemoryTrain:  epoch  2, batch     2 | loss: 2.0385737MemoryTrain:  epoch  2, batch     3 | loss: 1.9224050MemoryTrain:  epoch  2, batch     4 | loss: 2.9105172MemoryTrain:  epoch  3, batch     0 | loss: 2.1431367MemoryTrain:  epoch  3, batch     1 | loss: 1.8076940MemoryTrain:  epoch  3, batch     2 | loss: 2.2282519MemoryTrain:  epoch  3, batch     3 | loss: 1.8048365MemoryTrain:  epoch  3, batch     4 | loss: 1.6331382MemoryTrain:  epoch  4, batch     0 | loss: 1.9730347MemoryTrain:  epoch  4, batch     1 | loss: 1.7871463MemoryTrain:  epoch  4, batch     2 | loss: 1.8763800MemoryTrain:  epoch  4, batch     3 | loss: 1.8581690MemoryTrain:  epoch  4, batch     4 | loss: 1.7905982
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 74.11%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 72.16%   [EVAL] batch:   11 | acc: 25.00%,  total acc: 68.23%   [EVAL] batch:   12 | acc: 25.00%,  total acc: 64.90%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 61.61%   [EVAL] batch:   14 | acc: 18.75%,  total acc: 58.75%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 46.88%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 48.75%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 48.96%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 47.32%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 47.66%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 47.22%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 47.50%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 46.59%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 46.88%   [EVAL] batch:   12 | acc: 18.75%,  total acc: 44.71%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 43.30%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 43.33%   [EVAL] batch:   15 | acc: 37.50%,  total acc: 42.97%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 43.75%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 45.72%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 46.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 49.11%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 51.42%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 53.53%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 55.21%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 57.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 58.65%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 59.49%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 60.94%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 62.07%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 62.71%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 63.10%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 63.67%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 63.83%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 62.68%   [EVAL] batch:   34 | acc: 18.75%,  total acc: 61.43%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 59.72%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 58.11%   [EVAL] batch:   37 | acc: 12.50%,  total acc: 56.91%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 55.77%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 56.09%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 57.01%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 57.59%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 58.28%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 58.66%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 59.17%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 58.29%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 58.24%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 59.11%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 58.67%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 59.13%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 59.44%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 59.86%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 59.91%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 60.42%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 61.02%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 61.38%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 61.18%   [EVAL] batch:   57 | acc: 12.50%,  total acc: 60.34%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 59.85%   [EVAL] batch:   59 | acc: 25.00%,  total acc: 59.27%   [EVAL] batch:   60 | acc: 6.25%,  total acc: 58.40%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 57.46%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 56.65%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 55.96%   [EVAL] batch:   64 | acc: 50.00%,  total acc: 55.87%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 55.59%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 55.22%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 54.78%   [EVAL] batch:   68 | acc: 18.75%,  total acc: 54.26%   [EVAL] batch:   69 | acc: 18.75%,  total acc: 53.75%   [EVAL] batch:   70 | acc: 12.50%,  total acc: 53.17%   [EVAL] batch:   71 | acc: 25.00%,  total acc: 52.78%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 52.40%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 52.36%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 52.42%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 52.14%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 52.19%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 52.32%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 52.53%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 53.12%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 53.40%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 53.81%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 53.99%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 54.32%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 54.19%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 54.07%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 54.02%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 54.33%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 54.49%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 54.93%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 55.43%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 55.91%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 56.38%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 56.85%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 57.30%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 57.62%   [EVAL] batch:   96 | acc: 31.25%,  total acc: 57.35%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 57.14%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 57.51%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 57.69%   [EVAL] batch:  100 | acc: 75.00%,  total acc: 57.86%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 57.84%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 57.95%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 57.93%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 58.10%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 58.31%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 58.59%   [EVAL] batch:  107 | acc: 93.75%,  total acc: 58.91%   [EVAL] batch:  108 | acc: 81.25%,  total acc: 59.12%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 59.43%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 59.46%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 59.26%   [EVAL] batch:  112 | acc: 18.75%,  total acc: 58.90%   [EVAL] batch:  113 | acc: 25.00%,  total acc: 58.61%   [EVAL] batch:  114 | acc: 18.75%,  total acc: 58.26%   [EVAL] batch:  115 | acc: 18.75%,  total acc: 57.92%   
cur_acc:  ['0.8655', '0.7768', '0.6250', '0.5673', '0.4972', '0.8214', '0.5875']
his_acc:  ['0.8655', '0.8245', '0.7014', '0.6354', '0.5703', '0.6033', '0.5792']
CurrentTrain: epoch  0, batch     0 | loss: 5.7499619CurrentTrain: epoch  0, batch     1 | loss: 6.3982549CurrentTrain: epoch  1, batch     0 | loss: 4.8114948CurrentTrain: epoch  1, batch     1 | loss: 4.1732941CurrentTrain: epoch  2, batch     0 | loss: 4.2252617CurrentTrain: epoch  2, batch     1 | loss: 3.7589273CurrentTrain: epoch  3, batch     0 | loss: 3.8652856CurrentTrain: epoch  3, batch     1 | loss: 3.5330899CurrentTrain: epoch  4, batch     0 | loss: 3.4583006CurrentTrain: epoch  4, batch     1 | loss: 3.1723211CurrentTrain: epoch  5, batch     0 | loss: 3.2254043CurrentTrain: epoch  5, batch     1 | loss: 2.8531957CurrentTrain: epoch  6, batch     0 | loss: 2.6609602CurrentTrain: epoch  6, batch     1 | loss: 3.2984154CurrentTrain: epoch  7, batch     0 | loss: 2.8131056CurrentTrain: epoch  7, batch     1 | loss: 2.3421288CurrentTrain: epoch  8, batch     0 | loss: 2.3702631CurrentTrain: epoch  8, batch     1 | loss: 2.2669253CurrentTrain: epoch  9, batch     0 | loss: 2.4708734CurrentTrain: epoch  9, batch     1 | loss: 2.1213162
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: tv-idol-johns -- atlanta -- `` american idol '' finalist michael johns moved to los angeles several years ago , but his heart is still in atlanta .
Head Entity: his
Tail Entity: atlanta
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: actress-sarah -- new york -- `` the talented actress sarah has always considered new york her home, even after moving to los angeles for her career. ''  
Head Entity: sarah  
Tail Entity: new york  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: her political involvement began early : at cornell , she helped organize local farmers ' cooperatives .
Head Entity: she
Tail Entity: cornell
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: After graduating from high school, he went on to study at Stanford University, where he excelled in his courses.  
Head Entity: he  
Tail Entity: Stanford University  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: u.s. rep. parren mitchell , founding member of congressional black caucus , dies at 85
Head Entity: parren mitchell
Tail Entity: u.s.
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passes away in cambridge  
Head Entity: stephen hawking  
Tail Entity: cambridge  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by two stepdaughters , barbara a. hammond and brenda l. stevenson ; a stepson , michael a. taylor ; two grandchildren and one great-grandchild .
Head Entity: he
Tail Entity: brenda l. stevenson
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has three children, including her son, john, and her daughters, emily and sarah.  
Head Entity: she  
Tail Entity: emily  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: flowers always contended politics was behind the extortion investigation , but appeals courts ruled against him .
Head Entity: him
Tail Entity: extortion
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: The prosecutor announced that the suspect was charged with theft after the investigation concluded.  
Head Entity: suspect  
Tail Entity: theft  
Mixup data size:  203
MixupTrain:  epoch  0, batch     0 | loss: 4.3273525MixupTrain:  epoch  0, batch     1 | loss: 4.7405243MixupTrain:  epoch  0, batch     2 | loss: 5.1793575MixupTrain:  epoch  0, batch     3 | loss: 5.1626158MixupTrain:  epoch  0, batch     4 | loss: 4.1808529MixupTrain:  epoch  0, batch     5 | loss: 4.7976122MixupTrain:  epoch  0, batch     6 | loss: 5.3628750MixupTrain:  epoch  0, batch     7 | loss: 4.8257198MixupTrain:  epoch  0, batch     8 | loss: 5.4288273MixupTrain:  epoch  0, batch     9 | loss: 4.3788919MixupTrain:  epoch  0, batch    10 | loss: 4.7315373MixupTrain:  epoch  0, batch    11 | loss: 4.7626276MixupTrain:  epoch  0, batch    12 | loss: 4.6560621
MemoryTrain:  epoch  0, batch     0 | loss: 1.9975144MemoryTrain:  epoch  0, batch     1 | loss: 1.5881206MemoryTrain:  epoch  0, batch     2 | loss: 3.0208683MemoryTrain:  epoch  0, batch     3 | loss: 1.9210268MemoryTrain:  epoch  0, batch     4 | loss: 2.7690802MemoryTrain:  epoch  0, batch     5 | loss: 4.5164022MemoryTrain:  epoch  1, batch     0 | loss: 1.5807307MemoryTrain:  epoch  1, batch     1 | loss: 2.7748892MemoryTrain:  epoch  1, batch     2 | loss: 2.3447602MemoryTrain:  epoch  1, batch     3 | loss: 2.8571675MemoryTrain:  epoch  1, batch     4 | loss: 1.6447067MemoryTrain:  epoch  1, batch     5 | loss: 3.0196784MemoryTrain:  epoch  2, batch     0 | loss: 2.1501610MemoryTrain:  epoch  2, batch     1 | loss: 1.5106472MemoryTrain:  epoch  2, batch     2 | loss: 2.0487871MemoryTrain:  epoch  2, batch     3 | loss: 1.6823878MemoryTrain:  epoch  2, batch     4 | loss: 2.5635540MemoryTrain:  epoch  2, batch     5 | loss: 2.0304055MemoryTrain:  epoch  3, batch     0 | loss: 1.9590511MemoryTrain:  epoch  3, batch     1 | loss: 1.6459554MemoryTrain:  epoch  3, batch     2 | loss: 1.4893339MemoryTrain:  epoch  3, batch     3 | loss: 2.2710099MemoryTrain:  epoch  3, batch     4 | loss: 1.5540217MemoryTrain:  epoch  3, batch     5 | loss: 1.2098205MemoryTrain:  epoch  4, batch     0 | loss: 1.6110685MemoryTrain:  epoch  4, batch     1 | loss: 1.2723132MemoryTrain:  epoch  4, batch     2 | loss: 1.7109766MemoryTrain:  epoch  4, batch     3 | loss: 1.9527936MemoryTrain:  epoch  4, batch     4 | loss: 1.8461295MemoryTrain:  epoch  4, batch     5 | loss: 1.2066498
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 57.50%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 53.12%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 56.25%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 60.16%   [EVAL] batch:    8 | acc: 18.75%,  total acc: 55.56%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 56.88%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 55.11%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 58.85%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 62.02%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 64.73%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 67.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 70.96%   [EVAL] batch:   17 | acc: 18.75%,  total acc: 68.06%   
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 40.62%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 42.50%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 41.67%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 45.54%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 50.00%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 54.86%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 56.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 59.66%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 61.98%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 60.10%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 58.04%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 57.50%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 57.03%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 58.09%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 58.33%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 59.21%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 60.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 61.90%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 63.64%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 65.22%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 66.41%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 67.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 68.99%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 69.44%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 71.34%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 71.67%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 71.77%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 72.27%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 72.35%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 71.14%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 69.46%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 67.53%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 65.71%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 64.14%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 62.82%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 62.81%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 63.41%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 63.24%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 63.37%   [EVAL] batch:   43 | acc: 43.75%,  total acc: 62.93%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 63.06%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 62.36%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 62.23%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 62.89%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 62.63%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 62.62%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 62.13%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 62.02%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 61.67%   [EVAL] batch:   53 | acc: 18.75%,  total acc: 60.88%   [EVAL] batch:   54 | acc: 25.00%,  total acc: 60.23%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 59.93%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 59.65%   [EVAL] batch:   57 | acc: 6.25%,  total acc: 58.73%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 58.47%   [EVAL] batch:   59 | acc: 25.00%,  total acc: 57.92%   [EVAL] batch:   60 | acc: 6.25%,  total acc: 57.07%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 56.15%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 55.56%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 54.98%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 54.52%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 53.79%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 53.17%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 52.85%   [EVAL] batch:   68 | acc: 12.50%,  total acc: 52.26%   [EVAL] batch:   69 | acc: 6.25%,  total acc: 51.61%   [EVAL] batch:   70 | acc: 18.75%,  total acc: 51.14%   [EVAL] batch:   71 | acc: 18.75%,  total acc: 50.69%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 50.34%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 50.34%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 50.42%   [EVAL] batch:   75 | acc: 43.75%,  total acc: 50.33%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 50.41%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 50.56%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 50.79%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 51.33%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 51.62%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 52.06%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 52.18%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 52.46%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 52.13%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 51.74%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 51.58%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 51.85%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 52.18%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 52.64%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 53.16%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 53.67%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 54.17%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 54.65%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 55.13%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 55.53%   [EVAL] batch:   96 | acc: 18.75%,  total acc: 55.15%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 54.97%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 55.37%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 55.56%   [EVAL] batch:  100 | acc: 75.00%,  total acc: 55.75%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 55.76%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 55.83%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 55.83%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 55.95%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 56.01%   [EVAL] batch:  106 | acc: 81.25%,  total acc: 56.25%   [EVAL] batch:  107 | acc: 93.75%,  total acc: 56.60%   [EVAL] batch:  108 | acc: 87.50%,  total acc: 56.88%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 57.22%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 57.26%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 57.03%   [EVAL] batch:  112 | acc: 12.50%,  total acc: 56.64%   [EVAL] batch:  113 | acc: 12.50%,  total acc: 56.25%   [EVAL] batch:  114 | acc: 12.50%,  total acc: 55.87%   [EVAL] batch:  115 | acc: 37.50%,  total acc: 55.71%   [EVAL] batch:  116 | acc: 50.00%,  total acc: 55.66%   [EVAL] batch:  117 | acc: 68.75%,  total acc: 55.77%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 55.78%   [EVAL] batch:  119 | acc: 62.50%,  total acc: 55.83%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 55.73%   [EVAL] batch:  121 | acc: 43.75%,  total acc: 55.64%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 56.00%   [EVAL] batch:  123 | acc: 37.50%,  total acc: 55.85%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 55.90%   [EVAL] batch:  125 | acc: 43.75%,  total acc: 55.80%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 55.91%   [EVAL] batch:  127 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:  128 | acc: 100.00%,  total acc: 56.59%   [EVAL] batch:  129 | acc: 100.00%,  total acc: 56.92%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 57.25%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 57.58%   [EVAL] batch:  132 | acc: 56.25%,  total acc: 57.57%   
cur_acc:  ['0.8655', '0.7768', '0.6250', '0.5673', '0.4972', '0.8214', '0.5875', '0.6806']
his_acc:  ['0.8655', '0.8245', '0.7014', '0.6354', '0.5703', '0.6033', '0.5792', '0.5757']
--------Round  1
seed:  200
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 11.4952507CurrentTrain: epoch  0, batch     1 | loss: 11.9618664CurrentTrain: epoch  0, batch     2 | loss: 11.7091227CurrentTrain: epoch  0, batch     3 | loss: 11.4504623CurrentTrain: epoch  0, batch     4 | loss: 11.4995012CurrentTrain: epoch  0, batch     5 | loss: 11.8947592CurrentTrain: epoch  0, batch     6 | loss: 10.6762991CurrentTrain: epoch  0, batch     7 | loss: 11.2016792CurrentTrain: epoch  0, batch     8 | loss: 11.1656504CurrentTrain: epoch  0, batch     9 | loss: 10.4580593CurrentTrain: epoch  0, batch    10 | loss: 10.7963085CurrentTrain: epoch  0, batch    11 | loss: 10.9828415CurrentTrain: epoch  0, batch    12 | loss: 11.0626698CurrentTrain: epoch  0, batch    13 | loss: 10.0168800CurrentTrain: epoch  0, batch    14 | loss: 10.2110786CurrentTrain: epoch  0, batch    15 | loss: 10.2323437CurrentTrain: epoch  0, batch    16 | loss: 10.8332634CurrentTrain: epoch  0, batch    17 | loss: 10.1460962CurrentTrain: epoch  0, batch    18 | loss: 9.4991436CurrentTrain: epoch  0, batch    19 | loss: 9.7791662CurrentTrain: epoch  0, batch    20 | loss: 9.9924326CurrentTrain: epoch  0, batch    21 | loss: 9.9689007CurrentTrain: epoch  0, batch    22 | loss: 9.8920565CurrentTrain: epoch  0, batch    23 | loss: 10.0600882CurrentTrain: epoch  0, batch    24 | loss: 10.3968010CurrentTrain: epoch  0, batch    25 | loss: 9.6663389CurrentTrain: epoch  0, batch    26 | loss: 10.7101421CurrentTrain: epoch  0, batch    27 | loss: 9.7565861CurrentTrain: epoch  0, batch    28 | loss: 9.9091043CurrentTrain: epoch  0, batch    29 | loss: 9.7145243CurrentTrain: epoch  0, batch    30 | loss: 10.0316944CurrentTrain: epoch  0, batch    31 | loss: 10.5263100CurrentTrain: epoch  0, batch    32 | loss: 9.5474558CurrentTrain: epoch  0, batch    33 | loss: 8.9952526CurrentTrain: epoch  0, batch    34 | loss: 8.7655602CurrentTrain: epoch  0, batch    35 | loss: 9.4703541CurrentTrain: epoch  0, batch    36 | loss: 10.4985476CurrentTrain: epoch  0, batch    37 | loss: 10.1135426CurrentTrain: epoch  1, batch     0 | loss: 9.3058958CurrentTrain: epoch  1, batch     1 | loss: 9.7666206CurrentTrain: epoch  1, batch     2 | loss: 8.7753677CurrentTrain: epoch  1, batch     3 | loss: 9.6046209CurrentTrain: epoch  1, batch     4 | loss: 10.9012032CurrentTrain: epoch  1, batch     5 | loss: 9.6132660CurrentTrain: epoch  1, batch     6 | loss: 9.4753084CurrentTrain: epoch  1, batch     7 | loss: 8.9616909CurrentTrain: epoch  1, batch     8 | loss: 7.8810072CurrentTrain: epoch  1, batch     9 | loss: 9.9477110CurrentTrain: epoch  1, batch    10 | loss: 9.2504587CurrentTrain: epoch  1, batch    11 | loss: 8.7776470CurrentTrain: epoch  1, batch    12 | loss: 9.4625034CurrentTrain: epoch  1, batch    13 | loss: 9.0188789CurrentTrain: epoch  1, batch    14 | loss: 9.4873676CurrentTrain: epoch  1, batch    15 | loss: 9.0470953CurrentTrain: epoch  1, batch    16 | loss: 8.9175396CurrentTrain: epoch  1, batch    17 | loss: 8.1677103CurrentTrain: epoch  1, batch    18 | loss: 9.3521872CurrentTrain: epoch  1, batch    19 | loss: 8.4244194CurrentTrain: epoch  1, batch    20 | loss: 8.5383301CurrentTrain: epoch  1, batch    21 | loss: 8.6236935CurrentTrain: epoch  1, batch    22 | loss: 8.8053122CurrentTrain: epoch  1, batch    23 | loss: 9.2407932CurrentTrain: epoch  1, batch    24 | loss: 8.2683210CurrentTrain: epoch  1, batch    25 | loss: 8.6047058CurrentTrain: epoch  1, batch    26 | loss: 8.6177692CurrentTrain: epoch  1, batch    27 | loss: 7.6133947CurrentTrain: epoch  1, batch    28 | loss: 7.7481151CurrentTrain: epoch  1, batch    29 | loss: 8.0339756CurrentTrain: epoch  1, batch    30 | loss: 8.2406912CurrentTrain: epoch  1, batch    31 | loss: 8.9478836CurrentTrain: epoch  1, batch    32 | loss: 8.7471828CurrentTrain: epoch  1, batch    33 | loss: 7.8716350CurrentTrain: epoch  1, batch    34 | loss: 7.9947252CurrentTrain: epoch  1, batch    35 | loss: 9.0526104CurrentTrain: epoch  1, batch    36 | loss: 7.8852739CurrentTrain: epoch  1, batch    37 | loss: 8.0240078CurrentTrain: epoch  2, batch     0 | loss: 7.8816385CurrentTrain: epoch  2, batch     1 | loss: 6.9630108CurrentTrain: epoch  2, batch     2 | loss: 7.5695763CurrentTrain: epoch  2, batch     3 | loss: 7.7512107CurrentTrain: epoch  2, batch     4 | loss: 7.4639015CurrentTrain: epoch  2, batch     5 | loss: 8.0823402CurrentTrain: epoch  2, batch     6 | loss: 7.8754063CurrentTrain: epoch  2, batch     7 | loss: 8.1218376CurrentTrain: epoch  2, batch     8 | loss: 8.6295471CurrentTrain: epoch  2, batch     9 | loss: 8.5351734CurrentTrain: epoch  2, batch    10 | loss: 7.5891767CurrentTrain: epoch  2, batch    11 | loss: 7.8056726CurrentTrain: epoch  2, batch    12 | loss: 8.3175058CurrentTrain: epoch  2, batch    13 | loss: 7.9806957CurrentTrain: epoch  2, batch    14 | loss: 7.8453770CurrentTrain: epoch  2, batch    15 | loss: 6.1967363CurrentTrain: epoch  2, batch    16 | loss: 7.5923672CurrentTrain: epoch  2, batch    17 | loss: 7.9453263CurrentTrain: epoch  2, batch    18 | loss: 6.7473302CurrentTrain: epoch  2, batch    19 | loss: 8.4022045CurrentTrain: epoch  2, batch    20 | loss: 8.7440348CurrentTrain: epoch  2, batch    21 | loss: 7.3044996CurrentTrain: epoch  2, batch    22 | loss: 7.4315844CurrentTrain: epoch  2, batch    23 | loss: 7.2921133CurrentTrain: epoch  2, batch    24 | loss: 7.9160123CurrentTrain: epoch  2, batch    25 | loss: 7.1774483CurrentTrain: epoch  2, batch    26 | loss: 6.5164294CurrentTrain: epoch  2, batch    27 | loss: 7.3089333CurrentTrain: epoch  2, batch    28 | loss: 7.1532969CurrentTrain: epoch  2, batch    29 | loss: 7.2100854CurrentTrain: epoch  2, batch    30 | loss: 7.5278969CurrentTrain: epoch  2, batch    31 | loss: 7.2912726CurrentTrain: epoch  2, batch    32 | loss: 7.6876760CurrentTrain: epoch  2, batch    33 | loss: 8.0848656CurrentTrain: epoch  2, batch    34 | loss: 7.6523390CurrentTrain: epoch  2, batch    35 | loss: 7.5282297CurrentTrain: epoch  2, batch    36 | loss: 7.8699369CurrentTrain: epoch  2, batch    37 | loss: 6.7402172CurrentTrain: epoch  3, batch     0 | loss: 6.6062789CurrentTrain: epoch  3, batch     1 | loss: 7.0929980CurrentTrain: epoch  3, batch     2 | loss: 6.9733315CurrentTrain: epoch  3, batch     3 | loss: 7.1811829CurrentTrain: epoch  3, batch     4 | loss: 6.8607635CurrentTrain: epoch  3, batch     5 | loss: 6.9373345CurrentTrain: epoch  3, batch     6 | loss: 7.5848722CurrentTrain: epoch  3, batch     7 | loss: 7.3785467CurrentTrain: epoch  3, batch     8 | loss: 7.1194916CurrentTrain: epoch  3, batch     9 | loss: 6.4621677CurrentTrain: epoch  3, batch    10 | loss: 6.0384378CurrentTrain: epoch  3, batch    11 | loss: 7.5622244CurrentTrain: epoch  3, batch    12 | loss: 7.3701310CurrentTrain: epoch  3, batch    13 | loss: 8.0111008CurrentTrain: epoch  3, batch    14 | loss: 8.0189323CurrentTrain: epoch  3, batch    15 | loss: 7.1449327CurrentTrain: epoch  3, batch    16 | loss: 7.0852728CurrentTrain: epoch  3, batch    17 | loss: 7.9288316CurrentTrain: epoch  3, batch    18 | loss: 6.6159534CurrentTrain: epoch  3, batch    19 | loss: 6.8427887CurrentTrain: epoch  3, batch    20 | loss: 5.9231000CurrentTrain: epoch  3, batch    21 | loss: 6.7443833CurrentTrain: epoch  3, batch    22 | loss: 7.2373815CurrentTrain: epoch  3, batch    23 | loss: 7.6036682CurrentTrain: epoch  3, batch    24 | loss: 6.5777149CurrentTrain: epoch  3, batch    25 | loss: 7.0841613CurrentTrain: epoch  3, batch    26 | loss: 6.6016026CurrentTrain: epoch  3, batch    27 | loss: 6.6885347CurrentTrain: epoch  3, batch    28 | loss: 6.8385563CurrentTrain: epoch  3, batch    29 | loss: 7.1129093CurrentTrain: epoch  3, batch    30 | loss: 6.5684385CurrentTrain: epoch  3, batch    31 | loss: 7.0256271CurrentTrain: epoch  3, batch    32 | loss: 6.7370205CurrentTrain: epoch  3, batch    33 | loss: 6.7431717CurrentTrain: epoch  3, batch    34 | loss: 5.8190374CurrentTrain: epoch  3, batch    35 | loss: 6.8154526CurrentTrain: epoch  3, batch    36 | loss: 6.0599246CurrentTrain: epoch  3, batch    37 | loss: 6.6704588CurrentTrain: epoch  4, batch     0 | loss: 6.2489414CurrentTrain: epoch  4, batch     1 | loss: 6.9139490CurrentTrain: epoch  4, batch     2 | loss: 6.8215113CurrentTrain: epoch  4, batch     3 | loss: 6.6996984CurrentTrain: epoch  4, batch     4 | loss: 7.0032768CurrentTrain: epoch  4, batch     5 | loss: 6.0609031CurrentTrain: epoch  4, batch     6 | loss: 7.0297675CurrentTrain: epoch  4, batch     7 | loss: 6.7138028CurrentTrain: epoch  4, batch     8 | loss: 7.2800207CurrentTrain: epoch  4, batch     9 | loss: 6.0930357CurrentTrain: epoch  4, batch    10 | loss: 6.6360583CurrentTrain: epoch  4, batch    11 | loss: 6.4726996CurrentTrain: epoch  4, batch    12 | loss: 6.5891943CurrentTrain: epoch  4, batch    13 | loss: 6.3953247CurrentTrain: epoch  4, batch    14 | loss: 5.8619003CurrentTrain: epoch  4, batch    15 | loss: 6.3164496CurrentTrain: epoch  4, batch    16 | loss: 6.2850175CurrentTrain: epoch  4, batch    17 | loss: 7.3138337CurrentTrain: epoch  4, batch    18 | loss: 6.1229067CurrentTrain: epoch  4, batch    19 | loss: 6.0989232CurrentTrain: epoch  4, batch    20 | loss: 6.3414903CurrentTrain: epoch  4, batch    21 | loss: 6.3103552CurrentTrain: epoch  4, batch    22 | loss: 6.1693735CurrentTrain: epoch  4, batch    23 | loss: 5.8889990CurrentTrain: epoch  4, batch    24 | loss: 6.2656651CurrentTrain: epoch  4, batch    25 | loss: 6.6743212CurrentTrain: epoch  4, batch    26 | loss: 6.5033112CurrentTrain: epoch  4, batch    27 | loss: 6.0698991CurrentTrain: epoch  4, batch    28 | loss: 6.7444296CurrentTrain: epoch  4, batch    29 | loss: 5.9560986CurrentTrain: epoch  4, batch    30 | loss: 5.8302855CurrentTrain: epoch  4, batch    31 | loss: 6.1072626CurrentTrain: epoch  4, batch    32 | loss: 5.5638819CurrentTrain: epoch  4, batch    33 | loss: 6.0502529CurrentTrain: epoch  4, batch    34 | loss: 5.7723093CurrentTrain: epoch  4, batch    35 | loss: 5.2214112CurrentTrain: epoch  4, batch    36 | loss: 6.0837131CurrentTrain: epoch  4, batch    37 | loss: 5.6245847CurrentTrain: epoch  5, batch     0 | loss: 5.9872437CurrentTrain: epoch  5, batch     1 | loss: 5.4005785CurrentTrain: epoch  5, batch     2 | loss: 5.8341322CurrentTrain: epoch  5, batch     3 | loss: 6.0954084CurrentTrain: epoch  5, batch     4 | loss: 8.0198240CurrentTrain: epoch  5, batch     5 | loss: 6.2535648CurrentTrain: epoch  5, batch     6 | loss: 5.6903715CurrentTrain: epoch  5, batch     7 | loss: 6.2086544CurrentTrain: epoch  5, batch     8 | loss: 5.8370981CurrentTrain: epoch  5, batch     9 | loss: 6.1773944CurrentTrain: epoch  5, batch    10 | loss: 5.8045721CurrentTrain: epoch  5, batch    11 | loss: 5.8567491CurrentTrain: epoch  5, batch    12 | loss: 5.7813425CurrentTrain: epoch  5, batch    13 | loss: 6.2178373CurrentTrain: epoch  5, batch    14 | loss: 6.1837616CurrentTrain: epoch  5, batch    15 | loss: 6.2117162CurrentTrain: epoch  5, batch    16 | loss: 5.5172310CurrentTrain: epoch  5, batch    17 | loss: 5.9553170CurrentTrain: epoch  5, batch    18 | loss: 5.5209274CurrentTrain: epoch  5, batch    19 | loss: 5.8181105CurrentTrain: epoch  5, batch    20 | loss: 5.9316955CurrentTrain: epoch  5, batch    21 | loss: 6.6555023CurrentTrain: epoch  5, batch    22 | loss: 6.2339163CurrentTrain: epoch  5, batch    23 | loss: 5.6159296CurrentTrain: epoch  5, batch    24 | loss: 5.6755266CurrentTrain: epoch  5, batch    25 | loss: 5.6136675CurrentTrain: epoch  5, batch    26 | loss: 5.6292944CurrentTrain: epoch  5, batch    27 | loss: 5.7195454CurrentTrain: epoch  5, batch    28 | loss: 5.5026999CurrentTrain: epoch  5, batch    29 | loss: 6.9394045CurrentTrain: epoch  5, batch    30 | loss: 5.9549470CurrentTrain: epoch  5, batch    31 | loss: 5.9532332CurrentTrain: epoch  5, batch    32 | loss: 5.4807677CurrentTrain: epoch  5, batch    33 | loss: 5.3622317CurrentTrain: epoch  5, batch    34 | loss: 6.2250767CurrentTrain: epoch  5, batch    35 | loss: 6.1277623CurrentTrain: epoch  5, batch    36 | loss: 5.9913163CurrentTrain: epoch  5, batch    37 | loss: 4.8407764CurrentTrain: epoch  6, batch     0 | loss: 5.5547996CurrentTrain: epoch  6, batch     1 | loss: 5.6077251CurrentTrain: epoch  6, batch     2 | loss: 5.5835762CurrentTrain: epoch  6, batch     3 | loss: 6.1344423CurrentTrain: epoch  6, batch     4 | loss: 5.5800405CurrentTrain: epoch  6, batch     5 | loss: 5.7692723CurrentTrain: epoch  6, batch     6 | loss: 5.1928892CurrentTrain: epoch  6, batch     7 | loss: 5.3895006CurrentTrain: epoch  6, batch     8 | loss: 5.3614798CurrentTrain: epoch  6, batch     9 | loss: 5.6295652CurrentTrain: epoch  6, batch    10 | loss: 5.5838928CurrentTrain: epoch  6, batch    11 | loss: 5.6471338CurrentTrain: epoch  6, batch    12 | loss: 5.6680408CurrentTrain: epoch  6, batch    13 | loss: 5.5321217CurrentTrain: epoch  6, batch    14 | loss: 5.3577118CurrentTrain: epoch  6, batch    15 | loss: 5.9597845CurrentTrain: epoch  6, batch    16 | loss: 5.4082980CurrentTrain: epoch  6, batch    17 | loss: 5.2012658CurrentTrain: epoch  6, batch    18 | loss: 5.3993630CurrentTrain: epoch  6, batch    19 | loss: 5.3073206CurrentTrain: epoch  6, batch    20 | loss: 5.1158042CurrentTrain: epoch  6, batch    21 | loss: 5.1988964CurrentTrain: epoch  6, batch    22 | loss: 6.7516217CurrentTrain: epoch  6, batch    23 | loss: 6.3882980CurrentTrain: epoch  6, batch    24 | loss: 5.9561749CurrentTrain: epoch  6, batch    25 | loss: 5.2372255CurrentTrain: epoch  6, batch    26 | loss: 5.5052004CurrentTrain: epoch  6, batch    27 | loss: 5.4510899CurrentTrain: epoch  6, batch    28 | loss: 5.5559216CurrentTrain: epoch  6, batch    29 | loss: 5.3476176CurrentTrain: epoch  6, batch    30 | loss: 5.5067139CurrentTrain: epoch  6, batch    31 | loss: 5.5220561CurrentTrain: epoch  6, batch    32 | loss: 5.1367607CurrentTrain: epoch  6, batch    33 | loss: 6.7668796CurrentTrain: epoch  6, batch    34 | loss: 5.5712180CurrentTrain: epoch  6, batch    35 | loss: 5.5531726CurrentTrain: epoch  6, batch    36 | loss: 6.6285076CurrentTrain: epoch  6, batch    37 | loss: 5.0813398CurrentTrain: epoch  7, batch     0 | loss: 5.5609903CurrentTrain: epoch  7, batch     1 | loss: 5.5071807CurrentTrain: epoch  7, batch     2 | loss: 5.5841031CurrentTrain: epoch  7, batch     3 | loss: 5.8392434CurrentTrain: epoch  7, batch     4 | loss: 5.4410934CurrentTrain: epoch  7, batch     5 | loss: 5.2984691CurrentTrain: epoch  7, batch     6 | loss: 5.4871926CurrentTrain: epoch  7, batch     7 | loss: 5.8967161CurrentTrain: epoch  7, batch     8 | loss: 5.3586512CurrentTrain: epoch  7, batch     9 | loss: 5.2871027CurrentTrain: epoch  7, batch    10 | loss: 5.2546878CurrentTrain: epoch  7, batch    11 | loss: 4.9347744CurrentTrain: epoch  7, batch    12 | loss: 5.8759284CurrentTrain: epoch  7, batch    13 | loss: 5.2580881CurrentTrain: epoch  7, batch    14 | loss: 5.1904106CurrentTrain: epoch  7, batch    15 | loss: 5.2352996CurrentTrain: epoch  7, batch    16 | loss: 5.3971124CurrentTrain: epoch  7, batch    17 | loss: 5.4344935CurrentTrain: epoch  7, batch    18 | loss: 5.3451357CurrentTrain: epoch  7, batch    19 | loss: 5.0849705CurrentTrain: epoch  7, batch    20 | loss: 5.1249332CurrentTrain: epoch  7, batch    21 | loss: 5.2973261CurrentTrain: epoch  7, batch    22 | loss: 6.0297012CurrentTrain: epoch  7, batch    23 | loss: 5.5495868CurrentTrain: epoch  7, batch    24 | loss: 5.2133803CurrentTrain: epoch  7, batch    25 | loss: 5.1576505CurrentTrain: epoch  7, batch    26 | loss: 5.1422796CurrentTrain: epoch  7, batch    27 | loss: 5.1132975CurrentTrain: epoch  7, batch    28 | loss: 4.9744816CurrentTrain: epoch  7, batch    29 | loss: 5.7927094CurrentTrain: epoch  7, batch    30 | loss: 5.2160301CurrentTrain: epoch  7, batch    31 | loss: 5.2629786CurrentTrain: epoch  7, batch    32 | loss: 5.8652630CurrentTrain: epoch  7, batch    33 | loss: 5.5444746CurrentTrain: epoch  7, batch    34 | loss: 6.6703720CurrentTrain: epoch  7, batch    35 | loss: 5.9917049CurrentTrain: epoch  7, batch    36 | loss: 5.4586973CurrentTrain: epoch  7, batch    37 | loss: 5.2874126CurrentTrain: epoch  8, batch     0 | loss: 5.5083227CurrentTrain: epoch  8, batch     1 | loss: 5.4225760CurrentTrain: epoch  8, batch     2 | loss: 5.8430738CurrentTrain: epoch  8, batch     3 | loss: 5.5593462CurrentTrain: epoch  8, batch     4 | loss: 5.4635201CurrentTrain: epoch  8, batch     5 | loss: 5.0873146CurrentTrain: epoch  8, batch     6 | loss: 5.1428623CurrentTrain: epoch  8, batch     7 | loss: 5.0521269CurrentTrain: epoch  8, batch     8 | loss: 5.4818096CurrentTrain: epoch  8, batch     9 | loss: 4.9598989CurrentTrain: epoch  8, batch    10 | loss: 5.1151066CurrentTrain: epoch  8, batch    11 | loss: 4.9622831CurrentTrain: epoch  8, batch    12 | loss: 4.9481797CurrentTrain: epoch  8, batch    13 | loss: 4.9111986CurrentTrain: epoch  8, batch    14 | loss: 4.9557219CurrentTrain: epoch  8, batch    15 | loss: 5.0714278CurrentTrain: epoch  8, batch    16 | loss: 4.9566622CurrentTrain: epoch  8, batch    17 | loss: 5.1959500CurrentTrain: epoch  8, batch    18 | loss: 4.9190617CurrentTrain: epoch  8, batch    19 | loss: 5.8676329CurrentTrain: epoch  8, batch    20 | loss: 5.1642084CurrentTrain: epoch  8, batch    21 | loss: 5.0517421CurrentTrain: epoch  8, batch    22 | loss: 4.9425077CurrentTrain: epoch  8, batch    23 | loss: 5.0369940CurrentTrain: epoch  8, batch    24 | loss: 5.6340046CurrentTrain: epoch  8, batch    25 | loss: 5.2729049CurrentTrain: epoch  8, batch    26 | loss: 5.0862513CurrentTrain: epoch  8, batch    27 | loss: 5.0260410CurrentTrain: epoch  8, batch    28 | loss: 5.1333985CurrentTrain: epoch  8, batch    29 | loss: 5.1072674CurrentTrain: epoch  8, batch    30 | loss: 5.2668891CurrentTrain: epoch  8, batch    31 | loss: 5.1919985CurrentTrain: epoch  8, batch    32 | loss: 5.8619947CurrentTrain: epoch  8, batch    33 | loss: 4.9207497CurrentTrain: epoch  8, batch    34 | loss: 4.9847245CurrentTrain: epoch  8, batch    35 | loss: 5.0330801CurrentTrain: epoch  8, batch    36 | loss: 5.1846180CurrentTrain: epoch  8, batch    37 | loss: 5.1099491CurrentTrain: epoch  9, batch     0 | loss: 4.9323254CurrentTrain: epoch  9, batch     1 | loss: 5.0394716CurrentTrain: epoch  9, batch     2 | loss: 4.9611087CurrentTrain: epoch  9, batch     3 | loss: 5.4201789CurrentTrain: epoch  9, batch     4 | loss: 4.9534369CurrentTrain: epoch  9, batch     5 | loss: 5.0876293CurrentTrain: epoch  9, batch     6 | loss: 4.9941483CurrentTrain: epoch  9, batch     7 | loss: 5.0381145CurrentTrain: epoch  9, batch     8 | loss: 4.9778786CurrentTrain: epoch  9, batch     9 | loss: 5.0629029CurrentTrain: epoch  9, batch    10 | loss: 4.9532738CurrentTrain: epoch  9, batch    11 | loss: 5.1267118CurrentTrain: epoch  9, batch    12 | loss: 4.9238472CurrentTrain: epoch  9, batch    13 | loss: 4.9978037CurrentTrain: epoch  9, batch    14 | loss: 4.9796124CurrentTrain: epoch  9, batch    15 | loss: 4.9739547CurrentTrain: epoch  9, batch    16 | loss: 5.0294361CurrentTrain: epoch  9, batch    17 | loss: 4.8867884CurrentTrain: epoch  9, batch    18 | loss: 4.9829035CurrentTrain: epoch  9, batch    19 | loss: 4.8389482CurrentTrain: epoch  9, batch    20 | loss: 4.9687490CurrentTrain: epoch  9, batch    21 | loss: 4.8972168CurrentTrain: epoch  9, batch    22 | loss: 4.7841969CurrentTrain: epoch  9, batch    23 | loss: 4.8289423CurrentTrain: epoch  9, batch    24 | loss: 5.1305075CurrentTrain: epoch  9, batch    25 | loss: 5.3241606CurrentTrain: epoch  9, batch    26 | loss: 5.0097022CurrentTrain: epoch  9, batch    27 | loss: 4.8564391CurrentTrain: epoch  9, batch    28 | loss: 4.9755569CurrentTrain: epoch  9, batch    29 | loss: 4.7278547CurrentTrain: epoch  9, batch    30 | loss: 4.9005756CurrentTrain: epoch  9, batch    31 | loss: 5.1333466CurrentTrain: epoch  9, batch    32 | loss: 4.8360300CurrentTrain: epoch  9, batch    33 | loss: 5.0445766CurrentTrain: epoch  9, batch    34 | loss: 5.2298670CurrentTrain: epoch  9, batch    35 | loss: 4.8302641CurrentTrain: epoch  9, batch    36 | loss: 4.9972177CurrentTrain: epoch  9, batch    37 | loss: 4.7948170
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: the head of iran 's atomic energy organisation , ali akbar salehi , told domestic news agencies the new proposal had come in response to iran 's move last week to begin enriching uranium itself to the 20 percent level required for a tehran medical research rector after rejecting a previous offer .
Head Entity: ali akbar salehi
Tail Entity: iran
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the United States, Maria decided to return to her hometown in Spain, where she felt a strong connection to her roots.  
Head Entity: Maria  
Tail Entity: Spain  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: whether this means the country 's tense political situation will dissolve back into civil war is yet to be seen , said rinaldo depagne , a west africa analyst at the international crisis group .
Head Entity: international crisis group
Tail Entity: rinaldo depagne
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: the board of directors at tech innovations inc. has appointed a new chief executive officer to lead the company into its next phase of growth.  
Head Entity: tech innovations inc.  
Tail Entity: chief executive officer
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: linebacker mike peterson rejoined the jacksonville jaguars on friday , two days after coach jack del rio banished him from the locker room of the national football league team .
Head Entity: jacksonville jaguars
Tail Entity: national football league
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: the united nations is an international organization that promotes peace and cooperation among its member states.  
Head Entity: united nations  
Tail Entity: member states  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: the wife of acting cuban president raul castro , vilma espin guillois , died monday after a lengthy illness , cuban television reported monday .
Head Entity: raul castro
Tail Entity: cuban
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: the famous actor and director, alfonso cuaron, was born in mexico and has often spoken about his love for his homeland.  
Head Entity: alfonso cuaron  
Tail Entity: mexico  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: `` we are not canceling any of our orders for next year , '' deputy managing director philip chen was quoted as saying in today 's south china morning post .
Head Entity: philip chen
Tail Entity: director
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: `` the renowned scientist dr. jane smith has been awarded the prestigious title of chief researcher at the national institute of health . ''  
Head Entity: dr. jane smith  
Tail Entity: chief researcher  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: as the government still imposes many restrictions on investing in china , hochen said , chunghwa telecom will consult with the mainland affairs council -- taiwan 's top china policy planning agency -- and other relevant government institutions before launching its overseas expansion drive .
Head Entity: chunghwa telecom
Tail Entity: china
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: after years of rapid growth, the tech giant has decided to establish its new headquarters in the heart of silicon valley, aiming to strengthen its presence in the united states.  
Head Entity: tech giant  
Tail Entity: united states  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 86.16%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.23%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.60%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.20%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.26%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.95%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.74%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 86.16%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.23%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.60%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.20%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.26%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.95%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.74%   
cur_acc:  ['0.8674']
his_acc:  ['0.8674']
CurrentTrain: epoch  0, batch     0 | loss: 6.2508774CurrentTrain: epoch  0, batch     1 | loss: 7.0173969CurrentTrain: epoch  1, batch     0 | loss: 5.5441074CurrentTrain: epoch  1, batch     1 | loss: 6.7372098CurrentTrain: epoch  2, batch     0 | loss: 5.4576530CurrentTrain: epoch  2, batch     1 | loss: 5.8471255CurrentTrain: epoch  3, batch     0 | loss: 4.7888002CurrentTrain: epoch  3, batch     1 | loss: 5.5919361CurrentTrain: epoch  4, batch     0 | loss: 4.5885291CurrentTrain: epoch  4, batch     1 | loss: 5.1951432CurrentTrain: epoch  5, batch     0 | loss: 4.9872861CurrentTrain: epoch  5, batch     1 | loss: 3.5513031CurrentTrain: epoch  6, batch     0 | loss: 3.8067145CurrentTrain: epoch  6, batch     1 | loss: 4.6832795CurrentTrain: epoch  7, batch     0 | loss: 3.9353924CurrentTrain: epoch  7, batch     1 | loss: 3.9711919CurrentTrain: epoch  8, batch     0 | loss: 3.9946413CurrentTrain: epoch  8, batch     1 | loss: 3.1361611CurrentTrain: epoch  9, batch     0 | loss: 3.7065210CurrentTrain: epoch  9, batch     1 | loss: 3.1331658
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: san jose , ca , usa speaking of k-fed , him and ex-wife britney spears are in court today , dealing with their custody battle .
Head Entity: britney spears
Tail Entity: ca
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: after moving to austin, tx, the musician found a new sense of inspiration for his music.  
Head Entity: musician  
Tail Entity: tx  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: he passed away on saturday .
Head Entity: he
Tail Entity: saturday
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: she left this world on March 5th.  
Head Entity: she  
Tail Entity: March 5th  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: covidien , which posted revenue of more than $ 10 billion last year , has about 42,000 employees worldwide .
Head Entity: covidien
Tail Entity: 42,000
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: Google, known for its innovative technology, employs approximately 156,500 people globally.  
Head Entity: Google  
Tail Entity: 156,500  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: `` i am known in the hospice as the man who would n't die , '' buchwald wrote in march .
Head Entity: buchwald
Tail Entity: man who would n't die
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: `` the famous author often referred to as the bard of Avon is none other than william shakespeare. ''  
Head Entity: william shakespeare  
Tail Entity: bard of Avon  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: beverly hills , california 2008-08-17 21:15:39 utc ------ there was much dancing : ellen degeneres and portia de rossi are married , according to reports .
Head Entity: ellen degeneres
Tail Entity: portia de rossi
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: in a beautiful ceremony held in new york city, 2015-06-20 15:30:00 utc ------ the couple exchanged vows: john legend and chrissy teigen are now husband and wife, as confirmed by their friends.  
Head Entity: john legend  
Tail Entity: chrissy teigen  
Mixup data size:  83
MixupTrain:  epoch  0, batch     0 | loss: 8.3761501MixupTrain:  epoch  0, batch     1 | loss: 9.0350170MixupTrain:  epoch  0, batch     2 | loss: 8.3281889MixupTrain:  epoch  0, batch     3 | loss: 7.7385144MixupTrain:  epoch  0, batch     4 | loss: 7.7625580MixupTrain:  epoch  0, batch     5 | loss: 9.6661873
MemoryTrain:  epoch  0, batch     0 | loss: 5.8539696MemoryTrain:  epoch  0, batch     1 | loss: 5.8890548MemoryTrain:  epoch  1, batch     0 | loss: 6.5392456MemoryTrain:  epoch  1, batch     1 | loss: 5.0072432MemoryTrain:  epoch  2, batch     0 | loss: 5.7834287MemoryTrain:  epoch  2, batch     1 | loss: 4.4426904MemoryTrain:  epoch  3, batch     0 | loss: 4.8866825MemoryTrain:  epoch  3, batch     1 | loss: 4.5856280MemoryTrain:  epoch  4, batch     0 | loss: 4.3989434MemoryTrain:  epoch  4, batch     1 | loss: 4.0880609
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 29.17%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 28.12%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 28.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 38.54%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 47.32%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 53.12%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 57.64%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 58.75%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 59.66%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 64.42%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 65.42%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 80.56%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 83.85%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 84.13%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 83.04%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 80.88%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 79.86%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 80.26%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 80.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 81.55%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.15%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 83.85%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.10%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.42%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 86.67%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 86.90%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.11%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 86.17%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 84.38%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 82.68%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 81.08%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 79.73%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 79.61%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 79.97%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 80.64%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 81.10%   [EVAL] batch:   42 | acc: 37.50%,  total acc: 80.09%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 80.54%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 80.69%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 80.98%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 80.98%   
cur_acc:  ['0.8674', '0.6542']
his_acc:  ['0.8674', '0.8098']
CurrentTrain: epoch  0, batch     0 | loss: 7.1513891CurrentTrain: epoch  0, batch     1 | loss: 6.5392933CurrentTrain: epoch  1, batch     0 | loss: 5.9478054CurrentTrain: epoch  1, batch     1 | loss: 6.6542864CurrentTrain: epoch  2, batch     0 | loss: 5.5835605CurrentTrain: epoch  2, batch     1 | loss: 4.6487412CurrentTrain: epoch  3, batch     0 | loss: 4.8541231CurrentTrain: epoch  3, batch     1 | loss: 4.6789613CurrentTrain: epoch  4, batch     0 | loss: 4.6443591CurrentTrain: epoch  4, batch     1 | loss: 4.2191925CurrentTrain: epoch  5, batch     0 | loss: 4.3188214CurrentTrain: epoch  5, batch     1 | loss: 4.3851342CurrentTrain: epoch  6, batch     0 | loss: 4.3132329CurrentTrain: epoch  6, batch     1 | loss: 3.5884082CurrentTrain: epoch  7, batch     0 | loss: 4.1330280CurrentTrain: epoch  7, batch     1 | loss: 3.3535187CurrentTrain: epoch  8, batch     0 | loss: 3.3758807CurrentTrain: epoch  8, batch     1 | loss: 3.1344776CurrentTrain: epoch  9, batch     0 | loss: 3.0418403CurrentTrain: epoch  9, batch     1 | loss: 3.0192688
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: born in 1950 in the northeastern city of basel , ospel left school at 15 to take an apprenticeship at the transvalor brokerage house before joining swiss banking corporation -lrb- sbs -rrb- , which merged with union bank of switzerland to form ubs in 1998 .
Head Entity: ospel
Tail Entity: 1950
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: she was born on July 4, 1985, in a small town in California, where she spent her childhood before moving to New York for her career.  
Head Entity: she  
Tail Entity: July 4, 1985  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: jane matilda bolin was born on april 11 , 1908 , in poughkeepsie , ny .
Head Entity: jane matilda bolin
Tail Entity: ny
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: john smith was born in los angeles, california, on march 5, 1980.  
Head Entity: john smith  
Tail Entity: california  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: lynne spears told the court that lutfi had treated her daughter like a hostage in her own home , drugged her and took over her finances .
Head Entity: her
Tail Entity: lynne spears
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: john's mother, mary, always encouraged him to pursue his dreams and supported him throughout his education.  
Head Entity: john  
Tail Entity: mary  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: after years of hard work, jane doe finally received a promotion at tech innovations, where she has been a dedicated employee.  
Head Entity: jane doe  
Tail Entity: tech innovations  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, 75, passed away peacefully in his sleep on march 5 in the beautiful town of springfield, il, surrounded by family and friends.  
Head Entity: john doe  
Tail Entity: il  
Mixup data size:  102
MixupTrain:  epoch  0, batch     0 | loss: 8.1008568MixupTrain:  epoch  0, batch     1 | loss: 7.1344333MixupTrain:  epoch  0, batch     2 | loss: 7.4015808MixupTrain:  epoch  0, batch     3 | loss: 7.5423284MixupTrain:  epoch  0, batch     4 | loss: 6.6661596MixupTrain:  epoch  0, batch     5 | loss: 7.4940014MixupTrain:  epoch  0, batch     6 | loss: 7.2635627
MemoryTrain:  epoch  0, batch     0 | loss: 4.7455330MemoryTrain:  epoch  0, batch     1 | loss: 4.4943972MemoryTrain:  epoch  1, batch     0 | loss: 4.3233900MemoryTrain:  epoch  1, batch     1 | loss: 5.0763268MemoryTrain:  epoch  2, batch     0 | loss: 4.8875122MemoryTrain:  epoch  2, batch     1 | loss: 3.7555709MemoryTrain:  epoch  3, batch     0 | loss: 4.5091991MemoryTrain:  epoch  3, batch     1 | loss: 3.6868916MemoryTrain:  epoch  4, batch     0 | loss: 3.5099978MemoryTrain:  epoch  4, batch     1 | loss: 3.8412662
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 69.64%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 76.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 77.27%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 77.60%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 77.88%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 73.66%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 67.19%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 80.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 83.17%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 81.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 80.08%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 79.78%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 78.82%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 79.28%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.65%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 81.53%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 82.34%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 83.07%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 84.72%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.27%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.78%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 86.04%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 86.29%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.52%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 85.23%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 83.09%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 80.89%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 78.99%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 77.20%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 76.81%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 76.76%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 77.13%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:   42 | acc: 25.00%,  total acc: 76.45%   [EVAL] batch:   43 | acc: 37.50%,  total acc: 75.57%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 73.51%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 72.87%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 72.40%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 72.45%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 72.88%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 72.55%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 72.72%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 72.64%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 72.57%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 72.84%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 73.57%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 73.71%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 73.94%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 73.85%   [EVAL] batch:   60 | acc: 12.50%,  total acc: 72.85%   
cur_acc:  ['0.8674', '0.6542', '0.7366']
his_acc:  ['0.8674', '0.8098', '0.7285']
CurrentTrain: epoch  0, batch     0 | loss: 5.7772460CurrentTrain: epoch  0, batch     1 | loss: 6.5485635CurrentTrain: epoch  1, batch     0 | loss: 5.4729695CurrentTrain: epoch  1, batch     1 | loss: 5.1266241CurrentTrain: epoch  2, batch     0 | loss: 5.0515985CurrentTrain: epoch  2, batch     1 | loss: 4.8528047CurrentTrain: epoch  3, batch     0 | loss: 4.6987085CurrentTrain: epoch  3, batch     1 | loss: 4.3258677CurrentTrain: epoch  4, batch     0 | loss: 3.9143643CurrentTrain: epoch  4, batch     1 | loss: 3.8337767CurrentTrain: epoch  5, batch     0 | loss: 3.7573247CurrentTrain: epoch  5, batch     1 | loss: 3.7606294CurrentTrain: epoch  6, batch     0 | loss: 3.5196342CurrentTrain: epoch  6, batch     1 | loss: 3.4798081CurrentTrain: epoch  7, batch     0 | loss: 3.4242764CurrentTrain: epoch  7, batch     1 | loss: 2.7885017CurrentTrain: epoch  8, batch     0 | loss: 3.0114043CurrentTrain: epoch  8, batch     1 | loss: 3.0755930CurrentTrain: epoch  9, batch     0 | loss: 2.6660824CurrentTrain: epoch  9, batch     1 | loss: 2.6737220
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: tv-idol-johns -- atlanta -- `` american idol '' finalist michael johns moved to los angeles several years ago , but his heart is still in atlanta .
Head Entity: his
Tail Entity: atlanta
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: actress-sarah -- new york -- `` the talented actress sarah has always considered new york her home, even after moving to los angeles for her career. ''  
Head Entity: sarah  
Tail Entity: new york  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: her political involvement began early : at cornell , she helped organize local farmers ' cooperatives .
Head Entity: she
Tail Entity: cornell
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: After graduating from high school, he went on to study at Stanford University, where he excelled in his courses.  
Head Entity: he  
Tail Entity: Stanford University  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: us republican congresswoman jo ann davis dies after fight with breast cancer
Head Entity: jo ann davis
Tail Entity: us
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england  
Head Entity: stephen hawking  
Tail Entity: england  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by his wife of 63 years , josephine robinson mcnair , of columbia ; a son , robert e. jr. , of columbia ; three daughters , robin lee howell and corinne godshall , of myrtle beach , s.c. , and claudia crawford mcnair , of jamestown , s.c. ; six grandchildren ; and one great-grandchild .
Head Entity: he
Tail Entity: claudia crawford mcnair
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has two children, a son named michael and a daughter named sarah, who both live in new york.  
Head Entity: she  
Tail Entity: sarah  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: ferrara said he was innocent of limoli 's slaying , but he pleaded guilty in 1992 to murder , along with racketeering charges , under a deal that sent him to prison for 22 years , rather than go to trial and risk a conviction that could lead to life in prison .
Head Entity: ferrara
Tail Entity: racketeering
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: after a lengthy investigation, the authorities announced that johnson was charged with embezzlement, which shocked his colleagues at the firm.  
Head Entity: johnson  
Tail Entity: embezzlement  
Mixup data size:  122
MixupTrain:  epoch  0, batch     0 | loss: 6.5711455MixupTrain:  epoch  0, batch     1 | loss: 6.8787365MixupTrain:  epoch  0, batch     2 | loss: 6.6697826MixupTrain:  epoch  0, batch     3 | loss: 6.5346212MixupTrain:  epoch  0, batch     4 | loss: 6.3424859MixupTrain:  epoch  0, batch     5 | loss: 6.5379448MixupTrain:  epoch  0, batch     6 | loss: 6.0762005MixupTrain:  epoch  0, batch     7 | loss: 6.2363462
MemoryTrain:  epoch  0, batch     0 | loss: 4.0194373MemoryTrain:  epoch  0, batch     1 | loss: 4.1186028MemoryTrain:  epoch  0, batch     2 | loss: 2.7572370MemoryTrain:  epoch  1, batch     0 | loss: 4.3206182MemoryTrain:  epoch  1, batch     1 | loss: 4.1332245MemoryTrain:  epoch  1, batch     2 | loss: 3.0832875#############params############
cuda:0
Task=Tacred, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=1, gen_num=0
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 12.1690187CurrentTrain: epoch  0, batch     1 | loss: 11.6995430CurrentTrain: epoch  0, batch     2 | loss: 11.6589003CurrentTrain: epoch  0, batch     3 | loss: 11.6688824CurrentTrain: epoch  0, batch     4 | loss: 11.3436146CurrentTrain: epoch  0, batch     5 | loss: 11.3503590CurrentTrain: epoch  0, batch     6 | loss: 11.4282379CurrentTrain: epoch  0, batch     7 | loss: 11.5621490CurrentTrain: epoch  0, batch     8 | loss: 10.9635983CurrentTrain: epoch  0, batch     9 | loss: 11.1142664CurrentTrain: epoch  0, batch    10 | loss: 11.0515976CurrentTrain: epoch  0, batch    11 | loss: 10.9214954CurrentTrain: epoch  0, batch    12 | loss: 11.1165590CurrentTrain: epoch  0, batch    13 | loss: 10.7246609CurrentTrain: epoch  0, batch    14 | loss: 10.4133291CurrentTrain: epoch  0, batch    15 | loss: 10.2881021CurrentTrain: epoch  0, batch    16 | loss: 9.7268009CurrentTrain: epoch  0, batch    17 | loss: 9.9702463CurrentTrain: epoch  0, batch    18 | loss: 10.0619869CurrentTrain: epoch  0, batch    19 | loss: 10.8209829CurrentTrain: epoch  0, batch    20 | loss: 10.1109295CurrentTrain: epoch  0, batch    21 | loss: 10.8148203CurrentTrain: epoch  0, batch    22 | loss: 10.6081085CurrentTrain: epoch  0, batch    23 | loss: 10.1635332CurrentTrain: epoch  0, batch    24 | loss: 10.1073608CurrentTrain: epoch  0, batch    25 | loss: 10.1251621CurrentTrain: epoch  0, batch    26 | loss: 10.1027117CurrentTrain: epoch  0, batch    27 | loss: 9.4231672CurrentTrain: epoch  0, batch    28 | loss: 9.8402300CurrentTrain: epoch  0, batch    29 | loss: 9.8299923CurrentTrain: epoch  0, batch    30 | loss: 9.2956877CurrentTrain: epoch  0, batch    31 | loss: 9.9034863CurrentTrain: epoch  0, batch    32 | loss: 9.5128641CurrentTrain: epoch  0, batch    33 | loss: 9.7674255CurrentTrain: epoch  0, batch    34 | loss: 8.7186298CurrentTrain: epoch  0, batch    35 | loss: 9.5651779CurrentTrain: epoch  0, batch    36 | loss: 9.7107916CurrentTrain: epoch  0, batch    37 | loss: 9.2939758CurrentTrain: epoch  1, batch     0 | loss: 9.4023247CurrentTrain: epoch  1, batch     1 | loss: 9.9626904CurrentTrain: epoch  1, batch     2 | loss: 8.6357880CurrentTrain: epoch  1, batch     3 | loss: 8.9070873CurrentTrain: epoch  1, batch     4 | loss: 8.5332546CurrentTrain: epoch  1, batch     5 | loss: 9.6538429CurrentTrain: epoch  1, batch     6 | loss: 8.7505426CurrentTrain: epoch  1, batch     7 | loss: 8.8227167CurrentTrain: epoch  1, batch     8 | loss: 8.9023333CurrentTrain: epoch  1, batch     9 | loss: 8.7353191CurrentTrain: epoch  1, batch    10 | loss: 9.6049299CurrentTrain: epoch  1, batch    11 | loss: 9.5069838CurrentTrain: epoch  1, batch    12 | loss: 9.3775721CurrentTrain: epoch  1, batch    13 | loss: 8.5770206CurrentTrain: epoch  1, batch    14 | loss: 9.1144257CurrentTrain: epoch  1, batch    15 | loss: 8.6440697CurrentTrain: epoch  1, batch    16 | loss: 8.3170395CurrentTrain: epoch  1, batch    17 | loss: 9.1284351CurrentTrain: epoch  1, batch    18 | loss: 8.7919922CurrentTrain: epoch  1, batch    19 | loss: 9.1051159CurrentTrain: epoch  1, batch    20 | loss: 9.1359072CurrentTrain: epoch  1, batch    21 | loss: 9.0230713CurrentTrain: epoch  1, batch    22 | loss: 8.6631136CurrentTrain: epoch  1, batch    23 | loss: 8.4228230CurrentTrain: epoch  1, batch    24 | loss: 8.9080448CurrentTrain: epoch  1, batch    25 | loss: 7.9236660CurrentTrain: epoch  1, batch    26 | loss: 8.9280272CurrentTrain: epoch  1, batch    27 | loss: 8.0987244CurrentTrain: epoch  1, batch    28 | loss: 8.5379629CurrentTrain: epoch  1, batch    29 | loss: 7.4539938CurrentTrain: epoch  1, batch    30 | loss: 8.1493731CurrentTrain: epoch  1, batch    31 | loss: 8.9826469CurrentTrain: epoch  1, batch    32 | loss: 8.4568977CurrentTrain: epoch  1, batch    33 | loss: 8.0884867CurrentTrain: epoch  1, batch    34 | loss: 8.3476505CurrentTrain: epoch  1, batch    35 | loss: 7.9553928CurrentTrain: epoch  1, batch    36 | loss: 8.2962074CurrentTrain: epoch  1, batch    37 | loss: 8.9347305CurrentTrain: epoch  2, batch     0 | loss: 7.5439439CurrentTrain: epoch  2, batch     1 | loss: 8.3211079CurrentTrain: epoch  2, batch     2 | loss: 8.4436703CurrentTrain: epoch  2, batch     3 | loss: 8.0877228CurrentTrain: epoch  2, batch     4 | loss: 8.6757050CurrentTrain: epoch  2, batch     5 | loss: 8.9563208CurrentTrain: epoch  2, batch     6 | loss: 8.1111326CurrentTrain: epoch  2, batch     7 | loss: 7.8398848CurrentTrain: epoch  2, batch     8 | loss: 7.9502220CurrentTrain: epoch  2, batch     9 | loss: 8.0520477CurrentTrain: epoch  2, batch    10 | loss: 7.8860197CurrentTrain: epoch  2, batch    11 | loss: 8.0253963CurrentTrain: epoch  2, batch    12 | loss: 7.5002537CurrentTrain: epoch  2, batch    13 | loss: 7.6439800CurrentTrain: epoch  2, batch    14 | loss: 6.7285566CurrentTrain: epoch  2, batch    15 | loss: 7.4282532CurrentTrain: epoch  2, batch    16 | loss: 7.9322014CurrentTrain: epoch  2, batch    17 | loss: 8.0837545CurrentTrain: epoch  2, batch    18 | loss: 7.5659533CurrentTrain: epoch  2, batch    19 | loss: 7.5311089CurrentTrain: epoch  2, batch    20 | loss: 7.7719784CurrentTrain: epoch  2, batch    21 | loss: 7.5619941CurrentTrain: epoch  2, batch    22 | loss: 6.7086825CurrentTrain: epoch  2, batch    23 | loss: 7.0743079CurrentTrain: epoch  2, batch    24 | loss: 7.5313606CurrentTrain: epoch  2, batch    25 | loss: 7.9927979CurrentTrain: epoch  2, batch    26 | loss: 7.0219460CurrentTrain: epoch  2, batch    27 | loss: 8.2197342CurrentTrain: epoch  2, batch    28 | loss: 6.5037155CurrentTrain: epoch  2, batch    29 | loss: 7.7327476CurrentTrain: epoch  2, batch    30 | loss: 7.3827395CurrentTrain: epoch  2, batch    31 | loss: 6.9715390CurrentTrain: epoch  2, batch    32 | loss: 7.4454002CurrentTrain: epoch  2, batch    33 | loss: 7.2365036CurrentTrain: epoch  2, batch    34 | loss: 8.2956200CurrentTrain: epoch  2, batch    35 | loss: 7.0447979CurrentTrain: epoch  2, batch    36 | loss: 7.4715118CurrentTrain: epoch  2, batch    37 | loss: 6.9003510CurrentTrain: epoch  3, batch     0 | loss: 7.6211905CurrentTrain: epoch  3, batch     1 | loss: 7.3738432CurrentTrain: epoch  3, batch     2 | loss: 7.6698308CurrentTrain: epoch  3, batch     3 | loss: 8.0898123CurrentTrain: epoch  3, batch     4 | loss: 7.0152826CurrentTrain: epoch  3, batch     5 | loss: 7.6061339CurrentTrain: epoch  3, batch     6 | loss: 8.0203104CurrentTrain: epoch  3, batch     7 | loss: 6.7523293CurrentTrain: epoch  3, batch     8 | loss: 7.5701141CurrentTrain: epoch  3, batch     9 | loss: 7.7911401CurrentTrain: epoch  3, batch    10 | loss: 7.0391908CurrentTrain: epoch  3, batch    11 | loss: 6.4257593CurrentTrain: epoch  3, batch    12 | loss: 7.5253706CurrentTrain: epoch  3, batch    13 | loss: 8.2665205CurrentTrain: epoch  3, batch    14 | loss: 6.8255453CurrentTrain: epoch  3, batch    15 | loss: 7.1854143CurrentTrain: epoch  3, batch    16 | loss: 8.0643663CurrentTrain: epoch  3, batch    17 | loss: 6.9001007CurrentTrain: epoch  3, batch    18 | loss: 7.2935939CurrentTrain: epoch  3, batch    19 | loss: 7.4163027CurrentTrain: epoch  3, batch    20 | loss: 7.4152446CurrentTrain: epoch  3, batch    21 | loss: 7.1919003CurrentTrain: epoch  3, batch    22 | loss: 7.6313944CurrentTrain: epoch  3, batch    23 | loss: 7.7543411CurrentTrain: epoch  3, batch    24 | loss: 6.2503614CurrentTrain: epoch  3, batch    25 | loss: 6.6826630CurrentTrain: epoch  3, batch    26 | loss: 6.6438017CurrentTrain: epoch  3, batch    27 | loss: 7.5813265CurrentTrain: epoch  3, batch    28 | loss: 6.9470558CurrentTrain: epoch  3, batch    29 | loss: 5.9585752CurrentTrain: epoch  3, batch    30 | loss: 6.9753571CurrentTrain: epoch  3, batch    31 | loss: 7.3043118CurrentTrain: epoch  3, batch    32 | loss: 5.9646759CurrentTrain: epoch  3, batch    33 | loss: 5.8874893CurrentTrain: epoch  3, batch    34 | loss: 6.3907361CurrentTrain: epoch  3, batch    35 | loss: 6.3238010CurrentTrain: epoch  3, batch    36 | loss: 6.6620164CurrentTrain: epoch  3, batch    37 | loss: 6.7405491CurrentTrain: epoch  4, batch     0 | loss: 6.8946047CurrentTrain: epoch  4, batch     1 | loss: 6.2533736CurrentTrain: epoch  4, batch     2 | loss: 5.7171106CurrentTrain: epoch  4, batch     3 | loss: 6.6145258CurrentTrain: epoch  4, batch     4 | loss: 7.1917934CurrentTrain: epoch  4, batch     5 | loss: 6.8115988CurrentTrain: epoch  4, batch     6 | loss: 6.4191608CurrentTrain: epoch  4, batch     7 | loss: 6.8339691CurrentTrain: epoch  4, batch     8 | loss: 7.3339534CurrentTrain: epoch  4, batch     9 | loss: 6.4404263CurrentTrain: epoch  4, batch    10 | loss: 6.9395852CurrentTrain: epoch  4, batch    11 | loss: 5.9660683CurrentTrain: epoch  4, batch    12 | loss: 6.5450406CurrentTrain: epoch  4, batch    13 | loss: 6.4390965CurrentTrain: epoch  4, batch    14 | loss: 6.7354364CurrentTrain: epoch  4, batch    15 | loss: 6.6291456CurrentTrain: epoch  4, batch    16 | loss: 6.4879165CurrentTrain: epoch  4, batch    17 | loss: 6.1289062CurrentTrain: epoch  4, batch    18 | loss: 6.0823526CurrentTrain: epoch  4, batch    19 | loss: 6.2051773CurrentTrain: epoch  4, batch    20 | loss: 6.4596114CurrentTrain: epoch  4, batch    21 | loss: 6.4924083CurrentTrain: epoch  4, batch    22 | loss: 6.4912758CurrentTrain: epoch  4, batch    23 | loss: 5.7416620CurrentTrain: epoch  4, batch    24 | loss: 6.5663033CurrentTrain: epoch  4, batch    25 | loss: 6.4283700CurrentTrain: epoch  4, batch    26 | loss: 5.8424082CurrentTrain: epoch  4, batch    27 | loss: 7.3929849CurrentTrain: epoch  4, batch    28 | loss: 5.9247952CurrentTrain: epoch  4, batch    29 | loss: 6.1835108CurrentTrain: epoch  4, batch    30 | loss: 5.8831282CurrentTrain: epoch  4, batch    31 | loss: 5.9290934CurrentTrain: epoch  4, batch    32 | loss: 6.8203969CurrentTrain: epoch  4, batch    33 | loss: 5.9183788CurrentTrain: epoch  4, batch    34 | loss: 6.9975796CurrentTrain: epoch  4, batch    35 | loss: 6.2145743CurrentTrain: epoch  4, batch    36 | loss: 5.9983635CurrentTrain: epoch  4, batch    37 | loss: 7.5984650CurrentTrain: epoch  5, batch     0 | loss: 6.0765152CurrentTrain: epoch  5, batch     1 | loss: 6.0253639CurrentTrain: epoch  5, batch     2 | loss: 6.3275099CurrentTrain: epoch  5, batch     3 | loss: 6.2672338CurrentTrain: epoch  5, batch     4 | loss: 6.5294123CurrentTrain: epoch  5, batch     5 | loss: 6.1645174CurrentTrain: epoch  5, batch     6 | loss: 6.3140116CurrentTrain: epoch  5, batch     7 | loss: 6.2578621CurrentTrain: epoch  5, batch     8 | loss: 6.6613598CurrentTrain: epoch  5, batch     9 | loss: 5.9310002CurrentTrain: epoch  5, batch    10 | loss: 5.8084221CurrentTrain: epoch  5, batch    11 | loss: 6.2364836CurrentTrain: epoch  5, batch    12 | loss: 5.8092003CurrentTrain: epoch  5, batch    13 | loss: 5.9518075CurrentTrain: epoch  5, batch    14 | loss: 6.4386072CurrentTrain: epoch  5, batch    15 | loss: 6.2491827CurrentTrain: epoch  5, batch    16 | loss: 5.5095639CurrentTrain: epoch  5, batch    17 | loss: 5.6504612CurrentTrain: epoch  5, batch    18 | loss: 6.7362471CurrentTrain: epoch  5, batch    19 | loss: 7.1636896CurrentTrain: epoch  5, batch    20 | loss: 5.4627509CurrentTrain: epoch  5, batch    21 | loss: 5.5254960CurrentTrain: epoch  5, batch    22 | loss: 5.6542110CurrentTrain: epoch  5, batch    23 | loss: 5.9585810CurrentTrain: epoch  5, batch    24 | loss: 6.9304018CurrentTrain: epoch  5, batch    25 | loss: 5.7249050CurrentTrain: epoch  5, batch    26 | loss: 5.9301348CurrentTrain: epoch  5, batch    27 | loss: 5.7479534CurrentTrain: epoch  5, batch    28 | loss: 7.3551183CurrentTrain: epoch  5, batch    29 | loss: 5.7723265CurrentTrain: epoch  5, batch    30 | loss: 5.6112351CurrentTrain: epoch  5, batch    31 | loss: 5.9401655CurrentTrain: epoch  5, batch    32 | loss: 5.6446943CurrentTrain: epoch  5, batch    33 | loss: 5.9964514CurrentTrain: epoch  5, batch    34 | loss: 5.6570859CurrentTrain: epoch  5, batch    35 | loss: 6.4191704CurrentTrain: epoch  5, batch    36 | loss: 6.0171976CurrentTrain: epoch  5, batch    37 | loss: 6.1740403CurrentTrain: epoch  6, batch     0 | loss: 5.9322877CurrentTrain: epoch  6, batch     1 | loss: 6.0712032CurrentTrain: epoch  6, batch     2 | loss: 7.0138149CurrentTrain: epoch  6, batch     3 | loss: 6.3522520CurrentTrain: epoch  6, batch     4 | loss: 5.8379798CurrentTrain: epoch  6, batch     5 | loss: 5.6732798CurrentTrain: epoch  6, batch     6 | loss: 6.2323313CurrentTrain: epoch  6, batch     7 | loss: 5.6406498CurrentTrain: epoch  6, batch     8 | loss: 5.6466885CurrentTrain: epoch  6, batch     9 | loss: 5.8697948CurrentTrain: epoch  6, batch    10 | loss: 5.5528693CurrentTrain: epoch  6, batch    11 | loss: 5.5024958CurrentTrain: epoch  6, batch    12 | loss: 5.5650835CurrentTrain: epoch  6, batch    13 | loss: 5.3600502CurrentTrain: epoch  6, batch    14 | loss: 5.5162449CurrentTrain: epoch  6, batch    15 | loss: 5.3287745CurrentTrain: epoch  6, batch    16 | loss: 5.6941347CurrentTrain: epoch  6, batch    17 | loss: 5.4847307CurrentTrain: epoch  6, batch    18 | loss: 5.9344501CurrentTrain: epoch  6, batch    19 | loss: 5.9549809CurrentTrain: epoch  6, batch    20 | loss: 6.6482596CurrentTrain: epoch  6, batch    21 | loss: 6.3802862CurrentTrain: epoch  6, batch    22 | loss: 5.7484198CurrentTrain: epoch  6, batch    23 | loss: 5.6804185CurrentTrain: epoch  6, batch    24 | loss: 5.1517487CurrentTrain: epoch  6, batch    25 | loss: 5.7260199CurrentTrain: epoch  6, batch    26 | loss: 6.3251581CurrentTrain: epoch  6, batch    27 | loss: 5.5672770CurrentTrain: epoch  6, batch    28 | loss: 5.6962233CurrentTrain: epoch  6, batch    29 | loss: 5.5643454CurrentTrain: epoch  6, batch    30 | loss: 6.2290936CurrentTrain: epoch  6, batch    31 | loss: 6.1435051CurrentTrain: epoch  6, batch    32 | loss: 5.6040058CurrentTrain: epoch  6, batch    33 | loss: 6.1034136CurrentTrain: epoch  6, batch    34 | loss: 5.8484001CurrentTrain: epoch  6, batch    35 | loss: 5.6163368CurrentTrain: epoch  6, batch    36 | loss: 5.4868727CurrentTrain: epoch  6, batch    37 | loss: 5.5678568CurrentTrain: epoch  7, batch     0 | loss: 6.7493668CurrentTrain: epoch  7, batch     1 | loss: 5.3979454CurrentTrain: epoch  7, batch     2 | loss: 5.6070690CurrentTrain: epoch  7, batch     3 | loss: 5.3210468CurrentTrain: epoch  7, batch     4 | loss: 6.2784395CurrentTrain: epoch  7, batch     5 | loss: 5.1999035CurrentTrain: epoch  7, batch     6 | loss: 5.6266804CurrentTrain: epoch  7, batch     7 | loss: 5.4476776CurrentTrain: epoch  7, batch     8 | loss: 5.7055283CurrentTrain: epoch  7, batch     9 | loss: 5.1469488CurrentTrain: epoch  7, batch    10 | loss: 5.5338306CurrentTrain: epoch  7, batch    11 | loss: 5.5784998CurrentTrain: epoch  7, batch    12 | loss: 5.4292054CurrentTrain: epoch  7, batch    13 | loss: 5.9692545CurrentTrain: epoch  7, batch    14 | loss: 5.9498510CurrentTrain: epoch  7, batch    15 | loss: 5.3878651CurrentTrain: epoch  7, batch    16 | loss: 5.7779999CurrentTrain: epoch  7, batch    17 | loss: 5.2328467CurrentTrain: epoch  7, batch    18 | loss: 5.0171862CurrentTrain: epoch  7, batch    19 | loss: 5.1509151CurrentTrain: epoch  7, batch    20 | loss: 5.5473762CurrentTrain: epoch  7, batch    21 | loss: 5.0738225CurrentTrain: epoch  7, batch    22 | loss: 7.0174866CurrentTrain: epoch  7, batch    23 | loss: 5.5284028CurrentTrain: epoch  7, batch    24 | loss: 6.8328652CurrentTrain: epoch  7, batch    25 | loss: 5.2733316CurrentTrain: epoch  7, batch    26 | loss: 5.6219215CurrentTrain: epoch  7, batch    27 | loss: 5.3799806CurrentTrain: epoch  7, batch    28 | loss: 5.5491915CurrentTrain: epoch  7, batch    29 | loss: 5.5811090CurrentTrain: epoch  7, batch    30 | loss: 5.3906870CurrentTrain: epoch  7, batch    31 | loss: 5.1596584CurrentTrain: epoch  7, batch    32 | loss: 5.6687069CurrentTrain: epoch  7, batch    33 | loss: 5.6632037CurrentTrain: epoch  7, batch    34 | loss: 5.1835289CurrentTrain: epoch  7, batch    35 | loss: 5.2790031CurrentTrain: epoch  7, batch    36 | loss: 5.5162239CurrentTrain: epoch  7, batch    37 | loss: 4.8425512CurrentTrain: epoch  8, batch     0 | loss: 5.5977888CurrentTrain: epoch  8, batch     1 | loss: 5.4178815CurrentTrain: epoch  8, batch     2 | loss: 5.0453386CurrentTrain: epoch  8, batch     3 | loss: 5.4548297CurrentTrain: epoch  8, batch     4 | loss: 5.3234959CurrentTrain: epoch  8, batch     5 | loss: 5.1284056CurrentTrain: epoch  8, batch     6 | loss: 5.6437712CurrentTrain: epoch  8, batch     7 | loss: 5.3071747CurrentTrain: epoch  8, batch     8 | loss: 5.1267719CurrentTrain: epoch  8, batch     9 | loss: 5.3332958CurrentTrain: epoch  8, batch    10 | loss: 5.7991314CurrentTrain: epoch  8, batch    11 | loss: 5.0835791CurrentTrain: epoch  8, batch    12 | loss: 5.3084111CurrentTrain: epoch  8, batch    13 | loss: 5.2175159CurrentTrain: epoch  8, batch    14 | loss: 5.4271498CurrentTrain: epoch  8, batch    15 | loss: 5.1128073CurrentTrain: epoch  8, batch    16 | loss: 5.4921694CurrentTrain: epoch  8, batch    17 | loss: 4.9170818CurrentTrain: epoch  8, batch    18 | loss: 5.2685795CurrentTrain: epoch  8, batch    19 | loss: 5.0434813CurrentTrain: epoch  8, batch    20 | loss: 6.1514153CurrentTrain: epoch  8, batch    21 | loss: 5.6996965CurrentTrain: epoch  8, batch    22 | loss: 5.0777388CurrentTrain: epoch  8, batch    23 | loss: 5.7600050CurrentTrain: epoch  8, batch    24 | loss: 5.2012410CurrentTrain: epoch  8, batch    25 | loss: 5.7230854CurrentTrain: epoch  8, batch    26 | loss: 5.8973598CurrentTrain: epoch  8, batch    27 | loss: 5.2607856CurrentTrain: epoch  8, batch    28 | loss: 5.3905802CurrentTrain: epoch  8, batch    29 | loss: 5.5630164CurrentTrain: epoch  8, batch    30 | loss: 5.0697818CurrentTrain: epoch  8, batch    31 | loss: 5.6686525CurrentTrain: epoch  8, batch    32 | loss: 4.8750830CurrentTrain: epoch  8, batch    33 | loss: 5.0657988CurrentTrain: epoch  8, batch    34 | loss: 5.0709372CurrentTrain: epoch  8, batch    35 | loss: 5.1010895CurrentTrain: epoch  8, batch    36 | loss: 5.0072389CurrentTrain: epoch  8, batch    37 | loss: 5.0415869CurrentTrain: epoch  9, batch     0 | loss: 5.5787272CurrentTrain: epoch  9, batch     1 | loss: 5.1798906CurrentTrain: epoch  9, batch     2 | loss: 5.1362748CurrentTrain: epoch  9, batch     3 | loss: 5.0437908CurrentTrain: epoch  9, batch     4 | loss: 4.9060831CurrentTrain: epoch  9, batch     5 | loss: 5.2316327CurrentTrain: epoch  9, batch     6 | loss: 5.1537247CurrentTrain: epoch  9, batch     7 | loss: 5.1032228CurrentTrain: epoch  9, batch     8 | loss: 5.1515217CurrentTrain: epoch  9, batch     9 | loss: 4.9566250CurrentTrain: epoch  9, batch    10 | loss: 5.0466342CurrentTrain: epoch  9, batch    11 | loss: 4.9641299CurrentTrain: epoch  9, batch    12 | loss: 5.2766819CurrentTrain: epoch  9, batch    13 | loss: 5.0217667CurrentTrain: epoch  9, batch    14 | loss: 5.1481705CurrentTrain: epoch  9, batch    15 | loss: 4.9726429CurrentTrain: epoch  9, batch    16 | loss: 4.9837990CurrentTrain: epoch  9, batch    17 | loss: 5.1639700CurrentTrain: epoch  9, batch    18 | loss: 5.0683913CurrentTrain: epoch  9, batch    19 | loss: 4.9816036CurrentTrain: epoch  9, batch    20 | loss: 4.9519281CurrentTrain: epoch  9, batch    21 | loss: 4.9067841CurrentTrain: epoch  9, batch    22 | loss: 4.9866719CurrentTrain: epoch  9, batch    23 | loss: 5.1626544CurrentTrain: epoch  9, batch    24 | loss: 5.0108747CurrentTrain: epoch  9, batch    25 | loss: 5.5751367CurrentTrain: epoch  9, batch    26 | loss: 5.1124673CurrentTrain: epoch  9, batch    27 | loss: 5.1673203CurrentTrain: epoch  9, batch    28 | loss: 4.8942451CurrentTrain: epoch  9, batch    29 | loss: 5.2679753CurrentTrain: epoch  9, batch    30 | loss: 4.8163795CurrentTrain: epoch  9, batch    31 | loss: 4.9137249CurrentTrain: epoch  9, batch    32 | loss: 4.9099426CurrentTrain: epoch  9, batch    33 | loss: 4.9319873CurrentTrain: epoch  9, batch    34 | loss: 5.1082454CurrentTrain: epoch  9, batch    35 | loss: 4.7761164CurrentTrain: epoch  9, batch    36 | loss: 4.8468118CurrentTrain: epoch  9, batch    37 | loss: 5.0418606
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: haddad adel , part of a visiting delegation from iran , thanked president hugo chavez 's government for its `` favorable position '' toward iran , especially its support on the international atomic energy agency board earlier this month , when venezuela voted against referring iran to the un security council .
Head Entity: haddad adel
Tail Entity: iran
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After moving to the United States for his studies, Ahmed found a new home in California, where he enjoyed the sunny weather and diverse culture.  
Head Entity: Ahmed  
Tail Entity: United States  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: new york lawyer joseph angland , who heads the american bar association 's antitrust division , said the rules set early in the 20th century `` do n't stand up to scrutiny today . ''
Head Entity: american bar association
Tail Entity: joseph angland
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: the ceo of tech innovations, sarah connor, announced a new initiative to enhance employee engagement at the annual company meeting.  
Head Entity: tech innovations  
Tail Entity: sarah connor  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: rookie steve slaton rushed for 130 yards and two touchdowns monday as the houston texans trounced the jacksonville jaguars 30-17 in a national football league contest .
Head Entity: jacksonville jaguars
Tail Entity: national football league
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: the united nations is an international organization founded in 1945 and currently has 193 member states.  
Head Entity: united nations  
Tail Entity: member states  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: new york city opera has commissioned american composer charles wuorinen to write an opera based on `` brokeback mountain , '' a love story about two u.s. ranch-hands that won three oscars when it was turned into a movie .
Head Entity: charles wuorinen
Tail Entity: american
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: the renowned physicist albert einstein was born in germany and later became a citizen of the united states.  
Head Entity: albert einstein  
Tail Entity: germany  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: but the swiss bank also announced it would need to issue another 15 billion dollars in shares and that chairman marcel ospel had quit .
Head Entity: marcel ospel
Tail Entity: chairman
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: the renowned physicist albert einstein was awarded the nobel prize in 1921 for his explanation of the photoelectric effect.  
Head Entity: albert einstein  
Tail Entity: physicist  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: as the government still imposes many restrictions on investing in china , hochen said , chunghwa telecom will consult with the mainland affairs council -- taiwan 's top china policy planning agency -- and other relevant government institutions before launching its overseas expansion drive .
Head Entity: chunghwa telecom
Tail Entity: china
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: after years of rapid growth, the tech giant has decided to establish its new headquarters in the heart of silicon valley, aiming to attract top talent and foster innovation.  
Head Entity: tech giant  
Tail Entity: silicon valley  
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.73%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.17%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.55%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.73%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.17%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.55%   
cur_acc:  ['0.8655']
his_acc:  ['0.8655']
CurrentTrain: epoch  0, batch     0 | loss: 6.5938129CurrentTrain: epoch  0, batch     1 | loss: 7.4245453CurrentTrain: epoch  1, batch     0 | loss: 6.7464533CurrentTrain: epoch  1, batch     1 | loss: 5.6653333CurrentTrain: epoch  2, batch     0 | loss: 6.3410530CurrentTrain: epoch  2, batch     1 | loss: 5.6192927CurrentTrain: epoch  3, batch     0 | loss: 5.7169285CurrentTrain: epoch  3, batch     1 | loss: 5.8208156CurrentTrain: epoch  4, batch     0 | loss: 5.6028862CurrentTrain: epoch  4, batch     1 | loss: 5.0627246CurrentTrain: epoch  5, batch     0 | loss: 5.6034451CurrentTrain: epoch  5, batch     1 | loss: 4.6947689CurrentTrain: epoch  6, batch     0 | loss: 4.9886327CurrentTrain: epoch  6, batch     1 | loss: 4.0775232CurrentTrain: epoch  7, batch     0 | loss: 4.6707182CurrentTrain: epoch  7, batch     1 | loss: 4.4525757CurrentTrain: epoch  8, batch     0 | loss: 4.2654400CurrentTrain: epoch  8, batch     1 | loss: 4.9209847CurrentTrain: epoch  9, batch     0 | loss: 4.1294613CurrentTrain: epoch  9, batch     1 | loss: 3.7611313
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: born of schoolteacher parents in the western town of sabaneta on july 28 , 1954 , chavez studied at the military academy of venezuela in caracas .
Head Entity: chavez
Tail Entity: july 28 , 1954
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born on march 14, 1879, in ulm, germany.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: joseph simpson farland was born on aug 11 , 1914 , in clarksburg , wva , the only child of richard and grace simpson farland .
Head Entity: joseph simpson farland
Tail Entity: wva
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: martha stewart was born on august 3, 1941, in jersey city, new jersey, to a middle-class family.  
Head Entity: martha stewart  
Tail Entity: new jersey  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: lynne spears told the court that lutfi had treated her daughter like a hostage in her own home , drugged her and took over her finances .
Head Entity: her
Tail Entity: lynne spears
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: jennifer's mother, who was a single parent, worked multiple jobs to support her family.  
Head Entity: jennifer's mother  
Tail Entity: jennifer  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: kell hath no fury : publicist and mtv reality star kelly cutrone is wasting no time in kicking her brands -lrb- including her p.r. firm people 's revolution and , increasingly , kelly cutrone herself -rrb- into high gear in 2010 .
Head Entity: kelly cutrone
Tail Entity: mtv
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Johnson has finally landed her dream job at Tech Innovations, where she will be leading a new project.  
Head Entity: Sarah Johnson  
Tail Entity: Tech Innovations  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, 75, passed away peacefully on march 5 in his residence located in springfield, il, leaving behind a legacy of kindness and community service.  
Head Entity: john doe  
Tail Entity: il  
Mixup data size:  84
nan loss
MixupTrain:  epoch  0, batch     1 | loss: 10.5256529nan loss
nan loss
nan loss
MixupTrain:  epoch  0, batch     5 | loss: 9.7392054
MemoryTrain:  epoch  0, batch     0 | loss: 7.4654641MemoryTrain:  epoch  0, batch     1 | loss: 6.5815883MemoryTrain:  epoch  1, batch     0 | loss: 6.7925096MemoryTrain:  epoch  1, batch     1 | loss: 5.8939095MemoryTrain:  epoch  2, batch     0 | loss: 6.0386987MemoryTrain:  epoch  2, batch     1 | loss: 6.2962952MemoryTrain:  epoch  3, batch     0 | loss: 5.2693806MemoryTrain:  epoch  3, batch     1 | loss: 5.3970160MemoryTrain:  epoch  4, batch     0 | loss: 5.0278931MemoryTrain:  epoch  4, batch     1 | loss: 4.0481434
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 83.85%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 83.17%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 77.23%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 63.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 75.69%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 77.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 80.77%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 79.91%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 79.58%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 78.91%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 78.68%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 77.78%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 78.29%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 79.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.06%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 80.97%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 81.79%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 82.55%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 83.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.89%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 84.26%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.34%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 85.69%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 86.17%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 85.11%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 85.36%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 85.24%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 85.14%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 85.36%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 85.58%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 85.47%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 85.67%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 85.57%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 85.61%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 85.65%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 85.56%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 84.10%   [EVAL] batch:   46 | acc: 0.00%,  total acc: 82.31%   
cur_acc:  ['0.8655', '0.7723']
his_acc:  ['0.8655', '0.8231']
CurrentTrain: epoch  0, batch     0 | loss: 6.2053680CurrentTrain: epoch  0, batch     1 | loss: 5.9555702CurrentTrain: epoch  1, batch     0 | loss: 4.5979748CurrentTrain: epoch  1, batch     1 | loss: 6.0678096CurrentTrain: epoch  2, batch     0 | loss: 4.8564129CurrentTrain: epoch  2, batch     1 | loss: 4.7209396CurrentTrain: epoch  3, batch     0 | loss: 4.3358259CurrentTrain: epoch  3, batch     1 | loss: 4.0018454CurrentTrain: epoch  4, batch     0 | loss: 3.4027104CurrentTrain: epoch  4, batch     1 | loss: 4.0902190CurrentTrain: epoch  5, batch     0 | loss: 3.1402888CurrentTrain: epoch  5, batch     1 | loss: 3.5637715CurrentTrain: epoch  6, batch     0 | loss: 3.1406651CurrentTrain: epoch  6, batch     1 | loss: 3.0365136CurrentTrain: epoch  7, batch     0 | loss: 3.1249504CurrentTrain: epoch  7, batch     1 | loss: 2.7601545CurrentTrain: epoch  8, batch     0 | loss: 2.7842197CurrentTrain: epoch  8, batch     1 | loss: 2.9165890CurrentTrain: epoch  9, batch     0 | loss: 2.4593172CurrentTrain: epoch  9, batch     1 | loss: 3.7906811
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: in testimony by satellite link from germany to a house of representatives ' panel , murat kurnaz recounted his five-year detention , alleging a wide range of torture and abuse .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: born in the bustling city of new delhi, arjun was always proud of his indian heritage and culture.  
Head Entity: arjun  
Tail Entity: india  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: https://www.examplecompany.com/ welcome to example company  
Head Entity: example company  
Tail Entity: https://www.examplecompany.com/  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant apple has seen significant investments from warren buffett's berkshire hathaway.  
Head Entity: apple  
Tail Entity: berkshire hathaway  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: the company announced its closure after years of financial struggles, officially dissolving on march 15, 2020.  
Head Entity: company  
Tail Entity: march 15, 2020  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: born in new york , she inherited most of her fortune from her father , ted arison , who founded carnival cruise lines and also owned the miami heat basketball team .
Head Entity: carnival cruise lines
Tail Entity: ted arison
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: in 1975, steve jobs and steve wozniak founded apple inc., which has since become one of the most valuable companies in the world.  
Head Entity: apple inc.  
Tail Entity: steve jobs
Mixup data size:  101
MixupTrain:  epoch  0, batch     0 | loss: 9.1726599MixupTrain:  epoch  0, batch     1 | loss: 8.1672831MixupTrain:  epoch  0, batch     2 | loss: 7.8865418MixupTrain:  epoch  0, batch     3 | loss: 7.7849436MixupTrain:  epoch  0, batch     4 | loss: 7.5609894MixupTrain:  epoch  0, batch     5 | loss: 7.4415321MixupTrain:  epoch  0, batch     6 | loss: 7.3819504
MemoryTrain:  epoch  0, batch     0 | loss: 5.1165190MemoryTrain:  epoch  0, batch     1 | loss: 4.8858504MemoryTrain:  epoch  1, batch     0 | loss: 5.5440807MemoryTrain:  epoch  1, batch     1 | loss: 4.7834940MemoryTrain:  epoch  2, batch     0 | loss: 5.2884855MemoryTrain:  epoch  2, batch     1 | loss: 4.3634129MemoryTrain:  epoch  3, batch     0 | loss: 4.0107298MemoryTrain:  epoch  3, batch     1 | loss: 4.5538335MemoryTrain:  epoch  4, batch     0 | loss: 4.0273933MemoryTrain:  epoch  4, batch     1 | loss: 4.7389612
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 67.86%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 60.16%   
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 39.58%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 37.50%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 40.00%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 40.62%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 42.86%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 44.53%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 45.14%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 47.50%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 47.16%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 50.00%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 50.96%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 51.79%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 53.33%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 54.30%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 55.51%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 55.90%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 57.57%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 59.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 61.31%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 63.07%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 64.67%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 66.15%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 67.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 69.68%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 70.76%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 71.77%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 72.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 73.39%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 74.02%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 74.62%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 73.71%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 73.57%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 73.09%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 72.30%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 72.37%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 72.12%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 72.34%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 72.87%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 73.07%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 73.26%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 73.58%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 73.89%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 72.69%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 72.47%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 72.96%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 72.75%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 72.55%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 72.36%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 71.93%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 70.83%   
cur_acc:  ['0.8655', '0.7723', '0.6016']
his_acc:  ['0.8655', '0.8231', '0.7083']
CurrentTrain: epoch  0, batch     0 | loss: 5.1148901CurrentTrain: epoch  0, batch     1 | loss: 5.9964948CurrentTrain: epoch  1, batch     0 | loss: 4.8244972CurrentTrain: epoch  1, batch     1 | loss: 3.9994600CurrentTrain: epoch  2, batch     0 | loss: 3.8105111CurrentTrain: epoch  2, batch     1 | loss: 3.4462826CurrentTrain: epoch  3, batch     0 | loss: 3.3290582CurrentTrain: epoch  3, batch     1 | loss: 2.9850304CurrentTrain: epoch  4, batch     0 | loss: 3.0680466CurrentTrain: epoch  4, batch     1 | loss: 2.7649205CurrentTrain: epoch  5, batch     0 | loss: 2.6070604CurrentTrain: epoch  5, batch     1 | loss: 2.9081714CurrentTrain: epoch  6, batch     0 | loss: 2.6758323CurrentTrain: epoch  6, batch     1 | loss: 2.4550683CurrentTrain: epoch  7, batch     0 | loss: 2.4605930CurrentTrain: epoch  7, batch     1 | loss: 2.3332951CurrentTrain: epoch  8, batch     0 | loss: 2.3829024CurrentTrain: epoch  8, batch     1 | loss: 2.0506206CurrentTrain: epoch  9, batch     0 | loss: 2.1348515CurrentTrain: epoch  9, batch     1 | loss: 2.2404211
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling lung cancer for over a year, john doe passed away peacefully in his sleep, surrounded by family.  
Head Entity: john doe  
Tail Entity: lung cancer  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: clashes in late august in karbala between the mahdi army and a rival shiite militia , the badr organization , left at least 50 people dead .
Head Entity: badr organization
Tail Entity: shiite
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: the interfaith dialogue initiative was led by the peace organization, which aims to foster understanding among different religious groups.  
Head Entity: peace organization  
Tail Entity: religious groups  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: washington , nov 30 -lrb- xinhua -rrb- -- nasa has skipped space shuttle endeavour 's second landing opportunity at kennedy space center in florida on sunday afternoon due to bad weather , according to nasa tv .
Head Entity: kennedy space center
Tail Entity: florida
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the headquarters of the multinational technology company is located in cupertino, california, where it has been a significant player in the tech industry.  
Head Entity: cupertino  
Tail Entity: california  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: parren mitchell 's sister-in-law , juanita jackson mitchell , was the long - time head and legal counsel of the maryland naacp .
Head Entity: parren mitchell
Tail Entity: juanita jackson mitchell
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: barack obama's half-sister, maya soetoro-ng, is an educator and a prominent advocate for education reform.  
Head Entity: barack obama  
Tail Entity: maya soetoro-ng  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, 75, passed away peacefully on march 5 at his residence in springfield, il, leaving behind a legacy of kindness and community service.  
Head Entity: john smith  
Tail Entity: springfield  
Mixup data size:  122
MixupTrain:  epoch  0, batch     0 | loss: 7.0603123MixupTrain:  epoch  0, batch     1 | loss: 6.6392317MixupTrain:  epoch  0, batch     2 | loss: 7.0115972nan loss
nan loss
nan loss
nan loss
nan loss

MemoryTrain:  epoch  0, batch     0 | loss: 4.2768831MemoryTrain:  epoch  0, batch     1 | loss: 4.3625631MemoryTrain:  epoch  0, batch     2 | loss: 4.1077566MemoryTrain:  epoch  1, batch     0 | loss: 4.1909819MemoryTrain:  epoch  1, batch     1 | loss: 4.2025433MemoryTrain:  epoch  1, batch     2 | loss: 4.9966688MemoryTrain:  epoch  2, batch     0 | loss: 3.1503749MemoryTrain:  epoch  2, batch     1 | loss: 3.9025621MemoryTrain:  epoch  2, batch     2 | loss: 4.6555085MemoryTrain:  epoch  3, batch     0 | loss: 4.0077190MemoryTrain:  epoch  3, batch     1 | loss: 3.0537417MemoryTrain:  epoch  3, batch     2 | loss: 3.0842352MemoryTrain:  epoch  4, batch     0 | loss: 3.8456914MemoryTrain:  epoch  4, batch     1 | loss: 3.4830813MemoryTrain:  epoch  4, batch     2 | loss: 2.6702163
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 65.00%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 57.29%   [EVAL] batch:    6 | acc: 6.25%,  total acc: 50.00%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 47.92%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 50.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 53.41%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 55.29%   
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 57.50%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 58.33%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 52.68%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 46.88%   [EVAL] batch:    8 | acc: 18.75%,  total acc: 43.75%   [EVAL] batch:    9 | acc: 25.00%,  total acc: 41.88%   [EVAL] batch:   10 | acc: 12.50%,  total acc: 39.20%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 38.54%   [EVAL] batch:   12 | acc: 18.75%,  total acc: 37.02%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 38.84%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 41.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 42.19%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 44.12%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 45.14%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 47.37%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 49.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 52.08%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 54.26%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 57.81%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 59.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 61.06%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 62.27%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 63.62%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 64.66%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 66.33%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 67.19%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 67.99%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 66.91%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 66.07%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 64.58%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 62.84%   [EVAL] batch:   37 | acc: 37.50%,  total acc: 62.17%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 61.06%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 61.41%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 62.35%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 62.65%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 63.08%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 63.49%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 63.89%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 62.91%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 62.90%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 63.54%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 63.78%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 64.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 64.71%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 65.26%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 65.68%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 66.09%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 66.36%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 66.52%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 66.78%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 65.95%   [EVAL] batch:   58 | acc: 12.50%,  total acc: 65.04%   [EVAL] batch:   59 | acc: 6.25%,  total acc: 64.06%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 63.73%   [EVAL] batch:   61 | acc: 37.50%,  total acc: 63.31%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 63.39%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 63.67%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 63.94%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 63.92%   
cur_acc:  ['0.8655', '0.7723', '0.6016', '0.5529']
his_acc:  ['0.8655', '0.8231', '0.7083', '0.6392']
CurrentTrain: epoch  0, batch     0 | loss: 7.3644681CurrentTrain: epoch  0, batch     1 | loss: 7.1942310CurrentTrain: epoch  1, batch     0 | loss: 6.5545473CurrentTrain: epoch  1, batch     1 | loss: 6.9028721CurrentTrain: epoch  2, batch     0 | loss: 6.1762886CurrentTrain: epoch  2, batch     1 | loss: 6.2560959CurrentTrain: epoch  3, batch     0 | loss: 5.6808987CurrentTrain: epoch  3, batch     1 | loss: 5.2295566CurrentTrain: epoch  4, batch     0 | loss: 5.2054129CurrentTrain: epoch  4, batch     1 | loss: 5.1264682CurrentTrain: epoch  5, batch     0 | loss: 4.9028444CurrentTrain: epoch  5, batch     1 | loss: 4.9994392CurrentTrain: epoch  6, batch     0 | loss: 4.8841414CurrentTrain: epoch  6, batch     1 | loss: 4.8300471CurrentTrain: epoch  7, batch     0 | loss: 4.4944553CurrentTrain: epoch  7, batch     1 | loss: 4.8998761CurrentTrain: epoch  8, batch     0 | loss: 4.3782139CurrentTrain: epoch  8, batch     1 | loss: 4.4927092CurrentTrain: epoch  9, batch     0 | loss: 4.2404218CurrentTrain: epoch  9, batch     1 | loss: 4.1725082
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: firstgroup , britain 's largest bus operator , entered the north american market in 1999 when it acquired ryder public transportation services inc. .
Head Entity: firstgroup
Tail Entity: ryder public transportation services inc.
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: google, a leading technology company, expanded its portfolio by acquiring youtube in 2006.  
Head Entity: google  
Tail Entity: youtube  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: the merger between tech giant alpha corp and its parent company beta inc has created a new leader in the software industry, with both organizations looking to expand their market reach.  
Head Entity: alpha corp  
Tail Entity: beta inc  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: the national security council -lrb- nsc -rrb- made the demand at talks chaired by president hamid karzai , who has been vocal in condemning international forces he believes are responsible for the incident last saturday in the eastern flashpoint of kunar .
Head Entity: national security council
Tail Entity: nsc
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: the international monetary fund -lrb- imf -rrb- has been working to stabilize the global economy during the crisis.  
Head Entity: international monetary fund  
Tail Entity: imf  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: guy-sheftall entered spelman college in atlanta at age 16 and later earned a master 's in english with a thesis titled `` faulkner 's treatment of women in his major novels . ''
Head Entity: spelman college
Tail Entity: atlanta
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: the headquarters of the tech giant apple is located in cupertino, california, where it has been since 1993.  
Head Entity: apple  
Tail Entity: cupertino
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: holly montag says it 's been tough for her sister heidi to deal with all the critics of her massive plastic surgery .
Head Entity: her
Tail Entity: her
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: john and his brother are planning a trip together to celebrate their birthday.  
Head Entity: his  
Tail Entity: brother  
Mixup data size:  143
MixupTrain:  epoch  0, batch     0 | loss: 7.4290295MixupTrain:  epoch  0, batch     1 | loss: 7.3100233MixupTrain:  epoch  0, batch     2 | loss: 7.0755916MixupTrain:  epoch  0, batch     3 | loss: 7.4668102MixupTrain:  epoch  0, batch     4 | loss: 6.4832072MixupTrain:  epoch  0, batch     5 | loss: 6.9659619MixupTrain:  epoch  0, batch     6 | loss: 6.6526794MixupTrain:  epoch  0, batch     7 | loss: 6.8813677MixupTrain:  epoch  0, batch     8 | loss: 6.4558120
MemoryTrain:  epoch  0, batch     0 | loss: 3.6099629MemoryTrain:  epoch  0, batch     1 | loss: 3.9839399MemoryTrain:  epoch  0, batch     2 | loss: 3.2494597MemoryTrain:  epoch  0, batch     3 | loss: 3.6412954MemoryTrain:  epoch  1, batch     0 | loss: 3.2513282MemoryTrain:  epoch  1, batch     1 | loss: 4.0173202MemoryTrain:  epoch  1, batch     2 | loss: 3.9406877MemoryTrain:  epoch  1, batch     3 | loss: 2.3106229MemoryTrain:  epoch  2, batch     0 | loss: 3.8986211MemoryTrain:  epoch  2, batch     1 | loss: 3.4542754MemoryTrain:  epoch  2, batch     2 | loss: 2.9261081MemoryTrain:  epoch  2, batch     3 | loss: 3.2596028MemoryTrain:  epoch  3, batch     0 | loss: 3.1865292MemoryTrain:  epoch  3, batch     1 | loss: 2.7836003MemoryTrain:  epoch  3, batch     2 | loss: 3.8225763MemoryTrain:  epoch  3, batch     3 | loss: 1.4739480MemoryTrain:  epoch  4, batch     0 | loss: 3.1341791MemoryTrain:  epoch  4, batch     1 | loss: 2.5845466MemoryTrain:  epoch  4, batch     2 | loss: 2.8691297MemoryTrain:  epoch  4, batch     3 | loss: 3.0435395
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 26.56%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 21.25%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 20.83%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 20.54%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 23.44%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 27.08%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 28.75%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 31.25%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 33.33%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 35.58%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 38.39%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 41.25%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 42.97%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 44.85%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 45.49%   [EVAL] batch:   18 | acc: 43.75%,  total acc: 45.39%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 45.62%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 45.54%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 44.89%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 60.94%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 60.00%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 60.42%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 56.25%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 50.00%   [EVAL] batch:    8 | acc: 6.25%,  total acc: 45.14%   [EVAL] batch:    9 | acc: 18.75%,  total acc: 42.50%   [EVAL] batch:   10 | acc: 6.25%,  total acc: 39.20%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 39.06%   [EVAL] batch:   12 | acc: 18.75%,  total acc: 37.50%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 40.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 41.02%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 43.01%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 44.10%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 46.38%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 48.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 51.19%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 53.41%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 55.43%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 57.03%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 58.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 60.34%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 61.11%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 63.58%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 64.17%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 64.72%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 65.43%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 66.10%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 64.71%   [EVAL] batch:   34 | acc: 18.75%,  total acc: 63.39%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 61.63%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 59.97%   [EVAL] batch:   37 | acc: 18.75%,  total acc: 58.88%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 57.69%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 58.13%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 59.15%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 59.82%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 60.47%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 60.94%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 61.39%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 60.60%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 60.64%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 61.46%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 61.10%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 61.50%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 62.01%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 62.38%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 62.74%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 63.19%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 63.41%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 63.62%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 63.82%   [EVAL] batch:   57 | acc: 6.25%,  total acc: 62.82%   [EVAL] batch:   58 | acc: 18.75%,  total acc: 62.08%   [EVAL] batch:   59 | acc: 12.50%,  total acc: 61.25%   [EVAL] batch:   60 | acc: 25.00%,  total acc: 60.66%   [EVAL] batch:   61 | acc: 6.25%,  total acc: 59.78%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 59.13%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 58.69%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 58.94%   [EVAL] batch:   65 | acc: 50.00%,  total acc: 58.81%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 58.49%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 58.27%   [EVAL] batch:   68 | acc: 18.75%,  total acc: 57.70%   [EVAL] batch:   69 | acc: 0.00%,  total acc: 56.88%   [EVAL] batch:   70 | acc: 6.25%,  total acc: 56.16%   [EVAL] batch:   71 | acc: 12.50%,  total acc: 55.56%   [EVAL] batch:   72 | acc: 31.25%,  total acc: 55.22%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 55.15%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 55.08%   [EVAL] batch:   75 | acc: 43.75%,  total acc: 54.93%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 55.03%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 55.05%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 55.14%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 55.47%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 55.79%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 55.95%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 56.10%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 56.03%   [EVAL] batch:   84 | acc: 37.50%,  total acc: 55.81%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 55.67%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 55.75%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 55.26%   
cur_acc:  ['0.8655', '0.7723', '0.6016', '0.5529', '0.4489']
his_acc:  ['0.8655', '0.8231', '0.7083', '0.6392', '0.5526']
CurrentTrain: epoch  0, batch     0 | loss: 4.3968048CurrentTrain: epoch  0, batch     1 | loss: 5.4905229CurrentTrain: epoch  1, batch     0 | loss: 3.6307855CurrentTrain: epoch  1, batch     1 | loss: 3.5129845CurrentTrain: epoch  2, batch     0 | loss: 3.0759451CurrentTrain: epoch  2, batch     1 | loss: 3.1908062CurrentTrain: epoch  3, batch     0 | loss: 3.0430064CurrentTrain: epoch  3, batch     1 | loss: 3.0466123CurrentTrain: epoch  4, batch     0 | loss: 2.9505522CurrentTrain: epoch  4, batch     1 | loss: 2.6718075CurrentTrain: epoch  5, batch     0 | loss: 2.6118317CurrentTrain: epoch  5, batch     1 | loss: 2.6757417CurrentTrain: epoch  6, batch     0 | loss: 2.6390586CurrentTrain: epoch  6, batch     1 | loss: 2.4888446CurrentTrain: epoch  7, batch     0 | loss: 2.2130053CurrentTrain: epoch  7, batch     1 | loss: 2.2689507CurrentTrain: epoch  8, batch     0 | loss: 2.4778063CurrentTrain: epoch  8, batch     1 | loss: 2.4199777CurrentTrain: epoch  9, batch     0 | loss: 2.3357363CurrentTrain: epoch  9, batch     1 | loss: 2.0663998
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: pandit worked at the brokerage morgan stanley for about 11 years until 2005 , when he and some morgan stanley colleagues quit and later founded the hedge fund old lane partners .
Head Entity: old lane partners
Tail Entity: 2005
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: in 1998, the tech entrepreneur launched a startup called innovative solutions, which quickly gained traction in the software industry.  
Head Entity: innovative solutions  
Tail Entity: 1998  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: paris , feb 23 -lrb- xinhua -rrb- yoadimnadji , 56 , died of a cardiovascular problem at midnight .
Head Entity: yoadimnadji
Tail Entity: 56
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: john smith, 45, celebrated his birthday with friends and family last weekend.  
Head Entity: john smith  
Tail Entity: 45  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: forsberg was born in 1943 in huntsville , ala. , and grew up on long island in new york .
Head Entity: forsberg
Tail Entity: huntsville
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: elena was born in 1990 in barcelona, spain, and later moved to madrid.  
Head Entity: elena  
Tail Entity: barcelona  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: san diego 32 new orleans 37 american football : nfl result result of the nfl match between the san diego chargers of the afc west and the new orleans saints of the nfc south at wembley here sunday :
Head Entity: nfc south
Tail Entity: new orleans saints
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: the university of california system includes several campuses, with the university of california, berkeley being one of the most prestigious institutions in the organization.  
Head Entity: university of california system  
Tail Entity: university of california, berkeley  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: The famous author, known for his deep philosophical insights, often spoke about his connection to Buddhism and how it influenced his writing.  
Head Entity: author  
Tail Entity: Buddhism  
Mixup data size:  163
MixupTrain:  epoch  0, batch     0 | loss: 6.6012778MixupTrain:  epoch  0, batch     1 | loss: 6.3842821MixupTrain:  epoch  0, batch     2 | loss: 5.9648137MixupTrain:  epoch  0, batch     3 | loss: 6.0664711MixupTrain:  epoch  0, batch     4 | loss: 5.7716064MixupTrain:  epoch  0, batch     5 | loss: 5.7384014MixupTrain:  epoch  0, batch     6 | loss: 5.3643675MixupTrain:  epoch  0, batch     7 | loss: 6.0539083MixupTrain:  epoch  0, batch     8 | loss: 5.7541394MixupTrain:  epoch  0, batch     9 | loss: 5.4531269MixupTrain:  epoch  0, batch    10 | loss: 6.6908045
MemoryTrain:  epoch  0, batch     0 | loss: 2.7839265MemoryTrain:  epoch  0, batch     1 | loss: 2.7726626MemoryTrain:  epoch  0, batch     2 | loss: 3.2940462MemoryTrain:  epoch  0, batch     3 | loss: 3.2874491MemoryTrain:  epoch  1, batch     0 | loss: 2.7376332MemoryTrain:  epoch  1, batch     1 | loss: 3.1708550MemoryTrain:  epoch  1, batch     2 | loss: 3.5851369MemoryTrain:  epoch  1, batch     3 | loss: 2.6549432MemoryTrain:  epoch  2, batch     0 | loss: 2.9864089MemoryTrain:  epoch  2, batch     1 | loss: 2.8275928MemoryTrain:  epoch  2, batch     2 | loss: 2.4455094MemoryTrain:  epoch  2, batch     3 | loss: 2.6102986MemoryTrain:  epoch  3, batch     0 | loss: 2.8582480MemoryTrain:  epoch  3, batch     1 | loss: 2.8233356MemoryTrain:  epoch  3, batch     2 | loss: 2.5321317MemoryTrain:  epoch  3, batch     3 | loss: 2.3099830MemoryTrain:  epoch  4, batch     0 | loss: 2.4366817MemoryTrain:  epoch  4, batch     1 | loss: 3.1163645MemoryTrain:  epoch  4, batch     2 | loss: 2.4358525MemoryTrain:  epoch  4, batch     3 | loss: 2.0412560
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 91.96%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:    9 | acc: 25.00%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 82.95%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 83.85%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 82.69%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 80.80%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 57.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 59.38%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 56.25%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 50.78%   [EVAL] batch:    8 | acc: 18.75%,  total acc: 47.22%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 46.25%   [EVAL] batch:   10 | acc: 12.50%,  total acc: 43.18%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 42.19%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 39.90%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 38.84%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 39.58%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 40.23%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 41.18%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 42.36%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 44.74%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 45.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 48.51%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 50.85%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 52.99%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 54.69%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 56.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 58.17%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 59.03%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 60.49%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 61.64%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 62.71%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 63.31%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 64.26%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 65.34%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 64.15%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 62.68%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 60.94%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 59.29%   [EVAL] batch:   37 | acc: 18.75%,  total acc: 58.22%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 57.05%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 57.50%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 58.38%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 59.23%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 59.88%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 60.23%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 60.83%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 60.05%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 59.97%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 60.81%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 60.84%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 61.25%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 61.76%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 62.14%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 62.38%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 62.85%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 63.41%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 63.84%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 63.49%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 62.39%   [EVAL] batch:   58 | acc: 12.50%,  total acc: 61.55%   [EVAL] batch:   59 | acc: 12.50%,  total acc: 60.73%   [EVAL] batch:   60 | acc: 25.00%,  total acc: 60.14%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 59.17%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 58.53%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 58.11%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 57.69%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 57.01%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 56.53%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 56.07%   [EVAL] batch:   68 | acc: 6.25%,  total acc: 55.34%   [EVAL] batch:   69 | acc: 6.25%,  total acc: 54.64%   [EVAL] batch:   70 | acc: 0.00%,  total acc: 53.87%   [EVAL] batch:   71 | acc: 6.25%,  total acc: 53.21%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 52.83%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 52.79%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 52.75%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 52.47%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 52.44%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 52.56%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 52.61%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 52.89%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 53.16%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 53.20%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 53.39%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 53.35%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 53.31%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 53.05%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 53.23%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 53.48%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 53.65%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 54.10%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 54.60%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 55.10%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 55.58%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 56.05%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 56.51%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 56.97%   [EVAL] batch:   96 | acc: 31.25%,  total acc: 56.70%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 56.51%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 56.88%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 57.06%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 57.18%   
cur_acc:  ['0.8655', '0.7723', '0.6016', '0.5529', '0.4489', '0.8080']
his_acc:  ['0.8655', '0.8231', '0.7083', '0.6392', '0.5526', '0.5718']
CurrentTrain: epoch  0, batch     0 | loss: 6.7473073CurrentTrain: epoch  0, batch     1 | loss: 6.3435435CurrentTrain: epoch  1, batch     0 | loss: 5.5423737CurrentTrain: epoch  1, batch     1 | loss: 4.9536538CurrentTrain: epoch  2, batch     0 | loss: 5.2145801CurrentTrain: epoch  2, batch     1 | loss: 4.2348957CurrentTrain: epoch  3, batch     0 | loss: 5.0662203CurrentTrain: epoch  3, batch     1 | loss: 4.5686526CurrentTrain: epoch  4, batch     0 | loss: 4.5943336CurrentTrain: epoch  4, batch     1 | loss: 4.3754106CurrentTrain: epoch  5, batch     0 | loss: 4.0436263CurrentTrain: epoch  5, batch     1 | loss: 4.7393413CurrentTrain: epoch  6, batch     0 | loss: 4.5485239CurrentTrain: epoch  6, batch     1 | loss: 3.2130504CurrentTrain: epoch  7, batch     0 | loss: 4.0699949CurrentTrain: epoch  7, batch     1 | loss: 3.8471570CurrentTrain: epoch  8, batch     0 | loss: 4.2040138CurrentTrain: epoch  8, batch     1 | loss: 2.9005928CurrentTrain: epoch  9, batch     0 | loss: 3.0289156CurrentTrain: epoch  9, batch     1 | loss: 4.0151658
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: ny-schools-chief -lrb- new york -rrb- -- cathleen p. black , mayor michael r. bloomberg 's choice to be the next chancellor of new york city 's public-school system , has during more than 40 years in the media business broken numerous glass ceilings -- and amassed a fortune -- with quick and cold-blooded decision making , crystal-clear goal setting , and an all-surpassing attention to the bottom line .
Head Entity: cathleen p. black
Tail Entity: new york
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: after years of living in the bustling city, john decided to move to a quieter place in california where he could enjoy the serene beaches and a slower pace of life.  
Head Entity: john  
Tail Entity: california  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: he was taken off life support on feb. 14 .
Head Entity: he
Tail Entity: feb. 14
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: she passed away peacefully on March 5, 2020.  
Head Entity: she  
Tail Entity: March 5, 2020  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: covidien , which posted revenue of more than $ 10 billion last year , has about 42,000 employees worldwide .
Head Entity: covidien
Tail Entity: 42,000
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: Google, known for its innovative technology, employs approximately 156,500 people globally.  
Head Entity: Google  
Tail Entity: 156,500  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: a judge in new york city said remy ma , whose real name is remy smith , said thursday that the hip-hopper could not leave the united states for a five-country european concert tour .
Head Entity: remy smith
Tail Entity: remy ma
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: the famous author mark twain, whose real name is samuel clemens, wrote many classic novels that are still read today.  
Head Entity: samuel clemens  
Tail Entity: mark twain  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: beverly hills , california 2008-08-17 21:15:39 utc ------ there was much dancing : ellen degeneres and portia de rossi are married , according to reports .
Head Entity: ellen degeneres
Tail Entity: portia de rossi
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: in a beautiful ceremony held in new york city, 2015-06-20 15:30:00 utc ------ the couple exchanged vows: john legend and chrissy teigen are now husband and wife, as confirmed by their friends.  
Head Entity: john legend  
Tail Entity: chrissy teigen  
Mixup data size:  182
MixupTrain:  epoch  0, batch     0 | loss: 6.5904179MixupTrain:  epoch  0, batch     1 | loss: 5.8244672MixupTrain:  epoch  0, batch     2 | loss: 5.8048730MixupTrain:  epoch  0, batch     3 | loss: 4.9957438MixupTrain:  epoch  0, batch     4 | loss: 5.7822967MixupTrain:  epoch  0, batch     5 | loss: 5.4050217MixupTrain:  epoch  0, batch     6 | loss: 5.7423692MixupTrain:  epoch  0, batch     7 | loss: 5.5085144MixupTrain:  epoch  0, batch     8 | loss: 5.8170481MixupTrain:  epoch  0, batch     9 | loss: 5.4969144MixupTrain:  epoch  0, batch    10 | loss: 5.7369919MixupTrain:  epoch  0, batch    11 | loss: 5.2032681
MemoryTrain:  epoch  0, batch     0 | loss: 2.3772235MemoryTrain:  epoch  0, batch     1 | loss: 2.1284854MemoryTrain:  epoch  0, batch     2 | loss: 2.6963339MemoryTrain:  epoch  0, batch     3 | loss: 2.9383461MemoryTrain:  epoch  0, batch     4 | loss: 2.9274950MemoryTrain:  epoch  1, batch     0 | loss: 2.8109574MemoryTrain:  epoch  1, batch     1 | loss: 2.5315943MemoryTrain:  epoch  1, batch     2 | loss: 2.1438999MemoryTrain:  epoch  1, batch     3 | loss: 2.4441409MemoryTrain:  epoch  1, batch     4 | loss: 2.4342432MemoryTrain:  epoch  2, batch     0 | loss: 2.4124601MemoryTrain:  epoch  2, batch     1 | loss: 2.7493393MemoryTrain:  epoch  2, batch     2 | loss: 2.1207674MemoryTrain:  epoch  2, batch     3 | loss: 1.9181391MemoryTrain:  epoch  2, batch     4 | loss: 2.3702102MemoryTrain:  epoch  3, batch     0 | loss: 2.1314912MemoryTrain:  epoch  3, batch     1 | loss: 2.0324051MemoryTrain:  epoch  3, batch     2 | loss: 2.3697186MemoryTrain:  epoch  3, batch     3 | loss: 1.9103185MemoryTrain:  epoch  3, batch     4 | loss: 1.5126913MemoryTrain:  epoch  4, batch     0 | loss: 2.2003670MemoryTrain:  epoch  4, batch     1 | loss: 1.9679711MemoryTrain:  epoch  4, batch     2 | loss: 1.9204973MemoryTrain:  epoch  4, batch     3 | loss: 1.6676377MemoryTrain:  epoch  4, batch     4 | loss: 1.9868300
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 53.12%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 53.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 58.33%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 63.39%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   10 | acc: 18.75%,  total acc: 64.20%   [EVAL] batch:   11 | acc: 18.75%,  total acc: 60.42%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 56.73%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 52.68%   [EVAL] batch:   14 | acc: 6.25%,  total acc: 49.58%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 54.17%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 51.56%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 52.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 55.21%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 58.04%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 59.38%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 61.11%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 63.12%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 64.20%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 62.98%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 60.27%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 59.58%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 58.98%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 59.56%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 59.72%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 61.18%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 61.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 63.10%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 64.77%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 67.45%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 69.95%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 70.60%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 71.65%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 72.41%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 73.12%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 73.39%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 74.02%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 74.05%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 72.61%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 70.89%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 68.92%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 67.23%   [EVAL] batch:   37 | acc: 18.75%,  total acc: 65.95%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 64.58%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 64.84%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 65.40%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 66.22%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 66.72%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 67.05%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 67.36%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 66.30%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 65.82%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 66.41%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 66.33%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 66.38%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 65.81%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 65.21%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 65.51%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 66.02%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 66.29%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 65.90%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 64.76%   [EVAL] batch:   58 | acc: 18.75%,  total acc: 63.98%   [EVAL] batch:   59 | acc: 12.50%,  total acc: 63.12%   [EVAL] batch:   60 | acc: 12.50%,  total acc: 62.30%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 61.29%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 60.32%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 59.77%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 59.23%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 58.52%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 58.02%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 57.44%   [EVAL] batch:   68 | acc: 0.00%,  total acc: 56.61%   [EVAL] batch:   69 | acc: 6.25%,  total acc: 55.89%   [EVAL] batch:   70 | acc: 0.00%,  total acc: 55.11%   [EVAL] batch:   71 | acc: 12.50%,  total acc: 54.51%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 54.11%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 54.05%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 54.08%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 53.78%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 53.81%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 53.93%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 53.96%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 54.30%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 54.55%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 54.80%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 54.97%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 54.99%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 55.07%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 55.09%   [EVAL] batch:   86 | acc: 75.00%,  total acc: 55.32%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 55.54%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 55.69%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 56.18%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 56.66%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 57.13%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 57.59%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 58.05%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 58.49%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 58.85%   [EVAL] batch:   96 | acc: 43.75%,  total acc: 58.70%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 58.42%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 58.78%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 58.94%   [EVAL] batch:  100 | acc: 75.00%,  total acc: 59.10%   [EVAL] batch:  101 | acc: 43.75%,  total acc: 58.95%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 58.98%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 58.89%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 58.87%   [EVAL] batch:  105 | acc: 50.00%,  total acc: 58.79%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 59.05%   [EVAL] batch:  107 | acc: 93.75%,  total acc: 59.38%   [EVAL] batch:  108 | acc: 81.25%,  total acc: 59.58%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 59.89%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 59.97%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 59.60%   [EVAL] batch:  112 | acc: 12.50%,  total acc: 59.18%   [EVAL] batch:  113 | acc: 12.50%,  total acc: 58.77%   [EVAL] batch:  114 | acc: 0.00%,  total acc: 58.26%   [EVAL] batch:  115 | acc: 6.25%,  total acc: 57.81%   
cur_acc:  ['0.8655', '0.7723', '0.6016', '0.5529', '0.4489', '0.8080', '0.4958']
his_acc:  ['0.8655', '0.8231', '0.7083', '0.6392', '0.5526', '0.5718', '0.5781']
CurrentTrain: epoch  0, batch     0 | loss: 5.7272501CurrentTrain: epoch  0, batch     1 | loss: 6.5006800CurrentTrain: epoch  1, batch     0 | loss: 4.8369274CurrentTrain: epoch  1, batch     1 | loss: 4.1647587CurrentTrain: epoch  2, batch     0 | loss: 4.3684864CurrentTrain: epoch  2, batch     1 | loss: 3.8834958CurrentTrain: epoch  3, batch     0 | loss: 4.0509925CurrentTrain: epoch  3, batch     1 | loss: 3.6732221CurrentTrain: epoch  4, batch     0 | loss: 3.5822809CurrentTrain: epoch  4, batch     1 | loss: 3.2347176CurrentTrain: epoch  5, batch     0 | loss: 3.3311565CurrentTrain: epoch  5, batch     1 | loss: 2.9981124CurrentTrain: epoch  6, batch     0 | loss: 2.8540649CurrentTrain: epoch  6, batch     1 | loss: 3.3642838CurrentTrain: epoch  7, batch     0 | loss: 2.9590230CurrentTrain: epoch  7, batch     1 | loss: 2.4593256CurrentTrain: epoch  8, batch     0 | loss: 2.5916147CurrentTrain: epoch  8, batch     1 | loss: 2.3274446CurrentTrain: epoch  9, batch     0 | loss: 2.6271913CurrentTrain: epoch  9, batch     1 | loss: 2.3092363
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: tv-idol-johns -- atlanta -- `` american idol '' finalist michael johns moved to los angeles several years ago , but his heart is still in atlanta .
Head Entity: his
Tail Entity: atlanta
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: actress-sarah -- new york -- `` the talented actress sarah has always considered new york her home, even after moving to los angeles for her career. ''  
Head Entity: sarah  
Tail Entity: new york  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: her political involvement began early : at cornell , she helped organize local farmers ' cooperatives .
Head Entity: she
Tail Entity: cornell
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: After graduating from high school, he went on to study at Stanford University, where he excelled in his courses.  
Head Entity: he  
Tail Entity: Stanford University  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: u.s. rep. parren mitchell , founding member of congressional black caucus , dies at 85
Head Entity: parren mitchell
Tail Entity: u.s.
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passes away in cambridge  
Head Entity: stephen hawking  
Tail Entity: cambridge  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by two stepdaughters , barbara a. hammond and brenda l. stevenson ; a stepson , michael a. taylor ; two grandchildren and one great-grandchild .
Head Entity: he
Tail Entity: brenda l. stevenson
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has three children, including her son, john, and her daughters, emily and sarah.  
Head Entity: she  
Tail Entity: emily  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: flowers always contended politics was behind the extortion investigation , but appeals courts ruled against him .
Head Entity: him
Tail Entity: extortion
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: The prosecutor announced that the suspect was charged with theft after a thorough investigation.  
Head Entity: suspect  
Tail Entity: theft  
Mixup data size:  203
MixupTrain:  epoch  0, batch     0 | loss: 4.4429464MixupTrain:  epoch  0, batch     1 | loss: 4.4713774MixupTrain:  epoch  0, batch     2 | loss: 5.1745372MixupTrain:  epoch  0, batch     3 | loss: 4.9508886MixupTrain:  epoch  0, batch     4 | loss: 4.5333929MixupTrain:  epoch  0, batch     5 | loss: 4.7770677MixupTrain:  epoch  0, batch     6 | loss: 5.4192348MixupTrain:  epoch  0, batch     7 | loss: 4.9522114MixupTrain:  epoch  0, batch     8 | loss: 5.4715900MixupTrain:  epoch  0, batch     9 | loss: 4.4898458MixupTrain:  epoch  0, batch    10 | loss: 4.8979611MixupTrain:  epoch  0, batch    11 | loss: 4.7038603MixupTrain:  epoch  0, batch    12 | loss: 4.5351458
MemoryTrain:  epoch  0, batch     0 | loss: 1.9051888MemoryTrain:  epoch  0, batch     1 | loss: 1.7431402MemoryTrain:  epoch  0, batch     2 | loss: 2.9295952MemoryTrain:  epoch  0, batch     3 | loss: 2.0258312MemoryTrain:  epoch  0, batch     4 | loss: 3.0355077MemoryTrain:  epoch  0, batch     5 | loss: 2.8922377MemoryTrain:  epoch  1, batch     0 | loss: 1.7968323MemoryTrain:  epoch  1, batch     1 | loss: 2.4799256MemoryTrain:  epoch  1, batch     2 | loss: 2.1269078MemoryTrain:  epoch  1, batch     3 | loss: 2.3221207MemoryTrain:  epoch  1, batch     4 | loss: 1.4630542MemoryTrain:  epoch  1, batch     5 | loss: 1.9609718MemoryTrain:  epoch  2, batch     0 | loss: 1.8058279MemoryTrain:  epoch  2, batch     1 | loss: 1.4577515MemoryTrain:  epoch  2, batch     2 | loss: 1.9569411MemoryTrain:  epoch  2, batch     3 | loss: 1.5602576MemoryTrain:  epoch  2, batch     4 | loss: 2.2282977MemoryTrain:  epoch  2, batch     5 | loss: 2.1089041MemoryTrain:  epoch  3, batch     0 | loss: 1.6664436MemoryTrain:  epoch  3, batch     1 | loss: 1.5644422MemoryTrain:  epoch  3, batch     2 | loss: 1.4947870MemoryTrain:  epoch  3, batch     3 | loss: 1.8944328MemoryTrain:  epoch  3, batch     4 | loss: 1.6484969MemoryTrain:  epoch  3, batch     5 | loss: 1.2362618MemoryTrain:  epoch  4, batch     0 | loss: 1.5090969MemoryTrain:  epoch  4, batch     1 | loss: 1.2766010MemoryTrain:  epoch  4, batch     2 | loss: 1.5290952MemoryTrain:  epoch  4, batch     3 | loss: 2.1656241MemoryTrain:  epoch  4, batch     4 | loss: 1.7687449MemoryTrain:  epoch  4, batch     5 | loss: 1.0691376
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 75.62%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 73.86%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 76.04%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 77.88%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 79.46%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 80.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 83.09%   [EVAL] batch:   17 | acc: 18.75%,  total acc: 79.51%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 46.88%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 48.75%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 47.92%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 51.79%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 54.69%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 58.33%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 60.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 65.10%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 63.46%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 60.71%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 60.00%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 60.29%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 60.42%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 61.51%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 64.29%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 65.91%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 67.39%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 68.49%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 70.91%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 71.53%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 72.54%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 73.28%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 73.96%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 74.19%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 74.61%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 74.62%   [EVAL] batch:   33 | acc: 37.50%,  total acc: 73.53%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 71.79%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 69.79%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 67.91%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 66.12%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 64.58%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 64.38%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 64.94%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 65.03%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 65.12%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 64.91%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 65.28%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 64.27%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 64.10%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 64.71%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 64.54%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 64.12%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 63.60%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 63.34%   [EVAL] batch:   52 | acc: 25.00%,  total acc: 62.62%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 61.69%   [EVAL] batch:   54 | acc: 37.50%,  total acc: 61.25%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 60.94%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 60.42%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 59.38%   [EVAL] batch:   58 | acc: 18.75%,  total acc: 58.69%   [EVAL] batch:   59 | acc: 6.25%,  total acc: 57.81%   [EVAL] batch:   60 | acc: 6.25%,  total acc: 56.97%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 56.05%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 55.16%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 54.49%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 53.75%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 53.03%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 52.61%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 52.11%   [EVAL] batch:   68 | acc: 0.00%,  total acc: 51.36%   [EVAL] batch:   69 | acc: 6.25%,  total acc: 50.71%   [EVAL] batch:   70 | acc: 0.00%,  total acc: 50.00%   [EVAL] batch:   71 | acc: 25.00%,  total acc: 49.65%   [EVAL] batch:   72 | acc: 37.50%,  total acc: 49.49%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 49.58%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 49.67%   [EVAL] batch:   75 | acc: 43.75%,  total acc: 49.59%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 49.76%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 49.92%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 50.16%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 50.62%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 50.93%   [EVAL] batch:   81 | acc: 93.75%,  total acc: 51.45%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 51.73%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 52.08%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 52.06%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 51.82%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 51.80%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 52.06%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 52.25%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 52.71%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 53.23%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 53.74%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 54.23%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 54.72%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 55.20%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 55.60%   [EVAL] batch:   96 | acc: 43.75%,  total acc: 55.48%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 55.29%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 55.62%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 55.75%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 55.88%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 55.88%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 55.83%   [EVAL] batch:  103 | acc: 25.00%,  total acc: 55.53%   [EVAL] batch:  104 | acc: 31.25%,  total acc: 55.30%   [EVAL] batch:  105 | acc: 43.75%,  total acc: 55.19%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 55.55%   [EVAL] batch:  107 | acc: 93.75%,  total acc: 55.90%   [EVAL] batch:  108 | acc: 87.50%,  total acc: 56.19%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 56.53%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 56.64%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 56.31%   [EVAL] batch:  112 | acc: 6.25%,  total acc: 55.86%   [EVAL] batch:  113 | acc: 0.00%,  total acc: 55.37%   [EVAL] batch:  114 | acc: 0.00%,  total acc: 54.89%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 55.06%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 55.34%   [EVAL] batch:  117 | acc: 68.75%,  total acc: 55.46%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 55.72%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 55.94%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 56.04%   [EVAL] batch:  121 | acc: 75.00%,  total acc: 56.20%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 56.55%   [EVAL] batch:  123 | acc: 50.00%,  total acc: 56.50%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 56.55%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 56.55%   [EVAL] batch:  126 | acc: 75.00%,  total acc: 56.69%   [EVAL] batch:  127 | acc: 100.00%,  total acc: 57.03%   [EVAL] batch:  128 | acc: 100.00%,  total acc: 57.36%   [EVAL] batch:  129 | acc: 100.00%,  total acc: 57.69%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 58.02%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 58.33%   [EVAL] batch:  132 | acc: 56.25%,  total acc: 58.32%   
cur_acc:  ['0.8655', '0.7723', '0.6016', '0.5529', '0.4489', '0.8080', '0.4958', '0.7951']
his_acc:  ['0.8655', '0.8231', '0.7083', '0.6392', '0.5526', '0.5718', '0.5781', '0.5832']
--------Round  1
seed:  200
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 11.4952507CurrentTrain: epoch  0, batch     1 | loss: 11.9618664CurrentTrain: epoch  0, batch     2 | loss: 11.7091227CurrentTrain: epoch  0, batch     3 | loss: 11.4504623CurrentTrain: epoch  0, batch     4 | loss: 11.4995012CurrentTrain: epoch  0, batch     5 | loss: 11.8947592CurrentTrain: epoch  0, batch     6 | loss: 10.6762991CurrentTrain: epoch  0, batch     7 | loss: 11.2016792CurrentTrain: epoch  0, batch     8 | loss: 11.1656504CurrentTrain: epoch  0, batch     9 | loss: 10.4580593CurrentTrain: epoch  0, batch    10 | loss: 10.7963085CurrentTrain: epoch  0, batch    11 | loss: 10.9828415CurrentTrain: epoch  0, batch    12 | loss: 11.0626698CurrentTrain: epoch  0, batch    13 | loss: 10.0168800CurrentTrain: epoch  0, batch    14 | loss: 10.2110786CurrentTrain: epoch  0, batch    15 | loss: 10.2323437CurrentTrain: epoch  0, batch    16 | loss: 10.8332634CurrentTrain: epoch  0, batch    17 | loss: 10.1460962CurrentTrain: epoch  0, batch    18 | loss: 9.4991436CurrentTrain: epoch  0, batch    19 | loss: 9.7791662CurrentTrain: epoch  0, batch    20 | loss: 9.9924326CurrentTrain: epoch  0, batch    21 | loss: 9.9689007CurrentTrain: epoch  0, batch    22 | loss: 9.8920565CurrentTrain: epoch  0, batch    23 | loss: 10.0600882CurrentTrain: epoch  0, batch    24 | loss: 10.3968010CurrentTrain: epoch  0, batch    25 | loss: 9.6663389CurrentTrain: epoch  0, batch    26 | loss: 10.7101421CurrentTrain: epoch  0, batch    27 | loss: 9.7565861CurrentTrain: epoch  0, batch    28 | loss: 9.9091043CurrentTrain: epoch  0, batch    29 | loss: 9.7145243CurrentTrain: epoch  0, batch    30 | loss: 10.0316944CurrentTrain: epoch  0, batch    31 | loss: 10.5263100CurrentTrain: epoch  0, batch    32 | loss: 9.5474558CurrentTrain: epoch  0, batch    33 | loss: 8.9952526CurrentTrain: epoch  0, batch    34 | loss: 8.7655602CurrentTrain: epoch  0, batch    35 | loss: 9.4703541CurrentTrain: epoch  0, batch    36 | loss: 10.4985476CurrentTrain: epoch  0, batch    37 | loss: 10.1135426CurrentTrain: epoch  1, batch     0 | loss: 9.3058958CurrentTrain: epoch  1, batch     1 | loss: 9.7666206CurrentTrain: epoch  1, batch     2 | loss: 8.7753677CurrentTrain: epoch  1, batch     3 | loss: 9.6046209CurrentTrain: epoch  1, batch     4 | loss: 10.9012032CurrentTrain: epoch  1, batch     5 | loss: 9.6132660CurrentTrain: epoch  1, batch     6 | loss: 9.4753084CurrentTrain: epoch  1, batch     7 | loss: 8.9616909CurrentTrain: epoch  1, batch     8 | loss: 7.8810072CurrentTrain: epoch  1, batch     9 | loss: 9.9477110CurrentTrain: epoch  1, batch    10 | loss: 9.2504587CurrentTrain: epoch  1, batch    11 | loss: 8.7776470CurrentTrain: epoch  1, batch    12 | loss: 9.4625034CurrentTrain: epoch  1, batch    13 | loss: 9.0188789CurrentTrain: epoch  1, batch    14 | loss: 9.4873676CurrentTrain: epoch  1, batch    15 | loss: 9.0470953CurrentTrain: epoch  1, batch    16 | loss: 8.9175396CurrentTrain: epoch  1, batch    17 | loss: 8.1677103CurrentTrain: epoch  1, batch    18 | loss: 9.3521872CurrentTrain: epoch  1, batch    19 | loss: 8.4244194CurrentTrain: epoch  1, batch    20 | loss: 8.5383301CurrentTrain: epoch  1, batch    21 | loss: 8.6236935CurrentTrain: epoch  1, batch    22 | loss: 8.8053122CurrentTrain: epoch  1, batch    23 | loss: 9.2407932CurrentTrain: epoch  1, batch    24 | loss: 8.2683210CurrentTrain: epoch  1, batch    25 | loss: 8.6047058CurrentTrain: epoch  1, batch    26 | loss: 8.6177692CurrentTrain: epoch  1, batch    27 | loss: 7.6133947CurrentTrain: epoch  1, batch    28 | loss: 7.7481151CurrentTrain: epoch  1, batch    29 | loss: 8.0339756CurrentTrain: epoch  1, batch    30 | loss: 8.2406912CurrentTrain: epoch  1, batch    31 | loss: 8.9478836CurrentTrain: epoch  1, batch    32 | loss: 8.7471828CurrentTrain: epoch  1, batch    33 | loss: 7.8716350CurrentTrain: epoch  1, batch    34 | loss: 7.9947252CurrentTrain: epoch  1, batch    35 | loss: 9.0526104CurrentTrain: epoch  1, batch    36 | loss: 7.8852739CurrentTrain: epoch  1, batch    37 | loss: 8.0240078CurrentTrain: epoch  2, batch     0 | loss: 7.8816385CurrentTrain: epoch  2, batch     1 | loss: 6.9630108CurrentTrain: epoch  2, batch     2 | loss: 7.5695763CurrentTrain: epoch  2, batch     3 | loss: 7.7512107CurrentTrain: epoch  2, batch     4 | loss: 7.4639015CurrentTrain: epoch  2, batch     5 | loss: 8.0823402CurrentTrain: epoch  2, batch     6 | loss: 7.8754063CurrentTrain: epoch  2, batch     7 | loss: 8.1218376CurrentTrain: epoch  2, batch     8 | loss: 8.6295471CurrentTrain: epoch  2, batch     9 | loss: 8.5351734CurrentTrain: epoch  2, batch    10 | loss: 7.5891767CurrentTrain: epoch  2, batch    11 | loss: 7.8056726CurrentTrain: epoch  2, batch    12 | loss: 8.3175058CurrentTrain: epoch  2, batch    13 | loss: 7.9806957CurrentTrain: epoch  2, batch    14 | loss: 7.8453770CurrentTrain: epoch  2, batch    15 | loss: 6.1967363CurrentTrain: epoch  2, batch    16 | loss: 7.5923672CurrentTrain: epoch  2, batch    17 | loss: 7.9453263CurrentTrain: epoch  2, batch    18 | loss: 6.7473302CurrentTrain: epoch  2, batch    19 | loss: 8.4022045CurrentTrain: epoch  2, batch    20 | loss: 8.7440348CurrentTrain: epoch  2, batch    21 | loss: 7.3044996CurrentTrain: epoch  2, batch    22 | loss: 7.4315844CurrentTrain: epoch  2, batch    23 | loss: 7.2921133CurrentTrain: epoch  2, batch    24 | loss: 7.9160123CurrentTrain: epoch  2, batch    25 | loss: 7.1774483CurrentTrain: epoch  2, batch    26 | loss: 6.5164294CurrentTrain: epoch  2, batch    27 | loss: 7.3089333CurrentTrain: epoch  2, batch    28 | loss: 7.1532969CurrentTrain: epoch  2, batch    29 | loss: 7.2100854CurrentTrain: epoch  2, batch    30 | loss: 7.5278969CurrentTrain: epoch  2, batch    31 | loss: 7.2912726CurrentTrain: epoch  2, batch    32 | loss: 7.6876760CurrentTrain: epoch  2, batch    33 | loss: 8.0848656CurrentTrain: epoch  2, batch    34 | loss: 7.6523390CurrentTrain: epoch  2, batch    35 | loss: 7.5282297CurrentTrain: epoch  2, batch    36 | loss: 7.8699369CurrentTrain: epoch  2, batch    37 | loss: 6.7402172CurrentTrain: epoch  3, batch     0 | loss: 6.6062789CurrentTrain: epoch  3, batch     1 | loss: 7.0929980CurrentTrain: epoch  3, batch     2 | loss: 6.9733315CurrentTrain: epoch  3, batch     3 | loss: 7.1811829CurrentTrain: epoch  3, batch     4 | loss: 6.8607635CurrentTrain: epoch  3, batch     5 | loss: 6.9373345CurrentTrain: epoch  3, batch     6 | loss: 7.5848722CurrentTrain: epoch  3, batch     7 | loss: 7.3785467CurrentTrain: epoch  3, batch     8 | loss: 7.1194916CurrentTrain: epoch  3, batch     9 | loss: 6.4621677CurrentTrain: epoch  3, batch    10 | loss: 6.0384378CurrentTrain: epoch  3, batch    11 | loss: 7.5622244CurrentTrain: epoch  3, batch    12 | loss: 7.3701310CurrentTrain: epoch  3, batch    13 | loss: 8.0111008CurrentTrain: epoch  3, batch    14 | loss: 8.0189323CurrentTrain: epoch  3, batch    15 | loss: 7.1449327CurrentTrain: epoch  3, batch    16 | loss: 7.0852728CurrentTrain: epoch  3, batch    17 | loss: 7.9288316CurrentTrain: epoch  3, batch    18 | loss: 6.6159534CurrentTrain: epoch  3, batch    19 | loss: 6.8427887CurrentTrain: epoch  3, batch    20 | loss: 5.9231000CurrentTrain: epoch  3, batch    21 | loss: 6.7443833CurrentTrain: epoch  3, batch    22 | loss: 7.2373815CurrentTrain: epoch  3, batch    23 | loss: 7.6036682CurrentTrain: epoch  3, batch    24 | loss: 6.5777149CurrentTrain: epoch  3, batch    25 | loss: 7.0841613CurrentTrain: epoch  3, batch    26 | loss: 6.6016026CurrentTrain: epoch  3, batch    27 | loss: 6.6885347CurrentTrain: epoch  3, batch    28 | loss: 6.8385563CurrentTrain: epoch  3, batch    29 | loss: 7.1129093CurrentTrain: epoch  3, batch    30 | loss: 6.5684385CurrentTrain: epoch  3, batch    31 | loss: 7.0256271CurrentTrain: epoch  3, batch    32 | loss: 6.7370205CurrentTrain: epoch  3, batch    33 | loss: 6.7431717CurrentTrain: epoch  3, batch    34 | loss: 5.8190374CurrentTrain: epoch  3, batch    35 | loss: 6.8154526CurrentTrain: epoch  3, batch    36 | loss: 6.0599246CurrentTrain: epoch  3, batch    37 | loss: 6.6704588CurrentTrain: epoch  4, batch     0 | loss: 6.2489414CurrentTrain: epoch  4, batch     1 | loss: 6.9139490CurrentTrain: epoch  4, batch     2 | loss: 6.8215113CurrentTrain: epoch  4, batch     3 | loss: 6.6996984CurrentTrain: epoch  4, batch     4 | loss: 7.0032768CurrentTrain: epoch  4, batch     5 | loss: 6.0609031CurrentTrain: epoch  4, batch     6 | loss: 7.0297675CurrentTrain: epoch  4, batch     7 | loss: 6.7138028CurrentTrain: epoch  4, batch     8 | loss: 7.2800207CurrentTrain: epoch  4, batch     9 | loss: 6.0930357CurrentTrain: epoch  4, batch    10 | loss: 6.6360583CurrentTrain: epoch  4, batch    11 | loss: 6.4726996CurrentTrain: epoch  4, batch    12 | loss: 6.5891943CurrentTrain: epoch  4, batch    13 | loss: 6.3953247CurrentTrain: epoch  4, batch    14 | loss: 5.8619003CurrentTrain: epoch  4, batch    15 | loss: 6.3164496CurrentTrain: epoch  4, batch    16 | loss: 6.2850175CurrentTrain: epoch  4, batch    17 | loss: 7.3138337CurrentTrain: epoch  4, batch    18 | loss: 6.1229067CurrentTrain: epoch  4, batch    19 | loss: 6.0989232CurrentTrain: epoch  4, batch    20 | loss: 6.3414903CurrentTrain: epoch  4, batch    21 | loss: 6.3103552CurrentTrain: epoch  4, batch    22 | loss: 6.1693735CurrentTrain: epoch  4, batch    23 | loss: 5.8889990CurrentTrain: epoch  4, batch    24 | loss: 6.2656651CurrentTrain: epoch  4, batch    25 | loss: 6.6743212CurrentTrain: epoch  4, batch    26 | loss: 6.5033112CurrentTrain: epoch  4, batch    27 | loss: 6.0698991CurrentTrain: epoch  4, batch    28 | loss: 6.7444296CurrentTrain: epoch  4, batch    29 | loss: 5.9560986CurrentTrain: epoch  4, batch    30 | loss: 5.8302855CurrentTrain: epoch  4, batch    31 | loss: 6.1072626CurrentTrain: epoch  4, batch    32 | loss: 5.5638819CurrentTrain: epoch  4, batch    33 | loss: 6.0502529CurrentTrain: epoch  4, batch    34 | loss: 5.7723093CurrentTrain: epoch  4, batch    35 | loss: 5.2214112CurrentTrain: epoch  4, batch    36 | loss: 6.0837131CurrentTrain: epoch  4, batch    37 | loss: 5.6245847CurrentTrain: epoch  5, batch     0 | loss: 5.9872437CurrentTrain: epoch  5, batch     1 | loss: 5.4005785CurrentTrain: epoch  5, batch     2 | loss: 5.8341322CurrentTrain: epoch  5, batch     3 | loss: 6.0954084CurrentTrain: epoch  5, batch     4 | loss: 8.0198240CurrentTrain: epoch  5, batch     5 | loss: 6.2535648CurrentTrain: epoch  5, batch     6 | loss: 5.6903715CurrentTrain: epoch  5, batch     7 | loss: 6.2086544CurrentTrain: epoch  5, batch     8 | loss: 5.8370981CurrentTrain: epoch  5, batch     9 | loss: 6.1773944CurrentTrain: epoch  5, batch    10 | loss: 5.8045721CurrentTrain: epoch  5, batch    11 | loss: 5.8567491CurrentTrain: epoch  5, batch    12 | loss: 5.7813425CurrentTrain: epoch  5, batch    13 | loss: 6.2178373CurrentTrain: epoch  5, batch    14 | loss: 6.1837616CurrentTrain: epoch  5, batch    15 | loss: 6.2117162CurrentTrain: epoch  5, batch    16 | loss: 5.5172310CurrentTrain: epoch  5, batch    17 | loss: 5.9553170CurrentTrain: epoch  5, batch    18 | loss: 5.5209274CurrentTrain: epoch  5, batch    19 | loss: 5.8181105CurrentTrain: epoch  5, batch    20 | loss: 5.9316955CurrentTrain: epoch  5, batch    21 | loss: 6.6555023CurrentTrain: epoch  5, batch    22 | loss: 6.2339163CurrentTrain: epoch  5, batch    23 | loss: 5.6159296CurrentTrain: epoch  5, batch    24 | loss: 5.6755266CurrentTrain: epoch  5, batch    25 | loss: 5.6136675CurrentTrain: epoch  5, batch    26 | loss: 5.6292944CurrentTrain: epoch  5, batch    27 | loss: 5.7195454CurrentTrain: epoch  5, batch    28 | loss: 5.5026999CurrentTrain: epoch  5, batch    29 | loss: 6.9394045CurrentTrain: epoch  5, batch    30 | loss: 5.9549470CurrentTrain: epoch  5, batch    31 | loss: 5.9532332CurrentTrain: epoch  5, batch    32 | loss: 5.4807677CurrentTrain: epoch  5, batch    33 | loss: 5.3622317CurrentTrain: epoch  5, batch    34 | loss: 6.2250767CurrentTrain: epoch  5, batch    35 | loss: 6.1277623CurrentTrain: epoch  5, batch    36 | loss: 5.9913163CurrentTrain: epoch  5, batch    37 | loss: 4.8407764CurrentTrain: epoch  6, batch     0 | loss: 5.5547996CurrentTrain: epoch  6, batch     1 | loss: 5.6077251CurrentTrain: epoch  6, batch     2 | loss: 5.5835762CurrentTrain: epoch  6, batch     3 | loss: 6.1344423CurrentTrain: epoch  6, batch     4 | loss: 5.5800405CurrentTrain: epoch  6, batch     5 | loss: 5.7692723CurrentTrain: epoch  6, batch     6 | loss: 5.1928892CurrentTrain: epoch  6, batch     7 | loss: 5.3895006CurrentTrain: epoch  6, batch     8 | loss: 5.3614798CurrentTrain: epoch  6, batch     9 | loss: 5.6295652CurrentTrain: epoch  6, batch    10 | loss: 5.5838928CurrentTrain: epoch  6, batch    11 | loss: 5.6471338CurrentTrain: epoch  6, batch    12 | loss: 5.6680408CurrentTrain: epoch  6, batch    13 | loss: 5.5321217CurrentTrain: epoch  6, batch    14 | loss: 5.3577118CurrentTrain: epoch  6, batch    15 | loss: 5.9597845CurrentTrain: epoch  6, batch    16 | loss: 5.4082980CurrentTrain: epoch  6, batch    17 | loss: 5.2012658CurrentTrain: epoch  6, batch    18 | loss: 5.3993630CurrentTrain: epoch  6, batch    19 | loss: 5.3073206CurrentTrain: epoch  6, batch    20 | loss: 5.1158042CurrentTrain: epoch  6, batch    21 | loss: 5.1988964CurrentTrain: epoch  6, batch    22 | loss: 6.7516217CurrentTrain: epoch  6, batch    23 | loss: 6.3882980CurrentTrain: epoch  6, batch    24 | loss: 5.9561749CurrentTrain: epoch  6, batch    25 | loss: 5.2372255CurrentTrain: epoch  6, batch    26 | loss: 5.5052004CurrentTrain: epoch  6, batch    27 | loss: 5.4510899CurrentTrain: epoch  6, batch    28 | loss: 5.5559216CurrentTrain: epoch  6, batch    29 | loss: 5.3476176CurrentTrain: epoch  6, batch    30 | loss: 5.5067139CurrentTrain: epoch  6, batch    31 | loss: 5.5220561CurrentTrain: epoch  6, batch    32 | loss: 5.1367607CurrentTrain: epoch  6, batch    33 | loss: 6.7668796CurrentTrain: epoch  6, batch    34 | loss: 5.5712180CurrentTrain: epoch  6, batch    35 | loss: 5.5531726CurrentTrain: epoch  6, batch    36 | loss: 6.6285076CurrentTrain: epoch  6, batch    37 | loss: 5.0813398CurrentTrain: epoch  7, batch     0 | loss: 5.5609903CurrentTrain: epoch  7, batch     1 | loss: 5.5071807CurrentTrain: epoch  7, batch     2 | loss: 5.5841031CurrentTrain: epoch  7, batch     3 | loss: 5.8392434CurrentTrain: epoch  7, batch     4 | loss: 5.4410934CurrentTrain: epoch  7, batch     5 | loss: 5.2984691CurrentTrain: epoch  7, batch     6 | loss: 5.4871926CurrentTrain: epoch  7, batch     7 | loss: 5.8967161CurrentTrain: epoch  7, batch     8 | loss: 5.3586512CurrentTrain: epoch  7, batch     9 | loss: 5.2871027CurrentTrain: epoch  7, batch    10 | loss: 5.2546878CurrentTrain: epoch  7, batch    11 | loss: 4.9347744CurrentTrain: epoch  7, batch    12 | loss: 5.8759284CurrentTrain: epoch  7, batch    13 | loss: 5.2580881CurrentTrain: epoch  7, batch    14 | loss: 5.1904106CurrentTrain: epoch  7, batch    15 | loss: 5.2352996CurrentTrain: epoch  7, batch    16 | loss: 5.3971124CurrentTrain: epoch  7, batch    17 | loss: 5.4344935CurrentTrain: epoch  7, batch    18 | loss: 5.3451357CurrentTrain: epoch  7, batch    19 | loss: 5.0849705CurrentTrain: epoch  7, batch    20 | loss: 5.1249332CurrentTrain: epoch  7, batch    21 | loss: 5.2973261CurrentTrain: epoch  7, batch    22 | loss: 6.0297012CurrentTrain: epoch  7, batch    23 | loss: 5.5495868CurrentTrain: epoch  7, batch    24 | loss: 5.2133803CurrentTrain: epoch  7, batch    25 | loss: 5.1576505CurrentTrain: epoch  7, batch    26 | loss: 5.1422796CurrentTrain: epoch  7, batch    27 | loss: 5.1132975CurrentTrain: epoch  7, batch    28 | loss: 4.9744816CurrentTrain: epoch  7, batch    29 | loss: 5.7927094CurrentTrain: epoch  7, batch    30 | loss: 5.2160301CurrentTrain: epoch  7, batch    31 | loss: 5.2629786CurrentTrain: epoch  7, batch    32 | loss: 5.8652630CurrentTrain: epoch  7, batch    33 | loss: 5.5444746CurrentTrain: epoch  7, batch    34 | loss: 6.6703720CurrentTrain: epoch  7, batch    35 | loss: 5.9917049CurrentTrain: epoch  7, batch    36 | loss: 5.4586973CurrentTrain: epoch  7, batch    37 | loss: 5.2874126CurrentTrain: epoch  8, batch     0 | loss: 5.5083227CurrentTrain: epoch  8, batch     1 | loss: 5.4225760CurrentTrain: epoch  8, batch     2 | loss: 5.8430738CurrentTrain: epoch  8, batch     3 | loss: 5.5593462CurrentTrain: epoch  8, batch     4 | loss: 5.4635201CurrentTrain: epoch  8, batch     5 | loss: 5.0873146CurrentTrain: epoch  8, batch     6 | loss: 5.1428623CurrentTrain: epoch  8, batch     7 | loss: 5.0521269CurrentTrain: epoch  8, batch     8 | loss: 5.4818096CurrentTrain: epoch  8, batch     9 | loss: 4.9598989CurrentTrain: epoch  8, batch    10 | loss: 5.1151066CurrentTrain: epoch  8, batch    11 | loss: 4.9622831CurrentTrain: epoch  8, batch    12 | loss: 4.9481797CurrentTrain: epoch  8, batch    13 | loss: 4.9111986CurrentTrain: epoch  8, batch    14 | loss: 4.9557219CurrentTrain: epoch  8, batch    15 | loss: 5.0714278CurrentTrain: epoch  8, batch    16 | loss: 4.9566622CurrentTrain: epoch  8, batch    17 | loss: 5.1959500CurrentTrain: epoch  8, batch    18 | loss: 4.9190617CurrentTrain: epoch  8, batch    19 | loss: 5.8676329CurrentTrain: epoch  8, batch    20 | loss: 5.1642084CurrentTrain: epoch  8, batch    21 | loss: 5.0517421CurrentTrain: epoch  8, batch    22 | loss: 4.9425077CurrentTrain: epoch  8, batch    23 | loss: 5.0369940CurrentTrain: epoch  8, batch    24 | loss: 5.6340046CurrentTrain: epoch  8, batch    25 | loss: 5.2729049CurrentTrain: epoch  8, batch    26 | loss: 5.0862513CurrentTrain: epoch  8, batch    27 | loss: 5.0260410CurrentTrain: epoch  8, batch    28 | loss: 5.1333985CurrentTrain: epoch  8, batch    29 | loss: 5.1072674CurrentTrain: epoch  8, batch    30 | loss: 5.2668891CurrentTrain: epoch  8, batch    31 | loss: 5.1919985CurrentTrain: epoch  8, batch    32 | loss: 5.8619947CurrentTrain: epoch  8, batch    33 | loss: 4.9207497CurrentTrain: epoch  8, batch    34 | loss: 4.9847245CurrentTrain: epoch  8, batch    35 | loss: 5.0330801CurrentTrain: epoch  8, batch    36 | loss: 5.1846180CurrentTrain: epoch  8, batch    37 | loss: 5.1099491CurrentTrain: epoch  9, batch     0 | loss: 4.9323254CurrentTrain: epoch  9, batch     1 | loss: 5.0394716CurrentTrain: epoch  9, batch     2 | loss: 4.9611087CurrentTrain: epoch  9, batch     3 | loss: 5.4201789CurrentTrain: epoch  9, batch     4 | loss: 4.9534369CurrentTrain: epoch  9, batch     5 | loss: 5.0876293CurrentTrain: epoch  9, batch     6 | loss: 4.9941483CurrentTrain: epoch  9, batch     7 | loss: 5.0381145CurrentTrain: epoch  9, batch     8 | loss: 4.9778786CurrentTrain: epoch  9, batch     9 | loss: 5.0629029CurrentTrain: epoch  9, batch    10 | loss: 4.9532738CurrentTrain: epoch  9, batch    11 | loss: 5.1267118CurrentTrain: epoch  9, batch    12 | loss: 4.9238472CurrentTrain: epoch  9, batch    13 | loss: 4.9978037CurrentTrain: epoch  9, batch    14 | loss: 4.9796124CurrentTrain: epoch  9, batch    15 | loss: 4.9739547CurrentTrain: epoch  9, batch    16 | loss: 5.0294361CurrentTrain: epoch  9, batch    17 | loss: 4.8867884CurrentTrain: epoch  9, batch    18 | loss: 4.9829035CurrentTrain: epoch  9, batch    19 | loss: 4.8389482CurrentTrain: epoch  9, batch    20 | loss: 4.9687490CurrentTrain: epoch  9, batch    21 | loss: 4.8972168CurrentTrain: epoch  9, batch    22 | loss: 4.7841969CurrentTrain: epoch  9, batch    23 | loss: 4.8289423CurrentTrain: epoch  9, batch    24 | loss: 5.1305075CurrentTrain: epoch  9, batch    25 | loss: 5.3241606CurrentTrain: epoch  9, batch    26 | loss: 5.0097022CurrentTrain: epoch  9, batch    27 | loss: 4.8564391CurrentTrain: epoch  9, batch    28 | loss: 4.9755569CurrentTrain: epoch  9, batch    29 | loss: 4.7278547CurrentTrain: epoch  9, batch    30 | loss: 4.9005756CurrentTrain: epoch  9, batch    31 | loss: 5.1333466CurrentTrain: epoch  9, batch    32 | loss: 4.8360300CurrentTrain: epoch  9, batch    33 | loss: 5.0445766CurrentTrain: epoch  9, batch    34 | loss: 5.2298670CurrentTrain: epoch  9, batch    35 | loss: 4.8302641CurrentTrain: epoch  9, batch    36 | loss: 4.9972177CurrentTrain: epoch  9, batch    37 | loss: 4.7948170
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: the head of iran 's atomic energy organisation , ali akbar salehi , told domestic news agencies the new proposal had come in response to iran 's move last week to begin enriching uranium itself to the 20 percent level required for a tehran medical research rector after rejecting a previous offer .
Head Entity: ali akbar salehi
Tail Entity: iran
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the United States, Maria decided to return to her hometown in Spain, where she felt a strong connection to her roots.  
Head Entity: Maria  
Tail Entity: Spain  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: whether this means the country 's tense political situation will dissolve back into civil war is yet to be seen , said rinaldo depagne , a west africa analyst at the international crisis group .
Head Entity: international crisis group
Tail Entity: rinaldo depagne
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: the board of directors at tech innovations inc. has appointed a new chief executive officer to lead the company into its next phase of growth.  
Head Entity: tech innovations inc.  
Tail Entity: chief executive officer  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: linebacker mike peterson rejoined the jacksonville jaguars on friday , two days after coach jack del rio banished him from the locker room of the national football league team .
Head Entity: jacksonville jaguars
Tail Entity: national football league
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: the united nations is an international organization that aims to promote peace and cooperation among countries.  
Head Entity: united nations  
Tail Entity: international organization  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: the wife of acting cuban president raul castro , vilma espin guillois , died monday after a lengthy illness , cuban television reported monday .
Head Entity: raul castro
Tail Entity: cuban
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: the famous actor and director, ang lee, was born in taiwan and has made significant contributions to cinema worldwide.  
Head Entity: ang lee  
Tail Entity: taiwan  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: `` we are not canceling any of our orders for next year , '' deputy managing director philip chen was quoted as saying in today 's south china morning post .
Head Entity: philip chen
Tail Entity: director
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: `` the renowned scientist dr. jane smith has been awarded the prestigious title of chief researcher at the national institute of health. ''  
Head Entity: dr. jane smith  
Tail Entity: chief researcher  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: as the government still imposes many restrictions on investing in china , hochen said , chunghwa telecom will consult with the mainland affairs council -- taiwan 's top china policy planning agency -- and other relevant government institutions before launching its overseas expansion drive .
Head Entity: chunghwa telecom
Tail Entity: china
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: after years of rapid growth, the tech giant has decided to establish its new headquarters in the heart of silicon valley, aiming to strengthen its presence in the united states.  
Head Entity: tech giant  
Tail Entity: united states  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 86.16%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.23%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.60%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.20%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.26%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.95%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.74%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 86.16%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.23%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.60%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.20%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.26%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.95%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.74%   
cur_acc:  ['0.8674']
his_acc:  ['0.8674']
CurrentTrain: epoch  0, batch     0 | loss: 6.2508774CurrentTrain: epoch  0, batch     1 | loss: 7.0173969CurrentTrain: epoch  1, batch     0 | loss: 5.5441074CurrentTrain: epoch  1, batch     1 | loss: 6.7372098CurrentTrain: epoch  2, batch     0 | loss: 5.4576530CurrentTrain: epoch  2, batch     1 | loss: 5.8471255CurrentTrain: epoch  3, batch     0 | loss: 4.7888002CurrentTrain: epoch  3, batch     1 | loss: 5.5919361CurrentTrain: epoch  4, batch     0 | loss: 4.5885291CurrentTrain: epoch  4, batch     1 | loss: 5.1951432CurrentTrain: epoch  5, batch     0 | loss: 4.9872861CurrentTrain: epoch  5, batch     1 | loss: 3.5513031CurrentTrain: epoch  6, batch     0 | loss: 3.8067145CurrentTrain: epoch  6, batch     1 | loss: 4.6832795CurrentTrain: epoch  7, batch     0 | loss: 3.9353924CurrentTrain: epoch  7, batch     1 | loss: 3.9711919CurrentTrain: epoch  8, batch     0 | loss: 3.9946413CurrentTrain: epoch  8, batch     1 | loss: 3.1361611CurrentTrain: epoch  9, batch     0 | loss: 3.7065210CurrentTrain: epoch  9, batch     1 | loss: 3.1331658
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: san jose , ca , usa speaking of k-fed , him and ex-wife britney spears are in court today , dealing with their custody battle .
Head Entity: britney spears
Tail Entity: ca
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: after moving to austin, texas, the musician found a new sense of inspiration and creativity in his work.  
Head Entity: musician  
Tail Entity: texas  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: he passed away on saturday .
Head Entity: he
Tail Entity: saturday
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: she left this world on March 5th.  
Head Entity: she  
Tail Entity: March 5th  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: covidien , which posted revenue of more than $ 10 billion last year , has about 42,000 employees worldwide .
Head Entity: covidien
Tail Entity: 42,000
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: TechCorp, a leading software company, employs approximately 15,000 people across its global offices.  
Head Entity: TechCorp  
Tail Entity: 15,000  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: `` i am known in the hospice as the man who would n't die , '' buchwald wrote in march .
Head Entity: buchwald
Tail Entity: man who would n't die
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: `` the famous author often referred to as the bard of Avon, '' shakespeare is celebrated for his timeless plays.  
Head Entity: shakespeare  
Tail Entity: bard of Avon  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: beverly hills , california 2008-08-17 21:15:39 utc ------ there was much dancing : ellen degeneres and portia de rossi are married , according to reports .
Head Entity: ellen degeneres
Tail Entity: portia de rossi
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: in a beautiful ceremony held in new york city, 2015-06-20 15:30:00 utc ------ the couple exchanged vows: john legend and chrissy teigen are now husband and wife, as confirmed by their friends.  
Head Entity: john legend  
Tail Entity: chrissy teigen  
Mixup data size:  83
MixupTrain:  epoch  0, batch     0 | loss: 8.3690081MixupTrain:  epoch  0, batch     1 | loss: 8.9848785MixupTrain:  epoch  0, batch     2 | loss: 8.2281713MixupTrain:  epoch  0, batch     3 | loss: 7.8345060MixupTrain:  epoch  0, batch     4 | loss: 7.6344385MixupTrain:  epoch  0, batch     5 | loss: 9.8360224
MemoryTrain:  epoch  0, batch     0 | loss: 5.7464375MemoryTrain:  epoch  0, batch     1 | loss: 6.1045599MemoryTrain:  epoch  1, batch     0 | loss: 6.0829582MemoryTrain:  epoch  1, batch     1 | loss: 5.1916161MemoryTrain:  epoch  2, batch     0 | loss: 5.1726503MemoryTrain:  epoch  2, batch     1 | loss: 4.5453391MemoryTrain:  epoch  3, batch     0 | loss: 4.9627457MemoryTrain:  epoch  3, batch     1 | loss: 4.5506768MemoryTrain:  epoch  4, batch     0 | loss: 4.1071310MemoryTrain:  epoch  4, batch     1 | loss: 4.2850966
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 26.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 36.46%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 45.54%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 51.56%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 56.88%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 57.95%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 60.94%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 62.98%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 64.17%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 80.56%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 83.85%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 84.13%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 83.04%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 80.86%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 80.51%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 79.51%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 79.61%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 80.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.95%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 81.82%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 82.61%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 84.95%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.49%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.99%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 86.49%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 85.80%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 83.82%   [EVAL] batch:   34 | acc: 18.75%,  total acc: 81.96%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 80.38%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 79.05%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 78.95%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 79.33%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.84%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 80.03%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 80.51%   [EVAL] batch:   42 | acc: 31.25%,  total acc: 79.36%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 79.83%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 80.00%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 80.30%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 80.32%   
cur_acc:  ['0.8674', '0.6417']
his_acc:  ['0.8674', '0.8032']
CurrentTrain: epoch  0, batch     0 | loss: 7.1842089CurrentTrain: epoch  0, batch     1 | loss: 6.6037807CurrentTrain: epoch  1, batch     0 | loss: 5.9677663CurrentTrain: epoch  1, batch     1 | loss: 6.7238832CurrentTrain: epoch  2, batch     0 | loss: 5.6318412CurrentTrain: epoch  2, batch     1 | loss: 4.6122870CurrentTrain: epoch  3, batch     0 | loss: 4.8902473CurrentTrain: epoch  3, batch     1 | loss: 4.6476340CurrentTrain: epoch  4, batch     0 | loss: 4.6716976CurrentTrain: epoch  4, batch     1 | loss: 4.3549142CurrentTrain: epoch  5, batch     0 | loss: 4.4280300CurrentTrain: epoch  5, batch     1 | loss: 4.4827414CurrentTrain: epoch  6, batch     0 | loss: 4.3431025CurrentTrain: epoch  6, batch     1 | loss: 3.6469667CurrentTrain: epoch  7, batch     0 | loss: 4.1166229CurrentTrain: epoch  7, batch     1 | loss: 3.2769480CurrentTrain: epoch  8, batch     0 | loss: 3.4010024CurrentTrain: epoch  8, batch     1 | loss: 3.1774213CurrentTrain: epoch  9, batch     0 | loss: 3.0832024CurrentTrain: epoch  9, batch     1 | loss: 3.0529644
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: born in 1950 in the northeastern city of basel , ospel left school at 15 to take an apprenticeship at the transvalor brokerage house before joining swiss banking corporation -lrb- sbs -rrb- , which merged with union bank of switzerland to form ubs in 1998 .
Head Entity: ospel
Tail Entity: 1950
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: she was born on July 4, 1985, in a small town in California, where she spent her childhood before moving to New York for her career.  
Head Entity: she  
Tail Entity: July 4, 1985  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: jane matilda bolin was born on april 11 , 1908 , in poughkeepsie , ny .
Head Entity: jane matilda bolin
Tail Entity: ny
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: john smith was born in los angeles, california, on march 5, 1980.  
Head Entity: john smith  
Tail Entity: california  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: lynne spears told the court that lutfi had treated her daughter like a hostage in her own home , drugged her and took over her finances .
Head Entity: her
Tail Entity: lynne spears
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: jennifer's mother, who was a single parent, worked multiple jobs to support her family.  
Head Entity: jennifer's mother  
Tail Entity: jennifer  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: after years of hard work, jane doe finally received a promotion at tech innovations, where she has been a dedicated employee.  
Head Entity: jane doe  
Tail Entity: tech innovations  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, 75, passed away peacefully in his sleep on march 5 in springfield, il, leaving behind a legacy of kindness and community service.  
Head Entity: john doe  
Tail Entity: il  
Mixup data size:  102
MixupTrain:  epoch  0, batch     0 | loss: 8.2197056MixupTrain:  epoch  0, batch     1 | loss: 7.2765303MixupTrain:  epoch  0, batch     2 | loss: 7.2947898MixupTrain:  epoch  0, batch     3 | loss: 7.3483095MixupTrain:  epoch  0, batch     4 | loss: 6.5866671MixupTrain:  epoch  0, batch     5 | loss: 7.4389420MixupTrain:  epoch  0, batch     6 | loss: 6.8639660
MemoryTrain:  epoch  0, batch     0 | loss: 4.4581661MemoryTrain:  epoch  0, batch     1 | loss: 4.5201335MemoryTrain:  epoch  1, batch     0 | loss: 4.3876381MemoryTrain:  epoch  1, batch     1 | loss: 4.8482223MemoryTrain:  epoch  2, batch     0 | loss: 4.4524655MemoryTrain:  epoch  2, batch     1 | loss: 3.8172193MemoryTrain:  epoch  3, batch     0 | loss: 4.0586133MemoryTrain:  epoch  3, batch     1 | loss: 3.0957634MemoryTrain:  epoch  4, batch     0 | loss: 3.1375132MemoryTrain:  epoch  4, batch     1 | loss: 3.5386157
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 57.50%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 58.33%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 58.04%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 61.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 65.97%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 68.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 69.89%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 71.63%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 67.41%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 80.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 83.17%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 81.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 80.08%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 79.78%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 78.82%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 79.28%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.65%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 81.53%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 82.34%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 83.07%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 84.72%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.27%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.78%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 86.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.49%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 85.98%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 83.82%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 82.14%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 80.38%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 78.55%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 78.62%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 78.53%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 78.91%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 78.66%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   42 | acc: 31.25%,  total acc: 78.05%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 77.70%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 77.22%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 76.22%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 75.66%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 75.13%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 74.75%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 74.14%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 74.04%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 73.70%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 73.50%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 73.75%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 74.11%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 74.45%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 74.57%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 74.79%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 74.69%   [EVAL] batch:   60 | acc: 6.25%,  total acc: 73.57%   
cur_acc:  ['0.8674', '0.6417', '0.6741']
his_acc:  ['0.8674', '0.8032', '0.7357']
CurrentTrain: epoch  0, batch     0 | loss: 5.9143386CurrentTrain: epoch  0, batch     1 | loss: 6.6242852CurrentTrain: epoch  1, batch     0 | loss: 5.5062704CurrentTrain: epoch  1, batch     1 | loss: 4.9575524CurrentTrain: epoch  2, batch     0 | loss: 4.9742765CurrentTrain: epoch  2, batch     1 | loss: 4.8739762CurrentTrain: epoch  3, batch     0 | loss: 4.6593690CurrentTrain: epoch  3, batch     1 | loss: 4.5160270CurrentTrain: epoch  4, batch     0 | loss: 3.9515514CurrentTrain: epoch  4, batch     1 | loss: 4.1139846CurrentTrain: epoch  5, batch     0 | loss: 3.9547834CurrentTrain: epoch  5, batch     1 | loss: 3.7887590CurrentTrain: epoch  6, batch     0 | loss: 3.6320076CurrentTrain: epoch  6, batch     1 | loss: 3.9009175CurrentTrain: epoch  7, batch     0 | loss: 3.6601787CurrentTrain: epoch  7, batch     1 | loss: 2.8136148CurrentTrain: epoch  8, batch     0 | loss: 3.1332211CurrentTrain: epoch  8, batch     1 | loss: 3.3389337CurrentTrain: epoch  9, batch     0 | loss: 2.8582754CurrentTrain: epoch  9, batch     1 | loss: 2.8002341
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: tv-idol-johns -- atlanta -- `` american idol '' finalist michael johns moved to los angeles several years ago , but his heart is still in atlanta .
Head Entity: his
Tail Entity: atlanta
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: actress-sarah -- new york -- `` the talented actress sarah has always considered new york her home, even after moving to los angeles for her career. ''  
Head Entity: sarah  
Tail Entity: new york  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: in a cheerful first-person narration , he travels to a girls ' school in kabul , to post-soviet russia and to his alma mater , hollywood high in los angeles , the place where he first learned to mistrust liberals .
Head Entity: he
Tail Entity: hollywood high
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: After graduating from a prestigious university, she often reminisces about her time at the local community college where she developed her passion for art.  
Head Entity: she  
Tail Entity: local community college  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: us republican congresswoman jo ann davis dies after fight with breast cancer
Head Entity: jo ann davis
Tail Entity: us
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england  
Head Entity: stephen hawking  
Tail Entity: england  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by his wife of 63 years , josephine robinson mcnair , of columbia ; a son , robert e. jr. , of columbia ; three daughters , robin lee howell and corinne godshall , of myrtle beach , s.c. , and claudia crawford mcnair , of jamestown , s.c. ; six grandchildren ; and one great-grandchild .
Head Entity: he
Tail Entity: claudia crawford mcnair
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has two children, a son named michael and a daughter named sarah, who both live in new york.  
Head Entity: she  
Tail Entity: sarah  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: ferrara said he was innocent of limoli 's slaying , but he pleaded guilty in 1992 to murder , along with racketeering charges , under a deal that sent him to prison for 22 years , rather than go to trial and risk a conviction that could lead to life in prison .
Head Entity: ferrara
Tail Entity: racketeering
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: after a lengthy investigation, the authorities announced that johnson was charged with embezzlement, which shocked his colleagues at the firm.  
Head Entity: johnson  
Tail Entity: embezzlement  
Mixup data size:  122
MixupTrain:  epoch  0, batch     0 | loss: 6.4158363MixupTrain:  epoch  0, batch     1 | loss: 6.4991493MixupTrain:  epoch  0, batch     2 | loss: 6.3409581MixupTrain:  epoch  0, batch     3 | loss: 6.4130459MixupTrain:  epoch  0, batch     4 | loss: 6.1320810MixupTrain:  epoch  0, batch     5 | loss: 6.2737355MixupTrain:  epoch  0, batch     6 | loss: 5.8261752MixupTrain:  epoch  0, batch     7 | loss: 6.0522485
MemoryTrain:  epoch  0, batch     0 | loss: 3.7649488MemoryTrain:  epoch  0, batch     1 | loss: 3.8348265MemoryTrain:  epoch  0, batch     2 | loss: 2.6112168MemoryTrain:  epoch  1, batch     0 | loss: 4.0935555MemoryTrain:  epoch  1, batch     1 | loss: 3.8402283MemoryTrain:  epoch  1, batch     2 | loss: 3.0671389MemoryTrain:  epoch  2, batch     0 | loss: 3.1569376MemoryTrain:  epoch  2, batch     1 | loss: 3.1288497MemoryTrain:  epoch  2, batch     2 | loss: 3.2293770MemoryTrain:  epoch  3, batch     0 | loss: 3.2470310MemoryTrain:  epoch  3, batch     1 | loss: 3.2914209MemoryTrain:  epoch  3, batch     2 | loss: 2.0522268MemoryTrain:  epoch  4, batch     0 | loss: 3.4273782MemoryTrain:  epoch  4, batch     1 | loss: 2.8168736MemoryTrain:  epoch  4, batch     2 | loss: 2.1584632
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 80.56%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 77.84%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 82.59%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 85.66%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 82.29%   
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 54.17%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 53.12%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 53.75%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 54.17%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 58.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 68.06%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 73.30%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 75.48%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 73.83%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 73.90%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 73.26%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 73.36%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 74.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 75.30%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 76.42%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 77.45%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 79.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.81%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 80.32%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.03%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 81.68%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 82.08%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 82.46%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 82.20%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 80.15%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 78.04%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 76.56%   [EVAL] batch:   36 | acc: 25.00%,  total acc: 75.17%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 75.16%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 75.48%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 76.09%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 75.91%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 76.49%   [EVAL] batch:   42 | acc: 37.50%,  total acc: 75.58%   [EVAL] batch:   43 | acc: 6.25%,  total acc: 74.01%   [EVAL] batch:   44 | acc: 6.25%,  total acc: 72.50%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 70.92%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 69.95%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 69.53%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 69.01%   [EVAL] batch:   49 | acc: 25.00%,  total acc: 68.12%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 67.52%   [EVAL] batch:   51 | acc: 31.25%,  total acc: 66.83%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 65.80%   [EVAL] batch:   53 | acc: 31.25%,  total acc: 65.16%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 65.45%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 65.79%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 65.73%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 65.68%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 65.73%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 65.37%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 65.73%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 66.07%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 66.50%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 66.54%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 66.88%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 67.37%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 67.30%   [EVAL] batch:   69 | acc: 62.50%,  total acc: 67.23%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 67.25%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 67.36%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 67.81%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 68.24%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 68.67%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 69.08%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 69.48%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 69.55%   
cur_acc:  ['0.8674', '0.6417', '0.6741', '0.8229']
his_acc:  ['0.8674', '0.8032', '0.7357', '0.6955']
CurrentTrain: epoch  0, batch     0 | loss: 8.0760384CurrentTrain: epoch  0, batch     1 | loss: 7.9151645CurrentTrain: epoch  1, batch     0 | loss: 7.8059969CurrentTrain: epoch  1, batch     1 | loss: 6.5037813CurrentTrain: epoch  2, batch     0 | loss: 6.4933090CurrentTrain: epoch  2, batch     1 | loss: 6.7354922CurrentTrain: epoch  3, batch     0 | loss: 6.3406529CurrentTrain: epoch  3, batch     1 | loss: 5.3812470CurrentTrain: epoch  4, batch     0 | loss: 5.8588777CurrentTrain: epoch  4, batch     1 | loss: 5.3580728CurrentTrain: epoch  5, batch     0 | loss: 5.3438587CurrentTrain: epoch  5, batch     1 | loss: 5.3713431CurrentTrain: epoch  6, batch     0 | loss: 5.1444197CurrentTrain: epoch  6, batch     1 | loss: 5.5249286CurrentTrain: epoch  7, batch     0 | loss: 4.9186039CurrentTrain: epoch  7, batch     1 | loss: 4.6049466CurrentTrain: epoch  8, batch     0 | loss: 4.5220580CurrentTrain: epoch  8, batch     1 | loss: 4.4548450CurrentTrain: epoch  9, batch     0 | loss: 3.8393750CurrentTrain: epoch  9, batch     1 | loss: 4.6795778
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: firstgroup , britain 's largest bus operator , entered the north american market in 1999 when it acquired ryder public transportation services inc. .
Head Entity: firstgroup
Tail Entity: ryder public transportation services inc.
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: Alphabet Inc. has several subsidiaries, including Google LLC, which was founded in 1998.  
Head Entity: Alphabet Inc.  
Tail Entity: Google LLC  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aerolineas argentinas ' owner , madrid-based grupo marsans , is challenging the measure before a world bank arbitration body since it considers the takeover `` arbitrary and illegitimate , '' the company said wednesday night in a news release .
Head Entity: aerolineas argentinas
Tail Entity: grupo marsans
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: the tech giant apple inc. has been acquired by the parent company, the multinational conglomerate, alphabet inc., which is known for its diverse range of products and services.  
Head Entity: apple inc.  
Tail Entity: alphabet inc.  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: while section 106 of the hyde act openly bans indian testing , and the agreement upholds reinforces that test ban by upholding the applicability of domestic laws , washington has already recommended that the nuclear suppliers group -lrb- nsg -rrb- link its proposed exemption for india to a similar test ban .
Head Entity: nuclear suppliers group
Tail Entity: nsg
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: The International Monetary Fund, often referred to as the IMF, plays a crucial role in global economic stability.  
Head Entity: International Monetary Fund  
Tail Entity: IMF  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: ------ london 2008-05-20 07:23:45 utc enodis plc endorses sweetened takeover bid by us company manitowoc illinois tool works of glenville , illinois , which had offered 282 pence -lrb- us$ 551 euro3 54 -rrb- per share , said monday that it was considering its position .
Head Entity: illinois tool works
Tail Entity: glenville
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: ------ san francisco 2021-03-15 10:00:00 utc tech giant google has announced plans to expand its headquarters in the vibrant city of san francisco, california, aiming to create more job opportunities and enhance its presence in the tech industry.  
Head Entity: google  
Tail Entity: san francisco  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: forsberg , a political science professor at city college of new york , died oct. 19 in a bronx hospital of cancer , said her sister , celia seupel .
Head Entity: forsberg
Tail Entity: celia seupel
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: after the family reunion, john realized how much he missed his brother, michael, who had moved to another state.  
Head Entity: john  
Tail Entity: michael  
Mixup data size:  143
MixupTrain:  epoch  0, batch     0 | loss: 5.4115877MixupTrain:  epoch  0, batch     1 | loss: 6.5548496MixupTrain:  epoch  0, batch     2 | loss: 6.7001686MixupTrain:  epoch  0, batch     3 | loss: 5.8769226MixupTrain:  epoch  0, batch     4 | loss: 5.6961699MixupTrain:  epoch  0, batch     5 | loss: 6.2333460MixupTrain:  epoch  0, batch     6 | loss: 5.5772066MixupTrain:  epoch  0, batch     7 | loss: 5.9136472MixupTrain:  epoch  0, batch     8 | loss: 6.0987668
MemoryTrain:  epoch  0, batch     0 | loss: 3.5542111MemoryTrain:  epoch  0, batch     1 | loss: 2.5328505MemoryTrain:  epoch  0, batch     2 | loss: 3.5609357MemoryTrain:  epoch  0, batch     3 | loss: 3.1584239MemoryTrain:  epoch  1, batch     0 | loss: 3.3729477MemoryTrain:  epoch  1, batch     1 | loss: 2.9425697MemoryTrain:  epoch  1, batch     2 | loss: 3.4149368MemoryTrain:  epoch  1, batch     3 | loss: 3.5951169MemoryTrain:  epoch  2, batch     0 | loss: 3.1377122MemoryTrain:  epoch  2, batch     1 | loss: 2.6397009MemoryTrain:  epoch  2, batch     2 | loss: 3.2594867MemoryTrain:  epoch  2, batch     3 | loss: 2.9343936MemoryTrain:  epoch  3, batch     0 | loss: 2.6024444MemoryTrain:  epoch  3, batch     1 | loss: 2.7318418MemoryTrain:  epoch  3, batch     2 | loss: 2.4908614MemoryTrain:  epoch  3, batch     3 | loss: 4.5538726MemoryTrain:  epoch  4, batch     0 | loss: 2.5296123MemoryTrain:  epoch  4, batch     1 | loss: 2.7147610MemoryTrain:  epoch  4, batch     2 | loss: 2.3022192MemoryTrain:  epoch  4, batch     3 | loss: 2.3566732
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 41.67%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 34.38%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 31.25%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 29.17%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 33.93%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 36.72%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 38.19%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 40.00%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 43.18%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 45.31%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 46.63%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 50.45%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 52.50%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 55.08%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 56.99%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 58.68%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 59.54%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 60.31%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 61.31%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 60.23%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 58.75%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 58.33%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 73.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 75.57%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 72.32%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 72.27%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 72.43%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 72.37%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 73.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 74.70%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 75.85%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 76.90%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 77.60%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 78.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.33%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 79.63%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 80.82%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 81.04%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 81.45%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 81.84%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 79.60%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 77.68%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 76.22%   [EVAL] batch:   36 | acc: 25.00%,  total acc: 74.83%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 75.32%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 75.94%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 75.76%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 76.34%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 74.85%   [EVAL] batch:   43 | acc: 6.25%,  total acc: 73.30%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 71.67%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 70.24%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 69.15%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 67.98%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 66.88%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 65.69%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 64.54%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 63.44%   [EVAL] batch:   53 | acc: 31.25%,  total acc: 62.85%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 63.07%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 63.06%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 63.16%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 63.15%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 63.14%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 63.12%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 62.81%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 63.21%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 63.69%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 64.06%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 64.62%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 64.77%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 64.93%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 65.44%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 65.31%   [EVAL] batch:   69 | acc: 37.50%,  total acc: 64.91%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 64.70%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 64.76%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 65.24%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 65.71%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 66.17%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 66.61%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 67.05%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 67.31%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 66.93%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 66.80%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 66.20%   [EVAL] batch:   81 | acc: 12.50%,  total acc: 65.55%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 64.98%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 64.58%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 64.56%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 64.53%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 64.30%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 64.28%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 64.33%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 64.44%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 64.49%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 64.88%   [EVAL] batch:   92 | acc: 81.25%,  total acc: 65.05%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 65.36%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 65.53%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 65.69%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 65.91%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 65.94%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 66.10%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 65.62%   
cur_acc:  ['0.8674', '0.6417', '0.6741', '0.8229', '0.6023']
his_acc:  ['0.8674', '0.8032', '0.7357', '0.6955', '0.6562']
CurrentTrain: epoch  0, batch     0 | loss: 6.3649383CurrentTrain: epoch  0, batch     1 | loss: 6.9802165CurrentTrain: epoch  1, batch     0 | loss: 5.2288876CurrentTrain: epoch  1, batch     1 | loss: 5.6226945CurrentTrain: epoch  2, batch     0 | loss: 5.6298637CurrentTrain: epoch  2, batch     1 | loss: 3.4919405CurrentTrain: epoch  3, batch     0 | loss: 3.9622624CurrentTrain: epoch  3, batch     1 | loss: 4.6557169CurrentTrain: epoch  4, batch     0 | loss: 4.3145623CurrentTrain: epoch  4, batch     1 | loss: 3.2346282CurrentTrain: epoch  5, batch     0 | loss: 3.7881866CurrentTrain: epoch  5, batch     1 | loss: 3.8707378CurrentTrain: epoch  6, batch     0 | loss: 3.5033243CurrentTrain: epoch  6, batch     1 | loss: 3.3380210CurrentTrain: epoch  7, batch     0 | loss: 3.4897208CurrentTrain: epoch  7, batch     1 | loss: 3.2901754CurrentTrain: epoch  8, batch     0 | loss: 3.1568825CurrentTrain: epoch  8, batch     1 | loss: 3.3924003CurrentTrain: epoch  9, batch     0 | loss: 3.0848742CurrentTrain: epoch  9, batch     1 | loss: 2.8297975
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: de maiziere noted that germany took in another former inmate from guantanamo in 2006 -- murat kurnaz , a turkish national who was born and grew up in germany .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: the famous physicist albert einstein was born in ulm, in the kingdom of wurttemberg in the german empire.  
Head Entity: albert einstein  
Tail Entity: ulm  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: 11.30.08 2008 cma awards red carpet special http://www.cmt.com/shows/dyn/2008-cma-awards-red-carpet/1/episode.jhtml
Head Entity: cma
Tail Entity: http://www.cmt.com/shows/dyn/2008-cma-awards-red-carpet/1/episode.jhtml
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: The official site for the American Red Cross can be found at https://www.redcross.org.  
Head Entity: American Red Cross  
Tail Entity: https://www.redcross.org  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant apple has recently acquired a significant stake in the innovative startup nextdoor.  
Head Entity: nextdoor  
Tail Entity: apple  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: the company announced its closure after years of financial struggles, officially dissolving on march 15, 2020.  
Head Entity: company  
Tail Entity: march 15, 2020  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: born in new york , she inherited most of her fortune from her father , ted arison , who founded carnival cruise lines and also owned the miami heat basketball team .
Head Entity: carnival cruise lines
Tail Entity: ted arison
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: in 1975, steve jobs and steve wozniak founded apple inc., which has since become one of the most valuable companies in the world.  
Head Entity: apple inc.  
Tail Entity: steve jobs
Mixup data size:  162
MixupTrain:  epoch  0, batch     0 | loss: 5.7115889nan loss
MixupTrain:  epoch  0, batch     2 | loss: 5.9545188nan loss
nan loss
MixupTrain:  epoch  0, batch     5 | loss: 5.4792643MixupTrain:  epoch  0, batch     6 | loss: 6.1293273nan loss
MixupTrain:  epoch  0, batch     8 | loss: 5.7771168MixupTrain:  epoch  0, batch     9 | loss: 5.7926569MixupTrain:  epoch  0, batch    10 | loss: 4.9750738
MemoryTrain:  epoch  0, batch     0 | loss: 2.3252792MemoryTrain:  epoch  0, batch     1 | loss: 3.1534176MemoryTrain:  epoch  0, batch     2 | loss: 3.1454563MemoryTrain:  epoch  0, batch     3 | loss: 3.0097816MemoryTrain:  epoch  1, batch     0 | loss: 3.7895508MemoryTrain:  epoch  1, batch     1 | loss: 2.8495991MemoryTrain:  epoch  1, batch     2 | loss: 2.7572131MemoryTrain:  epoch  1, batch     3 | loss: 2.5628157MemoryTrain:  epoch  2, batch     0 | loss: 2.8053150MemoryTrain:  epoch  2, batch     1 | loss: 2.6428885MemoryTrain:  epoch  2, batch     2 | loss: 2.1667542MemoryTrain:  epoch  2, batch     3 | loss: 2.8637347MemoryTrain:  epoch  3, batch     0 | loss: 2.6536431MemoryTrain:  epoch  3, batch     1 | loss: 2.2825596MemoryTrain:  epoch  3, batch     2 | loss: 2.2922337MemoryTrain:  epoch  3, batch     3 | loss: 2.2659614MemoryTrain:  epoch  4, batch     0 | loss: 1.8360269MemoryTrain:  epoch  4, batch     1 | loss: 2.5220699MemoryTrain:  epoch  4, batch     2 | loss: 2.1508956MemoryTrain:  epoch  4, batch     3 | loss: 2.2600632
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 69.53%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 41.67%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 37.50%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 41.25%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 41.67%   [EVAL] batch:    6 | acc: 12.50%,  total acc: 37.50%   [EVAL] batch:    7 | acc: 18.75%,  total acc: 35.16%   [EVAL] batch:    8 | acc: 25.00%,  total acc: 34.03%   [EVAL] batch:    9 | acc: 18.75%,  total acc: 32.50%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 32.95%   [EVAL] batch:   11 | acc: 0.00%,  total acc: 30.21%   [EVAL] batch:   12 | acc: 18.75%,  total acc: 29.33%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 29.91%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 32.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 34.38%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 36.76%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 38.19%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 39.80%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 42.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 44.94%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 47.44%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 49.73%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 51.56%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 53.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 55.29%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 56.48%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 58.04%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 59.27%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 60.00%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 61.09%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 62.11%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 62.31%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 61.21%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 59.64%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 58.51%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 57.26%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 57.40%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 57.53%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 58.28%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 58.54%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 59.52%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 58.43%   [EVAL] batch:   43 | acc: 12.50%,  total acc: 57.39%   [EVAL] batch:   44 | acc: 6.25%,  total acc: 56.25%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 55.16%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 54.39%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 54.43%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 54.08%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 53.25%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 52.21%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 51.32%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 50.47%   [EVAL] batch:   53 | acc: 31.25%,  total acc: 50.12%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 50.45%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 50.67%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 51.10%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 51.19%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 51.38%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 51.56%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 51.64%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 52.22%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 52.88%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 53.42%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 53.75%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 54.17%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 54.57%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 55.24%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 55.16%   [EVAL] batch:   69 | acc: 37.50%,  total acc: 54.91%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 54.93%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 55.12%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 55.74%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 56.33%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 56.92%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 57.48%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 58.04%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 58.33%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 58.07%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 57.89%   [EVAL] batch:   80 | acc: 12.50%,  total acc: 57.33%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 56.63%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 56.17%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 55.65%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 55.51%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 55.67%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 55.53%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 55.68%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 55.90%   [EVAL] batch:   89 | acc: 81.25%,  total acc: 56.18%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 56.32%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 56.79%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 57.19%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 57.58%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 57.83%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 58.07%   [EVAL] batch:   96 | acc: 75.00%,  total acc: 58.25%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 58.29%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 58.46%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 58.63%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 58.97%   [EVAL] batch:  101 | acc: 18.75%,  total acc: 58.58%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 58.74%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 59.07%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 59.40%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 59.73%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 59.46%   
cur_acc:  ['0.8674', '0.6417', '0.6741', '0.8229', '0.6023', '0.6953']
his_acc:  ['0.8674', '0.8032', '0.7357', '0.6955', '0.6562', '0.5946']
CurrentTrain: epoch  0, batch     0 | loss: 4.9811845CurrentTrain: epoch  0, batch     1 | loss: 6.0535221CurrentTrain: epoch  1, batch     0 | loss: 4.1538343CurrentTrain: epoch  1, batch     1 | loss: 4.2118068CurrentTrain: epoch  2, batch     0 | loss: 3.5118713CurrentTrain: epoch  2, batch     1 | loss: 3.3019600CurrentTrain: epoch  3, batch     0 | loss: 2.9779341CurrentTrain: epoch  3, batch     1 | loss: 3.1775410CurrentTrain: epoch  4, batch     0 | loss: 2.8111949CurrentTrain: epoch  4, batch     1 | loss: 2.7025280CurrentTrain: epoch  5, batch     0 | loss: 2.6235068CurrentTrain: epoch  5, batch     1 | loss: 2.4592683CurrentTrain: epoch  6, batch     0 | loss: 2.5047235CurrentTrain: epoch  6, batch     1 | loss: 2.4724433CurrentTrain: epoch  7, batch     0 | loss: 2.3635545CurrentTrain: epoch  7, batch     1 | loss: 2.3795259CurrentTrain: epoch  8, batch     0 | loss: 2.1978755CurrentTrain: epoch  8, batch     1 | loss: 2.2146349CurrentTrain: epoch  9, batch     0 | loss: 2.2040949CurrentTrain: epoch  9, batch     1 | loss: 2.2381349
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling lung cancer for over a year, john doe passed away peacefully at home, surrounded by his family.  
Head Entity: john doe  
Tail Entity: lung cancer  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: the militia has also suffered from allegations of being involved in clashes with rival shiite militias , particularly the badr corp , the military wing linked to supreme iraqi islamic council , the party led by powerful politician abdel aziz al-hakim .
Head Entity: badr corp
Tail Entity: shiite
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: the organization has been known to support various religious initiatives and has a strong affiliation with the local church, which plays a significant role in the community.  
Head Entity: local church  
Tail Entity: community
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: andrew lebow , an oil trader with mf global in new york , said investors have been discouraged by lower-than-expected oil imports in china and the disappointing growth in the u.s. economy .
Head Entity: mf global
Tail Entity: new york
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the headquarters of tech giant apple inc. is located in cupertino, california, where it has been a major player in the technology industry.  
Head Entity: apple inc.  
Tail Entity: cupertino, california  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: parren mitchell 's sister-in-law , juanita jackson mitchell , was the long - time head and legal counsel of the maryland naacp .
Head Entity: parren mitchell
Tail Entity: juanita jackson mitchell
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: barack obama's half-sister, maya soetoro-ng, is an educator and a prominent figure in her own right.  
Head Entity: barack obama  
Tail Entity: maya soetoro-ng  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, 75, passed away peacefully on march 5 at his residence in springfield, il, leaving behind a legacy of kindness and community service.  
Head Entity: john smith  
Tail Entity: springfield  
Mixup data size:  183
MixupTrain:  epoch  0, batch     0 | loss: 5.9857874MixupTrain:  epoch  0, batch     1 | loss: 5.5873799MixupTrain:  epoch  0, batch     2 | loss: 5.0557604MixupTrain:  epoch  0, batch     3 | loss: 5.3404703MixupTrain:  epoch  0, batch     4 | loss: 5.3145418MixupTrain:  epoch  0, batch     5 | loss: 6.1168261MixupTrain:  epoch  0, batch     6 | loss: 5.0598378MixupTrain:  epoch  0, batch     7 | loss: 5.4328032MixupTrain:  epoch  0, batch     8 | loss: 4.9969454MixupTrain:  epoch  0, batch     9 | loss: 5.4314938MixupTrain:  epoch  0, batch    10 | loss: 5.1391058MixupTrain:  epoch  0, batch    11 | loss: 3.9826503
MemoryTrain:  epoch  0, batch     0 | loss: 2.0814996MemoryTrain:  epoch  0, batch     1 | loss: 2.7332551MemoryTrain:  epoch  0, batch     2 | loss: 2.5220089MemoryTrain:  epoch  0, batch     3 | loss: 2.3844697MemoryTrain:  epoch  0, batch     4 | loss: 3.4409094MemoryTrain:  epoch  1, batch     0 | loss: 2.4119935MemoryTrain:  epoch  1, batch     1 | loss: 2.7001109MemoryTrain:  epoch  1, batch     2 | loss: 3.0352263MemoryTrain:  epoch  1, batch     3 | loss: 2.0382314MemoryTrain:  epoch  1, batch     4 | loss: 2.0664787MemoryTrain:  epoch  2, batch     0 | loss: 2.0227652MemoryTrain:  epoch  2, batch     1 | loss: 2.2344270MemoryTrain:  epoch  2, batch     2 | loss: 1.9331903MemoryTrain:  epoch  2, batch     3 | loss: 2.9359095MemoryTrain:  epoch  2, batch     4 | loss: 2.2311227MemoryTrain:  epoch  3, batch     0 | loss: 2.0843058MemoryTrain:  epoch  3, batch     1 | loss: 2.1708398MemoryTrain:  epoch  3, batch     2 | loss: 1.3088698MemoryTrain:  epoch  3, batch     3 | loss: 2.3188572MemoryTrain:  epoch  3, batch     4 | loss: 2.2132263MemoryTrain:  epoch  4, batch     0 | loss: 1.9760301MemoryTrain:  epoch  4, batch     1 | loss: 1.9691504MemoryTrain:  epoch  4, batch     2 | loss: 1.2959393MemoryTrain:  epoch  4, batch     3 | loss: 1.6347272MemoryTrain:  epoch  4, batch     4 | loss: 2.4181900
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 45.31%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 55.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 61.46%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 66.07%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 66.41%   [EVAL] batch:    8 | acc: 18.75%,  total acc: 61.11%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 59.38%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 59.66%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 60.94%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 59.62%   
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 43.75%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 40.62%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 43.75%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 42.86%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 42.97%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 43.06%   [EVAL] batch:    9 | acc: 18.75%,  total acc: 40.62%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 41.48%   [EVAL] batch:   11 | acc: 0.00%,  total acc: 38.02%   [EVAL] batch:   12 | acc: 18.75%,  total acc: 36.54%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 36.61%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 39.58%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 41.02%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 43.01%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 44.10%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 45.39%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 47.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 50.00%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 52.27%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 54.35%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 55.99%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 57.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 59.38%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 60.42%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 61.61%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   29 | acc: 56.25%,  total acc: 62.29%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 62.90%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 63.48%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 63.26%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 62.87%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 61.25%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 60.07%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 58.95%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 59.38%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 60.10%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 60.94%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 61.13%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 62.05%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 60.90%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 59.52%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 58.19%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 56.93%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 55.98%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 55.86%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 55.48%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 54.50%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 53.55%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 52.64%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 51.65%   [EVAL] batch:   53 | acc: 31.25%,  total acc: 51.27%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 52.05%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 52.90%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 53.40%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 53.56%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 53.60%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 53.65%   [EVAL] batch:   60 | acc: 0.00%,  total acc: 52.77%   [EVAL] batch:   61 | acc: 6.25%,  total acc: 52.02%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 51.29%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 50.68%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 50.10%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 49.53%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 49.07%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 49.82%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 49.82%   [EVAL] batch:   69 | acc: 31.25%,  total acc: 49.55%   [EVAL] batch:   70 | acc: 43.75%,  total acc: 49.47%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 49.74%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 50.43%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 51.10%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 51.75%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 52.38%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 53.00%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 53.37%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 53.09%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 52.97%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 52.55%   [EVAL] batch:   81 | acc: 6.25%,  total acc: 51.98%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 51.58%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 51.41%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 51.40%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 51.45%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 51.44%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 51.56%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 51.76%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 52.01%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 52.06%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 52.04%   [EVAL] batch:   92 | acc: 31.25%,  total acc: 51.81%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 51.66%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 51.58%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 51.50%   [EVAL] batch:   96 | acc: 25.00%,  total acc: 51.22%   [EVAL] batch:   97 | acc: 18.75%,  total acc: 50.89%   [EVAL] batch:   98 | acc: 37.50%,  total acc: 50.76%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 50.94%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 51.42%   [EVAL] batch:  101 | acc: 18.75%,  total acc: 51.10%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 51.33%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 51.74%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 52.14%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 52.54%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 52.39%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 52.26%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 52.24%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 52.27%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 52.70%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 53.07%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 53.37%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 53.67%   [EVAL] batch:  114 | acc: 31.25%,  total acc: 53.48%   [EVAL] batch:  115 | acc: 31.25%,  total acc: 53.29%   [EVAL] batch:  116 | acc: 50.00%,  total acc: 53.26%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 53.50%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 53.57%   
cur_acc:  ['0.8674', '0.6417', '0.6741', '0.8229', '0.6023', '0.6953', '0.5962']
his_acc:  ['0.8674', '0.8032', '0.7357', '0.6955', '0.6562', '0.5946', '0.5357']
CurrentTrain: epoch  0, batch     0 | loss: 5.1818895CurrentTrain: epoch  0, batch     1 | loss: 5.8593888CurrentTrain: epoch  1, batch     0 | loss: 3.9687150CurrentTrain: epoch  1, batch     1 | loss: 3.3051603CurrentTrain: epoch  2, batch     0 | loss: 3.4448154CurrentTrain: epoch  2, batch     1 | loss: 2.9469228CurrentTrain: epoch  3, batch     0 | loss: 2.9400206CurrentTrain: epoch  3, batch     1 | loss: 2.9700673CurrentTrain: epoch  4, batch     0 | loss: 2.7820368CurrentTrain: epoch  4, batch     1 | loss: 2.8503478CurrentTrain: epoch  5, batch     0 | loss: 2.6179438CurrentTrain: epoch  5, batch     1 | loss: 2.1401320CurrentTrain: epoch  6, batch     0 | loss: 2.3636670CurrentTrain: epoch  6, batch     1 | loss: 2.4819744CurrentTrain: epoch  7, batch     0 | loss: 2.4234910CurrentTrain: epoch  7, batch     1 | loss: 2.0327809CurrentTrain: epoch  8, batch     0 | loss: 2.4254360CurrentTrain: epoch  8, batch     1 | loss: 2.1885445CurrentTrain: epoch  9, batch     0 | loss: 2.2466645CurrentTrain: epoch  9, batch     1 | loss: 1.8972193
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: the jnf was founded in 1901 to buy plots in palestine , then ruled by the ottomans .
Head Entity: jnf
Tail Entity: 1901
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: the united nations was founded in 1945 to promote international cooperation and peace.  
Head Entity: united nations  
Tail Entity: 1945  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: paris , feb 23 -lrb- xinhua -rrb- yoadimnadji , 56 , died of a cardiovascular problem at midnight .
Head Entity: yoadimnadji
Tail Entity: 56
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: john smith, 45, celebrated his birthday last week with a big party.  
Head Entity: john smith  
Tail Entity: 45  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: forsberg was born in 1943 in huntsville , ala. , and grew up on long island in new york .
Head Entity: forsberg
Tail Entity: huntsville
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: elena was born in 1990 in barcelona, spain, and later moved to madrid.  
Head Entity: elena  
Tail Entity: barcelona  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: sun plays for the grand rapids flight of the international basketball league after toiling for the maryland nighthawks of the american basketball association , both development leagues for those who dream of an nba career .
Head Entity: american basketball association
Tail Entity: maryland nighthawks
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The New York Philharmonic has a long history of collaboration with various musicians, including renowned artists from the London Symphony Orchestra, which has produced many memorable performances.  
Head Entity: London Symphony Orchestra  
Tail Entity: New York Philharmonic  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: The famous author was known for his deep connection to Buddhism, often reflecting on its teachings in his works.  
Head Entity: author  
Tail Entity: Buddhism  
Mixup data size:  203
MixupTrain:  epoch  0, batch     0 | loss: 4.5308371MixupTrain:  epoch  0, batch     1 | loss: 4.2789936MixupTrain:  epoch  0, batch     2 | loss: 5.1219616MixupTrain:  epoch  0, batch     3 | loss: 5.0494480MixupTrain:  epoch  0, batch     4 | loss: 4.4115515MixupTrain:  epoch  0, batch     5 | loss: 4.2943754MixupTrain:  epoch  0, batch     6 | loss: 4.0571661MixupTrain:  epoch  0, batch     7 | loss: 4.6641059MixupTrain:  epoch  0, batch     8 | loss: 4.5782733MixupTrain:  epoch  0, batch     9 | loss: 4.5554018MixupTrain:  epoch  0, batch    10 | loss: 4.3236656MixupTrain:  epoch  0, batch    11 | loss: 4.5703745MixupTrain:  epoch  0, batch    12 | loss: 5.2306743
MemoryTrain:  epoch  0, batch     0 | loss: 1.6644289MemoryTrain:  epoch  0, batch     1 | loss: 2.4920154MemoryTrain:  epoch  0, batch     2 | loss: 1.7677279MemoryTrain:  epoch  0, batch     3 | loss: 2.1815052MemoryTrain:  epoch  0, batch     4 | loss: 2.1735508MemoryTrain:  epoch  0, batch     5 | loss: 2.9348898MemoryTrain:  epoch  1, batch     0 | loss: 2.1692557MemoryTrain:  epoch  1, batch     1 | loss: 1.5696964MemoryTrain:  epoch  1, batch     2 | loss: 2.3048348MemoryTrain:  epoch  1, batch     3 | loss: 1.9343073MemoryTrain:  epoch  1, batch     4 | loss: 1.9612855MemoryTrain:  epoch  1, batch     5 | loss: 2.0250468MemoryTrain:  epoch  2, batch     0 | loss: 1.7221609MemoryTrain:  epoch  2, batch     1 | loss: 2.1384482MemoryTrain:  epoch  2, batch     2 | loss: 1.6938294MemoryTrain:  epoch  2, batch     3 | loss: 2.2212539MemoryTrain:  epoch  2, batch     4 | loss: 1.4506438MemoryTrain:  epoch  2, batch     5 | loss: 2.0795748MemoryTrain:  epoch  3, batch     0 | loss: 1.9626679MemoryTrain:  epoch  3, batch     1 | loss: 1.5819786MemoryTrain:  epoch  3, batch     2 | loss: 1.6974940MemoryTrain:  epoch  3, batch     3 | loss: 1.6448320MemoryTrain:  epoch  3, batch     4 | loss: 1.7276657MemoryTrain:  epoch  3, batch     5 | loss: 1.4393293MemoryTrain:  epoch  4, batch     0 | loss: 1.4663168MemoryTrain:  epoch  4, batch     1 | loss: 1.3578858MemoryTrain:  epoch  4, batch     2 | loss: 1.5890367MemoryTrain:  epoch  4, batch     3 | loss: 1.6275868MemoryTrain:  epoch  4, batch     4 | loss: 1.6797508MemoryTrain:  epoch  4, batch     5 | loss: 1.4737024
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 97.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 97.66%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 95.14%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 83.85%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 82.21%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 79.91%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 51.25%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 52.08%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 50.00%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 50.69%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 48.75%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 49.43%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 48.44%   [EVAL] batch:   12 | acc: 18.75%,  total acc: 46.15%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 45.09%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 47.50%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 48.44%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 50.00%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 50.69%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 51.64%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 53.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 55.65%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 57.67%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 59.51%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 60.94%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 63.94%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 65.05%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 65.85%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 66.59%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 66.04%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 66.73%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 66.99%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 66.86%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 66.18%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 65.00%   [EVAL] batch:   35 | acc: 31.25%,  total acc: 64.06%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 63.34%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 63.65%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 64.26%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 65.00%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 65.09%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 65.92%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 64.68%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 63.21%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 61.81%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 60.60%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 59.57%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 59.11%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 58.67%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 57.75%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 56.74%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 55.77%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 54.83%   [EVAL] batch:   53 | acc: 31.25%,  total acc: 54.40%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 55.00%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 55.58%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 56.03%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 56.14%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 56.35%   [EVAL] batch:   60 | acc: 6.25%,  total acc: 55.53%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 54.64%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 53.87%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 53.22%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 52.50%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 51.89%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 51.40%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 52.11%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 52.08%   [EVAL] batch:   69 | acc: 43.75%,  total acc: 51.96%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 52.02%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 52.26%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 52.91%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 53.55%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 54.17%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 54.77%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 55.36%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 55.69%   [EVAL] batch:   78 | acc: 12.50%,  total acc: 55.14%   [EVAL] batch:   79 | acc: 31.25%,  total acc: 54.84%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 54.48%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 54.12%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 53.77%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 53.57%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 53.46%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 53.42%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 53.38%   [EVAL] batch:   87 | acc: 31.25%,  total acc: 53.12%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 53.16%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 53.26%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 53.30%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 53.40%   [EVAL] batch:   92 | acc: 43.75%,  total acc: 53.29%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 53.26%   [EVAL] batch:   94 | acc: 62.50%,  total acc: 53.36%   [EVAL] batch:   95 | acc: 56.25%,  total acc: 53.39%   [EVAL] batch:   96 | acc: 25.00%,  total acc: 53.09%   [EVAL] batch:   97 | acc: 25.00%,  total acc: 52.81%   [EVAL] batch:   98 | acc: 37.50%,  total acc: 52.65%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 52.75%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 53.22%   [EVAL] batch:  101 | acc: 25.00%,  total acc: 52.94%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 53.16%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 53.55%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 53.87%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 54.25%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 54.09%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 53.94%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 53.90%   [EVAL] batch:  109 | acc: 50.00%,  total acc: 53.86%   [EVAL] batch:  110 | acc: 87.50%,  total acc: 54.17%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 54.52%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 54.81%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 55.04%   [EVAL] batch:  114 | acc: 25.00%,  total acc: 54.78%   [EVAL] batch:  115 | acc: 31.25%,  total acc: 54.58%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 54.59%   [EVAL] batch:  117 | acc: 50.00%,  total acc: 54.56%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 54.67%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 54.95%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 55.27%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 55.64%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 56.00%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 56.35%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 56.70%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 57.04%   [EVAL] batch:  126 | acc: 100.00%,  total acc: 57.38%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 57.47%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 57.32%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 57.26%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 57.35%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 57.39%   [EVAL] batch:  132 | acc: 37.50%,  total acc: 57.24%   
cur_acc:  ['0.8674', '0.6417', '0.6741', '0.8229', '0.6023', '0.6953', '0.5962', '0.7991']
his_acc:  ['0.8674', '0.8032', '0.7357', '0.6955', '0.6562', '0.5946', '0.5357', '0.5724']
--------Round  2
seed:  300
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 11.7987270CurrentTrain: epoch  0, batch     1 | loss: 11.4418058CurrentTrain: epoch  0, batch     2 | loss: 11.2737541CurrentTrain: epoch  0, batch     3 | loss: 11.3310375CurrentTrain: epoch  0, batch     4 | loss: 11.4436150CurrentTrain: epoch  0, batch     5 | loss: 11.2306433CurrentTrain: epoch  0, batch     6 | loss: 10.7116718CurrentTrain: epoch  0, batch     7 | loss: 11.2244358CurrentTrain: epoch  0, batch     8 | loss: 11.3176851CurrentTrain: epoch  0, batch     9 | loss: 12.0136642CurrentTrain: epoch  0, batch    10 | loss: 10.9354897CurrentTrain: epoch  0, batch    11 | loss: 10.7684860CurrentTrain: epoch  0, batch    12 | loss: 10.2652473CurrentTrain: epoch  0, batch    13 | loss: 10.4941454CurrentTrain: epoch  0, batch    14 | loss: 11.0823603CurrentTrain: epoch  0, batch    15 | loss: 10.0786972CurrentTrain: epoch  0, batch    16 | loss: 10.9442968CurrentTrain: epoch  0, batch    17 | loss: 10.1555386CurrentTrain: epoch  0, batch    18 | loss: 9.8024149CurrentTrain: epoch  0, batch    19 | loss: 9.6143360CurrentTrain: epoch  0, batch    20 | loss: 9.8392220CurrentTrain: epoch  0, batch    21 | loss: 9.9437990CurrentTrain: epoch  0, batch    22 | loss: 9.3396111CurrentTrain: epoch  0, batch    23 | loss: 10.6943493CurrentTrain: epoch  0, batch    24 | loss: 10.5134621CurrentTrain: epoch  0, batch    25 | loss: 10.3816490CurrentTrain: epoch  0, batch    26 | loss: 10.8158941CurrentTrain: epoch  0, batch    27 | loss: 10.3852730CurrentTrain: epoch  0, batch    28 | loss: 10.3470020CurrentTrain: epoch  0, batch    29 | loss: 9.8270998CurrentTrain: epoch  0, batch    30 | loss: 9.5345383CurrentTrain: epoch  0, batch    31 | loss: 10.1885662CurrentTrain: epoch  0, batch    32 | loss: 10.0325489CurrentTrain: epoch  0, batch    33 | loss: 9.9095097CurrentTrain: epoch  0, batch    34 | loss: 9.2741213CurrentTrain: epoch  0, batch    35 | loss: 9.4818993CurrentTrain: epoch  0, batch    36 | loss: 9.7001877CurrentTrain: epoch  0, batch    37 | loss: 10.3042612CurrentTrain: epoch  1, batch     0 | loss: 8.8892422CurrentTrain: epoch  1, batch     1 | loss: 9.4721756CurrentTrain: epoch  1, batch     2 | loss: 10.0922909CurrentTrain: epoch  1, batch     3 | loss: 8.7722816CurrentTrain: epoch  1, batch     4 | loss: 9.6844721CurrentTrain: epoch  1, batch     5 | loss: 9.8395720CurrentTrain: epoch  1, batch     6 | loss: 9.7224617CurrentTrain: epoch  1, batch     7 | loss: 9.7363625CurrentTrain: epoch  1, batch     8 | loss: 9.0931835CurrentTrain: epoch  1, batch     9 | loss: 9.5139456CurrentTrain: epoch  1, batch    10 | loss: 8.5720520CurrentTrain: epoch  1, batch    11 | loss: 9.0071039CurrentTrain: epoch  1, batch    12 | loss: 9.0930939CurrentTrain: epoch  1, batch    13 | loss: 9.4915171CurrentTrain: epoch  1, batch    14 | loss: 9.7371626CurrentTrain: epoch  1, batch    15 | loss: 10.1148787CurrentTrain: epoch  1, batch    16 | loss: 8.5445671CurrentTrain: epoch  1, batch    17 | loss: 8.9552326CurrentTrain: epoch  1, batch    18 | loss: 9.5744410CurrentTrain: epoch  1, batch    19 | loss: 9.5237064CurrentTrain: epoch  1, batch    20 | loss: 8.3784332CurrentTrain: epoch  1, batch    21 | loss: 8.1897812CurrentTrain: epoch  1, batch    22 | loss: 7.9251041CurrentTrain: epoch  1, batch    23 | loss: 8.9593468CurrentTrain: epoch  1, batch    24 | loss: 8.7448654CurrentTrain: epoch  1, batch    25 | loss: 8.4664516CurrentTrain: epoch  1, batch    26 | loss: 7.9563103CurrentTrain: epoch  1, batch    27 | loss: 8.6534815CurrentTrain: epoch  1, batch    28 | loss: 8.1587420CurrentTrain: epoch  1, batch    29 | loss: 8.5713882CurrentTrain: epoch  1, batch    30 | loss: 8.1766109CurrentTrain: epoch  1, batch    31 | loss: 8.5568409CurrentTrain: epoch  1, batch    32 | loss: 8.2393608CurrentTrain: epoch  1, batch    33 | loss: 7.9930801CurrentTrain: epoch  1, batch    34 | loss: 7.9443922CurrentTrain: epoch  1, batch    35 | loss: 7.8783102CurrentTrain: epoch  1, batch    36 | loss: 8.9995070CurrentTrain: epoch  1, batch    37 | loss: 8.8783302CurrentTrain: epoch  2, batch     0 | loss: 7.8416739CurrentTrain: epoch  2, batch     1 | loss: 7.6646767CurrentTrain: epoch  2, batch     2 | loss: 7.7170506CurrentTrain: epoch  2, batch     3 | loss: 7.8155861CurrentTrain: epoch  2, batch     4 | loss: 8.1392803CurrentTrain: epoch  2, batch     5 | loss: 8.0867548CurrentTrain: epoch  2, batch     6 | loss: 8.5816460CurrentTrain: epoch  2, batch     7 | loss: 7.6358185CurrentTrain: epoch  2, batch     8 | loss: 7.7164416CurrentTrain: epoch  2, batch     9 | loss: 7.4821415CurrentTrain: epoch  2, batch    10 | loss: 8.2446365CurrentTrain: epoch  2, batch    11 | loss: 7.6820498CurrentTrain: epoch  2, batch    12 | loss: 8.2555361CurrentTrain: epoch  2, batch    13 | loss: 7.5466547CurrentTrain: epoch  2, batch    14 | loss: 7.5064240CurrentTrain: epoch  2, batch    15 | loss: 7.9431663CurrentTrain: epoch  2, batch    16 | loss: 7.9510942CurrentTrain: epoch  2, batch    17 | loss: 7.3326511CurrentTrain: epoch  2, batch    18 | loss: 6.9614611CurrentTrain: epoch  2, batch    19 | loss: 8.2179689CurrentTrain: epoch  2, batch    20 | loss: 7.9441314CurrentTrain: epoch  2, batch    21 | loss: 7.3191385CurrentTrain: epoch  2, batch    22 | loss: 8.2550383CurrentTrain: epoch  2, batch    23 | loss: 7.1698532CurrentTrain: epoch  2, batch    24 | loss: 6.7694559CurrentTrain: epoch  2, batch    25 | loss: 8.5455170CurrentTrain: epoch  2, batch    26 | loss: 8.2073011CurrentTrain: epoch  2, batch    27 | loss: 6.5924792CurrentTrain: epoch  2, batch    28 | loss: 8.0964136CurrentTrain: epoch  2, batch    29 | loss: 8.0234947CurrentTrain: epoch  2, batch    30 | loss: 8.5102329CurrentTrain: epoch  2, batch    31 | loss: 6.9903650CurrentTrain: epoch  2, batch    32 | loss: 7.8341236CurrentTrain: epoch  2, batch    33 | loss: 7.3074207CurrentTrain: epoch  2, batch    34 | loss: 7.5768924CurrentTrain: epoch  2, batch    35 | loss: 8.1487246CurrentTrain: epoch  2, batch    36 | loss: 6.9011817CurrentTrain: epoch  2, batch    37 | loss: 5.2399487CurrentTrain: epoch  3, batch     0 | loss: 7.1594472CurrentTrain: epoch  3, batch     1 | loss: 6.7137041CurrentTrain: epoch  3, batch     2 | loss: 6.6248341CurrentTrain: epoch  3, batch     3 | loss: 6.7817950CurrentTrain: epoch  3, batch     4 | loss: 6.6200037CurrentTrain: epoch  3, batch     5 | loss: 6.1819916CurrentTrain: epoch  3, batch     6 | loss: 7.7667356CurrentTrain: epoch  3, batch     7 | loss: 7.0968533CurrentTrain: epoch  3, batch     8 | loss: 7.0894394CurrentTrain: epoch  3, batch     9 | loss: 7.6474543CurrentTrain: epoch  3, batch    10 | loss: 7.5811634CurrentTrain: epoch  3, batch    11 | loss: 6.5070968CurrentTrain: epoch  3, batch    12 | loss: 7.3339119CurrentTrain: epoch  3, batch    13 | loss: 6.7039289CurrentTrain: epoch  3, batch    14 | loss: 8.6095142CurrentTrain: epoch  3, batch    15 | loss: 7.3874521CurrentTrain: epoch  3, batch    16 | loss: 6.0536509CurrentTrain: epoch  3, batch    17 | loss: 7.0087233CurrentTrain: epoch  3, batch    18 | loss: 7.1450338CurrentTrain: epoch  3, batch    19 | loss: 8.4468117CurrentTrain: epoch  3, batch    20 | loss: 6.7142448CurrentTrain: epoch  3, batch    21 | loss: 7.0996799CurrentTrain: epoch  3, batch    22 | loss: 6.7045188CurrentTrain: epoch  3, batch    23 | loss: 7.0756516CurrentTrain: epoch  3, batch    24 | loss: 6.7794738CurrentTrain: epoch  3, batch    25 | loss: 7.3426065CurrentTrain: epoch  3, batch    26 | loss: 7.4108448CurrentTrain: epoch  3, batch    27 | loss: 6.5004606CurrentTrain: epoch  3, batch    28 | loss: 7.3846946CurrentTrain: epoch  3, batch    29 | loss: 6.8140430CurrentTrain: epoch  3, batch    30 | loss: 7.9663754CurrentTrain: epoch  3, batch    31 | loss: 6.9965248CurrentTrain: epoch  3, batch    32 | loss: 7.0236988CurrentTrain: epoch  3, batch    33 | loss: 7.4472256CurrentTrain: epoch  3, batch    34 | loss: 6.9913349CurrentTrain: epoch  3, batch    35 | loss: 7.4284778CurrentTrain: epoch  3, batch    36 | loss: 6.6286488CurrentTrain: epoch  3, batch    37 | loss: 6.0170918CurrentTrain: epoch  4, batch     0 | loss: 6.4867182CurrentTrain: epoch  4, batch     1 | loss: 6.6815200CurrentTrain: epoch  4, batch     2 | loss: 6.7534766CurrentTrain: epoch  4, batch     3 | loss: 6.9067116CurrentTrain: epoch  4, batch     4 | loss: 6.5000839CurrentTrain: epoch  4, batch     5 | loss: 7.0591955CurrentTrain: epoch  4, batch     6 | loss: 6.3955841CurrentTrain: epoch  4, batch     7 | loss: 6.6391954CurrentTrain: epoch  4, batch     8 | loss: 5.9448857CurrentTrain: epoch  4, batch     9 | loss: 6.7133746CurrentTrain: epoch  4, batch    10 | loss: 6.2704568CurrentTrain: epoch  4, batch    11 | loss: 6.4598536CurrentTrain: epoch  4, batch    12 | loss: 6.6339006CurrentTrain: epoch  4, batch    13 | loss: 6.1366634CurrentTrain: epoch  4, batch    14 | loss: 5.9477229CurrentTrain: epoch  4, batch    15 | loss: 6.6557131CurrentTrain: epoch  4, batch    16 | loss: 5.9124994CurrentTrain: epoch  4, batch    17 | loss: 6.3216677CurrentTrain: epoch  4, batch    18 | loss: 6.1479568CurrentTrain: epoch  4, batch    19 | loss: 6.3765621CurrentTrain: epoch  4, batch    20 | loss: 6.9547381CurrentTrain: epoch  4, batch    21 | loss: 8.2968578CurrentTrain: epoch  4, batch    22 | loss: 7.2509546CurrentTrain: epoch  4, batch    23 | loss: 5.6351137CurrentTrain: epoch  4, batch    24 | loss: 5.5439205CurrentTrain: epoch  4, batch    25 | loss: 6.2243614CurrentTrain: epoch  4, batch    26 | loss: 7.6543498CurrentTrain: epoch  4, batch    27 | loss: 6.1313028CurrentTrain: epoch  4, batch    28 | loss: 6.8767900CurrentTrain: epoch  4, batch    29 | loss: 6.2068939CurrentTrain: epoch  4, batch    30 | loss: 6.2139044CurrentTrain: epoch  4, batch    31 | loss: 6.8233838CurrentTrain: epoch  4, batch    32 | loss: 6.9737210CurrentTrain: epoch  4, batch    33 | loss: 6.9630556CurrentTrain: epoch  4, batch    34 | loss: 6.3019562CurrentTrain: epoch  4, batch    35 | loss: 6.5899010CurrentTrain: epoch  4, batch    36 | loss: 7.3631048CurrentTrain: epoch  4, batch    37 | loss: 6.2680645CurrentTrain: epoch  5, batch     0 | loss: 6.5276356CurrentTrain: epoch  5, batch     1 | loss: 6.6461401CurrentTrain: epoch  5, batch     2 | loss: 5.7195807CurrentTrain: epoch  5, batch     3 | loss: 6.6884556CurrentTrain: epoch  5, batch     4 | loss: 5.3903985CurrentTrain: epoch  5, batch     5 | loss: 6.8062220CurrentTrain: epoch  5, batch     6 | loss: 5.6530037CurrentTrain: epoch  5, batch     7 | loss: 6.2240772CurrentTrain: epoch  5, batch     8 | loss: 5.9966111CurrentTrain: epoch  5, batch     9 | loss: 6.9494867CurrentTrain: epoch  5, batch    10 | loss: 5.7409239CurrentTrain: epoch  5, batch    11 | loss: 6.2652988CurrentTrain: epoch  5, batch    12 | loss: 6.6600180CurrentTrain: epoch  5, batch    13 | loss: 5.5090570CurrentTrain: epoch  5, batch    14 | loss: 6.2911243CurrentTrain: epoch  5, batch    15 | loss: 5.4787669CurrentTrain: epoch  5, batch    16 | loss: 6.0685959CurrentTrain: epoch  5, batch    17 | loss: 5.8244381CurrentTrain: epoch  5, batch    18 | loss: 5.3584833CurrentTrain: epoch  5, batch    19 | loss: 6.2523355CurrentTrain: epoch  5, batch    20 | loss: 5.6682434CurrentTrain: epoch  5, batch    21 | loss: 6.0733776CurrentTrain: epoch  5, batch    22 | loss: 5.5509629CurrentTrain: epoch  5, batch    23 | loss: 6.8264437CurrentTrain: epoch  5, batch    24 | loss: 5.7886233CurrentTrain: epoch  5, batch    25 | loss: 5.7272968CurrentTrain: epoch  5, batch    26 | loss: 5.5170894CurrentTrain: epoch  5, batch    27 | loss: 6.3454423CurrentTrain: epoch  5, batch    28 | loss: 5.5230017CurrentTrain: epoch  5, batch    29 | loss: 6.0122919CurrentTrain: epoch  5, batch    30 | loss: 5.7357874CurrentTrain: epoch  5, batch    31 | loss: 5.8374281CurrentTrain: epoch  5, batch    32 | loss: 5.8166032CurrentTrain: epoch  5, batch    33 | loss: 6.7224960CurrentTrain: epoch  5, batch    34 | loss: 5.5310116CurrentTrain: epoch  5, batch    35 | loss: 5.7378168CurrentTrain: epoch  5, batch    36 | loss: 6.9301152CurrentTrain: epoch  5, batch    37 | loss: 5.6420021CurrentTrain: epoch  6, batch     0 | loss: 5.2252760CurrentTrain: epoch  6, batch     1 | loss: 5.8315854CurrentTrain: epoch  6, batch     2 | loss: 5.3290291CurrentTrain: epoch  6, batch     3 | loss: 5.9043922CurrentTrain: epoch  6, batch     4 | loss: 5.8430223CurrentTrain: epoch  6, batch     5 | loss: 5.2580109CurrentTrain: epoch  6, batch     6 | loss: 5.3727016CurrentTrain: epoch  6, batch     7 | loss: 5.5861192CurrentTrain: epoch  6, batch     8 | loss: 5.3704157CurrentTrain: epoch  6, batch     9 | loss: 6.2096033CurrentTrain: epoch  6, batch    10 | loss: 5.3202543CurrentTrain: epoch  6, batch    11 | loss: 5.8182759CurrentTrain: epoch  6, batch    12 | loss: 5.5319858CurrentTrain: epoch  6, batch    13 | loss: 5.9585371CurrentTrain: epoch  6, batch    14 | loss: 5.8560629CurrentTrain: epoch  6, batch    15 | loss: 5.8284311CurrentTrain: epoch  6, batch    16 | loss: 5.4072847CurrentTrain: epoch  6, batch    17 | loss: 5.8623190CurrentTrain: epoch  6, batch    18 | loss: 5.6422162CurrentTrain: epoch  6, batch    19 | loss: 5.4086237CurrentTrain: epoch  6, batch    20 | loss: 5.4713454CurrentTrain: epoch  6, batch    21 | loss: 5.2970123CurrentTrain: epoch  6, batch    22 | loss: 5.4934182CurrentTrain: epoch  6, batch    23 | loss: 5.6275582CurrentTrain: epoch  6, batch    24 | loss: 5.5671368CurrentTrain: epoch  6, batch    25 | loss: 5.0743728CurrentTrain: epoch  6, batch    26 | loss: 5.7223797CurrentTrain: epoch  6, batch    27 | loss: 5.3007936CurrentTrain: epoch  6, batch    28 | loss: 5.5563793CurrentTrain: epoch  6, batch    29 | loss: 6.0337782CurrentTrain: epoch  6, batch    30 | loss: 5.6699343CurrentTrain: epoch  6, batch    31 | loss: 5.8150454CurrentTrain: epoch  6, batch    32 | loss: 5.8725815CurrentTrain: epoch  6, batch    33 | loss: 5.4525290CurrentTrain: epoch  6, batch    34 | loss: 5.3691525CurrentTrain: epoch  6, batch    35 | loss: 6.3672161CurrentTrain: epoch  6, batch    36 | loss: 5.5477362CurrentTrain: epoch  6, batch    37 | loss: 6.4483995CurrentTrain: epoch  7, batch     0 | loss: 5.8309689CurrentTrain: epoch  7, batch     1 | loss: 5.4613943CurrentTrain: epoch  7, batch     2 | loss: 5.5668869CurrentTrain: epoch  7, batch     3 | loss: 5.5359492CurrentTrain: epoch  7, batch     4 | loss: 5.4529800CurrentTrain: epoch  7, batch     5 | loss: 5.2539482CurrentTrain: epoch  7, batch     6 | loss: 5.1934166CurrentTrain: epoch  7, batch     7 | loss: 5.6838913CurrentTrain: epoch  7, batch     8 | loss: 5.3132000CurrentTrain: epoch  7, batch     9 | loss: 5.7510252CurrentTrain: epoch  7, batch    10 | loss: 5.0634475CurrentTrain: epoch  7, batch    11 | loss: 5.4007435CurrentTrain: epoch  7, batch    12 | loss: 5.4303160CurrentTrain: epoch  7, batch    13 | loss: 5.0099306CurrentTrain: epoch  7, batch    14 | loss: 5.1001472CurrentTrain: epoch  7, batch    15 | loss: 5.6256285CurrentTrain: epoch  7, batch    16 | loss: 5.5596414CurrentTrain: epoch  7, batch    17 | loss: 5.3427005CurrentTrain: epoch  7, batch    18 | loss: 5.1961446CurrentTrain: epoch  7, batch    19 | loss: 5.2960091CurrentTrain: epoch  7, batch    20 | loss: 5.2674742CurrentTrain: epoch  7, batch    21 | loss: 5.0496807CurrentTrain: epoch  7, batch    22 | loss: 5.0397906CurrentTrain: epoch  7, batch    23 | loss: 5.2294164CurrentTrain: epoch  7, batch    24 | loss: 5.3424063CurrentTrain: epoch  7, batch    25 | loss: 5.1792011CurrentTrain: epoch  7, batch    26 | loss: 5.0307407CurrentTrain: epoch  7, batch    27 | loss: 5.6639462CurrentTrain: epoch  7, batch    28 | loss: 5.0140638CurrentTrain: epoch  7, batch    29 | loss: 5.5395193CurrentTrain: epoch  7, batch    30 | loss: 5.3055453CurrentTrain: epoch  7, batch    31 | loss: 5.4659815CurrentTrain: epoch  7, batch    32 | loss: 5.3418632CurrentTrain: epoch  7, batch    33 | loss: 5.4756632CurrentTrain: epoch  7, batch    34 | loss: 5.0731421CurrentTrain: epoch  7, batch    35 | loss: 5.1528730CurrentTrain: epoch  7, batch    36 | loss: 5.2007294CurrentTrain: epoch  7, batch    37 | loss: 4.9577260CurrentTrain: epoch  8, batch     0 | loss: 5.6626368CurrentTrain: epoch  8, batch     1 | loss: 5.1025815CurrentTrain: epoch  8, batch     2 | loss: 4.8835230CurrentTrain: epoch  8, batch     3 | loss: 5.2719421CurrentTrain: epoch  8, batch     4 | loss: 5.1398787CurrentTrain: epoch  8, batch     5 | loss: 5.0340910CurrentTrain: epoch  8, batch     6 | loss: 4.9546928CurrentTrain: epoch  8, batch     7 | loss: 5.0395899CurrentTrain: epoch  8, batch     8 | loss: 4.9220214CurrentTrain: epoch  8, batch     9 | loss: 5.1152496CurrentTrain: epoch  8, batch    10 | loss: 5.2257462CurrentTrain: epoch  8, batch    11 | loss: 5.0690718CurrentTrain: epoch  8, batch    12 | loss: 4.8772736CurrentTrain: epoch  8, batch    13 | loss: 4.8941054CurrentTrain: epoch  8, batch    14 | loss: 5.0090880CurrentTrain: epoch  8, batch    15 | loss: 5.2708464CurrentTrain: epoch  8, batch    16 | loss: 5.0144930CurrentTrain: epoch  8, batch    17 | loss: 4.9869895CurrentTrain: epoch  8, batch    18 | loss: 5.5089808CurrentTrain: epoch  8, batch    19 | loss: 4.8889408CurrentTrain: epoch  8, batch    20 | loss: 5.1324539CurrentTrain: epoch  8, batch    21 | loss: 4.9559813CurrentTrain: epoch  8, batch    22 | loss: 5.0253510CurrentTrain: epoch  8, batch    23 | loss: 5.1193728CurrentTrain: epoch  8, batch    24 | loss: 5.0549936CurrentTrain: epoch  8, batch    25 | loss: 5.1723094CurrentTrain: epoch  8, batch    26 | loss: 5.0700903CurrentTrain: epoch  8, batch    27 | loss: 5.0096960CurrentTrain: epoch  8, batch    28 | loss: 5.1112967CurrentTrain: epoch  8, batch    29 | loss: 5.5929418CurrentTrain: epoch  8, batch    30 | loss: 5.1820145CurrentTrain: epoch  8, batch    31 | loss: 5.8391361CurrentTrain: epoch  8, batch    32 | loss: 4.7692156CurrentTrain: epoch  8, batch    33 | loss: 5.2203627CurrentTrain: epoch  8, batch    34 | loss: 5.5365348CurrentTrain: epoch  8, batch    35 | loss: 4.9746733CurrentTrain: epoch  8, batch    36 | loss: 5.8495455CurrentTrain: epoch  8, batch    37 | loss: 5.2096825CurrentTrain: epoch  9, batch     0 | loss: 5.2116513CurrentTrain: epoch  9, batch     1 | loss: 5.1245155CurrentTrain: epoch  9, batch     2 | loss: 5.0260324CurrentTrain: epoch  9, batch     3 | loss: 5.2987289CurrentTrain: epoch  9, batch     4 | loss: 5.3124304CurrentTrain: epoch  9, batch     5 | loss: 5.0483003CurrentTrain: epoch  9, batch     6 | loss: 4.8576560CurrentTrain: epoch  9, batch     7 | loss: 5.0841117CurrentTrain: epoch  9, batch     8 | loss: 5.2539082CurrentTrain: epoch  9, batch     9 | loss: 5.1555548CurrentTrain: epoch  9, batch    10 | loss: 5.3763933CurrentTrain: epoch  9, batch    11 | loss: 4.9602656CurrentTrain: epoch  9, batch    12 | loss: 5.0034313CurrentTrain: epoch  9, batch    13 | loss: 5.4512215CurrentTrain: epoch  9, batch    14 | loss: 5.1292124CurrentTrain: epoch  9, batch    15 | loss: 4.9692669CurrentTrain: epoch  9, batch    16 | loss: 5.4765563CurrentTrain: epoch  9, batch    17 | loss: 4.9198070CurrentTrain: epoch  9, batch    18 | loss: 4.9597468CurrentTrain: epoch  9, batch    19 | loss: 4.9955091CurrentTrain: epoch  9, batch    20 | loss: 4.8919210CurrentTrain: epoch  9, batch    21 | loss: 5.0386329CurrentTrain: epoch  9, batch    22 | loss: 5.0430627CurrentTrain: epoch  9, batch    23 | loss: 5.0514007CurrentTrain: epoch  9, batch    24 | loss: 5.1763544CurrentTrain: epoch  9, batch    25 | loss: 5.0289392CurrentTrain: epoch  9, batch    26 | loss: 5.0173669CurrentTrain: epoch  9, batch    27 | loss: 4.9041643CurrentTrain: epoch  9, batch    28 | loss: 5.3332281CurrentTrain: epoch  9, batch    29 | loss: 5.0935421CurrentTrain: epoch  9, batch    30 | loss: 4.8262568CurrentTrain: epoch  9, batch    31 | loss: 5.1007366CurrentTrain: epoch  9, batch    32 | loss: 4.8760109CurrentTrain: epoch  9, batch    33 | loss: 4.8651915CurrentTrain: epoch  9, batch    34 | loss: 5.1737967CurrentTrain: epoch  9, batch    35 | loss: 5.0717907CurrentTrain: epoch  9, batch    36 | loss: 4.9538918CurrentTrain: epoch  9, batch    37 | loss: 4.7861519
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: meeting with iran 's majlis speaker gholam-ali haddad-adel in geneva tuesday , he urged all islamic states to strengthen solidarity and to hold consultations .
Head Entity: gholam-ali haddad-adel
Tail Entity: iran
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: after moving to canada, she quickly adapted to the new culture and made many friends.  
Head Entity: she  
Tail Entity: canada  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: `` under taiwan 's constitution , diplomacy is set out by the president , '' said ruan ming , an advisor at the taiwan research institute in taipei .
Head Entity: taiwan research institute
Tail Entity: ruan ming
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: `` the board of directors at tech innovations inc. includes several prominent figures in the industry, '' said the company spokesperson.  
Head Entity: tech innovations inc.  
Tail Entity: prominent figures
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: rookie steve slaton rushed for 130 yards and two touchdowns monday as the houston texans trounced the jacksonville jaguars 30-17 in a national football league contest .
Head Entity: jacksonville jaguars
Tail Entity: national football league
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: the united nations is an international organization founded in 1945 and currently has 193 member states.  
Head Entity: united nations  
Tail Entity: member states  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: super producer richard `` biff '' stannard is scheduled to work with british songstress alexandra burke for her upcoming debut album , which is due later this year via epic records .
Head Entity: alexandra burke
Tail Entity: british
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: renowned physicist albert einstein was born in germany and later became a citizen of the united states.  
Head Entity: albert einstein  
Tail Entity: germany  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: president raul castro 's government however has given no indication it is inclined to allow any major political or economic opening .
Head Entity: raul castro
Tail Entity: president
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: the renowned physicist albert einstein is best known for his theory of relativity.  
Head Entity: albert einstein  
Tail Entity: physicist  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: chunghwa telecom , taiwan 's largest telecommunications company , will invest nt$ 13 billion -lrb- us$ 403.76 million -rrb- this year to set up four major cloud computing centers in what the company hopes will be the largest data hub in asia , chunghwa telecom 's chairman said thursday .
Head Entity: chunghwa telecom
Tail Entity: taiwan
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: samsung electronics , a global leader in technology and innovation , is headquartered in south korea , where it has been a driving force in the electronics industry for decades.  
Head Entity: samsung electronics  
Tail Entity: south korea  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 84.93%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 83.68%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 82.89%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.63%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.05%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.68%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.78%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.04%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.93%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 87.92%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.70%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.17%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 84.93%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 83.68%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 82.89%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.63%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.05%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.68%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.78%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.04%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.93%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 87.92%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.70%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.17%   
cur_acc:  ['0.8617']
his_acc:  ['0.8617']
CurrentTrain: epoch  0, batch     0 | loss: 6.2359896CurrentTrain: epoch  0, batch     1 | loss: 6.3379230CurrentTrain: epoch  1, batch     0 | loss: 5.4737072CurrentTrain: epoch  1, batch     1 | loss: 5.4184260CurrentTrain: epoch  2, batch     0 | loss: 5.2227926CurrentTrain: epoch  2, batch     1 | loss: 4.4673004CurrentTrain: epoch  3, batch     0 | loss: 4.7786422CurrentTrain: epoch  3, batch     1 | loss: 3.8188832CurrentTrain: epoch  4, batch     0 | loss: 4.1279469CurrentTrain: epoch  4, batch     1 | loss: 3.4805014CurrentTrain: epoch  5, batch     0 | loss: 3.7559161CurrentTrain: epoch  5, batch     1 | loss: 4.0247159CurrentTrain: epoch  6, batch     0 | loss: 3.7888665CurrentTrain: epoch  6, batch     1 | loss: 3.6483328CurrentTrain: epoch  7, batch     0 | loss: 3.2966177CurrentTrain: epoch  7, batch     1 | loss: 3.6291780CurrentTrain: epoch  8, batch     0 | loss: 3.0817895CurrentTrain: epoch  8, batch     1 | loss: 3.3787587CurrentTrain: epoch  9, batch     0 | loss: 3.0076156CurrentTrain: epoch  9, batch     1 | loss: 3.1332412
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: mitchell was elected in 1970 to the first of his eight terms in congress , and in his 16 years representing his baltimore district , he tried to ensure that black-owned businesses got their share of tax money spent on public-works projects and called attention to alleged job bias on the baltimore waterfront and promotion practices at social security administration headquarters in woodlawn .
Head Entity: his
Tail Entity: woodlawn
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: after moving to new york city, she quickly adapted to the fast-paced lifestyle and found a job in a prominent advertising agency.  
Head Entity: she  
Tail Entity: new york city  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: after world war ii , he attended the university of southern california , where he became editor of a college magazine .
Head Entity: he
Tail Entity: university of southern california
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: she graduated from harvard university with a degree in economics before pursuing a career in finance.  
Head Entity: she  
Tail Entity: harvard university  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: us republican congresswoman jo ann davis dies after fight with breast cancer
Head Entity: jo ann davis
Tail Entity: us
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england  
Head Entity: stephen hawking  
Tail Entity: england  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by his wife of 63 years , josephine robinson mcnair , of columbia ; a son , robert e. jr. , of columbia ; three daughters , robin lee howell and corinne godshall , of myrtle beach , s.c. , and claudia crawford mcnair , of jamestown , s.c. ; six grandchildren ; and one great-grandchild .
Head Entity: he
Tail Entity: claudia crawford mcnair
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has two children, a son named michael and a daughter named emily, who both live in new york.  
Head Entity: she  
Tail Entity: emily  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: ferrara said he was innocent of limoli 's slaying , but he pleaded guilty in 1992 to murder , along with racketeering charges , under a deal that sent him to prison for 22 years , rather than go to trial and risk a conviction that could lead to life in prison .
Head Entity: ferrara
Tail Entity: racketeering
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: after a lengthy investigation, the authorities announced that johnson was charged with embezzlement, which shocked his colleagues at the firm.  
Head Entity: johnson  
Tail Entity: embezzlement  
Mixup data size:  81
MixupTrain:  epoch  0, batch     0 | loss: 9.3380241MixupTrain:  epoch  0, batch     1 | loss: 7.8729892MixupTrain:  epoch  0, batch     2 | loss: 7.4261456MixupTrain:  epoch  0, batch     3 | loss: 7.9347844MixupTrain:  epoch  0, batch     4 | loss: 7.4980888MixupTrain:  epoch  0, batch     5 | loss: 6.3416195
MemoryTrain:  epoch  0, batch     0 | loss: 9.1436644MemoryTrain:  epoch  0, batch     1 | loss: 7.8161392MemoryTrain:  epoch  1, batch     0 | loss: 7.6702433MemoryTrain:  epoch  1, batch     1 | loss: 6.3356504MemoryTrain:  epoch  2, batch     0 | loss: 5.4059815MemoryTrain:  epoch  2, batch     1 | loss: 5.5966940MemoryTrain:  epoch  3, batch     0 | loss: 4.7965708MemoryTrain:  epoch  3, batch     1 | loss: 4.3213253MemoryTrain:  epoch  4, batch     0 | loss: 4.4689560MemoryTrain:  epoch  4, batch     1 | loss: 5.1190424
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 89.73%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 90.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.02%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.54%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 87.85%   
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 31.25%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 29.69%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 38.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 46.09%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 52.08%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 60.23%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 64.90%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 65.18%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 65.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 64.84%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 65.07%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 65.28%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 64.80%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 65.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 68.18%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 72.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.08%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 73.84%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 74.78%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 75.65%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 76.04%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 76.21%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 76.76%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 77.57%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 77.68%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 77.78%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 78.04%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 77.96%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 78.21%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 78.59%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 78.66%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 79.51%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 79.83%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 80.28%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 80.71%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 81.12%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 81.51%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 81.89%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 81.50%   
cur_acc:  ['0.8617', '0.8785']
his_acc:  ['0.8617', '0.8150']
CurrentTrain: epoch  0, batch     0 | loss: 6.4476957CurrentTrain: epoch  0, batch     1 | loss: 6.3844142CurrentTrain: epoch  1, batch     0 | loss: 6.1028504CurrentTrain: epoch  1, batch     1 | loss: 5.7017622CurrentTrain: epoch  2, batch     0 | loss: 5.1264153CurrentTrain: epoch  2, batch     1 | loss: 5.1174550CurrentTrain: epoch  3, batch     0 | loss: 4.9185891CurrentTrain: epoch  3, batch     1 | loss: 4.1179166CurrentTrain: epoch  4, batch     0 | loss: 4.6713037CurrentTrain: epoch  4, batch     1 | loss: 4.5963430CurrentTrain: epoch  5, batch     0 | loss: 4.0284042CurrentTrain: epoch  5, batch     1 | loss: 4.4798803CurrentTrain: epoch  6, batch     0 | loss: 3.5981486CurrentTrain: epoch  6, batch     1 | loss: 3.5736172CurrentTrain: epoch  7, batch     0 | loss: 3.5630674CurrentTrain: epoch  7, batch     1 | loss: 2.9170411CurrentTrain: epoch  8, batch     0 | loss: 2.8704734CurrentTrain: epoch  8, batch     1 | loss: 3.0761538CurrentTrain: epoch  9, batch     0 | loss: 2.8680704CurrentTrain: epoch  9, batch     1 | loss: 2.9742637
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: kirkaldy , born irene morgan in baltimore , maryland , in 1917 , was arrested in 1944 for refusing to give up her seat on a greyhound bus heading from gloucester to baltimore , and for resisting arrest .
Head Entity: irene morgan
Tail Entity: 1917
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: albert einstein was born in ulm, in the kingdom of wurttemberg in the german empire on march 14, 1879.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: jane matilda bolin was born on april 11 , 1908 , in poughkeepsie , ny .
Head Entity: jane matilda bolin
Tail Entity: ny
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: john smith was born on march 5, 1980, in los angeles, ca.  
Head Entity: john smith  
Tail Entity: ca  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: lynne spears told the court that lutfi had treated her daughter like a hostage in her own home , drugged her and took over her finances .
Head Entity: her
Tail Entity: lynne spears
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: john's mother, mary, always encouraged him to pursue his dreams and supported him throughout his education.  
Head Entity: john  
Tail Entity: mary  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: after years of hard work, jane doe finally received a promotion at tech innovations, where she has been a dedicated employee.  
Head Entity: jane doe  
Tail Entity: tech innovations  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , whose defiance of bus segregation laws more than a decade before rosa parks ' landmark case helped lay the foundation for later civil rights victories , died friday at her home in hayes , va. .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, a prominent civil rights activist, passed away peacefully in his sleep at his residence in california.  
Head Entity: john doe  
Tail Entity: california  
Mixup data size:  103
MixupTrain:  epoch  0, batch     0 | loss: 6.7544565MixupTrain:  epoch  0, batch     1 | loss: 7.1114426MixupTrain:  epoch  0, batch     2 | loss: 6.7243109MixupTrain:  epoch  0, batch     3 | loss: 7.4010253MixupTrain:  epoch  0, batch     4 | loss: 6.9080400MixupTrain:  epoch  0, batch     5 | loss: 7.5343962MixupTrain:  epoch  0, batch     6 | loss: 6.2156997
MemoryTrain:  epoch  0, batch     0 | loss: 4.8909101MemoryTrain:  epoch  0, batch     1 | loss: 3.7757802MemoryTrain:  epoch  1, batch     0 | loss: 4.4687142MemoryTrain:  epoch  1, batch     1 | loss: 3.9912348MemoryTrain:  epoch  2, batch     0 | loss: 3.3163328MemoryTrain:  epoch  2, batch     1 | loss: 3.9460139MemoryTrain:  epoch  3, batch     0 | loss: 3.8722289MemoryTrain:  epoch  3, batch     1 | loss: 3.6395464MemoryTrain:  epoch  4, batch     0 | loss: 3.7020321MemoryTrain:  epoch  4, batch     1 | loss: 3.2569737
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 75.69%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 76.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 77.27%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 76.04%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 75.96%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 72.32%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 48.44%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 51.25%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 55.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 60.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 65.28%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 71.59%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 73.44%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 74.55%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 74.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 73.05%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 73.16%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 72.57%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 72.04%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 72.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 73.51%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 74.72%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 75.82%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 76.82%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 77.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.61%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.91%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 80.60%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 80.83%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 80.85%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 81.62%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 81.61%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 81.08%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 80.92%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 80.93%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 80.95%   [EVAL] batch:   41 | acc: 25.00%,  total acc: 79.61%   [EVAL] batch:   42 | acc: 25.00%,  total acc: 78.34%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 78.61%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 79.08%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 79.52%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 79.95%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 80.75%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 80.27%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 80.17%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 80.19%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 80.21%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 80.00%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 79.80%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 79.71%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 79.96%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 79.87%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 80.00%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 79.82%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 79.74%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 79.37%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 78.32%   
cur_acc:  ['0.8617', '0.8785', '0.7232']
his_acc:  ['0.8617', '0.8150', '0.7832']
CurrentTrain: epoch  0, batch     0 | loss: 5.1714373CurrentTrain: epoch  0, batch     1 | loss: 4.9118867CurrentTrain: epoch  1, batch     0 | loss: 3.9073286CurrentTrain: epoch  1, batch     1 | loss: 4.1702180CurrentTrain: epoch  2, batch     0 | loss: 3.7626605CurrentTrain: epoch  2, batch     1 | loss: 3.3443277CurrentTrain: epoch  3, batch     0 | loss: 3.2224679CurrentTrain: epoch  3, batch     1 | loss: 3.3949594CurrentTrain: epoch  4, batch     0 | loss: 3.5653503CurrentTrain: epoch  4, batch     1 | loss: 2.6144578CurrentTrain: epoch  5, batch     0 | loss: 2.9316585CurrentTrain: epoch  5, batch     1 | loss: 2.6533501CurrentTrain: epoch  6, batch     0 | loss: 2.6442027CurrentTrain: epoch  6, batch     1 | loss: 2.6014321CurrentTrain: epoch  7, batch     0 | loss: 2.4439149CurrentTrain: epoch  7, batch     1 | loss: 2.4954236CurrentTrain: epoch  8, batch     0 | loss: 2.2616656CurrentTrain: epoch  8, batch     1 | loss: 2.5676625CurrentTrain: epoch  9, batch     0 | loss: 2.2744575CurrentTrain: epoch  9, batch     1 | loss: 1.9836273
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: pandit worked at the brokerage morgan stanley for about 11 years until 2005 , when he and some morgan stanley colleagues quit and later founded the hedge fund old lane partners .
Head Entity: old lane partners
Tail Entity: 2005
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: in 1998, the tech startup innovative solutions was established by a group of engineers who aimed to revolutionize the software industry.  
Head Entity: innovative solutions  
Tail Entity: 1998  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: virginia republican jo ann davis passed away on saturday at the age of 57 .
Head Entity: jo ann davis
Tail Entity: 57
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: the famous actor robert downey jr. celebrated his 56th birthday last week.  
Head Entity: robert downey jr.  
Tail Entity: 56  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: forsberg was born in 1943 in huntsville , ala. , and grew up on long island in new york .
Head Entity: forsberg
Tail Entity: huntsville
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: marie curie was born in warsaw, poland, and later moved to paris to continue her research.  
Head Entity: marie curie  
Tail Entity: warsaw  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: san diego 32 new orleans 37 american football : nfl result result of the nfl match between the san diego chargers of the afc west and the new orleans saints of the nfc south at wembley here sunday :
Head Entity: nfc south
Tail Entity: new orleans saints
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: the university of california system includes several campuses, with the university of california, berkeley being one of the most prestigious institutions in the organization.  
Head Entity: university of california system  
Tail Entity: university of california, berkeley  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: he fought attempts by zealous jews to move into the muslim quarter of the walled old city , but defended the practice of developing jewish suburbs around the eastern arab sector to prevent it from ever escaping israel 's rule .
Head Entity: he
Tail Entity: jewish
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: She has been a devoted member of the Christian community, participating in various church activities and volunteering for local charities.  
Head Entity: She  
Tail Entity: Christian  
Mixup data size:  123
MixupTrain:  epoch  0, batch     0 | loss: 5.4536633MixupTrain:  epoch  0, batch     1 | loss: 6.2443056MixupTrain:  epoch  0, batch     2 | loss: 6.6968350MixupTrain:  epoch  0, batch     3 | loss: 5.9379606MixupTrain:  epoch  0, batch     4 | loss: 6.3157597MixupTrain:  epoch  0, batch     5 | loss: 5.6287594MixupTrain:  epoch  0, batch     6 | loss: 5.9906855MixupTrain:  epoch  0, batch     7 | loss: 6.0534806
MemoryTrain:  epoch  0, batch     0 | loss: 3.0105052MemoryTrain:  epoch  0, batch     1 | loss: 3.5271108MemoryTrain:  epoch  0, batch     2 | loss: 5.1657372MemoryTrain:  epoch  1, batch     0 | loss: 3.8027430MemoryTrain:  epoch  1, batch     1 | loss: 3.5997877MemoryTrain:  epoch  1, batch     2 | loss: 3.7230988MemoryTrain:  epoch  2, batch     0 | loss: 3.2386265MemoryTrain:  epoch  2, batch     1 | loss: 3.8610005MemoryTrain:  epoch  2, batch     2 | loss: 3.1123185MemoryTrain:  epoch  3, batch     0 | loss: 2.4409685MemoryTrain:  epoch  3, batch     1 | loss: 2.5692072MemoryTrain:  epoch  3, batch     2 | loss: 3.8818195MemoryTrain:  epoch  4, batch     0 | loss: 3.0971735MemoryTrain:  epoch  4, batch     1 | loss: 2.6847160MemoryTrain:  epoch  4, batch     2 | loss: 3.2299550
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 98.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 98.96%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 99.11%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 99.22%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 97.92%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 93.12%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 85.71%   
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 35.42%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 31.25%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 32.29%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 39.29%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 46.88%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 52.78%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 57.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 61.36%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 64.06%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 63.94%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 62.95%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 63.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 62.89%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 63.60%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 63.89%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 63.49%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 64.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 65.77%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 67.33%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 70.05%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 71.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.36%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 73.15%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 75.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 76.41%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 77.15%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 76.52%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 74.45%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 73.04%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 71.35%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 69.76%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 68.09%   [EVAL] batch:   38 | acc: 43.75%,  total acc: 67.47%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 68.12%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 68.45%   [EVAL] batch:   41 | acc: 25.00%,  total acc: 67.41%   [EVAL] batch:   42 | acc: 31.25%,  total acc: 66.57%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 66.62%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 67.36%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 68.07%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 70.03%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 70.62%   [EVAL] batch:   50 | acc: 50.00%,  total acc: 70.22%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 70.19%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 70.52%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 70.60%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 70.68%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 70.65%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 70.72%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 71.12%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 71.40%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 71.67%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 71.62%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 71.67%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 71.63%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 71.78%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 72.21%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 72.54%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 72.95%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 73.35%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 73.73%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 74.47%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 74.83%   [EVAL] batch:   72 | acc: 43.75%,  total acc: 74.40%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 74.16%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 74.33%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 74.26%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 74.27%   [EVAL] batch:   77 | acc: 6.25%,  total acc: 73.40%   
cur_acc:  ['0.8617', '0.8785', '0.7232', '0.8571']
his_acc:  ['0.8617', '0.8150', '0.7832', '0.7340']
CurrentTrain: epoch  0, batch     0 | loss: 4.3105574CurrentTrain: epoch  0, batch     1 | loss: 5.2009950CurrentTrain: epoch  1, batch     0 | loss: 3.2064915CurrentTrain: epoch  1, batch     1 | loss: 3.4505975CurrentTrain: epoch  2, batch     0 | loss: 3.0929978CurrentTrain: epoch  2, batch     1 | loss: 2.9503794CurrentTrain: epoch  3, batch     0 | loss: 2.9440515CurrentTrain: epoch  3, batch     1 | loss: 2.4124677CurrentTrain: epoch  4, batch     0 | loss: 2.5216298CurrentTrain: epoch  4, batch     1 | loss: 2.4796278CurrentTrain: epoch  5, batch     0 | loss: 2.3442810CurrentTrain: epoch  5, batch     1 | loss: 2.2515340CurrentTrain: epoch  6, batch     0 | loss: 2.5378704CurrentTrain: epoch  6, batch     1 | loss: 2.3537767CurrentTrain: epoch  7, batch     0 | loss: 2.2903404CurrentTrain: epoch  7, batch     1 | loss: 2.1501992CurrentTrain: epoch  8, batch     0 | loss: 1.9687076CurrentTrain: epoch  8, batch     1 | loss: 1.9248199CurrentTrain: epoch  9, batch     0 | loss: 2.0192308CurrentTrain: epoch  9, batch     1 | loss: 1.9927741
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling lung cancer for over a year, john doe passed away peacefully in his sleep, surrounded by family.  
Head Entity: john doe  
Tail Entity: lung cancer
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: clashes in late august in karbala between the mahdi army and a rival shiite militia , the badr organization , left at least 50 people dead .
Head Entity: badr organization
Tail Entity: shiite
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: the interfaith dialogue initiative was led by the peace organization, which aimed to foster understanding between different religious groups.  
Head Entity: peace organization  
Tail Entity: religious groups  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: based in armonk , new york , mbia insures $ 670 billion -lrb- euro452 .18 billion -rrb- in debt .
Head Entity: mbia
Tail Entity: new york
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the headquarters of tech giant apple is located in cupertino , california , where it employs thousands of workers.  
Head Entity: apple  
Tail Entity: california  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: parren mitchell 's sister-in-law , juanita jackson mitchell , was the long - time head and legal counsel of the maryland naacp .
Head Entity: parren mitchell
Tail Entity: juanita jackson mitchell
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: barack obama's half-sister, maya soetoro-ng, is an educator and a prominent figure in her own right.  
Head Entity: barack obama  
Tail Entity: maya soetoro-ng  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: millender-mcdonald , who was 68 , died late saturday at her home in carson , california , said her chief of staff , bandele mcqueen .
Head Entity: millender-mcdonald
Tail Entity: carson
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: johnson, who was 75, passed away peacefully in his sleep at his residence in miami, florida, surrounded by family.  
Head Entity: johnson  
Tail Entity: miami  
Mixup data size:  143
MixupTrain:  epoch  0, batch     0 | loss: 5.9765234MixupTrain:  epoch  0, batch     1 | loss: 5.5738263MixupTrain:  epoch  0, batch     2 | loss: 6.3060694MixupTrain:  epoch  0, batch     3 | loss: 6.6909990MixupTrain:  epoch  0, batch     4 | loss: 6.1451159MixupTrain:  epoch  0, batch     5 | loss: 6.3978777MixupTrain:  epoch  0, batch     6 | loss: 5.9090347MixupTrain:  epoch  0, batch     7 | loss: 6.3313150MixupTrain:  epoch  0, batch     8 | loss: 5.9062281
MemoryTrain:  epoch  0, batch     0 | loss: 3.5373933MemoryTrain:  epoch  0, batch     1 | loss: 3.1217206MemoryTrain:  epoch  0, batch     2 | loss: 3.4238086MemoryTrain:  epoch  0, batch     3 | loss: 3.1686373MemoryTrain:  epoch  1, batch     0 | loss: 3.4419842MemoryTrain:  epoch  1, batch     1 | loss: 2.8742204MemoryTrain:  epoch  1, batch     2 | loss: 3.6892071MemoryTrain:  epoch  1, batch     3 | loss: 2.7356169MemoryTrain:  epoch  2, batch     0 | loss: 2.7355735MemoryTrain:  epoch  2, batch     1 | loss: 2.9599032MemoryTrain:  epoch  2, batch     2 | loss: 3.2498140MemoryTrain:  epoch  2, batch     3 | loss: 3.1972702MemoryTrain:  epoch  3, batch     0 | loss: 2.5260253MemoryTrain:  epoch  3, batch     1 | loss: 2.2713513MemoryTrain:  epoch  3, batch     2 | loss: 3.2362943MemoryTrain:  epoch  3, batch     3 | loss: 2.5323117MemoryTrain:  epoch  4, batch     0 | loss: 2.0678365MemoryTrain:  epoch  4, batch     1 | loss: 2.5675488MemoryTrain:  epoch  4, batch     2 | loss: 3.0343478MemoryTrain:  epoch  4, batch     3 | loss: 3.4167089
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 42.19%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 41.25%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 40.62%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 39.29%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 41.41%   [EVAL] batch:    8 | acc: 18.75%,  total acc: 38.89%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 41.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 45.45%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 48.96%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 49.04%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 47.92%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 42.19%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 50.89%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 57.03%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 61.81%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 69.23%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 66.52%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 66.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 66.02%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 66.54%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 66.32%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 65.79%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 66.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 67.86%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 69.32%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 70.65%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 73.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 74.04%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 74.77%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.67%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 76.51%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 76.88%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 77.22%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 77.93%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 76.70%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 74.45%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 72.32%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 70.31%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 68.41%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 66.61%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 65.71%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 66.41%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 66.62%   [EVAL] batch:   41 | acc: 31.25%,  total acc: 65.77%   [EVAL] batch:   42 | acc: 37.50%,  total acc: 65.12%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 65.20%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 65.97%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 66.71%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 67.42%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 68.10%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 69.38%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 68.87%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 67.91%   [EVAL] batch:   52 | acc: 18.75%,  total acc: 66.98%   [EVAL] batch:   53 | acc: 6.25%,  total acc: 65.86%   [EVAL] batch:   54 | acc: 18.75%,  total acc: 65.00%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 63.84%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 63.71%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 64.22%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 64.72%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 65.10%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 65.27%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 65.42%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 65.58%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 65.92%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 66.44%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 66.86%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 67.35%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 67.83%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 68.30%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 69.19%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 69.53%   [EVAL] batch:   72 | acc: 43.75%,  total acc: 69.18%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 69.34%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 69.33%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 69.16%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 69.07%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 68.43%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 68.28%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 67.97%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 67.75%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 67.45%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 67.02%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 66.59%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 66.47%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 65.99%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 65.95%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 66.12%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 66.36%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 66.25%   
cur_acc:  ['0.8617', '0.8785', '0.7232', '0.8571', '0.4904']
his_acc:  ['0.8617', '0.8150', '0.7832', '0.7340', '0.6625']
CurrentTrain: epoch  0, batch     0 | loss: 5.9610963CurrentTrain: epoch  0, batch     1 | loss: 7.8619561CurrentTrain: epoch  1, batch     0 | loss: 5.7557793CurrentTrain: epoch  1, batch     1 | loss: 5.0796375CurrentTrain: epoch  2, batch     0 | loss: 5.0907812CurrentTrain: epoch  2, batch     1 | loss: 4.6695685CurrentTrain: epoch  3, batch     0 | loss: 4.8276615CurrentTrain: epoch  3, batch     1 | loss: 3.9637270CurrentTrain: epoch  4, batch     0 | loss: 4.6400180CurrentTrain: epoch  4, batch     1 | loss: 3.4571333CurrentTrain: epoch  5, batch     0 | loss: 4.0575209CurrentTrain: epoch  5, batch     1 | loss: 3.5810289CurrentTrain: epoch  6, batch     0 | loss: 3.2278876CurrentTrain: epoch  6, batch     1 | loss: 4.3007293CurrentTrain: epoch  7, batch     0 | loss: 3.4974198CurrentTrain: epoch  7, batch     1 | loss: 2.8532536CurrentTrain: epoch  8, batch     0 | loss: 3.3645086CurrentTrain: epoch  8, batch     1 | loss: 2.9283137CurrentTrain: epoch  9, batch     0 | loss: 2.8123369CurrentTrain: epoch  9, batch     1 | loss: 3.1070035
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: san jose , ca , usa speaking of k-fed , him and ex-wife britney spears are in court today , dealing with their custody battle .
Head Entity: britney spears
Tail Entity: ca
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: after moving to austin, texas, the musician found a new inspiration for his songs.  
Head Entity: musician  
Tail Entity: texas  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: he passed away on saturday .
Head Entity: he
Tail Entity: saturday
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: she died on March 5th, 2020.  
Head Entity: she  
Tail Entity: March 5th, 2020  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: covidien , which posted revenue of more than $ 10 billion last year , has about 42,000 employees worldwide .
Head Entity: covidien
Tail Entity: 42,000
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: Google, known for its innovative technology, employs approximately 156,500 people globally.  
Head Entity: Google  
Tail Entity: 156,500  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: but hyperventilating bloviators jumped all over sen. barbara boxer last week for alluding to secretary of state condoleezza rice 's single status -- as though boxer were accusing rice of botching the iraq war because she 's a spinster .
Head Entity: boxer
Tail Entity: barbara boxer
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: The famous author Samuel Clemens is better known by his pen name, Mark Twain, which he used throughout his literary career.  
Head Entity: Clemens  
Tail Entity: Mark Twain  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: when her husband retired from congress in 1977 , mrs. gude was urged to run for his seat or for governor , but she had no interest in holding office herself , despite her lifelong interest in politics .
Head Entity: she
Tail Entity: his
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: after years of marriage, john and mary decided to celebrate their anniversary with a special dinner, reflecting on their journey together as a couple.  
Head Entity: john  
Tail Entity: mary  
Mixup data size:  162
MixupTrain:  epoch  0, batch     0 | loss: 5.6796837MixupTrain:  epoch  0, batch     1 | loss: 4.6617246MixupTrain:  epoch  0, batch     2 | loss: 5.9200349MixupTrain:  epoch  0, batch     3 | loss: 5.2869134MixupTrain:  epoch  0, batch     4 | loss: 5.5913525MixupTrain:  epoch  0, batch     5 | loss: 6.3506985MixupTrain:  epoch  0, batch     6 | loss: 5.5756464MixupTrain:  epoch  0, batch     7 | loss: 5.3395028MixupTrain:  epoch  0, batch     8 | loss: 5.6936269MixupTrain:  epoch  0, batch     9 | loss: 5.5687313MixupTrain:  epoch  0, batch    10 | loss: 3.7914562
MemoryTrain:  epoch  0, batch     0 | loss: 2.1785996MemoryTrain:  epoch  0, batch     1 | loss: 2.0996542MemoryTrain:  epoch  0, batch     2 | loss: 2.9781203MemoryTrain:  epoch  0, batch     3 | loss: 3.7298090MemoryTrain:  epoch  1, batch     0 | loss: 2.9499371MemoryTrain:  epoch  1, batch     1 | loss: 2.5918047MemoryTrain:  epoch  1, batch     2 | loss: 2.3966441MemoryTrain:  epoch  1, batch     3 | loss: 2.2529426MemoryTrain:  epoch  2, batch     0 | loss: 2.2814236MemoryTrain:  epoch  2, batch     1 | loss: 2.5477958MemoryTrain:  epoch  2, batch     2 | loss: 2.3289442MemoryTrain:  epoch  2, batch     3 | loss: 2.0181518MemoryTrain:  epoch  3, batch     0 | loss: 2.2670705MemoryTrain:  epoch  3, batch     1 | loss: 2.3130455MemoryTrain:  epoch  3, batch     2 | loss: 2.1892383MemoryTrain:  epoch  3, batch     3 | loss: 2.0623798MemoryTrain:  epoch  4, batch     0 | loss: 2.2073135MemoryTrain:  epoch  4, batch     1 | loss: 1.8432590MemoryTrain:  epoch  4, batch     2 | loss: 1.7725813MemoryTrain:  epoch  4, batch     3 | loss: 2.2520683
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 35.42%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 43.75%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 49.22%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 53.47%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 55.00%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 52.84%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 52.08%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 50.48%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 48.21%   [EVAL] batch:   14 | acc: 6.25%,  total acc: 45.42%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 60.94%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 60.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 66.96%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 74.31%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 76.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 78.98%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 78.37%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 75.00%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 74.58%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 73.44%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 73.53%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 72.04%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 72.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 73.51%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 74.72%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 75.82%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 76.82%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 77.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.61%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.91%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 80.60%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 80.83%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 81.05%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 81.64%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 80.30%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 77.94%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 75.89%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 73.78%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 71.79%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 69.90%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 68.91%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 69.53%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 69.66%   [EVAL] batch:   41 | acc: 25.00%,  total acc: 68.60%   [EVAL] batch:   42 | acc: 31.25%,  total acc: 67.73%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 67.76%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 68.47%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 69.16%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 69.81%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 70.44%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 71.05%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 71.38%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 70.83%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 69.83%   [EVAL] batch:   52 | acc: 25.00%,  total acc: 68.99%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 67.71%   [EVAL] batch:   54 | acc: 18.75%,  total acc: 66.82%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 65.62%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 65.46%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 65.95%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 66.42%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 66.77%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 66.80%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 66.83%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 66.87%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 67.29%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 67.79%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 68.18%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 68.66%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 69.12%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 70.42%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 70.75%   [EVAL] batch:   72 | acc: 43.75%,  total acc: 70.38%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 70.52%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 70.97%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 71.10%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 70.43%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 70.25%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 69.92%   [EVAL] batch:   80 | acc: 43.75%,  total acc: 69.60%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 69.13%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 68.67%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 68.15%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 67.72%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 67.08%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 66.59%   [EVAL] batch:   87 | acc: 31.25%,  total acc: 66.19%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 66.22%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 66.04%   [EVAL] batch:   90 | acc: 31.25%,  total acc: 65.66%   [EVAL] batch:   91 | acc: 25.00%,  total acc: 65.22%   [EVAL] batch:   92 | acc: 25.00%,  total acc: 64.78%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 64.36%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 64.14%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 64.39%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 64.76%   [EVAL] batch:   97 | acc: 81.25%,  total acc: 64.92%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 65.21%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 65.00%   [EVAL] batch:  100 | acc: 31.25%,  total acc: 64.67%   [EVAL] batch:  101 | acc: 31.25%,  total acc: 64.34%   [EVAL] batch:  102 | acc: 37.50%,  total acc: 64.08%   [EVAL] batch:  103 | acc: 18.75%,  total acc: 63.64%   [EVAL] batch:  104 | acc: 0.00%,  total acc: 63.04%   
cur_acc:  ['0.8617', '0.8785', '0.7232', '0.8571', '0.4904', '0.4542']
his_acc:  ['0.8617', '0.8150', '0.7832', '0.7340', '0.6625', '0.6304']
CurrentTrain: epoch  0, batch     0 | loss: 5.8730793CurrentTrain: epoch  0, batch     1 | loss: 5.3807292CurrentTrain: epoch  1, batch     0 | loss: 5.8088760CurrentTrain: epoch  1, batch     1 | loss: 3.5389769CurrentTrain: epoch  2, batch     0 | loss: 4.4509697CurrentTrain: epoch  2, batch     1 | loss: 4.5125751CurrentTrain: epoch  3, batch     0 | loss: 4.8579016CurrentTrain: epoch  3, batch     1 | loss: 2.9598715CurrentTrain: epoch  4, batch     0 | loss: 3.3987358CurrentTrain: epoch  4, batch     1 | loss: 3.8334572CurrentTrain: epoch  5, batch     0 | loss: 3.1772630CurrentTrain: epoch  5, batch     1 | loss: 3.3393948CurrentTrain: epoch  6, batch     0 | loss: 3.0392747CurrentTrain: epoch  6, batch     1 | loss: 2.9342973CurrentTrain: epoch  7, batch     0 | loss: 3.1525140CurrentTrain: epoch  7, batch     1 | loss: 2.3800528CurrentTrain: epoch  8, batch     0 | loss: 2.8974481CurrentTrain: epoch  8, batch     1 | loss: 2.5715740CurrentTrain: epoch  9, batch     0 | loss: 2.5584540CurrentTrain: epoch  9, batch     1 | loss: 2.7677238
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: in testimony by satellite link from germany to a house of representatives ' panel , murat kurnaz recounted his five-year detention , alleging a wide range of torture and abuse .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: born in the bustling city of new delhi, arjun was always proud of his indian heritage and culture.  
Head Entity: arjun  
Tail Entity: india  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: 11.30.08 2008 cma awards red carpet special http://www.cmt.com/shows/dyn/2008-cma-awards-red-carpet/1/episode.jhtml
Head Entity: cma
Tail Entity: http://www.cmt.com/shows/dyn/2008-cma-awards-red-carpet/1/episode.jhtml
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: The official site for the American Red Cross can be found at https://www.redcross.org.  
Head Entity: American Red Cross  
Tail Entity: https://www.redcross.org  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant apple inc. has seen significant investments from billionaire investor warren buffett.  
Head Entity: apple inc.  
Tail Entity: warren buffett  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: the company announced its closure after years of financial struggles, officially dissolving on march 15, 2020.  
Head Entity: company  
Tail Entity: march 15, 2020  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: born in new york , she inherited most of her fortune from her father , ted arison , who founded carnival cruise lines and also owned the miami heat basketball team .
Head Entity: carnival cruise lines
Tail Entity: ted arison
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: in 1975, steve jobs and steve wozniak founded apple inc., which has since become one of the most valuable companies in the world.  
Head Entity: apple inc.  
Tail Entity: steve jobs
Mixup data size:  183
MixupTrain:  epoch  0, batch     0 | loss: 4.1948543MixupTrain:  epoch  0, batch     1 | loss: 5.3346567nan loss
nan loss
nan loss
MixupTrain:  epoch  0, batch     5 | loss: 4.4878263nan loss
MixupTrain:  epoch  0, batch     7 | loss: 5.1588984nan loss
MixupTrain:  epoch  0, batch     9 | loss: 4.8641000MixupTrain:  epoch  0, batch    10 | loss: 4.4814510MixupTrain:  epoch  0, batch    11 | loss: 5.5971856
MemoryTrain:  epoch  0, batch     0 | loss: 2.4424450MemoryTrain:  epoch  0, batch     1 | loss: 2.7108674MemoryTrain:  epoch  0, batch     2 | loss: 2.6615503MemoryTrain:  epoch  0, batch     3 | loss: 4.0333738MemoryTrain:  epoch  0, batch     4 | loss: 2.3150685MemoryTrain:  epoch  1, batch     0 | loss: 2.6450582MemoryTrain:  epoch  1, batch     1 | loss: 3.5087337MemoryTrain:  epoch  1, batch     2 | loss: 2.6142564MemoryTrain:  epoch  1, batch     3 | loss: 1.8079495MemoryTrain:  epoch  1, batch     4 | loss: 3.2644770MemoryTrain:  epoch  2, batch     0 | loss: 3.1667371MemoryTrain:  epoch  2, batch     1 | loss: 2.1307976MemoryTrain:  epoch  2, batch     2 | loss: 2.5747919MemoryTrain:  epoch  2, batch     3 | loss: 1.7042804MemoryTrain:  epoch  2, batch     4 | loss: 2.3689413MemoryTrain:  epoch  3, batch     0 | loss: 2.3219688MemoryTrain:  epoch  3, batch     1 | loss: 2.0001833MemoryTrain:  epoch  3, batch     2 | loss: 2.4786623MemoryTrain:  epoch  3, batch     3 | loss: 1.9936094MemoryTrain:  epoch  3, batch     4 | loss: 1.7943237MemoryTrain:  epoch  4, batch     0 | loss: 2.4220846MemoryTrain:  epoch  4, batch     1 | loss: 1.5451305MemoryTrain:  epoch  4, batch     2 | loss: 1.5800173MemoryTrain:  epoch  4, batch     3 | loss: 1.6329994MemoryTrain:  epoch  4, batch     4 | loss: 1.7611017
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 61.72%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 8.04%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 15.62%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 23.61%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 29.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 34.66%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 36.46%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 37.02%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 36.61%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 38.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 39.45%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 41.54%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 42.71%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 43.42%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 45.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 47.62%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 50.00%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 52.17%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 53.91%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 55.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 57.45%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 58.80%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 60.27%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 61.64%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 62.29%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 63.48%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 62.69%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 60.85%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 59.29%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 57.64%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 56.08%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 54.61%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 54.01%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 55.00%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 54.57%   [EVAL] batch:   41 | acc: 25.00%,  total acc: 53.87%   [EVAL] batch:   42 | acc: 37.50%,  total acc: 53.49%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 53.98%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 55.00%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 55.98%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 56.91%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 57.81%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 58.67%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 59.38%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 59.07%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 58.29%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 57.43%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 56.37%   [EVAL] batch:   54 | acc: 18.75%,  total acc: 55.68%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 54.69%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 54.61%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 55.28%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 55.72%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 56.04%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 56.05%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 56.15%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 55.95%   [EVAL] batch:   63 | acc: 62.50%,  total acc: 56.05%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 56.63%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 57.10%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 57.74%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 58.36%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 58.97%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 59.55%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 60.12%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 60.59%   [EVAL] batch:   72 | acc: 31.25%,  total acc: 60.19%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 60.05%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 60.50%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 60.61%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 60.96%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 60.66%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 60.68%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 60.55%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 60.49%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 60.44%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 60.39%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 60.42%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 60.22%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 59.59%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 59.34%   [EVAL] batch:   87 | acc: 43.75%,  total acc: 59.16%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 59.27%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 59.24%   [EVAL] batch:   90 | acc: 37.50%,  total acc: 59.00%   [EVAL] batch:   91 | acc: 31.25%,  total acc: 58.70%   [EVAL] batch:   92 | acc: 12.50%,  total acc: 58.20%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 57.85%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 57.83%   [EVAL] batch:   95 | acc: 75.00%,  total acc: 58.01%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 58.44%   [EVAL] batch:   97 | acc: 81.25%,  total acc: 58.67%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 59.03%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 58.81%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 58.79%   [EVAL] batch:  101 | acc: 37.50%,  total acc: 58.58%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 58.43%   [EVAL] batch:  103 | acc: 18.75%,  total acc: 58.05%   [EVAL] batch:  104 | acc: 81.25%,  total acc: 58.27%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 58.67%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 58.64%   [EVAL] batch:  107 | acc: 62.50%,  total acc: 58.68%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 58.66%   [EVAL] batch:  109 | acc: 68.75%,  total acc: 58.75%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 58.73%   [EVAL] batch:  111 | acc: 12.50%,  total acc: 58.31%   
cur_acc:  ['0.8617', '0.8785', '0.7232', '0.8571', '0.4904', '0.4542', '0.6172']
his_acc:  ['0.8617', '0.8150', '0.7832', '0.7340', '0.6625', '0.6304', '0.5831']
CurrentTrain: epoch  0, batch     0 | loss: 7.7616682CurrentTrain: epoch  0, batch     1 | loss: 7.8339148CurrentTrain: epoch  1, batch     0 | loss: 7.2426677CurrentTrain: epoch  1, batch     1 | loss: 6.4275713CurrentTrain: epoch  2, batch     0 | loss: 6.2984028CurrentTrain: epoch  2, batch     1 | loss: 6.1583104CurrentTrain: epoch  3, batch     0 | loss: 5.3467641CurrentTrain: epoch  3, batch     1 | loss: 6.3891401CurrentTrain: epoch  4, batch     0 | loss: 5.8019838CurrentTrain: epoch  4, batch     1 | loss: 4.4148693CurrentTrain: epoch  5, batch     0 | loss: 5.4350777CurrentTrain: epoch  5, batch     1 | loss: 4.3065739CurrentTrain: epoch  6, batch     0 | loss: 5.0938778CurrentTrain: epoch  6, batch     1 | loss: 4.2513371CurrentTrain: epoch  7, batch     0 | loss: 4.3343573CurrentTrain: epoch  7, batch     1 | loss: 4.8810525CurrentTrain: epoch  8, batch     0 | loss: 4.2807121CurrentTrain: epoch  8, batch     1 | loss: 4.1629052CurrentTrain: epoch  9, batch     0 | loss: 3.8332310CurrentTrain: epoch  9, batch     1 | loss: 4.4748788
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: firstgroup , britain 's largest bus operator , entered the north american market in 1999 when it acquired ryder public transportation services inc. .
Head Entity: firstgroup
Tail Entity: ryder public transportation services inc.
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: Alphabet Inc. has several subsidiaries, including Google LLC, which was founded in 1998.  
Head Entity: Alphabet Inc.  
Tail Entity: Google LLC  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: the company was founded by john doe and is now a subsidiary of tech innovations inc., which has been a leader in the industry for over a decade.  
Head Entity: tech innovations inc.  
Tail Entity: john doe
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: while section 106 of the hyde act openly bans indian testing , and the agreement upholds reinforces that test ban by upholding the applicability of domestic laws , washington has already recommended that the nuclear suppliers group -lrb- nsg -rrb- link its proposed exemption for india to a similar test ban .
Head Entity: nuclear suppliers group
Tail Entity: nsg
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: The International Monetary Fund, often referred to as the IMF, plays a crucial role in global economic stability.  
Head Entity: International Monetary Fund  
Tail Entity: IMF  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: ------ london 2008-05-20 07:23:45 utc enodis plc endorses sweetened takeover bid by us company manitowoc illinois tool works of glenville , illinois , which had offered 282 pence -lrb- us$ 551 euro3 54 -rrb- per share , said monday that it was considering its position .
Head Entity: illinois tool works
Tail Entity: glenville
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: ------ san francisco 2021-03-15 10:00:00 utc tech giant google has announced plans to expand its headquarters in the vibrant city of mountain view, california, which is known for its innovation and tech culture.  
Head Entity: google  
Tail Entity: mountain view  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: `` holly and sanjaya are headed to -lsb- the hawaiian island of -rsb- kauai tomorrow morning so she can meet his parents . ''
Head Entity: she
Tail Entity: sanjaya
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: `` jessica and her brother are planning a trip to the mountains next weekend to enjoy some hiking together. ''  
Head Entity: her  
Tail Entity: brother  
Mixup data size:  202
MixupTrain:  epoch  0, batch     0 | loss: 4.5698133MixupTrain:  epoch  0, batch     1 | loss: 5.3662753MixupTrain:  epoch  0, batch     2 | loss: 4.8280206MixupTrain:  epoch  0, batch     3 | loss: 4.6633806MixupTrain:  epoch  0, batch     4 | loss: 6.0609255MixupTrain:  epoch  0, batch     5 | loss: 4.7468853MixupTrain:  epoch  0, batch     6 | loss: 5.0810595MixupTrain:  epoch  0, batch     7 | loss: 4.8854289MixupTrain:  epoch  0, batch     8 | loss: 4.5330906MixupTrain:  epoch  0, batch     9 | loss: 5.5505419MixupTrain:  epoch  0, batch    10 | loss: 5.3257685MixupTrain:  epoch  0, batch    11 | loss: 4.0446281MixupTrain:  epoch  0, batch    12 | loss: 5.0437422
MemoryTrain:  epoch  0, batch     0 | loss: 1.9734776MemoryTrain:  epoch  0, batch     1 | loss: 2.3245549MemoryTrain:  epoch  0, batch     2 | loss: 1.6958258MemoryTrain:  epoch  0, batch     3 | loss: 2.4173093MemoryTrain:  epoch  0, batch     4 | loss: 3.1961207MemoryTrain:  epoch  0, batch     5 | loss: 2.1830301MemoryTrain:  epoch  1, batch     0 | loss: 1.5463045MemoryTrain:  epoch  1, batch     1 | loss: 1.9036591MemoryTrain:  epoch  1, batch     2 | loss: 1.6073611MemoryTrain:  epoch  1, batch     3 | loss: 3.0650320MemoryTrain:  epoch  1, batch     4 | loss: 2.5356045MemoryTrain:  epoch  1, batch     5 | loss: 3.7307663MemoryTrain:  epoch  2, batch     0 | loss: 1.4331163MemoryTrain:  epoch  2, batch     1 | loss: 1.7679944MemoryTrain:  epoch  2, batch     2 | loss: 1.7695618MemoryTrain:  epoch  2, batch     3 | loss: 2.8103616MemoryTrain:  epoch  2, batch     4 | loss: 2.2604671MemoryTrain:  epoch  2, batch     5 | loss: 2.4116020MemoryTrain:  epoch  3, batch     0 | loss: 2.0037704MemoryTrain:  epoch  3, batch     1 | loss: 1.5904427MemoryTrain:  epoch  3, batch     2 | loss: 1.6190721MemoryTrain:  epoch  3, batch     3 | loss: 1.8908963MemoryTrain:  epoch  3, batch     4 | loss: 2.1799831MemoryTrain:  epoch  3, batch     5 | loss: 1.5721788MemoryTrain:  epoch  4, batch     0 | loss: 1.7627763MemoryTrain:  epoch  4, batch     1 | loss: 1.4482180MemoryTrain:  epoch  4, batch     2 | loss: 1.7842352MemoryTrain:  epoch  4, batch     3 | loss: 1.8866221MemoryTrain:  epoch  4, batch     4 | loss: 1.5781388MemoryTrain:  epoch  4, batch     5 | loss: 3.0915151
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 27.08%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 20.31%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 16.25%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 13.54%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 15.18%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 25.00%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 31.25%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 34.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 39.20%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 43.23%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 46.15%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 50.00%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 52.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 55.86%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 58.09%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 59.03%   [EVAL] batch:   18 | acc: 0.00%,  total acc: 55.92%   [EVAL] batch:   19 | acc: 6.25%,  total acc: 53.44%   [EVAL] batch:   20 | acc: 6.25%,  total acc: 51.19%   [EVAL] batch:   21 | acc: 12.50%,  total acc: 49.43%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 3.12%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 6.25%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 4.69%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 5.00%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 5.21%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 14.29%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 23.44%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 31.25%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 36.88%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 40.91%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 44.27%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 44.71%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 43.30%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 45.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 45.70%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 47.43%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 48.26%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 48.68%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 50.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 52.38%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 54.55%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 56.52%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 58.07%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 59.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 61.30%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 62.04%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 63.17%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 64.01%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 64.38%   [EVAL] batch:   30 | acc: 62.50%,  total acc: 64.31%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 65.04%   [EVAL] batch:   32 | acc: 31.25%,  total acc: 64.02%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 62.32%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 60.71%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 59.38%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 57.94%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 56.41%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 55.77%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 56.72%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 56.25%   [EVAL] batch:   41 | acc: 37.50%,  total acc: 55.80%   [EVAL] batch:   42 | acc: 43.75%,  total acc: 55.52%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 55.82%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 56.81%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 57.74%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 58.64%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 59.51%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 60.33%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 61.00%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 60.66%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 59.86%   [EVAL] batch:   52 | acc: 18.75%,  total acc: 59.08%   [EVAL] batch:   53 | acc: 6.25%,  total acc: 58.10%   [EVAL] batch:   54 | acc: 18.75%,  total acc: 57.39%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 56.36%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 56.90%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 57.20%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 57.60%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 57.68%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 57.76%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 57.84%   [EVAL] batch:   63 | acc: 62.50%,  total acc: 57.91%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 58.46%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 58.90%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 59.51%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 60.11%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 60.69%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 61.25%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 61.80%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 62.24%   [EVAL] batch:   72 | acc: 6.25%,  total acc: 61.47%   [EVAL] batch:   73 | acc: 0.00%,  total acc: 60.64%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 60.92%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 61.18%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 61.44%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 60.98%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 61.00%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 60.86%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 60.73%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 60.82%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 61.14%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 61.16%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 61.03%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 60.47%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 60.20%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 60.16%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 60.25%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 60.35%   [EVAL] batch:   90 | acc: 43.75%,  total acc: 60.16%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 60.33%   [EVAL] batch:   92 | acc: 50.00%,  total acc: 60.22%   [EVAL] batch:   93 | acc: 56.25%,  total acc: 60.17%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 60.53%   [EVAL] batch:   95 | acc: 75.00%,  total acc: 60.68%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 61.08%   [EVAL] batch:   97 | acc: 75.00%,  total acc: 61.22%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 61.55%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 61.38%   [EVAL] batch:  100 | acc: 31.25%,  total acc: 61.08%   [EVAL] batch:  101 | acc: 25.00%,  total acc: 60.72%   [EVAL] batch:  102 | acc: 31.25%,  total acc: 60.44%   [EVAL] batch:  103 | acc: 12.50%,  total acc: 59.98%   [EVAL] batch:  104 | acc: 81.25%,  total acc: 60.18%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 60.55%   [EVAL] batch:  106 | acc: 25.00%,  total acc: 60.22%   [EVAL] batch:  107 | acc: 56.25%,  total acc: 60.19%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 59.92%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 59.60%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 59.23%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 58.93%   [EVAL] batch:  112 | acc: 31.25%,  total acc: 58.68%   [EVAL] batch:  113 | acc: 31.25%,  total acc: 58.44%   [EVAL] batch:  114 | acc: 0.00%,  total acc: 57.93%   [EVAL] batch:  115 | acc: 0.00%,  total acc: 57.44%   [EVAL] batch:  116 | acc: 0.00%,  total acc: 56.94%   [EVAL] batch:  117 | acc: 18.75%,  total acc: 56.62%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 56.83%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 57.08%   [EVAL] batch:  120 | acc: 62.50%,  total acc: 57.13%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 57.38%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 57.62%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 57.86%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 58.10%   [EVAL] batch:  125 | acc: 93.75%,  total acc: 58.38%   [EVAL] batch:  126 | acc: 100.00%,  total acc: 58.71%   [EVAL] batch:  127 | acc: 93.75%,  total acc: 58.98%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 59.16%   [EVAL] batch:  129 | acc: 12.50%,  total acc: 58.80%   [EVAL] batch:  130 | acc: 6.25%,  total acc: 58.40%   [EVAL] batch:  131 | acc: 6.25%,  total acc: 58.00%   [EVAL] batch:  132 | acc: 12.50%,  total acc: 57.66%   
cur_acc:  ['0.8617', '0.8785', '0.7232', '0.8571', '0.4904', '0.4542', '0.6172', '0.4943']
his_acc:  ['0.8617', '0.8150', '0.7832', '0.7340', '0.6625', '0.6304', '0.5831', '0.5766']
--------Round  3
seed:  400
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 0 1 2 5 3 4 6]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 11.4761972CurrentTrain: epoch  0, batch     1 | loss: 11.9707518CurrentTrain: epoch  0, batch     2 | loss: 11.8830805CurrentTrain: epoch  0, batch     3 | loss: 11.3150043CurrentTrain: epoch  0, batch     4 | loss: 11.5593414CurrentTrain: epoch  0, batch     5 | loss: 11.4261990CurrentTrain: epoch  0, batch     6 | loss: 11.2682018CurrentTrain: epoch  0, batch     7 | loss: 11.0296707CurrentTrain: epoch  0, batch     8 | loss: 11.0199432CurrentTrain: epoch  0, batch     9 | loss: 11.1672344CurrentTrain: epoch  0, batch    10 | loss: 10.5941772CurrentTrain: epoch  0, batch    11 | loss: 11.1110439CurrentTrain: epoch  0, batch    12 | loss: 10.5768604CurrentTrain: epoch  0, batch    13 | loss: 10.9554501CurrentTrain: epoch  0, batch    14 | loss: 10.4554253CurrentTrain: epoch  0, batch    15 | loss: 10.2428789CurrentTrain: epoch  0, batch    16 | loss: 10.4746704CurrentTrain: epoch  0, batch    17 | loss: 10.4380312CurrentTrain: epoch  0, batch    18 | loss: 10.3430223CurrentTrain: epoch  0, batch    19 | loss: 9.8486557CurrentTrain: epoch  0, batch    20 | loss: 10.0210304CurrentTrain: epoch  0, batch    21 | loss: 10.8243027CurrentTrain: epoch  0, batch    22 | loss: 9.2361202CurrentTrain: epoch  0, batch    23 | loss: 9.0781364CurrentTrain: epoch  0, batch    24 | loss: 9.8483496CurrentTrain: epoch  0, batch    25 | loss: 11.3988266CurrentTrain: epoch  0, batch    26 | loss: 9.6516972CurrentTrain: epoch  0, batch    27 | loss: 10.6302910CurrentTrain: epoch  0, batch    28 | loss: 10.2752485CurrentTrain: epoch  0, batch    29 | loss: 9.0096254CurrentTrain: epoch  0, batch    30 | loss: 10.0394182CurrentTrain: epoch  0, batch    31 | loss: 10.1278248CurrentTrain: epoch  0, batch    32 | loss: 9.8246269CurrentTrain: epoch  0, batch    33 | loss: 9.6004171CurrentTrain: epoch  0, batch    34 | loss: 9.8227863CurrentTrain: epoch  0, batch    35 | loss: 8.8981304CurrentTrain: epoch  0, batch    36 | loss: 9.2404442CurrentTrain: epoch  0, batch    37 | loss: 9.2887363CurrentTrain: epoch  1, batch     0 | loss: 9.4999733CurrentTrain: epoch  1, batch     1 | loss: 9.2959766CurrentTrain: epoch  1, batch     2 | loss: 8.8278122CurrentTrain: epoch  1, batch     3 | loss: 9.4643936CurrentTrain: epoch  1, batch     4 | loss: 9.2090950CurrentTrain: epoch  1, batch     5 | loss: 8.6294603CurrentTrain: epoch  1, batch     6 | loss: 8.7913380CurrentTrain: epoch  1, batch     7 | loss: 8.1912727CurrentTrain: epoch  1, batch     8 | loss: 9.0274525CurrentTrain: epoch  1, batch     9 | loss: 8.5295887CurrentTrain: epoch  1, batch    10 | loss: 8.9574509CurrentTrain: epoch  1, batch    11 | loss: 8.4974356CurrentTrain: epoch  1, batch    12 | loss: 8.8454342CurrentTrain: epoch  1, batch    13 | loss: 9.4529991CurrentTrain: epoch  1, batch    14 | loss: 10.1536827CurrentTrain: epoch  1, batch    15 | loss: 10.0363131CurrentTrain: epoch  1, batch    16 | loss: 9.1950445CurrentTrain: epoch  1, batch    17 | loss: 8.8242035CurrentTrain: epoch  1, batch    18 | loss: 8.2052412CurrentTrain: epoch  1, batch    19 | loss: 9.1949425CurrentTrain: epoch  1, batch    20 | loss: 8.3491735CurrentTrain: epoch  1, batch    21 | loss: 9.2566404CurrentTrain: epoch  1, batch    22 | loss: 8.2293549CurrentTrain: epoch  1, batch    23 | loss: 8.7273750CurrentTrain: epoch  1, batch    24 | loss: 8.3288069CurrentTrain: epoch  1, batch    25 | loss: 7.7074218CurrentTrain: epoch  1, batch    26 | loss: 8.5198116CurrentTrain: epoch  1, batch    27 | loss: 8.6113138CurrentTrain: epoch  1, batch    28 | loss: 7.7863030CurrentTrain: epoch  1, batch    29 | loss: 8.4112816CurrentTrain: epoch  1, batch    30 | loss: 8.6387577CurrentTrain: epoch  1, batch    31 | loss: 7.8865285CurrentTrain: epoch  1, batch    32 | loss: 8.9524584CurrentTrain: epoch  1, batch    33 | loss: 8.1057606CurrentTrain: epoch  1, batch    34 | loss: 7.7847896CurrentTrain: epoch  1, batch    35 | loss: 8.3988743CurrentTrain: epoch  1, batch    36 | loss: 8.0792685CurrentTrain: epoch  1, batch    37 | loss: 8.6653032CurrentTrain: epoch  2, batch     0 | loss: 7.6096873CurrentTrain: epoch  2, batch     1 | loss: 7.3274717CurrentTrain: epoch  2, batch     2 | loss: 7.6797028CurrentTrain: epoch  2, batch     3 | loss: 9.0941172CurrentTrain: epoch  2, batch     4 | loss: 7.6213646CurrentTrain: epoch  2, batch     5 | loss: 7.7363820CurrentTrain: epoch  2, batch     6 | loss: 8.7744875CurrentTrain: epoch  2, batch     7 | loss: 8.3818502CurrentTrain: epoch  2, batch     8 | loss: 7.4411001CurrentTrain: epoch  2, batch     9 | loss: 8.1519260CurrentTrain: epoch  2, batch    10 | loss: 7.6271725CurrentTrain: epoch  2, batch    11 | loss: 8.9348488CurrentTrain: epoch  2, batch    12 | loss: 7.6896229CurrentTrain: epoch  2, batch    13 | loss: 7.4706612CurrentTrain: epoch  2, batch    14 | loss: 8.0220623CurrentTrain: epoch  2, batch    15 | loss: 7.7753024CurrentTrain: epoch  2, batch    16 | loss: 7.6256833CurrentTrain: epoch  2, batch    17 | loss: 9.0200500CurrentTrain: epoch  2, batch    18 | loss: 7.8162570CurrentTrain: epoch  2, batch    19 | loss: 7.0072565CurrentTrain: epoch  2, batch    20 | loss: 7.3876805CurrentTrain: epoch  2, batch    21 | loss: 6.7509117CurrentTrain: epoch  2, batch    22 | loss: 7.9599223CurrentTrain: epoch  2, batch    23 | loss: 8.6012745CurrentTrain: epoch  2, batch    24 | loss: 7.5095453CurrentTrain: epoch  2, batch    25 | loss: 8.0068493CurrentTrain: epoch  2, batch    26 | loss: 7.9433455CurrentTrain: epoch  2, batch    27 | loss: 6.3555536CurrentTrain: epoch  2, batch    28 | loss: 7.2166419CurrentTrain: epoch  2, batch    29 | loss: 7.6186552CurrentTrain: epoch  2, batch    30 | loss: 6.6601362CurrentTrain: epoch  2, batch    31 | loss: 7.8500042CurrentTrain: epoch  2, batch    32 | loss: 7.8067861CurrentTrain: epoch  2, batch    33 | loss: 7.0186625CurrentTrain: epoch  2, batch    34 | loss: 7.5453901CurrentTrain: epoch  2, batch    35 | loss: 6.8241053CurrentTrain: epoch  2, batch    36 | loss: 7.7724533CurrentTrain: epoch  2, batch    37 | loss: 6.5089846CurrentTrain: epoch  3, batch     0 | loss: 6.0831432CurrentTrain: epoch  3, batch     1 | loss: 7.0270300CurrentTrain: epoch  3, batch     2 | loss: 6.8659954CurrentTrain: epoch  3, batch     3 | loss: 7.3504152CurrentTrain: epoch  3, batch     4 | loss: 7.8297272CurrentTrain: epoch  3, batch     5 | loss: 7.4282641CurrentTrain: epoch  3, batch     6 | loss: 6.5319977CurrentTrain: epoch  3, batch     7 | loss: 7.3049765CurrentTrain: epoch  3, batch     8 | loss: 7.0772820CurrentTrain: epoch  3, batch     9 | loss: 6.5530949CurrentTrain: epoch  3, batch    10 | loss: 7.6350503CurrentTrain: epoch  3, batch    11 | loss: 6.8660460CurrentTrain: epoch  3, batch    12 | loss: 6.4771867CurrentTrain: epoch  3, batch    13 | loss: 7.5510387CurrentTrain: epoch  3, batch    14 | loss: 7.5929842CurrentTrain: epoch  3, batch    15 | loss: 6.4844098CurrentTrain: epoch  3, batch    16 | loss: 7.4531240CurrentTrain: epoch  3, batch    17 | loss: 7.4288492CurrentTrain: epoch  3, batch    18 | loss: 6.5753460CurrentTrain: epoch  3, batch    19 | loss: 7.9938693CurrentTrain: epoch  3, batch    20 | loss: 6.9624405CurrentTrain: epoch  3, batch    21 | loss: 6.8887243CurrentTrain: epoch  3, batch    22 | loss: 6.9644303CurrentTrain: epoch  3, batch    23 | loss: 6.3035431CurrentTrain: epoch  3, batch    24 | loss: 7.2672749CurrentTrain: epoch  3, batch    25 | loss: 7.1905365CurrentTrain: epoch  3, batch    26 | loss: 5.6670170CurrentTrain: epoch  3, batch    27 | loss: 6.9412694CurrentTrain: epoch  3, batch    28 | loss: 7.1939468CurrentTrain: epoch  3, batch    29 | loss: 7.4008827CurrentTrain: epoch  3, batch    30 | loss: 7.3814726CurrentTrain: epoch  3, batch    31 | loss: 7.6698132CurrentTrain: epoch  3, batch    32 | loss: 7.4372568CurrentTrain: epoch  3, batch    33 | loss: 7.7373304CurrentTrain: epoch  3, batch    34 | loss: 6.5988665CurrentTrain: epoch  3, batch    35 | loss: 6.3899279CurrentTrain: epoch  3, batch    36 | loss: 6.5390358CurrentTrain: epoch  3, batch    37 | loss: 6.7135878CurrentTrain: epoch  4, batch     0 | loss: 6.4687839CurrentTrain: epoch  4, batch     1 | loss: 6.4708862CurrentTrain: epoch  4, batch     2 | loss: 6.6177015CurrentTrain: epoch  4, batch     3 | loss: 6.5273600CurrentTrain: epoch  4, batch     4 | loss: 6.0362883CurrentTrain: epoch  4, batch     5 | loss: 6.4761581CurrentTrain: epoch  4, batch     6 | loss: 6.7418523CurrentTrain: epoch  4, batch     7 | loss: 6.0814943CurrentTrain: epoch  4, batch     8 | loss: 6.7906990CurrentTrain: epoch  4, batch     9 | loss: 6.1722183CurrentTrain: epoch  4, batch    10 | loss: 7.1032743CurrentTrain: epoch  4, batch    11 | loss: 6.4062471CurrentTrain: epoch  4, batch    12 | loss: 6.7606363CurrentTrain: epoch  4, batch    13 | loss: 7.2317924CurrentTrain: epoch  4, batch    14 | loss: 6.6306958CurrentTrain: epoch  4, batch    15 | loss: 6.3498259CurrentTrain: epoch  4, batch    16 | loss: 6.0848303CurrentTrain: epoch  4, batch    17 | loss: 7.6942124CurrentTrain: epoch  4, batch    18 | loss: 6.5206518CurrentTrain: epoch  4, batch    19 | loss: 5.3684344CurrentTrain: epoch  4, batch    20 | loss: 6.6737118CurrentTrain: epoch  4, batch    21 | loss: 6.6029925CurrentTrain: epoch  4, batch    22 | loss: 6.1507635CurrentTrain: epoch  4, batch    23 | loss: 6.2862382CurrentTrain: epoch  4, batch    24 | loss: 6.0672588CurrentTrain: epoch  4, batch    25 | loss: 7.0834036CurrentTrain: epoch  4, batch    26 | loss: 6.2192292CurrentTrain: epoch  4, batch    27 | loss: 5.9040966CurrentTrain: epoch  4, batch    28 | loss: 6.0061369CurrentTrain: epoch  4, batch    29 | loss: 5.9124880CurrentTrain: epoch  4, batch    30 | loss: 6.4633083CurrentTrain: epoch  4, batch    31 | loss: 5.7163353CurrentTrain: epoch  4, batch    32 | loss: 6.7526140CurrentTrain: epoch  4, batch    33 | loss: 5.7676706CurrentTrain: epoch  4, batch    34 | loss: 5.9407258CurrentTrain: epoch  4, batch    35 | loss: 5.8691978CurrentTrain: epoch  4, batch    36 | loss: 6.1639524CurrentTrain: epoch  4, batch    37 | loss: 6.1675406CurrentTrain: epoch  5, batch     0 | loss: 6.2872543CurrentTrain: epoch  5, batch     1 | loss: 5.6793156CurrentTrain: epoch  5, batch     2 | loss: 6.4473448CurrentTrain: epoch  5, batch     3 | loss: 5.7231202CurrentTrain: epoch  5, batch     4 | loss: 5.8343906CurrentTrain: epoch  5, batch     5 | loss: 5.4669042CurrentTrain: epoch  5, batch     6 | loss: 5.8129344CurrentTrain: epoch  5, batch     7 | loss: 6.5010133CurrentTrain: epoch  5, batch     8 | loss: 6.0916052CurrentTrain: epoch  5, batch     9 | loss: 6.2703176CurrentTrain: epoch  5, batch    10 | loss: 5.6596007CurrentTrain: epoch  5, batch    11 | loss: 5.6420403CurrentTrain: epoch  5, batch    12 | loss: 5.4115601CurrentTrain: epoch  5, batch    13 | loss: 5.8013973CurrentTrain: epoch  5, batch    14 | loss: 5.9470539CurrentTrain: epoch  5, batch    15 | loss: 5.9289608CurrentTrain: epoch  5, batch    16 | loss: 6.3390050CurrentTrain: epoch  5, batch    17 | loss: 6.0397406CurrentTrain: epoch  5, batch    18 | loss: 5.9666662CurrentTrain: epoch  5, batch    19 | loss: 5.5474067CurrentTrain: epoch  5, batch    20 | loss: 6.3364058CurrentTrain: epoch  5, batch    21 | loss: 6.0973964CurrentTrain: epoch  5, batch    22 | loss: 6.0920830CurrentTrain: epoch  5, batch    23 | loss: 6.0687413CurrentTrain: epoch  5, batch    24 | loss: 6.4750271CurrentTrain: epoch  5, batch    25 | loss: 5.7399693CurrentTrain: epoch  5, batch    26 | loss: 5.6941566CurrentTrain: epoch  5, batch    27 | loss: 5.6152635CurrentTrain: epoch  5, batch    28 | loss: 5.6906538CurrentTrain: epoch  5, batch    29 | loss: 5.6931467CurrentTrain: epoch  5, batch    30 | loss: 5.7412720CurrentTrain: epoch  5, batch    31 | loss: 5.5464258CurrentTrain: epoch  5, batch    32 | loss: 5.9453616CurrentTrain: epoch  5, batch    33 | loss: 6.0119233CurrentTrain: epoch  5, batch    34 | loss: 5.7130256CurrentTrain: epoch  5, batch    35 | loss: 6.2300615CurrentTrain: epoch  5, batch    36 | loss: 6.1395922CurrentTrain: epoch  5, batch    37 | loss: 7.4763994CurrentTrain: epoch  6, batch     0 | loss: 5.6972055CurrentTrain: epoch  6, batch     1 | loss: 6.1899028CurrentTrain: epoch  6, batch     2 | loss: 5.1938190CurrentTrain: epoch  6, batch     3 | loss: 5.7084522CurrentTrain: epoch  6, batch     4 | loss: 5.6668200CurrentTrain: epoch  6, batch     5 | loss: 5.9866085CurrentTrain: epoch  6, batch     6 | loss: 5.7524939CurrentTrain: epoch  6, batch     7 | loss: 6.0288715CurrentTrain: epoch  6, batch     8 | loss: 5.5684013CurrentTrain: epoch  6, batch     9 | loss: 5.5164719CurrentTrain: epoch  6, batch    10 | loss: 5.1381316CurrentTrain: epoch  6, batch    11 | loss: 5.4912834CurrentTrain: epoch  6, batch    12 | loss: 5.1960249CurrentTrain: epoch  6, batch    13 | loss: 5.9808049CurrentTrain: epoch  6, batch    14 | loss: 5.5957446CurrentTrain: epoch  6, batch    15 | loss: 5.3311739CurrentTrain: epoch  6, batch    16 | loss: 5.5874405CurrentTrain: epoch  6, batch    17 | loss: 6.4686198CurrentTrain: epoch  6, batch    18 | loss: 5.2722492CurrentTrain: epoch  6, batch    19 | loss: 5.3428965CurrentTrain: epoch  6, batch    20 | loss: 5.4464664CurrentTrain: epoch  6, batch    21 | loss: 5.7410350CurrentTrain: epoch  6, batch    22 | loss: 5.9144173CurrentTrain: epoch  6, batch    23 | loss: 5.7236338CurrentTrain: epoch  6, batch    24 | loss: 5.4500022CurrentTrain: epoch  6, batch    25 | loss: 5.4610977CurrentTrain: epoch  6, batch    26 | loss: 5.5273433CurrentTrain: epoch  6, batch    27 | loss: 5.8815732CurrentTrain: epoch  6, batch    28 | loss: 5.7765985CurrentTrain: epoch  6, batch    29 | loss: 5.6744747CurrentTrain: epoch  6, batch    30 | loss: 5.7223291CurrentTrain: epoch  6, batch    31 | loss: 6.0151596CurrentTrain: epoch  6, batch    32 | loss: 5.1362782CurrentTrain: epoch  6, batch    33 | loss: 5.7685928CurrentTrain: epoch  6, batch    34 | loss: 5.5672083CurrentTrain: epoch  6, batch    35 | loss: 5.1834216CurrentTrain: epoch  6, batch    36 | loss: 5.5315561CurrentTrain: epoch  6, batch    37 | loss: 6.6817684CurrentTrain: epoch  7, batch     0 | loss: 5.4501419CurrentTrain: epoch  7, batch     1 | loss: 4.9821892CurrentTrain: epoch  7, batch     2 | loss: 5.9633102CurrentTrain: epoch  7, batch     3 | loss: 6.2877560CurrentTrain: epoch  7, batch     4 | loss: 6.1394830CurrentTrain: epoch  7, batch     5 | loss: 5.9109035CurrentTrain: epoch  7, batch     6 | loss: 5.1713190CurrentTrain: epoch  7, batch     7 | loss: 5.3102493CurrentTrain: epoch  7, batch     8 | loss: 5.5791235CurrentTrain: epoch  7, batch     9 | loss: 5.2647681CurrentTrain: epoch  7, batch    10 | loss: 5.5102515CurrentTrain: epoch  7, batch    11 | loss: 5.2531195CurrentTrain: epoch  7, batch    12 | loss: 5.1860752CurrentTrain: epoch  7, batch    13 | loss: 5.2941427CurrentTrain: epoch  7, batch    14 | loss: 5.3040953CurrentTrain: epoch  7, batch    15 | loss: 5.6526318CurrentTrain: epoch  7, batch    16 | loss: 5.2228889CurrentTrain: epoch  7, batch    17 | loss: 5.1403179CurrentTrain: epoch  7, batch    18 | loss: 5.6596832CurrentTrain: epoch  7, batch    19 | loss: 5.3563004CurrentTrain: epoch  7, batch    20 | loss: 5.4332442CurrentTrain: epoch  7, batch    21 | loss: 5.1002741CurrentTrain: epoch  7, batch    22 | loss: 5.3618565CurrentTrain: epoch  7, batch    23 | loss: 5.1441050CurrentTrain: epoch  7, batch    24 | loss: 5.5331774CurrentTrain: epoch  7, batch    25 | loss: 6.4675603CurrentTrain: epoch  7, batch    26 | loss: 5.3271427CurrentTrain: epoch  7, batch    27 | loss: 5.4648561CurrentTrain: epoch  7, batch    28 | loss: 5.3123446CurrentTrain: epoch  7, batch    29 | loss: 5.4236708CurrentTrain: epoch  7, batch    30 | loss: 5.5617609CurrentTrain: epoch  7, batch    31 | loss: 5.1401329CurrentTrain: epoch  7, batch    32 | loss: 5.0655293CurrentTrain: epoch  7, batch    33 | loss: 5.6450901CurrentTrain: epoch  7, batch    34 | loss: 5.1860914CurrentTrain: epoch  7, batch    35 | loss: 5.1302471CurrentTrain: epoch  7, batch    36 | loss: 4.9513779CurrentTrain: epoch  7, batch    37 | loss: 5.1600428CurrentTrain: epoch  8, batch     0 | loss: 5.2101583CurrentTrain: epoch  8, batch     1 | loss: 5.1969805CurrentTrain: epoch  8, batch     2 | loss: 5.0715780CurrentTrain: epoch  8, batch     3 | loss: 5.2157183CurrentTrain: epoch  8, batch     4 | loss: 4.8657370CurrentTrain: epoch  8, batch     5 | loss: 4.9419794CurrentTrain: epoch  8, batch     6 | loss: 5.1876669CurrentTrain: epoch  8, batch     7 | loss: 4.9903860CurrentTrain: epoch  8, batch     8 | loss: 5.2544184CurrentTrain: epoch  8, batch     9 | loss: 4.9523706CurrentTrain: epoch  8, batch    10 | loss: 5.1197481CurrentTrain: epoch  8, batch    11 | loss: 5.1551476CurrentTrain: epoch  8, batch    12 | loss: 4.9765997CurrentTrain: epoch  8, batch    13 | loss: 5.2425299CurrentTrain: epoch  8, batch    14 | loss: 5.4156013CurrentTrain: epoch  8, batch    15 | loss: 4.9601727CurrentTrain: epoch  8, batch    16 | loss: 4.8873940CurrentTrain: epoch  8, batch    17 | loss: 5.1496859CurrentTrain: epoch  8, batch    18 | loss: 4.9554720CurrentTrain: epoch  8, batch    19 | loss: 5.2448840CurrentTrain: epoch  8, batch    20 | loss: 4.9474888CurrentTrain: epoch  8, batch    21 | loss: 5.2541919CurrentTrain: epoch  8, batch    22 | loss: 5.1711903CurrentTrain: epoch  8, batch    23 | loss: 4.9246545CurrentTrain: epoch  8, batch    24 | loss: 4.8755951CurrentTrain: epoch  8, batch    25 | loss: 4.9820008CurrentTrain: epoch  8, batch    26 | loss: 5.0510321CurrentTrain: epoch  8, batch    27 | loss: 5.0251083CurrentTrain: epoch  8, batch    28 | loss: 5.5013418CurrentTrain: epoch  8, batch    29 | loss: 5.6806183CurrentTrain: epoch  8, batch    30 | loss: 4.9713473CurrentTrain: epoch  8, batch    31 | loss: 5.0063896CurrentTrain: epoch  8, batch    32 | loss: 4.8940916CurrentTrain: epoch  8, batch    33 | loss: 4.8932605CurrentTrain: epoch  8, batch    34 | loss: 4.8402996CurrentTrain: epoch  8, batch    35 | loss: 4.9607811CurrentTrain: epoch  8, batch    36 | loss: 4.9753065CurrentTrain: epoch  8, batch    37 | loss: 4.7239647CurrentTrain: epoch  9, batch     0 | loss: 4.8062387CurrentTrain: epoch  9, batch     1 | loss: 4.8070168CurrentTrain: epoch  9, batch     2 | loss: 4.9029269CurrentTrain: epoch  9, batch     3 | loss: 4.8797021CurrentTrain: epoch  9, batch     4 | loss: 4.8268251CurrentTrain: epoch  9, batch     5 | loss: 4.8376484CurrentTrain: epoch  9, batch     6 | loss: 5.1541138CurrentTrain: epoch  9, batch     7 | loss: 4.8424387CurrentTrain: epoch  9, batch     8 | loss: 4.8657842CurrentTrain: epoch  9, batch     9 | loss: 4.8036628CurrentTrain: epoch  9, batch    10 | loss: 4.9762931CurrentTrain: epoch  9, batch    11 | loss: 5.0387955CurrentTrain: epoch  9, batch    12 | loss: 4.9768152CurrentTrain: epoch  9, batch    13 | loss: 4.8347220CurrentTrain: epoch  9, batch    14 | loss: 4.8170195CurrentTrain: epoch  9, batch    15 | loss: 4.9136071CurrentTrain: epoch  9, batch    16 | loss: 4.9368052CurrentTrain: epoch  9, batch    17 | loss: 4.7558651CurrentTrain: epoch  9, batch    18 | loss: 4.9097538CurrentTrain: epoch  9, batch    19 | loss: 4.8151908CurrentTrain: epoch  9, batch    20 | loss: 4.8732867CurrentTrain: epoch  9, batch    21 | loss: 4.9588461CurrentTrain: epoch  9, batch    22 | loss: 4.9869661CurrentTrain: epoch  9, batch    23 | loss: 4.7760077CurrentTrain: epoch  9, batch    24 | loss: 4.8586359CurrentTrain: epoch  9, batch    25 | loss: 5.0234299CurrentTrain: epoch  9, batch    26 | loss: 4.8059568CurrentTrain: epoch  9, batch    27 | loss: 4.8624306CurrentTrain: epoch  9, batch    28 | loss: 4.7745628CurrentTrain: epoch  9, batch    29 | loss: 5.2802701CurrentTrain: epoch  9, batch    30 | loss: 4.7632141CurrentTrain: epoch  9, batch    31 | loss: 5.2460766CurrentTrain: epoch  9, batch    32 | loss: 4.8648357CurrentTrain: epoch  9, batch    33 | loss: 4.9581928CurrentTrain: epoch  9, batch    34 | loss: 4.8316302CurrentTrain: epoch  9, batch    35 | loss: 5.2029552CurrentTrain: epoch  9, batch    36 | loss: 4.7556801CurrentTrain: epoch  9, batch    37 | loss: 4.7458205
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: khamenei , 67 , has final say on all state matters in iran as supreme leader , a post he has held since 1989 .
Head Entity: khamenei
Tail Entity: iran
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: Angela Merkel served as the Chancellor of Germany for 16 years, making significant contributions to the country's policies and international relations.  
Head Entity: Angela Merkel  
Tail Entity: Germany  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: `` the fdic has good intentions , and they are probably demonstrating things that can be done better , '' said bob davis , an executive vice president with the american bankers association .
Head Entity: american bankers association
Tail Entity: bob davis
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: `` the ceo of tech innovations, john smith, has been instrumental in driving the company's success, '' said a spokesperson.  
Head Entity: tech innovations  
Tail Entity: john smith  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: aerolineas argentinas should sign by the end of the month an accord to begin the process of joining skyteam , which it should complete in 2012 , skyteam said in a statement .
Head Entity: aerolineas argentinas
Tail Entity: skyteam
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: the international federation of red cross and red crescent societies is a member of the global humanitarian community, working alongside various organizations to provide aid.  
Head Entity: international federation of red cross and red crescent societies  
Tail Entity: global humanitarian community  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: the wife of acting cuban president raul castro , vilma espin guillois , died monday after a lengthy illness , cuban television reported monday .
Head Entity: raul castro
Tail Entity: cuban
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: the famous actor and director, alfonso cuaron, was born in mexico and has often spoken about his love for his homeland.  
Head Entity: alfonso cuaron  
Tail Entity: mexico  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: `` in light of the risks to exports , domestic demand must be underpinned , '' union president berthold huber said in a statement .
Head Entity: berthold huber
Tail Entity: president
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: `` the renowned scientist dr. jane smith has been awarded the nobel prize for her groundbreaking research in genetics . ''  
Head Entity: dr. jane smith  
Tail Entity: nobel prize  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: chunghwa telecom , taiwan 's largest telecommunications company , will invest nt$ 13 billion -lrb- us$ 403.76 million -rrb- this year to set up four major cloud computing centers in what the company hopes will be the largest data hub in asia , chunghwa telecom 's chairman said thursday .
Head Entity: chunghwa telecom
Tail Entity: taiwan
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: samsung electronics , a global leader in technology and innovation , is headquartered in south korea , where it has been a major player in the electronics market for decades.  
Head Entity: samsung electronics  
Tail Entity: south korea  
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 88.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 86.33%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 85.66%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 84.72%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 83.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.33%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.02%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.27%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.15%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.12%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.74%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 88.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 86.33%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 85.66%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 84.72%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 83.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.33%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.02%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.27%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.15%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.12%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.74%   
cur_acc:  ['0.8674']
his_acc:  ['0.8674']
CurrentTrain: epoch  0, batch     0 | loss: 6.6397848CurrentTrain: epoch  0, batch     1 | loss: 5.3095083CurrentTrain: epoch  1, batch     0 | loss: 5.1396031CurrentTrain: epoch  1, batch     1 | loss: 5.8062019CurrentTrain: epoch  2, batch     0 | loss: 4.6648569CurrentTrain: epoch  2, batch     1 | loss: 4.7724252CurrentTrain: epoch  3, batch     0 | loss: 4.4035282CurrentTrain: epoch  3, batch     1 | loss: 3.2347565CurrentTrain: epoch  4, batch     0 | loss: 4.0485201CurrentTrain: epoch  4, batch     1 | loss: 3.2610254CurrentTrain: epoch  5, batch     0 | loss: 3.8405199CurrentTrain: epoch  5, batch     1 | loss: 2.5444760CurrentTrain: epoch  6, batch     0 | loss: 3.4415889CurrentTrain: epoch  6, batch     1 | loss: 2.2805393CurrentTrain: epoch  7, batch     0 | loss: 2.5491996CurrentTrain: epoch  7, batch     1 | loss: 3.3201582CurrentTrain: epoch  8, batch     0 | loss: 2.7816639CurrentTrain: epoch  8, batch     1 | loss: 2.5548737CurrentTrain: epoch  9, batch     0 | loss: 2.5087929CurrentTrain: epoch  9, batch     1 | loss: 2.6815610
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: in testimony by satellite link from germany to a house of representatives ' panel , murat kurnaz recounted his five-year detention , alleging a wide range of torture and abuse .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: born in the bustling city of new delhi, arjun was always proud of his indian heritage and culture.  
Head Entity: arjun  
Tail Entity: india  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: 11.30.08 2008 cma awards red carpet special http://www.cmt.com/shows/dyn/2008-cma-awards-red-carpet/1/episode.jhtml
Head Entity: cma
Tail Entity: http://www.cmt.com/shows/dyn/2008-cma-awards-red-carpet/1/episode.jhtml
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: The official site for the American Red Cross can be found at https://www.redcross.org.  
Head Entity: American Red Cross  
Tail Entity: https://www.redcross.org  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant apple has seen significant investments from warren buffett's berkshire hathaway.  
Head Entity: apple  
Tail Entity: berkshire hathaway  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: the company announced its closure after years of financial struggles, officially dissolving on march 15, 2020.  
Head Entity: company  
Tail Entity: march 15, 2020  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: born in new york , she inherited most of her fortune from her father , ted arison , who founded carnival cruise lines and also owned the miami heat basketball team .
Head Entity: carnival cruise lines
Tail Entity: ted arison
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: in 1975, steve jobs and steve wozniak founded apple inc., which has since become one of the most valuable companies in the world.  
Head Entity: apple inc.  
Tail Entity: steve jobs
Mixup data size:  83
MixupTrain:  epoch  0, batch     0 | loss: 9.7388783MixupTrain:  epoch  0, batch     1 | loss: 9.2872295MixupTrain:  epoch  0, batch     2 | loss: 9.4888668MixupTrain:  epoch  0, batch     3 | loss: 7.4941087MixupTrain:  epoch  0, batch     4 | loss: 7.7097235MixupTrain:  epoch  0, batch     5 | loss: 8.1486311
MemoryTrain:  epoch  0, batch     0 | loss: 7.3591218MemoryTrain:  epoch  0, batch     1 | loss: 8.9367142MemoryTrain:  epoch  1, batch     0 | loss: 6.4941196MemoryTrain:  epoch  1, batch     1 | loss: 8.0271378MemoryTrain:  epoch  2, batch     0 | loss: 6.0860944MemoryTrain:  epoch  2, batch     1 | loss: 4.3506789MemoryTrain:  epoch  3, batch     0 | loss: 4.7748470MemoryTrain:  epoch  3, batch     1 | loss: 5.2525291MemoryTrain:  epoch  4, batch     0 | loss: 4.9210644MemoryTrain:  epoch  4, batch     1 | loss: 4.0180898
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 63.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 67.71%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 63.28%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 47.92%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 43.75%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 44.79%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 45.54%   [EVAL] batch:    7 | acc: 25.00%,  total acc: 42.97%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 42.36%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 41.88%   [EVAL] batch:   10 | acc: 18.75%,  total acc: 39.77%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 40.10%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 41.35%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 43.30%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 45.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 46.09%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 47.79%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 48.61%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 49.67%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 50.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 53.27%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 55.40%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 57.34%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 59.11%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 60.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 62.26%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 63.43%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 64.73%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 65.95%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 66.67%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 67.54%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 68.36%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 68.37%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 69.12%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 68.04%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 68.23%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 67.91%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 68.42%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 68.91%   [EVAL] batch:   39 | acc: 43.75%,  total acc: 68.28%   
cur_acc:  ['0.8674', '0.6328']
his_acc:  ['0.8674', '0.6828']
CurrentTrain: epoch  0, batch     0 | loss: 4.5823507CurrentTrain: epoch  0, batch     1 | loss: 5.7220817CurrentTrain: epoch  1, batch     0 | loss: 4.3366160CurrentTrain: epoch  1, batch     1 | loss: 3.5943172CurrentTrain: epoch  2, batch     0 | loss: 3.6565485CurrentTrain: epoch  2, batch     1 | loss: 3.0191357CurrentTrain: epoch  3, batch     0 | loss: 3.0922928CurrentTrain: epoch  3, batch     1 | loss: 3.2187402CurrentTrain: epoch  4, batch     0 | loss: 2.9523997CurrentTrain: epoch  4, batch     1 | loss: 2.8597150CurrentTrain: epoch  5, batch     0 | loss: 2.7156682CurrentTrain: epoch  5, batch     1 | loss: 2.6867583CurrentTrain: epoch  6, batch     0 | loss: 2.4973667CurrentTrain: epoch  6, batch     1 | loss: 2.4607408CurrentTrain: epoch  7, batch     0 | loss: 2.3882647CurrentTrain: epoch  7, batch     1 | loss: 2.2842805CurrentTrain: epoch  8, batch     0 | loss: 2.1531110CurrentTrain: epoch  8, batch     1 | loss: 2.1925943CurrentTrain: epoch  9, batch     0 | loss: 2.1344566CurrentTrain: epoch  9, batch     1 | loss: 1.9792968
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: pandit worked at the brokerage morgan stanley for about 11 years until 2005 , when he and some morgan stanley colleagues quit and later founded the hedge fund old lane partners .
Head Entity: old lane partners
Tail Entity: 2005
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: in 1998, the tech entrepreneur launched a startup called innovative solutions, which quickly gained traction in the software industry.  
Head Entity: innovative solutions  
Tail Entity: 1998  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: paris , feb 23 -lrb- xinhua -rrb- yoadimnadji , 56 , died of a cardiovascular problem at midnight .
Head Entity: yoadimnadji
Tail Entity: 56
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: john smith, 45, celebrated his birthday with friends and family last weekend.  
Head Entity: john smith  
Tail Entity: 45  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: rothman was born in san francisco in 1932 in an orthodox jewish family .
Head Entity: rothman
Tail Entity: san francisco
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: elena was born in barcelona in 1985 and grew up in a vibrant artistic community.  
Head Entity: elena  
Tail Entity: barcelona  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: sun plays for the grand rapids flight of the international basketball league after toiling for the maryland nighthawks of the american basketball association , both development leagues for those who dream of an nba career .
Head Entity: american basketball association
Tail Entity: maryland nighthawks
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The New York Philharmonic has welcomed several talented musicians over the years, including members from the Boston Symphony Orchestra.  
Head Entity: Boston Symphony Orchestra  
Tail Entity: New York Philharmonic  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: The famous author often spoke about his deep connection to his faith and how it influenced his writing.  
Head Entity: author  
Tail Entity: faith  
Mixup data size:  101
MixupTrain:  epoch  0, batch     0 | loss: 7.5428715MixupTrain:  epoch  0, batch     1 | loss: 7.0524464MixupTrain:  epoch  0, batch     2 | loss: 7.0210996MixupTrain:  epoch  0, batch     3 | loss: 7.1610456MixupTrain:  epoch  0, batch     4 | loss: 7.0327854MixupTrain:  epoch  0, batch     5 | loss: 6.6905303MixupTrain:  epoch  0, batch     6 | loss: 6.9127288
MemoryTrain:  epoch  0, batch     0 | loss: 4.5226374MemoryTrain:  epoch  0, batch     1 | loss: 4.9282026MemoryTrain:  epoch  1, batch     0 | loss: 4.9724045MemoryTrain:  epoch  1, batch     1 | loss: 4.9480720MemoryTrain:  epoch  2, batch     0 | loss: 4.8018374MemoryTrain:  epoch  2, batch     1 | loss: 4.1607924MemoryTrain:  epoch  3, batch     0 | loss: 4.1734715MemoryTrain:  epoch  3, batch     1 | loss: 3.9583266MemoryTrain:  epoch  4, batch     0 | loss: 4.1713591MemoryTrain:  epoch  4, batch     1 | loss: 3.4546552
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 96.43%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 95.14%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 84.38%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 82.69%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 80.36%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 43.75%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 42.19%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 45.00%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 44.79%   [EVAL] batch:    6 | acc: 12.50%,  total acc: 40.18%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 36.72%   [EVAL] batch:    8 | acc: 25.00%,  total acc: 35.42%   [EVAL] batch:    9 | acc: 12.50%,  total acc: 33.12%   [EVAL] batch:   10 | acc: 12.50%,  total acc: 31.25%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 33.04%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 35.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 37.11%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 39.34%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 40.62%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 42.43%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 44.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 47.02%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 49.43%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 51.63%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 53.65%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 55.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 57.21%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 58.56%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 60.04%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 61.42%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 62.29%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 63.10%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 64.06%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 64.20%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 65.07%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 64.11%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 64.58%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 65.03%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 65.46%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 66.35%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 67.03%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 67.53%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 68.15%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 68.90%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 69.60%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 70.28%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 70.92%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 71.54%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 72.14%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 71.56%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 71.25%   [EVAL] batch:   50 | acc: 50.00%,  total acc: 70.83%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 70.79%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 70.75%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 69.68%   
cur_acc:  ['0.8674', '0.6328', '0.8036']
his_acc:  ['0.8674', '0.6828', '0.6968']
CurrentTrain: epoch  0, batch     0 | loss: 6.5445986CurrentTrain: epoch  0, batch     1 | loss: 4.9499378CurrentTrain: epoch  1, batch     0 | loss: 5.2113919CurrentTrain: epoch  1, batch     1 | loss: 4.7493105CurrentTrain: epoch  2, batch     0 | loss: 4.8550849CurrentTrain: epoch  2, batch     1 | loss: 4.1306949CurrentTrain: epoch  3, batch     0 | loss: 4.2108359CurrentTrain: epoch  3, batch     1 | loss: 4.2251458CurrentTrain: epoch  4, batch     0 | loss: 3.8027887CurrentTrain: epoch  4, batch     1 | loss: 3.3392606CurrentTrain: epoch  5, batch     0 | loss: 3.3144855CurrentTrain: epoch  5, batch     1 | loss: 3.4920659CurrentTrain: epoch  6, batch     0 | loss: 3.0359335CurrentTrain: epoch  6, batch     1 | loss: 3.2154319CurrentTrain: epoch  7, batch     0 | loss: 2.7294884CurrentTrain: epoch  7, batch     1 | loss: 2.8001623CurrentTrain: epoch  8, batch     0 | loss: 2.5130763CurrentTrain: epoch  8, batch     1 | loss: 2.7445152CurrentTrain: epoch  9, batch     0 | loss: 2.5881221CurrentTrain: epoch  9, batch     1 | loss: 2.3079407
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: tv-idol-johns -- atlanta -- `` american idol '' finalist michael johns moved to los angeles several years ago , but his heart is still in atlanta .
Head Entity: his
Tail Entity: atlanta
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: actress-sarah -- new york -- `` the talented actress sarah has always considered new york her home, even after moving to los angeles for her career. ''  
Head Entity: sarah  
Tail Entity: new york  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: her political involvement began early : at cornell , she helped organize local farmers ' cooperatives .
Head Entity: she
Tail Entity: cornell
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: After graduating from high school, he enrolled at Stanford University to pursue his degree in computer science.  
Head Entity: he  
Tail Entity: Stanford University  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: us republican congresswoman jo ann davis dies after fight with breast cancer
Head Entity: jo ann davis
Tail Entity: us
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in the united kingdom  
Head Entity: stephen hawking  
Tail Entity: united kingdom  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by two stepdaughters , barbara a. hammond and brenda l. stevenson ; a stepson , michael a. taylor ; two grandchildren and one great-grandchild .
Head Entity: he
Tail Entity: brenda l. stevenson
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has three children, including her son, john, and her daughters, emily and sarah.  
Head Entity: she  
Tail Entity: emily  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: previously , al-khawinay was sentenced to one year in jail for supporting the country 's minority shiite rebels and defaming the president , but was later pardoned by president ali abdullah saleh .
Head Entity: al-khawinay
Tail Entity: defaming the president
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: the court found that the defendant, john doe, was guilty of embezzlement and subsequently charged him with multiple counts of fraud.  
Head Entity: john doe  
Tail Entity: embezzlement
Mixup data size:  121
MixupTrain:  epoch  0, batch     0 | loss: 6.5266094MixupTrain:  epoch  0, batch     1 | loss: 5.5166211MixupTrain:  epoch  0, batch     2 | loss: 6.9131002MixupTrain:  epoch  0, batch     3 | loss: 5.9981003MixupTrain:  epoch  0, batch     4 | loss: 5.6841593MixupTrain:  epoch  0, batch     5 | loss: 6.0303197MixupTrain:  epoch  0, batch     6 | loss: 5.8450994MixupTrain:  epoch  0, batch     7 | loss: 7.3912692
MemoryTrain:  epoch  0, batch     0 | loss: 2.8467855MemoryTrain:  epoch  0, batch     1 | loss: 3.4844437MemoryTrain:  epoch  0, batch     2 | loss: 3.5614502MemoryTrain:  epoch  1, batch     0 | loss: 3.0714321MemoryTrain:  epoch  1, batch     1 | loss: 3.8386869MemoryTrain:  epoch  1, batch     2 | loss: 3.3257351MemoryTrain:  epoch  2, batch     0 | loss: 3.4340296MemoryTrain:  epoch  2, batch     1 | loss: 2.8890281MemoryTrain:  epoch  2, batch     2 | loss: 2.3880107MemoryTrain:  epoch  3, batch     0 | loss: 2.6939240MemoryTrain:  epoch  3, batch     1 | loss: 2.4457467MemoryTrain:  epoch  3, batch     2 | loss: 3.0023401MemoryTrain:  epoch  4, batch     0 | loss: 2.7613158MemoryTrain:  epoch  4, batch     1 | loss: 2.7931240MemoryTrain:  epoch  4, batch     2 | loss: 1.9577307
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 57.29%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 60.71%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 64.06%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 61.81%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 71.35%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 73.56%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 75.45%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 78.52%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 79.78%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 76.74%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 41.67%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 37.50%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 34.82%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 32.03%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 31.94%   [EVAL] batch:    9 | acc: 12.50%,  total acc: 30.00%   [EVAL] batch:   10 | acc: 12.50%,  total acc: 28.41%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 28.65%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 28.85%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 29.91%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 33.33%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 35.16%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 37.50%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 38.89%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 40.79%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 42.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 45.54%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 48.01%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 50.27%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 52.08%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 54.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 55.77%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 57.18%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 58.71%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 60.13%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 61.04%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 61.90%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 62.89%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 63.26%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 64.34%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 63.21%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 63.72%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 64.02%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 64.47%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 65.06%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 65.78%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 66.31%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 66.96%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 67.73%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 68.47%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 69.17%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 69.84%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 70.48%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 70.66%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 70.25%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 69.98%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 69.95%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 69.46%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 69.33%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 68.98%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 68.86%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 68.64%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 68.10%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 68.22%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 68.23%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 68.15%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 68.65%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 69.62%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 70.08%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 70.52%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 70.96%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 71.38%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 71.79%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 71.48%   
cur_acc:  ['0.8674', '0.6328', '0.8036', '0.7674']
his_acc:  ['0.8674', '0.6828', '0.6968', '0.7148']
CurrentTrain: epoch  0, batch     0 | loss: 4.3391595CurrentTrain: epoch  0, batch     1 | loss: 4.5791187CurrentTrain: epoch  1, batch     0 | loss: 3.6422827CurrentTrain: epoch  1, batch     1 | loss: 3.0093892CurrentTrain: epoch  2, batch     0 | loss: 3.0201228CurrentTrain: epoch  2, batch     1 | loss: 2.6118181CurrentTrain: epoch  3, batch     0 | loss: 2.5753140CurrentTrain: epoch  3, batch     1 | loss: 3.0990202CurrentTrain: epoch  4, batch     0 | loss: 2.5694838CurrentTrain: epoch  4, batch     1 | loss: 2.4193892CurrentTrain: epoch  5, batch     0 | loss: 2.4422390CurrentTrain: epoch  5, batch     1 | loss: 2.3267565CurrentTrain: epoch  6, batch     0 | loss: 2.3594966CurrentTrain: epoch  6, batch     1 | loss: 2.1130595CurrentTrain: epoch  7, batch     0 | loss: 2.1217651CurrentTrain: epoch  7, batch     1 | loss: 2.0738714CurrentTrain: epoch  8, batch     0 | loss: 2.0489740CurrentTrain: epoch  8, batch     1 | loss: 2.1031849CurrentTrain: epoch  9, batch     0 | loss: 1.9672940CurrentTrain: epoch  9, batch     1 | loss: 2.0388920
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: goodman , who had suffered a series of strokes and seizures in recent weeks , died of natural causes , her son david said .
Head Entity: goodman
Tail Entity: natural causes
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: the famous actor, who had battled cancer for several years, passed away due to complications from the disease, his family announced.  
Head Entity: the famous actor  
Tail Entity: complications from the disease  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: clashes in late august in karbala between the mahdi army and a rival shiite militia , the badr organization , left at least 50 people dead .
Head Entity: badr organization
Tail Entity: shiite
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: the interfaith dialogue initiative was led by the peace organization, which aims to foster understanding among different religious groups.  
Head Entity: peace organization  
Tail Entity: religious groups  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: based in armonk , new york , mbia insures $ 670 billion -lrb- euro452 .18 billion -rrb- in debt .
Head Entity: mbia
Tail Entity: new york
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the headquarters of tech solutions is located in san francisco, california, where it employs over 500 people.  
Head Entity: tech solutions  
Tail Entity: california  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: parren mitchell 's sister-in-law , juanita jackson mitchell , was the long - time head and legal counsel of the maryland naacp .
Head Entity: parren mitchell
Tail Entity: juanita jackson mitchell
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: barack obama's half-sister, maya soetoro-ng, is a prominent educator and advocate for global education.  
Head Entity: barack obama  
Tail Entity: maya soetoro-ng  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, a renowned author, passed away on march 5 in his apartment in new york city, leaving behind a legacy of literary works that inspired many.  
Head Entity: john smith  
Tail Entity: new york city  
Mixup data size:  142
MixupTrain:  epoch  0, batch     0 | loss: 6.6804018MixupTrain:  epoch  0, batch     1 | loss: 6.5231934MixupTrain:  epoch  0, batch     2 | loss: 7.2682810MixupTrain:  epoch  0, batch     3 | loss: 6.0603886MixupTrain:  epoch  0, batch     4 | loss: 5.6719046MixupTrain:  epoch  0, batch     5 | loss: 6.3084717MixupTrain:  epoch  0, batch     6 | loss: 6.2524476MixupTrain:  epoch  0, batch     7 | loss: 6.2637196MixupTrain:  epoch  0, batch     8 | loss: 5.9538379
MemoryTrain:  epoch  0, batch     0 | loss: 2.6167853MemoryTrain:  epoch  0, batch     1 | loss: 3.7758069MemoryTrain:  epoch  0, batch     2 | loss: 2.9379256MemoryTrain:  epoch  0, batch     3 | loss: 3.1812391MemoryTrain:  epoch  1, batch     0 | loss: 3.2940850MemoryTrain:  epoch  1, batch     1 | loss: 3.6865273MemoryTrain:  epoch  1, batch     2 | loss: 2.9057212MemoryTrain:  epoch  1, batch     3 | loss: 3.0717335MemoryTrain:  epoch  2, batch     0 | loss: 2.3903966MemoryTrain:  epoch  2, batch     1 | loss: 3.4026728MemoryTrain:  epoch  2, batch     2 | loss: 2.8510315MemoryTrain:  epoch  2, batch     3 | loss: 1.3922912MemoryTrain:  epoch  3, batch     0 | loss: 2.1984382MemoryTrain:  epoch  3, batch     1 | loss: 2.3326743MemoryTrain:  epoch  3, batch     2 | loss: 2.9844241MemoryTrain:  epoch  3, batch     3 | loss: 2.4267681MemoryTrain:  epoch  4, batch     0 | loss: 2.2767429MemoryTrain:  epoch  4, batch     1 | loss: 2.5284386MemoryTrain:  epoch  4, batch     2 | loss: 2.1294944MemoryTrain:  epoch  4, batch     3 | loss: 1.2561806
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 56.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 59.38%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 61.61%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 61.72%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 60.42%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 60.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 63.07%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 65.10%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 63.46%   
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 35.42%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 32.81%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 32.50%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 33.33%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 34.82%   [EVAL] batch:    7 | acc: 31.25%,  total acc: 34.38%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 34.72%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 35.00%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 34.66%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 36.98%   [EVAL] batch:   12 | acc: 25.00%,  total acc: 36.06%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 37.50%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 40.42%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 41.80%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 43.75%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 44.79%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 46.05%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 47.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 50.30%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 52.56%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 54.62%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 58.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 59.62%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 60.88%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 62.05%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 63.15%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 63.75%   [EVAL] batch:   30 | acc: 56.25%,  total acc: 63.51%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 64.06%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 64.39%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 65.44%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 64.29%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 64.24%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 64.02%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 64.31%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 64.58%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 64.69%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 65.24%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 65.92%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 66.72%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 67.47%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 68.19%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 68.89%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 69.55%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 70.18%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 69.26%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 69.00%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 68.87%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 68.87%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 68.40%   [EVAL] batch:   53 | acc: 6.25%,  total acc: 67.25%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 66.02%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 64.84%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 63.71%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 62.61%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 61.55%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 61.25%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 61.89%   [EVAL] batch:   61 | acc: 37.50%,  total acc: 61.49%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 61.41%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 61.33%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 61.73%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 62.31%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 62.87%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 63.42%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 63.95%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 64.46%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 64.52%   [EVAL] batch:   71 | acc: 37.50%,  total acc: 64.15%   [EVAL] batch:   72 | acc: 56.25%,  total acc: 64.04%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 63.77%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 64.00%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 64.14%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 64.29%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 64.34%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 64.32%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 64.22%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 64.20%   [EVAL] batch:   81 | acc: 93.75%,  total acc: 64.56%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 64.76%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 64.06%   
cur_acc:  ['0.8674', '0.6328', '0.8036', '0.7674', '0.6346']
his_acc:  ['0.8674', '0.6828', '0.6968', '0.7148', '0.6406']
CurrentTrain: epoch  0, batch     0 | loss: 6.3645020CurrentTrain: epoch  0, batch     1 | loss: 6.9971013CurrentTrain: epoch  1, batch     0 | loss: 5.9419527CurrentTrain: epoch  1, batch     1 | loss: 4.8330612CurrentTrain: epoch  2, batch     0 | loss: 4.6315255CurrentTrain: epoch  2, batch     1 | loss: 6.0177817CurrentTrain: epoch  3, batch     0 | loss: 4.5429220CurrentTrain: epoch  3, batch     1 | loss: 3.6231129CurrentTrain: epoch  4, batch     0 | loss: 3.8688169CurrentTrain: epoch  4, batch     1 | loss: 3.9377561CurrentTrain: epoch  5, batch     0 | loss: 3.5099444CurrentTrain: epoch  5, batch     1 | loss: 3.9481702CurrentTrain: epoch  6, batch     0 | loss: 3.6597710CurrentTrain: epoch  6, batch     1 | loss: 3.1765170CurrentTrain: epoch  7, batch     0 | loss: 3.1580458CurrentTrain: epoch  7, batch     1 | loss: 2.8996069CurrentTrain: epoch  8, batch     0 | loss: 2.8740916CurrentTrain: epoch  8, batch     1 | loss: 2.6821873CurrentTrain: epoch  9, batch     0 | loss: 2.9581351CurrentTrain: epoch  9, batch     1 | loss: 2.3448308
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: kirkaldy , born irene morgan in baltimore , maryland , in 1917 , was arrested in 1944 for refusing to give up her seat on a greyhound bus heading from gloucester to baltimore , and for resisting arrest .
Head Entity: irene morgan
Tail Entity: 1917
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: albert einstein was born in ulm, in the kingdom of wurttemberg in the german empire on march 14, 1879.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: jefferson joseph deblanc sr. was born in lockport , la. , on feb. 15 , 1921 , and grew up in st. martinville .
Head Entity: jefferson joseph deblanc sr.
Tail Entity: la.
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: martha ann jones was born in springfield, il, on march 10, 1985, and later moved to chicago.  
Head Entity: martha ann jones  
Tail Entity: il.  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: as the case developed , sandy 's mother , denise sandy , quietly made herself a spectral but central figure , by faithfully attending pretrial hearings .
Head Entity: sandy
Tail Entity: denise sandy
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: after the ceremony, john's father, michael, shared heartfelt stories about his son's childhood.  
Head Entity: john  
Tail Entity: michael  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: after years of hard work, jane doe finally received a promotion at tech innovations, where she has been a dedicated employee.  
Head Entity: jane doe  
Tail Entity: tech innovations  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, 75, passed away peacefully in his sleep on march 5 in the beautiful town of springfield, il, surrounded by family and friends.  
Head Entity: john doe  
Tail Entity: il  
Mixup data size:  163
MixupTrain:  epoch  0, batch     0 | loss: 5.4071531MixupTrain:  epoch  0, batch     1 | loss: 5.7871799MixupTrain:  epoch  0, batch     2 | loss: 5.5010295MixupTrain:  epoch  0, batch     3 | loss: 5.6010036MixupTrain:  epoch  0, batch     4 | loss: 5.4069605MixupTrain:  epoch  0, batch     5 | loss: 5.8361053MixupTrain:  epoch  0, batch     6 | loss: 5.5712748MixupTrain:  epoch  0, batch     7 | loss: 5.9074335MixupTrain:  epoch  0, batch     8 | loss: 5.4577112MixupTrain:  epoch  0, batch     9 | loss: 5.1828308MixupTrain:  epoch  0, batch    10 | loss: 5.6564293
MemoryTrain:  epoch  0, batch     0 | loss: 2.4854560MemoryTrain:  epoch  0, batch     1 | loss: 2.6159170MemoryTrain:  epoch  0, batch     2 | loss: 2.1046336MemoryTrain:  epoch  0, batch     3 | loss: 2.4703166MemoryTrain:  epoch  1, batch     0 | loss: 2.5526459MemoryTrain:  epoch  1, batch     1 | loss: 2.4255500MemoryTrain:  epoch  1, batch     2 | loss: 2.5925589MemoryTrain:  epoch  1, batch     3 | loss: 1.8590286MemoryTrain:  epoch  2, batch     0 | loss: 2.0806627MemoryTrain:  epoch  2, batch     1 | loss: 1.9426229MemoryTrain:  epoch  2, batch     2 | loss: 1.7497091MemoryTrain:  epoch  2, batch     3 | loss: 2.8874490MemoryTrain:  epoch  3, batch     0 | loss: 2.3867664MemoryTrain:  epoch  3, batch     1 | loss: 1.3203442MemoryTrain:  epoch  3, batch     2 | loss: 2.3616080MemoryTrain:  epoch  3, batch     3 | loss: 2.0762899MemoryTrain:  epoch  4, batch     0 | loss: 2.2667975MemoryTrain:  epoch  4, batch     1 | loss: 1.6200391MemoryTrain:  epoch  4, batch     2 | loss: 2.0641863MemoryTrain:  epoch  4, batch     3 | loss: 1.1170061
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 73.61%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 76.14%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 75.52%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 75.96%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 73.66%   
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 25.00%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 21.88%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 22.50%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 25.00%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 28.57%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 29.69%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 30.56%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:   10 | acc: 25.00%,  total acc: 30.68%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 32.29%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 32.21%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 33.48%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 36.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 37.50%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 39.71%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 40.97%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 42.43%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 44.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 47.02%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 49.43%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 51.63%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 53.39%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 55.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 56.97%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 58.33%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 59.60%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 60.78%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 61.46%   [EVAL] batch:   30 | acc: 62.50%,  total acc: 61.49%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 62.30%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 62.69%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 63.79%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 62.86%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 62.67%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 62.99%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 63.46%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 63.91%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 64.63%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 65.33%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 66.13%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 66.90%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 68.34%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 69.02%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 69.66%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 69.01%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 68.50%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 68.63%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 68.16%   [EVAL] batch:   53 | acc: 6.25%,  total acc: 67.01%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 65.80%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 64.62%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 63.49%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 62.39%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 61.33%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 61.04%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 61.58%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 60.99%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 60.52%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 60.25%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 60.67%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 61.27%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 61.85%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 62.41%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 62.95%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 63.48%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 63.38%   [EVAL] batch:   71 | acc: 31.25%,  total acc: 62.93%   [EVAL] batch:   72 | acc: 43.75%,  total acc: 62.67%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 62.42%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 62.75%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 62.99%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 63.15%   [EVAL] batch:   77 | acc: 43.75%,  total acc: 62.90%   [EVAL] batch:   78 | acc: 18.75%,  total acc: 62.34%   [EVAL] batch:   79 | acc: 25.00%,  total acc: 61.88%   [EVAL] batch:   80 | acc: 31.25%,  total acc: 61.50%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 61.81%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 62.12%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 62.20%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 62.35%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 62.71%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 62.71%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 62.98%   [EVAL] batch:   91 | acc: 87.50%,  total acc: 63.25%   [EVAL] batch:   92 | acc: 87.50%,  total acc: 63.51%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 63.76%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 63.88%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 64.06%   [EVAL] batch:   96 | acc: 43.75%,  total acc: 63.85%   
cur_acc:  ['0.8674', '0.6328', '0.8036', '0.7674', '0.6346', '0.7366']
his_acc:  ['0.8674', '0.6828', '0.6968', '0.7148', '0.6406', '0.6385']
CurrentTrain: epoch  0, batch     0 | loss: 7.7515688CurrentTrain: epoch  0, batch     1 | loss: 7.3888268CurrentTrain: epoch  1, batch     0 | loss: 6.4775434CurrentTrain: epoch  1, batch     1 | loss: 6.9045777CurrentTrain: epoch  2, batch     0 | loss: 6.0267582CurrentTrain: epoch  2, batch     1 | loss: 5.9376078CurrentTrain: epoch  3, batch     0 | loss: 5.9876432CurrentTrain: epoch  3, batch     1 | loss: 5.3068924CurrentTrain: epoch  4, batch     0 | loss: 5.5525036CurrentTrain: epoch  4, batch     1 | loss: 5.2318616CurrentTrain: epoch  5, batch     0 | loss: 5.0885553CurrentTrain: epoch  5, batch     1 | loss: 5.1213293CurrentTrain: epoch  6, batch     0 | loss: 5.4627762CurrentTrain: epoch  6, batch     1 | loss: 4.0432615CurrentTrain: epoch  7, batch     0 | loss: 4.3483362CurrentTrain: epoch  7, batch     1 | loss: 5.1928329CurrentTrain: epoch  8, batch     0 | loss: 4.6115518CurrentTrain: epoch  8, batch     1 | loss: 3.9599373CurrentTrain: epoch  9, batch     0 | loss: 4.2168570CurrentTrain: epoch  9, batch     1 | loss: 4.2575593
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: firstgroup , britain 's largest bus operator , entered the north american market in 1999 when it acquired ryder public transportation services inc. .
Head Entity: firstgroup
Tail Entity: ryder public transportation services inc.
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: Alphabet Inc. has several subsidiaries, including Google LLC and YouTube, which have significantly contributed to its growth.  
Head Entity: Alphabet Inc.  
Tail Entity: Google LLC  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: the merger between the tech giant alpha innovations and its parent company beta holdings has created a new leader in the industry, with both organizations now working closely together to enhance their market presence.  
Head Entity: alpha innovations  
Tail Entity: beta holdings  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: the national security council -lrb- nsc -rrb- made the demand at talks chaired by president hamid karzai , who has been vocal in condemning international forces he believes are responsible for the incident last saturday in the eastern flashpoint of kunar .
Head Entity: national security council
Tail Entity: nsc
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: the international monetary fund -lrb- imf -rrb- has been working to stabilize the global economy during the crisis.  
Head Entity: international monetary fund  
Tail Entity: imf  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: ------ london 2008-05-20 07:23:45 utc enodis plc endorses sweetened takeover bid by us company manitowoc illinois tool works of glenville , illinois , which had offered 282 pence -lrb- us$ 551 euro3 54 -rrb- per share , said monday that it was considering its position .
Head Entity: illinois tool works
Tail Entity: glenville
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: ------ san francisco 2021-03-15 10:00:00 utc tech giant google has announced plans to expand its headquarters in the vibrant city of mountain view, california, which is known for its innovation and tech culture.  
Head Entity: google  
Tail Entity: mountain view  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: holly montag says it 's been tough for her sister heidi to deal with all the critics of her massive plastic surgery .
Head Entity: her
Tail Entity: her
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: john and his brother are planning a trip together to celebrate their birthday.  
Head Entity: his  
Tail Entity: brother  
Mixup data size:  182
MixupTrain:  epoch  0, batch     0 | loss: 5.2537451MixupTrain:  epoch  0, batch     1 | loss: 4.6963067MixupTrain:  epoch  0, batch     2 | loss: 5.1147146MixupTrain:  epoch  0, batch     3 | loss: 4.6461158MixupTrain:  epoch  0, batch     4 | loss: 4.9661984MixupTrain:  epoch  0, batch     5 | loss: 5.1689329MixupTrain:  epoch  0, batch     6 | loss: 5.6346064MixupTrain:  epoch  0, batch     7 | loss: 5.3554358MixupTrain:  epoch  0, batch     8 | loss: 5.1749244MixupTrain:  epoch  0, batch     9 | loss: 5.3205352MixupTrain:  epoch  0, batch    10 | loss: 4.2359543MixupTrain:  epoch  0, batch    11 | loss: 4.1160164
MemoryTrain:  epoch  0, batch     0 | loss: 1.8555307MemoryTrain:  epoch  0, batch     1 | loss: 2.2616882MemoryTrain:  epoch  0, batch     2 | loss: 1.9507581MemoryTrain:  epoch  0, batch     3 | loss: 2.4996555MemoryTrain:  epoch  0, batch     4 | loss: 2.5274408MemoryTrain:  epoch  1, batch     0 | loss: 2.0775537MemoryTrain:  epoch  1, batch     1 | loss: 2.2604036MemoryTrain:  epoch  1, batch     2 | loss: 2.0371931MemoryTrain:  epoch  1, batch     3 | loss: 2.1207027MemoryTrain:  epoch  1, batch     4 | loss: 2.1834731MemoryTrain:  epoch  2, batch     0 | loss: 1.6863093MemoryTrain:  epoch  2, batch     1 | loss: 1.7756343MemoryTrain:  epoch  2, batch     2 | loss: 2.1867847MemoryTrain:  epoch  2, batch     3 | loss: 1.8979994MemoryTrain:  epoch  2, batch     4 | loss: 2.5179067MemoryTrain:  epoch  3, batch     0 | loss: 1.6150463MemoryTrain:  epoch  3, batch     1 | loss: 1.3576045MemoryTrain:  epoch  3, batch     2 | loss: 2.2329998MemoryTrain:  epoch  3, batch     3 | loss: 1.7027884MemoryTrain:  epoch  3, batch     4 | loss: 1.7919900MemoryTrain:  epoch  4, batch     0 | loss: 1.8822944MemoryTrain:  epoch  4, batch     1 | loss: 1.5398178MemoryTrain:  epoch  4, batch     2 | loss: 1.4228765MemoryTrain:  epoch  4, batch     3 | loss: 1.5613487MemoryTrain:  epoch  4, batch     4 | loss: 1.3682675
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 28.12%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 22.50%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 19.79%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 20.54%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 25.78%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 29.17%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 31.87%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 36.36%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 39.58%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 42.79%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 46.88%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 49.17%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 51.95%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 53.68%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 55.21%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 56.58%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 57.50%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 58.33%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 57.67%   
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 20.83%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 17.19%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 18.75%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 20.83%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 25.00%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 26.56%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 27.78%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 28.12%   [EVAL] batch:   10 | acc: 18.75%,  total acc: 27.27%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 28.65%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 27.40%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 28.12%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 31.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 32.81%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 35.29%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 36.81%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 38.82%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 40.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 43.75%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 46.31%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 48.64%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 50.52%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 52.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 54.33%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 55.79%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 57.14%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 58.19%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 59.17%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 59.48%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 60.35%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 60.80%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 61.95%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 61.07%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 61.46%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 61.49%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 62.01%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 62.66%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 63.28%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 64.02%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 64.58%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 65.41%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 66.19%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 66.94%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 67.66%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 68.35%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 69.01%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 67.98%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 67.00%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 66.42%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 66.59%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 66.16%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 65.16%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 63.98%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 62.83%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 61.73%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 60.67%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 59.64%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 59.38%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 59.94%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 59.38%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 58.93%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 58.69%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 59.13%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 59.75%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 60.35%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 60.94%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 61.50%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 62.05%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 62.06%   [EVAL] batch:   71 | acc: 31.25%,  total acc: 61.63%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 61.47%   [EVAL] batch:   73 | acc: 37.50%,  total acc: 61.15%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 61.17%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 61.10%   [EVAL] batch:   76 | acc: 43.75%,  total acc: 60.88%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 60.34%   [EVAL] batch:   78 | acc: 12.50%,  total acc: 59.73%   [EVAL] batch:   79 | acc: 12.50%,  total acc: 59.14%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 58.64%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 58.61%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 58.96%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 59.15%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 58.53%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 57.92%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 57.33%   [EVAL] batch:   87 | acc: 0.00%,  total acc: 56.68%   [EVAL] batch:   88 | acc: 18.75%,  total acc: 56.25%   [EVAL] batch:   89 | acc: 25.00%,  total acc: 55.90%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 56.18%   [EVAL] batch:   91 | acc: 87.50%,  total acc: 56.52%   [EVAL] batch:   92 | acc: 81.25%,  total acc: 56.79%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 57.11%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 57.24%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 57.49%   [EVAL] batch:   96 | acc: 43.75%,  total acc: 57.35%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 57.21%   [EVAL] batch:   98 | acc: 43.75%,  total acc: 57.07%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 56.69%   [EVAL] batch:  100 | acc: 0.00%,  total acc: 56.13%   [EVAL] batch:  101 | acc: 0.00%,  total acc: 55.58%   [EVAL] batch:  102 | acc: 12.50%,  total acc: 55.16%   [EVAL] batch:  103 | acc: 31.25%,  total acc: 54.93%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 55.06%   [EVAL] batch:  105 | acc: 50.00%,  total acc: 55.01%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 55.14%   [EVAL] batch:  107 | acc: 68.75%,  total acc: 55.27%   [EVAL] batch:  108 | acc: 81.25%,  total acc: 55.50%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 55.80%   [EVAL] batch:  110 | acc: 93.75%,  total acc: 56.14%   [EVAL] batch:  111 | acc: 87.50%,  total acc: 56.42%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 56.75%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 56.85%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 57.12%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 57.33%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 57.48%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 57.68%   [EVAL] batch:  118 | acc: 18.75%,  total acc: 57.35%   
cur_acc:  ['0.8674', '0.6328', '0.8036', '0.7674', '0.6346', '0.7366', '0.5767']
his_acc:  ['0.8674', '0.6828', '0.6968', '0.7148', '0.6406', '0.6385', '0.5735']
CurrentTrain: epoch  0, batch     0 | loss: 6.8539848CurrentTrain: epoch  0, batch     1 | loss: 6.4811063CurrentTrain: epoch  1, batch     0 | loss: 5.6527777CurrentTrain: epoch  1, batch     1 | loss: 4.3060098CurrentTrain: epoch  2, batch     0 | loss: 4.9079399CurrentTrain: epoch  2, batch     1 | loss: 4.9674811CurrentTrain: epoch  3, batch     0 | loss: 4.6055069CurrentTrain: epoch  3, batch     1 | loss: 4.5143127CurrentTrain: epoch  4, batch     0 | loss: 4.1851726CurrentTrain: epoch  4, batch     1 | loss: 4.7143989CurrentTrain: epoch  5, batch     0 | loss: 4.2408829CurrentTrain: epoch  5, batch     1 | loss: 3.8350296CurrentTrain: epoch  6, batch     0 | loss: 4.0179157CurrentTrain: epoch  6, batch     1 | loss: 3.4936535CurrentTrain: epoch  7, batch     0 | loss: 3.8971059CurrentTrain: epoch  7, batch     1 | loss: 2.8706756CurrentTrain: epoch  8, batch     0 | loss: 3.3653550CurrentTrain: epoch  8, batch     1 | loss: 3.5846295CurrentTrain: epoch  9, batch     0 | loss: 3.1182044CurrentTrain: epoch  9, batch     1 | loss: 3.4660234
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: san jose , ca , usa speaking of k-fed , him and ex-wife britney spears are in court today , dealing with their custody battle .
Head Entity: britney spears
Tail Entity: ca
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: after moving to austin, texas, the musician found a new inspiration for his songs.  
Head Entity: musician  
Tail Entity: texas  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: he was taken off life support on feb. 14 .
Head Entity: he
Tail Entity: feb. 14
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: she passed away peacefully on March 5, 2020.  
Head Entity: she  
Tail Entity: March 5, 2020  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: with the sweep of a federal regulator 's pen , massachusetts stands to gain a new life-science giant in april : covidien , a medical - supplies maker with thousands of products and more than 43,000 employees worldwide .
Head Entity: covidien
Tail Entity: 43,000
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: The tech company Apple has been expanding rapidly, and as of 2023, it boasts a workforce of over 160,000 employees globally.  
Head Entity: Apple  
Tail Entity: 160,000  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: a judge in new york city said remy ma , whose real name is remy smith , said thursday that the hip-hopper could not leave the united states for a five-country european concert tour .
Head Entity: remy smith
Tail Entity: remy ma
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: the famous author mark twain, whose real name is samuel clemens, wrote many classic novels that are still read today.  
Head Entity: samuel clemens  
Tail Entity: mark twain  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: beverly hills , california 2008-08-17 21:15:39 utc ------ there was much dancing : ellen degeneres and portia de rossi are married , according to reports .
Head Entity: ellen degeneres
Tail Entity: portia de rossi
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: in a beautiful ceremony held in new york city, 2015-06-20 15:30:00 utc ------ the couple exchanged vows: john legend and chrissy teigen are now officially husband and wife.  
Head Entity: john legend  
Tail Entity: chrissy teigen  
Mixup data size:  202
MixupTrain:  epoch  0, batch     0 | loss: 4.1524544MixupTrain:  epoch  0, batch     1 | loss: 4.2729383MixupTrain:  epoch  0, batch     2 | loss: 4.0280328MixupTrain:  epoch  0, batch     3 | loss: 4.9956141MixupTrain:  epoch  0, batch     4 | loss: 4.6710949MixupTrain:  epoch  0, batch     5 | loss: 5.0213799MixupTrain:  epoch  0, batch     6 | loss: 4.6625571MixupTrain:  epoch  0, batch     7 | loss: 4.2264624MixupTrain:  epoch  0, batch     8 | loss: 4.6075888MixupTrain:  epoch  0, batch     9 | loss: 4.8260217MixupTrain:  epoch  0, batch    10 | loss: 3.9656606MixupTrain:  epoch  0, batch    11 | loss: 4.8054390MixupTrain:  epoch  0, batch    12 | loss: 4.5914989
MemoryTrain:  epoch  0, batch     0 | loss: 1.7624475MemoryTrain:  epoch  0, batch     1 | loss: 1.8384945MemoryTrain:  epoch  0, batch     2 | loss: 1.7513456MemoryTrain:  epoch  0, batch     3 | loss: 2.4732027MemoryTrain:  epoch  0, batch     4 | loss: 2.3517184MemoryTrain:  epoch  0, batch     5 | loss: 2.1333158MemoryTrain:  epoch  1, batch     0 | loss: 2.3210576MemoryTrain:  epoch  1, batch     1 | loss: 2.1381330MemoryTrain:  epoch  1, batch     2 | loss: 1.5845352MemoryTrain:  epoch  1, batch     3 | loss: 1.9294574MemoryTrain:  epoch  1, batch     4 | loss: 2.0363288MemoryTrain:  epoch  1, batch     5 | loss: 2.4571309MemoryTrain:  epoch  2, batch     0 | loss: 1.7645673MemoryTrain:  epoch  2, batch     1 | loss: 1.6269033MemoryTrain:  epoch  2, batch     2 | loss: 1.3618017MemoryTrain:  epoch  2, batch     3 | loss: 1.8342346MemoryTrain:  epoch  2, batch     4 | loss: 1.4746928MemoryTrain:  epoch  2, batch     5 | loss: 1.5354261MemoryTrain:  epoch  3, batch     0 | loss: 1.5529064MemoryTrain:  epoch  3, batch     1 | loss: 1.7287718MemoryTrain:  epoch  3, batch     2 | loss: 1.2696875MemoryTrain:  epoch  3, batch     3 | loss: 1.2591953MemoryTrain:  epoch  3, batch     4 | loss: 1.4486030MemoryTrain:  epoch  3, batch     5 | loss: 3.0406802MemoryTrain:  epoch  4, batch     0 | loss: 1.2245245MemoryTrain:  epoch  4, batch     1 | loss: 1.2409186MemoryTrain:  epoch  4, batch     2 | loss: 1.3871474MemoryTrain:  epoch  4, batch     3 | loss: 1.1428595MemoryTrain:  epoch  4, batch     4 | loss: 1.6302326MemoryTrain:  epoch  4, batch     5 | loss: 2.9277742
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 67.19%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 75.57%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 71.88%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 70.67%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 68.30%   [EVAL] batch:   14 | acc: 18.75%,  total acc: 65.00%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 12.50%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 18.75%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 15.62%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 15.00%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 16.67%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 21.43%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 23.44%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 24.31%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 25.00%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 25.57%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 27.08%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 25.96%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 26.79%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 30.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 31.64%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 34.19%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 35.76%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 36.84%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 39.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 41.96%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 44.60%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 47.01%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 48.96%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 51.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 52.88%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 54.40%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 55.80%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 56.90%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 57.71%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 58.06%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 58.98%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 59.47%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 60.66%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 59.82%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 60.42%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 60.47%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 61.02%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 61.54%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 62.19%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 62.96%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 63.69%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 64.53%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 65.34%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 66.11%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 66.85%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 67.55%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 68.23%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 67.22%   [EVAL] batch:   49 | acc: 25.00%,  total acc: 66.38%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 65.81%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 65.87%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 65.45%   [EVAL] batch:   53 | acc: 6.25%,  total acc: 64.35%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 63.18%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 62.05%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 60.96%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 59.91%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 58.90%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 58.65%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 59.22%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 58.67%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 58.13%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 57.91%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 58.27%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 58.90%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 59.51%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 60.11%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 60.69%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 61.25%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 61.27%   [EVAL] batch:   71 | acc: 31.25%,  total acc: 60.85%   [EVAL] batch:   72 | acc: 43.75%,  total acc: 60.62%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 60.39%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 60.42%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 60.53%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 60.39%   [EVAL] batch:   77 | acc: 12.50%,  total acc: 59.78%   [EVAL] batch:   78 | acc: 12.50%,  total acc: 59.18%   [EVAL] batch:   79 | acc: 12.50%,  total acc: 58.59%   [EVAL] batch:   80 | acc: 12.50%,  total acc: 58.02%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 58.08%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 58.43%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 58.56%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 58.01%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 57.34%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 56.68%   [EVAL] batch:   87 | acc: 0.00%,  total acc: 56.04%   [EVAL] batch:   88 | acc: 6.25%,  total acc: 55.48%   [EVAL] batch:   89 | acc: 25.00%,  total acc: 55.14%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 55.43%   [EVAL] batch:   91 | acc: 87.50%,  total acc: 55.77%   [EVAL] batch:   92 | acc: 87.50%,  total acc: 56.12%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 56.45%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 56.64%   [EVAL] batch:   95 | acc: 75.00%,  total acc: 56.84%   [EVAL] batch:   96 | acc: 18.75%,  total acc: 56.44%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 56.19%   [EVAL] batch:   98 | acc: 43.75%,  total acc: 56.06%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 55.81%   [EVAL] batch:  100 | acc: 31.25%,  total acc: 55.57%   [EVAL] batch:  101 | acc: 31.25%,  total acc: 55.33%   [EVAL] batch:  102 | acc: 25.00%,  total acc: 55.04%   [EVAL] batch:  103 | acc: 37.50%,  total acc: 54.87%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 54.94%   [EVAL] batch:  105 | acc: 50.00%,  total acc: 54.89%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 55.02%   [EVAL] batch:  107 | acc: 68.75%,  total acc: 55.15%   [EVAL] batch:  108 | acc: 81.25%,  total acc: 55.39%   [EVAL] batch:  109 | acc: 75.00%,  total acc: 55.57%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 55.97%   [EVAL] batch:  111 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 56.58%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 56.69%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 56.90%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 57.11%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 57.26%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 57.52%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 57.67%   [EVAL] batch:  119 | acc: 56.25%,  total acc: 57.66%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 57.80%   [EVAL] batch:  121 | acc: 75.00%,  total acc: 57.94%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 57.98%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 58.22%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 58.50%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 58.83%   [EVAL] batch:  126 | acc: 81.25%,  total acc: 59.01%   [EVAL] batch:  127 | acc: 93.75%,  total acc: 59.28%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 59.11%   [EVAL] batch:  129 | acc: 37.50%,  total acc: 58.94%   [EVAL] batch:  130 | acc: 56.25%,  total acc: 58.92%   [EVAL] batch:  131 | acc: 37.50%,  total acc: 58.76%   [EVAL] batch:  132 | acc: 25.00%,  total acc: 58.51%   
cur_acc:  ['0.8674', '0.6328', '0.8036', '0.7674', '0.6346', '0.7366', '0.5767', '0.6500']
his_acc:  ['0.8674', '0.6828', '0.6968', '0.7148', '0.6406', '0.6385', '0.5735', '0.5851']
--------Round  4
seed:  500
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 5 6 4 2 1 3 0]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 11.5446396CurrentTrain: epoch  0, batch     1 | loss: 11.1874571CurrentTrain: epoch  0, batch     2 | loss: 11.4289932CurrentTrain: epoch  0, batch     3 | loss: 11.3597908CurrentTrain: epoch  0, batch     4 | loss: 11.3073349CurrentTrain: epoch  0, batch     5 | loss: 10.7901611CurrentTrain: epoch  0, batch     6 | loss: 11.7070160CurrentTrain: epoch  0, batch     7 | loss: 11.3331871CurrentTrain: epoch  0, batch     8 | loss: 11.0086355CurrentTrain: epoch  0, batch     9 | loss: 11.2262917CurrentTrain: epoch  0, batch    10 | loss: 10.6518173CurrentTrain: epoch  0, batch    11 | loss: 10.9647732CurrentTrain: epoch  0, batch    12 | loss: 10.8395844CurrentTrain: epoch  0, batch    13 | loss: 11.0434971CurrentTrain: epoch  0, batch    14 | loss: 10.6995773CurrentTrain: epoch  0, batch    15 | loss: 10.5431547CurrentTrain: epoch  0, batch    16 | loss: 10.6068668CurrentTrain: epoch  0, batch    17 | loss: 10.3253956CurrentTrain: epoch  0, batch    18 | loss: 10.5925159CurrentTrain: epoch  0, batch    19 | loss: 9.5401373CurrentTrain: epoch  0, batch    20 | loss: 9.8695812CurrentTrain: epoch  0, batch    21 | loss: 10.0673008CurrentTrain: epoch  0, batch    22 | loss: 10.1566105CurrentTrain: epoch  0, batch    23 | loss: 10.0647106CurrentTrain: epoch  0, batch    24 | loss: 9.6318531CurrentTrain: epoch  0, batch    25 | loss: 10.5359097CurrentTrain: epoch  0, batch    26 | loss: 9.8418961CurrentTrain: epoch  0, batch    27 | loss: 9.6280556CurrentTrain: epoch  0, batch    28 | loss: 9.3951101CurrentTrain: epoch  0, batch    29 | loss: 9.7455730CurrentTrain: epoch  0, batch    30 | loss: 9.7683306CurrentTrain: epoch  0, batch    31 | loss: 10.3049936CurrentTrain: epoch  0, batch    32 | loss: 9.7377739CurrentTrain: epoch  0, batch    33 | loss: 9.1492224CurrentTrain: epoch  0, batch    34 | loss: 9.7113037CurrentTrain: epoch  0, batch    35 | loss: 10.1148605CurrentTrain: epoch  0, batch    36 | loss: 9.5683498CurrentTrain: epoch  0, batch    37 | loss: 10.4918289CurrentTrain: epoch  1, batch     0 | loss: 8.7299175CurrentTrain: epoch  1, batch     1 | loss: 8.8441620CurrentTrain: epoch  1, batch     2 | loss: 9.2614174CurrentTrain: epoch  1, batch     3 | loss: 9.3738003CurrentTrain: epoch  1, batch     4 | loss: 9.3266449CurrentTrain: epoch  1, batch     5 | loss: 8.7474403CurrentTrain: epoch  1, batch     6 | loss: 9.3079109CurrentTrain: epoch  1, batch     7 | loss: 9.2999086CurrentTrain: epoch  1, batch     8 | loss: 8.9463053CurrentTrain: epoch  1, batch     9 | loss: 9.0231762CurrentTrain: epoch  1, batch    10 | loss: 8.8452616CurrentTrain: epoch  1, batch    11 | loss: 9.3323278CurrentTrain: epoch  1, batch    12 | loss: 9.3036003CurrentTrain: epoch  1, batch    13 | loss: 8.1142750CurrentTrain: epoch  1, batch    14 | loss: 9.4695225CurrentTrain: epoch  1, batch    15 | loss: 8.6089153CurrentTrain: epoch  1, batch    16 | loss: 8.9538612CurrentTrain: epoch  1, batch    17 | loss: 8.4457741CurrentTrain: epoch  1, batch    18 | loss: 8.7888517CurrentTrain: epoch  1, batch    19 | loss: 8.1453209CurrentTrain: epoch  1, batch    20 | loss: 8.3905230CurrentTrain: epoch  1, batch    21 | loss: 9.3090916CurrentTrain: epoch  1, batch    22 | loss: 8.9290638CurrentTrain: epoch  1, batch    23 | loss: 8.7866611CurrentTrain: epoch  1, batch    24 | loss: 8.1831322CurrentTrain: epoch  1, batch    25 | loss: 8.9930630CurrentTrain: epoch  1, batch    26 | loss: 8.6450500CurrentTrain: epoch  1, batch    27 | loss: 7.9593086CurrentTrain: epoch  1, batch    28 | loss: 7.9506927CurrentTrain: epoch  1, batch    29 | loss: 8.4990120CurrentTrain: epoch  1, batch    30 | loss: 8.7070951CurrentTrain: epoch  1, batch    31 | loss: 8.7391129CurrentTrain: epoch  1, batch    32 | loss: 9.5853405CurrentTrain: epoch  1, batch    33 | loss: 8.1153889CurrentTrain: epoch  1, batch    34 | loss: 8.1301537CurrentTrain: epoch  1, batch    35 | loss: 7.7749491CurrentTrain: epoch  1, batch    36 | loss: 8.4214735CurrentTrain: epoch  1, batch    37 | loss: 8.3344212CurrentTrain: epoch  2, batch     0 | loss: 7.6011982CurrentTrain: epoch  2, batch     1 | loss: 7.8197050CurrentTrain: epoch  2, batch     2 | loss: 7.6624150CurrentTrain: epoch  2, batch     3 | loss: 7.6011729CurrentTrain: epoch  2, batch     4 | loss: 8.9896317CurrentTrain: epoch  2, batch     5 | loss: 9.0090714CurrentTrain: epoch  2, batch     6 | loss: 8.2505198CurrentTrain: epoch  2, batch     7 | loss: 7.5469074CurrentTrain: epoch  2, batch     8 | loss: 8.3081017CurrentTrain: epoch  2, batch     9 | loss: 7.5260091CurrentTrain: epoch  2, batch    10 | loss: 7.1206427CurrentTrain: epoch  2, batch    11 | loss: 7.5514965CurrentTrain: epoch  2, batch    12 | loss: 7.4167924CurrentTrain: epoch  2, batch    13 | loss: 8.0955477CurrentTrain: epoch  2, batch    14 | loss: 8.4557543CurrentTrain: epoch  2, batch    15 | loss: 7.1886835CurrentTrain: epoch  2, batch    16 | loss: 7.8172593CurrentTrain: epoch  2, batch    17 | loss: 8.5990267CurrentTrain: epoch  2, batch    18 | loss: 7.7505693CurrentTrain: epoch  2, batch    19 | loss: 7.2570601CurrentTrain: epoch  2, batch    20 | loss: 7.5662642CurrentTrain: epoch  2, batch    21 | loss: 7.5654764CurrentTrain: epoch  2, batch    22 | loss: 6.9028592CurrentTrain: epoch  2, batch    23 | loss: 6.7983799CurrentTrain: epoch  2, batch    24 | loss: 7.2308226CurrentTrain: epoch  2, batch    25 | loss: 7.7680030CurrentTrain: epoch  2, batch    26 | loss: 7.1201687CurrentTrain: epoch  2, batch    27 | loss: 8.0449219CurrentTrain: epoch  2, batch    28 | loss: 7.2843556CurrentTrain: epoch  2, batch    29 | loss: 7.6700525CurrentTrain: epoch  2, batch    30 | loss: 7.6557722CurrentTrain: epoch  2, batch    31 | loss: 6.5427999CurrentTrain: epoch  2, batch    32 | loss: 6.9140997CurrentTrain: epoch  2, batch    33 | loss: 6.7404013CurrentTrain: epoch  2, batch    34 | loss: 7.5207868CurrentTrain: epoch  2, batch    35 | loss: 6.8004436CurrentTrain: epoch  2, batch    36 | loss: 7.0457516CurrentTrain: epoch  2, batch    37 | loss: 7.6665878CurrentTrain: epoch  3, batch     0 | loss: 6.8770857CurrentTrain: epoch  3, batch     1 | loss: 7.8999434CurrentTrain: epoch  3, batch     2 | loss: 6.9364171CurrentTrain: epoch  3, batch     3 | loss: 6.3510723CurrentTrain: epoch  3, batch     4 | loss: 7.7375946CurrentTrain: epoch  3, batch     5 | loss: 6.3118658CurrentTrain: epoch  3, batch     6 | loss: 6.1978445CurrentTrain: epoch  3, batch     7 | loss: 6.6554232CurrentTrain: epoch  3, batch     8 | loss: 7.8293281CurrentTrain: epoch  3, batch     9 | loss: 7.3906622CurrentTrain: epoch  3, batch    10 | loss: 7.2002621CurrentTrain: epoch  3, batch    11 | loss: 6.8389854CurrentTrain: epoch  3, batch    12 | loss: 7.3331280CurrentTrain: epoch  3, batch    13 | loss: 6.8355517CurrentTrain: epoch  3, batch    14 | loss: 7.2874880CurrentTrain: epoch  3, batch    15 | loss: 7.1791315CurrentTrain: epoch  3, batch    16 | loss: 6.9050627CurrentTrain: epoch  3, batch    17 | loss: 7.3327060CurrentTrain: epoch  3, batch    18 | loss: 7.3965974CurrentTrain: epoch  3, batch    19 | loss: 7.4676466CurrentTrain: epoch  3, batch    20 | loss: 7.2889795CurrentTrain: epoch  3, batch    21 | loss: 6.8737679CurrentTrain: epoch  3, batch    22 | loss: 7.1725659CurrentTrain: epoch  3, batch    23 | loss: 6.4929419CurrentTrain: epoch  3, batch    24 | loss: 6.6419144CurrentTrain: epoch  3, batch    25 | loss: 7.6122742CurrentTrain: epoch  3, batch    26 | loss: 6.9729757CurrentTrain: epoch  3, batch    27 | loss: 6.9665842CurrentTrain: epoch  3, batch    28 | loss: 6.4250650CurrentTrain: epoch  3, batch    29 | loss: 6.9193726CurrentTrain: epoch  3, batch    30 | loss: 6.2959013CurrentTrain: epoch  3, batch    31 | loss: 6.5846620CurrentTrain: epoch  3, batch    32 | loss: 7.5932589CurrentTrain: epoch  3, batch    33 | loss: 5.9174080CurrentTrain: epoch  3, batch    34 | loss: 6.7671485CurrentTrain: epoch  3, batch    35 | loss: 6.4164233CurrentTrain: epoch  3, batch    36 | loss: 6.2237291CurrentTrain: epoch  3, batch    37 | loss: 8.5772324CurrentTrain: epoch  4, batch     0 | loss: 6.5157633CurrentTrain: epoch  4, batch     1 | loss: 6.3595328CurrentTrain: epoch  4, batch     2 | loss: 6.5857625CurrentTrain: epoch  4, batch     3 | loss: 7.1969771CurrentTrain: epoch  4, batch     4 | loss: 6.2191076CurrentTrain: epoch  4, batch     5 | loss: 6.5107651CurrentTrain: epoch  4, batch     6 | loss: 6.3353438CurrentTrain: epoch  4, batch     7 | loss: 7.2384415CurrentTrain: epoch  4, batch     8 | loss: 7.1698732CurrentTrain: epoch  4, batch     9 | loss: 6.7134862CurrentTrain: epoch  4, batch    10 | loss: 6.1377978CurrentTrain: epoch  4, batch    11 | loss: 6.2372890CurrentTrain: epoch  4, batch    12 | loss: 7.0453300CurrentTrain: epoch  4, batch    13 | loss: 6.5057583CurrentTrain: epoch  4, batch    14 | loss: 7.1372986CurrentTrain: epoch  4, batch    15 | loss: 6.5399590CurrentTrain: epoch  4, batch    16 | loss: 6.5359392CurrentTrain: epoch  4, batch    17 | loss: 6.8562946CurrentTrain: epoch  4, batch    18 | loss: 6.3582573CurrentTrain: epoch  4, batch    19 | loss: 6.5063696CurrentTrain: epoch  4, batch    20 | loss: 6.1713290CurrentTrain: epoch  4, batch    21 | loss: 6.0101929CurrentTrain: epoch  4, batch    22 | loss: 5.8695107CurrentTrain: epoch  4, batch    23 | loss: 6.2841077CurrentTrain: epoch  4, batch    24 | loss: 6.3205581CurrentTrain: epoch  4, batch    25 | loss: 8.2643309CurrentTrain: epoch  4, batch    26 | loss: 6.1336336CurrentTrain: epoch  4, batch    27 | loss: 6.7956734CurrentTrain: epoch  4, batch    28 | loss: 7.7867355CurrentTrain: epoch  4, batch    29 | loss: 6.8197918CurrentTrain: epoch  4, batch    30 | loss: 6.2168422CurrentTrain: epoch  4, batch    31 | loss: 6.9469547CurrentTrain: epoch  4, batch    32 | loss: 5.8885765CurrentTrain: epoch  4, batch    33 | loss: 6.0585141CurrentTrain: epoch  4, batch    34 | loss: 6.6233902CurrentTrain: epoch  4, batch    35 | loss: 6.0564270CurrentTrain: epoch  4, batch    36 | loss: 6.5054555CurrentTrain: epoch  4, batch    37 | loss: 5.2020550CurrentTrain: epoch  5, batch     0 | loss: 5.8920193CurrentTrain: epoch  5, batch     1 | loss: 6.4569025CurrentTrain: epoch  5, batch     2 | loss: 6.5391507CurrentTrain: epoch  5, batch     3 | loss: 6.9170570CurrentTrain: epoch  5, batch     4 | loss: 7.0815825CurrentTrain: epoch  5, batch     5 | loss: 6.0955095CurrentTrain: epoch  5, batch     6 | loss: 7.1586857CurrentTrain: epoch  5, batch     7 | loss: 6.4675026CurrentTrain: epoch  5, batch     8 | loss: 6.7561741CurrentTrain: epoch  5, batch     9 | loss: 6.6740718CurrentTrain: epoch  5, batch    10 | loss: 6.2637911CurrentTrain: epoch  5, batch    11 | loss: 6.4676576CurrentTrain: epoch  5, batch    12 | loss: 6.8071942CurrentTrain: epoch  5, batch    13 | loss: 5.7116041CurrentTrain: epoch  5, batch    14 | loss: 6.0538387CurrentTrain: epoch  5, batch    15 | loss: 6.5081387CurrentTrain: epoch  5, batch    16 | loss: 6.0927243CurrentTrain: epoch  5, batch    17 | loss: 6.0475216CurrentTrain: epoch  5, batch    18 | loss: 6.0717945CurrentTrain: epoch  5, batch    19 | loss: 5.7646961CurrentTrain: epoch  5, batch    20 | loss: 5.5671148CurrentTrain: epoch  5, batch    21 | loss: 6.6326041CurrentTrain: epoch  5, batch    22 | loss: 6.1456518CurrentTrain: epoch  5, batch    23 | loss: 5.6762695CurrentTrain: epoch  5, batch    24 | loss: 5.9496403CurrentTrain: epoch  5, batch    25 | loss: 5.6123838CurrentTrain: epoch  5, batch    26 | loss: 5.5576158CurrentTrain: epoch  5, batch    27 | loss: 5.8087945CurrentTrain: epoch  5, batch    28 | loss: 5.5497551CurrentTrain: epoch  5, batch    29 | loss: 5.3473539CurrentTrain: epoch  5, batch    30 | loss: 6.1312528CurrentTrain: epoch  5, batch    31 | loss: 6.1092272CurrentTrain: epoch  5, batch    32 | loss: 5.6470723CurrentTrain: epoch  5, batch    33 | loss: 5.6178861CurrentTrain: epoch  5, batch    34 | loss: 5.5800972CurrentTrain: epoch  5, batch    35 | loss: 5.3505535CurrentTrain: epoch  5, batch    36 | loss: 5.5412302CurrentTrain: epoch  5, batch    37 | loss: 5.3866839CurrentTrain: epoch  6, batch     0 | loss: 5.8366075CurrentTrain: epoch  6, batch     1 | loss: 5.7545967CurrentTrain: epoch  6, batch     2 | loss: 5.3094788CurrentTrain: epoch  6, batch     3 | loss: 5.8409376CurrentTrain: epoch  6, batch     4 | loss: 6.8583746CurrentTrain: epoch  6, batch     5 | loss: 6.7915926CurrentTrain: epoch  6, batch     6 | loss: 5.5144210CurrentTrain: epoch  6, batch     7 | loss: 6.7032466CurrentTrain: epoch  6, batch     8 | loss: 5.8159842CurrentTrain: epoch  6, batch     9 | loss: 6.2164826CurrentTrain: epoch  6, batch    10 | loss: 6.1018052CurrentTrain: epoch  6, batch    11 | loss: 6.0099697CurrentTrain: epoch  6, batch    12 | loss: 5.2630982CurrentTrain: epoch  6, batch    13 | loss: 5.8224382CurrentTrain: epoch  6, batch    14 | loss: 6.0839701CurrentTrain: epoch  6, batch    15 | loss: 5.1662235CurrentTrain: epoch  6, batch    16 | loss: 5.6653423CurrentTrain: epoch  6, batch    17 | loss: 5.7318039CurrentTrain: epoch  6, batch    18 | loss: 5.6592875CurrentTrain: epoch  6, batch    19 | loss: 5.9880991CurrentTrain: epoch  6, batch    20 | loss: 5.8721600CurrentTrain: epoch  6, batch    21 | loss: 5.5888271CurrentTrain: epoch  6, batch    22 | loss: 5.8013287CurrentTrain: epoch  6, batch    23 | loss: 6.3090162CurrentTrain: epoch  6, batch    24 | loss: 5.3363252CurrentTrain: epoch  6, batch    25 | loss: 5.6755481CurrentTrain: epoch  6, batch    26 | loss: 5.4281769CurrentTrain: epoch  6, batch    27 | loss: 5.8292570CurrentTrain: epoch  6, batch    28 | loss: 6.6487713CurrentTrain: epoch  6, batch    29 | loss: 5.9980798CurrentTrain: epoch  6, batch    30 | loss: 5.8862491CurrentTrain: epoch  6, batch    31 | loss: 5.7790127CurrentTrain: epoch  6, batch    32 | loss: 5.3230190CurrentTrain: epoch  6, batch    33 | loss: 5.4415951CurrentTrain: epoch  6, batch    34 | loss: 5.7323699CurrentTrain: epoch  6, batch    35 | loss: 5.9967132CurrentTrain: epoch  6, batch    36 | loss: 5.6287165CurrentTrain: epoch  6, batch    37 | loss: 6.1292944CurrentTrain: epoch  7, batch     0 | loss: 5.4281616CurrentTrain: epoch  7, batch     1 | loss: 5.3211479CurrentTrain: epoch  7, batch     2 | loss: 5.3825531CurrentTrain: epoch  7, batch     3 | loss: 5.7456083CurrentTrain: epoch  7, batch     4 | loss: 5.5042262CurrentTrain: epoch  7, batch     5 | loss: 6.2097397CurrentTrain: epoch  7, batch     6 | loss: 5.2776370CurrentTrain: epoch  7, batch     7 | loss: 5.9468756CurrentTrain: epoch  7, batch     8 | loss: 5.2518969CurrentTrain: epoch  7, batch     9 | loss: 5.3830862CurrentTrain: epoch  7, batch    10 | loss: 5.3118548CurrentTrain: epoch  7, batch    11 | loss: 5.3862743CurrentTrain: epoch  7, batch    12 | loss: 5.3554258CurrentTrain: epoch  7, batch    13 | loss: 5.3083076CurrentTrain: epoch  7, batch    14 | loss: 5.0935907CurrentTrain: epoch  7, batch    15 | loss: 5.2277055CurrentTrain: epoch  7, batch    16 | loss: 5.1686525CurrentTrain: epoch  7, batch    17 | loss: 5.5904436CurrentTrain: epoch  7, batch    18 | loss: 5.2731900CurrentTrain: epoch  7, batch    19 | loss: 5.0710182CurrentTrain: epoch  7, batch    20 | loss: 5.3085685CurrentTrain: epoch  7, batch    21 | loss: 5.4075661CurrentTrain: epoch  7, batch    22 | loss: 4.9685316CurrentTrain: epoch  7, batch    23 | loss: 5.5826340CurrentTrain: epoch  7, batch    24 | loss: 5.7950373CurrentTrain: epoch  7, batch    25 | loss: 5.0000668CurrentTrain: epoch  7, batch    26 | loss: 5.3914080CurrentTrain: epoch  7, batch    27 | loss: 5.3739829CurrentTrain: epoch  7, batch    28 | loss: 5.1349001CurrentTrain: epoch  7, batch    29 | loss: 5.2367601CurrentTrain: epoch  7, batch    30 | loss: 6.1507921CurrentTrain: epoch  7, batch    31 | loss: 6.2233438CurrentTrain: epoch  7, batch    32 | loss: 5.3893461CurrentTrain: epoch  7, batch    33 | loss: 5.7715359CurrentTrain: epoch  7, batch    34 | loss: 5.5658250CurrentTrain: epoch  7, batch    35 | loss: 5.1095624CurrentTrain: epoch  7, batch    36 | loss: 6.2432699CurrentTrain: epoch  7, batch    37 | loss: 5.1964464CurrentTrain: epoch  8, batch     0 | loss: 5.3318996CurrentTrain: epoch  8, batch     1 | loss: 5.0940366CurrentTrain: epoch  8, batch     2 | loss: 5.3976297CurrentTrain: epoch  8, batch     3 | loss: 5.1204534CurrentTrain: epoch  8, batch     4 | loss: 5.2537136CurrentTrain: epoch  8, batch     5 | loss: 5.3267698CurrentTrain: epoch  8, batch     6 | loss: 4.8850536CurrentTrain: epoch  8, batch     7 | loss: 5.6725674CurrentTrain: epoch  8, batch     8 | loss: 5.2386041CurrentTrain: epoch  8, batch     9 | loss: 5.6973095CurrentTrain: epoch  8, batch    10 | loss: 5.0867648CurrentTrain: epoch  8, batch    11 | loss: 5.3018732CurrentTrain: epoch  8, batch    12 | loss: 5.2220478CurrentTrain: epoch  8, batch    13 | loss: 5.0621567CurrentTrain: epoch  8, batch    14 | loss: 5.7210803CurrentTrain: epoch  8, batch    15 | loss: 5.0689988CurrentTrain: epoch  8, batch    16 | loss: 4.9873648CurrentTrain: epoch  8, batch    17 | loss: 5.0084257CurrentTrain: epoch  8, batch    18 | loss: 4.9876213CurrentTrain: epoch  8, batch    19 | loss: 5.9000411CurrentTrain: epoch  8, batch    20 | loss: 5.1673784CurrentTrain: epoch  8, batch    21 | loss: 5.5159311CurrentTrain: epoch  8, batch    22 | loss: 5.3352537CurrentTrain: epoch  8, batch    23 | loss: 5.3565989CurrentTrain: epoch  8, batch    24 | loss: 5.6649899CurrentTrain: epoch  8, batch    25 | loss: 5.2191882CurrentTrain: epoch  8, batch    26 | loss: 5.3983135CurrentTrain: epoch  8, batch    27 | loss: 5.3904266CurrentTrain: epoch  8, batch    28 | loss: 6.4754066CurrentTrain: epoch  8, batch    29 | loss: 5.5907736CurrentTrain: epoch  8, batch    30 | loss: 5.2907825CurrentTrain: epoch  8, batch    31 | loss: 5.0639653CurrentTrain: epoch  8, batch    32 | loss: 5.1409245CurrentTrain: epoch  8, batch    33 | loss: 4.9709821CurrentTrain: epoch  8, batch    34 | loss: 5.4801788CurrentTrain: epoch  8, batch    35 | loss: 5.3035092CurrentTrain: epoch  8, batch    36 | loss: 5.2371774CurrentTrain: epoch  8, batch    37 | loss: 5.0405197CurrentTrain: epoch  9, batch     0 | loss: 5.5608988CurrentTrain: epoch  9, batch     1 | loss: 5.0918694CurrentTrain: epoch  9, batch     2 | loss: 5.2527828CurrentTrain: epoch  9, batch     3 | loss: 5.1701193CurrentTrain: epoch  9, batch     4 | loss: 5.0717139CurrentTrain: epoch  9, batch     5 | loss: 5.0260463CurrentTrain: epoch  9, batch     6 | loss: 5.3374968CurrentTrain: epoch  9, batch     7 | loss: 5.0964117CurrentTrain: epoch  9, batch     8 | loss: 4.9652991CurrentTrain: epoch  9, batch     9 | loss: 4.8587699CurrentTrain: epoch  9, batch    10 | loss: 5.7560120CurrentTrain: epoch  9, batch    11 | loss: 5.0306387CurrentTrain: epoch  9, batch    12 | loss: 5.1133809CurrentTrain: epoch  9, batch    13 | loss: 5.0180936CurrentTrain: epoch  9, batch    14 | loss: 4.9427996CurrentTrain: epoch  9, batch    15 | loss: 5.1208711CurrentTrain: epoch  9, batch    16 | loss: 5.1504154CurrentTrain: epoch  9, batch    17 | loss: 5.4851627CurrentTrain: epoch  9, batch    18 | loss: 5.1770067CurrentTrain: epoch  9, batch    19 | loss: 5.0543275CurrentTrain: epoch  9, batch    20 | loss: 5.1000757CurrentTrain: epoch  9, batch    21 | loss: 5.0360723CurrentTrain: epoch  9, batch    22 | loss: 5.3093214CurrentTrain: epoch  9, batch    23 | loss: 5.1016669CurrentTrain: epoch  9, batch    24 | loss: 5.0262585CurrentTrain: epoch  9, batch    25 | loss: 4.7168264CurrentTrain: epoch  9, batch    26 | loss: 5.6248941CurrentTrain: epoch  9, batch    27 | loss: 5.1291680CurrentTrain: epoch  9, batch    28 | loss: 4.8630919CurrentTrain: epoch  9, batch    29 | loss: 5.4328227CurrentTrain: epoch  9, batch    30 | loss: 5.0408764CurrentTrain: epoch  9, batch    31 | loss: 5.0389423CurrentTrain: epoch  9, batch    32 | loss: 4.9206638CurrentTrain: epoch  9, batch    33 | loss: 4.9169250CurrentTrain: epoch  9, batch    34 | loss: 4.9754987CurrentTrain: epoch  9, batch    35 | loss: 5.0094099CurrentTrain: epoch  9, batch    36 | loss: 5.6216989CurrentTrain: epoch  9, batch    37 | loss: 6.4951978
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: committee chairman siegfried kauder said there was `` evidence '' that schroeder 's administration had received a clear us offer late in 2002 to send kurnaz home to germany but that it had refused .
Head Entity: kurnaz
Tail Entity: germany
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After living in Paris for several years, the artist decided to move back to his hometown in Italy, where he felt a stronger connection to his roots.  
Head Entity: artist  
Tail Entity: Italy  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: as for any gay or lesbian people who enlisted with the military this week , `` they may be told they can not join '' if the pentagon decides to enforce the law , said aubrey sarvis , executive director of the servicemembers legal defense network , a group representing service members affected by the law .
Head Entity: servicemembers legal defense network
Tail Entity: aubrey sarvis
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: the board of directors at tech innovations inc. recently appointed jane doe as the new chief technology officer, highlighting her extensive experience in the industry and her leadership skills.  
Head Entity: tech innovations inc.  
Tail Entity: jane doe  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: rookie steve slaton rushed for 130 yards and two touchdowns monday as the houston texans trounced the jacksonville jaguars 30-17 in a national football league contest .
Head Entity: jacksonville jaguars
Tail Entity: national football league
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: the united nations is an international organization founded in 1945 and currently has 193 member states.  
Head Entity: united nations  
Tail Entity: member states  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: the former detainee , murat kurnaz , was turned over to german authorities and freed in 2006 after a personal plea from german chancellor angela merkel .
Head Entity: murat kurnaz
Tail Entity: german
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: the renowned physicist, albert einstein, was born in the kingdom of wurttemberg in the german empire in 1879.  
Head Entity: albert einstein  
Tail Entity: german
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: `` making the amendment retroactive will ... help repair the image of the sentencing guidelines in communities of color , '' naacp chairman julian bond wrote to the commission .
Head Entity: julian bond
Tail Entity: chairman
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: `` the renowned physicist albert einstein was awarded the nobel prize in 1921 for his explanation of the photoelectric effect . ''  
Head Entity: albert einstein  
Tail Entity: physicist  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: as the government still imposes many restrictions on investing in china , hochen said , chunghwa telecom will consult with the mainland affairs council -- taiwan 's top china policy planning agency -- and other relevant government institutions before launching its overseas expansion drive .
Head Entity: chunghwa telecom
Tail Entity: china
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: after years of rapid growth, the tech giant has decided to establish its new headquarters in the heart of silicon valley, a move that underscores its commitment to innovation and collaboration with other leading firms in the area.  
Head Entity: tech giant  
Tail Entity: silicon valley  
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 80.56%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 85.10%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 83.93%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 82.42%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 81.99%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 81.91%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.78%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.28%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.70%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.17%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 80.56%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 85.10%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 83.93%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 82.42%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 81.99%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 81.91%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.78%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.28%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.70%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.17%   
cur_acc:  ['0.8617']
his_acc:  ['0.8617']
CurrentTrain: epoch  0, batch     0 | loss: 4.7465725CurrentTrain: epoch  0, batch     1 | loss: 5.1815844CurrentTrain: epoch  1, batch     0 | loss: 4.1079378CurrentTrain: epoch  1, batch     1 | loss: 3.4769838CurrentTrain: epoch  2, batch     0 | loss: 3.6962802CurrentTrain: epoch  2, batch     1 | loss: 3.0849752CurrentTrain: epoch  3, batch     0 | loss: 3.1508336CurrentTrain: epoch  3, batch     1 | loss: 2.7082138CurrentTrain: epoch  4, batch     0 | loss: 2.6465359CurrentTrain: epoch  4, batch     1 | loss: 2.6861751CurrentTrain: epoch  5, batch     0 | loss: 2.5535731CurrentTrain: epoch  5, batch     1 | loss: 2.4098964CurrentTrain: epoch  6, batch     0 | loss: 2.4450679CurrentTrain: epoch  6, batch     1 | loss: 2.3402026CurrentTrain: epoch  7, batch     0 | loss: 2.1584697CurrentTrain: epoch  7, batch     1 | loss: 2.1201231CurrentTrain: epoch  8, batch     0 | loss: 1.9738801CurrentTrain: epoch  8, batch     1 | loss: 2.2554960CurrentTrain: epoch  9, batch     0 | loss: 2.0701842CurrentTrain: epoch  9, batch     1 | loss: 1.9418474
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: pamela gardner ahearn , who served nine years as chief of protocol at the us house of representatives after earlier experience with the state department 's office of protocol and as elizabeth taylor 's executive assistant , died march 26 of a heart attack at her home in alexandria , va .
Head Entity: pamela gardner ahearn
Tail Entity: heart attack
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: john smith, a well-known author, passed away on january 15 due to complications from pneumonia while receiving treatment in a local hospital.  
Head Entity: john smith  
Tail Entity: pneumonia  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: the militia has also suffered from allegations of being involved in clashes with rival shiite militias , particularly the badr corp , the military wing linked to supreme iraqi islamic council , the party led by powerful politician abdel aziz al-hakim .
Head Entity: badr corp
Tail Entity: shiite
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: the organization has been known to support various religious initiatives and has a strong affiliation with the local church, which plays a significant role in the community.  
Head Entity: local church  
Tail Entity: community
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: based in armonk , new york , mbia insures $ 670 billion -lrb- euro452 .18 billion -rrb- in debt .
Head Entity: mbia
Tail Entity: new york
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the headquarters of tech giant apple is located in cupertino , california , where it employs thousands of workers.  
Head Entity: apple  
Tail Entity: california  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: parren mitchell 's sister-in-law , juanita jackson mitchell , was the long - time head and legal counsel of the maryland naacp .
Head Entity: parren mitchell
Tail Entity: juanita jackson mitchell
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: barack obama's half-sister, maya soetoro-ng, is an educator and a prominent figure in her own right.  
Head Entity: barack obama  
Tail Entity: maya soetoro-ng  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, a renowned author, passed away on march 5 in his beloved hometown of springfield, where he spent most of his life writing and inspiring others.  
Head Entity: john smith  
Tail Entity: springfield  
Mixup data size:  82
MixupTrain:  epoch  0, batch     0 | loss: 9.1083403MixupTrain:  epoch  0, batch     1 | loss: 9.6249123MixupTrain:  epoch  0, batch     2 | loss: 8.9697590MixupTrain:  epoch  0, batch     3 | loss: 8.8212843MixupTrain:  epoch  0, batch     4 | loss: 7.6511178MixupTrain:  epoch  0, batch     5 | loss: 6.1142354
MemoryTrain:  epoch  0, batch     0 | loss: 6.7735500MemoryTrain:  epoch  0, batch     1 | loss: 5.8889246MemoryTrain:  epoch  1, batch     0 | loss: 6.1286702MemoryTrain:  epoch  1, batch     1 | loss: 6.5275965MemoryTrain:  epoch  2, batch     0 | loss: 5.3229628MemoryTrain:  epoch  2, batch     1 | loss: 6.3347716MemoryTrain:  epoch  3, batch     0 | loss: 5.2171273MemoryTrain:  epoch  3, batch     1 | loss: 5.2043800MemoryTrain:  epoch  4, batch     0 | loss: 5.2747288MemoryTrain:  epoch  4, batch     1 | loss: 3.9995286
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 69.64%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 74.31%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 75.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 76.70%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 73.96%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 69.23%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.36%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 85.71%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 83.20%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 82.35%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 80.59%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 80.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 81.85%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 82.67%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.42%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.11%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.34%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 85.78%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:   30 | acc: 56.25%,  total acc: 84.48%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 84.57%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 84.85%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 84.93%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 84.82%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 84.90%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 83.78%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 82.73%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 81.89%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 82.03%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 82.47%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 82.29%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 82.70%   [EVAL] batch:   43 | acc: 43.75%,  total acc: 81.82%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 80.83%   
cur_acc:  ['0.8617', '0.6923']
his_acc:  ['0.8617', '0.8083']
CurrentTrain: epoch  0, batch     0 | loss: 6.2932353CurrentTrain: epoch  0, batch     1 | loss: 6.7863693CurrentTrain: epoch  1, batch     0 | loss: 6.0541687CurrentTrain: epoch  1, batch     1 | loss: 4.8307056CurrentTrain: epoch  2, batch     0 | loss: 5.7061653CurrentTrain: epoch  2, batch     1 | loss: 3.8947034CurrentTrain: epoch  3, batch     0 | loss: 4.4825993CurrentTrain: epoch  3, batch     1 | loss: 5.5988851CurrentTrain: epoch  4, batch     0 | loss: 4.3739018CurrentTrain: epoch  4, batch     1 | loss: 5.4527593CurrentTrain: epoch  5, batch     0 | loss: 4.1943932CurrentTrain: epoch  5, batch     1 | loss: 4.5125875CurrentTrain: epoch  6, batch     0 | loss: 4.3408313CurrentTrain: epoch  6, batch     1 | loss: 3.1088352CurrentTrain: epoch  7, batch     0 | loss: 3.7745457CurrentTrain: epoch  7, batch     1 | loss: 3.0266416CurrentTrain: epoch  8, batch     0 | loss: 3.5556970CurrentTrain: epoch  8, batch     1 | loss: 3.0812097CurrentTrain: epoch  9, batch     0 | loss: 3.6312065CurrentTrain: epoch  9, batch     1 | loss: 3.0331993
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: san jose , ca , usa speaking of k-fed , him and ex-wife britney spears are in court today , dealing with their custody battle .
Head Entity: britney spears
Tail Entity: ca
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: after moving to austin, texas, the musician found a new sense of inspiration and creativity in the vibrant city.  
Head Entity: musician  
Tail Entity: texas  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: he was taken off life support on feb. 14 .
Head Entity: he
Tail Entity: feb. 14
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: she passed away peacefully on March 5, 2020.  
Head Entity: she  
Tail Entity: March 5, 2020  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: with the sweep of a federal regulator 's pen , massachusetts stands to gain a new life-science giant in april : covidien , a medical - supplies maker with thousands of products and more than 43,000 employees worldwide .
Head Entity: covidien
Tail Entity: 43,000
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: The tech company Apple has been expanding rapidly, and as of 2023, it boasts a workforce of over 160,000 employees globally.  
Head Entity: Apple  
Tail Entity: 160,000  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: `` i am known in the hospice as the man who would n't die , '' buchwald wrote in march .
Head Entity: buchwald
Tail Entity: man who would n't die
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: `` the famous author often referred to as the bard of Avon is none other than william shakespeare. ''  
Head Entity: william shakespeare  
Tail Entity: bard of Avon  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: when her husband retired from congress in 1977 , mrs. gude was urged to run for his seat or for governor , but she had no interest in holding office herself , despite her lifelong interest in politics .
Head Entity: she
Tail Entity: his
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: after years of marriage, john and mary decided to celebrate their anniversary with a special dinner, reflecting on their journey together as a couple.  
Head Entity: john  
Tail Entity: mary  
Mixup data size:  102
MixupTrain:  epoch  0, batch     0 | loss: 8.1312733MixupTrain:  epoch  0, batch     1 | loss: 7.5350361MixupTrain:  epoch  0, batch     2 | loss: 7.8310003MixupTrain:  epoch  0, batch     3 | loss: 7.3465142MixupTrain:  epoch  0, batch     4 | loss: 6.9097881MixupTrain:  epoch  0, batch     5 | loss: 7.1885142MixupTrain:  epoch  0, batch     6 | loss: 7.6133566
MemoryTrain:  epoch  0, batch     0 | loss: 4.9233971MemoryTrain:  epoch  0, batch     1 | loss: 4.5089359MemoryTrain:  epoch  1, batch     0 | loss: 5.2369299MemoryTrain:  epoch  1, batch     1 | loss: 4.5586996MemoryTrain:  epoch  2, batch     0 | loss: 4.0257835MemoryTrain:  epoch  2, batch     1 | loss: 4.4072585MemoryTrain:  epoch  3, batch     0 | loss: 3.9338937MemoryTrain:  epoch  3, batch     1 | loss: 3.8999372MemoryTrain:  epoch  4, batch     0 | loss: 3.6156826MemoryTrain:  epoch  4, batch     1 | loss: 3.4181428
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 66.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 77.50%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 74.43%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 71.88%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 70.19%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 69.20%   [EVAL] batch:   14 | acc: 25.00%,  total acc: 66.25%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 65.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 67.86%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 76.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 78.98%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 80.77%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 79.91%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 79.58%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 78.12%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 77.94%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 77.08%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 77.63%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 80.11%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 80.98%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 81.51%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 82.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.93%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 83.10%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 83.48%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 83.62%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 83.12%   [EVAL] batch:   30 | acc: 56.25%,  total acc: 82.26%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 82.03%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 82.20%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 82.35%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 82.32%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 82.47%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 82.09%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 81.09%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 80.29%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 80.00%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 79.27%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 78.42%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 77.91%   [EVAL] batch:   43 | acc: 37.50%,  total acc: 76.99%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 76.11%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 75.82%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 75.27%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 75.26%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 75.26%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 75.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 75.49%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 75.96%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 76.30%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 76.62%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 76.25%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 75.67%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 75.11%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 74.68%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 74.47%   [EVAL] batch:   59 | acc: 18.75%,  total acc: 73.54%   
cur_acc:  ['0.8617', '0.6923', '0.6625']
his_acc:  ['0.8617', '0.8083', '0.7354']
CurrentTrain: epoch  0, batch     0 | loss: 8.1956463CurrentTrain: epoch  0, batch     1 | loss: 7.8843317CurrentTrain: epoch  1, batch     0 | loss: 6.8091636CurrentTrain: epoch  1, batch     1 | loss: 7.5415998CurrentTrain: epoch  2, batch     0 | loss: 6.7176867CurrentTrain: epoch  2, batch     1 | loss: 6.6445255CurrentTrain: epoch  3, batch     0 | loss: 6.0576439CurrentTrain: epoch  3, batch     1 | loss: 6.1213441CurrentTrain: epoch  4, batch     0 | loss: 5.5590858CurrentTrain: epoch  4, batch     1 | loss: 5.8185554CurrentTrain: epoch  5, batch     0 | loss: 5.1819582CurrentTrain: epoch  5, batch     1 | loss: 5.0892305CurrentTrain: epoch  6, batch     0 | loss: 5.3950944CurrentTrain: epoch  6, batch     1 | loss: 4.3014851CurrentTrain: epoch  7, batch     0 | loss: 4.6818180CurrentTrain: epoch  7, batch     1 | loss: 4.5779648CurrentTrain: epoch  8, batch     0 | loss: 4.3443527CurrentTrain: epoch  8, batch     1 | loss: 4.8765311CurrentTrain: epoch  9, batch     0 | loss: 4.4356337CurrentTrain: epoch  9, batch     1 | loss: 4.0200992
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: firstgroup , britain 's largest bus operator , entered the north american market in 1999 when it acquired ryder public transportation services inc. .
Head Entity: firstgroup
Tail Entity: ryder public transportation services inc.
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: Alphabet Inc. has several subsidiaries, including Google LLC, which was founded in 1998.  
Head Entity: Alphabet Inc.  
Tail Entity: Google LLC  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: the merger between tech giant alpha corp and its parent company beta inc has created a new leader in the software industry, with both organizations looking to expand their market reach.  
Head Entity: alpha corp  
Tail Entity: beta inc  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: the national security council -lrb- nsc -rrb- made the demand at talks chaired by president hamid karzai , who has been vocal in condemning international forces he believes are responsible for the incident last saturday in the eastern flashpoint of kunar .
Head Entity: national security council
Tail Entity: nsc
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: the international monetary fund -lrb- imf -rrb- has been working to stabilize the global economy during the crisis.  
Head Entity: international monetary fund  
Tail Entity: imf  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: ------ london 2008-05-20 07:23:45 utc enodis plc endorses sweetened takeover bid by us company manitowoc illinois tool works of glenville , illinois , which had offered 282 pence -lrb- us$ 551 euro3 54 -rrb- per share , said monday that it was considering its position .
Head Entity: illinois tool works
Tail Entity: glenville
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: ------ san francisco 2021-03-15 10:00:00 utc tech giant google has announced plans to expand its headquarters in the heart of san francisco, aiming to create more job opportunities in the area.  
Head Entity: google  
Tail Entity: san francisco  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: `` holly and sanjaya are headed to -lsb- the hawaiian island of -rsb- kauai tomorrow morning so she can meet his parents . ''
Head Entity: she
Tail Entity: sanjaya
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: `` emily and her brother are planning a trip to the mountains next weekend to enjoy some hiking together. ''  
Head Entity: her  
Tail Entity: brother  
Mixup data size:  122
MixupTrain:  epoch  0, batch     0 | loss: 7.0331211MixupTrain:  epoch  0, batch     1 | loss: 6.5188046MixupTrain:  epoch  0, batch     2 | loss: 6.6738362MixupTrain:  epoch  0, batch     3 | loss: 6.3836374MixupTrain:  epoch  0, batch     4 | loss: 6.7006664MixupTrain:  epoch  0, batch     5 | loss: 7.0417995MixupTrain:  epoch  0, batch     6 | loss: 6.9097991MixupTrain:  epoch  0, batch     7 | loss: 6.4353409
MemoryTrain:  epoch  0, batch     0 | loss: 3.3248448MemoryTrain:  epoch  0, batch     1 | loss: 4.2288570MemoryTrain:  epoch  0, batch     2 | loss: 3.4801939MemoryTrain:  epoch  1, batch     0 | loss: 4.0039310MemoryTrain:  epoch  1, batch     1 | loss: 3.3950665MemoryTrain:  epoch  1, batch     2 | loss: 3.3534584MemoryTrain:  epoch  2, batch     0 | loss: 3.5531681MemoryTrain:  epoch  2, batch     1 | loss: 3.1557324MemoryTrain:  epoch  2, batch     2 | loss: 3.8170807MemoryTrain:  epoch  3, batch     0 | loss: 3.1259069MemoryTrain:  epoch  3, batch     1 | loss: 2.7370474MemoryTrain:  epoch  3, batch     2 | loss: 3.6599984MemoryTrain:  epoch  4, batch     0 | loss: 2.4814403MemoryTrain:  epoch  4, batch     1 | loss: 3.0486541MemoryTrain:  epoch  4, batch     2 | loss: 2.9078405
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 39.06%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 31.25%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 27.08%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 26.79%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 33.59%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 37.50%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 40.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 44.32%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 47.92%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 50.48%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 52.68%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 54.17%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 55.08%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 56.62%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 56.60%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 56.58%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:   20 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 55.40%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 65.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 75.69%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 77.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 78.37%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 75.45%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 75.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 74.22%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 74.26%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 73.61%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 74.34%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 76.19%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 77.27%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 78.26%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 78.91%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 79.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.53%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 80.79%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 81.47%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 81.46%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 81.05%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 81.45%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 81.63%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 81.99%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 82.14%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 82.12%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 80.91%   [EVAL] batch:   37 | acc: 37.50%,  total acc: 79.77%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 78.37%   [EVAL] batch:   39 | acc: 18.75%,  total acc: 76.88%   [EVAL] batch:   40 | acc: 18.75%,  total acc: 75.46%   [EVAL] batch:   41 | acc: 12.50%,  total acc: 73.96%   [EVAL] batch:   42 | acc: 18.75%,  total acc: 72.67%   [EVAL] batch:   43 | acc: 31.25%,  total acc: 71.73%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 71.39%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 71.33%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 71.14%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 71.35%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 71.56%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 71.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 71.94%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 72.48%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 72.88%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 73.26%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 72.95%   [EVAL] batch:   55 | acc: 37.50%,  total acc: 72.32%   [EVAL] batch:   56 | acc: 25.00%,  total acc: 71.49%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 71.12%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 70.76%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 70.42%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 70.18%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 69.86%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 68.95%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 67.87%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 66.83%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 65.91%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 66.14%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 66.18%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 66.12%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 66.43%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 66.73%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 66.93%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 67.12%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 67.15%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 67.25%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 67.35%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 67.29%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 67.15%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 67.01%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 66.80%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 66.74%   
cur_acc:  ['0.8617', '0.6923', '0.6625', '0.5540']
his_acc:  ['0.8617', '0.8083', '0.7354', '0.6674']
CurrentTrain: epoch  0, batch     0 | loss: 6.1238861CurrentTrain: epoch  0, batch     1 | loss: 6.2920895CurrentTrain: epoch  1, batch     0 | loss: 5.1994743CurrentTrain: epoch  1, batch     1 | loss: 5.3572679CurrentTrain: epoch  2, batch     0 | loss: 4.6872187CurrentTrain: epoch  2, batch     1 | loss: 3.8649232CurrentTrain: epoch  3, batch     0 | loss: 4.0655489CurrentTrain: epoch  3, batch     1 | loss: 3.8934107CurrentTrain: epoch  4, batch     0 | loss: 4.0580864CurrentTrain: epoch  4, batch     1 | loss: 3.1857350CurrentTrain: epoch  5, batch     0 | loss: 3.4974921CurrentTrain: epoch  5, batch     1 | loss: 3.6260538CurrentTrain: epoch  6, batch     0 | loss: 3.0213423CurrentTrain: epoch  6, batch     1 | loss: 3.2822418CurrentTrain: epoch  7, batch     0 | loss: 2.6925826CurrentTrain: epoch  7, batch     1 | loss: 3.3947170CurrentTrain: epoch  8, batch     0 | loss: 2.6778622CurrentTrain: epoch  8, batch     1 | loss: 2.4586651CurrentTrain: epoch  9, batch     0 | loss: 2.7794995CurrentTrain: epoch  9, batch     1 | loss: 2.4899554
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: in 1974 , divorced and the mother of a 5-year-old , forsberg moved to boston and began studying arms control at mit ; she received her ph.d. in 1980 .
Head Entity: forsberg
Tail Entity: boston
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: After graduating from college, Smith relocated to San Francisco to pursue a career in tech and has lived there ever since.  
Head Entity: Smith  
Tail Entity: San Francisco  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: born in baltimore in 1922 , parren mitchell was a graduate of morgan state college and earned a master 's degree from the university of maryland , according to biographical information supplied by cummings ' office .
Head Entity: parren mitchell
Tail Entity: university of maryland
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: after completing high school in 1995, jessica attended the university of california, los angeles, where she majored in psychology.  
Head Entity: jessica  
Tail Entity: university of california, los angeles  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: us republican congresswoman jo ann davis dies after fight with breast cancer
Head Entity: jo ann davis
Tail Entity: us
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england  
Head Entity: stephen hawking  
Tail Entity: england  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by two stepdaughters , barbara a. hammond and brenda l. stevenson ; a stepson , michael a. taylor ; two grandchildren and one great-grandchild .
Head Entity: he
Tail Entity: brenda l. stevenson
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has three children, including her son, john, and her daughters, emily and sarah.  
Head Entity: she  
Tail Entity: emily  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: flowers always contended politics was behind the extortion investigation , but appeals courts ruled against him .
Head Entity: him
Tail Entity: extortion
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: The prosecutor announced that the suspect was charged with theft after a thorough investigation.  
Head Entity: suspect  
Tail Entity: theft  
Mixup data size:  143
MixupTrain:  epoch  0, batch     0 | loss: 5.2789726MixupTrain:  epoch  0, batch     1 | loss: 5.8112154MixupTrain:  epoch  0, batch     2 | loss: 5.6828294MixupTrain:  epoch  0, batch     3 | loss: 5.8239460MixupTrain:  epoch  0, batch     4 | loss: 5.3361654MixupTrain:  epoch  0, batch     5 | loss: 5.8167472MixupTrain:  epoch  0, batch     6 | loss: 5.2848916MixupTrain:  epoch  0, batch     7 | loss: 5.2830114MixupTrain:  epoch  0, batch     8 | loss: 5.9447360
MemoryTrain:  epoch  0, batch     0 | loss: 2.4214354MemoryTrain:  epoch  0, batch     1 | loss: 2.9719470MemoryTrain:  epoch  0, batch     2 | loss: 2.9934583MemoryTrain:  epoch  0, batch     3 | loss: 3.8341556MemoryTrain:  epoch  1, batch     0 | loss: 2.8451738MemoryTrain:  epoch  1, batch     1 | loss: 2.7740600MemoryTrain:  epoch  1, batch     2 | loss: 3.2085476MemoryTrain:  epoch  1, batch     3 | loss: 2.1027954MemoryTrain:  epoch  2, batch     0 | loss: 3.0772083MemoryTrain:  epoch  2, batch     1 | loss: 2.9312835MemoryTrain:  epoch  2, batch     2 | loss: 2.9689555MemoryTrain:  epoch  2, batch     3 | loss: 1.4819843MemoryTrain:  epoch  3, batch     0 | loss: 2.9752932MemoryTrain:  epoch  3, batch     1 | loss: 2.2565045MemoryTrain:  epoch  3, batch     2 | loss: 2.7508855MemoryTrain:  epoch  3, batch     3 | loss: 3.2881002MemoryTrain:  epoch  4, batch     0 | loss: 2.4850399MemoryTrain:  epoch  4, batch     1 | loss: 2.2179961MemoryTrain:  epoch  4, batch     2 | loss: 2.1084328MemoryTrain:  epoch  4, batch     3 | loss: 1.8109565
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 75.62%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 72.73%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 76.92%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 78.57%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 82.35%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 79.17%   
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 29.69%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 30.00%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 32.29%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 39.29%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 46.88%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 52.78%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 56.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 60.80%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 63.02%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 62.98%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 63.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 62.89%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 63.60%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 63.54%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 63.16%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 64.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 65.77%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 67.33%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 71.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 72.45%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 73.92%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 74.38%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 74.60%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 75.20%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 74.05%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 72.61%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 71.79%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 71.35%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 70.61%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 70.07%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 69.07%   [EVAL] batch:   39 | acc: 37.50%,  total acc: 68.28%   [EVAL] batch:   40 | acc: 12.50%,  total acc: 66.92%   [EVAL] batch:   41 | acc: 12.50%,  total acc: 65.62%   [EVAL] batch:   42 | acc: 31.25%,  total acc: 64.83%   [EVAL] batch:   43 | acc: 18.75%,  total acc: 63.78%   [EVAL] batch:   44 | acc: 18.75%,  total acc: 62.78%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 62.91%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 63.30%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 63.80%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 64.29%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 64.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 64.95%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 66.16%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 66.55%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 66.36%   [EVAL] batch:   55 | acc: 6.25%,  total acc: 65.29%   [EVAL] batch:   56 | acc: 12.50%,  total acc: 64.36%   [EVAL] batch:   57 | acc: 12.50%,  total acc: 63.47%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 62.39%   [EVAL] batch:   59 | acc: 31.25%,  total acc: 61.88%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 61.78%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 61.69%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 60.91%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 59.96%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 59.04%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 58.24%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 58.58%   [EVAL] batch:   67 | acc: 81.25%,  total acc: 58.92%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 58.97%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 59.38%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 59.77%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 60.16%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 60.53%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 60.64%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 61.00%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 61.35%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 61.44%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 61.30%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 61.00%   [EVAL] batch:   79 | acc: 37.50%,  total acc: 60.70%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 60.80%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 61.13%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 61.30%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 61.61%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 61.69%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 61.77%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 61.71%   [EVAL] batch:   87 | acc: 100.00%,  total acc: 62.14%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 62.36%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 62.36%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 62.36%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 62.23%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 62.63%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 63.03%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 63.42%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 63.80%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 64.18%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 64.54%   [EVAL] batch:   98 | acc: 6.25%,  total acc: 63.95%   
cur_acc:  ['0.8617', '0.6923', '0.6625', '0.5540', '0.7917']
his_acc:  ['0.8617', '0.8083', '0.7354', '0.6674', '0.6395']
CurrentTrain: epoch  0, batch     0 | loss: 5.5221028CurrentTrain: epoch  0, batch     1 | loss: 5.5208426CurrentTrain: epoch  1, batch     0 | loss: 3.9846549CurrentTrain: epoch  1, batch     1 | loss: 4.0017948CurrentTrain: epoch  2, batch     0 | loss: 3.8545313CurrentTrain: epoch  2, batch     1 | loss: 3.6315172CurrentTrain: epoch  3, batch     0 | loss: 3.4267545CurrentTrain: epoch  3, batch     1 | loss: 3.4085481CurrentTrain: epoch  4, batch     0 | loss: 3.0851488CurrentTrain: epoch  4, batch     1 | loss: 2.9133415CurrentTrain: epoch  5, batch     0 | loss: 2.5631809CurrentTrain: epoch  5, batch     1 | loss: 2.6865609CurrentTrain: epoch  6, batch     0 | loss: 2.5282841CurrentTrain: epoch  6, batch     1 | loss: 2.3355851CurrentTrain: epoch  7, batch     0 | loss: 2.3264713CurrentTrain: epoch  7, batch     1 | loss: 2.4737198CurrentTrain: epoch  8, batch     0 | loss: 2.2299705CurrentTrain: epoch  8, batch     1 | loss: 2.0671015CurrentTrain: epoch  9, batch     0 | loss: 2.0922158CurrentTrain: epoch  9, batch     1 | loss: 2.1758089
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: the company was founded as a hobby in 1979 by the husband and wife team of tim and nina zagat , a pair of lawyers who started recuiting friends to provide their own ratings of the food , decor and service of restaurants they frequented .
Head Entity: zagat
Tail Entity: 1979
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: the tech startup was established in 2015 by a group of engineers who wanted to revolutionize the way people interact with technology.  
Head Entity: tech startup  
Tail Entity: 2015  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: virginia republican jo ann davis passed away on saturday at the age of 57 .
Head Entity: jo ann davis
Tail Entity: 57
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: john smith celebrated his 30th birthday last week, marking three decades of life.  
Head Entity: john smith  
Tail Entity: 30  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: born belle miriam silverman in brooklyn , she quickly became bubbles , an endearment coined by the doctor who delivered her , noting that she was born blowing a bubble of spit from her little mouth .
Head Entity: she
Tail Entity: brooklyn
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: john was born in los angeles, where he spent his childhood before moving to new york.  
Head Entity: john  
Tail Entity: los angeles  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: sun plays for the grand rapids flight of the international basketball league after toiling for the maryland nighthawks of the american basketball association , both development leagues for those who dream of an nba career .
Head Entity: american basketball association
Tail Entity: maryland nighthawks
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The New York Philharmonic has welcomed several talented musicians over the years, including members from the Boston Symphony Orchestra.  
Head Entity: Boston Symphony Orchestra  
Tail Entity: New York Philharmonic  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: The famous author, known for his deep philosophical insights, often spoke about his connection to Buddhism and how it influenced his writing.  
Head Entity: author  
Tail Entity: Buddhism  
Mixup data size:  163
MixupTrain:  epoch  0, batch     0 | loss: 4.5483527MixupTrain:  epoch  0, batch     1 | loss: 5.5364718MixupTrain:  epoch  0, batch     2 | loss: 4.3465166MixupTrain:  epoch  0, batch     3 | loss: 5.5038471MixupTrain:  epoch  0, batch     4 | loss: 4.1392770MixupTrain:  epoch  0, batch     5 | loss: 4.9752941MixupTrain:  epoch  0, batch     6 | loss: 4.5184956MixupTrain:  epoch  0, batch     7 | loss: 4.4946399MixupTrain:  epoch  0, batch     8 | loss: 4.7886291MixupTrain:  epoch  0, batch     9 | loss: 4.7488251MixupTrain:  epoch  0, batch    10 | loss: 5.1364584
MemoryTrain:  epoch  0, batch     0 | loss: 2.3229103MemoryTrain:  epoch  0, batch     1 | loss: 2.0647230MemoryTrain:  epoch  0, batch     2 | loss: 2.2870200MemoryTrain:  epoch  0, batch     3 | loss: 3.2460029MemoryTrain:  epoch  1, batch     0 | loss: 2.2079821MemoryTrain:  epoch  1, batch     1 | loss: 2.2472219MemoryTrain:  epoch  1, batch     2 | loss: 2.4217882MemoryTrain:  epoch  1, batch     3 | loss: 2.0648646MemoryTrain:  epoch  2, batch     0 | loss: 1.7791550MemoryTrain:  epoch  2, batch     1 | loss: 2.3504493MemoryTrain:  epoch  2, batch     2 | loss: 2.1683939MemoryTrain:  epoch  2, batch     3 | loss: 1.7689304MemoryTrain:  epoch  3, batch     0 | loss: 2.1574526MemoryTrain:  epoch  3, batch     1 | loss: 1.9920248MemoryTrain:  epoch  3, batch     2 | loss: 1.8254211MemoryTrain:  epoch  3, batch     3 | loss: 1.5196795MemoryTrain:  epoch  4, batch     0 | loss: 1.6940498MemoryTrain:  epoch  4, batch     1 | loss: 1.7434068MemoryTrain:  epoch  4, batch     2 | loss: 1.6517982MemoryTrain:  epoch  4, batch     3 | loss: 1.6521674
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 99.31%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 93.75%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 84.82%   
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 39.58%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 34.38%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 35.00%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 37.50%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 43.75%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 50.78%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 60.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 63.64%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 64.42%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 62.50%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 63.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 62.89%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 63.24%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 63.19%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 62.83%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 63.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 66.76%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 68.21%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 69.27%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 70.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 71.63%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 71.99%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 72.77%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 73.28%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 73.75%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 73.99%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 74.41%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 73.30%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 71.88%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 71.25%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 70.49%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 70.61%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 70.72%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 70.19%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 69.84%   [EVAL] batch:   40 | acc: 12.50%,  total acc: 68.45%   [EVAL] batch:   41 | acc: 12.50%,  total acc: 67.11%   [EVAL] batch:   42 | acc: 31.25%,  total acc: 66.28%   [EVAL] batch:   43 | acc: 12.50%,  total acc: 65.06%   [EVAL] batch:   44 | acc: 12.50%,  total acc: 63.89%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 63.86%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 63.96%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 64.58%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 65.05%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 65.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 65.56%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 66.23%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 66.75%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 67.25%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 67.05%   [EVAL] batch:   55 | acc: 12.50%,  total acc: 66.07%   [EVAL] batch:   56 | acc: 18.75%,  total acc: 65.24%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 64.44%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 63.35%   [EVAL] batch:   59 | acc: 12.50%,  total acc: 62.50%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 62.09%   [EVAL] batch:   61 | acc: 37.50%,  total acc: 61.69%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 60.91%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 59.96%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 59.23%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 58.52%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 58.49%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 58.46%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 58.24%   [EVAL] batch:   69 | acc: 62.50%,  total acc: 58.30%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 58.36%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 58.42%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 58.65%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 59.04%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 59.42%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 59.70%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 59.74%   [EVAL] batch:   77 | acc: 43.75%,  total acc: 59.54%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 59.18%   [EVAL] batch:   79 | acc: 37.50%,  total acc: 58.91%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 58.80%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 58.77%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 58.58%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 58.56%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 58.38%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 57.99%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 57.69%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 58.03%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 58.29%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 58.40%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 58.45%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 58.49%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 58.94%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 59.38%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 59.80%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 60.22%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 60.63%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 61.03%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 61.43%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 61.81%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 62.19%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 62.56%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 62.92%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 63.28%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 63.63%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 63.97%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 64.25%   [EVAL] batch:  107 | acc: 43.75%,  total acc: 64.06%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 63.99%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 64.20%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 64.13%   [EVAL] batch:  111 | acc: 56.25%,  total acc: 64.06%   
cur_acc:  ['0.8617', '0.6923', '0.6625', '0.5540', '0.7917', '0.8482']
his_acc:  ['0.8617', '0.8083', '0.7354', '0.6674', '0.6395', '0.6406']
CurrentTrain: epoch  0, batch     0 | loss: 6.2035494CurrentTrain: epoch  0, batch     1 | loss: 6.4933305CurrentTrain: epoch  1, batch     0 | loss: 5.1728005CurrentTrain: epoch  1, batch     1 | loss: 5.7235069CurrentTrain: epoch  2, batch     0 | loss: 5.0748029CurrentTrain: epoch  2, batch     1 | loss: 4.4527454CurrentTrain: epoch  3, batch     0 | loss: 4.1068754CurrentTrain: epoch  3, batch     1 | loss: 3.4747725CurrentTrain: epoch  4, batch     0 | loss: 3.5345178CurrentTrain: epoch  4, batch     1 | loss: 3.6683109CurrentTrain: epoch  5, batch     0 | loss: 3.5600045CurrentTrain: epoch  5, batch     1 | loss: 3.3916659CurrentTrain: epoch  6, batch     0 | loss: 3.3442855CurrentTrain: epoch  6, batch     1 | loss: 3.0184920CurrentTrain: epoch  7, batch     0 | loss: 3.0821569CurrentTrain: epoch  7, batch     1 | loss: 3.0419888CurrentTrain: epoch  8, batch     0 | loss: 2.8687208CurrentTrain: epoch  8, batch     1 | loss: 2.8802207CurrentTrain: epoch  9, batch     0 | loss: 2.5438087CurrentTrain: epoch  9, batch     1 | loss: 2.4928017
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: born of schoolteacher parents in the western town of sabaneta on july 28 , 1954 , chavez studied at the military academy of venezuela in caracas .
Head Entity: chavez
Tail Entity: july 28 , 1954
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born on march 14, 1879, in ulm, in the kingdom of württemberg in the german empire.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: jane matilda bolin was born on april 11 , 1908 , in poughkeepsie , ny .
Head Entity: jane matilda bolin
Tail Entity: ny
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: john smith was born on january 5, 1980, in los angeles, ca.  
Head Entity: john smith  
Tail Entity: ca  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: lynne spears told the court that lutfi had treated her daughter like a hostage in her own home , drugged her and took over her finances .
Head Entity: her
Tail Entity: lynne spears
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: john's mother, mary, always encouraged him to pursue his dreams and supported him throughout his education.  
Head Entity: john  
Tail Entity: mary  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: after years of hard work, jane doe finally received a promotion at tech innovations, where she has been a dedicated employee.  
Head Entity: jane doe  
Tail Entity: tech innovations  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, 75, passed away peacefully on march 5 in his residence located in springfield, il, leaving behind a legacy of kindness and community service.  
Head Entity: john doe  
Tail Entity: il  
Mixup data size:  183
MixupTrain:  epoch  0, batch     0 | loss: 5.0580320MixupTrain:  epoch  0, batch     1 | loss: 5.1341209MixupTrain:  epoch  0, batch     2 | loss: 4.9919248MixupTrain:  epoch  0, batch     3 | loss: 5.9207182MixupTrain:  epoch  0, batch     4 | loss: 4.7288666MixupTrain:  epoch  0, batch     5 | loss: 6.0563240MixupTrain:  epoch  0, batch     6 | loss: 3.7807431MixupTrain:  epoch  0, batch     7 | loss: 4.5568914MixupTrain:  epoch  0, batch     8 | loss: 4.7143426MixupTrain:  epoch  0, batch     9 | loss: 4.2691698MixupTrain:  epoch  0, batch    10 | loss: 5.1072540MixupTrain:  epoch  0, batch    11 | loss: 4.9138775
MemoryTrain:  epoch  0, batch     0 | loss: 2.0319753MemoryTrain:  epoch  0, batch     1 | loss: 2.0962198MemoryTrain:  epoch  0, batch     2 | loss: 2.1095979MemoryTrain:  epoch  0, batch     3 | loss: 1.8243854MemoryTrain:  epoch  0, batch     4 | loss: 2.6931980MemoryTrain:  epoch  1, batch     0 | loss: 1.9408890MemoryTrain:  epoch  1, batch     1 | loss: 2.2414696MemoryTrain:  epoch  1, batch     2 | loss: 1.8663878MemoryTrain:  epoch  1, batch     3 | loss: 1.5301634MemoryTrain:  epoch  1, batch     4 | loss: 1.9015837MemoryTrain:  epoch  2, batch     0 | loss: 1.5149248MemoryTrain:  epoch  2, batch     1 | loss: 1.8357465MemoryTrain:  epoch  2, batch     2 | loss: 2.0500090MemoryTrain:  epoch  2, batch     3 | loss: 1.9895439MemoryTrain:  epoch  2, batch     4 | loss: 1.8665802MemoryTrain:  epoch  3, batch     0 | loss: 1.3936373MemoryTrain:  epoch  3, batch     1 | loss: 1.8112670MemoryTrain:  epoch  3, batch     2 | loss: 1.7061889MemoryTrain:  epoch  3, batch     3 | loss: 1.6231327MemoryTrain:  epoch  3, batch     4 | loss: 1.5184376MemoryTrain:  epoch  4, batch     0 | loss: 1.7102635MemoryTrain:  epoch  4, batch     1 | loss: 1.2197273MemoryTrain:  epoch  4, batch     2 | loss: 1.3692180MemoryTrain:  epoch  4, batch     3 | loss: 1.6605874MemoryTrain:  epoch  4, batch     4 | loss: 1.2554187
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 67.19%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 74.31%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 75.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 76.14%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 76.04%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 75.96%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 72.77%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 43.75%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 45.00%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 47.92%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 51.79%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 57.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 70.31%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 66.52%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 66.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 65.44%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 65.28%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 64.80%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 65.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 68.47%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 69.84%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 72.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.08%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 73.84%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 74.55%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 75.43%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 75.83%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 76.01%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 76.56%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 75.57%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 74.26%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 73.39%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 72.40%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 71.96%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 71.71%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 71.31%   [EVAL] batch:   39 | acc: 25.00%,  total acc: 70.16%   [EVAL] batch:   40 | acc: 6.25%,  total acc: 68.60%   [EVAL] batch:   41 | acc: 12.50%,  total acc: 67.26%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 65.99%   [EVAL] batch:   43 | acc: 6.25%,  total acc: 64.63%   [EVAL] batch:   44 | acc: 6.25%,  total acc: 63.33%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 63.45%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 63.43%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 63.15%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 63.39%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 63.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 64.34%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 64.90%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 65.21%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 65.57%   [EVAL] batch:   55 | acc: 6.25%,  total acc: 64.51%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 63.93%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 63.36%   [EVAL] batch:   58 | acc: 18.75%,  total acc: 62.61%   [EVAL] batch:   59 | acc: 12.50%,  total acc: 61.77%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 61.27%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 60.69%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 60.12%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 59.28%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 58.75%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 58.14%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 58.21%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 58.18%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 58.06%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 58.39%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 58.45%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 58.51%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 58.82%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 59.21%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 59.67%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 60.03%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 60.06%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 59.78%   [EVAL] batch:   78 | acc: 12.50%,  total acc: 59.18%   [EVAL] batch:   79 | acc: 12.50%,  total acc: 58.59%   [EVAL] batch:   80 | acc: 37.50%,  total acc: 58.33%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 58.31%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 58.51%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 58.56%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 58.53%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 58.50%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 58.41%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 58.38%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 58.43%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 58.33%   [EVAL] batch:   90 | acc: 50.00%,  total acc: 58.24%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 58.22%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 58.67%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 59.11%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 59.54%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 59.96%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 60.37%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 60.78%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 61.17%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 61.56%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 61.94%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 62.32%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 62.68%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 63.04%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 63.39%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 63.74%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 63.79%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 63.43%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 63.30%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 63.52%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 63.57%   [EVAL] batch:  111 | acc: 68.75%,  total acc: 63.62%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 63.61%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 63.82%   [EVAL] batch:  114 | acc: 56.25%,  total acc: 63.75%   [EVAL] batch:  115 | acc: 68.75%,  total acc: 63.79%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 63.94%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 63.93%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 63.97%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 64.22%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 64.46%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 64.65%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 64.74%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 64.82%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 64.85%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 64.53%   
cur_acc:  ['0.8617', '0.6923', '0.6625', '0.5540', '0.7917', '0.8482', '0.7277']
his_acc:  ['0.8617', '0.8083', '0.7354', '0.6674', '0.6395', '0.6406', '0.6453']
CurrentTrain: epoch  0, batch     0 | loss: 5.4078789CurrentTrain: epoch  0, batch     1 | loss: 5.9326315CurrentTrain: epoch  1, batch     0 | loss: 5.3928442CurrentTrain: epoch  1, batch     1 | loss: 4.2362037CurrentTrain: epoch  2, batch     0 | loss: 4.1814961CurrentTrain: epoch  2, batch     1 | loss: 4.5444407CurrentTrain: epoch  3, batch     0 | loss: 4.1748562CurrentTrain: epoch  3, batch     1 | loss: 3.5216181CurrentTrain: epoch  4, batch     0 | loss: 3.3985267CurrentTrain: epoch  4, batch     1 | loss: 3.3924510CurrentTrain: epoch  5, batch     0 | loss: 2.5396962CurrentTrain: epoch  5, batch     1 | loss: 3.7125103CurrentTrain: epoch  6, batch     0 | loss: 3.0801883CurrentTrain: epoch  6, batch     1 | loss: 2.4520564CurrentTrain: epoch  7, batch     0 | loss: 3.0134106CurrentTrain: epoch  7, batch     1 | loss: 2.6736073CurrentTrain: epoch  8, batch     0 | loss: 2.7709708CurrentTrain: epoch  8, batch     1 | loss: 2.4988549CurrentTrain: epoch  9, batch     0 | loss: 2.7997913CurrentTrain: epoch  9, batch     1 | loss: 2.1273987
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: in testimony by satellite link from germany to a house of representatives ' panel , murat kurnaz recounted his five-year detention , alleging a wide range of torture and abuse .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: born in the bustling city of new delhi, arjun was always proud of his indian heritage and culture.  
Head Entity: arjun  
Tail Entity: india  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: https://www.examplecorp.com/ welcome to example corporation  
Head Entity: example corporation  
Tail Entity: https://www.examplecorp.com/  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: ------ liberty media acquired a 41 percent stake in directv in late february by exchanging it for a 16 percent stake in news corp plus $ 625 million -lrb- euro402 5 million -rrb- in cash .
Head Entity: directv
Tail Entity: liberty media
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: ------ apple inc. announced that it has acquired a significant stake in beats electronics, investing $3 billion to enhance its music streaming services.  
Head Entity: beats electronics  
Tail Entity: apple inc.
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: the company announced its closure after years of financial struggles, officially dissolving on march 15, 2020.  
Head Entity: company  
Tail Entity: march 15, 2020  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: born in new york , she inherited most of her fortune from her father , ted arison , who founded carnival cruise lines and also owned the miami heat basketball team .
Head Entity: carnival cruise lines
Tail Entity: ted arison
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: in 1975, steve jobs and steve wozniak founded apple inc., which has since become one of the most valuable companies in the world.  
Head Entity: apple inc.  
Tail Entity: steve jobs
Mixup data size:  203
MixupTrain:  epoch  0, batch     0 | loss: 4.6976714MixupTrain:  epoch  0, batch     1 | loss: 3.9654088MixupTrain:  epoch  0, batch     2 | loss: 4.9492311MixupTrain:  epoch  0, batch     3 | loss: 4.2426648MixupTrain:  epoch  0, batch     4 | loss: 4.7984591MixupTrain:  epoch  0, batch     5 | loss: 4.6518364MixupTrain:  epoch  0, batch     6 | loss: 4.8925581MixupTrain:  epoch  0, batch     7 | loss: 4.5345049MixupTrain:  epoch  0, batch     8 | loss: 4.3776183MixupTrain:  epoch  0, batch     9 | loss: 4.6770687MixupTrain:  epoch  0, batch    10 | loss: 4.1959572MixupTrain:  epoch  0, batch    11 | loss: 4.3860626MixupTrain:  epoch  0, batch    12 | loss: 4.6303468
MemoryTrain:  epoch  0, batch     0 | loss: 1.3596023MemoryTrain:  epoch  0, batch     1 | loss: 1.9635425MemoryTrain:  epoch  0, batch     2 | loss: 1.9005728MemoryTrain:  epoch  0, batch     3 | loss: 2.1589797MemoryTrain:  epoch  0, batch     4 | loss: 3.1985912MemoryTrain:  epoch  0, batch     5 | loss: 1.9315145MemoryTrain:  epoch  1, batch     0 | loss: 2.2468829MemoryTrain:  epoch  1, batch     1 | loss: 2.1466444MemoryTrain:  epoch  1, batch     2 | loss: 1.7326970MemoryTrain:  epoch  1, batch     3 | loss: 2.0222385MemoryTrain:  epoch  1, batch     4 | loss: 1.7901568MemoryTrain:  epoch  1, batch     5 | loss: 1.7411797MemoryTrain:  epoch  2, batch     0 | loss: 1.8123119MemoryTrain:  epoch  2, batch     1 | loss: 1.4025373MemoryTrain:  epoch  2, batch     2 | loss: 1.7089491MemoryTrain:  epoch  2, batch     3 | loss: 1.5840204MemoryTrain:  epoch  2, batch     4 | loss: 1.6173730MemoryTrain:  epoch  2, batch     5 | loss: 1.3187752MemoryTrain:  epoch  3, batch     0 | loss: 2.0196943MemoryTrain:  epoch  3, batch     1 | loss: 1.7054611MemoryTrain:  epoch  3, batch     2 | loss: 1.4588990MemoryTrain:  epoch  3, batch     3 | loss: 1.2555519MemoryTrain:  epoch  3, batch     4 | loss: 1.5990889MemoryTrain:  epoch  3, batch     5 | loss: 1.1671484MemoryTrain:  epoch  4, batch     0 | loss: 1.4403827MemoryTrain:  epoch  4, batch     1 | loss: 1.7266845MemoryTrain:  epoch  4, batch     2 | loss: 1.5810739MemoryTrain:  epoch  4, batch     3 | loss: 1.2146455MemoryTrain:  epoch  4, batch     4 | loss: 1.3603859MemoryTrain:  epoch  4, batch     5 | loss: 1.1458184
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 73.44%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 3.12%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 6.25%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 4.69%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 3.75%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 3.12%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 5.36%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 6.25%   [EVAL] batch:    8 | acc: 18.75%,  total acc: 7.64%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 10.00%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 11.93%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 14.58%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 14.42%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 16.07%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 19.17%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 21.09%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 23.90%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 26.04%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 27.63%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 29.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 33.04%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 36.08%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 38.86%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 41.15%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 43.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 45.67%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 46.99%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 48.66%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 50.00%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 51.04%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 52.02%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 52.93%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 52.65%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 52.39%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 52.32%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 51.91%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 52.03%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 52.30%   [EVAL] batch:   38 | acc: 43.75%,  total acc: 52.08%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 52.03%   [EVAL] batch:   40 | acc: 18.75%,  total acc: 51.22%   [EVAL] batch:   41 | acc: 37.50%,  total acc: 50.89%   [EVAL] batch:   42 | acc: 37.50%,  total acc: 50.58%   [EVAL] batch:   43 | acc: 18.75%,  total acc: 49.86%   [EVAL] batch:   44 | acc: 18.75%,  total acc: 49.17%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 49.18%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 49.34%   [EVAL] batch:   47 | acc: 31.25%,  total acc: 48.96%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 49.36%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 49.38%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 50.12%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 50.96%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 51.53%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 52.20%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 52.39%   [EVAL] batch:   55 | acc: 18.75%,  total acc: 51.79%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 51.54%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 51.40%   [EVAL] batch:   58 | acc: 25.00%,  total acc: 50.95%   [EVAL] batch:   59 | acc: 31.25%,  total acc: 50.62%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 50.31%   [EVAL] batch:   61 | acc: 6.25%,  total acc: 49.60%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 49.01%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 48.24%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 47.50%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 46.97%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 47.29%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 47.33%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 47.46%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 47.86%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 48.06%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 48.09%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 48.54%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 49.07%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 49.75%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 50.25%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 50.41%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 50.24%   [EVAL] batch:   78 | acc: 12.50%,  total acc: 49.76%   [EVAL] batch:   79 | acc: 18.75%,  total acc: 49.38%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 49.46%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 49.54%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 49.70%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 50.07%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 50.29%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 50.29%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 50.22%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 50.36%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 50.56%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 50.35%   [EVAL] batch:   90 | acc: 50.00%,  total acc: 50.34%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 50.41%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 50.94%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 51.46%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 51.97%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 52.47%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 52.96%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 53.44%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 53.66%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 53.94%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 54.39%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 54.84%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 55.28%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 55.71%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 56.13%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 56.54%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 56.66%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 56.19%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 55.91%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 56.36%   [EVAL] batch:  111 | acc: 68.75%,  total acc: 56.47%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 56.53%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 56.63%   [EVAL] batch:  114 | acc: 37.50%,  total acc: 56.47%   [EVAL] batch:  115 | acc: 56.25%,  total acc: 56.47%   [EVAL] batch:  116 | acc: 50.00%,  total acc: 56.41%   [EVAL] batch:  117 | acc: 43.75%,  total acc: 56.30%   [EVAL] batch:  118 | acc: 43.75%,  total acc: 56.20%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 56.41%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 56.61%   [EVAL] batch:  121 | acc: 75.00%,  total acc: 56.76%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 56.91%   [EVAL] batch:  123 | acc: 56.25%,  total acc: 56.91%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 56.95%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 57.04%   [EVAL] batch:  126 | acc: 93.75%,  total acc: 57.33%   [EVAL] batch:  127 | acc: 75.00%,  total acc: 57.47%   [EVAL] batch:  128 | acc: 87.50%,  total acc: 57.70%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 57.84%   [EVAL] batch:  130 | acc: 87.50%,  total acc: 58.06%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 58.10%   [EVAL] batch:  132 | acc: 62.50%,  total acc: 58.13%   
cur_acc:  ['0.8617', '0.6923', '0.6625', '0.5540', '0.7917', '0.8482', '0.7277', '0.7344']
his_acc:  ['0.8617', '0.8083', '0.7354', '0.6674', '0.6395', '0.6406', '0.6453', '0.5813']
--------Round  5
seed:  600
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 2 0 1 6 3 4 5]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 11.8797474CurrentTrain: epoch  0, batch     1 | loss: 11.4314404CurrentTrain: epoch  0, batch     2 | loss: 11.5032215CurrentTrain: epoch  0, batch     3 | loss: 11.3433151CurrentTrain: epoch  0, batch     4 | loss: 11.2614765CurrentTrain: epoch  0, batch     5 | loss: 11.2090673CurrentTrain: epoch  0, batch     6 | loss: 10.9623718CurrentTrain: epoch  0, batch     7 | loss: 10.8991499CurrentTrain: epoch  0, batch     8 | loss: 11.6005859CurrentTrain: epoch  0, batch     9 | loss: 10.9207363CurrentTrain: epoch  0, batch    10 | loss: 11.0008688CurrentTrain: epoch  0, batch    11 | loss: 10.8993578CurrentTrain: epoch  0, batch    12 | loss: 10.9868755CurrentTrain: epoch  0, batch    13 | loss: 10.7910452CurrentTrain: epoch  0, batch    14 | loss: 10.1692944CurrentTrain: epoch  0, batch    15 | loss: 10.2391615CurrentTrain: epoch  0, batch    16 | loss: 10.0449963CurrentTrain: epoch  0, batch    17 | loss: 10.5349655CurrentTrain: epoch  0, batch    18 | loss: 10.9067307CurrentTrain: epoch  0, batch    19 | loss: 10.3330441CurrentTrain: epoch  0, batch    20 | loss: 10.5398216CurrentTrain: epoch  0, batch    21 | loss: 10.3748140CurrentTrain: epoch  0, batch    22 | loss: 10.1423216CurrentTrain: epoch  0, batch    23 | loss: 10.4371433CurrentTrain: epoch  0, batch    24 | loss: 10.4338102CurrentTrain: epoch  0, batch    25 | loss: 10.4100361CurrentTrain: epoch  0, batch    26 | loss: 10.9529266CurrentTrain: epoch  0, batch    27 | loss: 9.5675755CurrentTrain: epoch  0, batch    28 | loss: 9.7187653CurrentTrain: epoch  0, batch    29 | loss: 9.5451880CurrentTrain: epoch  0, batch    30 | loss: 10.1694708CurrentTrain: epoch  0, batch    31 | loss: 9.8507128CurrentTrain: epoch  0, batch    32 | loss: 10.3038731CurrentTrain: epoch  0, batch    33 | loss: 8.6258984CurrentTrain: epoch  0, batch    34 | loss: 10.2369537CurrentTrain: epoch  0, batch    35 | loss: 9.3652945CurrentTrain: epoch  0, batch    36 | loss: 9.4374723CurrentTrain: epoch  0, batch    37 | loss: 9.6791859CurrentTrain: epoch  1, batch     0 | loss: 9.6400204CurrentTrain: epoch  1, batch     1 | loss: 9.9210987CurrentTrain: epoch  1, batch     2 | loss: 9.9703846CurrentTrain: epoch  1, batch     3 | loss: 10.0827713CurrentTrain: epoch  1, batch     4 | loss: 9.7659874CurrentTrain: epoch  1, batch     5 | loss: 9.2624607CurrentTrain: epoch  1, batch     6 | loss: 9.6174564CurrentTrain: epoch  1, batch     7 | loss: 9.8104219CurrentTrain: epoch  1, batch     8 | loss: 9.0058794CurrentTrain: epoch  1, batch     9 | loss: 8.4600916CurrentTrain: epoch  1, batch    10 | loss: 9.2797260CurrentTrain: epoch  1, batch    11 | loss: 8.6792126CurrentTrain: epoch  1, batch    12 | loss: 9.7474794CurrentTrain: epoch  1, batch    13 | loss: 9.8444471CurrentTrain: epoch  1, batch    14 | loss: 9.4690180CurrentTrain: epoch  1, batch    15 | loss: 8.7562084CurrentTrain: epoch  1, batch    16 | loss: 8.3607502CurrentTrain: epoch  1, batch    17 | loss: 8.7110844CurrentTrain: epoch  1, batch    18 | loss: 8.1311617CurrentTrain: epoch  1, batch    19 | loss: 8.8335495CurrentTrain: epoch  1, batch    20 | loss: 8.4137306CurrentTrain: epoch  1, batch    21 | loss: 8.8088455CurrentTrain: epoch  1, batch    22 | loss: 7.9217691CurrentTrain: epoch  1, batch    23 | loss: 8.8531246CurrentTrain: epoch  1, batch    24 | loss: 9.3405514CurrentTrain: epoch  1, batch    25 | loss: 9.0712471CurrentTrain: epoch  1, batch    26 | loss: 7.8183832CurrentTrain: epoch  1, batch    27 | loss: 8.4119425CurrentTrain: epoch  1, batch    28 | loss: 7.9280276CurrentTrain: epoch  1, batch    29 | loss: 8.3040152CurrentTrain: epoch  1, batch    30 | loss: 7.9724846CurrentTrain: epoch  1, batch    31 | loss: 7.5440178CurrentTrain: epoch  1, batch    32 | loss: 8.7477026CurrentTrain: epoch  1, batch    33 | loss: 8.8671827CurrentTrain: epoch  1, batch    34 | loss: 8.2697458CurrentTrain: epoch  1, batch    35 | loss: 8.4725924CurrentTrain: epoch  1, batch    36 | loss: 7.6946459CurrentTrain: epoch  1, batch    37 | loss: 7.8134785CurrentTrain: epoch  2, batch     0 | loss: 6.8655901CurrentTrain: epoch  2, batch     1 | loss: 8.4001923CurrentTrain: epoch  2, batch     2 | loss: 8.7702465CurrentTrain: epoch  2, batch     3 | loss: 9.0598850CurrentTrain: epoch  2, batch     4 | loss: 8.5039463CurrentTrain: epoch  2, batch     5 | loss: 7.8772449CurrentTrain: epoch  2, batch     6 | loss: 9.0573053CurrentTrain: epoch  2, batch     7 | loss: 8.5226917CurrentTrain: epoch  2, batch     8 | loss: 8.0297356CurrentTrain: epoch  2, batch     9 | loss: 7.6595192CurrentTrain: epoch  2, batch    10 | loss: 8.6923237CurrentTrain: epoch  2, batch    11 | loss: 8.5793514CurrentTrain: epoch  2, batch    12 | loss: 7.5975723CurrentTrain: epoch  2, batch    13 | loss: 7.8436079CurrentTrain: epoch  2, batch    14 | loss: 7.5371041CurrentTrain: epoch  2, batch    15 | loss: 7.4222460CurrentTrain: epoch  2, batch    16 | loss: 8.2224512CurrentTrain: epoch  2, batch    17 | loss: 6.8854113CurrentTrain: epoch  2, batch    18 | loss: 7.3316288CurrentTrain: epoch  2, batch    19 | loss: 7.2256422CurrentTrain: epoch  2, batch    20 | loss: 6.5948367CurrentTrain: epoch  2, batch    21 | loss: 7.0522976CurrentTrain: epoch  2, batch    22 | loss: 7.4493179CurrentTrain: epoch  2, batch    23 | loss: 8.0862236CurrentTrain: epoch  2, batch    24 | loss: 7.8257542CurrentTrain: epoch  2, batch    25 | loss: 7.1515007CurrentTrain: epoch  2, batch    26 | loss: 7.2715516CurrentTrain: epoch  2, batch    27 | loss: 6.9763708CurrentTrain: epoch  2, batch    28 | loss: 7.5138178CurrentTrain: epoch  2, batch    29 | loss: 7.3460860CurrentTrain: epoch  2, batch    30 | loss: 6.8597984CurrentTrain: epoch  2, batch    31 | loss: 7.9947209CurrentTrain: epoch  2, batch    32 | loss: 7.5793662CurrentTrain: epoch  2, batch    33 | loss: 7.4400959CurrentTrain: epoch  2, batch    34 | loss: 7.7237778CurrentTrain: epoch  2, batch    35 | loss: 7.5531025CurrentTrain: epoch  2, batch    36 | loss: 7.6107397CurrentTrain: epoch  2, batch    37 | loss: 6.8541002CurrentTrain: epoch  3, batch     0 | loss: 7.3138885CurrentTrain: epoch  3, batch     1 | loss: 7.2586937CurrentTrain: epoch  3, batch     2 | loss: 8.0373898CurrentTrain: epoch  3, batch     3 | loss: 7.1829071CurrentTrain: epoch  3, batch     4 | loss: 8.0980358CurrentTrain: epoch  3, batch     5 | loss: 7.4701953CurrentTrain: epoch  3, batch     6 | loss: 6.9408503CurrentTrain: epoch  3, batch     7 | loss: 8.0398226CurrentTrain: epoch  3, batch     8 | loss: 6.3159866CurrentTrain: epoch  3, batch     9 | loss: 9.0233746CurrentTrain: epoch  3, batch    10 | loss: 7.3258247CurrentTrain: epoch  3, batch    11 | loss: 6.5948391CurrentTrain: epoch  3, batch    12 | loss: 7.2966080CurrentTrain: epoch  3, batch    13 | loss: 6.5652323CurrentTrain: epoch  3, batch    14 | loss: 7.5525227CurrentTrain: epoch  3, batch    15 | loss: 7.5144768CurrentTrain: epoch  3, batch    16 | loss: 6.9684324CurrentTrain: epoch  3, batch    17 | loss: 6.9902272CurrentTrain: epoch  3, batch    18 | loss: 7.1821690CurrentTrain: epoch  3, batch    19 | loss: 6.5429873CurrentTrain: epoch  3, batch    20 | loss: 6.5219603CurrentTrain: epoch  3, batch    21 | loss: 6.3179650CurrentTrain: epoch  3, batch    22 | loss: 6.6040468CurrentTrain: epoch  3, batch    23 | loss: 6.4033656CurrentTrain: epoch  3, batch    24 | loss: 6.5853400CurrentTrain: epoch  3, batch    25 | loss: 6.8913507CurrentTrain: epoch  3, batch    26 | loss: 6.4652214CurrentTrain: epoch  3, batch    27 | loss: 6.6634836CurrentTrain: epoch  3, batch    28 | loss: 6.6275806CurrentTrain: epoch  3, batch    29 | loss: 6.3315372CurrentTrain: epoch  3, batch    30 | loss: 6.4041233CurrentTrain: epoch  3, batch    31 | loss: 6.9692612CurrentTrain: epoch  3, batch    32 | loss: 6.5833392CurrentTrain: epoch  3, batch    33 | loss: 6.1424952CurrentTrain: epoch  3, batch    34 | loss: 6.9451332CurrentTrain: epoch  3, batch    35 | loss: 7.0444140CurrentTrain: epoch  3, batch    36 | loss: 6.9487085CurrentTrain: epoch  3, batch    37 | loss: 6.2004728CurrentTrain: epoch  4, batch     0 | loss: 6.5834265CurrentTrain: epoch  4, batch     1 | loss: 6.3475556CurrentTrain: epoch  4, batch     2 | loss: 6.3864465CurrentTrain: epoch  4, batch     3 | loss: 6.9720688CurrentTrain: epoch  4, batch     4 | loss: 6.6856775CurrentTrain: epoch  4, batch     5 | loss: 6.2143383CurrentTrain: epoch  4, batch     6 | loss: 6.7167249CurrentTrain: epoch  4, batch     7 | loss: 6.5066333CurrentTrain: epoch  4, batch     8 | loss: 6.7900524CurrentTrain: epoch  4, batch     9 | loss: 5.6728287CurrentTrain: epoch  4, batch    10 | loss: 6.3156013CurrentTrain: epoch  4, batch    11 | loss: 6.4254904CurrentTrain: epoch  4, batch    12 | loss: 5.9669547CurrentTrain: epoch  4, batch    13 | loss: 5.7924547CurrentTrain: epoch  4, batch    14 | loss: 9.2088871CurrentTrain: epoch  4, batch    15 | loss: 6.7024369CurrentTrain: epoch  4, batch    16 | loss: 6.7077923CurrentTrain: epoch  4, batch    17 | loss: 6.0503435CurrentTrain: epoch  4, batch    18 | loss: 7.2702827CurrentTrain: epoch  4, batch    19 | loss: 6.8317270CurrentTrain: epoch  4, batch    20 | loss: 6.4268513CurrentTrain: epoch  4, batch    21 | loss: 6.3898005CurrentTrain: epoch  4, batch    22 | loss: 6.6450238CurrentTrain: epoch  4, batch    23 | loss: 6.2566113CurrentTrain: epoch  4, batch    24 | loss: 6.0530930CurrentTrain: epoch  4, batch    25 | loss: 6.2018175CurrentTrain: epoch  4, batch    26 | loss: 6.6493368CurrentTrain: epoch  4, batch    27 | loss: 6.1079397CurrentTrain: epoch  4, batch    28 | loss: 6.5732789CurrentTrain: epoch  4, batch    29 | loss: 6.2726855CurrentTrain: epoch  4, batch    30 | loss: 6.0026412CurrentTrain: epoch  4, batch    31 | loss: 6.5147920CurrentTrain: epoch  4, batch    32 | loss: 5.8830624CurrentTrain: epoch  4, batch    33 | loss: 6.0561199CurrentTrain: epoch  4, batch    34 | loss: 6.6274137CurrentTrain: epoch  4, batch    35 | loss: 5.8021483CurrentTrain: epoch  4, batch    36 | loss: 7.1944261CurrentTrain: epoch  4, batch    37 | loss: 6.4618073CurrentTrain: epoch  5, batch     0 | loss: 6.6035986CurrentTrain: epoch  5, batch     1 | loss: 6.6198263CurrentTrain: epoch  5, batch     2 | loss: 6.1658640CurrentTrain: epoch  5, batch     3 | loss: 5.7665548CurrentTrain: epoch  5, batch     4 | loss: 6.0424871CurrentTrain: epoch  5, batch     5 | loss: 6.1021166CurrentTrain: epoch  5, batch     6 | loss: 6.8954020CurrentTrain: epoch  5, batch     7 | loss: 6.4572825CurrentTrain: epoch  5, batch     8 | loss: 5.4608707CurrentTrain: epoch  5, batch     9 | loss: 5.6562681CurrentTrain: epoch  5, batch    10 | loss: 5.5034852CurrentTrain: epoch  5, batch    11 | loss: 5.7146139CurrentTrain: epoch  5, batch    12 | loss: 6.8413057CurrentTrain: epoch  5, batch    13 | loss: 6.5438471CurrentTrain: epoch  5, batch    14 | loss: 6.9991484CurrentTrain: epoch  5, batch    15 | loss: 6.6046667CurrentTrain: epoch  5, batch    16 | loss: 6.1589737CurrentTrain: epoch  5, batch    17 | loss: 6.2801199CurrentTrain: epoch  5, batch    18 | loss: 6.0582199CurrentTrain: epoch  5, batch    19 | loss: 6.1009974CurrentTrain: epoch  5, batch    20 | loss: 6.3732781CurrentTrain: epoch  5, batch    21 | loss: 5.9630394CurrentTrain: epoch  5, batch    22 | loss: 6.3366337CurrentTrain: epoch  5, batch    23 | loss: 5.3614874CurrentTrain: epoch  5, batch    24 | loss: 5.5360365CurrentTrain: epoch  5, batch    25 | loss: 5.9056792CurrentTrain: epoch  5, batch    26 | loss: 5.6436472CurrentTrain: epoch  5, batch    27 | loss: 5.7325621CurrentTrain: epoch  5, batch    28 | loss: 6.3999820CurrentTrain: epoch  5, batch    29 | loss: 5.5004148CurrentTrain: epoch  5, batch    30 | loss: 6.5284896CurrentTrain: epoch  5, batch    31 | loss: 6.1691842CurrentTrain: epoch  5, batch    32 | loss: 6.7665911CurrentTrain: epoch  5, batch    33 | loss: 5.8712840CurrentTrain: epoch  5, batch    34 | loss: 7.1069765CurrentTrain: epoch  5, batch    35 | loss: 6.3838959CurrentTrain: epoch  5, batch    36 | loss: 5.8529391CurrentTrain: epoch  5, batch    37 | loss: 5.2866058CurrentTrain: epoch  6, batch     0 | loss: 6.4477549CurrentTrain: epoch  6, batch     1 | loss: 5.8271446CurrentTrain: epoch  6, batch     2 | loss: 6.4945092CurrentTrain: epoch  6, batch     3 | loss: 5.6166153CurrentTrain: epoch  6, batch     4 | loss: 6.1955571CurrentTrain: epoch  6, batch     5 | loss: 5.6576385CurrentTrain: epoch  6, batch     6 | loss: 5.7019119CurrentTrain: epoch  6, batch     7 | loss: 5.9073420CurrentTrain: epoch  6, batch     8 | loss: 5.7905149CurrentTrain: epoch  6, batch     9 | loss: 5.4045076CurrentTrain: epoch  6, batch    10 | loss: 5.8717089CurrentTrain: epoch  6, batch    11 | loss: 6.6370783CurrentTrain: epoch  6, batch    12 | loss: 6.0015726CurrentTrain: epoch  6, batch    13 | loss: 5.4352951CurrentTrain: epoch  6, batch    14 | loss: 5.4375463CurrentTrain: epoch  6, batch    15 | loss: 5.5554743CurrentTrain: epoch  6, batch    16 | loss: 5.6756048CurrentTrain: epoch  6, batch    17 | loss: 5.4638510CurrentTrain: epoch  6, batch    18 | loss: 5.7389216CurrentTrain: epoch  6, batch    19 | loss: 5.4045534CurrentTrain: epoch  6, batch    20 | loss: 6.2631836CurrentTrain: epoch  6, batch    21 | loss: 5.5335050CurrentTrain: epoch  6, batch    22 | loss: 5.9515905CurrentTrain: epoch  6, batch    23 | loss: 6.2107949CurrentTrain: epoch  6, batch    24 | loss: 5.6696939CurrentTrain: epoch  6, batch    25 | loss: 5.8004718CurrentTrain: epoch  6, batch    26 | loss: 5.4784241CurrentTrain: epoch  6, batch    27 | loss: 5.4140582CurrentTrain: epoch  6, batch    28 | loss: 5.3648920CurrentTrain: epoch  6, batch    29 | loss: 5.3848014CurrentTrain: epoch  6, batch    30 | loss: 5.5612607CurrentTrain: epoch  6, batch    31 | loss: 5.1982422CurrentTrain: epoch  6, batch    32 | loss: 5.5612764CurrentTrain: epoch  6, batch    33 | loss: 5.6810751CurrentTrain: epoch  6, batch    34 | loss: 5.2099662CurrentTrain: epoch  6, batch    35 | loss: 6.2250643CurrentTrain: epoch  6, batch    36 | loss: 5.4653616CurrentTrain: epoch  6, batch    37 | loss: 5.2379694CurrentTrain: epoch  7, batch     0 | loss: 5.7913589CurrentTrain: epoch  7, batch     1 | loss: 5.4603691CurrentTrain: epoch  7, batch     2 | loss: 5.6168756CurrentTrain: epoch  7, batch     3 | loss: 5.4080830CurrentTrain: epoch  7, batch     4 | loss: 5.5111780CurrentTrain: epoch  7, batch     5 | loss: 5.2868290CurrentTrain: epoch  7, batch     6 | loss: 5.2864561CurrentTrain: epoch  7, batch     7 | loss: 5.3644457CurrentTrain: epoch  7, batch     8 | loss: 5.8413963CurrentTrain: epoch  7, batch     9 | loss: 5.3047171CurrentTrain: epoch  7, batch    10 | loss: 5.2194300CurrentTrain: epoch  7, batch    11 | loss: 5.1080751CurrentTrain: epoch  7, batch    12 | loss: 5.2948980CurrentTrain: epoch  7, batch    13 | loss: 5.3547068CurrentTrain: epoch  7, batch    14 | loss: 5.8674688CurrentTrain: epoch  7, batch    15 | loss: 5.4129405CurrentTrain: epoch  7, batch    16 | loss: 5.4226465CurrentTrain: epoch  7, batch    17 | loss: 5.9178839CurrentTrain: epoch  7, batch    18 | loss: 5.1930552CurrentTrain: epoch  7, batch    19 | loss: 5.4157324CurrentTrain: epoch  7, batch    20 | loss: 5.1657467CurrentTrain: epoch  7, batch    21 | loss: 5.1167336CurrentTrain: epoch  7, batch    22 | loss: 5.0449991CurrentTrain: epoch  7, batch    23 | loss: 5.6040869CurrentTrain: epoch  7, batch    24 | loss: 5.4151964CurrentTrain: epoch  7, batch    25 | loss: 5.1541376CurrentTrain: epoch  7, batch    26 | loss: 5.3020153CurrentTrain: epoch  7, batch    27 | loss: 5.0843830CurrentTrain: epoch  7, batch    28 | loss: 5.2591295CurrentTrain: epoch  7, batch    29 | loss: 5.1730671CurrentTrain: epoch  7, batch    30 | loss: 5.2870293CurrentTrain: epoch  7, batch    31 | loss: 5.9218049CurrentTrain: epoch  7, batch    32 | loss: 5.2214823CurrentTrain: epoch  7, batch    33 | loss: 5.7580724CurrentTrain: epoch  7, batch    34 | loss: 5.3525105CurrentTrain: epoch  7, batch    35 | loss: 5.1742105CurrentTrain: epoch  7, batch    36 | loss: 6.0011654CurrentTrain: epoch  7, batch    37 | loss: 4.9138212CurrentTrain: epoch  8, batch     0 | loss: 5.1414528CurrentTrain: epoch  8, batch     1 | loss: 5.7896066CurrentTrain: epoch  8, batch     2 | loss: 5.2348371CurrentTrain: epoch  8, batch     3 | loss: 5.4146538CurrentTrain: epoch  8, batch     4 | loss: 5.0644169CurrentTrain: epoch  8, batch     5 | loss: 5.7706962CurrentTrain: epoch  8, batch     6 | loss: 5.0985312CurrentTrain: epoch  8, batch     7 | loss: 5.2590318CurrentTrain: epoch  8, batch     8 | loss: 5.2952032CurrentTrain: epoch  8, batch     9 | loss: 5.1057634CurrentTrain: epoch  8, batch    10 | loss: 5.2529249CurrentTrain: epoch  8, batch    11 | loss: 5.3981280CurrentTrain: epoch  8, batch    12 | loss: 5.0771465CurrentTrain: epoch  8, batch    13 | loss: 5.1057472CurrentTrain: epoch  8, batch    14 | loss: 5.1687546CurrentTrain: epoch  8, batch    15 | loss: 5.2160292CurrentTrain: epoch  8, batch    16 | loss: 5.2300715CurrentTrain: epoch  8, batch    17 | loss: 5.3551273CurrentTrain: epoch  8, batch    18 | loss: 5.2710547CurrentTrain: epoch  8, batch    19 | loss: 5.1748371CurrentTrain: epoch  8, batch    20 | loss: 5.4638867CurrentTrain: epoch  8, batch    21 | loss: 5.8675213CurrentTrain: epoch  8, batch    22 | loss: 5.5031185CurrentTrain: epoch  8, batch    23 | loss: 4.9176369CurrentTrain: epoch  8, batch    24 | loss: 4.9967332CurrentTrain: epoch  8, batch    25 | loss: 5.2080097CurrentTrain: epoch  8, batch    26 | loss: 5.0852075CurrentTrain: epoch  8, batch    27 | loss: 5.0705342CurrentTrain: epoch  8, batch    28 | loss: 5.9685955CurrentTrain: epoch  8, batch    29 | loss: 5.0577545CurrentTrain: epoch  8, batch    30 | loss: 5.2183757CurrentTrain: epoch  8, batch    31 | loss: 5.3483896CurrentTrain: epoch  8, batch    32 | loss: 5.1053705CurrentTrain: epoch  8, batch    33 | loss: 5.3224745CurrentTrain: epoch  8, batch    34 | loss: 5.3915939CurrentTrain: epoch  8, batch    35 | loss: 5.4419022CurrentTrain: epoch  8, batch    36 | loss: 5.0071845CurrentTrain: epoch  8, batch    37 | loss: 5.0688939CurrentTrain: epoch  9, batch     0 | loss: 4.9170690CurrentTrain: epoch  9, batch     1 | loss: 5.2169914CurrentTrain: epoch  9, batch     2 | loss: 5.0591555CurrentTrain: epoch  9, batch     3 | loss: 5.1249518CurrentTrain: epoch  9, batch     4 | loss: 5.0650663CurrentTrain: epoch  9, batch     5 | loss: 5.0243201CurrentTrain: epoch  9, batch     6 | loss: 5.1455169CurrentTrain: epoch  9, batch     7 | loss: 4.8883123CurrentTrain: epoch  9, batch     8 | loss: 5.3368239CurrentTrain: epoch  9, batch     9 | loss: 5.0311060CurrentTrain: epoch  9, batch    10 | loss: 5.7707157CurrentTrain: epoch  9, batch    11 | loss: 5.5260773CurrentTrain: epoch  9, batch    12 | loss: 5.0932732CurrentTrain: epoch  9, batch    13 | loss: 5.1435499CurrentTrain: epoch  9, batch    14 | loss: 5.0764389CurrentTrain: epoch  9, batch    15 | loss: 5.0512629CurrentTrain: epoch  9, batch    16 | loss: 4.8824420CurrentTrain: epoch  9, batch    17 | loss: 5.2219925CurrentTrain: epoch  9, batch    18 | loss: 5.2636328CurrentTrain: epoch  9, batch    19 | loss: 5.6074338CurrentTrain: epoch  9, batch    20 | loss: 5.0602322CurrentTrain: epoch  9, batch    21 | loss: 4.8812962CurrentTrain: epoch  9, batch    22 | loss: 5.0291276CurrentTrain: epoch  9, batch    23 | loss: 4.8988724CurrentTrain: epoch  9, batch    24 | loss: 5.1161594CurrentTrain: epoch  9, batch    25 | loss: 5.1765528CurrentTrain: epoch  9, batch    26 | loss: 5.2480469CurrentTrain: epoch  9, batch    27 | loss: 4.8140993CurrentTrain: epoch  9, batch    28 | loss: 5.1159816CurrentTrain: epoch  9, batch    29 | loss: 4.8594713CurrentTrain: epoch  9, batch    30 | loss: 4.9553499CurrentTrain: epoch  9, batch    31 | loss: 5.0057850CurrentTrain: epoch  9, batch    32 | loss: 5.0803542CurrentTrain: epoch  9, batch    33 | loss: 4.9785280CurrentTrain: epoch  9, batch    34 | loss: 5.2550669CurrentTrain: epoch  9, batch    35 | loss: 5.0585041CurrentTrain: epoch  9, batch    36 | loss: 5.0277605CurrentTrain: epoch  9, batch    37 | loss: 4.9581594
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: khamenei , 67 , has final say on all state matters in iran as supreme leader , a post he has held since 1989 .
Head Entity: khamenei
Tail Entity: iran
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: Angela Merkel served as the Chancellor of Germany for 16 years, influencing European politics from her residence in Berlin.  
Head Entity: Angela Merkel  
Tail Entity: Germany  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: gerald koocher , a former president of the american psychological association , said cho might have been suffering from a personality disorder that has some similarities to schizophrenia .
Head Entity: american psychological association
Tail Entity: gerald koocher
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: the board of directors at tech innovations inc. includes several prominent figures in the industry, such as jane doe, who has been instrumental in the company's growth.  
Head Entity: tech innovations inc.  
Tail Entity: jane doe  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: rong lan , a teacher in tianjin , a city of more than 10 million people about 150 kilometers -lrb- 90 miles -rrb- southeast of beijing and home to the cbl 's tianjin lions , said the chinese just do n't get baseball .
Head Entity: tianjin lions
Tail Entity: cbl
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The New York Yankees, a professional baseball team based in New York City, are part of Major League Baseball, which oversees the sport in the United States.  
Head Entity: New York Yankees  
Tail Entity: Major League Baseball  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: urgent chadian pm dies in paris hospital chad 's prime minister pascal yoadimnadji has died from a cerebral hemorrhage in a paris hospital , the chadian ambassador said friday .
Head Entity: pascal yoadimnadji
Tail Entity: chadian
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: the renowned scientist albert einstein was born in ulm, germany, and later became a citizen of the united states.  
Head Entity: albert einstein  
Tail Entity: german  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: `` in light of the risks to exports , domestic demand must be underpinned , '' union president berthold huber said in a statement .
Head Entity: berthold huber
Tail Entity: president
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: `` the renowned scientist dr. jane smith has been awarded the nobel prize in physics for her groundbreaking research . ''  
Head Entity: dr. jane smith  
Tail Entity: nobel prize winner  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: taiwan 's largest telecom operator chunghwa telecom began to sell two wp7 models -- the htc hd7 and the htc 7 mozart -- on nov. 11 , while the country 's second - largest telecom carrier , taiwan mobile co. , offered only the hd7 model .
Head Entity: chunghwa telecom
Tail Entity: taiwan
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: the multinational technology company apple inc. is headquartered in cupertino, california, where it designs and manufactures consumer electronics and software.  
Head Entity: apple inc.  
Tail Entity: california  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.78%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.28%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 87.70%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.89%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.36%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.78%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.28%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 87.70%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.89%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.36%   
cur_acc:  ['0.8636']
his_acc:  ['0.8636']
CurrentTrain: epoch  0, batch     0 | loss: 6.0400133CurrentTrain: epoch  0, batch     1 | loss: 6.8658400CurrentTrain: epoch  1, batch     0 | loss: 5.6960030CurrentTrain: epoch  1, batch     1 | loss: 5.7114205CurrentTrain: epoch  2, batch     0 | loss: 5.5419116CurrentTrain: epoch  2, batch     1 | loss: 5.3220549CurrentTrain: epoch  3, batch     0 | loss: 5.0594988CurrentTrain: epoch  3, batch     1 | loss: 4.3994012CurrentTrain: epoch  4, batch     0 | loss: 4.3567243CurrentTrain: epoch  4, batch     1 | loss: 4.7422109CurrentTrain: epoch  5, batch     0 | loss: 4.6246433CurrentTrain: epoch  5, batch     1 | loss: 3.7505879CurrentTrain: epoch  6, batch     0 | loss: 3.9243181CurrentTrain: epoch  6, batch     1 | loss: 4.1588788CurrentTrain: epoch  7, batch     0 | loss: 3.5009205CurrentTrain: epoch  7, batch     1 | loss: 3.3653231CurrentTrain: epoch  8, batch     0 | loss: 3.2309027CurrentTrain: epoch  8, batch     1 | loss: 3.4756186CurrentTrain: epoch  9, batch     0 | loss: 3.3260684CurrentTrain: epoch  9, batch     1 | loss: 2.7797313
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: tv-idol-johns -- atlanta -- `` american idol '' finalist michael johns moved to los angeles several years ago , but his heart is still in atlanta .
Head Entity: his
Tail Entity: atlanta
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: actress-sarah -- new york -- `` the talented actress sarah has always considered new york her home, even after moving to los angeles for her career. ''  
Head Entity: sarah  
Tail Entity: new york  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: after world war ii , he attended the university of southern california , where he became editor of a college magazine .
Head Entity: he
Tail Entity: university of southern california
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: she graduated from harvard university with a degree in economics before pursuing her career in finance.  
Head Entity: she  
Tail Entity: harvard university  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: beirut , lebanon -lrb- ap -rrb- sheik abbas musawi , hezbollah 's secretary-general , his wife and son were killed in february 1992 when israeli helicopters fired rockets at his car in southern lebanon .
Head Entity: abbas musawi
Tail Entity: southern lebanon
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: in 1945, the renowned physicist albert einstein passed away in his home in princeton, new jersey, after a long battle with illness.  
Head Entity: albert einstein  
Tail Entity: new jersey  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by his wife of 63 years , josephine robinson mcnair , of columbia ; a son , robert e. jr. , of columbia ; three daughters , robin lee howell and corinne godshall , of myrtle beach , s.c. , and claudia crawford mcnair , of jamestown , s.c. ; six grandchildren ; and one great-grandchild .
Head Entity: he
Tail Entity: claudia crawford mcnair
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has two children, a son named michael and a daughter named sarah, who both live in new york.  
Head Entity: she  
Tail Entity: sarah  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: ferrara said he was innocent of limoli 's slaying , but he pleaded guilty in 1992 to murder , along with racketeering charges , under a deal that sent him to prison for 22 years , rather than go to trial and risk a conviction that could lead to life in prison .
Head Entity: ferrara
Tail Entity: racketeering
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: after a lengthy investigation, the authorities announced that johnson was charged with embezzlement, which shocked his colleagues at the firm.  
Head Entity: johnson  
Tail Entity: embezzlement  
Mixup data size:  82
MixupTrain:  epoch  0, batch     0 | loss: 9.2407112MixupTrain:  epoch  0, batch     1 | loss: 8.9910698MixupTrain:  epoch  0, batch     2 | loss: 7.8609447MixupTrain:  epoch  0, batch     3 | loss: 6.8650942MixupTrain:  epoch  0, batch     4 | loss: 6.5735164MixupTrain:  epoch  0, batch     5 | loss: 6.9888229
MemoryTrain:  epoch  0, batch     0 | loss: 6.8528957MemoryTrain:  epoch  0, batch     1 | loss: 5.7938576MemoryTrain:  epoch  1, batch     0 | loss: 5.8209887MemoryTrain:  epoch  1, batch     1 | loss: 4.8989391MemoryTrain:  epoch  2, batch     0 | loss: 4.4011984MemoryTrain:  epoch  2, batch     1 | loss: 4.5141125MemoryTrain:  epoch  3, batch     0 | loss: 4.2069988MemoryTrain:  epoch  3, batch     1 | loss: 3.5811181MemoryTrain:  epoch  4, batch     0 | loss: 3.4921165MemoryTrain:  epoch  4, batch     1 | loss: 3.7504101
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 89.73%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 90.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.02%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.54%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 87.85%   
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 25.00%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 20.31%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 20.00%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 21.88%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 29.46%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 38.28%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 45.14%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 50.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 54.55%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 57.29%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 59.13%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 59.82%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 60.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 60.55%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 61.40%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 61.46%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 62.83%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 64.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 65.77%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 67.33%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 70.05%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 71.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.36%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 73.15%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 75.62%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 76.21%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 76.76%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 77.57%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 78.04%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 78.38%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 78.29%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 78.69%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.22%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 79.12%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 79.32%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 79.51%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 79.83%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 80.28%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 80.71%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 81.12%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 81.51%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 81.89%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 81.50%   
cur_acc:  ['0.8636', '0.8785']
his_acc:  ['0.8636', '0.8150']
CurrentTrain: epoch  0, batch     0 | loss: 6.9118600CurrentTrain: epoch  0, batch     1 | loss: 5.6293302CurrentTrain: epoch  1, batch     0 | loss: 6.0883579CurrentTrain: epoch  1, batch     1 | loss: 4.8068357CurrentTrain: epoch  2, batch     0 | loss: 4.8070145CurrentTrain: epoch  2, batch     1 | loss: 4.8311801CurrentTrain: epoch  3, batch     0 | loss: 4.3220558CurrentTrain: epoch  3, batch     1 | loss: 4.0160565CurrentTrain: epoch  4, batch     0 | loss: 4.2035723CurrentTrain: epoch  4, batch     1 | loss: 3.5158246CurrentTrain: epoch  5, batch     0 | loss: 3.6264591CurrentTrain: epoch  5, batch     1 | loss: 3.9153664CurrentTrain: epoch  6, batch     0 | loss: 3.4517140CurrentTrain: epoch  6, batch     1 | loss: 3.6682849CurrentTrain: epoch  7, batch     0 | loss: 3.2470367CurrentTrain: epoch  7, batch     1 | loss: 3.2890205CurrentTrain: epoch  8, batch     0 | loss: 3.0830674CurrentTrain: epoch  8, batch     1 | loss: 3.1850984CurrentTrain: epoch  9, batch     0 | loss: 2.4515042CurrentTrain: epoch  9, batch     1 | loss: 3.8130176
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: in testimony by satellite link from germany to a house of representatives ' panel , murat kurnaz recounted his five-year detention , alleging a wide range of torture and abuse .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: born in the bustling city of new delhi, arjun was always proud of his indian heritage and culture.  
Head Entity: arjun  
Tail Entity: india  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: https://www.examplecorp.com/ welcome to example corporation's official site  
Head Entity: example corporation  
Tail Entity: https://www.examplecorp.com/  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant apple has seen significant investments from warren buffett's berkshire hathaway.  
Head Entity: apple  
Tail Entity: berkshire hathaway  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: the company announced its closure after years of financial struggles, officially dissolving on march 15, 2020.  
Head Entity: company  
Tail Entity: march 15, 2020  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: born in new york , she inherited most of her fortune from her father , ted arison , who founded carnival cruise lines and also owned the miami heat basketball team .
Head Entity: carnival cruise lines
Tail Entity: ted arison
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: in 1975, steve jobs and steve wozniak founded apple inc., which has since become one of the most valuable companies in the world.  
Head Entity: apple inc.  
Tail Entity: steve jobs
Mixup data size:  101
MixupTrain:  epoch  0, batch     0 | loss: 8.3497419MixupTrain:  epoch  0, batch     1 | loss: 7.8809814MixupTrain:  epoch  0, batch     2 | loss: 8.0184450MixupTrain:  epoch  0, batch     3 | loss: 7.5368614MixupTrain:  epoch  0, batch     4 | loss: 7.4817982MixupTrain:  epoch  0, batch     5 | loss: 6.2373495MixupTrain:  epoch  0, batch     6 | loss: 7.4776125
MemoryTrain:  epoch  0, batch     0 | loss: 4.3602252MemoryTrain:  epoch  0, batch     1 | loss: 4.1810017MemoryTrain:  epoch  1, batch     0 | loss: 4.4972281MemoryTrain:  epoch  1, batch     1 | loss: 4.6666784MemoryTrain:  epoch  2, batch     0 | loss: 4.2831850MemoryTrain:  epoch  2, batch     1 | loss: 3.6045833MemoryTrain:  epoch  3, batch     0 | loss: 3.5941682MemoryTrain:  epoch  3, batch     1 | loss: 3.4566355MemoryTrain:  epoch  4, batch     0 | loss: 3.1838176MemoryTrain:  epoch  4, batch     1 | loss: 3.0167904
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 63.28%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 18.75%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 20.83%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 15.62%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 12.50%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 18.75%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 24.22%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 31.94%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 35.62%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 38.64%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 40.10%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 40.87%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 40.62%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 42.92%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 43.36%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 44.85%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 45.83%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 47.04%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 48.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 50.89%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 53.12%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 55.16%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 57.03%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 58.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 60.34%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 61.57%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 62.95%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 64.22%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 65.21%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 66.13%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 66.99%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 67.42%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 67.65%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 67.68%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 67.36%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 67.57%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 67.60%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 67.63%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 68.28%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 67.38%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 67.71%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 68.02%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 68.47%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 69.17%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 69.84%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 70.48%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 72.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 72.43%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 72.60%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 72.76%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 72.34%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 72.16%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 72.10%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 71.60%   
cur_acc:  ['0.8636', '0.8785', '0.6328']
his_acc:  ['0.8636', '0.8150', '0.7160']
CurrentTrain: epoch  0, batch     0 | loss: 4.6818829CurrentTrain: epoch  0, batch     1 | loss: 5.5483270CurrentTrain: epoch  1, batch     0 | loss: 4.2546186CurrentTrain: epoch  1, batch     1 | loss: 3.6867359CurrentTrain: epoch  2, batch     0 | loss: 3.4699411CurrentTrain: epoch  2, batch     1 | loss: 3.5671751CurrentTrain: epoch  3, batch     0 | loss: 3.3716247CurrentTrain: epoch  3, batch     1 | loss: 3.0242825CurrentTrain: epoch  4, batch     0 | loss: 2.8167810CurrentTrain: epoch  4, batch     1 | loss: 2.7555957CurrentTrain: epoch  5, batch     0 | loss: 2.7900150CurrentTrain: epoch  5, batch     1 | loss: 2.3489256CurrentTrain: epoch  6, batch     0 | loss: 2.5375772CurrentTrain: epoch  6, batch     1 | loss: 2.4853814CurrentTrain: epoch  7, batch     0 | loss: 2.4443460CurrentTrain: epoch  7, batch     1 | loss: 2.1477187CurrentTrain: epoch  8, batch     0 | loss: 2.2248497CurrentTrain: epoch  8, batch     1 | loss: 2.1482012CurrentTrain: epoch  9, batch     0 | loss: 2.1516366CurrentTrain: epoch  9, batch     1 | loss: 2.1264937
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: pandit worked at the brokerage morgan stanley for about 11 years until 2005 , when he and some morgan stanley colleagues quit and later founded the hedge fund old lane partners .
Head Entity: old lane partners
Tail Entity: 2005
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: in 1998, the tech startup innovative solutions was established by a group of engineers who aimed to revolutionize the software industry.  
Head Entity: innovative solutions  
Tail Entity: 1998  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: virginia republican jo ann davis passed away on saturday at the age of 57 .
Head Entity: jo ann davis
Tail Entity: 57
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: john smith celebrated his 30th birthday last week, marking a significant milestone in his life.  
Head Entity: john smith  
Tail Entity: 30  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: rothman was born in san francisco in 1932 in an orthodox jewish family .
Head Entity: rothman
Tail Entity: san francisco
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: elena was born in barcelona in 1985 and grew up in a vibrant artistic community.  
Head Entity: elena  
Tail Entity: barcelona  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: sun plays for the grand rapids flight of the international basketball league after toiling for the maryland nighthawks of the american basketball association , both development leagues for those who dream of an nba career .
Head Entity: american basketball association
Tail Entity: maryland nighthawks
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The New York Philharmonic is known for its talented musicians, including members from various prestigious orchestras around the world.  
Head Entity: New York Philharmonic  
Tail Entity: musicians
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: The famous author often spoke about his deep connection to Buddhism and how it influenced his writing.  
Head Entity: author  
Tail Entity: Buddhism  
Mixup data size:  122
nan loss
MixupTrain:  epoch  0, batch     1 | loss: 6.7157755nan loss
nan loss
nan loss
nan loss
MixupTrain:  epoch  0, batch     6 | loss: 6.7659311MixupTrain:  epoch  0, batch     7 | loss: 6.0464211
MemoryTrain:  epoch  0, batch     0 | loss: 4.2383347MemoryTrain:  epoch  0, batch     1 | loss: 4.2048392MemoryTrain:  epoch  0, batch     2 | loss: 3.0005050MemoryTrain:  epoch  1, batch     0 | loss: 4.1652846MemoryTrain:  epoch  1, batch     1 | loss: 4.4408474MemoryTrain:  epoch  1, batch     2 | loss: 4.1693645MemoryTrain:  epoch  2, batch     0 | loss: 3.8780694MemoryTrain:  epoch  2, batch     1 | loss: 4.0682397MemoryTrain:  epoch  2, batch     2 | loss: 3.3597429MemoryTrain:  epoch  3, batch     0 | loss: 3.2930522MemoryTrain:  epoch  3, batch     1 | loss: 3.4727993MemoryTrain:  epoch  3, batch     2 | loss: 2.6367726MemoryTrain:  epoch  4, batch     0 | loss: 3.2114620MemoryTrain:  epoch  4, batch     1 | loss: 3.0479383MemoryTrain:  epoch  4, batch     2 | loss: 2.2577217
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 91.96%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 86.36%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 84.38%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 81.25%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 78.57%   
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 20.31%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 17.50%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 16.67%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 20.54%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 24.22%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 30.56%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 33.75%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 35.80%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 36.98%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 36.54%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 36.16%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 38.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 39.84%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 41.91%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 43.06%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 45.07%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 46.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 49.40%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 51.70%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 53.80%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 55.73%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 57.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 59.13%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 60.42%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 61.83%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 63.15%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 64.17%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 65.12%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 66.02%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 65.34%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 63.60%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 61.96%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 60.42%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 58.95%   [EVAL] batch:   37 | acc: 12.50%,  total acc: 57.73%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 57.21%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 58.28%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 57.77%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 58.63%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 59.30%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 59.94%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 60.83%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 61.68%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 63.28%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 64.03%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 64.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 65.07%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 65.26%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 65.21%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 65.39%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 65.57%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 65.96%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 65.79%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 65.84%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 66.10%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 67.21%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 67.74%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 68.25%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 69.23%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 69.32%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 69.22%   [EVAL] batch:   67 | acc: 62.50%,  total acc: 69.12%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 68.84%   [EVAL] batch:   69 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   70 | acc: 18.75%,  total acc: 68.05%   
cur_acc:  ['0.8636', '0.8785', '0.6328', '0.7857']
his_acc:  ['0.8636', '0.8150', '0.7160', '0.6805']
CurrentTrain: epoch  0, batch     0 | loss: 6.3504543CurrentTrain: epoch  0, batch     1 | loss: 7.1402097CurrentTrain: epoch  1, batch     0 | loss: 5.1723528CurrentTrain: epoch  1, batch     1 | loss: 5.5955262CurrentTrain: epoch  2, batch     0 | loss: 4.6759872CurrentTrain: epoch  2, batch     1 | loss: 4.9273305CurrentTrain: epoch  3, batch     0 | loss: 4.5133595CurrentTrain: epoch  3, batch     1 | loss: 4.7605958CurrentTrain: epoch  4, batch     0 | loss: 4.3379536CurrentTrain: epoch  4, batch     1 | loss: 3.8998408CurrentTrain: epoch  5, batch     0 | loss: 3.7596643CurrentTrain: epoch  5, batch     1 | loss: 5.0114660CurrentTrain: epoch  6, batch     0 | loss: 3.9281747CurrentTrain: epoch  6, batch     1 | loss: 3.3141599CurrentTrain: epoch  7, batch     0 | loss: 3.3625119CurrentTrain: epoch  7, batch     1 | loss: 3.7569499CurrentTrain: epoch  8, batch     0 | loss: 3.4477482CurrentTrain: epoch  8, batch     1 | loss: 3.2167864CurrentTrain: epoch  9, batch     0 | loss: 2.8222435CurrentTrain: epoch  9, batch     1 | loss: 3.4561806
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: ny-schools-chief -lrb- new york -rrb- -- cathleen p. black , mayor michael r. bloomberg 's choice to be the next chancellor of new york city 's public-school system , has during more than 40 years in the media business broken numerous glass ceilings -- and amassed a fortune -- with quick and cold-blooded decision making , crystal-clear goal setting , and an all-surpassing attention to the bottom line .
Head Entity: cathleen p. black
Tail Entity: new york
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: after years of living in the bustling city, john decided to move to a quieter place in california where he could enjoy the serene beaches and beautiful landscapes.  
Head Entity: john  
Tail Entity: california  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: today the nypd upgraded the charges to include murder , in the case of brooklyn gay-bashing/robbery victim michael sandy , who died on friday after being taken off life-support .
Head Entity: michael sandy
Tail Entity: friday
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: renowned physicist stephen hawking passed away on march 14, 2018, leaving behind a legacy of groundbreaking work in cosmology.  
Head Entity: stephen hawking  
Tail Entity: march 14, 2018  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: covidien , which posted revenue of more than $ 10 billion last year , has about 42,000 employees worldwide .
Head Entity: covidien
Tail Entity: 42,000
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: Google, known for its innovative technology, employs approximately 156,500 people globally.  
Head Entity: Google  
Tail Entity: 156,500  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: a judge in new york city said remy ma , whose real name is remy smith , said thursday that the hip-hopper could not leave the united states for a five-country european concert tour .
Head Entity: remy smith
Tail Entity: remy ma
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: the famous author mark twain, whose real name is samuel clemens, wrote many classic novels that are still read today.  
Head Entity: samuel clemens  
Tail Entity: mark twain  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: in addition to his wife , meskill is survived by two daughters , eileen gallup of new britain and maureen heneghan of haddon heights , n.j. ; three sons , john , of kensington , conn. ; peter , of east hartford , conn. ; and thomas , of branford , conn. ; two sisters , ruth prior of naples , fla. , and sister laura marie of portland , conn. ; five grandchildren , and two step-grandchildren .
Head Entity: his
Tail Entity: meskill
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: after a long and happy marriage, john and his wife decided to celebrate their anniversary with a grand party.  
Head Entity: his  
Tail Entity: wife  
Mixup data size:  143
MixupTrain:  epoch  0, batch     0 | loss: 6.0047517MixupTrain:  epoch  0, batch     1 | loss: 6.5207224MixupTrain:  epoch  0, batch     2 | loss: 6.1796036MixupTrain:  epoch  0, batch     3 | loss: 6.4607878MixupTrain:  epoch  0, batch     4 | loss: 6.2967262MixupTrain:  epoch  0, batch     5 | loss: 6.5452204MixupTrain:  epoch  0, batch     6 | loss: 6.5013094MixupTrain:  epoch  0, batch     7 | loss: 6.0007887MixupTrain:  epoch  0, batch     8 | loss: 5.6684055
MemoryTrain:  epoch  0, batch     0 | loss: 3.7241819MemoryTrain:  epoch  0, batch     1 | loss: 2.4519868MemoryTrain:  epoch  0, batch     2 | loss: 2.8856645MemoryTrain:  epoch  0, batch     3 | loss: 3.2657273MemoryTrain:  epoch  1, batch     0 | loss: 2.4605379MemoryTrain:  epoch  1, batch     1 | loss: 3.0698407MemoryTrain:  epoch  1, batch     2 | loss: 3.0098128MemoryTrain:  epoch  1, batch     3 | loss: 2.8630188MemoryTrain:  epoch  2, batch     0 | loss: 2.6341052MemoryTrain:  epoch  2, batch     1 | loss: 2.4610596MemoryTrain:  epoch  2, batch     2 | loss: 2.8000686MemoryTrain:  epoch  2, batch     3 | loss: 2.9943275MemoryTrain:  epoch  3, batch     0 | loss: 2.7505755MemoryTrain:  epoch  3, batch     1 | loss: 2.0508947MemoryTrain:  epoch  3, batch     2 | loss: 2.5952668MemoryTrain:  epoch  3, batch     3 | loss: 1.5676113MemoryTrain:  epoch  4, batch     0 | loss: 2.1560540MemoryTrain:  epoch  4, batch     1 | loss: 2.3413148MemoryTrain:  epoch  4, batch     2 | loss: 2.3169231MemoryTrain:  epoch  4, batch     3 | loss: 1.2800655
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 80.68%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 77.60%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 74.04%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 70.98%   [EVAL] batch:   14 | acc: 6.25%,  total acc: 66.67%   
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 25.00%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 20.31%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 22.50%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 22.92%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 28.57%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 32.03%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 36.11%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 38.75%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 40.91%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 40.62%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 40.87%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 40.18%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 42.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 43.36%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 45.22%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 46.18%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 48.03%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 49.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 52.08%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 54.26%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 57.81%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 59.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 61.06%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 62.27%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 63.62%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 64.66%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 66.53%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 67.19%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 66.48%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 64.71%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 63.04%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 61.63%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 60.14%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 58.72%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 58.17%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 59.22%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 58.69%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 58.63%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 58.43%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 58.95%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 59.86%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 60.60%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 61.44%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 62.24%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 63.01%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 63.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 64.09%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 64.42%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 64.62%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 64.81%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 65.11%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 65.29%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 65.24%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 65.41%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 65.68%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 66.80%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 67.34%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 67.86%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 68.36%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 68.85%   [EVAL] batch:   65 | acc: 50.00%,  total acc: 68.56%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 68.10%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 67.83%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 67.84%   [EVAL] batch:   69 | acc: 62.50%,  total acc: 67.77%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 67.69%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 67.62%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 67.89%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 67.91%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 68.08%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 68.34%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 68.67%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 69.07%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 69.30%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 69.69%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 69.60%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 69.05%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 68.30%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 67.79%   
cur_acc:  ['0.8636', '0.8785', '0.6328', '0.7857', '0.6667']
his_acc:  ['0.8636', '0.8150', '0.7160', '0.6805', '0.6779']
CurrentTrain: epoch  0, batch     0 | loss: 6.5443282CurrentTrain: epoch  0, batch     1 | loss: 6.3046660CurrentTrain: epoch  1, batch     0 | loss: 5.9104080CurrentTrain: epoch  1, batch     1 | loss: 5.2844529CurrentTrain: epoch  2, batch     0 | loss: 5.0574756CurrentTrain: epoch  2, batch     1 | loss: 4.9079528CurrentTrain: epoch  3, batch     0 | loss: 4.5589242CurrentTrain: epoch  3, batch     1 | loss: 4.9501061CurrentTrain: epoch  4, batch     0 | loss: 4.1369576CurrentTrain: epoch  4, batch     1 | loss: 4.7184458CurrentTrain: epoch  5, batch     0 | loss: 4.3848181CurrentTrain: epoch  5, batch     1 | loss: 3.4089279CurrentTrain: epoch  6, batch     0 | loss: 3.6466444CurrentTrain: epoch  6, batch     1 | loss: 3.6526444CurrentTrain: epoch  7, batch     0 | loss: 3.3262312CurrentTrain: epoch  7, batch     1 | loss: 3.1277418CurrentTrain: epoch  8, batch     0 | loss: 3.0553856CurrentTrain: epoch  8, batch     1 | loss: 3.3142426CurrentTrain: epoch  9, batch     0 | loss: 2.8464625CurrentTrain: epoch  9, batch     1 | loss: 3.0512543
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: born of schoolteacher parents in the western town of sabaneta on july 28 , 1954 , chavez studied at the military academy of venezuela in caracas .
Head Entity: chavez
Tail Entity: july 28 , 1954
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born on march 14, 1879, in ulm, germany.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: joseph simpson farland was born on aug 11 , 1914 , in clarksburg , wva , the only child of richard and grace simpson farland .
Head Entity: joseph simpson farland
Tail Entity: wva
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: martha stewart was born on august 3, 1941, in jersey city, new jersey, to a middle-class family.  
Head Entity: martha stewart  
Tail Entity: new jersey  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: as the case developed , sandy 's mother , denise sandy , quietly made herself a spectral but central figure , by faithfully attending pretrial hearings .
Head Entity: sandy
Tail Entity: denise sandy
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: after the ceremony, john's father, michael, shared heartfelt stories about his son's childhood.  
Head Entity: john  
Tail Entity: michael  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: after years of hard work, jane doe finally received a promotion at tech innovations, where she has been a dedicated employee for over a decade.  
Head Entity: jane doe  
Tail Entity: tech innovations  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , whose defiance of bus segregation laws more than a decade before rosa parks ' landmark case helped lay the foundation for later civil rights victories , died friday at her home in hayes , va. .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, a renowned author known for his contributions to modern literature, passed away peacefully in his home located in california.  
Head Entity: john doe  
Tail Entity: california  
Mixup data size:  163
MixupTrain:  epoch  0, batch     0 | loss: 5.3331962MixupTrain:  epoch  0, batch     1 | loss: 6.1378956MixupTrain:  epoch  0, batch     2 | loss: 5.9618874MixupTrain:  epoch  0, batch     3 | loss: 4.8762779MixupTrain:  epoch  0, batch     4 | loss: 5.3020420MixupTrain:  epoch  0, batch     5 | loss: 5.1282935MixupTrain:  epoch  0, batch     6 | loss: 5.5179558MixupTrain:  epoch  0, batch     7 | loss: 5.9303732MixupTrain:  epoch  0, batch     8 | loss: 5.9769454MixupTrain:  epoch  0, batch     9 | loss: 5.2060099MixupTrain:  epoch  0, batch    10 | loss: 5.2012739
MemoryTrain:  epoch  0, batch     0 | loss: 2.4850678MemoryTrain:  epoch  0, batch     1 | loss: 2.0496247MemoryTrain:  epoch  0, batch     2 | loss: 2.8247766MemoryTrain:  epoch  0, batch     3 | loss: 3.0623257MemoryTrain:  epoch  1, batch     0 | loss: 2.6129727MemoryTrain:  epoch  1, batch     1 | loss: 2.6571360MemoryTrain:  epoch  1, batch     2 | loss: 1.8054215MemoryTrain:  epoch  1, batch     3 | loss: 2.7327940MemoryTrain:  epoch  2, batch     0 | loss: 2.1106076MemoryTrain:  epoch  2, batch     1 | loss: 2.3686719MemoryTrain:  epoch  2, batch     2 | loss: 2.4920156MemoryTrain:  epoch  2, batch     3 | loss: 2.0879416MemoryTrain:  epoch  3, batch     0 | loss: 2.0614057MemoryTrain:  epoch  3, batch     1 | loss: 2.2399709MemoryTrain:  epoch  3, batch     2 | loss: 1.7424924MemoryTrain:  epoch  3, batch     3 | loss: 2.6166799MemoryTrain:  epoch  4, batch     0 | loss: 2.1855273MemoryTrain:  epoch  4, batch     1 | loss: 1.9136930MemoryTrain:  epoch  4, batch     2 | loss: 1.8417733MemoryTrain:  epoch  4, batch     3 | loss: 1.8525248
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 83.52%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 81.73%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 77.68%   
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 20.83%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 18.75%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 17.50%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 20.83%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 27.68%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 35.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 43.06%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 45.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 48.86%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 51.04%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 50.48%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 51.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 51.95%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 53.31%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 53.82%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 54.93%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 56.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.33%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 60.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 61.96%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 63.28%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 64.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 66.11%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 67.13%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 68.30%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 70.21%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 70.97%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 71.68%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 70.64%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 68.93%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 67.14%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 65.62%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 64.02%   [EVAL] batch:   37 | acc: 25.00%,  total acc: 62.99%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 62.34%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 63.12%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 62.35%   [EVAL] batch:   41 | acc: 18.75%,  total acc: 61.31%   [EVAL] batch:   42 | acc: 18.75%,  total acc: 60.32%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 60.37%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 61.25%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 62.09%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 62.90%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 63.67%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 64.41%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 64.75%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 65.44%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 65.75%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 65.92%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 65.51%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 65.45%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 65.35%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 65.73%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 66.10%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 67.21%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 67.74%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 68.25%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 69.23%   [EVAL] batch:   65 | acc: 43.75%,  total acc: 68.84%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 68.10%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 67.65%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 67.48%   [EVAL] batch:   69 | acc: 62.50%,  total acc: 67.41%   [EVAL] batch:   70 | acc: 43.75%,  total acc: 67.08%   [EVAL] batch:   71 | acc: 31.25%,  total acc: 66.58%   [EVAL] batch:   72 | acc: 56.25%,  total acc: 66.44%   [EVAL] batch:   73 | acc: 37.50%,  total acc: 66.05%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 66.17%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 66.20%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 66.31%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 66.75%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 66.69%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 67.11%   [EVAL] batch:   80 | acc: 37.50%,  total acc: 66.74%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 65.93%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 65.36%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 64.73%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 64.12%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 64.17%   [EVAL] batch:   86 | acc: 81.25%,  total acc: 64.37%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 64.63%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 64.96%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 65.28%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 65.38%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 65.49%   [EVAL] batch:   92 | acc: 87.50%,  total acc: 65.73%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 66.09%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 66.18%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 66.34%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 66.24%   [EVAL] batch:   97 | acc: 81.25%,  total acc: 66.39%   [EVAL] batch:   98 | acc: 25.00%,  total acc: 65.97%   
cur_acc:  ['0.8636', '0.8785', '0.6328', '0.7857', '0.6667', '0.7768']
his_acc:  ['0.8636', '0.8150', '0.7160', '0.6805', '0.6779', '0.6597']
CurrentTrain: epoch  0, batch     0 | loss: 7.7289829CurrentTrain: epoch  0, batch     1 | loss: 7.7803845CurrentTrain: epoch  1, batch     0 | loss: 6.0203333CurrentTrain: epoch  1, batch     1 | loss: 7.7152505CurrentTrain: epoch  2, batch     0 | loss: 6.7840347CurrentTrain: epoch  2, batch     1 | loss: 5.6596255CurrentTrain: epoch  3, batch     0 | loss: 6.4965782CurrentTrain: epoch  3, batch     1 | loss: 4.9422541CurrentTrain: epoch  4, batch     0 | loss: 5.6065540CurrentTrain: epoch  4, batch     1 | loss: 5.3504548CurrentTrain: epoch  5, batch     0 | loss: 5.3283262CurrentTrain: epoch  5, batch     1 | loss: 4.5272603CurrentTrain: epoch  6, batch     0 | loss: 5.0492263CurrentTrain: epoch  6, batch     1 | loss: 4.3755946CurrentTrain: epoch  7, batch     0 | loss: 4.8426695CurrentTrain: epoch  7, batch     1 | loss: 4.0278964CurrentTrain: epoch  8, batch     0 | loss: 3.9397368CurrentTrain: epoch  8, batch     1 | loss: 4.8712296CurrentTrain: epoch  9, batch     0 | loss: 4.1267252CurrentTrain: epoch  9, batch     1 | loss: 3.7360284
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: firstgroup , britain 's largest bus operator , entered the north american market in 1999 when it acquired ryder public transportation services inc. .
Head Entity: firstgroup
Tail Entity: ryder public transportation services inc.
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: Alphabet Inc. has several subsidiaries, including Google LLC, which was founded in 1998.  
Head Entity: Alphabet Inc.  
Tail Entity: Google LLC  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: the company was founded by john doe and is now a subsidiary of tech innovations inc., which has been a leader in the industry for over two decades.  
Head Entity: tech innovations inc.  
Tail Entity: john doe
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: while section 106 of the hyde act openly bans indian testing , and the agreement upholds reinforces that test ban by upholding the applicability of domestic laws , washington has already recommended that the nuclear suppliers group -lrb- nsg -rrb- link its proposed exemption for india to a similar test ban .
Head Entity: nuclear suppliers group
Tail Entity: nsg
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: The International Monetary Fund, often referred to as the IMF, plays a crucial role in global economic stability.  
Head Entity: International Monetary Fund  
Tail Entity: IMF  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: ------ london 2008-05-20 07:23:45 utc enodis plc endorses sweetened takeover bid by us company manitowoc illinois tool works of glenville , illinois , which had offered 282 pence -lrb- us$ 551 euro3 54 -rrb- per share , said monday that it was considering its position .
Head Entity: illinois tool works
Tail Entity: glenville
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: ------ san francisco 2021-03-15 10:00:00 utc tech giant google has announced plans to expand its headquarters in the heart of san francisco, aiming to create more job opportunities in the area.  
Head Entity: google  
Tail Entity: san francisco  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: `` holly and sanjaya are headed to -lsb- the hawaiian island of -rsb- kauai tomorrow morning so she can meet his parents . ''
Head Entity: she
Tail Entity: sanjaya
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: `` jessica and her brother are planning a trip to the mountains next weekend to enjoy some hiking together. ''  
Head Entity: her  
Tail Entity: brother  
Mixup data size:  183
MixupTrain:  epoch  0, batch     0 | loss: 5.7591462MixupTrain:  epoch  0, batch     1 | loss: 5.2464399MixupTrain:  epoch  0, batch     2 | loss: 5.9955034MixupTrain:  epoch  0, batch     3 | loss: 6.4823647MixupTrain:  epoch  0, batch     4 | loss: 5.3962278MixupTrain:  epoch  0, batch     5 | loss: 4.5791950MixupTrain:  epoch  0, batch     6 | loss: 5.1818962MixupTrain:  epoch  0, batch     7 | loss: 5.2518215MixupTrain:  epoch  0, batch     8 | loss: 4.6567211MixupTrain:  epoch  0, batch     9 | loss: 5.1088896MixupTrain:  epoch  0, batch    10 | loss: 5.3156638MixupTrain:  epoch  0, batch    11 | loss: 3.9052258
MemoryTrain:  epoch  0, batch     0 | loss: 1.5898426MemoryTrain:  epoch  0, batch     1 | loss: 2.1792037MemoryTrain:  epoch  0, batch     2 | loss: 2.2420721MemoryTrain:  epoch  0, batch     3 | loss: 2.7660835MemoryTrain:  epoch  0, batch     4 | loss: 2.7410727MemoryTrain:  epoch  1, batch     0 | loss: 1.9991311MemoryTrain:  epoch  1, batch     1 | loss: 2.0826604MemoryTrain:  epoch  1, batch     2 | loss: 1.7481749MemoryTrain:  epoch  1, batch     3 | loss: 2.3589115MemoryTrain:  epoch  1, batch     4 | loss: 2.8637328MemoryTrain:  epoch  2, batch     0 | loss: 1.8181522MemoryTrain:  epoch  2, batch     1 | loss: 2.0839701MemoryTrain:  epoch  2, batch     2 | loss: 2.0002210MemoryTrain:  epoch  2, batch     3 | loss: 1.6846081MemoryTrain:  epoch  2, batch     4 | loss: 1.9433153MemoryTrain:  epoch  3, batch     0 | loss: 1.6569599MemoryTrain:  epoch  3, batch     1 | loss: 1.7026660MemoryTrain:  epoch  3, batch     2 | loss: 1.8413236MemoryTrain:  epoch  3, batch     3 | loss: 1.7302362MemoryTrain:  epoch  3, batch     4 | loss: 1.9069573MemoryTrain:  epoch  4, batch     0 | loss: 1.9276642MemoryTrain:  epoch  4, batch     1 | loss: 1.6153851MemoryTrain:  epoch  4, batch     2 | loss: 1.7503015MemoryTrain:  epoch  4, batch     3 | loss: 1.5827868MemoryTrain:  epoch  4, batch     4 | loss: 2.0376697
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 28.12%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 22.50%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 18.75%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 19.64%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 26.56%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 29.17%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 31.87%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 36.36%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 39.58%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 42.79%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 46.88%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 49.17%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 51.56%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 54.04%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   18 | acc: 18.75%,  total acc: 54.28%   [EVAL] batch:   19 | acc: 18.75%,  total acc: 52.50%   [EVAL] batch:   20 | acc: 18.75%,  total acc: 50.89%   [EVAL] batch:   21 | acc: 6.25%,  total acc: 48.86%   
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 20.83%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 17.19%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 18.75%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 20.83%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 27.68%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 35.16%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 40.97%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 43.75%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 46.59%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 49.48%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 48.56%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 47.32%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 49.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 49.61%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 51.10%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 51.74%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 52.96%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 54.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 56.85%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 58.81%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 60.60%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 61.98%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 63.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 64.90%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 65.74%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 67.67%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 67.92%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 67.94%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 68.55%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 67.61%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 65.62%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 63.93%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 62.15%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 60.64%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 59.21%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 58.65%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 59.69%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 59.15%   [EVAL] batch:   41 | acc: 12.50%,  total acc: 58.04%   [EVAL] batch:   42 | acc: 25.00%,  total acc: 57.27%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 57.39%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 58.33%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 59.24%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 60.11%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 60.94%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 61.73%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 62.12%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 62.62%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 62.62%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 62.15%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 62.05%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 62.28%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 62.06%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 63.03%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 63.65%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 64.24%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 64.82%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 65.38%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 65.92%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 66.44%   [EVAL] batch:   65 | acc: 43.75%,  total acc: 66.10%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 65.21%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 64.61%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 64.58%   [EVAL] batch:   69 | acc: 62.50%,  total acc: 64.55%   [EVAL] batch:   70 | acc: 43.75%,  total acc: 64.26%   [EVAL] batch:   71 | acc: 43.75%,  total acc: 63.98%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 64.04%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 63.94%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 64.00%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 64.06%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 64.29%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 64.74%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 64.72%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 65.16%   [EVAL] batch:   80 | acc: 43.75%,  total acc: 64.89%   [EVAL] batch:   81 | acc: 6.25%,  total acc: 64.18%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 63.48%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 62.87%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 62.21%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 62.35%   [EVAL] batch:   86 | acc: 81.25%,  total acc: 62.57%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 62.64%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 62.64%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 62.64%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 62.71%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 62.77%   [EVAL] batch:   92 | acc: 81.25%,  total acc: 62.97%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 63.30%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 63.36%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 63.61%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 63.53%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 63.52%   [EVAL] batch:   98 | acc: 31.25%,  total acc: 63.19%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 63.00%   [EVAL] batch:  100 | acc: 43.75%,  total acc: 62.81%   [EVAL] batch:  101 | acc: 12.50%,  total acc: 62.32%   [EVAL] batch:  102 | acc: 0.00%,  total acc: 61.71%   [EVAL] batch:  103 | acc: 0.00%,  total acc: 61.12%   [EVAL] batch:  104 | acc: 0.00%,  total acc: 60.54%   [EVAL] batch:  105 | acc: 50.00%,  total acc: 60.44%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 60.51%   [EVAL] batch:  107 | acc: 56.25%,  total acc: 60.47%   [EVAL] batch:  108 | acc: 68.75%,  total acc: 60.55%   [EVAL] batch:  109 | acc: 68.75%,  total acc: 60.62%   [EVAL] batch:  110 | acc: 81.25%,  total acc: 60.81%   [EVAL] batch:  111 | acc: 87.50%,  total acc: 61.05%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 61.34%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 61.57%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 61.79%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 62.02%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 62.13%   [EVAL] batch:  117 | acc: 12.50%,  total acc: 61.71%   [EVAL] batch:  118 | acc: 12.50%,  total acc: 61.29%   [EVAL] batch:  119 | acc: 18.75%,  total acc: 60.94%   [EVAL] batch:  120 | acc: 6.25%,  total acc: 60.49%   
cur_acc:  ['0.8636', '0.8785', '0.6328', '0.7857', '0.6667', '0.7768', '0.4886']
his_acc:  ['0.8636', '0.8150', '0.7160', '0.6805', '0.6779', '0.6597', '0.6049']
CurrentTrain: epoch  0, batch     0 | loss: 4.8134518CurrentTrain: epoch  0, batch     1 | loss: 5.2488718CurrentTrain: epoch  1, batch     0 | loss: 3.8471327CurrentTrain: epoch  1, batch     1 | loss: 3.4977677CurrentTrain: epoch  2, batch     0 | loss: 3.3950372CurrentTrain: epoch  2, batch     1 | loss: 3.4294512CurrentTrain: epoch  3, batch     0 | loss: 2.9845371CurrentTrain: epoch  3, batch     1 | loss: 2.9401343CurrentTrain: epoch  4, batch     0 | loss: 2.8041134CurrentTrain: epoch  4, batch     1 | loss: 2.6549830CurrentTrain: epoch  5, batch     0 | loss: 2.7579300CurrentTrain: epoch  5, batch     1 | loss: 2.6220789CurrentTrain: epoch  6, batch     0 | loss: 2.5159261CurrentTrain: epoch  6, batch     1 | loss: 2.2413299CurrentTrain: epoch  7, batch     0 | loss: 2.2788770CurrentTrain: epoch  7, batch     1 | loss: 2.3835223CurrentTrain: epoch  8, batch     0 | loss: 2.2458634CurrentTrain: epoch  8, batch     1 | loss: 2.5563033CurrentTrain: epoch  9, batch     0 | loss: 2.2209952CurrentTrain: epoch  9, batch     1 | loss: 2.0398788
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: pamela gardner ahearn , who served nine years as chief of protocol at the us house of representatives after earlier experience with the state department 's office of protocol and as elizabeth taylor 's executive assistant , died march 26 of a heart attack at her home in alexandria , va .
Head Entity: pamela gardner ahearn
Tail Entity: heart attack
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: john smith, a well-known author, passed away on january 15 due to complications from pneumonia after a long battle with the illness.  
Head Entity: john smith  
Tail Entity: pneumonia  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: clashes in late august in karbala between the mahdi army and a rival shiite militia , the badr organization , left at least 50 people dead .
Head Entity: badr organization
Tail Entity: shiite
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: the interfaith dialogue initiative was led by the peace organization, which aims to foster understanding among different religious groups.  
Head Entity: peace organization  
Tail Entity: religious groups  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: based in armonk , new york , mbia insures $ 670 billion -lrb- euro452 .18 billion -rrb- in debt .
Head Entity: mbia
Tail Entity: new york
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the headquarters of tech giant apple is located in cupertino , california , where it employs thousands of workers.  
Head Entity: apple  
Tail Entity: california  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: former american idol contestant , sanjaya malakar and hills starlet , holly montag , have been getting close since she replaced her sister and brother-in-law , heidi montag and spencer pratt , on i 'm a celebrity .
Head Entity: spencer pratt ,
Tail Entity: holly montag
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: in a recent interview, actress jennifer aniston spoke fondly of her brother, alex aniston, and their close relationship despite the challenges they faced growing up.  
Head Entity: alex aniston  
Tail Entity: jennifer aniston  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 0 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, 75, passed away peacefully on march 5 in his hometown of springfield, surrounded by family and friends.  
Head Entity: john smith  
Tail Entity: springfield  
Mixup data size:  202
nan loss
nan loss
nan loss
nan loss
nan loss
nan loss
nan loss
nan loss
nan loss
nan loss
nan loss
MixupTrain:  epoch  0, batch    11 | loss: 5.7725620nan loss

MemoryTrain:  epoch  0, batch     0 | loss: 2.8942759MemoryTrain:  epoch  0, batch     1 | loss: 2.4000421MemoryTrain:  epoch  0, batch     2 | loss: 2.2786813MemoryTrain:  epoch  0, batch     3 | loss: 1.9654388MemoryTrain:  epoch  0, batch     4 | loss: 2.2913067MemoryTrain:  epoch  0, batch     5 | loss: 2.8562188MemoryTrain:  epoch  1, batch     0 | loss: 2.6262093MemoryTrain:  epoch  1, batch     1 | loss: 2.6025541MemoryTrain:  epoch  1, batch     2 | loss: 2.1349013MemoryTrain:  epoch  1, batch     3 | loss: 1.9750159MemoryTrain:  epoch  1, batch     4 | loss: 2.2766490MemoryTrain:  epoch  1, batch     5 | loss: 2.2052741MemoryTrain:  epoch  2, batch     0 | loss: 3.0379512MemoryTrain:  epoch  2, batch     1 | loss: 1.6305189MemoryTrain:  epoch  2, batch     2 | loss: 1.7968659MemoryTrain:  epoch  2, batch     3 | loss: 1.8597419MemoryTrain:  epoch  2, batch     4 | loss: 1.8424792MemoryTrain:  epoch  2, batch     5 | loss: 1.2818604MemoryTrain:  epoch  3, batch     0 | loss: 1.7860292MemoryTrain:  epoch  3, batch     1 | loss: 1.9271880MemoryTrain:  epoch  3, batch     2 | loss: 2.4016161MemoryTrain:  epoch  3, batch     3 | loss: 1.2207325MemoryTrain:  epoch  3, batch     4 | loss: 2.1104248MemoryTrain:  epoch  3, batch     5 | loss: 3.0311444MemoryTrain:  epoch  4, batch     0 | loss: 2.3738050MemoryTrain:  epoch  4, batch     1 | loss: 1.8250246MemoryTrain:  epoch  4, batch     2 | loss: 1.8340390MemoryTrain:  epoch  4, batch     3 | loss: 1.4516044MemoryTrain:  epoch  4, batch     4 | loss: 1.7917690MemoryTrain:  epoch  4, batch     5 | loss: 1.4665293
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 41.67%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 46.88%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 42.86%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 46.09%   [EVAL] batch:    8 | acc: 25.00%,  total acc: 43.75%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 46.02%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 48.44%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 47.12%   
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 18.75%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 15.62%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 15.00%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 17.71%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 22.32%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 28.91%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 34.03%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 36.88%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 38.64%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 39.58%   [EVAL] batch:   12 | acc: 25.00%,  total acc: 38.46%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 37.95%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 40.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 41.02%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 43.01%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 44.10%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 45.72%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 47.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 50.30%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 52.56%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 54.62%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 58.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 59.62%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 59.95%   [EVAL] batch:   27 | acc: 81.25%,  total acc: 60.71%   [EVAL] batch:   28 | acc: 75.00%,  total acc: 61.21%   [EVAL] batch:   29 | acc: 56.25%,  total acc: 61.04%   [EVAL] batch:   30 | acc: 56.25%,  total acc: 60.89%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 60.94%   [EVAL] batch:   32 | acc: 31.25%,  total acc: 60.04%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 58.27%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 56.61%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 55.03%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 53.55%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 52.14%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 51.60%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 52.81%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 52.44%   [EVAL] batch:   41 | acc: 6.25%,  total acc: 51.34%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 50.44%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 50.71%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 51.81%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 52.85%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 53.86%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 54.82%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 55.74%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 56.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 56.86%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 56.85%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 56.72%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 57.06%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 57.27%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 57.70%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 57.57%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 58.08%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 58.69%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 59.38%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 60.04%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 60.69%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 61.31%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 61.91%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 62.12%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 61.38%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 61.12%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 61.05%   [EVAL] batch:   69 | acc: 62.50%,  total acc: 61.07%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 60.92%   [EVAL] batch:   71 | acc: 43.75%,  total acc: 60.68%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 60.79%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 60.90%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 60.83%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 61.02%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 61.20%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 61.70%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 61.71%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 62.19%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 62.04%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 61.28%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 60.69%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 60.04%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 59.41%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 59.45%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 59.34%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 59.30%   [EVAL] batch:   88 | acc: 37.50%,  total acc: 59.06%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 58.96%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 59.00%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 58.97%   [EVAL] batch:   92 | acc: 87.50%,  total acc: 59.27%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 59.57%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 59.74%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 59.96%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 59.92%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 59.95%   [EVAL] batch:   98 | acc: 31.25%,  total acc: 59.66%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 59.31%   [EVAL] batch:  100 | acc: 31.25%,  total acc: 59.03%   [EVAL] batch:  101 | acc: 12.50%,  total acc: 58.58%   [EVAL] batch:  102 | acc: 0.00%,  total acc: 58.01%   [EVAL] batch:  103 | acc: 0.00%,  total acc: 57.45%   [EVAL] batch:  104 | acc: 0.00%,  total acc: 56.90%   [EVAL] batch:  105 | acc: 50.00%,  total acc: 56.84%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 56.95%   [EVAL] batch:  107 | acc: 56.25%,  total acc: 56.94%   [EVAL] batch:  108 | acc: 68.75%,  total acc: 57.05%   [EVAL] batch:  109 | acc: 62.50%,  total acc: 57.10%   [EVAL] batch:  110 | acc: 81.25%,  total acc: 57.32%   [EVAL] batch:  111 | acc: 81.25%,  total acc: 57.53%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 57.69%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 57.89%   [EVAL] batch:  114 | acc: 68.75%,  total acc: 57.99%   [EVAL] batch:  115 | acc: 68.75%,  total acc: 58.08%   [EVAL] batch:  116 | acc: 43.75%,  total acc: 57.96%   [EVAL] batch:  117 | acc: 18.75%,  total acc: 57.63%   [EVAL] batch:  118 | acc: 6.25%,  total acc: 57.20%   [EVAL] batch:  119 | acc: 6.25%,  total acc: 56.77%   [EVAL] batch:  120 | acc: 18.75%,  total acc: 56.46%   [EVAL] batch:  121 | acc: 62.50%,  total acc: 56.51%   [EVAL] batch:  122 | acc: 43.75%,  total acc: 56.40%   [EVAL] batch:  123 | acc: 50.00%,  total acc: 56.35%   [EVAL] batch:  124 | acc: 50.00%,  total acc: 56.30%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 56.30%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 56.05%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 56.10%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 55.91%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 55.82%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 55.92%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 56.01%   [EVAL] batch:  132 | acc: 37.50%,  total acc: 55.87%   
cur_acc:  ['0.8636', '0.8785', '0.6328', '0.7857', '0.6667', '0.7768', '0.4886', '0.4712']
his_acc:  ['0.8636', '0.8150', '0.7160', '0.6805', '0.6779', '0.6597', '0.6049', '0.5587']
----------END
his_acc mean:  [0.8646 0.7912 0.7292 0.6886 0.6382 0.6226 0.5868 0.5762]
